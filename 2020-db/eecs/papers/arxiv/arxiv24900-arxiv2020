arxiv-1701-08401 | When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum | http://arxiv.org/abs/1701.08401 | id:1701.08401 author:Dimitri Van De Ville, Robin Demesmaeker, Maria Giulia Preti category:cs.LG cs.CV  published:2017-01-29 summary:Network models play an important role in studying complex systems in many scientific disciplines. Graph signal processing is receiving growing interest as to design novel tools to combine the analysis of topology and signals. The graph Fourier transform, defined as the eigendecomposition of the graph Laplacian, allows extending conventional signal-processing operations to graphs. One main feature is to let emerge global organization from local interactions; i.e., the Fiedler vector has the smallest non-zero eigenvalue and is key for Laplacian embedding and graph clustering. Here, we introduce the design of Slepian graph signals, by maximizing energy concentration in a predefined subgraph for a given spectral bandlimit. We also establish a link with classical Laplacian embedding and graph clustering, for which the graph Slepian design can serve as a generalization. version:1
arxiv-1701-08398 | Re-ranking Person Re-identification with k-reciprocal Encoding | http://arxiv.org/abs/1701.08398 | id:1701.08398 author:Zhun Zhong, Liang Zheng, Donglin Cao, Shaozi Li category:cs.CV  published:2017-01-29 summary:When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method. version:1
arxiv-1701-08393 | Faceness-Net: Face Detection through Deep Facial Part Responses | http://arxiv.org/abs/1701.08393 | id:1701.08393 author:Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2017-01-29 summary:We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE. version:1
arxiv-1701-08381 | Random Forest regression for manifold-valued responses | http://arxiv.org/abs/1701.08381 | id:1701.08381 author:Dimosthenis Tsagkrasoulis, Giovanni Montana category:stat.ML stat.ME  published:2017-01-29 summary:An increasing array of biomedical and computer vision applications requires the predictive modeling of complex data, for example images and shapes. The main challenge when predicting such objects lies in the fact that they do not comply to the assumptions of Euclidean geometry. Rather, they occupy non-linear spaces, a.k.a. manifolds, where it is difficult to define concepts such as coordinates, vectors and expected values. In this work, we construct a non-parametric predictive methodology for manifold-valued objects, based on a distance modification of the Random Forest algorithm. Our method is versatile and can be applied both in cases where the response space is a well-defined manifold, but also when such knowledge is not available. Model fitting and prediction phases only require the definition of a suitable distance function for the observed responses. We validate our methodology using simulations and apply it on a series of illustrative image completion applications, showcasing superior predictive performance, compared to various established regression methods. version:1
arxiv-1701-08380 | The HASYv2 dataset | http://arxiv.org/abs/1701.08380 | id:1701.08380 author:Martin Thoma category:cs.CV  published:2017-01-29 summary:This paper describes the HASYv2 dataset. HASY is a publicly available, free of charge dataset of single symbols similar to MNIST. It contains 168233 instances of 369 classes. HASY contains two challenges: A classification challenge with 10 pre-defined folds for 10-fold cross-validation and a verification challenge. version:1
arxiv-1701-08378 | MSCM-LiFe: Multi-scale cross modal linear feature for horizon detection in maritime images | http://arxiv.org/abs/1701.08378 | id:1701.08378 author:D. K. Prasad, D. Rajan, C. K. Prasath, L. Rachmawati, E. Rajabaly, C. Quek category:cs.CV  published:2017-01-29 summary:This paper proposes a new method for horizon detection called the multi-scale cross modal linear feature. This method integrates three different concepts related to the presence of horizon in maritime images to increase the accuracy of horizon detection. Specifically it uses the persistence of horizon in multi-scale median filtering, and its detection as a linear feature commonly detected by two different methods, namely the Hough transform of edgemap and the intensity gradient. We demonstrate the performance of the method over 13 videos comprising of more than 3000 frames and show that the proposed method detects horizon with small error in most of the cases, outperforming three state-of-the-art methods. version:1
arxiv-1701-08376 | VINet: Visual-Inertial Odometry as a Sequence-to-Sequence Learning Problem | http://arxiv.org/abs/1701.08376 | id:1701.08376 author:Ronald Clark, Sen Wang, Hongkai Wen, Andrew Markham, Niki Trigoni category:cs.CV  published:2017-01-29 summary:In this paper we present an on-manifold sequence-to-sequence learning approach to motion estimation using visual and inertial sensors. It is to the best of our knowledge the first end-to-end trainable method for visual-inertial odometry which performs fusion of the data at an intermediate feature-representation level. Our method has numerous advantages over traditional approaches. Specifically, it eliminates the need for tedious manual synchronization of the camera and IMU as well as eliminating the need for manual calibration between the IMU and camera. A further advantage is that our model naturally and elegantly incorporates domain specific information which significantly mitigates drift. We show that our approach is competitive with state-of-the-art traditional methods when accurate calibration data is available and can be trained to outperform them in the presence of calibration and synchronization errors. version:1
arxiv-1701-08374 | Feature base fusion for splicing forgery detection based on neuro fuzzy | http://arxiv.org/abs/1701.08374 | id:1701.08374 author:Habib Ghaffari Hadigheh, Ghazali bin sulong category:cs.CV cs.AI cs.LG  published:2017-01-29 summary:Most of researches on image forensics have been mainly focused on detection of artifacts introduced by a single processing tool. They lead in the development of many specialized algorithms looking for one or more particular footprints under specific settings. Naturally, the performance of such algorithms are not perfect, and accordingly the provided output might be noisy, inaccurate and only partially correct. Furthermore, a forged image in practical scenarios is often the result of utilizing several tools available by image-processing software systems. Therefore, reliable tamper detection requires developing more poweful tools to deal with various tempering scenarios. Fusion of forgery detection tools based on Fuzzy Inference System has been used before for addressing this problem. Adjusting the membership functions and defining proper fuzzy rules for attaining to better results are time-consuming processes. This can be accounted as main disadvantage of fuzzy inference systems. In this paper, a Neuro-Fuzzy inference system for fusion of forgery detection tools is developed. The neural network characteristic of these systems provides appropriate tool for automatically adjusting the membership functions. Moreover, initial fuzzy inference system is generated based on fuzzy clustering techniques. The proposed framework is implemented and validated on a benchmark image splicing data set in which three forgery detection tools are fused based on adaptive Neuro-Fuzzy inference system. The outcome of the proposed method reveals that applying Neuro Fuzzy inference systems could be a better approach for fusion of forgery detection tools. version:1
arxiv-1701-08349 | Supervised Multilayer Sparse Coding Networks for Image Classification | http://arxiv.org/abs/1701.08349 | id:1701.08349 author:Xiaoxia Sun, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV  published:2017-01-29 summary:In this paper, we propose a novel multilayer sparse coding network capable of efficiently adapting its own regularization parameters to a given dataset. The network is trained end-to-end with a supervised task-driven learning algorithm via error backpropagation. During training, the network learns both the dictionaries and the regularization parameters of each sparse coding layer so that the reconstructive dictionaries are smoothly transformed into increasingly discriminative representations. We also incorporate a new weighted sparse coding scheme into our sparse recovery procedure, offering the system more flexibility to adjust sparsity levels. Furthermore, we have devised a sparse coding layer utilizing a 'skinny' dictionary. Integral to computational efficiency, these skinny dictionaries compress the high dimensional sparse codes into lower dimensional structures. The adaptivity and discriminability of our 13-layer sparse coding network are demonstrated on four benchmark datasets, namely Cifar-10, Cifar-100, SVHN and MNIST, most of which are considered difficult for sparse coding models. Experimental results show that our architecture overwhelmingly outperforms traditional one-layer sparse coding architectures while using much fewer parameters. Moreover, our multilayer architecture fuses the benefits of depth with sparse coding's characteristic ability to operate on smaller datasets. In such data-constrained scenarios, we demonstrate our technique can overcome the limitations of deep neural networks by exceeding the state of the art in accuracy. version:1
arxiv-1701-08341 | Pooling Facial Segments to Face: The Shallow and Deep Ends | http://arxiv.org/abs/1701.08341 | id:1701.08341 author:Upal Mahbub, Sayantan Sarkar, Rama Chellappa category:cs.CV  published:2017-01-29 summary:Generic face detection algorithms do not perform very well in the mobile domain due to significant presence of occluded and partially visible faces. One promising technique to handle the challenge of partial faces is to design face detectors based on facial segments. In this paper two such face detectors namely, SegFace and DeepSegFace, are proposed that detect the presence of a face given arbitrary combinations of certain face segments. Both methods use proposals from facial segments as input that are found using weak boosted classifiers. SegFace is a shallow and fast algorithm using traditional features, tailored for situations where real time constraints must be satisfied. On the other hand, DeepSegFace is a more powerful algorithm based on a deep convolutional neutral network (DCNN) architecture. DeepSegFace offers certain advantages over other DCNN-based face detectors as it requires relatively little amount of data to train by utilizing a novel data augmentation scheme and is very robust to occlusion by design. Extensive experiments show the superiority of the proposed methods, specially DeepSegFace, over other state-of-the-art face detectors in terms of precision-recall and ROC curve on two mobile face datasets. version:1
arxiv-1701-08340 | Extracting Bilingual Persian Italian Lexicon from Comparable Corpora Using Different Types of Seed Dictionaries | http://arxiv.org/abs/1701.08340 | id:1701.08340 author:Ebrahim Ansari, M. H. Sadreddini, Lucio Grandinetti, Mehdi Sheikhalishahi category:cs.CL  published:2017-01-29 summary:Bilingual dictionaries are very important in various fields of natural language processing. In recent years, research on extracting new bilingual lexicons from non-parallel (comparable) corpora have been proposed. Almost all use a small existing dictionary or other resource to make an initial list called the "seed dictionary". In this paper we discuss the use of different types of dictionaries as the initial starting list for creating a bilingual Persian-Italian lexicon from a comparable corpus. Our experiments apply state-of-the-art techniques on three different seed dictionaries; an existing dictionary, a dictionary created with pivot-based schema, and a dictionary extracted from a small Persian-Italian parallel text. The interesting challenge of our approach is to find a way to combine different dictionaries together in order to produce a better and more accurate lexicon. In order to combine seed dictionaries, we propose two different combination models and examine the effect of our novel combination models on various comparable corpora that have differing degrees of comparability. We conclude with a proposal for a new weighting system to improve the extracted lexicon. The experimental results produced by our implementation show the efficiency of our proposed models. version:1
arxiv-1701-08339 | Using English as Pivot to Extract Persian-Italian Parallel Sentences from Non-Parallel Corpora | http://arxiv.org/abs/1701.08339 | id:1701.08339 author:Ebrahim Ansari, M. H. Sadreddini, Mostafa Sheikhalishahi, Richard Wallace, Fatemeh Alimardani category:cs.CL  published:2017-01-29 summary:The effectiveness of a statistical machine translation system (SMT) is very dependent upon the amount of parallel corpus used in the training phase. For low-resource language pairs there are not enough parallel corpora to build an accurate SMT. In this paper, a novel approach is presented to extract bilingual Persian-Italian parallel sentences from a non-parallel (comparable) corpus. In this study, English is used as the pivot language to compute the matching scores between source and target sentences and candidate selection phase. Additionally, a new monolingual sentence similarity metric, Normalized Google Distance (NGD) is proposed to improve the matching process. Moreover, some extensions of the baseline system are applied to improve the quality of extracted sentences measured with BLEU. Experimental results show that using the new pivot based extraction can increase the quality of bilingual corpus significantly and consequently improves the performance of the Persian-Italian SMT system. version:1
arxiv-1701-08318 | Deep Recurrent Neural Network for Protein Function Prediction from Sequence | http://arxiv.org/abs/1701.08318 | id:1701.08318 author:Xueliang Liu category:q-bio.QM cs.LG q-bio.BM stat.ML  published:2017-01-28 summary:As high-throughput biological sequencing becomes faster and cheaper, the need to extract useful information from sequencing becomes ever more paramount, often limited by low-throughput experimental characterizations. For proteins, accurate prediction of their functions directly from their primary amino-acid sequences has been a long standing challenge. Here, machine learning using artificial recurrent neural networks (RNN) was applied towards classification of protein function directly from primary sequence without sequence alignment, heuristic scoring or feature engineering. The RNN models containing long-short-term-memory (LSTM) units trained on public, annotated datasets from UniProt achieved high performance for in-class prediction of four important protein functions tested, particularly compared to other machine learning algorithms using sequence-derived protein features. RNN models were used also for out-of-class predictions of phylogenetically distinct protein families with similar functions, including proteins of the CRISPR-associated nuclease, ferritin-like iron storage and cytochrome P450 families. Applying the trained RNN models on the partially unannotated UniRef100 database predicted not only candidates validated by existing annotations but also currently unannotated sequences. Some RNN predictions for the ferritin-like iron sequestering function were experimentally validated, even though their sequences differ significantly from known, characterized proteins and from each other and cannot be easily predicted using popular bioinformatics methods. As sequencing and experimental characterization data increases rapidly, the machine-learning approach based on RNN could be useful for discovery and prediction of homologues for a wide range of protein functions. version:1
arxiv-1701-09083 | Efficient Rank Aggregation via Lehmer Codes | http://arxiv.org/abs/1701.09083 | id:1701.09083 author:Pan Li, Arya Mazumdar, Olgica Milenkovic category:cs.LG cs.AI  published:2017-01-28 summary:We propose a novel rank aggregation method based on converting permutations into their corresponding Lehmer codes or other subdiagonal images. Lehmer codes, also known as inversion vectors, are vector representations of permutations in which each coordinate can take values not restricted by the values of other coordinates. This transformation allows for decoupling of the coordinates and for performing aggregation via simple scalar median or mode computations. We present simulation results illustrating the performance of this completely parallelizable approach and analytically prove that both the mode and median aggregation procedure recover the correct centroid aggregate with small sample complexity when the permutations are drawn according to the well-known Mallows models. The proposed Lehmer code approach may also be used on partial rankings, with similar performance guarantees. version:1
arxiv-1701-08305 | Multiclass MinMax Rank Aggregation | http://arxiv.org/abs/1701.08305 | id:1701.08305 author:Pan Li, Olgica Milenkovic category:cs.LG cs.AI q-bio.QM stat.ML  published:2017-01-28 summary:We introduce a new family of minmax rank aggregation problems under two distance measures, the Kendall {\tau} and the Spearman footrule. As the problems are NP-hard, we proceed to describe a number of constant-approximation algorithms for solving them. We conclude with illustrative applications of the aggregation methods on the Mallows model and genomic data. version:1
arxiv-1701-08303 | Drug-Drug Interaction Extraction from Biomedical Text Using Long Short Term Memory Network | http://arxiv.org/abs/1701.08303 | id:1701.08303 author:Sunil Kumar Sahu, Ashish Anand category:cs.CL  published:2017-01-28 summary:A drug can affect the activity of other drugs, when administered together, in both synergistic or antagonistic ways. In one hand synergistic effects lead to improved therapeutic outcomes, antagonistic consequences can be life-threatening, leading to increased healthcare cost, or may even cause death. Thus, identification of unknown drug-drug interaction (DDI) is an important concern for efficient and effective healthcare. Although there exist multiple resources for DDI, they often unable to keep pace with rich amount of information available in fast growing biomedical texts including literature. Most existing methods model DDI extraction from text as classification problem and mainly rely on handcrafted features. Some of these features further depends on domain specific tools. Recently neural network models using latent features has shown to be perform similar or better than the other existing models using handcrafted features. In this paper, we present three models namely, B-LSTM, AB-LSTM and Joint AB-LSTM based on long short-term memory (LSTM) network. All three models utilize word and position embedding as latent features and thus do not rely on feature engineering. Further use of bidirectional long short-term memory (Bi-LSTM) networks allow to extract optimal features from the whole sentence. The two models, AB-LSTM and Joint AB-LSTM also use attentive pooling in the output of Bi-LSTM layer to assign weights to features. Our experimental results on the SemEval-2013 DDI extraction dataset shows that the Joint AB-LSTM model outperforms all the existing methods, including those relying on handcrafted features. The other two proposed models also perform competitively with state-of-the-art methods. version:1
arxiv-1701-07368 | Deep Local Video Feature for Action Recognition | http://arxiv.org/abs/1701.07368 | id:1701.07368 author:Zhenzhong Lan, Yi Zhu, Alexander G. Hauptmann category:cs.CV  published:2017-01-25 summary:We investigate the problem of representing an entire video using CNN features for human action recognition. Currently, limited by GPU memory, we have not been able to feed a whole video into CNN/RNNs for end-to-end learning. A common practice is to use sampled frames as inputs and video labels as supervision. One major problem of this popular approach is that the local samples may not contain the information indicated by global labels. To deal with this problem, we propose to treat the deep networks trained on local inputs as local feature extractors. After extracting local features, we aggregate them into global features and train another mapping function on the same training data to map the global features into global labels. We study a set of problems regarding this new type of local features such as how to aggregate them into global features. Experimental results on HMDB51 and UCF101 datasets show that, for these new local features, a simple maximum pooling on the sparsely sampled features lead to significant performance improvement. version:2
arxiv-1701-08291 | Treelogy: A Novel Tree Classifier Utilizing Deep and Hand-crafted Representations | http://arxiv.org/abs/1701.08291 | id:1701.08291 author:İlke Çuğu, Eren Şener, Çağrı Erciyes, Burak Balcı, Emre Akın, Itır Önal, Ahmet Oğuz Akyüz category:cs.CV 68-06  published:2017-01-28 summary:We propose a novel tree classification system called Treelogy, that fuses deep representations with hand-crafted features obtained from leaf images to perform leaf-based plant classification. Key to this system are segmentation of the leaf from an untextured background, using convolutional neural networks (CNNs) for learning deep representations, extracting hand-crafted features with a number of image processing techniques, training a linear SVM with feature vectors, merging SVM and CNN results, and identifying the species from a dataset of 57 trees. Our classification results show that fusion of deep representations with hand-crafted features leads to the highest accuracy. The proposed algorithm is embedded in a smart-phone application, which is publicly available. Furthermore, our novel dataset comprised of 5408 leaf images is also made public for use of other researchers. version:1
arxiv-1701-08289 | Face Detection using Deep Learning: An Improved Faster RCNN Approach | http://arxiv.org/abs/1701.08289 | id:1701.08289 author:Xudong Sun, Pengcheng Wu, Steven C. H. Hoi category:cs.CV  published:2017-01-28 summary:In this report, we present a new face detection scheme using deep learning and achieve the state-of-the-art detection performance on the well-known FDDB face detetion benchmark evaluation. In particular, we improve the state-of-the-art faster RCNN framework by combining a number of strategies, including feature concatenation, hard negative mining, multi-scale training, model pretraining, and proper calibration of key parameters. As a consequence, the proposed scheme obtained the state-of-the-art face detection performance, making it the best model in terms of ROC curves among all the published methods on the FDDB benchmark. version:1
arxiv-1701-08280 | Pruned non-local means | http://arxiv.org/abs/1701.08280 | id:1701.08280 author:Sanjay Ghosh, Amit K. Mandal, Kunal N. Chaudhury category:cs.CV  published:2017-01-28 summary:In Non-Local Means (NLM), each pixel is denoised by performing a weighted averaging of its neighboring pixels, where the weights are computed using image patches. We demonstrate that the denoising performance of NLM can be improved by pruning the neighboring pixels, namely, by rejecting neighboring pixels whose weights are below a certain threshold $\lambda$. While pruning can potentially reduce pixel averaging in uniform-intensity regions, we demonstrate that there is generally an overall improvement in the denoising performance. In particular, the improvement comes from pixels situated close to edges and corners. The success of the proposed method strongly depends on the choice of the global threshold $\lambda$, which in turn depends on the noise level and the image characteristics. We show how Stein's unbiased estimator of the mean-squared error can be used to optimally tune $\lambda$, at a marginal computational overhead. We present some representative denoising results to demonstrate the superior performance of the proposed method over NLM and its variants. version:1
arxiv-1701-07717 | Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro | http://arxiv.org/abs/1701.07717 | id:1701.07717 author:Zhedong Zheng, Liang Zheng, Yi Yang category:cs.CV  published:2017-01-26 summary:In this paper, we mainly contribute a simple semi-supervised pipeline which only uses the original training set without extra data collection. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial networks (GANs) are used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO) scheme. It assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves a ResNet baseline. We verify the proposed method on a practical task: person re-identification (re-ID). This task aims to retrieve the query person from other cameras. We adopt DCGAN for sample generation, and a baseline convolutional neural network (CNN) for embedding learning. In our experiment, we show that adding the GAN-generated data effectively improves the discriminative ability of the learned feature embedding. We evaluate the re-ID performance on two large-scale datasets: Market1501 and CUHK03. We obtain +4.37% and +1.6% improvement in rank-1 precision over the CNN baseline on Market1501 and CUHK03, respectively. version:2
arxiv-1701-08269 | Systems of natural-language-facilitated human-robot cooperation: A review | http://arxiv.org/abs/1701.08269 | id:1701.08269 author:Rui Liu, Xiaoli Zhang category:cs.RO cs.AI cs.CL cs.HC  published:2017-01-28 summary:Natural-language-facilitated human-robot cooperation (NLC), in which natural language (NL) is used to share knowledge between a human and a robot for conducting intuitive human-robot cooperation (HRC), is continuously developing in the recent decade. Currently, NLC is used in several robotic domains such as manufacturing, daily assistance and health caregiving. It is necessary to summarize current NLC-based robotic systems and discuss the future developing trends, providing helpful information for future NLC research. In this review, we first analyzed the driving forces behind the NLC research. Regarding to a robot s cognition level during the cooperation, the NLC implementations then were categorized into four types {NL-based control, NL-based robot training, NL-based task execution, NL-based social companion} for comparison and discussion. Last based on our perspective and comprehensive paper review, the future research trends were discussed. version:1
arxiv-1701-08261 | Exploiting saliency for object segmentation from image level labels | http://arxiv.org/abs/1701.08261 | id:1701.08261 author:Seong Joon Oh, Rodrigo Benenson, Anna Khoreva, Zeynep Akata, Mario Fritz, Bernt Schiele category:cs.CV  published:2017-01-28 summary:There have been remarkable improvements in the semantic labelling task in the recent years. However, the state of the art methods rely on large-scale pixel-level annotations. This paper studies the problem of training a pixel-wise semantic labeller network from image-level annotations of the present object classes. Recently, it has been shown that high quality seeds indicating discriminative object regions can be obtained from image-level labels. Without additional information, obtaining the full extent of the object is an inherently ill-posed problem due to co-occurrences. We propose using a saliency model as additional information and hereby exploit prior knowledge on the object extent and image statistics. We show how to combine both information sources in order to recover 80% of the fully supervised performance - which is the new state of the art in weakly supervised training for pixel-wise semantic labelling. version:1
arxiv-1701-08259 | Detection, Segmentation and Recognition of Face and its Features Using Neural Network | http://arxiv.org/abs/1701.08259 | id:1701.08259 author:Smriti Tikoo, Nitin Malik category:cs.CV cs.NE  published:2017-01-28 summary:Face detection and recognition has been prevalent with research scholars and diverse approaches have been incorporated till date to serve purpose. The rampant advent of biometric analysis systems, which may be full body scanners, or iris detection and recognition systems and the finger print recognition systems, and surveillance systems deployed for safety and security purposes have contributed to inclination towards same. Advances has been made with frontal view, lateral view of the face or using facial expressions such as anger, happiness and gloominess, still images and video image to be used for detection and recognition. This led to newer methods for face detection and recognition to be introduced in achieving accurate results and economically feasible and extremely secure. Techniques such as Principal Component analysis (PCA), Independent component analysis (ICA), Linear Discriminant Analysis (LDA), have been the predominant ones to be used. But with improvements needed in the previous approaches Neural Networks based recognition was like boon to the industry. It not only enhanced the recognition but also the efficiency of the process. Choosing Backpropagation as the learning method was clearly out of its efficiency to recognize nonlinear faces with an acceptance ratio of more than 90% and execution time of only few seconds. version:1
arxiv-1701-08257 | Detection of Face using Viola Jones and Recognition using Back Propagation Neural Network | http://arxiv.org/abs/1701.08257 | id:1701.08257 author:Smriti Tikoo, Nitin Malik category:cs.CV  published:2017-01-28 summary:Detection and recognition of the facial images of people is an intricate problem which has garnered much attention during recent years due to its ever increasing applications in numerous fields. It continues to pose a challenge in finding a robust solution to it. Its scope extends to catering the security, commercial and law enforcement applications. Research for moreover a decade on this subject has brought about remarkable development with the modus operandi like human computer interaction, biometric analysis and content based coding of images, videos and surveillance. A trivial task for brain but cumbersome to be imitated artificially. The commonalities in faces does pose a problem on various grounds but features such as skin color, gender differentiate a person from the other. In this paper the facial detection has been carried out using Viola-Jones algorithm and recognition of face has been done using Back Propagation Neural Network (BPNN). version:1
arxiv-1701-08254 | Entropic Causality and Greedy Minimum Entropy Coupling | http://arxiv.org/abs/1701.08254 | id:1701.08254 author:Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath, Babak Hassibi category:cs.IT cs.AI math.IT stat.ML  published:2017-01-28 summary:We study the problem of identifying the causal relationship between two discrete random variables from observational data. We recently proposed a novel framework called entropic causality that works in a very general functional model but makes the assumption that the unobserved exogenous variable has small entropy in the true causal direction. This framework requires the solution of a minimum entropy coupling problem: Given marginal distributions of m discrete random variables, each on n states, find the joint distribution with minimum entropy, that respects the given marginals. This corresponds to minimizing a concave function of nm variables over a convex polytope defined by nm linear constraints, called a transportation polytope. Unfortunately, it was recently shown that this minimum entropy coupling problem is NP-hard, even for 2 variables with n states. Even representing points (joint distributions) over this space can require exponential complexity (in n, m) if done naively. In our recent work we introduced an efficient greedy algorithm to find an approximate solution for this problem. In this paper we analyze this algorithm and establish two results: that our algorithm always finds a local minimum and also is within an additive approximation error from the unknown global optimum. version:1
arxiv-1701-08251 | Image-Grounded Conversations: Multimodal Context for Natural Question and Response Generation | http://arxiv.org/abs/1701.08251 | id:1701.08251 author:Nasrin Mostafazadeh, Chris Brockett, Bill Dolan, Michel Galley, Jianfeng Gao, Georgios P. Spithourakis, Lucy Vanderwende category:cs.CL cs.AI cs.CV  published:2017-01-28 summary:The popularity of image sharing on social media reflects the important role visual context plays in everyday conversation. In this paper, we present a novel task, Image-Grounded Conversations (IGC), in which natural-sounding conversations are generated about shared photographic images. We investigate this task using training data derived from image-grounded conversations on social media and introduce a new dataset of crowd-sourced conversations for benchmarking progress. Experiments using deep neural network models trained on social media data show that the combination of visual and textual context can enhance the quality of generated conversational turns. In human evaluation, a gap between human performance and that of both neural and retrieval architectures suggests that IGC presents an interesting challenge for vision and language research. version:1
arxiv-1701-08237 | An Efficient Algebraic Solution to the Perspective-Three-Point Problem | http://arxiv.org/abs/1701.08237 | id:1701.08237 author:Tong Ke, Stergios Roumeliotis category:cs.CV  published:2017-01-28 summary:In this work, we present an algebraic solution to the classical perspective-3-point (P3P) problem for determining the position and attitude of a camera from observations of three known reference points. In contrast to previous approaches, we first directly determine the camera's attitude by employing the corresponding geometric constraints to formulate a system of trigonometric equations. This is then efficiently solved, following an algebraic approach, to determine the unknown rotation matrix and subsequently the camera's position. As compared to recent alternatives, our method avoids computing unnecessary (and potentially numerically unstable) intermediate results, and thus achieves higher numerical accuracy and robustness at a lower computational cost. These benefits are validated through extensive Monte-Carlo simulations for both nominal and close-to-singular geometric configurations. version:1
arxiv-1701-08229 | Feature Studies to Inform the Classification of Depressive Symptoms from Twitter Data for Population Health | http://arxiv.org/abs/1701.08229 | id:1701.08229 author:Danielle Mowery, Craig Bryan, Mike Conway category:cs.IR cs.CL cs.CY cs.SI  published:2017-01-28 summary:The utility of Twitter data as a medium to support population-level mental health monitoring is not well understood. In an effort to better understand the predictive power of supervised machine learning classifiers and the influence of feature sets for efficiently classifying depression-related tweets on a large-scale, we conducted two feature study experiments. In the first experiment, we assessed the contribution of feature groups such as lexical information (e.g., unigrams) and emotions (e.g., strongly negative) using a feature ablation study. In the second experiment, we determined the percentile of top ranked features that produced the optimal classification performance by applying a three-step feature elimination approach. In the first experiment, we observed that lexical features are critical for identifying depressive symptoms, specifically for depressed mood (-35 points) and for disturbed sleep (-43 points). In the second experiment, we observed that the optimal F1-score performance of top ranked features in percentiles variably ranged across classes e.g., fatigue or loss of energy (5th percentile, 288 features) to depressed mood (55th percentile, 3,168 features) suggesting there is no consistent count of features for predicting depressive-related tweets. We conclude that simple lexical features and reduced feature sets can produce comparable results to larger feature sets. version:1
arxiv-1701-08222 | Sampling Without Time: Recovering Echoes of Light via Temporal Phase Retrieval | http://arxiv.org/abs/1701.08222 | id:1701.08222 author:Ayush Bhandari, Aurelien Bourquard, Ramesh Raskar category:cs.IT cs.CV math.IT  published:2017-01-27 summary:This paper considers the problem of sampling and reconstruction of a continuous-time sparse signal without assuming the knowledge of the sampling instants or the sampling rate. This topic has its roots in the problem of recovering multiple echoes of light from its low-pass filtered and auto-correlated, time-domain measurements. Our work is closely related to the topic of sparse phase retrieval and in this context, we discuss the advantage of phase-free measurements. While this problem is ill-posed, cues based on physical constraints allow for its appropriate regularization. We validate our theory with experiments based on customized, optical time-of-flight imaging sensors. What singles out our approach is that our sensing method allows for temporal phase retrieval as opposed to the usual case of spatial phase retrieval. Preliminary experiments and results demonstrate a compelling capability of our phase-retrieval based imaging device. version:1
arxiv-1701-08198 | Adversarial Evaluation of Dialogue Models | http://arxiv.org/abs/1701.08198 | id:1701.08198 author:Anjuli Kannan, Oriol Vinyals category:cs.CL  published:2017-01-27 summary:The recent application of RNN encoder-decoder models has resulted in substantial progress in fully data-driven dialogue systems, but evaluation remains a challenge. An adversarial loss could be a way to directly evaluate the extent to which generated dialogue responses sound like they came from a human. This could reduce the need for human evaluation, while more directly evaluating on a generative task. In this work, we investigate this idea by training an RNN to discriminate a dialogue model's samples from human-generated samples. Although we find some evidence this setup could be viable, we also note that many issues remain in its practical application. We discuss both aspects and conclude that future work is warranted. version:1
arxiv-1701-08757 | Bayesian Learning of Consumer Preferences for Residential Demand Response | http://arxiv.org/abs/1701.08757 | id:1701.08757 author:Mikhail V. Goubko, Sergey O. Kuznetsov, Alexey A. Neznanov, Dmitry I. Ignatov category:cs.LG cs.SY stat.ML I.2.6  published:2017-01-27 summary:In coming years residential consumers will face real-time electricity tariffs with energy prices varying day to day, and effective energy saving will require automation - a recommender system, which learns consumer's preferences from her actions. A consumer chooses a scenario of home appliance use to balance her comfort level and the energy bill. We propose a Bayesian learning algorithm to estimate the comfort level function from the history of appliance use. In numeric experiments with datasets generated from a simulation model of a consumer interacting with small home appliances the algorithm outperforms popular regression analysis tools. Our approach can be extended to control an air heating and conditioning system, which is responsible for up to half of a household's energy bill. version:1
arxiv-1701-08180 | Camera-Trap Images Segmentation using Multi-Layer Robust Principal Component Analysis | http://arxiv.org/abs/1701.08180 | id:1701.08180 author:Jhony-Heriberto Giraldo-Zuluaga, Alexander Gomez, Augusto Salazar, Angélica Diaz-Pulido category:cs.CV  published:2017-01-27 summary:Camera trapping is a technique to study wildlife using automatic triggered cameras. However, camera trapping collects a lot of false positives (images without animals), which must be segmented before the classification step. This paper presents a Multi-Layer Robust Principal Component Analysis (RPCA) for camera-trap images segmentation. Our Multi-Layer RPCA uses histogram equalization and Gaussian filter as pre-processing, texture and color descriptors as features, and morphological filters with active contour as post-processing. The experiments focus on computing the sparse and low-rank matrices with different amounts of camera-trap images. We tested the Multi-Layer RPCA in our camera-trap database. To our best knowledge, this paper is the first work proposing Multi-Layer RPCA and using it for camera-trap images segmentation. version:1
arxiv-1701-08702 | Bangla Word Clustering Based on Tri-gram, 4-gram and 5-gram Language Model | http://arxiv.org/abs/1701.08702 | id:1701.08702 author:Dipaloke Saha, Md Saddam Hossain, MD. Saiful Islam, Sabir Ismail category:cs.CL  published:2017-01-27 summary:In this paper, we describe a research method that generates Bangla word clusters on the basis of relating to meaning in language and contextual similarity. The importance of word clustering is in parts of speech (POS) tagging, word sense disambiguation, text classification, recommender system, spell checker, grammar checker, knowledge discover and for many others Natural Language Processing (NLP) applications. In the history of word clustering, English and some other languages have already implemented some methods on word clustering efficiently. But due to lack of the resources, word clustering in Bangla has not been still implemented efficiently. Presently, its implementation is in the beginning stage. In some research of word clustering in English based on preceding and next five words of a key word they found an efficient result. Now, we are trying to implement the tri-gram, 4-gram and 5-gram model of word clustering for Bangla to observe which one is the best among them. We have started our research with quite a large corpus of approximate 1 lakh Bangla words. We are using a machine learning technique in this research. We will generate word clusters and analyze the clusters by testing some different threshold values. version:1
arxiv-1701-08140 | Network classification with applications to brain connectomics | http://arxiv.org/abs/1701.08140 | id:1701.08140 author:Jesús D. Arroyo Relión, Daniel Kessler, Elizaveta Levina, Stephan F. Taylor category:stat.ME stat.ML  published:2017-01-27 summary:While statistical analysis of a single network has received a lot of attention in recent years, with a focus on social networks, analysis of a sample of networks presents its own challenges which require a different set of analytic tools. Here we study the problem of classification of networks with labeled nodes, motivated by applications in neuroimaging. Brain networks are constructed from imaging data to represent functional connectivity between regions of the brain, and previous work has shown the potential of such networks to distinguish between various brain disorders, giving rise to a network (or graph) classification problem. Existing approaches to graph classification tend to either treat all edge weights as a long vector, ignoring the network structure, or focus on the graph topology while ignoring the edge weights. Our goal here is to design a graph classification method that uses both the individual edge information and the network structure of the data in a computationally efficient way. We are also interested in obtaining a parsimonious and interpretable representation of differences in brain connectivity patterns between classes, which requires variable selection. We propose a graph classification method that uses edge weights as variables but incorporates the network nature of the data via penalties that promotes sparsity in the number of nodes. We implement the method via efficient convex optimization algorithms and show good performance on data from two fMRI studies of schizophrenia. version:1
arxiv-1701-08118 | Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis | http://arxiv.org/abs/1701.08118 | id:1701.08118 author:Björn Ross, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, Michael Wojatzki category:cs.CL  published:2017-01-27 summary:Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation. version:1
arxiv-1701-08107 | Deconvolution and Restoration of Optical Endomicroscopy Images | http://arxiv.org/abs/1701.08107 | id:1701.08107 author:Ahmed Karam Eldaly, Yoann Altmann, Antonios Perperidis, Nikola Krstajic, Tushar Choudhary, Kevin Dhaliwal, Stephen McLaughlin category:cs.CV stat.AP  published:2017-01-27 summary:Optical endomicroscopy (OEM) is an emerging technology platform with preclinical and clinical imaging utility. Pulmonary OEM via multicore fibres has the potential to provide in vivo in situ molecular signatures of disease such as infection and inflammation. However, enhancing the quality of data acquired by this technique for better visualization and subsequent analysis remains a challenging problem. Cross coupling between fiber cores is one of the main reasons of poor detection performance (i.e., inflammation, bacteria, etc.). In this work, we address the problem of deconvolution and restoration of OEM data. We propose and compare four methods, three are based on the alternating direction method of multipliers (ADMM) and one is based on Markov chain Monte Carlo (MCMC) methods. Results on both synthetic and real datasets illustrate the effectiveness of the proposed methods. version:1
arxiv-1701-08106 | Faster Discovery of Faster System Configurations with Spectral Learning | http://arxiv.org/abs/1701.08106 | id:1701.08106 author:Vivek Nair, Tim Menzies, Norbert Siegmund, Sven Apel category:cs.SE cs.LG  published:2017-01-27 summary:Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors - less than 10% prediction error, with a standard deviation of less than 2%. When compared to the state of the art, WHAT (a) requires 2 to 10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance. version:1
arxiv-1701-08092 | Double-sided probing by map of Asplünd's distances using Logarithmic Image Processing in the framework of Mathematical Morphology | http://arxiv.org/abs/1701.08092 | id:1701.08092 author:Guillaume Noyel, Michel Jourlin category:cs.CV math.NA  published:2017-01-27 summary:We establish the link between Mathematical Morphology and the map of Aspl\"und's distances between a probe and a grey scale function, using the Logarithmic Image Processing scalar multiplication. We demonstrate that the map is the logarithm of the ratio between a dilation and an erosion of the function by a structuring function: the probe. The dilations and erosions are mappings from the lattice of the images into the lattice of the positive functions. Using a flat structuring element, the expression of the map of Aspl\"und's distances can be simplified with a dilation and an erosion of the image, these mappings stays in the lattice of the images. We illustrate our approach by an example of pattern matching with a non-flat structuring function. version:1
arxiv-1701-08074 | Model-Free Control of Thermostatically Controlled Loads Connected to a District Heating Network | http://arxiv.org/abs/1701.08074 | id:1701.08074 author:Bert J. Claessens, Dirk Vanhoudt, Johan Desmedt, Frederik Ruelens category:cs.SY cs.LG  published:2017-01-27 summary:Optimal control of thermostatically controlled loads connected to a district heating network is considered a sequential decision- making problem under uncertainty. The practicality of a direct model-based approach is compromised by two challenges, namely scalability due to the large dimensionality of the problem and the system identification required to identify an accurate model. To help in mitigating these problems, this paper leverages on recent developments in reinforcement learning in combination with a market-based multi-agent system to obtain a scalable solution that obtains a significant performance improvement in a practical learning time. The control approach is applied on a scenario comprising 100 thermostatically controlled loads connected to a radial district heating network supplied by a central combined heat and power plant. Both for an energy arbitrage and a peak shaving objective, the control approach requires 60 days to obtain a performance within 65% of a theoretical lower bound on the cost. version:1
arxiv-1701-08071 | Emotion Recognition From Speech With Recurrent Neural Networks | http://arxiv.org/abs/1701.08071 | id:1701.08071 author:Vladimir Chernykh, Grigoriy Sterling, Pavel Prihodko category:cs.CL  published:2017-01-27 summary:In this paper the task of emotion recognition from speech is considered. Proposed approach uses deep recurrent neural network trained on a sequence of acoustic features calculated over small speech intervals. At the same time special probabilistic-nature CTC loss function allows to consider long utterances containing both emotional and unemotional parts. The effectiveness of such an approach is shown in two ways. First one is the comparison with recent advances in this field. While second way implies measuring human performance on the same task, which also was done by authors. version:1
arxiv-1701-08055 | Modelling Competitive Sports: Bradley-Terry-Élő Models for Supervised and On-Line Learning of Paired Competition Outcomes | http://arxiv.org/abs/1701.08055 | id:1701.08055 author:Franz J. Király, Zhaozhi Qian category:stat.ML cs.LG stat.AP stat.ME  published:2017-01-27 summary:Prediction and modelling of competitive sports outcomes has received much recent attention, especially from the Bayesian statistics and machine learning communities. In the real world setting of outcome prediction, the seminal \'{E}l\H{o} update still remains, after more than 50 years, a valuable baseline which is difficult to improve upon, though in its original form it is a heuristic and not a proper statistical "model". Mathematically, the \'{E}l\H{o} rating system is very closely related to the Bradley-Terry models, which are usually used in an explanatory fashion rather than in a predictive supervised or on-line learning setting. Exploiting this close link between these two model classes and some newly observed similarities, we propose a new supervised learning framework with close similarities to logistic regression, low-rank matrix completion and neural networks. Building on it, we formulate a class of structured log-odds models, unifying the desirable properties found in the above: supervised probabilistic prediction of scores and wins/draws/losses, batch/epoch and on-line learning, as well as the possibility to incorporate features in the prediction, without having to sacrifice simplicity, parsimony of the Bradley-Terry models, or computational efficiency of \'{E}l\H{o}'s original approach. We validate the structured log-odds modelling approach in synthetic experiments and English Premier League outcomes, where the added expressivity yields the best predictions reported in the state-of-art, close to the quality of contemporary betting odds. version:1
arxiv-1701-08694 | A Comparative Study on Different Types of Approaches to Bengali document Categorization | http://arxiv.org/abs/1701.08694 | id:1701.08694 author:Md. Saiful Islam, Fazla Elahi Md Jubayer, Syed Ikhtiar Ahmed category:cs.CL cs.LG  published:2017-01-27 summary:Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\"ive Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article. version:1
arxiv-1701-08706 | Document Decomposition of Bangla Printed Text | http://arxiv.org/abs/1701.08706 | id:1701.08706 author:Md. Fahad Hasan, Tasmin Afroz, Sabir Ismail, Md. Saiful Islam category:cs.CV cs.CL  published:2017-01-27 summary:Today all kind of information is getting digitized and along with all this digitization, the huge archive of various kinds of documents is being digitized too. We know that, Optical Character Recognition is the method through which, newspapers and other paper documents convert into digital resources. But, it is a fact that this method works on texts only. As a result, if we try to process any document which contains non-textual zones, then we will get garbage texts as output. That is why; in order to digitize documents properly they should be prepossessed carefully. And while preprocessing, segmenting document in different regions according to the category properly is most important. But, the Optical Character Recognition processes available for Bangla language have no such algorithm that can categorize a newspaper/book page fully. So we worked to decompose a document into its several parts like headlines, sub headlines, columns, images etc. And if the input is skewed and rotated, then the input was also deskewed and de-rotated. To decompose any Bangla document we found out the edges of the input image. Then we find out the horizontal and vertical area of every pixel where it lies in. Later on the input image was cut according to these areas. Then we pick each and every sub image and found out their height-width ratio, line height. Then according to these values the sub images were categorized. To deskew the image we found out the skew angle and de skewed the image according to this angle. To de-rotate the image we used the line height, matra line, pixel ratio of matra line. version:1
arxiv-1701-08156 | A Comprehensive Survey on Bengali Phoneme Recognition | http://arxiv.org/abs/1701.08156 | id:1701.08156 author:Sadia Tasnim Swarna, Shamim Ehsan, Md. Saiful Islam, Marium E Jannat category:cs.SD cs.CL  published:2017-01-27 summary:Hidden Markov model based various phoneme recognition methods for Bengali language is reviewed. Automatic phoneme recognition for Bengali language using multilayer neural network is reviewed. Usefulness of multilayer neural network over single layer neural network is discussed. Bangla phonetic feature table construction and enhancement for Bengali speech recognition is also discussed. Comparison among these methods is discussed. version:1
arxiv-1701-08027 | LocDyn: Robust Distributed Localization for Mobile Underwater Networks | http://arxiv.org/abs/1701.08027 | id:1701.08027 author:Cláudia Soares, João Gomes, Beatriz Ferreira, João Paulo Costeira category:cs.MA math.OC stat.ML  published:2017-01-27 summary:How to self-localize large teams of underwater nodes using only noisy range measurements? How to do it in a distributed way, and incorporating dynamics into the problem? How to reject outliers and produce trustworthy position estimates? The stringent acoustic communication channel and the accuracy needs of our geophysical survey application demand faster and more accurate localization methods. We approach dynamic localization as a MAP estimation problem where the prior encodes dynamics, and we devise a convex relaxation method that takes advantage of previous estimates at each measurement acquisition step; The algorithm converges at an optimal rate for first order methods. LocDyn is distributed: there is no fusion center responsible for processing acquired data and the same simple computations are performed for each node. LocDyn is accurate: experiments attest to a smaller positioning error than a comparable Kalman filter. LocDyn is robust: it rejects outlier noise, while the comparing methods succumb in terms of positioning error. version:1
arxiv-1701-08025 | UmUTracker: A versatile MATLAB program for automated particle tracking of 2D light microscopy or 3D digital holography data | http://arxiv.org/abs/1701.08025 | id:1701.08025 author:Hanqing Zhang, Tim Stangner, Krister Wiklund, Alvaro Rodriguez, Magnus Andersson category:cs.CV  published:2017-01-27 summary:We present a versatile and fast MATLAB program (UmUTracker) that automatically detects and tracks particles by analyzing long video sequences acquired by either light microscopy or digital holography microscopy. Our program finds the 2D particle center position using an isosceles triangle transform and the axial position by a fast implementation of Rayleigh-Sommerfeld numerical reconstruction algorithm using a one dimensional radial intensity profile. To validate the accuracy of our program we test each module individually in controlled experiments. First, we determine the lateral position of polystyrene particles recorded under bright field microscopy and digital holography conditions. Second, we reconstruct the axial position of the same particles by analyzing synthetic and experimentally acquired holograms. Thereafter, as a proof of concept, we profile the fluid flow in a 100 micrometer high flow chamber. This result agrees with computational fluid dynamic simulations. On a regular desktop computer UmUTracker can detect, analyze, and track a single particle at 5 frames per second for a template size of 201 x 201 in a 1024 x 1024 image. To enhance usability and to make it easy to implement new functions we used object-oriented programming. UmUTracker is suitable for studies related to: particle dynamics, cell localization, colloids and microfluidic flow measurement. version:1
arxiv-1701-08006 | Quasi-homography warps in image stitching | http://arxiv.org/abs/1701.08006 | id:1701.08006 author:Nan Li, Yifang Xu, Chao Wang category:cs.CV  published:2017-01-27 summary:Naturalness of warping is gaining extensive attention in image stitching. Recent warps such as SPHP, AANAP and GSP, use a global similarity to effectively mitigate projective distortion (which enlarges regions), however, they necessarily bring in perspective distortion (which generates inconsistency). In this paper, we propose a quasi-homography warp, which balances perspective distortion against projective distortion in the non-overlapping region, to create natural-looking mosaics. Our approach formulates the warp as a solution of a system of bivariate equations, where perspective distortion and projective distortion are characterized as slope preservation and scale linearization respectively. Our proposed warp only relies on a global homography thus is totally parameter-free. A comprehensive experiment shows that quasi-homography outperforms some state-of-the-art warps in urban scenes, including homography, AutoStitch and SPHP. A user study demonstrates that quasi-homography wins most users' favor as well, comparing to homography and SPHP. version:1
arxiv-1701-03400 | Scaling Binarized Neural Networks on Reconfigurable Logic | http://arxiv.org/abs/1701.03400 | id:1701.03400 author:Nicholas J. Fraser, Yaman Umuroglu, Giulio Gambardella, Michaela Blott, Philip Leong, Magnus Jahre, Kees Vissers category:cs.CV cs.LG  published:2017-01-12 summary:Binarized neural networks (BNNs) are gaining interest in the deep learning community due to their significantly lower computational and memory cost. They are particularly well suited to reconfigurable logic devices, which contain an abundance of fine-grained compute resources and can result in smaller, lower power implementations, or conversely in higher classification rates. Towards this end, the Finn framework was recently proposed for building fast and flexible field programmable gate array (FPGA) accelerators for BNNs. Finn utilized a novel set of optimizations that enable efficient mapping of BNNs to hardware and implemented fully connected, non-padded convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. However, FINN was not evaluated on larger topologies due to the size of the chosen FPGA, and exhibited decreased accuracy due to lack of padding. In this paper, we improve upon Finn to show how padding can be employed on BNNs while still maintaining a 1-bit datapath and high accuracy. Based on this technique, we demonstrate numerous experiments to illustrate flexibility and scalability of the approach. In particular, we show that a large BNN requiring 1.2 billion operations per frame running on an ADM-PCIE-8K5 platform can classify images at 12 kFPS with 671 us latency while drawing less than 41 W board power and classifying CIFAR-10 images at 88.7% accuracy. Our implementation of this network achieves 14.8 trillion operations per second. We believe this is the fastest classification rate reported to date on this benchmark at this level of accuracy. version:2
arxiv-1701-04077 | Breeding electric zebras in the fields of Medicine | http://arxiv.org/abs/1701.04077 | id:1701.04077 author:Federico Cabitza category:cs.LG  published:2017-01-15 summary:A few notes on the use of machine learning in medicine and the related unintended consequences. version:3
arxiv-1701-07974 | Reinforced backpropagation improves test performance of deep networks: a toy-model study | http://arxiv.org/abs/1701.07974 | id:1701.07974 author:Haiping Huang, Taro Toyoizumi category:cs.LG cs.NE  published:2017-01-27 summary:Standard error backpropagation is used in almost all modern deep network training. However, it typically suffers from proliferation of saddle points in high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a good parameter region of better generalization capabilities, especially based on rough insights about the landscape of the error surface. Here, we propose a simple extension of the backpropagation, namely reinforced backpropagation, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. Extensive numerical simulations on a toy deep learning model verify its excellent performance. The reinforced backpropagation can significantly improve test performance of the deep network training, especially when the data are scarce. The performance is even better than that of state-of-the-art stochastic optimization algorithm called Adam, with an extra advantage of less computer memory required. version:1
arxiv-1701-07955 | Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic and Visualize Its Change Over Time | http://arxiv.org/abs/1701.07955 | id:1701.07955 author:Syed Mehedi Hasan Nirob, Md. Kazi Nayeem, Md. Saiful Islam category:cs.IR cs.CL  published:2017-01-27 summary:Trending topic of newspapers is an indicator to understand the situation of a country and also a way to evaluate the particular newspaper. This paper represents a model describing few techniques to select trending topics from Bangla Newspaper. Topics that are discussed more frequently than other in Bangla newspaper will be marked and how a very famous topic loses its importance with the change of time and another topic takes its place will be demonstrated. Data from two popular Bangla Newspaper with date and time were collected. Statistical analysis was performed after on these data after preprocessing. Popular and most used keywords were extracted from the stream of Bangla keyword with this analysis. This model can also cluster category wise news trend or a list of news trend in daily or weekly basis with enough data. A pattern can be found on their news trend too. Comparison among past news trend of Bangla newspapers will give a visualization of the situation of Bangladesh. This visualization will be helpful to predict future trending topics of Bangla Newspaper. version:1
arxiv-1701-07953 | The Price of Differential Privacy For Online Learning | http://arxiv.org/abs/1701.07953 | id:1701.07953 author:Naman Agarwal, Karan Singh category:cs.LG stat.ML  published:2017-01-27 summary:We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal $\tilde{O}(\sqrt{T})$ regret bounds. In the full-information setting, our results demonstrate that $(\epsilon, \delta)$-differential privacy may be ensured for free - in particular, the regret bounds scale as $O(\sqrt{T})+\tilde{O}\big(\frac{1}{\epsilon}\log \frac{1}{\delta}\big)$. For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of $O\Big(\frac{\sqrt{T\log T}}{\epsilon}\log \frac{1}{\delta}\Big)$, while the previously best known bound was $\tilde{O}\Big(\frac{T^{\frac{3}{4}}}{\epsilon}\Big)$. version:1
arxiv-1701-07950 | Beyond Evolutionary Algorithms for Search-based Software Engineering | http://arxiv.org/abs/1701.07950 | id:1701.07950 author:Jianfeng Chen, Vivek Nair, Tim Menzies category:cs.SE cs.NE  published:2017-01-27 summary:Context:Evolutionary algorithms typically require large number of evaluations (of solutions) to reach their conclusions - which can be very slow and expensive to evaluate. Objective:To solve search-based SE problems, using fewer evaluations than evolutionary methods. Method:Instead of mutating a small population, we build a very large initial population which is then culled using a recursive bi-clustering binary chop approach. We evaluate this approach on multiple software engineering(SE) models, unconstrained as well as constrained, and compare its performance with standard evolutionary algorithms. Results:Using just a few evaluations (under 100), we can obtain the comparable results to standard evolutionary algorithms. Conclusion:Just because something works, and is widespread use, does not necessarily mean that there is no value in seeking methods to improve that method. Before undertaking SBSE optimization task using traditional EAs, it is recommended to try other techniques, like those explored here, to obtain the same results with fewer evaluations. version:1
arxiv-1701-03974 | An Online Convex Optimization Approach to Dynamic Network Resource Allocation | http://arxiv.org/abs/1701.03974 | id:1701.03974 author:Tianyi Chen, Qing Ling, Georgios B. Giannakis category:cs.SY cs.LG math.OC stat.ML  published:2017-01-14 summary:Existing approaches to online convex optimization (OCO) make sequential one-slot-ahead decisions, which lead to (possibly adversarial) losses that drive subsequent decision iterates. Their performance is evaluated by the so-called regret that measures the difference of losses between the online solution and the best yet fixed overall solution in hindsight. The present paper deals with online convex optimization involving adversarial loss functions and adversarial constraints, where the constraints are revealed after making decisions, and can be tolerable to instantaneous violations but must be satisfied in the long term. Performance of an online algorithm in this setting is assessed by: i) the difference of its losses relative to the best dynamic solution with one-slot-ahead information of the loss function and the constraint (that is here termed dynamic regret); and, ii) the accumulated amount of constraint violations (that is here termed dynamic fit). In this context, a modified online saddle-point (MOSP) scheme is developed, and proved to simultaneously yield sub-linear dynamic regret and fit, provided that the accumulated variations of per-slot minimizers and constraints are sub-linearly growing with time. MOSP is also applied to the dynamic network resource allocation task, and it is compared with the well-known stochastic dual gradient method. Under various scenarios, numerical experiments demonstrate the performance gain of MOSP relative to the state-of-the-art. version:2
arxiv-1701-07920 | Subset Selection for Multiple Linear Regression via Optimization | http://arxiv.org/abs/1701.07920 | id:1701.07920 author:Young Woong Park, Diego Klabjan category:stat.ML  published:2017-01-27 summary:Subset selection in multiple linear regression is to choose a subset of candidate explanatory variables that tradeoff error and the number of variables selected. We built mathematical programming models for subset selection and compare the performance of an LP-based branch-and-bound algorithm with tailored valid inequalities to known heuristics. We found that our models quickly find a quality solution while the rest of the time is spent to prove optimality. Our models are also applicable with slight modifications to the case with more candidate explanatory variables than observations. For this case, we provide mathematical programming models, propose new criteria, and develop heuristic algorithms based on mathematical programming. version:1
arxiv-1701-03329 | A Data-Oriented Model of Literary Language | http://arxiv.org/abs/1701.03329 | id:1701.03329 author:Andreas van Cranenburgh, Rens Bod category:cs.CL  published:2017-01-12 summary:We consider the task of predicting how literary a text is, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments, mined from the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of lexical and syntactic features, and explains 76.0 % of the variation in literary ratings. version:2
arxiv-1701-07901 | Deep Region Hashing for Efficient Large-scale Instance Search from Images | http://arxiv.org/abs/1701.07901 | id:1701.07901 author:Jingkuan Song, Tao He, Lianli Gao, Xing Xu, Heng Tao Shen category:cs.CV  published:2017-01-26 summary:Instance Search (INS) is a fundamental problem for many applications, while it is more challenging comparing to traditional image search since the relevancy is defined at the instance level. Existing works have demonstrated the success of many complex ensemble systems that are typically conducted by firstly generating object proposals, and then extracting handcrafted and/or CNN features of each proposal for matching. However, object bounding box proposals and feature extraction are often conducted in two separated steps, thus the effectiveness of these methods collapses. Also, due to the large amount of generated proposals, matching speed becomes the bottleneck that limits its application to large-scale datasets. To tackle these issues, in this paper we propose an effective and efficient Deep Region Hashing (DRH) approach for large-scale INS using an image patch as the query. Specifically, DRH is an end-to-end deep neural network which consists of object proposal, feature extraction, and hash code generation. DRH shares full-image convolutional feature map with the region proposal network, thus enabling nearly cost-free region proposals. Also, each high-dimensional, real-valued region features are mapped onto a low-dimensional, compact binary codes for the efficient object region level matching on large-scale dataset. Experimental results on four datasets show that our DRH can achieve even better performance than the state-of-the-arts in terms of MAP, while the efficiency is improved by nearly 100 times. version:1
arxiv-1701-07899 | Nonlinear network-based quantitative trait prediction from transcriptomic data | http://arxiv.org/abs/1701.07899 | id:1701.07899 author:Emilie Devijver, Mélina Gallopin, Emeline Perthame category:stat.AP stat.ME stat.ML  published:2017-01-26 summary:Quantitatively predicting phenotype variables by the expression changes in a set of candidate genes is of great interest in molecular biology but it is also a challenging task for several reasons. First, the collected biological observations might be heterogeneous and correspond to different biological mechanisms. Secondly, the gene expression variables used to predict the phenotype are potentially highly correlated since genes interact though unknown regulatory networks. In this paper, we present a novel approach designed to predict quantitative trait from transcriptomic data, taking into account the heterogeneity in biological samples and the hidden gene regulatory networks underlying different biological mechanisms. The proposed model performs well on prediction but it is also fully parametric, which facilitates the downstream biological interpretation. The model provides clusters of individuals based on the relation between gene expression data and the phenotype, and also leads to infer a gene regulatory network specific for each cluster of individuals. We perform numerical simulations to demonstrate that our model is competitive with other prediction models, and we demonstrate the predictive performance and the interpretability of our model to predict alcohol sensitivity from transcriptomic data on real data from Drosophila Melanogaster Genetic Reference Panel (DGRP). version:1
arxiv-1701-07895 | Information Theoretic Limits for Linear Prediction with Graph-Structured Sparsity | http://arxiv.org/abs/1701.07895 | id:1701.07895 author:Adarsh Barik, Jean Honorio, Mohit Tawarmalani category:cs.LG cs.IT math.IT stat.ML  published:2017-01-26 summary:We analyze the necessary number of samples for sparse vector recovery in a noisy linear prediction setup. This model includes problems such as linear regression and classification. We focus on structured graph models. In particular, we prove that sufficient number of samples for the weighted graph model proposed by Hegde and others is also necessary. We use the Fano's inequality on well constructed ensembles as our main tool in establishing information theoretic lower bounds. version:1
arxiv-1701-07880 | emLam -- a Hungarian Language Modeling baseline | http://arxiv.org/abs/1701.07880 | id:1701.07880 author:Dávid Márk Nemeskey category:cs.CL I.2.7  published:2017-01-26 summary:This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungar- ian benchmark corpus is introduced. version:1
arxiv-1701-07875 | Wasserstein GAN | http://arxiv.org/abs/1701.07875 | id:1701.07875 author:Martin Arjovsky, Soumith Chintala, Léon Bottou category:stat.ML cs.LG  published:2017-01-26 summary:We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions. version:1
arxiv-1701-07852 | An Empirical Analysis of Feature Engineering for Predictive Modeling | http://arxiv.org/abs/1701.07852 | id:1701.07852 author:Jeff Heaton category:cs.LG  published:2017-01-26 summary:Machine learning models, such as neural networks, decision trees, random forests and gradient boosting machines accept a feature vector and provide a prediction. These models learn in a supervised fashion where a set of feature vectors with expected output is provided. It is very common practice to engineer new features from the provided feature set. Such engineered features will either augment, or replace portions of the existing feature vector. These engineered features are essentially calculated fields, based on the values of the other features. Engineering such features is primarily a manual, time-consuming task. Additionally, each type of model will respond differently to different types of engineered features. This paper reports on empirical research to demonstrate what types of engineered features are best suited to which machine learning model type. This is accomplished by generating several datasets that are designed to benefit from a particular type of engineered feature. The experiment demonstrates to what degree the machine learning model is capable of synthesizing the needed feature on its own. If a model is capable of synthesizing an engineered feature, it is not necessary to provide that feature. The research demonstrated that the studied models do indeed perform differently with various types of engineered features. version:1
arxiv-1701-07842 | Learning Asynchronous Typestates for Android Classes | http://arxiv.org/abs/1701.07842 | id:1701.07842 author:Arjun Radhakrishna, Nicholas Lewchenko, Shawn Meier, Sergio Mover, Krishna Chaitanya Sripada, Damien Zufferey, Bor-Yuh Evan Chang, Pavol Černý category:cs.LO cs.LG cs.PL  published:2017-01-26 summary:In event-driven programming frameworks, such as Android, the client and the framework interact using callins (framework methods that the client invokes) and callbacks (client methods that the framework invokes). The protocols for interacting with these frameworks can often be described by finite-state machines we dub *asynchronous typestates*. Asynchronous typestates are akin to classical typestates, with the key difference that their outputs (callbacks) are produced asynchronously. We present an algorithm to infer asynchronous typestates for Android framework classes. It is based on the L* algorithm that uses membership and equivalence queries. We show how to implement these queries for Android classes. Membership queries are implemented using testing. Under realistic assumptions, equivalence queries can be implemented using membership queries. We provide an improved algorithm for equivalence queries that is better suited for our application than the algorithms from literature. Instead of using a bound on the size of the typestate to be learned, our algorithm uses a *distinguisher bound*. The distinguisher bound quantifies how two states in the typestate are locally different. We implement our approach and evaluate it empirically. We use our tool, Starling, to learn asynchronous typestates for Android classes both for cases where one is already provided by the documentation, and for cases where the documentation is unclear. The results show that Starling learns asynchronous typestates accurately and efficiently. Additionally, in several cases, the synthesized asynchronous typestates uncovered surprising and undocumented behaviors. version:1
arxiv-1701-07808 | Linear convergence of SDCA in statistical estimation | http://arxiv.org/abs/1701.07808 | id:1701.07808 author:Chao Qu, Huan Xu category:stat.ML cs.LG  published:2017-01-26 summary:In this paper, we consider stochastic Dual Coordinate (SDCA)\textbf{ without} strongly convex or convex assumption, which covers many useful models such as Lasso, group Lasso, and logistic regression with $\ell_1$ regularization, corrected Lasso and linear regression with SCAD regularizer. We prove that under some mild condition called restricted strong convexity satisfied by above examples, the convergence rate is still \textbf{linear } up to the statistical precision of model which is much sharp than the previous work with sub-linear result. version:1
arxiv-1701-07795 | Match-Tensor: a Deep Relevance Model for Search | http://arxiv.org/abs/1701.07795 | id:1701.07795 author:Aaron Jaech, Hetunandan Kamisetty, Eric Ringger, Charlie Clarke category:cs.IR cs.CL  published:2017-01-26 summary:The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models. version:1
arxiv-1701-08081 | Design of PI Controller for Automatic Generation Control of Multi Area Interconnected Power System using Bacterial Foraging Optimization | http://arxiv.org/abs/1701.08081 | id:1701.08081 author:Naresh Kumari, Nitin Malik, A. N. Jha, Gaddam Mallesham category:cs.SY cs.NE  published:2017-01-26 summary:The system comprises of three interconnected power system networks based on thermal, wind and hydro power generation. The load variation in any one of the network results in frequency deviation in all the connected systems.The PI controllers have been connected separately with each system for the frequency control and the gains (Kp and Ki) of all the controllers have been optimized along with frequency bias (Bi) and speed regulation parameter (Ri). The computationally intelligent techniques like bacterial foraging optimization (BFO) and particle swarm optimization (PSO) have been applied for the tuning of controller gains along with variable parameters Bi and Ri. The gradient descent (GD) based conventional method has also been applied for optimizing the parameters Kp, Ki,Bi and Ri.The frequency responses are obtained with all the methods. The performance index chosen is the integral square error (ISE). The settling time, peak overshoot and peak undershoot of all the frequency responses on applying three optimization techniques have been compared. It has been observed that the peak overshoot and peak undershoot significantly reduce with BFO technique followed by the PSO and GD techniques. While obtaining such optimum response the settling time is increased marginally with bacterial foraging technique due to large number of mathematical equations used for the computation in BFO. The comparison of frequency response using three techniques show the superiority of BFO over the PSO and GD techniques. The designing of the system and tuning of the parameters with three techniques has been done in MATLAB/SIMULINK environment. version:1
arxiv-1701-08100 | The Causal Frame Problem: An Algorithmic Perspective | http://arxiv.org/abs/1701.08100 | id:1701.08100 author:Ardavan Salehi Nobandegani, Ioannis N. Psaromiligkos category:cs.AI q-bio.NC stat.ML  published:2017-01-26 summary:The Frame Problem (FP) is a puzzle in philosophy of mind and epistemology, articulated by the Stanford Encyclopedia of Philosophy as follows: "How do we account for our apparent ability to make decisions on the basis only of what is relevant to an ongoing situation without having explicitly to consider all that is not relevant?" In this work, we focus on the causal variant of the FP, the Causal Frame Problem (CFP). Assuming that a reasoner's mental causal model can be (implicitly) represented by a causal Bayes net, we first introduce a notion called Potential Level (PL). PL, in essence, encodes the relative position of a node with respect to its neighbors in a causal Bayes net. Drawing on the psychological literature on causal judgment, we substantiate the claim that PL may bear on how time is encoded in the mind. Using PL, we propose an inference framework, called the PL-based Inference Framework (PLIF), which permits a boundedly-rational approach to the CFP to be formally articulated at Marr's algorithmic level of analysis. We show that our proposed framework, PLIF, is consistent with a wide range of findings in causal judgment literature, and that PL and PLIF make a number of predictions, some of which are already supported by existing findings. version:1
arxiv-1701-07274 | Deep Reinforcement Learning: An Overview | http://arxiv.org/abs/1701.07274 | id:1701.07274 author:Yuxi Li category:cs.LG  published:2017-01-25 summary:We give an overview of recent exciting achievements of deep reinforcement learning (RL). We start with background of deep learning and reinforcement learning, as well as introduction of testbeds. Next we discuss Deep Q-Network (DQN) and its extensions, asynchronous methods, policy optimization, reward, and planning. After that, we talk about attention and memory, unsupervised learning, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, spoken dialogue systems (a.k.a. chatbot), machine translation, text sequence prediction, neural architecture design, personalized web services, healthcare, finance, and music generation. We mention topics/papers not reviewed yet. After listing a collection of RL resources, we close with discussions. version:2
arxiv-1701-07767 | Riemannian-geometry-based modeling and clustering of network-wide non-stationary time series: The brain-network case | http://arxiv.org/abs/1701.07767 | id:1701.07767 author:Konstantinos Slavakis, Shiva Salsabilian, David S. Wack, Sarah F. Muldoon, Henry E. Baidoo-Williams, Jean M. Vettel, Matthew Cieslak, Scott T. Grafton category:cs.LG stat.ML  published:2017-01-26 summary:This paper advocates Riemannian multi-manifold modeling in the context of network-wide non-stationary time-series analysis. Time-series data, collected sequentially over time and across a network, yield features which are viewed as points in or close to a union of multiple submanifolds of a Riemannian manifold, and distinguishing disparate time series amounts to clustering multiple Riemannian submanifolds. To support the claim that exploiting the latent Riemannian geometry behind many statistical features of time series is beneficial to learning from network data, this paper focuses on brain networks and puts forth two feature-generation schemes for network-wide dynamic time series. The first is motivated by Granger-causality arguments and uses an auto-regressive moving average model to map low-rank linear vector subspaces, spanned by column vectors of appropriately defined observability matrices, to points into the Grassmann manifold. The second utilizes (non-linear) dependencies among network nodes by introducing kernel-based partial correlations to generate points in the manifold of positive-definite matrices. Capitilizing on recently developed research on clustering Riemannian submanifolds, an algorithm is provided for distinguishing time series based on their geometrical properties, revealed within Riemannian feature spaces. Extensive numerical tests demonstrate that the proposed framework outperforms classical and state-of-the-art techniques in clustering brain-network states/structures hidden beneath synthetic fMRI time series and brain-activity signals generated from real brain-network structural connectivity matrices. version:1
arxiv-1701-07761 | A theoretical framework for evaluating forward feature selection methods based on mutual information | http://arxiv.org/abs/1701.07761 | id:1701.07761 author:Francisco Macedo, M. Rosário Oliveira, António Pacheco, Rui Valadas category:stat.ML cs.LG  published:2017-01-26 summary:Feature selection problems arise in a variety of applications, such as microarray analysis, clinical prediction, text categorization, image classification and face recognition, multi-label learning, and classification of internet traffic. Among the various classes of methods, forward feature selection methods based on mutual information have become very popular and are widely used in practice. However, comparative evaluations of these methods have been limited by being based on specific datasets and classifiers. In this paper, we develop a theoretical framework that allows evaluating the methods based on their theoretical properties. Our framework is grounded on the properties of the target objective function that the methods try to approximate, and on a novel categorization of features, according to their contribution to the explanation of the class; we derive upper and lower bounds for the target objective function and relate these bounds with the feature types. Then, we characterize the types of approximations taken by the methods, and analyze how these approximations cope with the good properties of the target objective function. Additionally, we develop a distributional setting designed to illustrate the various deficiencies of the methods, and provide several examples of wrong feature selections. Based on our work, we identify clearly the methods that should be avoided, and the methods that currently have the best performance. version:1
arxiv-1701-07756 | Dynamic time warping distance for message propagation classification in Twitter | http://arxiv.org/abs/1701.07756 | id:1701.07756 author:Siwar Jendoubi, Arnaud Martin, Ludovic Liétard, Boutheina Ben Yaghlane, Hend Ben Hadji category:cs.AI cs.SI stat.ML  published:2017-01-26 summary:Social messages classification is a research domain that has attracted the attention of many researchers in these last years. Indeed, the social message is different from ordinary text because it has some special characteristics like its shortness. Then the development of new approaches for the processing of the social message is now essential to make its classification more efficient. In this paper, we are mainly interested in the classification of social messages based on their spreading on online social networks (OSN). We proposed a new distance metric based on the Dynamic Time Warping distance and we use it with the probabilistic and the evidential k Nearest Neighbors (k-NN) classifiers to classify propagation networks (PrNets) of messages. The propagation network is a directed acyclic graph (DAG) that is used to record propagation traces of the message, the traversed links and their types. We tested the proposed metric with the chosen k-NN classifiers on real world propagation traces that were collected from Twitter social network and we got good classification accuracies. version:1
arxiv-1701-07732 | Pose Invariant Embedding for Deep Person Re-identification | http://arxiv.org/abs/1701.07732 | id:1701.07732 author:Liang Zheng, Yujia Huang, Huchuan Lu, Yi Yang category:cs.CV  published:2017-01-26 summary:Pedestrian misalignment, which mainly arises from detector errors and pose variations, is a critical problem for a robust person re-identification (re-ID) system. With bad alignment, the background noise will significantly compromise the feature learning and matching process. To address this problem, this paper introduces the pose invariant embedding (PIE) as a pedestrian descriptor. First, in order to align pedestrians to a standard pose, the PoseBox structure is introduced, which is generated through pose estimation followed by affine transformations. Second, to reduce the impact of pose estimation errors and information loss during PoseBox construction, we design a PoseBox fusion (PBF) CNN architecture that takes the original image, the PoseBox, and the pose estimation confidence as input. The proposed PIE descriptor is thus defined as the fully connected layer of the PBF network for the retrieval task. Experiments are conducted on the Market-1501, CUHK03, and VIPeR datasets. We show that PoseBox alone yields decent re-ID accuracy and that when integrated in the PBF network, the learned PIE descriptor produces competitive performance compared with the state-of-the-art approaches. version:1
arxiv-1701-07681 | Fast and Accurate Time Series Classification with WEASEL | http://arxiv.org/abs/1701.07681 | id:1701.07681 author:Patrick Schäfer, Ulf Leser category:cs.DS cs.LG stat.ML  published:2017-01-26 summary:Time series (TS) occur in many scientific and commercial applications, ranging from earth surveillance to industry automation to the smart grids. An important type of TS analysis is classification, which can, for instance, improve energy load forecasting in smart grids by detecting the types of electronic devices based on their energy consumption profiles recorded by automatic sensors. Such sensor-driven applications are very often characterized by (a) very long TS and (b) very large TS datasets needing classification. However, current methods to time series classification (TSC) cannot cope with such data volumes at acceptable accuracy; they are either scalable but offer only inferior classification quality, or they achieve state-of-the-art classification quality but cannot scale to large data volumes. In this paper, we present WEASEL (Word ExtrAction for time SEries cLassification), a novel TSC method which is both scalable and accurate. Like other state-of-the-art TSC methods, WEASEL transforms time series into feature vectors, using a sliding-window approach, which are then analyzed through a machine learning classifier. The novelty of WEASEL lies in its specific method for deriving features, resulting in a much smaller yet much more discriminative feature set. On the popular UCR benchmark of 85 TS datasets, WEASEL is more accurate than the best current non-ensemble algorithms at orders-of-magnitude lower classification and training times, and it is almost as accurate as ensemble classifiers, whose computational complexity makes them inapplicable even for mid-size datasets. The outstanding robustness of WEASEL is also confirmed by experiments on two real smart grid datasets, where it out-of-the-box achieves almost the same accuracy as highly tuned, domain-specific methods. version:1
arxiv-1701-07675 | Sparse Ternary Codes for similarity search have higher coding gain than dense binary codes | http://arxiv.org/abs/1701.07675 | id:1701.07675 author:Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov, Taras Holotyak category:cs.IT cs.CV cs.IR math.IT  published:2017-01-26 summary:This paper addresses the problem of Approximate Nearest Neighbor (ANN) search in pattern recognition where feature vectors in a database are encoded as compact codes in order to speed-up the similarity search in large-scale databases. Considering the ANN problem from an information-theoretic perspective, we interpret it as an encoding which maps the original feature vectors to a less-entropic sparse representation while requiring them to be as informative as possible. We then define the coding gain for ANN search using information-theoretic measures. We next show that the classical approach to this problem which consists of binarization of the projected vectors is sub-optimal. Instead, we show that a recently proposed ternary encoding achieves higher coding gains. version:1
arxiv-1701-07604 | Super-resolution Using Constrained Deep Texture Synthesis | http://arxiv.org/abs/1701.07604 | id:1701.07604 author:Libin Sun, James Hays category:cs.CV  published:2017-01-26 summary:Hallucinating high frequency image details in single image super-resolution is a challenging task. Traditional super-resolution methods tend to produce oversmoothed output images due to the ambiguity in mapping between low and high resolution patches. We build on recent success in deep learning based texture synthesis and show that this rich feature space can facilitate successful transfer and synthesis of high frequency image details to improve the visual quality of super-resolution results on a wide variety of natural textures and images. version:1
arxiv-1701-05131 | Basic protocols in quantum reinforcement learning with superconducting circuits | http://arxiv.org/abs/1701.05131 | id:1701.05131 author:Lucas Lamata category:quant-ph cond-mat.mes-hall cond-mat.supr-con cs.AI stat.ML  published:2017-01-18 summary:Superconducting circuit technologies have recently achieved quantum protocols involving closed feedback loops. Quantum artificial intelligence and quantum machine learning are emerging fields inside quantum technologies which may enable quantum devices to acquire information from the outer world and improve themselves via a learning process. Here we propose the implementation of basic protocols in quantum reinforcement learning, with superconducting circuits employing feedback-loop control. We introduce diverse scenarios for proof-of-principle experiments with state-of-the-art superconducting circuit technologies and analyze their feasibility in presence of imperfections. The field of quantum artificial intelligence implemented with superconducting circuits paves the way for enhanced quantum control and quantum computation protocols. version:2
arxiv-1701-07570 | Strongly Adaptive Regret Implies Optimally Dynamic Regret | http://arxiv.org/abs/1701.07570 | id:1701.07570 author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG  published:2017-01-26 summary:To cope with changing environments, recent literature in online learning has introduced the concepts of adaptive regret and dynamic regret independently. In this paper, we illustrate an intrinsic connection between these two concepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation. This observation implies that strongly adaptive algorithms can be directly leveraged to minimize the dynamic regret. As a result, we present a series of strongly adaptive algorithms whose dynamic regrets are minimax optimal for convex functions, exponentially concave functions, and strongly convex functions, respectively. To the best of our knowledge, this is first time that such kind of dynamic regret bound is established for exponentially concave functions. Moreover, all of those adaptive algorithms do not need any prior knowledge of the functional variation, which is a significant advantage over previous specialized methods for minimizing dynamic regret. version:1
arxiv-1701-07543 | FPGA Architecture for Deep Learning and its application to Planetary Robotics | http://arxiv.org/abs/1701.07543 | id:1701.07543 author:Pranay Gankidi, Jekan Thangavelautham category:cs.LG astro-ph.IM cs.RO  published:2017-01-26 summary:Autonomous control systems onboard planetary rovers and spacecraft benefit from having cognitive capabilities like learning so that they can adapt to unexpected situations in-situ. Q-learning is a form of reinforcement learning and it has been efficient in solving certain class of learning problems. However, embedded systems onboard planetary rovers and spacecraft rarely implement learning algorithms due to the constraints faced in the field, like processing power, chip size, convergence rate and costs due to the need for radiation hardening. These challenges present a compelling need for a portable, low-power, area efficient hardware accelerator to make learning algorithms practical onboard space hardware. This paper presents a FPGA implementation of Q-learning with Artificial Neural Networks (ANN). This method matches the massive parallelism inherent in neural network software with the fine-grain parallelism of an FPGA hardware thereby dramatically reducing processing time. Mars Science Laboratory currently uses Xilinx-Space-grade Virtex FPGA devices for image processing, pyrotechnic operation control and obstacle avoidance. We simulate and program our architecture on a Xilinx Virtex 7 FPGA. The architectural implementation for a single neuron Q-learning and a more complex Multilayer Perception (MLP) Q-learning accelerator has been demonstrated. The results show up to a 43-fold speed up by Virtex 7 FPGAs compared to a conventional Intel i5 2.3 GHz CPU. Finally, we simulate the proposed architecture using the Symphony simulator and compiler from Xilinx, and evaluate the performance and power consumption. version:1
arxiv-1701-02481 | Implicitly Incorporating Morphological Information into Word Embedding | http://arxiv.org/abs/1701.02481 | id:1701.02481 author:Yang Xu, Jiawei Liu category:cs.CL cs.LG  published:2017-01-10 summary:In this paper, we propose three novel models to enhance word embedding by implicitly using morphological information. Experiments on word similarity and syntactic analogy show that the implicit models are superior to traditional explicit ones. Our models outperform all state-of-the-art baselines and significantly improve the performance on both tasks. Moreover, our performance on the smallest corpus is similar to the performance of CBOW on the corpus which is five times the size of ours. Parameter analysis indicates that the implicit models can supplement semantic information during the word embedding training process. version:2
arxiv-1701-07483 | A Model-based Projection Technique for Segmenting Customers | http://arxiv.org/abs/1701.07483 | id:1701.07483 author:Srikanth Jagabathula, Lakshminarayanan Subramanian, Ashwin Venkataraman category:stat.ME cs.LG stat.AP stat.ML  published:2017-01-25 summary:We consider the problem of segmenting a large population of customers into non-overlapping groups with similar preferences, using diverse preference observations such as purchases, ratings, clicks, etc. over subsets of items. We focus on the setting where the universe of items is large (ranging from thousands to millions) and unstructured (lacking well-defined attributes) and each customer provides observations for only a few items. These data characteristics limit the applicability of existing techniques in marketing and machine learning. To overcome these limitations, we propose a model-based projection technique, which transforms the diverse set of observations into a more comparable scale and deals with missing data by projecting the transformed data onto a low-dimensional space. We then cluster the projected data to obtain the customer segments. Theoretically, we derive precise necessary and sufficient conditions that guarantee asymptotic recovery of the true customer segments. Empirically, we demonstrate the speed and performance of our method in two real-world case studies: (a) 84% improvement in the accuracy of new movie recommendations on the MovieLens data set and (b) 6% improvement in the performance of similar item recommendations algorithm on an offline dataset at eBay. We show that our method outperforms standard latent-class and demographic-based techniques. version:1
arxiv-1701-07474 | Exploiting Convolutional Neural Network for Risk Prediction with Medical Feature Embedding | http://arxiv.org/abs/1701.07474 | id:1701.07474 author:Zhengping Che, Yu Cheng, Zhaonan Sun, Yan Liu category:cs.LG stat.ML  published:2017-01-25 summary:The widespread availability of electronic health records (EHRs) promises to usher in the era of personalized medicine. However, the problem of extracting useful clinical representations from longitudinal EHR data remains challenging. In this paper, we explore deep neural network models with learned medical feature embedding to deal with the problems of high dimensionality and temporality. Specifically, we use a multi-layer convolutional neural network (CNN) to parameterize the model and is thus able to capture complex non-linear longitudinal evolution of EHRs. Our model can effectively capture local/short temporal dependency in EHRs, which is beneficial for risk prediction. To account for high dimensionality, we use the embedding medical features in the CNN model which hold the natural medical concepts. Our initial experiments produce promising results and demonstrate the effectiveness of both the medical feature embedding and the proposed convolutional neural network in risk prediction on cohorts of congestive heart failure and diabetes patients compared with several strong baselines. version:1
arxiv-1701-07403 | Learning Light Transport the Reinforced Way | http://arxiv.org/abs/1701.07403 | id:1701.07403 author:Ken Dahm, Alexander Keller category:cs.LG cs.GR  published:2017-01-25 summary:We show that the equations of reinforcement learning and light transport simulation are related integral equations. Based on this correspondence, a scheme to learn importance while sampling path space is derived. The new approach is demonstrated in a consistent light transport simulation algorithm that uses reinforcement learning to progressively learn where light comes from. As using this information for importance sampling includes information about visibility, too, the number of light transport paths with non-zero contribution is dramatically increased, resulting in much less noisy images within a fixed time budget. version:1
arxiv-1701-07393 | Recovering 3D Planar Arrangements from Videos | http://arxiv.org/abs/1701.07393 | id:1701.07393 author:Shuai Du, Youyi Zheng category:cs.CV  published:2017-01-25 summary:Acquiring 3D geometry of real world objects has various applications in 3D digitization, such as navigation and content generation in virtual environments. Image remains one of the most popular media for such visual tasks due to its simplicity of acquisition. Traditional image-based 3D reconstruction approaches heavily exploit point-to-point correspondence among multiple images to estimate camera motion and 3D geometry. Establishing point-to-point correspondence lies at the center of the 3D reconstruction pipeline, which however is easily prone to errors. In this paper, we propose an optimization framework which traces image points using a novel structure-guided dynamic tracking algorithm and estimates both the camera motion and a 3D structure model by enforcing a set of planar constraints. The key to our method is a structure model represented as a set of planes and their arrangements. Constraints derived from the structure model is used both in the correspondence establishment stage and the bundle adjustment stage in our reconstruction pipeline. Experiments show that our algorithm can effectively localize structure correspondence across dense image frames while faithfully reconstructing the camera motion and the underlying structured 3D model. version:1
arxiv-1701-08716 | Does Weather Matter? Causal Analysis of TV Logs | http://arxiv.org/abs/1701.08716 | id:1701.08716 author:Shi Zong, Branislav Kveton, Shlomo Berkovsky, Azin Ashkan, Nikos Vlassis, Zheng Wen category:cs.CY cs.LG  published:2017-01-25 summary:Weather affects our mood and behaviors, and many aspects of our life. When it is sunny, most people become happier; but when it rains, some people get depressed. Despite this evidence and the abundance of data, weather has mostly been overlooked in the machine learning and data science research. This work presents a causal analysis of how weather affects TV watching patterns. We show that some weather attributes, such as pressure and precipitation, cause major changes in TV watching patterns. To the best of our knowledge, this is the first large-scale causal study of the impact of weather on TV watching patterns. version:1
arxiv-1701-07372 | A Multi-view RGB-D Approach for Human Pose Estimation in Operating Rooms | http://arxiv.org/abs/1701.07372 | id:1701.07372 author:Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin, Nicolas Padoy category:cs.CV  published:2017-01-25 summary:Many approaches have been proposed for human pose estimation in single and multi-view RGB images. However, some environments, such as the operating room, are still very challenging for state-of-the-art RGB methods. In this paper, we propose an approach for multi-view 3D human pose estimation from RGB-D images and demonstrate the benefits of using the additional depth channel for pose refinement beyond its use for the generation of improved features. The proposed method permits the joint detection and estimation of the poses without knowing a priori the number of persons present in the scene. We evaluate this approach on a novel multi-view RGB-D dataset acquired during live surgeries and annotated with ground truth 3D poses. version:1
arxiv-1701-07354 | Photographic dataset: playing cards | http://arxiv.org/abs/1701.07354 | id:1701.07354 author:David Villacis, Santeri Kaupinmäki, Samuli Siltanen, Teemu Helenius category:cs.CV physics.data-an  published:2017-01-25 summary:This is a photographic dataset collected for testing image processing algorithms. The idea is to have images that can exploit the properties of total variation, therefore a set of playing cards was distributed on the scene. The dataset is made available at www.fips.fi/photographic_dataset2.php version:1
arxiv-1701-07275 | Universal representations:The missing link between faces, text, planktons, and cat breeds | http://arxiv.org/abs/1701.07275 | id:1701.07275 author:Hakan Bilen, Andrea Vedaldi category:cs.CV stat.ML  published:2017-01-25 summary:With the advent of large labelled datasets and high-capacity models, the performance of machine vision systems has been improving rapidly. However, the technology has still major limitations, starting from the fact that different vision problems are still solved by different models, trained from scratch or fine-tuned on the target data. The human visual system, in stark contrast, learns a universal representation for vision in the early life of an individual. This representation works well for an enormous variety of vision problems, with little or no change, with the major advantage of requiring little training data to solve any of them. version:1
arxiv-1701-07266 | k*-Nearest Neighbors: From Global to Local | http://arxiv.org/abs/1701.07266 | id:1701.07266 author:Oren Anava, Kfir Y. Levy category:stat.ML cs.LG  published:2017-01-25 summary:The weighted k-nearest neighbors algorithm is one of the most fundamental non-parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods. version:1
arxiv-1701-07248 | Distributed methods for synchronization of orthogonal matrices over graphs | http://arxiv.org/abs/1701.07248 | id:1701.07248 author:Johan Thunberg, Florian Bernard, Jorge Goncalves category:math.OC cs.CV cs.DC cs.MA  published:2017-01-25 summary:This paper addresses the problem of synchronizing orthogonal matrices over directed graphs. For synchronized transformations (or matrices), composite transformations over loops equal the identity. We formulate the synchronization problem as a least-squares optimization problem with nonlinear constraints. The synchronization problem appears as one of the key components in applications ranging from 3D-localization to image registration. The main contributions of this work can be summarized as the introduction of two novel algorithms; one for symmetric graphs and one for graphs that are possibly asymmetric. Under general conditions, the former has guaranteed convergence to the solution of a spectral relaxation to the synchronization problem. The latter is stable for small step sizes when the graph is quasi-strongly connected. The proposed methods are verified in numerical simulations. version:1
arxiv-1701-07243 | Decoding Epileptogenesis in a Reduced State Space | http://arxiv.org/abs/1701.07243 | id:1701.07243 author:François G. Meyer, Alexander M. Benison, Zachariah Smith, Daniel S. Barth category:q-bio.NC cs.LG q-bio.QM  published:2017-01-25 summary:We describe here the recent results of a multidisciplinary effort to design a biomarker that can actively and continuously decode the progressive changes in neuronal organization leading to epilepsy, a process known as epileptogenesis. Using an animal model of acquired epilepsy, wechronically record hippocampal evoked potentials elicited by an auditory stimulus. Using a set of reduced coordinates, our algorithm can identify universal smooth low-dimensional configurations of the auditory evoked potentials that correspond to distinct stages of epileptogenesis. We use a hidden Markov model to learn the dynamics of the evoked potential, as it evolves along these smooth low-dimensional subsets. We provide experimental evidence that the biomarker is able to exploit subtle changes in the evoked potential to reliably decode the stage of epileptogenesis and predict whether an animal will eventually recover from the injury, or develop spontaneous seizures. version:1
arxiv-1701-07232 | Learn&Fuzz: Machine Learning for Input Fuzzing | http://arxiv.org/abs/1701.07232 | id:1701.07232 author:Patrice Godefroid, Hila Peleg, Rishabh Singh category:cs.AI cs.CR cs.LG cs.PL cs.SE  published:2017-01-25 summary:Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss (and measure) the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn&fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs. version:1
arxiv-1701-07213 | Learning from Label Proportions in Brain-Computer Interfaces: Online Unsupervised Learning with Guarantees | http://arxiv.org/abs/1701.07213 | id:1701.07213 author:D Hübner, T Verhoeven, K Schmid, K-R Müller, M Tangermann, P-J Kindermans category:stat.ML cs.HC q-bio.NC  published:2017-01-25 summary:Objective: Using traditional approaches, a Brain-Computer Interface (BCI) requires the collection of calibration data for new subjects prior to online use. Calibration time can be reduced or eliminated e.g.~by transfer of a pre-trained classifier or unsupervised adaptive classification methods which learn from scratch and adapt over time. While such heuristics work well in practice, none of them can provide theoretical guarantees. Our objective is to modify an event-related potential (ERP) paradigm to work in unison with the machine learning decoder to achieve a reliable calibration-less decoding with a guarantee to recover the true class means. Method: We introduce learning from label proportions (LLP) to the BCI community as a new unsupervised, and easy-to-implement classification approach for ERP-based BCIs. The LLP estimates the mean target and non-target responses based on known proportions of these two classes in different groups of the data. We modified a visual ERP speller to meet the requirements of the LLP. For evaluation, we ran simulations on artificially created data sets and conducted an online BCI study with N=13 subjects performing a copy-spelling task. Results: Theoretical considerations show that LLP is guaranteed to minimize the loss function similarly to a corresponding supervised classifier. It performed well in simulations and in the online application, where 84.5% of characters were spelled correctly on average without prior calibration. Significance: The continuously adapting LLP classifier is the first unsupervised decoder for ERP BCIs guaranteed to find the true class means. This makes it an ideal solution to avoid a tedious calibration and to tackle non-stationarities in the data. Additionally, LLP works on complementary principles compared to existing unsupervised methods, allowing for their further enhancement when combined with LLP. version:1
arxiv-1701-07204 | Fast Exact k-Means, k-Medians and Bregman Divergence Clustering in 1D | http://arxiv.org/abs/1701.07204 | id:1701.07204 author:Allan Grønlund, Kasper Green Larsen, Alexander Mathiasen, Jesper Sindahl Nielsen category:cs.DS cs.AI cs.LG  published:2017-01-25 summary:The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension $d\ge 2$, however, for the 1D case there exist exact polynomial time algorithms. The current state of the art is a $O(kn^2)$ dynamic programming algorithm that uses $O(nk)$ space. We present a new algorithm improving this to $O(kn \log n)$ time and optimal $O(n)$ space. We generalize our algorithm to work for the absolute distance instead of squared distance and to work for any Bregman Divergence as well. version:1
arxiv-1701-07194 | Privileged Multi-label Learning | http://arxiv.org/abs/1701.07194 | id:1701.07194 author:Shan You, Chang Xu, Yunhe Wang, Chao Xu, Dacheng Tao category:stat.ML cs.LG  published:2017-01-25 summary:This paper presents privileged multi-label learning (PrML) to explore and exploit the relationship between labels in multi-label learning problems. We suggest that for each individual label, it cannot only be implicitly connected with other labels via the low-rank constraint over label predictors, but also its performance on examples can receive the explicit comments from other labels together acting as an \emph{Oracle teacher}. We generate privileged label feature for each example and its individual label, and then integrate it into the framework of low-rank based multi-label learning. The proposed algorithm can therefore comprehensively explore and exploit label relationships by inheriting all the merits of privileged information and low-rank constraints. We show that PrML can be efficiently solved by dual coordinate descent algorithm using iterative optimization strategy with cheap updates. Experiments on benchmark datasets show that through privileged label features, the performance can be significantly improved and PrML is superior to several competing methods in most cases. version:1
arxiv-1701-07179 | Malicious URL Detection using Machine Learning: A Survey | http://arxiv.org/abs/1701.07179 | id:1701.07179 author:Doyen Sahoo, Chenghao Liu, Steven C. H. Hoi category:cs.LG cs.CR  published:2017-01-25 summary:Malicious URL, a.k.a. malicious website, is a common and serious threat to cybersecurity. Malicious URLs host unsolicited content (spam, phishing, drive-by exploits, etc.) and lure unsuspecting users to become victims of scams (monetary loss, theft of private information, and malware installation), and cause losses of billions of dollars every year. It is imperative to detect and act on such threats in a timely manner. Traditionally, this detection is done mostly through the usage of blacklists. However, blacklists cannot be exhaustive, and lack the ability to detect newly generated malicious URLs. To improve the generality of malicious URL detectors, machine learning techniques have been explored with increasing attention in recent years. This article aims to provide a comprehensive survey and a structural understanding of Malicious URL Detection techniques using machine learning. We present the formal formulation of Malicious URL Detection as a machine learning task, and categorize and review the contributions of literature studies that addresses different dimensions of this problem (feature representation, algorithm design, etc.). Further, this article provides a timely and comprehensive survey for a range of different audiences, not only for machine learning researchers and engineers in academia, but also for professionals and practitioners in cybersecurity industry, to help them understand the state of the art and facilitate their own research and practical applications. We also discuss practical issues in system design, open research challenges, and point out some important directions for future research. version:1
arxiv-1701-04128 | Understanding the Effective Receptive Field in Deep Convolutional Neural Networks | http://arxiv.org/abs/1701.04128 | id:1701.04128 author:Wenjie Luo, Yujia Li, Raquel Urtasun, Richard Zemel category:cs.CV cs.AI cs.LG  published:2017-01-15 summary:We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small. version:2
arxiv-1701-07174 | Towards End-to-End Face Recognition through Alignment Learning | http://arxiv.org/abs/1701.07174 | id:1701.07174 author:Yuanyi Zhong, Jiansheng Chen, Bo Huang category:cs.CV  published:2017-01-25 summary:Plenty of effective methods have been proposed for face recognition during the past decade. Although these methods differ essentially in many aspects, a common practice of them is to specifically align the facial area based on the prior knowledge of human face structure before feature extraction. In most systems, the face alignment module is implemented independently. This has actually caused difficulties in the designing and training of end-to-end face recognition models. In this paper we study the possibility of alignment learning in end-to-end face recognition, in which neither prior knowledge on facial landmarks nor artificially defined geometric transformations are required. Specifically, spatial transformer layers are inserted in front of the feature extraction layers in a Convolutional Neural Network (CNN) for face recognition. Only human identity clues are used for driving the neural network to automatically learn the most suitable geometric transformation and the most appropriate facial area for the recognition task. To ensure reproducibility, our model is trained purely on the publicly available CASIA-WebFace dataset, and is tested on the Labeled Face in the Wild (LFW) dataset. We have achieved a verification accuracy of 99.08\% which is comparable to state-of-the-art single model based methods. version:1
arxiv-1701-07166 | Personalized Classifier Ensemble Pruning Framework for Mobile Crowdsourcing | http://arxiv.org/abs/1701.07166 | id:1701.07166 author:Shaowei Wang, Liusheng Huang, Pengzhan Wang, Hongli Xu, Wei Yang category:cs.DC cs.HC cs.LG  published:2017-01-25 summary:Ensemble learning has been widely employed by mobile applications, ranging from environmental sensing to activity recognitions. One of the fundamental issue in ensemble learning is the trade-off between classification accuracy and computational costs, which is the goal of ensemble pruning. During crowdsourcing, the centralized aggregator releases ensemble learning models to a large number of mobile participants for task evaluation or as the crowdsourcing learning results, while different participants may seek for different levels of the accuracy-cost trade-off. However, most of existing ensemble pruning approaches consider only one identical level of such trade-off. In this study, we present an efficient ensemble pruning framework for personalized accuracy-cost trade-offs via multi-objective optimization. Specifically, for the commonly used linear-combination style of the trade-off, we provide an objective-mixture optimization to further reduce the number of ensemble candidates. Experimental results show that our framework is highly efficient for personalized ensemble pruning, and achieves much better pruning performance with objective-mixture optimization when compared to state-of-art approaches. version:1
arxiv-1701-07164 | Understanding the Historic Emergence of Diversity in Painting via Color Contrast | http://arxiv.org/abs/1701.07164 | id:1701.07164 author:Byunghwee Lee, Daniel Kim, Hawoong Jeong, Seunghye Sun, Juyong Park category:cs.CV physics.soc-ph  published:2017-01-25 summary:Painting is an art form that has long functioned as major channel for communication and creative expression. Understanding how painting has evolved over the centuries is therefore an essential component for understanding cultural history, intricately linked with developments in aesthetics, science, and technology. The explosive growth in the ranges of stylistic diversity in painting starting in the nineteenth century, for example, is understood to be the hallmark of a stark departure from traditional norms on multidisciplinary fronts. Yet, there exist few quantitative frameworks that allow us to characterize such developments on an extensive scale, which would require both robust statistical methods for quantifying the complexities of artistic styles and data of sufficient quality and quantity to which we can fruitfully apply them. Here we propose an analytical framework that allows us to capture the stylistic evolution of paintings based on the color contrast relationships that also incorporates the geometric separation between pixels of images in a large-scale archive of 179,853 images. We first measure how paintings have evolved over time, and then characterize the remarkable explosive growth in diversity and individuality in the modern era. Our analysis demonstrates how robust scientific methods married with large-scale, high-quality data can reveal interesting patterns that lie behind the complexities of art and culture. version:1
arxiv-1701-07158 | An Edge Driven Wavelet Frame Model for Image Restoration | http://arxiv.org/abs/1701.07158 | id:1701.07158 author:Jae Kyu Choi, Bin Dong, Xiaoqun Zhang category:math.NA cs.CV math.FA  published:2017-01-25 summary:Wavelet frame systems are known to be effective in capturing singularities from noisy and degraded images. In this paper, we introduce a new edge driven wavelet frame model for image restoration by approximating images as piecewise smooth functions. With an implicit representation of image singularities sets, the proposed model inflicts different strength of regularization on smooth and singular image regions and edges. The proposed edge driven model is robust to both image approximation and singularity estimation. The implicit formulation also enables an asymptotic analysis of the proposed models and a rigorous connection between the discrete model and a general continuous variational model. Finally, numerical results on image inpainting and deblurring show that the proposed model is compared favorably against several popular image restoration models. version:1
arxiv-1701-07149 | Hierarchical Recurrent Attention Network for Response Generation | http://arxiv.org/abs/1701.07149 | id:1701.07149 author:Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma category:cs.CL  published:2017-01-25 summary:We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation. version:1
arxiv-1701-07148 | CP-decomposition with Tensor Power Method for Convolutional Neural Networks Compression | http://arxiv.org/abs/1701.07148 | id:1701.07148 author:Marcella Astrid, Seung-Ik Lee category:cs.LG  published:2017-01-25 summary:Convolutional Neural Networks (CNNs) has shown a great success in many areas including complex image classification tasks. However, they need a lot of memory and computational cost, which hinders them from running in relatively low-end smart devices such as smart phones. We propose a CNN compression method based on CP-decomposition and Tensor Power Method. We also propose an iterative fine tuning, with which we fine-tune the whole network after decomposing each layer, but before decomposing the next layer. Significant reduction in memory and computation cost is achieved compared to state-of-the-art previous work with no more accuracy loss. version:1
arxiv-1701-07125 | jsCoq: Towards Hybrid Theorem Proving Interfaces | http://arxiv.org/abs/1701.07125 | id:1701.07125 author:Emilio Jesús Gallego Arias, Benoît Pin, Pierre Jouvelot category:cs.PL cs.HC cs.LG cs.LO  published:2017-01-25 summary:We describe jsCcoq, a new platform and user environment for the Coq interactive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015 specification, and it is typically run inside a standards-compliant browser, without the need of external servers or services. Targeting educational use, jsCoq allows the user to start interaction with proof scripts right away, thanks to its self-contained nature. Indeed, a full Coq environment is packed along the proof scripts, easing distribution and installation. Starting to use jsCoq is as easy as clicking on a link. The current release ships more than 10 popular Coq libraries, and supports popular books such as Software Foundations or Certified Programming with Dependent Types. The new target platform has opened up new interaction and display possibilities. It has also fostered the development of some new Coq-related technology. In particular, we have implemented a new serialization-based protocol for interaction with the proof assistant, as well as a new package format for library distribution. version:1
arxiv-1701-07122 | Learning Multi-level Region Consistency with Dense Multi-label Networks for Semantic Segmentation | http://arxiv.org/abs/1701.07122 | id:1701.07122 author:Tong Shen, Guosheng Lin, Chunhua Shen, Ian Reid category:cs.CV  published:2017-01-25 summary:Semantic image segmentation is a fundamental task in image understanding. Per-pixel semantic labelling of an image benefits greatly from the ability to consider region consistency both locally and globally. However, many Fully Convolutional Network based methods do not impose such consistency, which may give rise to noisy and implausible predictions. We address this issue by proposing a dense multi-label network module that is able to encourage the region consistency at different levels. This simple but effective module can be easily integrated into any semantic segmentation systems. With comprehensive experiments, we show that the dense multi-label can successfully remove the implausible labels and clear the confusion so as to boost the performance of semantic segmentation systems. version:1
arxiv-1701-04944 | A Machine Learning Alternative to P-values | http://arxiv.org/abs/1701.04944 | id:1701.04944 author:Min Lu, Hemant Ishwaran category:stat.ML cs.LG  published:2017-01-18 summary:This paper presents an alternative approach to p-values in regression settings. This approach, whose origins can be traced to machine learning, is based on the leave-one-out bootstrap for prediction error. In machine learning this is called the out-of-bag (OOB) error. To obtain the OOB error for a model, one draws a bootstrap sample and fits the model to the in-sample data. The out-of-sample prediction error for the model is obtained by calculating the prediction error for the model using the out-of-sample data. Repeating and averaging yields the OOB error, which represents a robust cross-validated estimate of the accuracy of the underlying model. By a simple modification to the bootstrap data involving "noising up" a variable, the OOB method yields a variable importance (VIMP) index, which directly measures how much a specific variable contributes to the prediction precision of a model. VIMP provides a scientifically interpretable measure of the effect size of a variable, we call the "predictive effect size", that holds whether the researcher's model is correct or not, unlike the p-value whose calculation is based on the assumed correctness of the model. We also discuss a marginal VIMP index, also easily calculated, which measures the marginal effect of a variable, or what we call "the discovery effect". The OOB procedure can be applied to both parametric and nonparametric regression models and requires only that the researcher can repeatedly fit their model to bootstrap and modified bootstrap data. We illustrate this approach on a survival data set involving patients with systolic heart failure and to a simulated survival data set where the model is incorrectly specified to illustrate its robustness to model misspecification. version:4
arxiv-1701-07114 | On the Effectiveness of Discretizing Quantitative Attributes in Linear Classifiers | http://arxiv.org/abs/1701.07114 | id:1701.07114 author:Nayyar A. Zaidi, Yang Du, Geoffrey I. Webb category:cs.LG  published:2017-01-24 summary:Learning algorithms that learn linear models often have high representation bias on real-world problems. In this paper, we show that this representation bias can be greatly reduced by discretization. Discretization is a common procedure in machine learning that is used to convert a quantitative attribute into a qualitative one. It is often motivated by the limitation of some learners to qualitative data. Discretization loses information, as fewer distinctions between instances are possible using discretized data relative to undiscretized data. In consequence, where discretization is not essential, it might appear desirable to avoid it. However, it has been shown that discretization often substantially reduces the error of the linear generative Bayesian classifier naive Bayes. This motivates a systematic study of the effectiveness of discretizing quantitative attributes for other linear classifiers. In this work, we study the effect of discretization on the performance of linear classifiers optimizing three distinct discriminative objective functions --- logistic regression (optimizing negative log-likelihood), support vector classifiers (optimizing hinge loss) and a zero-hidden layer artificial neural network (optimizing mean-square-error). We show that discretization can greatly increase the accuracy of these linear discriminative learners by reducing their representation bias, especially on big datasets. We substantiate our claims with an empirical study on $42$ benchmark datasets. version:1
arxiv-1701-03077 | A More General Robust Loss Function | http://arxiv.org/abs/1701.03077 | id:1701.03077 author:Jonathan T. Barron category:cs.CV cs.LG stat.ML  published:2017-01-11 summary:We present a loss function which can be viewed as a generalization of many popular loss functions used in robust statistics: the Cauchy/Lorentzian, Welsch, and generalized Charbonnier loss functions (and by transitivity the L2, L1, L1-L2, and pseudo-Huber/Charbonnier loss functions). We describe and visualize this loss, and document several of its useful properties. version:2
arxiv-1701-06981 | Multi-Layer Generalized Linear Estimation | http://arxiv.org/abs/1701.06981 | id:1701.06981 author:Andre Manoel, Florent Krzakala, Marc Mézard, Lenka Zdeborová category:cs.IT cond-mat.stat-mech math.IT stat.ML  published:2017-01-24 summary:We consider the problem of reconstructing a signal from multi-layered (possibly) non-linear measurements. Using non-rigorous but standard methods from statistical physics we present the Multi-Layer Approximate Message Passing (ML-AMP) algorithm for computing marginal probabilities of the corresponding estimation problem and derive the associated state evolution equations to analyze its performance. We also give the expression of the asymptotic free energy and the minimal information-theoretically achievable reconstruction error. Finally, we present some applications of this measurement model for compressed sensing and perceptron learning with structured matrices/patterns, and for a simple model of estimation of latent variables in an auto-encoder. version:1
arxiv-1701-06972 | Deep Network Guided Proof Search | http://arxiv.org/abs/1701.06972 | id:1701.06972 author:Sarah Loos, Geoffrey Irving, Christian Szegedy, Cezary Kaliszyk category:cs.AI cs.LG cs.LO  published:2017-01-24 summary:Deep learning techniques lie at the heart of several significant AI advances in recent years including object recognition and detection, image captioning, machine translation, speech recognition and synthesis, and playing the game of Go. Automated first-order theorem provers can aid in the formalization and verification of mathematical theorems and play a crucial role in program analysis, theory reasoning, security, interpolation, and system verification. Here we suggest deep learning based guidance in the proof search of the theorem prover E. We train and compare several deep neural network models on the traces of existing ATP proofs of Mizar statements and use them to select processed clauses during proof search. We give experimental evidence that with a hybrid, two-phase approach, deep learning based guidance can significantly reduce the average number of proof search steps while increasing the number of theorems proved. Using a few proof guidance strategies that leverage deep neural networks, we have found first-order proofs of 7.36% of the first-order logic translations of the Mizar Mathematical Library theorems that did not previously have ATP generated proofs. This increases the ratio of statements in the corpus with ATP generated proofs from 56% to 59%. version:1
arxiv-1701-06078 | Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive Patterns in Vowel Acoustics | http://arxiv.org/abs/1701.06078 | id:1701.06078 author:Sungkyun Chang, Kyogu Lee category:cs.SD cs.LG  published:2017-01-21 summary:Most of the previous approaches to lyrics-to-audio alignment used a pre-developed automatic speech recognition (ASR) system that innately suffered from several difficulties to adapt the speech model to individual singers. A significant aspect missing in previous works is the self-learnability of repetitive vowel patterns in the singing voice, where the vowel part used is more consistent than the consonant part. Based on this, our system first learns a discriminative subspace of vowel sequences, based on weighted symmetric non-negative matrix factorization (WS-NMF), by taking the self-similarity of a standard acoustic feature as an input. Then, we make use of canonical time warping (CTW), derived from a recent computer vision technique, to find an optimal spatiotemporal transformation between the text and the acoustic sequences. Experiments with Korean and English data sets showed that deploying this method after a pre-developed, unsupervised, singing source separation achieved more promising results than other state-of-the-art unsupervised approaches and an existing ASR-based system. version:2
arxiv-1701-06944 | Motion Segmentation via Global and Local Sparse Subspace Optimization | http://arxiv.org/abs/1701.06944 | id:1701.06944 author:Michael Ying Yang, Hanno Ackermann, Weiyao Lin, Sitong Feng, Bodo Rosenhahn category:cs.CV  published:2017-01-24 summary:In this paper, we propose a new framework for segmenting feature-based moving objects under affine subspace model. Since the feature trajectories in practice are high-dimensional and contain a lot of noise, we firstly apply the sparse PCA to represent the original trajectories with a low-dimensional global subspace, which consists of the orthogonal sparse principal vectors. Subsequently, the local subspace separation will be achieved via automatically searching the sparse representation of the nearest neighbors for each projected data. In order to refine the local subspace estimation result and deal with the missing data problem, we propose an error estimation to encourage the projected data that span a same local subspace to be clustered together. In the end, the segmentation of different motions is achieved through the spectral clustering on an affinity matrix, which is constructed with both the error estimation and sparse neighbors optimization. We test our method extensively and compare it with state-of-the-art methods on the Hopkins 155 dataset and Freiburg-Berkeley Motion Segmentation dataset. The results show that our method is comparable with the other motion segmentation methods, and in many cases exceed them in terms of precision and computation time. version:1
arxiv-1701-06859 | Sparse models for Computer Vision | http://arxiv.org/abs/1701.06859 | id:1701.06859 author:Laurent Perrinet category:cs.CV q-bio.NC  published:2017-01-24 summary:The representation of images in the brain is known to be sparse. That is, as neural activity is recorded in a visual area ---for instance the primary visual cortex of primates--- only a few neurons are active at a given time with respect to the whole population. It is believed that such a property reflects the efficient match of the representation with the statistics of natural scenes. Applying such a paradigm to computer vision therefore seems a promising approach towards more biomimetic algorithms. Herein, we will describe a biologically-inspired approach to this problem. First, we will describe an unsupervised learning paradigm which is particularly adapted to the efficient coding of image patches. Then, we will outline a complete multi-scale framework ---SparseLets--- implementing a biologically inspired sparse representation of natural images. Finally, we will propose novel methods for integrating prior information into these algorithms and provide some preliminary experimental results. We will conclude by giving some perspective on applying such algorithms to computer vision. More specifically, we will propose that bio-inspired approaches may be applied to computer vision using predictive coding schemes, sparse models being one simple and efficient instance of such schemes. version:1
arxiv-1701-06854 | A Hybrid Approach to Wide-Baseline Image Matching | http://arxiv.org/abs/1701.06854 | id:1701.06854 author:Rahul Mitra, Jiakai Zhang, Sharat Chandran, Arjun Jain category:cs.CV  published:2017-01-24 summary:Recent works such as DEEPDESC, DEEPCOMPARE have proposed the learning of robust local image descriptors using a Siamese convolutional neural network directly from images instead of handcrafting them like traditional descriptors such as SIFT and MROGH. Though these algorithms show the state-of-the-art results on the Multi-View Stereo (MVS) dataset, they fail to accomplish many challenging real world tasks such as stitching image panoramas, primarily due to the limited performance of finding correspondence. In this paper, we propose a novel hybrid algorithm with which we are able to harness the power of a learning based approach along with the discriminative advantages that traditional descriptors have to offer. We also propose the PhotoSynth dataset, with size of an order of magnitude more that the traditional MVS dataset in terms of the number of scenes, images, patches along with positive and negative correspondence. Our PhotoSynth dataset also has better coverage of the overall viewpoint, scale, and lighting challenges than the MVS dataset. We evaluate our approach on two data sets which provides images having high viewpoints difference and wide-baselines. One of them is Graffiti scene from the Oxford Affine Covariant Regions Dataset (ACRD) for matching images with 2D affine transformations. The other is the Fountain-P11 dataset for images with 3D projective transformations. We report, to the best of our knowledge, the best results till date on the ACRD Graffiti scene compared to descriptors such as SIFT, MROGH or any other learnt descriptors such as DEEPDESC. version:1
arxiv-1701-06852 | Incorporating Prior Information in Compressive Online Robust Principal Component Analysis | http://arxiv.org/abs/1701.06852 | id:1701.06852 author:Huynh Van Luong, Nikos Deligiannis, Jurgen Seiler, Soren Forchhammer, Andre Kaup category:cs.IT cs.AI math.IT stat.ML  published:2017-01-24 summary:We consider an online version of the robust Principle Component Analysis (PCA), which arises naturally in time-varying source separations such as video foreground-background separation. This paper proposes a compressive online robust PCA with prior information for recursively separating a sequences of frames into sparse and low-rank components from a small set of measurements. In contrast to conventional batch-based PCA, which processes all the frames directly, the proposed method processes measurements taken from each frame. Moreover, this method can efficiently incorporate multiple prior information, namely previous reconstructed frames, to improve the separation and thereafter, update the prior information for the next frame. We utilize multiple prior information by solving $n\text{-}\ell_{1}$ minimization for incorporating the previous sparse components and using incremental singular value decomposition ($\mathrm{SVD}$) for exploiting the previous low-rank components. We also establish theoretical bounds on the number of measurements required to guarantee successful separation under assumptions of static or slowly-changing low-rank components. Using numerical experiments, we evaluate our bounds and the performance of the proposed algorithm. The advantage of incorporating prior information is illustrated by adding in the comparision the performance of our algorithm without using prior information. version:1
arxiv-1701-04851 | Face Synthesis from Facial Identity Features | http://arxiv.org/abs/1701.04851 | id:1701.04851 author:Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, William T. Freeman category:cs.CV stat.ML  published:2017-01-17 summary:We present a method for synthesizing a frontal, neutral-expression image of a person's face given an input face photograph. This is achieved by learning to generate facial landmarks and textures from features extracted from a facial-recognition network. Unlike previous approaches, our encoding feature vector is largely invariant to lighting, pose, and facial expression. Exploiting this invariance, we train our decoder network using only frontal, neutral-expression photographs. Since these photographs are well aligned, we can decompose them into a sparse set of landmark points and aligned texture maps. The decoder then predicts landmarks and textures independently and combines them using a differentiable image warping operation. The resulting images can be used for a number of applications, such as analyzing facial attributes, exposure and white balance adjustment, or creating a 3-D avatar. version:2
arxiv-1701-06806 | A Survey of Quantum Learning Theory | http://arxiv.org/abs/1701.06806 | id:1701.06806 author:Srinivasan Arunachalam, Ronald de Wolf category:quant-ph cs.CC cs.LG  published:2017-01-24 summary:This paper surveys quantum learning theory: the theoretical aspects of machine learning using quantum computers. We describe the main results known for three models of learning: exact learning from membership queries, and Probably Approximately Correct (PAC) and agnostic learning from classical or quantum examples. version:1
arxiv-1701-06805 | Learning Arbitrary Potentials in CRFs with Gradient Descent | http://arxiv.org/abs/1701.06805 | id:1701.06805 author:Måns Larsson, Fredrik Kahl, Shuai Zheng, Anurag Arnab, Philip Torr, Richard Hartley category:cs.CV  published:2017-01-24 summary:Are we using the right potential functions in the Markov- and Conditional- Random Field graphical models that are popular in the Vision community? Semantic segmentation and other pixel-level labelling tasks have made significant progress recently due to the deep learning paradigm. However, most state-of-the-art structured prediction methods also include a random field model with a hand-crafted Gaussian potential to model spatial priors, label consistencies and feature-based image conditioning. In this paper, we challenge this view by introducing a new inference and learning framework which can learn arbitrary pairwise CRF potentials. Both standard spatial and high-dimensional bilateral kernels are considered. Our framework is based on the observation that CRF inference can be achieved via projected gradient descent and consequently, can easily be integrated in deep neural networks to allow for end-to-end training. We compare our approach to several other recent frameworks, both from a theoretical and experimental perspective, and conclude that one can improve performance by using learned potentials. version:1
arxiv-1701-06796 | Discriminative Neural Topic Models | http://arxiv.org/abs/1701.06796 | id:1701.06796 author:Gaurav Pandey, Ambedkar Dukkipati category:cs.LG  published:2017-01-24 summary:We propose a neural network based approach for learning topics from text and image datasets. The model makes no assumptions about the conditional distribution of the observed features given the latent topics. This allows us to perform topic modelling efficiently using sentences of documents and patches of images as observed features, rather than limiting ourselves to words. Moreover, the proposed approach is online, and hence can be used for streaming data. Furthermore, since the approach utilizes neural networks, it can be implemented on GPU with ease, and hence it is very scalable. version:1
arxiv-1701-07398 | Learning an attention model in an artificial visual system | http://arxiv.org/abs/1701.07398 | id:1701.07398 author:Alon Hazan, Yuval Harel, Ron Meir category:cs.CV cs.AI  published:2017-01-24 summary:The Human visual perception of the world is of a large fixed image that is highly detailed and sharp. However, receptor density in the retina is not uniform: a small central region called the fovea is very dense and exhibits high resolution, whereas a peripheral region around it has much lower spatial resolution. Thus, contrary to our perception, we are only able to observe a very small region around the line of sight with high resolution. The perception of a complete and stable view is aided by an attention mechanism that directs the eyes to the numerous points of interest within the scene. The eyes move between these targets in quick, unconscious movements, known as "saccades". Once a target is centered at the fovea, the eyes fixate for a fraction of a second while the visual system extracts the necessary information. An artificial visual system was built based on a fully recurrent neural network set within a reinforcement learning protocol, and learned to attend to regions of interest while solving a classification task. The model is consistent with several experimentally observed phenomena, and suggests novel predictions. version:1
arxiv-1701-06772 | Training Group Orthogonal Neural Networks with Privileged Information | http://arxiv.org/abs/1701.06772 | id:1701.06772 author:Yunpeng Chen, Xiaojie Jin, Jiashi Feng, Shuicheng Yan category:cs.CV  published:2017-01-24 summary:Learning rich and diverse feature representation are always desired for deep convolutional neural networks (CNNs). Besides, when auxiliary annotations are available for specific data, simply ignoring them would be a great waste. In this paper, we incorporate these auxiliary annotations as privileged information and propose a novel CNN model that is able to maximize inherent diversity of a CNN model such that the model can learn better feature representation with a stronger generalization ability. More specifically, we propose a group orthogonal convolutional neural network (GoCNN) to learn features from foreground and background in an orthogonal way by exploiting privileged information for optimization, which automatically emphasizes feature diversity within a single model. Experiments on two benchmark datasets, ImageNet and PASCAL VOC, well demonstrate the effectiveness and high generalization ability of our proposed GoCNN models. version:1
arxiv-1701-04724 | On the Sample Complexity of Graphical Model Selection for Non-Stationary Processes | http://arxiv.org/abs/1701.04724 | id:1701.04724 author:Nguyen Tran Quang, Alexander Jung category:cs.LG stat.ML  published:2017-01-17 summary:We formulate and analyze a graphical model selection method for inferring the conditional independence graph of a high-dimensional non-stationary Gaussian random process (time series) from a finite-length observation. The observed process samples are assumed uncorrelated over time but having different covariance matrices. We characterize the sample complexity of graphical model selection for such processes by analyzing a particular selection method, which is based on sparse neighborhood regression. Our results indicate, similar to the case of i.i.d. samples, accurate GMS is possible even in the high- dimensional regime if the underlying conditional independence graph is sufficiently sparse. version:2
arxiv-1701-06751 | Collective Vertex Classification Using Recursive Neural Network | http://arxiv.org/abs/1701.06751 | id:1701.06751 author:Qiongkai Xu, Qing Wang, Chenchen Xu, Lizhen Qu category:cs.LG cs.SI  published:2017-01-24 summary:Collective classification of vertices is a task of assigning categories to each vertex in a graph based on both vertex attributes and link structure. Nevertheless, some existing approaches do not use the features of neighbouring vertices properly, due to the noise introduced by these features. In this paper, we propose a graph-based recursive neural network framework for collective vertex classification. In this framework, we generate hidden representations from both attributes of vertices and representations of neighbouring vertices via recursive neural networks. Under this framework, we explore two types of recursive neural units, naive recursive neural unit and long short-term memory unit. We have conducted experiments on four real-world network datasets. The experimental results show that our frame- work with long short-term memory model achieves better results and outperforms several competitive baseline methods. version:1
arxiv-1701-06749 | Robust mixture modelling using sub-Gaussian stable distribution | http://arxiv.org/abs/1701.06749 | id:1701.06749 author:Mahdi Teimouri, Saeid Rezakhah, Adel Mohammdpour category:stat.ML  published:2017-01-24 summary:Heavy-tailed distributions are widely used in robust mixture modelling due to possessing thick tails. As a computationally tractable subclass of the stable distributions, sub-Gaussian $\alpha$-stable distribution received much interest in the literature. Here, we introduce a type of expectation maximization algorithm that estimates parameters of a mixture of sub-Gaussian stable distributions. A comparative study, in the presence of some well-known mixture models, is performed to show the robustness and performance of the mixture of sub-Gaussian $\alpha$-stable distributions for modelling, simulated, synthetic, and real data. version:1
arxiv-1701-06731 | Weak Adaptive Submodularity and Group-Based Active Diagnosis with Applications to State Estimation with Persistent Sensor Faults | http://arxiv.org/abs/1701.06731 | id:1701.06731 author:Sze Zheng Yong, Lingyun Gao, Necmiye Ozay category:math.OC cs.SY stat.ML  published:2017-01-24 summary:In this paper, we consider adaptive decision-making problems for stochastic state estimation with partial observations. First, we introduce the concept of weak adaptive submodularity, a generalization of adaptive submodularity, which has found great success in solving challenging adaptive state estimation problems. Then, for the problem of active diagnosis, i.e., discrete state estimation via active sensing, we show that an adaptive greedy policy has a near-optimal performance guarantee when the reward function possesses this property. We further show that the reward function for group-based active diagnosis, which arises in applications such as medical diagnosis and state estimation with persistent sensor faults, is also weakly adaptive submodular. Finally, in experiments of state estimation for an aircraft electrical system with persistent sensor faults, we observe that an adaptive greedy policy performs equally well as an exhaustive search. version:1
arxiv-1701-06725 | A Contextual Bandit Approach for Stream-Based Active Learning | http://arxiv.org/abs/1701.06725 | id:1701.06725 author:Linqi Song, Jie Xu category:cs.LG  published:2017-01-24 summary:Contextual bandit algorithms -- a class of multi-armed bandit algorithms that exploit the contextual information -- have been shown to be effective in solving sequential decision making problems under uncertainty. A common assumption adopted in the literature is that the realized (ground truth) reward by taking the selected action is observed by the learner at no cost, which, however, is not realistic in many practical scenarios. When observing the ground truth reward is costly, a key challenge for the learner is how to judiciously acquire the ground truth by assessing the benefits and costs in order to balance learning efficiency and learning cost. From the information theoretic perspective, a perhaps even more interesting question is how much efficiency might be lost due to this cost. In this paper, we design a novel contextual bandit-based learning algorithm and endow it with the active learning capability. The key feature of our algorithm is that in addition to sending a query to an annotator for the ground truth, prior information about the ground truth learned by the learner is sent together, thereby reducing the query cost. We prove that by carefully choosing the algorithm parameters, the learning regret of the proposed algorithm achieves the same order as that of conventional contextual bandit algorithms in cost-free scenarios, implying that, surprisingly, cost due to acquiring the ground truth does not increase the learning regret in the long-run. Our analysis shows that prior information about the ground truth plays a critical role in improving the system performance in scenarios where active learning is necessary. version:1
arxiv-1701-06715 | A graph cut approach to 3D tree delineation, using integrated airborne LiDAR and hyperspectral imagery | http://arxiv.org/abs/1701.06715 | id:1701.06715 author:Juheon Lee, David Coomes, Carola-Bibiane Schonlieb, Xiaohao Cai, Jan Lellmann, Michele Dalponte, Yadvinder Malhi, Nathalie Butt, Mike Morecroft category:cs.CV  published:2017-01-24 summary:Recognising individual trees within remotely sensed imagery has important applications in forest ecology and management. Several algorithms for tree delineation have been suggested, mostly based on locating local maxima or inverted basins in raster canopy height models (CHMs) derived from Light Detection And Ranging (LiDAR) data or photographs. However, these algorithms often lead to inaccurate estimates of forest stand characteristics due to the limited information content of raster CHMs. Here we develop a 3D tree delineation method which uses graph cut to delineate trees from the full 3D LiDAR point cloud, and also makes use of any optical imagery available (hyperspectral imagery in our case). First, conventional methods are used to locate local maxima in the CHM and generate an initial map of trees. Second, a graph is built from the LiDAR point cloud, fused with the hyperspectral data. For computational efficiency, the feature space of hyperspectral imagery is reduced using robust PCA. Third, a multi-class normalised cut is applied to the graph, using the initial map of trees to constrain the number of clusters and their locations. Finally, recursive normalised cut is used to subdivide, if necessary, each of the clusters identified by the initial analysis. We call this approach Multiclass Cut followed by Recursive Cut (MCRC). The effectiveness of MCRC was tested using three datasets: i) NewFor, ii) a coniferous forest in the Italian Alps, and iii) a deciduous woodland in the UK. The performance of MCRC was usually superior to that of other delineation methods, and was further improved by including high-resolution optical imagery. Since MCRC delineates the entire LiDAR point cloud in 3D, it allows individual crown characteristics to be measured. By making full use of the data available, graph cut has the potential to considerably improve the accuracy of tree delineation. version:1
arxiv-1701-06708 | Speech Map: A Statistical Multimodal Atlas of 4D Tongue Motion During Speech from Tagged and Cine MR Images | http://arxiv.org/abs/1701.06708 | id:1701.06708 author:Jonghye Woo, Fangxu Xing, Maureen Stone, Jordan Green, Timothy G. Reese, Thomas J. Brady, Van J. Wedeen, Jerry L. Prince, Georges El Fakhri category:cs.CV  published:2017-01-24 summary:Quantitative measurement of functional and anatomical traits of 4D tongue motion in the course of speech or other lingual behaviors remains a major challenge in scientific research and clinical applications. Here, we introduce a statistical multimodal atlas of 4D tongue motion using healthy subjects that enables a combined quantitative characterization of tongue motion in a reference anatomical configuration. This atlas framework, termed speech map, combines cine- and tagged-MRI in order to provide both the anatomic reference and motion information during speech. Our approach involves a series of steps including (1) construction of a common reference anatomical configuration from cine-MRI, (2) motion estimation from tagged-MRI, (3) transformation of the motion estimations to the reference anatomical configuration, (4) correction of potential time mismatch across subjects, and (5) computation of motion quantities such as Lagrangian strain. Using this framework, the anatomic configuration of the tongue appears motionless, while the motion fields and associated strain measurements change over the time course of speech. In addition, to form a succinct representation of the high-dimensional and complex motion fields, principal component analysis is carried out to characterize the central tendencies and variations of motion fields of our speech tasks. Our proposed method provides a platform to quantitatively and objectively explain the differences and variability of tongue motion by illuminating internal motion and strain that have so far been intractable. The findings are used to understand how tongue function for speech is limited by abnormal internal motion and strain in glossectomy patients. version:1
arxiv-1701-06675 | Dynamic Mortality Risk Predictions in Pediatric Critical Care Using Recurrent Neural Networks | http://arxiv.org/abs/1701.06675 | id:1701.06675 author:M Aczon, D Ledbetter, L Ho, A Gunny, A Flynn, J Williams, R Wetzel category:stat.ML cs.NE math.DS q-bio.QM  published:2017-01-23 summary:Viewing the trajectory of a patient as a dynamical system, a recurrent neural network was developed to learn the course of patient encounters in the Pediatric Intensive Care Unit (PICU) of a major tertiary care center. Data extracted from Electronic Medical Records (EMR) of about 12000 patients who were admitted to the PICU over a period of more than 10 years were leveraged. The RNN model ingests a sequence of measurements which include physiologic observations, laboratory results, administered drugs and interventions, and generates temporally dynamic predictions for in-ICU mortality at user-specified times. The RNN's ICU mortality predictions offer significant improvements over those from two clinically-used scores and static machine learning algorithms. version:1
arxiv-1701-06659 | DSSD : Deconvolutional Single Shot Detector | http://arxiv.org/abs/1701.06659 | id:1701.06659 author:Cheng-Yang Fu, Wei Liu, Ananth Ranga, Ambrish Tyagi, Alexander C. Berg category:cs.CV  published:2017-01-23 summary:The main contribution of this paper is an approach for introducing additional context into state-of-the-art general object detection. To achieve this we first combine a state-of-the-art classifier (Residual-101[14]) with a fast detection framework (SSD[18]). We then augment SSD+Residual-101 with deconvolution layers to introduce additional large-scale context in object detection and improve accuracy, especially for small objects, calling our resulting system DSSD for deconvolutional single shot detector. While these two contributions are easily described at a high-level, a naive implementation does not succeed. Instead we show that carefully adding additional stages of learned transformations, specifically a module for feed-forward connections in deconvolution and a new output module, enables this new approach and forms a potential way forward for further detection research. Results are shown on both PASCAL VOC and COCO detection. Our DSSD with $513 \times 513$ input achieves 81.5% mAP on VOC2007 test, 80.0% mAP on VOC2012 test, and 33.2% mAP on COCO, outperforming a state-of-the-art method R-FCN[3] on each dataset. version:1
arxiv-1701-06655 | Patchwork Kriging for Large-scale Gaussian Process Regression | http://arxiv.org/abs/1701.06655 | id:1701.06655 author:Chiwoo Park, Daniel Apley category:cs.LG stat.ML 68T01 G.3  published:2017-01-23 summary:This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This effectively solves the well-known discontinuity problem that degrades the boundary accuracy of existing local partitioned GP methods. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and two high dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches. version:1
arxiv-1701-06652 | Convex Parameterizations and Fidelity Bounds for Nonlinear Identification and Reduced-Order Modelling | http://arxiv.org/abs/1701.06652 | id:1701.06652 author:Mark M. Tobenkin, Ian R. Manchester, Alexandre Megretski category:cs.SY cs.LG math.OC  published:2017-01-23 summary:Model instability and poor prediction of long-term behavior are common problems when modeling dynamical systems using nonlinear "black-box" techniques. Direct optimization of the long-term predictions, often called simulation error minimization, leads to optimization problems that are generally non-convex in the model parameters and suffer from multiple local minima. In this work we present methods which address these problems through convex optimization, based on Lagrangian relaxation, dissipation inequalities, contraction theory, and semidefinite programming. We demonstrate the proposed methods with a model order reduction task for electronic circuit design and the identification of a pneumatic actuator from experiment. version:1
arxiv-1701-06649 | Constant Size Molecular Descriptors For Use With Machine Learning | http://arxiv.org/abs/1701.06649 | id:1701.06649 author:Christopher R. Collins, Geoffrey J. Gordon, O. Anatole von Lilienfeld, David J. Yaron category:physics.chem-ph stat.ML  published:2017-01-23 summary:A set of molecular descriptors whose length is independent of molecular size is developed for machine learning models that target thermodynamic and electronic properties of molecules. These features are evaluated by monitoring performance of kernel ridge regression models on well-studied data sets of small organic molecules. The features include connectivity counts, which require only the bonding pattern of the molecule, and encoded distances, which summarize distances between both bonded and non-bonded atoms and so require the full molecular geometry. In addition to having constant size, these features summarize information regarding the local environment of atoms and bonds, such that models can take advantage of similarities resulting from the presence of similar chemical fragments across molecules. Combining these two types of features leads to models whose performance is comparable to or better than the current state of the art. The features introduced here have the advantage of leading to models that may be trained on smaller molecules and then used successfully on larger molecules. version:1
arxiv-1701-06643 | Residual and Plain Convolutional Neural Networks for 3D Brain MRI Classification | http://arxiv.org/abs/1701.06643 | id:1701.06643 author:Sergey Korolev, Amir Safiullin, Mikhail Belyaev, Yulia Dodonova category:cs.CV  published:2017-01-23 summary:In the recent years there have been a number of studies that applied deep learning algorithms to neuroimaging data. Pipelines used in those studies mostly require multiple processing steps for feature extraction, although modern advancements in deep learning for image classification can provide a powerful framework for automatic feature generation and more straightforward analysis. In this paper, we show how similar performance can be achieved skipping these feature extraction steps with the residual and plain 3D convolutional neural network architectures. We demonstrate the performance of the proposed approach for classification of Alzheimer's disease versus mild cognitive impairment and normal controls on the Alzheimer's Disease National Initiative (ADNI) dataset of 3D structural MRI brain scans. version:1
arxiv-1701-06641 | Perceptually Optimized Image Rendering | http://arxiv.org/abs/1701.06641 | id:1701.06641 author:Valero Laparra, Alex Berardino, Johannes Ballé, Eero P. Simoncelli category:cs.CV cs.AI cs.GR  published:2017-01-23 summary:We develop a framework for rendering photographic images, taking into account display limitations, so as to optimize perceptual similarity between the rendered image and the original scene. We formulate this as a constrained optimization problem, in which we minimize a measure of perceptual dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics the early stage transformations of the human visual system. When rendering images acquired with higher dynamic range than that of the display, we find that the optimized solution boosts the contrast of low-contrast features without introducing significant artifacts, yielding results of comparable visual quality to current state-of-the art methods with no manual intervention or parameter settings. We also examine a variety of other display constraints, including limitations on minimum luminance (black point), mean luminance (as a proxy for energy consumption), and quantized luminance levels (halftoning). Finally, we show that the method may be used to enhance details and contrast of images degraded by optical scattering (e.g. fog). version:1
arxiv-1701-06607 | Stable Recovery Of Sparse Vectors From Random Sinusoidal Feature Maps | http://arxiv.org/abs/1701.06607 | id:1701.06607 author:Mohammadreza Soltani, Chinmay Hegde category:stat.ML  published:2017-01-23 summary:Random sinusoidal features are a popular approach for speeding up kernel-based inference in large datasets. Prior to the inference stage, the approach suggests performing dimensionality reduction by first multiplying each data vector by a random Gaussian matrix, and then computing an element-wise sinusoid. Theoretical analysis shows that collecting a sufficient number of such features can be reliably used for subsequent inference in kernel classification and regression. In this work, we demonstrate that with a mild increase in the dimension of the embedding, it is also possible to reconstruct the data vector from such random sinusoidal features, provided that the underlying data is sparse enough. In particular, we propose a numerically stable algorithm for reconstructing the data vector given the nonlinear features, and analyze its sample complexity. Our algorithm can be extended to other types of structured inverse problems, such as demixing a pair of sparse (but incoherent) vectors. We support the efficacy of our approach via numerical experiments. version:1
arxiv-1701-06605 | Identifying Nonlinear 1-Step Causal Influences in Presence of Latent Variables | http://arxiv.org/abs/1701.06605 | id:1701.06605 author:Saber Salehkaleybar, Jalal Etesami, Negar Kiyavash category:cs.IT cs.LG math.IT stat.ME  published:2017-01-23 summary:We propose an approach for learning the causal structure in stochastic dynamical systems with a $1$-step functional dependency in the presence of latent variables. We propose an information-theoretic approach that allows us to recover the causal relations among the observed variables as long as the latent variables evolve without exogenous noise. We further propose an efficient learning method based on linear regression for the special sub-case when the dynamics are restricted to be linear. We validate the performance of our approach via numerical simulations. version:1
arxiv-1701-06599 | Unsupervised Joint Mining of Deep Features and Image Labels for Large-scale Radiology Image Categorization and Scene Recognition | http://arxiv.org/abs/1701.06599 | id:1701.06599 author:Xiaosong Wang, Le Lu, Hoo-chang Shin, Lauren Kim, Mohammadhadi Bagheri, Isabella Nogues, Jianhua Yao, Ronald M. Summers category:cs.CV  published:2017-01-23 summary:The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of "Google Search" and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for joint mining of deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized "convergence" of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75.3%, compared to the state-of-the-art supervised classification accuracy of 81.0% (when both are based on the VGG-VD model). version:1
arxiv-1701-06597 | Iterative Thresholding for Demixing Structured Superpositions in High Dimensions | http://arxiv.org/abs/1701.06597 | id:1701.06597 author:Mohammadreza Soltani, Chinmay Hegde category:stat.ML  published:2017-01-23 summary:We consider the demixing problem of two (or more) high-dimensional vectors from nonlinear observations when the number of such observations is far less than the ambient dimension of the underlying vectors. Specifically, we demonstrate an algorithm that stably estimate the underlying components under general \emph{structured sparsity} assumptions on these components. Specifically, we show that for certain types of structured superposition models, our method provably recovers the components given merely $n = \mathcal{O}(s)$ samples where $s$ denotes the number of nonzero entries in the underlying components. Moreover, our method achieves a fast (linear) convergence rate, and also exhibits fast (near-linear) per-iteration complexity for certain types of structured models. We also provide a range of simulations to illustrate the performance of the proposed algorithm. version:1
arxiv-1109-2304 | Efficient Minimization of Higher Order Submodular Functions using Monotonic Boolean Functions | http://arxiv.org/abs/1109.2304 | id:1109.2304 author:Srikumar Ramalingam, Chris Russell, Lubor Ladicky, Philip H. S. Torr category:cs.DS cs.CV cs.DM  published:2011-09-11 summary:Submodular function minimization is a key problem in a wide variety of applications in machine learning, economics, game theory, computer vision, and many others. The general solver has a complexity of $O(n^3 \log^2 n . E +n^4 {\log}^{O(1)} n)$ where $E$ is the time required to evaluate the function and $n$ is the number of variables \cite{Lee2015}. On the other hand, many computer vision and machine learning problems are defined over special subclasses of submodular functions that can be written as the sum of many submodular cost functions defined over cliques containing few variables. In such functions, the pseudo-Boolean (or polynomial) representation \cite{BorosH02} of these subclasses are of degree (or order, or clique size) $k$ where $k \ll n$. In this work, we develop efficient algorithms for the minimization of this useful subclass of submodular functions. To do this, we define novel mapping that transform submodular functions of order $k$ into quadratic ones. The underlying idea is to use auxiliary variables to model the higher order terms and the transformation is found using a carefully constructed linear program. In particular, we model the auxiliary variables as monotonic Boolean functions, allowing us to obtain a compact transformation using as few auxiliary variables as possible. version:2
arxiv-1701-06548 | Regularizing Neural Networks by Penalizing Confident Output Distributions | http://arxiv.org/abs/1701.06548 | id:1701.06548 author:Gabriel Pereyra, George Tucker, Jan Chorowski, Łukasz Kaiser, Geoffrey Hinton category:cs.NE cs.LG  published:2017-01-23 summary:We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers. version:1
arxiv-1701-06538 | Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer | http://arxiv.org/abs/1701.06538 | id:1701.06538 author:Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean category:cs.LG cs.CL cs.NE stat.ML  published:2017-01-23 summary:The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost. version:1
arxiv-1701-06532 | ENIGMA: Efficient Learning-based Inference Guiding Machine | http://arxiv.org/abs/1701.06532 | id:1701.06532 author:Jan Jakubův, Josef Urban category:cs.LO cs.AI cs.LG  published:2017-01-23 summary:ENIGMA is a learning-based method for guiding given clause selection in saturation-based theorem provers. Clauses from many proof searches are classified as positive and negative based on their participation in the proofs. An efficient classification model is trained on this data, using fast feature-based characterization of the clauses . The learned model is then tightly linked with the core prover and used as a basis of a new parameterized evaluation heuristic that provides fast ranking of all generated clauses. The approach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing a large increase of E's performance. version:1
arxiv-1701-06521 | Incorporating Global Visual Features into Attention-Based Neural Machine Translation | http://arxiv.org/abs/1701.06521 | id:1701.06521 author:Iacer Calixto, Qun Liu, Nick Campbell category:cs.CL I.2.7  published:2017-01-23 summary:We introduce multi-modal, attention-based neural machine translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. We utilise global image features extracted using a pre-trained convolutional neural network and incorporate them (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments, we evaluate how these different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all metrics evaluated on this data set. version:1
arxiv-1701-06511 | Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification | http://arxiv.org/abs/1701.06511 | id:1701.06511 author:Bikash Joshi, Massih-Reza Amini, Ioannis Partalas, Franck Iutzeler, Yury Maximov category:stat.ML cs.LG  published:2017-01-23 summary:We address the problem of multiclass classification in the case where the number of classes is very large. We propose a multiclass to binary reduction strategy, in which we transform the original problem into a binary classification one over pairs of examples. We derive generalization bounds for the error of the classifier of pairs using local Rademacher complexity, and a double sampling strategy (in the terms of examples and classes) that speeds up the training phase while maintaining a very low memory usage. Experiments are carried for text classification on DMOZ and Wikipedia collections with up to 20,000 classes in order to show the efficiency of the proposed method. version:1
arxiv-1701-06508 | The Impact of Random Models on Clustering Similarity | http://arxiv.org/abs/1701.06508 | id:1701.06508 author:Alexander J Gates, Yong-Yeol Ahn category:stat.ML  published:2017-01-23 summary:Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, while multiple runs of K-means clustering returns clusterings with a fixed number of clusters, the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on results and argue that the choice should be carefully justified. version:1
arxiv-1701-06487 | Dirty Pixels: Optimizing Image Classification Architectures for Raw Sensor Data | http://arxiv.org/abs/1701.06487 | id:1701.06487 author:Steven Diamond, Vincent Sitzmann, Stephen Boyd, Gordon Wetzstein, Felix Heide category:cs.CV  published:2017-01-23 summary:Real-world sensors suffer from noise, blur, and other imperfections that make high-level computer vision tasks like scene segmentation, tracking, and scene understanding difficult. Making high-level computer vision networks robust is imperative for real-world applications like autonomous driving, robotics, and surveillance. We propose a novel end-to-end differentiable architecture for joint denoising, deblurring, and classification that makes classification robust to realistic noise and blur. The proposed architecture dramatically improves the accuracy of a classification network in low light and other challenging conditions, outperforming alternative approaches such as retraining the network on noisy and blurry images and preprocessing raw sensor inputs with conventional denoising and deblurring algorithms. The architecture learns denoising and deblurring pipelines optimized for classification whose outputs differ markedly from those of state-of-the-art denoising and deblurring methods, preserving fine detail at the cost of more noise and artifacts. Our results suggest that the best low-level image processing for computer vision is different from existing algorithms designed to produce visually pleasing images. The principles used to design the proposed architecture easily extend to other high-level computer vision tasks and image formation models, providing a general framework for integrating low-level and high-level image processing. version:1
arxiv-1701-06462 | Using Convolutional Neural Networks to Count Palm Trees in Satellite Images | http://arxiv.org/abs/1701.06462 | id:1701.06462 author:Eu Koon Cheang, Teik Koon Cheang, Yong Haur Tay category:cs.CV  published:2017-01-23 summary:In this paper we propose a supervised learning system for counting and localizing palm trees in high-resolution, panchromatic satellite imagery (40cm/pixel to 1.5m/pixel). A convolutional neural network classifier trained on a set of palm and no-palm images is applied across a satellite image scene in a sliding window fashion. The resultant confidence map is smoothed with a uniform filter. A non-maximal suppression is applied onto the smoothed confidence map to obtain peaks. Trained with a small dataset of 500 images of size 40x40 cropped from satellite images, the system manages to achieve a tree count accuracy of over 99%. version:1
arxiv-1701-06452 | Learning what to look in chest X-rays with a recurrent visual attention model | http://arxiv.org/abs/1701.06452 | id:1701.06452 author:Petros-Pavlos Ypsilantis, Giovanni Montana category:stat.ML cs.CV cs.LG  published:2017-01-23 summary:X-rays are commonly performed imaging tests that use small amounts of radiation to produce pictures of the organs, tissues, and bones of the body. X-rays of the chest are used to detect abnormalities or diseases of the airways, blood vessels, bones, heart, and lungs. In this work we present a stochastic attention-based model that is capable of learning what regions within a chest X-ray scan should be visually explored in order to conclude that the scan contains a specific radiological abnormality. The proposed model is a recurrent neural network (RNN) that learns to sequentially sample the entire X-ray and focus only on informative areas that are likely to contain the relevant information. We report on experiments carried out with more than $100,000$ X-rays containing enlarged hearts or medical devices. The model has been trained using reinforcement learning methods to learn task-specific policies. version:1
arxiv-1701-06450 | Identification of Unmodeled Objects from Symbolic Descriptions | http://arxiv.org/abs/1701.06450 | id:1701.06450 author:Andrea Baisero, Stefan Otte, Peter Englert, Marc Toussaint category:stat.ML cs.AI  published:2017-01-23 summary:Successful human-robot cooperation hinges on each agent's ability to process and exchange information about the shared environment and the task at hand. Human communication is primarily based on symbolic abstractions of object properties, rather than precise quantitative measures. A comprehensive robotic framework thus requires an integrated communication module which is able to establish a link and convert between perceptual and abstract information. The ability to interpret composite symbolic descriptions enables an autonomous agent to a) operate in unstructured and cluttered environments, in tasks which involve unmodeled or never seen before objects; and b) exploit the aggregation of multiple symbolic properties as an instance of ensemble learning, to improve identification performance even when the individual predicates encode generic information or are imprecisely grounded. We propose a discriminative probabilistic model which interprets symbolic descriptions to identify the referent object contextually w.r.t.\ the structure of the environment and other objects. The model is trained using a collected dataset of identifications, and its performance is evaluated by quantitative measures and a live demo developed on the PR2 robot platform, which integrates elements of perception, object extraction, object identification and grasping. version:1
arxiv-1701-06439 | Segmentation-free Vehicle License Plate Recognition using ConvNet-RNN | http://arxiv.org/abs/1701.06439 | id:1701.06439 author:Teik Koon Cheang, Yong Shean Chong, Yong Haur Tay category:cs.CV  published:2017-01-23 summary:While vehicle license plate recognition (VLPR) is usually done with a sliding window approach, it can have limited performance on datasets with characters that are of variable width. This can be solved by hand-crafting algorithms to prescale the characters. While this approach can work fairly well, the recognizer is only aware of the pixels within each detector window, and fails to account for other contextual information that might be present in other parts of the image. A sliding window approach also requires training data in the form of presegmented characters, which can be more difficult to obtain. In this paper, we propose a unified ConvNet-RNN model to recognize real-world captured license plate photographs. By using a Convolutional Neural Network (ConvNet) to perform feature extraction and using a Recurrent Neural Network (RNN) for sequencing, we address the problem of sliding window approaches being unable to access the context of the entire image by feeding the entire image as input to the ConvNet. This has the added benefit of being able to perform end-to-end training of the entire model on labelled, full license plate images. Experimental results comparing the ConvNet-RNN architecture to a sliding window-based approach shows that the ConvNet-RNN architecture performs significantly better. version:1
arxiv-1701-06421 | Comparative study on supervised learning methods for identifying phytoplankton species | http://arxiv.org/abs/1701.06421 | id:1701.06421 author:Thi-Thu-Hong Phan, Emilie Poisson Caillault, André Bigand category:stat.ML cs.LG  published:2017-01-23 summary:Phytoplankton plays an important role in marine ecosystem. It is defined as a biological factor to assess marine quality. The identification of phytoplankton species has a high potential for monitoring environmental, climate changes and for evaluating water quality. However, phytoplankton species identification is not an easy task owing to their variability and ambiguity due to thousands of micro and pico-plankton species. Therefore, the aim of this paper is to build a framework for identifying phytoplankton species and to perform a comparison on different features types and classifiers. We propose a new features type extracted from raw signals of phytoplankton species. We then analyze the performance of various classifiers on the proposed features type as well as two other features types for finding the robust one. Through experiments, it is found that Random Forest using the proposed features gives the best classification results with average accuracy up to 98.24%. version:1
arxiv-1701-06393 | Nonsmooth Analysis and Subgradient Methods for Averaging in Dynamic Time Warping Spaces | http://arxiv.org/abs/1701.06393 | id:1701.06393 author:David Schultz, Brijnesh Jain category:cs.CV  published:2017-01-23 summary:Time series averaging in dynamic time warping (DTW) spaces has been successfully applied to improve pattern recognition systems. This article proposes and analyzes subgradient methods for the problem of finding a sample mean in DTW spaces. The class of subgradient methods generalizes existing sample mean algorithms such as DTW Barycenter Averaging (DBA). We show that DBA is a majorize-minimize algorithm that converges to necessary conditions of optimality after finitely many iterations. Empirical results show that for increasing sample sizes the proposed stochastic subgradient (SSG) algorithm is more stable and finds better solutions in shorter time than the DBA algorithm on average. Therefore, SSG is useful in online settings and for non-small sample sizes. The theoretical and empirical results open new paths for devising sample mean algorithms: nonsmooth optimization methods and modified variants of pairwise averaging methods. version:1
arxiv-1701-04869 | 3D Morphology Prediction of Progressive Spinal Deformities from Probabilistic Modeling of Discriminant Manifolds | http://arxiv.org/abs/1701.04869 | id:1701.04869 author:Samuel Kadoury, William Mandel, Marjolaine Roy-Beaudry, Marie-Lyne Nault, Stefan Parent category:cs.LG stat.ML  published:2017-01-17 summary:We introduce a novel approach for predicting the progression of adolescent idiopathic scoliosis from 3D spine models reconstructed from biplanar X-ray images. Recent progress in machine learning have allowed to improve classification and prognosis rates, but lack a probabilistic framework to measure uncertainty in the data. We propose a discriminative probabilistic manifold embedding where locally linear mappings transform data points from high-dimensional space to corresponding low-dimensional coordinates. A discriminant adjacency matrix is constructed to maximize the separation between progressive and non-progressive groups of patients diagnosed with scoliosis, while minimizing the distance in latent variables belonging to the same class. To predict the evolution of deformation, a baseline reconstruction is projected onto the manifold, from which a spatiotemporal regression model is built from parallel transport curves inferred from neighboring exemplars. Rate of progression is modulated from the spine flexibility and curve magnitude of the 3D spine deformation. The method was tested on 745 reconstructions from 133 subjects using longitudinal 3D reconstructions of the spine, with results demonstrating the discriminatory framework can identify between progressive and non-progressive of scoliotic patients with a classification rate of 81% and prediction differences of 2.1$^{o}$ in main curve angulation, outperforming other manifold learning methods. Our method achieved a higher prediction accuracy and improved the modeling of spatiotemporal morphological changes in highly deformed spines compared to other learning methods. version:2
arxiv-1701-06351 | Person Re-Identification via Recurrent Feature Aggregation | http://arxiv.org/abs/1701.06351 | id:1701.06351 author:Yichao Yan, Bingbing Ni, Zhichao Song, Chao Ma, Yan Yan, Xiaokang Yang category:cs.CV  published:2017-01-23 summary:We address the person re-identification problem by effectively exploiting a globally discriminative feature representation from a sequence of tracked human regions/patches. This is in contrast to previous person re-id works, which rely on either single frame based person to person patch matching, or graph based sequence to sequence matching. We show that a progressive/sequential fusion framework based on long short term memory (LSTM) network aggregates the frame-wise human region representation at each time stamp and yields a sequence level human feature representation. Since LSTM nodes can remember and propagate previously accumulated good features and forget newly input inferior ones, even with simple hand-crafted features, the proposed recurrent feature aggregation network (RFA-Net) is effective in generating highly discriminative sequence level human representations. Extensive experimental results on two person re-identification benchmarks demonstrate that the proposed method performs favorably against state-of-the-art person re-identification methods. version:1
arxiv-1702-02092 | Characterisation of speech diversity using self-organising maps | http://arxiv.org/abs/1702.02092 | id:1702.02092 author:Tom A. F. Anderson, David M. W. Powers category:cs.CL cs.NE cs.SD  published:2017-01-23 summary:We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter is a semi-supervised method comprised of self-organising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In this paper, we report a method to evaluate pronunciation using multilevel SOMs with /hVd/ single syllable utterances for the study of vowels, for Australian pronunciation. version:1
arxiv-1701-06279 | dna2vec: Consistent vector representations of variable-length k-mers | http://arxiv.org/abs/1701.06279 | id:1701.06279 author:Patrick Ng category:q-bio.QM cs.CL cs.LG stat.ML  published:2017-01-23 summary:One of the ubiquitous representation of long DNA sequence is dividing it into shorter k-mer components. Unfortunately, the straightforward vector encoding of k-mer as a one-hot vector is vulnerable to the curse of dimensionality. Worse yet, the distance between any pair of one-hot vectors is equidistant. This is particularly problematic when applying the latest machine learning algorithms to solve problems in biological sequence analysis. In this paper, we propose a novel method to train distributed representations of variable-length k-mers. Our method is based on the popular word embedding model word2vec, which is trained on a shallow two-layer neural network. Our experiments provide evidence that the summing of dna2vec vectors is akin to nucleotides concatenation. We also demonstrate that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec vectors. version:1
arxiv-1701-05652 | Dual Recovery Network with Online Compensation for Image Super-Resolution | http://arxiv.org/abs/1701.05652 | id:1701.05652 author:Sifeng Xia, Wenhan Yang, Tiesong Zhao, Jiaying Liu category:cs.CV  published:2017-01-20 summary:The image super-resolution (SR) methods will essentially lead to a loss of some high-frequency (HF) information when predicting high-resolution (HR) images from low-resolution (LR) images without using external references. To address that, we additionally utilize online retrieved data to facilitate image SR in a unified deep framework. A novel dual high-frequency recovery network (DHN) is proposed to predict an HR image with three parts: an LR image, an internal inferred HF (IHF) map (HF missing part inferred solely from the LR image) and an external extracted HF (EHF) map. In particular, we infer the HF information based on both the LR image and similar HR references which is retrieved online. For the EHF map, we align the references with affine transformation and then in the aligned references, part of HF signals are extracted by the proposed DHN to compensate for the HF loss. Extensive experimental results demonstrate that our DHN achieves notably better performance than state-of-the-art SR methods. version:2
arxiv-1701-06247 | A Multichannel Convolutional Neural Network For Cross-language Dialog State Tracking | http://arxiv.org/abs/1701.06247 | id:1701.06247 author:Hongjie Shi, Takashi Ushio, Mitsuru Endo, Katsuyoshi Yamagami, Noriaki Horii category:cs.CL cs.AI cs.LG  published:2017-01-23 summary:The fifth Dialog State Tracking Challenge (DSTC5) introduces a new cross-language dialog state tracking scenario, where the participants are asked to build their trackers based on the English training corpus, while evaluating them with the unlabeled Chinese corpus. Although the computer-generated translations for both English and Chinese corpus are provided in the dataset, these translations contain errors and careless use of them can easily hurt the performance of the built trackers. To address this problem, we propose a multichannel Convolutional Neural Networks (CNN) architecture, in which we treat English and Chinese language as different input channels of one single CNN model. In the evaluation of DSTC5, we found that such multichannel architecture can effectively improve the robustness against translation errors. Additionally, our method for DSTC5 is purely machine learning based and requires no prior knowledge about the target language. We consider this a desirable property for building a tracker in the cross-language context, as not every developer will be familiar with both languages. version:1
arxiv-1701-06233 | What the Language You Tweet Says About Your Occupation | http://arxiv.org/abs/1701.06233 | id:1701.06233 author:Tianran Hu, Haoyuan Xiao, Thuy-vy Thi Nguyen, Jiebo Luo category:cs.CY cs.AI cs.CL cs.LG  published:2017-01-22 summary:Many aspects of people's lives are proven to be deeply connected to their jobs. In this paper, we first investigate the distinct characteristics of major occupation categories based on tweets. From multiple social media platforms, we gather several types of user information. From users' LinkedIn webpages, we learn their proficiencies. To overcome the ambiguity of self-reported information, a soft clustering approach is applied to extract occupations from crowd-sourced data. Eight job categories are extracted, including Marketing, Administrator, Start-up, Editor, Software Engineer, Public Relation, Office Clerk, and Designer. Meanwhile, users' posts on Twitter provide cues for understanding their linguistic styles, interests, and personalities. Our results suggest that people of different jobs have unique tendencies in certain language styles and interests. Our results also clearly reveal distinctive levels in terms of Big Five Traits for different jobs. Finally, a classifier is built to predict job types based on the features extracted from tweets. A high accuracy indicates a strong discrimination power of language features for job prediction task. version:1
arxiv-1701-06225 | Predicting Demographics of High-Resolution Geographies with Geotagged Tweets | http://arxiv.org/abs/1701.06225 | id:1701.06225 author:Omar Montasser, Daniel Kifer category:cs.LG cs.SI stat.ML  published:2017-01-22 summary:In this paper, we consider the problem of predicting demographics of geographic units given geotagged Tweets that are composed within these units. Traditional survey methods that offer demographics estimates are usually limited in terms of geographic resolution, geographic boundaries, and time intervals. Thus, it would be highly useful to develop computational methods that can complement traditional survey methods by offering demographics estimates at finer geographic resolutions, with flexible geographic boundaries (i.e. not confined to administrative boundaries), and at different time intervals. While prior work has focused on predicting demographics and health statistics at relatively coarse geographic resolutions such as the county-level or state-level, we introduce an approach to predict demographics at finer geographic resolutions such as the blockgroup-level. For the task of predicting gender and race/ethnicity counts at the blockgroup-level, an approach adapted from prior work to our problem achieves an average correlation of 0.389 (gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms this prior approach with an average correlation of 0.671 (gender) and 0.692 (race). version:1
arxiv-1701-06190 | A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction | http://arxiv.org/abs/1701.06190 | id:1701.06190 author:Yoonsik Kim, Insung Hwang, Nam Ik Cho category:cs.CV  published:2017-01-22 summary:The inception network has been shown to provide good performance on image classification problems, but there are not much evidences that it is also effective for the image restoration or pixel-wise labeling problems. For image restoration problems, the pooling is generally not used because the decimated features are not helpful for the reconstruction of an image as the output. Moreover, most deep learning architectures for the restoration problems do not use dense prediction that need lots of training parameters. From these observations, for enjoying the performance of inception-like structure on the image based problems we propose a new convolutional network-in-network structure. The proposed network can be considered a modification of inception structure where pool projection and pooling layer are removed for maintaining the entire feature map size, and a larger kernel filter is added instead. Proposed network greatly reduces the number of parameters on account of removed dense prediction and pooling, which is an advantage, but may also reduce the receptive field in each layer. Hence, we add a larger kernel than the original inception structure for not increasing the depth of layers. The proposed structure is applied to typical image-to-image learning problems, i.e., the problems where the size of input and output are same such as skin detection, semantic segmentation, and compression artifacts reduction. Extensive experiments show that the proposed network brings comparable or better results than the state-of-the-art convolutional neural networks for these problems. version:1
arxiv-1701-06183 | Image Compression with SVD : A New Quality Metric Based On Energy Ratio | http://arxiv.org/abs/1701.06183 | id:1701.06183 author:Henri Bruno Razafindradina, Paul Auguste Randriamitantsoa, Nicolas Raft Razafindrakoto category:cs.CV  published:2017-01-22 summary:Digital image compression is a technique that allows to reduce the size of an image in order to increase the capacity storage devices and to optimize the use of network bandwidth. The quality of compressed images with the techniques based on the discrete cosine transform or the wavelet transform is generally measured with PSNR or SSIM. Theses metrics are not suitable to images compressed with the singular values decomposition. This paper presents a new metric based on the energy ratio to measure the quality of the images coded with the SVD. A series of tests on 512 * 512 pixels images show that, for a rank k = 40 corresponding to a SSIM = 0,94 or PSNR = 35 dB, 99,9% of the energy are restored. Three areas of image quality assessments were identified. This new metric is also very accurate and could overcome the weaknesses of PSNR and SSIM. version:1
arxiv-1701-06171 | Greedy Compositional Clustering for Unsupervised Learning of Hierarchical Compositional Models | http://arxiv.org/abs/1701.06171 | id:1701.06171 author:Adam Kortylewski, Clemens Blumer, Thomas Vetter category:cs.CV  published:2017-01-22 summary:This paper proposes to integrate a feature pursuit learning process into a greedy bottom-up learning scheme. The algorithm combines the benefits of bottom-up and top-down approaches for learning hierarchical models: It allows to induce the hierarchical structure of objects in an unsupervised manner, while avoiding a hard decision on the activation of parts. We follow the principle of compositionality by assembling higher-order parts from elements of lower layers in the hierarchy. The parts are learned greedily with an EM-type process that iterates between image encoding and part re-learning. The process stops when a candidate part is not able to find a free niche in the image. The algorithm proceeds layer by layer in a bottom-up manner until no further compositions are found. A subsequent top-down process composes the learned hierarchical shape vocabulary into a holistic object model. Experimental evaluation of the approach shows state-of-the-art performance on a domain adaptation task. Moreover, we demonstrate the capability of learning complex, semantically meaningful hierarchical compositional models without supervision. version:1
arxiv-1701-07046 | Large Scale Novel Object Discovery in 3D | http://arxiv.org/abs/1701.07046 | id:1701.07046 author:Siddharth Srivastava, Gaurav Sharma, Brejesh Lall category:cs.CV  published:2017-01-22 summary:We present a method for discovering objects in 3D point clouds from sensors like Microsoft Kinect. We utilize supervoxels generated directly from the point cloud data and design a Siamese network building on a recently proposed 3D convolutional neural network architecture. At training, we assume the availability of the some known objects---these are used to train a non-linear embedding of supervoxels using the Siamese network, by optimizing the criteria that supervoxels which fall on the same object should be closer than those which fall on different objects, in the embedding space. We do not assume the objects during test to be known, and perform clustering, in the embedding space learned, of supervoxels to effectively perform novel object discovery. We validate the method with quantitative results showing that it can discover numerous unseen objects while being trained on only a few dense 3D models. We also show convincing qualitative results of object discovery in point cloud data when the test objects, either specific instances or even their categories, were never seen during training. version:1
arxiv-1701-06141 | Perception-based energy functions in seam-cutting | http://arxiv.org/abs/1701.06141 | id:1701.06141 author:Nan Li, Tianli Liao, Chao Wang category:cs.CV  published:2017-01-22 summary:Image stitching is challenging in consumer-level photography, due to alignment difficulties in unconstrained shooting environment. Recent studies show that seam-cutting approaches can effectively relieve artifacts generated by local misalignment. Normally, seam-cutting is described in terms of energy minimization, however, few of existing methods consider human perception in their energy functions, which sometimes causes that a seam with minimum energy is not most invisible in the overlapping region. In this paper, we propose a novel perception-based energy function in the seam-cutting framework, which considers the nonlinearity and the nonuniformity of human perception in energy minimization. Our perception-based approach adopts a sigmoid metric to characterize the perception of color discrimination, and a saliency weight to simulate that human eyes incline to pay more attention to salient objects. In addition, our seam-cutting composition can be easily implemented into other stitching pipelines. Experiments show that our method outperforms the seam-cutting method of the normal energy function, and a user study demonstrates that our composed results are more consistent with human perception. version:1
arxiv-1701-06123 | Optimization on Product Submanifolds of Convolution Kernels | http://arxiv.org/abs/1701.06123 | id:1701.06123 author:Mete Ozay, Takayuki Okatani category:cs.CV cs.LG cs.NE  published:2017-01-22 summary:We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). First, we explore metric and curvature properties of PEMs in terms of component manifolds. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of the C-SGD considering sectional curvature properties of PEMs. In the theoretical results, we expound the constraints that can be used to compute adaptive step sizes of the C-SGD in order to assure the asymptotic convergence. version:1
arxiv-1701-06121 | Multimodal Fusion via a Series of Transfers for Noise Removal | http://arxiv.org/abs/1701.06121 | id:1701.06121 author:Chang-Hwan Son, Xiao-Ping Zhang category:cs.CV  published:2017-01-22 summary:Near-infrared imaging has been considered as a solution to provide high quality photographs in dim lighting conditions. This imaging system captures two types of multimodal images: one is near-infrared gray image (NGI) and the other is the visible color image (VCI). NGI is noise-free but it is grayscale, whereas the VCI has colors but it contains noise. Moreover, there exist serious edge and brightness discrepancies between NGI and VCI. To deal with this problem, a new transfer-based fusion method is proposed for noise removal. Different from conventional fusion approaches, the proposed method conducts a series of transfers: contrast, detail, and color transfers. First, the proposed contrast and detail transfers aim at solving the serious discrepancy problem, thereby creating a new noise-free and detail-preserving NGI. Second, the proposed color transfer models the unknown colors from the denoised VCI via a linear transform, and then transfers natural-looking colors into the newly generated NGI. Experimental results show that the proposed transfer-based fusion method is highly successful in solving the discrepancy problem, thereby describing edges and textures clearly as well as removing noise completely on the fused images. Most of all, the proposed method is superior to conventional fusion methods and guided filtering, and even the state-of-the-art fusion methods based on scale map and layer decomposition. version:1
arxiv-1701-06120 | Effective and Extensible Feature Extraction Method Using Genetic Algorithm-Based Frequency-Domain Feature Search for Epileptic EEG Multi-classification | http://arxiv.org/abs/1701.06120 | id:1701.06120 author:Tingxi Wen, Zhongnan Zhang category:cs.LG cs.IT math.IT stat.ML  published:2017-01-22 summary:In this paper, a genetic algorithm-based frequency-domain feature search (GAFDS) method is proposed for the electroencephalogram (EEG) analysis of epilepsy. In this method, frequency-domain features are first searched and then combined with nonlinear features. Subsequently, these features are selected and optimized to classify EEG signals. The extracted features are analyzed experimentally. The features extracted by GAFDS show remarkable independence, and they are superior to the nonlinear features in terms of the ratio of inter-class distance and intra-class distance. Moreover, the proposed feature search method can additionally search for features of instantaneous frequency in a signal after Hilbert transformation. The classification results achieved using these features are reasonable, thus, GAFDS exhibits good extensibility. Multiple classic classifiers (i.e., $k$-nearest neighbor, linear discriminant analysis, decision tree, AdaBoost, multilayer perceptron, and Na\"ive Bayes) achieve good results by using the features generated by GAFDS method and the optimized selection. Specifically, the accuracies for the two-classification and three-classification problems may reach up to 99% and 97%, respectively. Results of several cross-validation experiments illustrate that GAFDS is effective in feature extraction for EEG classification. Therefore, the proposed feature selection and optimization model can improve classification accuracy. version:1
arxiv-1701-06109 | DeadNet: Identifying Phototoxicity from Label-free Microscopy Images of Cells using Deep ConvNets | http://arxiv.org/abs/1701.06109 | id:1701.06109 author:David Richmond, Anna Payne-Tobin Jost, Talley Lambert, Jennifer Waters, Hunter Elliott category:cs.CV q-bio.QM  published:2017-01-22 summary:Exposure to intense illumination light is an unavoidable consequence of fluorescence microscopy, and poses a risk to the health of the sample in every live-cell fluorescence microscopy experiment. Furthermore, the possible side-effects of phototoxicity on the scientific conclusions that are drawn from an imaging experiment are often unaccounted for. Previously, controlling for phototoxicity in imaging experiments required additional labels and experiments, limiting its widespread application. Here we provide a proof-of-principle demonstration that the phototoxic effects of an imaging experiment can be identified directly from a single phase-contrast image using deep convolutional neural networks (ConvNets). This lays the groundwork for an automated tool for assessing cell health in a wide range of imaging experiments. Interpretability of such a method is crucial for its adoption. We take steps towards interpreting the classification mechanism of the trained ConvNet by visualizing salient features of images that contribute to accurate classification. version:1
arxiv-1701-06106 | Neurogenesis-Inspired Dictionary Learning: Online Model Adaption in a Changing World | http://arxiv.org/abs/1701.06106 | id:1701.06106 author:Sahil Garg, Irina Rish, Guillermo Cecchi, Aurelie Lozano category:cs.LG stat.ML  published:2017-01-22 summary:In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the neuronal-birth is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements. version:1
arxiv-1701-06075 | Label Propagation on K-partite Graphs with Heterophily | http://arxiv.org/abs/1701.06075 | id:1701.06075 author:Dingxiong Deng, Fan Bai, Yiqi Tang, Shuigeng Zhou, Cyrus Shahabi, Linhong Zhu category:cs.LG cs.AI cs.SI  published:2017-01-21 summary:In this paper, for the first time, we study label propagation in heterogeneous graphs under heterophily assumption. Homophily label propagation (i.e., two connected nodes share similar labels) in homogeneous graph (with same types of vertices and relations) has been extensively studied before. Unfortunately, real-life networks are heterogeneous, they contain different types of vertices (e.g., users, images, texts) and relations (e.g., friendships, co-tagging) and allow for each node to propagate both the same and opposite copy of labels to its neighbors. We propose a $\mathcal{K}$-partite label propagation model to handle the mystifying combination of heterogeneous nodes/relations and heterophily propagation. With this model, we develop a novel label inference algorithm framework with update rules in near-linear time complexity. Since real networks change over time, we devise an incremental approach, which supports fast updates for both new data and evidence (e.g., ground truth labels) with guaranteed efficiency. We further provide a utility function to automatically determine whether an incremental or a re-modeling approach is favored. Extensive experiments on real datasets have verified the effectiveness and efficiency of our approach, and its superiority over the state-of-the-art label propagation methods. version:1
arxiv-1701-05954 | Learning Policies for Markov Decision Processes from Data | http://arxiv.org/abs/1701.05954 | id:1701.05954 author:Manjesh K. Hanawal, Hao Liu, Henghui Zhu, Ioannis Ch. Paschalidis category:math.OC cs.LG stat.ML I.2.8; I.2.9; I.2.6  published:2017-01-21 summary:We consider the problem of learning a policy for a Markov decision process consistent with data captured on the state-actions pairs followed by the policy. We assume that the policy belongs to a class of parameterized policies which are defined using features associated with the state-action pairs. The features are known a priori, however, only an unknown subset of them could be relevant. The policy parameters that correspond to an observed target policy are recovered using $\ell_1$-regularized logistic regression that best fits the observed state-action samples. We establish bounds on the difference between the average reward of the estimated and the original policy (regret) in terms of the generalization error and the ergodic coefficient of the underlying Markov chain. To that end, we combine sample complexity theory and sensitivity analysis of the stationary distribution of Markov chains. Our analysis suggests that to achieve regret within order $O(\sqrt{\epsilon})$, it suffices to use training sample size on the order of $\Omega(\log n \cdot poly(1/\epsilon))$, where $n$ is the number of the features. We demonstrate the effectiveness of our method on a synthetic robot navigation example. version:1
arxiv-1701-05936 | The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R | http://arxiv.org/abs/1701.05936 | id:1701.05936 author:Yaohui Zeng, Patrick Breheny category:stat.CO stat.ML  published:2017-01-20 summary:Penalized regression models such as the lasso have been extensively applied to analyzing high-dimensional data sets. However, due to memory limitations, existing R packages like glmnet and ncvreg are not capable of fitting lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are increasingly seen in many areas such as genetics, genomics, biomedical imaging, and high-frequency finance. In this research, we implement an R package called biglasso that tackles this challenge. biglasso utilizes memory-mapped files to store the massive data on the disk, only reading data into memory when necessary during model fitting, and is thus able to handle out-of-core computation seamlessly. Moreover, it's equipped with newly proposed, more efficient feature screening rules, which substantially accelerate the computation. Benchmarking experiments show that our biglasso package, as compared to existing popular ones like glmnet, is much more memory- and computation-efficient. We further analyze a 31 GB real data set on a laptop with only 16 GB RAM to demonstrate the out-of-core computation capability of biglasso in analyzing massive data sets that cannot be accommodated by existing R packages. version:1
arxiv-1701-05935 | Integration of Preferences in Decomposition Multi-Objective Optimization | http://arxiv.org/abs/1701.05935 | id:1701.05935 author:Ke Li, Kalyanmoy Deb, Xin Yao category:cs.NE  published:2017-01-20 summary:Most existing studies on evolutionary multi-objective optimization focus on approximating the whole Pareto-optimal front. Nevertheless, rather than the whole front, which demands for too many points (especially in a high-dimensional space), the decision maker might only interest in a partial region, called the region of interest. In this case, solutions outside this region can be noisy to the decision making procedure. Even worse, there is no guarantee that we can find the preferred solutions when tackling problems with complicated properties or a large number of objectives. In this paper, we develop a systematic way to incorporate the decision maker's preference information into the decomposition-based evolutionary multi-objective optimization methods. Generally speaking, our basic idea is a non-uniform mapping scheme by which the originally uniformly distributed reference points on a canonical simplex can be mapped to the new positions close to the aspiration level vector specified by the decision maker. By these means, we are able to steer the search process towards the region of interest either directly or in an interactive manner and also handle a large number of objectives. In the meanwhile, the boundary solutions can be approximated given the decision maker's requirements. Furthermore, the extent of the region of the interest is intuitively understandable and controllable in a closed form. Extensive experiments, both proof-of-principle and on a variety of problems with 3 to 10 objectives, fully demonstrate the effectiveness of our proposed method for approximating the preferred solutions in the region of interest. version:1
arxiv-1701-05927 | Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis | http://arxiv.org/abs/1701.05927 | id:1701.05927 author:Luke de Oliveira, Michela Paganini, Benjamin Nachman category:stat.ML hep-ex physics.data-an  published:2017-01-20 summary:We provide a bridge between generative modeling in the Machine Learning community and simulated physical processes in High Energy Particle Physics by applying a novel Generative Adversarial Network (GAN) architecture to the production of jet images -- 2D representations of energy depositions from particles interacting with a calorimeter. We propose a simple architecture, the Location-Aware Generative Adversarial Network, that learns to produce realistic radiation patterns from simulated high energy particle collisions. The pixel intensities of GAN-generated images faithfully span over many orders of magnitude and exhibit the desired low-dimensional physical properties (i.e., jet mass, n-subjettiness, etc.). We shed light on limitations, and provide a novel empirical validation of image quality and validity of GAN-produced simulations of the natural world. This work provides a base for further explorations of GANs for use in faster simulation in High Energy Particle Physics. version:1
arxiv-1701-05923 | Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks | http://arxiv.org/abs/1701.05923 | id:1701.05923 author:Rahul Dey, Fathi M. Salem category:cs.NE stat.ML  published:2017-01-20 summary:The paper evaluates three variants of the Gated Recurrent Unit (GRU) in recurrent neural networks (RNN) by reducing parameters in the update and reset gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and show that these GRU-RNN variant models perform as well as the original GRU RNN model while reducing the computational expense. version:1
arxiv-1701-05847 | End-To-End Visual Speech Recognition With LSTMs | http://arxiv.org/abs/1701.05847 | id:1701.05847 author:Stavros Petridis, Zuwei Li, Maja Pantic category:cs.CV cs.CL  published:2017-01-20 summary:Traditional visual speech recognition systems consist of two stages, feature extraction and classification. Recently, several deep learning approaches have been presented which automatically extract features from the mouth images and aim to replace the feature extraction stage. However, research on joint learning of features and classification is very limited. In this work, we present an end-to-end visual speech recognition system based on Long-Short Memory (LSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and perform classification and also achieves state-of-the-art performance in visual speech classification. The model consists of two streams which extract features directly from the mouth and difference images, respectively. The temporal dynamics in each stream are modelled by an LSTM and the fusion of the two streams takes place via a Bidirectional LSTM (BLSTM). An absolute improvement of 9.7% over the base line is reported on the OuluVS2 database, and 1.5% on the CUAVE database when compared with other methods which use a similar visual front-end. version:1
arxiv-1701-07395 | Case Study of a highly automated Layout Analysis and OCR of an incunabulum: 'Der Heiligen Leben' (1488) | http://arxiv.org/abs/1701.07395 | id:1701.07395 author:Christian Reul, Marco Dittrich, Martin Gruner category:cs.CV  published:2017-01-20 summary:This paper provides the first thorough documentation of a high quality digitization process applied to an early printed book from the incunabulum period (1450-1500). The entire OCR related workflow including preprocessing, layout analysis and text recognition is illustrated in detail using the example of 'Der Heiligen Leben', printed in Nuremberg in 1488. For each step the required time expenditure was recorded. The character recognition yielded excellent results both on character (97.57%) and word (92.19%) level. Furthermore, a comparison of a highly automated (LAREX) and a manual (Aletheia) method for layout analysis was performed. By considerably automating the segmentation the required human effort was reduced significantly from over 100 hours to less than six hours, resulting in only a slight drop in OCR accuracy. Realistic estimates for the human effort necessary for full text extraction from incunabula can be derived from this study. The printed pages of the complete work together with the OCR result is available online ready to be inspected and downloaded. version:1
arxiv-1701-05818 | Fusion of Heterogeneous Data in Convolutional Networks for Urban Semantic Labeling (Invited Paper) | http://arxiv.org/abs/1701.05818 | id:1701.05818 author:Nicolas Audebert, Bertrand Le Saux, Sébastien Lefèvre category:cs.NE cs.CV  published:2017-01-20 summary:In this work, we present a novel module to perform fusion of heterogeneous data using fully convolutional networks for semantic labeling. We introduce residual correction as a way to learn how to fuse predictions coming out of a dual stream architecture. Especially, we perform fusion of DSM and IRRG optical data on the ISPRS Vaihingen dataset over a urban area and obtain new state-of-the-art results. version:1
arxiv-1701-05804 | Detectability thresholds in networks with dynamic link and community structure | http://arxiv.org/abs/1701.05804 | id:1701.05804 author:Paolo Barucca, Fabrizio Lillo, Piero Mazzarisi, Daniele Tantari category:cs.SI cs.LG physics.soc-ph stat.ML  published:2017-01-20 summary:We study the inference of a model of temporal networks in which both communities and links keep memory of previous network state. By considering maximum likelihood inference from single snapshot observation of the network, we show that link persistence decreases the detectability threshold, preventing the inference of communities even when they are in principle strong enough, while community persistence tends to restore this possibility. Then we show that the inferred communities share a maximum overlap with those of a specific previous instant of time, corresponding to the maximum of a time-lagged assortativity parameter, and therefore they can be closer to those of the past than of the present. We analytically characterize these transitions as a function of the memory parameters of the model. version:1
arxiv-1701-05779 | Empirical Study of Drone Sound Detection in Real-Life Environment with Deep Neural Networks | http://arxiv.org/abs/1701.05779 | id:1701.05779 author:Sungho Jeon, Jong-Woo Shin, Young-Jun Lee, Woong-Hee Kim, YoungHyoun Kwon, Hae-Yong Yang category:cs.SD cs.LG  published:2017-01-20 summary:This work aims to investigate the use of deep neural network to detect commercial hobby drones in real-life environments by analyzing their sound data. The purpose of work is to contribute to a system for detecting drones used for malicious purposes, such as for terrorism. Specifically, we present a method capable of detecting the presence of commercial hobby drones as a binary classification problem based on sound event detection. We recorded the sound produced by a few popular commercial hobby drones, and then augmented this data with diverse environmental sound data to remedy the scarcity of drone sound data in diverse environments. We investigated the effectiveness of state-of-the-art event sound classification methods, i.e., a Gaussian Mixture Model (GMM), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN), for drone sound detection. Our empirical results, which were obtained with a testing dataset collected on an urban street, confirmed the effectiveness of these models for operating in a real environment. In summary, our RNN models showed the best detection performance with an F-Score of 0.8009 with 240 ms of input audio with a short processing time, indicating their applicability to real-time detection systems. version:1
arxiv-1701-05306 | Estimating Individual Treatment Effect in Observational Data Using Random Forest Methods | http://arxiv.org/abs/1701.05306 | id:1701.05306 author:Min Lu, Saad Sadiq, Daniel J. Feaster, Hemant Ishwaran category:stat.ML  published:2017-01-19 summary:Estimation of individual treatment effect in observational data is complicated due to the challenges of confounding and selection bias. A useful inferential framework to address this is the counterfactual (potential outcomes) model which takes the hypothetical stance of asking what if an individual had received both treatments. Making use of random forests (RF) within the counterfactual framework we estimate individual treatment effects by directly modeling the response. We find accurate estimation of individual treatment effects is possible even in complex heterogeneous settings but that the type of RF approach plays an important role in accuracy. Methods designed to be adaptive to confounding, when used in parallel with out-of-sample estimation, do best. One method found to be especially promising is counterfactual synthetic forests. We illustrate this new methodology by applying it to a large comparative effectiveness trial, Project Aware, in order to explore the role drug use plays in sexual risk. The analysis reveals important connections between risky behavior, drug usage, and sexual risk. version:2
arxiv-1701-05766 | A Large-scale Dataset and Benchmark for Similar Trademark Retrieval | http://arxiv.org/abs/1701.05766 | id:1701.05766 author:Osman Tursun, Cemal Aker, Sinan Kalkan category:cs.CV  published:2017-01-20 summary:Trademark retrieval (TR) has become an important yet challenging problem due to an ever increasing trend in trademark applications and infringement incidents. There have been many promising attempts for the TR problem, which, however, fell impracticable since they were evaluated with limited and mostly trivial datasets. In this paper, we provide a large-scale dataset with benchmark queries with which different TR approaches can be evaluated systematically. Moreover, we provide a baseline on this benchmark using the widely-used methods applied to TR in the literature. Furthermore, we identify and correct two important issues in TR approaches that were not addressed before: reversal of contrast, and presence of irrelevant text in trademarks severely affect the TR methods. Lastly, we applied deep learning, namely, several popular Convolutional Neural Network models, to the TR problem. To the best of the authors, this is the first attempt to do so. version:1
arxiv-1701-05763 | Multivariate Confidence Intervals | http://arxiv.org/abs/1701.05763 | id:1701.05763 author:Jussi Korpela, Emilia Oikarinen, Kai Puolamäki, Antti Ukkonen category:stat.AP stat.ML G.3  published:2017-01-20 summary:Confidence intervals are a popular way to visualize and analyze data distributions. Unlike p-values, they can convey information both about statistical significance as well as effect size. However, very little work exists on applying confidence intervals to multivariate data. In this paper we define confidence intervals for multivariate data that extend the one-dimensional definition in a natural way. In our definition every variable is associated with its own confidence interval as usual, but a data vector can be outside of a few of these, and still be considered to be within the confidence area. We analyze the problem and show that the resulting confidence areas retain the good qualities of their one-dimensional counterparts: they are informative and easy to interpret. Furthermore, we show that the problem of finding multivariate confidence intervals is hard, but provide efficient approximate algorithms to solve the problem. version:1
arxiv-1701-05748 | Robust Intrinsic and Extrinsic Calibration of RGB-D Cameras | http://arxiv.org/abs/1701.05748 | id:1701.05748 author:Filippo Basso, Emanuele Menegatti, Alberto Pretto category:cs.CV cs.RO  published:2017-01-20 summary:Color-depth cameras (RGB-D cameras) have become the primary sensors in most robotics systems, from service robotics to industrial robotics applications. Typical consumer-grade RGB-D cameras are provided with a coarse intrinsic and extrinsic calibration that generally does not meet the accuracy requirements needed by many robotics applications (e.g., high accuracy 3D environment reconstruction and mapping, high precision object recognition and localization, ...). In this paper, we propose a human-friendly, reliable and accurate calibration framework that enables to easily estimate both the intrinsic and extrinsic parameters of a general color-depth sensor couple. Our approach is based on a novel, two components, measurement error model that unifies the error sources of different RGB-D pairs based on different technologies, such as structured-light 3D cameras and time-of-flight cameras. The proposed correction model is implemented using two different parametric undistortion maps that provide the calibrated readings by means of linear combinations of control functions. Our method provides some important advantages compared to other state-of-the-art systems: it is general (i.e., well suited for different types of sensors), it is based on an easy and stable calibration protocol, it provides a greater calibration accuracy, and it has been implemented within the ROS robotics framework. We report detailed and comprehensive experimental validations and performance comparisons to support our statements. version:1
arxiv-1701-07396 | LAREX - A semi-automatic open-source Tool for Layout Analysis and Region Extraction on Early Printed Books | http://arxiv.org/abs/1701.07396 | id:1701.07396 author:Christian Reul, Uwe Springmann, Frank Puppe category:cs.CV cs.AI  published:2017-01-20 summary:A semi-automatic open-source tool for layout analysis on early printed books is presented. LAREX uses a rule based connected components approach which is very fast, easily comprehensible for the user and allows an intuitive manual correction if necessary. The PageXML format is used to support integration into existing OCR workflows. Evaluations showed that LAREX provides an efficient and flexible way to segment pages of early printed books. version:1
arxiv-1701-05730 | Using LLVM-based JIT Compilation in Genetic Programming | http://arxiv.org/abs/1701.05730 | id:1701.05730 author:Michal Gregor, Juraj Spalek category:cs.NE  published:2017-01-20 summary:The paper describes an approach to implementing genetic programming, which uses the LLVM library to just-in-time compile/interpret the evolved abstract syntax trees. The solution is described in some detail, including a parser (based on FlexC++ and BisonC++) that can construct the trees from a simple toy language with C-like syntax. The approach is compared with a previous implementation (based on direct execution of trees using polymorphic functors) in terms of execution speed. version:1
arxiv-1701-05703 | Automatic Generation of Typographic Font from a Small Font Subset | http://arxiv.org/abs/1701.05703 | id:1701.05703 author:Tomo Miyazaki, Tatsunori Tsuchiya, Yoshihiro Sugaya, Shinichiro Omachi, Masakazu Iwamura, Seiichi Uchida, Koichi Kise category:cs.CV  published:2017-01-20 summary:This paper addresses the automatic generation of a typographic font from a subset of characters. Specifically, we use a subset of a typographic font to extrapolate additional characters. Consequently, we obtain a complete font containing a number of characters sufficient for daily use. The automated generation of Japanese fonts is in high demand because a Japanese font requires over 1,000 characters. Unfortunately, professional typographers create most fonts, resulting in significant financial and time investments for font generation. The proposed method can be a great aid for font creation because designers do not need to create the majority of the characters for a new font. The proposed method uses strokes from given samples for font generation. The strokes, from which we construct characters, are extracted by exploiting a character skeleton dataset. This study makes three main contributions: a novel method of extracting strokes from characters, which is applicable to both standard fonts and their variations; a fully automated approach for constructing characters; and a selection method for sample characters. We demonstrate our proposed method by generating 2,965 characters in 47 fonts. Objective and subjective evaluations verify that the generated characters are similar to handmade characters. version:1
arxiv-1701-05691 | A Novel Variable Selection Method based on Frequent Pattern Tree for Real-time Traffic Accident Risk Prediction | http://arxiv.org/abs/1701.05691 | id:1701.05691 author:Lei Lin, Qian Wang, Adel W. Sadek category:stat.AP cs.LG  published:2017-01-20 summary:Traffic accident data are usually noisy, contain missing values, and heterogeneous. How to select the most important variables to improve real-time traffic accident risk prediction has become a concern of many recent studies. This paper proposes a novel variable selection method based on the Frequent Pattern tree (FP tree) algorithm. First, all the frequent patterns in the traffic accident dataset are discovered. Then for each frequent pattern, a new criterion, called the Relative Object Purity Ratio (ROPR) which we proposed, is calculated. This ROPR is added to the importance score of the variables that differentiates one frequent pattern from the others. To test the proposed method, a dataset was compiled from the traffic accidents records detected by only one detector on interstate highway I-64 in Virginia in 2005. This data set was then linked to other variables such as real-time traffic information and weather conditions. Both the proposed method based on the FP tree algorithm, as well as the widely utilized, random forest method, were then used to identify the important variables or the Virginia data set. The results indicate that there are some differences between the variables deemed important by the FP tree and those selected by the random forest method. Following this, two baseline models (i.e. a nearest neighbor (k-NN) method and a Bayesian network) were developed to predict accident risk based on the variables identified by both the FP tree method and the random forest method. The results show that the models based on the variable selection using the FP tree performed better than those based on the random forest method for several versions of the k-NN and Bayesian network models.The best results were derived from a Bayesian network model using variables from FP tree. That model could predict 61.11% of accidents accurately, while having a false alarm rate of 38.16%. version:1
arxiv-1701-05681 | Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete Source Code Fragments | http://arxiv.org/abs/1701.05681 | id:1701.05681 author:Edwin Dauber, Aylin Caliskan-Islam, Richard Harang, Rachel Greenstadt category:cs.LG cs.CR  published:2017-01-20 summary:Program authorship attribution has implications for the privacy of programmers who wish to contribute code anonymously. While previous work has shown that complete files that are individually authored can be attributed, we show here for the first time that accounts belonging to open source contributors containing short, incomplete, and typically uncompilable fragments can also be effectively attributed. We propose a technique for authorship attribution of contributor accounts containing small source code samples, such as those that can be obtained from version control systems or other direct comparison of sequential versions. We show that while application of previous methods to individual small source code samples yields an accuracy of about 73% for 106 programmers as a baseline, by ensembling and averaging the classification probabilities of a sufficiently large set of samples belonging to the same author we achieve 99% accuracy for assigning the set of samples to the correct author. Through these results, we demonstrate that attribution is an important threat to privacy for programmers even in real-world collaborative environments such as GitHub. Additionally, we propose the use of calibration curves to identify samples by unknown and previously unencountered authors in the open world setting. We show that we can also use these calibration curves in the case that we do not have linking information and thus are forced to classify individual samples directly. This is because the calibration curves allow us to identify which samples are more likely to have been correctly attributed. Using such a curve can help an analyst choose a cut-off point which will prevent most misclassifications, at the cost of causing the rejection of some of the more dubious correct attributions. version:1
arxiv-1701-05676 | Efficient Feature Matching by Progressive Candidate Search | http://arxiv.org/abs/1701.05676 | id:1701.05676 author:Sehyung Lee, Jongwoo Lim, Il Hong Suh category:cs.CV  published:2017-01-20 summary:We present a novel feature matching algorithm that systematically utilizes the geometric properties of features such as position, scale, and orientation, in addition to the conventional descriptor vectors. In challenging scenes with the presence of repetitive patterns or with a large viewpoint change, it is hard to find the correct correspondences using feature descriptors only, since the descriptor distances of the correct matches may not be the least among the candidates due to appearance changes. Assuming that the layout of the nearby features does not changed much, we propose the bidirectional transfer measure to gauge the geometric consistency of a pair of feature correspondences. The feature matching problem is formulated as a Markov random field (MRF) which uses descriptor distances and relative geometric similarities together. The unmatched features are explicitly modeled in the MRF to minimize its negative impact. For speed and stability, instead of solving the MRF on the entire features at once, we start with a small set of confident feature matches, and then progressively search the candidates in nearby features and expand the MRF with them. Experimental comparisons show that the proposed algorithm finds better feature correspondences, i.e. more matches with higher inlier ratio, in many challenging scenes with much lower computational cost than the state-of-the-art algorithms. version:1
arxiv-1701-05672 | Stability Enhanced Large-Margin Classifier Selection | http://arxiv.org/abs/1701.05672 | id:1701.05672 author:Will Wei Sun, Guang Cheng, Yufeng Liu category:stat.ML  published:2017-01-20 summary:Stability is an important aspect of a classification procedure because unstable predictions can potentially reduce users' trust in a classification system and also harm the reproducibility of scientific conclusions. The major goal of our work is to introduce a novel concept of classification instability, i.e., decision boundary instability (DBI), and incorporate it with the generalization error (GE) as a standard for selecting the most accurate and stable classifier. Specifically, we implement a two-stage algorithm: (i) initially select a subset of classifiers whose estimated GEs are not significantly different from the minimal estimated GE among all the candidate classifiers; (ii) the optimal classifier is chosen as the one achieving the minimal DBI among the subset selected in stage (i). This general selection principle applies to both linear and nonlinear classifiers. Large-margin classifiers are used as a prototypical example to illustrate the above idea. Our selection method is shown to be consistent in the sense that the optimal classifier simultaneously achieves the minimal GE and the minimal DBI. Various simulations and real examples further demonstrate the advantage of our method over several alternative approaches. version:1
arxiv-1701-02511 | Heterogeneous Unsupervised Cross-domain Transfer Learning | http://arxiv.org/abs/1701.02511 | id:1701.02511 author:Feng Liu, Guanquan Zhang, Haiyan Lu, Jie Lu category:cs.LG stat.ML 15A18 58B10  published:2017-01-10 summary:Transfer learning addresses the problem of how to leverage previously acquired knowledge (a source domain) to improve the efficiency of learning in a new domain (the target domain). Although transfer learning has been widely researched in the last decade, existing research still has two restrictions: 1) the feature spaces of the domains must be homogeneous; and 2) the target domain must have at least a few labeled instances. These restrictions significantly limit transfer learning models when transferring knowledge across domains, especially in the big data era. To completely break through both of these bottlenecks, a theorem for reliable unsupervised knowledge transfer is proposed to avoid negative transfers, and a Grassmann manifold is applied to measure the distance between heterogeneous feature spaces. Based on this theorem and the Grassmann manifold, this study proposes two heterogeneous unsupervised knowledge transfer (HeUKT) models - known as RLG and GLG. The RLG uses a linear monotonic map (LMM) to reliably project two heterogeneous feature spaces onto a latent feature space and applies geodesic flow kernel (GFK) model to transfers knowledge between two the projected domains. The GLG optimizes the LMM to achieve the highest possible accuracy and guarantees that the geometric properties of the domains remain unchanged during the transfer process. To test the overall effectiveness of two models, this paper reorganizes five public datasets into ten heterogeneous cross-domain tasks across three application fields: credit assessment, text classification, and cancer detection. Extensive experiments demonstrate that the proposed models deliver superior performance over current benchmarks, and that these HeUKT models are a promising way to give computers the associative ability to judge unknown things using related known knowledge. version:2
arxiv-1701-01573 | Distinguishing Posed and Spontaneous Smiles by Facial Dynamics | http://arxiv.org/abs/1701.01573 | id:1701.01573 author:Bappaditya Mandal, David Lee, Nizar Ouarti category:cs.CV  published:2017-01-06 summary:Smile is one of the key elements in identifying emotions and present state of mind of an individual. In this work, we propose a cluster of approaches to classify posed and spontaneous smiles using deep convolutional neural network (CNN) face features, local phase quantization (LPQ), dense optical flow and histogram of gradient (HOG). Eulerian Video Magnification (EVM) is used for micro-expression smile amplification along with three normalization procedures for distinguishing posed and spontaneous smiles. Although the deep CNN face model is trained with large number of face images, HOG features outperforms this model for overall face smile classification task. Using EVM to amplify micro-expressions did not have a significant impact on classification accuracy, while the normalizing facial features improved classification accuracy. Unlike many manual or semi-automatic methodologies, our approach aims to automatically classify all smiles into either `spontaneous' or `posed' categories, by using support vector machines (SVM). Experimental results on large UvA-NEMO smile database show promising results as compared to other relevant methods. version:2
arxiv-1701-05654 | Bayesian Network Learning via Topological Order | http://arxiv.org/abs/1701.05654 | id:1701.05654 author:Young Woong Park, Diego Klabjan category:stat.ML cs.DS  published:2017-01-20 summary:We propose a mixed integer programming (MIP) model and iterative algorithms based on topological orders to solve optimization problems with acyclic constraints on a directed graph. The proposed MIP model has a significantly lower number of constraints compared to popular MIP models based on cycle elimination constraints and triangular inequalities. The proposed iterative algorithms use gradient descent and iterative reordering approaches, respectively, for searching topological orders. A computational experiment is presented for the Gaussian Bayesian network learning problem, an optimization problem minimizing the sum of squared errors of regression models with L1 penalty over a feature network with application of gene network inference in bioinformatics. version:1
arxiv-1701-05305 | Random Forest Missing Data Algorithms | http://arxiv.org/abs/1701.05305 | id:1701.05305 author:Fei Tang, Hemant Ishwaran category:stat.ML  published:2017-01-19 summary:Random forest (RF) missing data algorithms are an attractive approach for dealing with missing data. They have the desirable properties of being able to handle mixed types of missing data, they are adaptive to interactions and nonlinearity, and they have the potential to scale to big data settings. Currently there are many different RF imputation algorithms but relatively little guidance about their efficacy, which motivated us to study their performance. Using a large, diverse collection of data sets, performance of various RF algorithms was assessed under different missing data mechanisms. Algorithms included proximity imputation, on the fly imputation, and imputation utilizing multivariate unsupervised and supervised splitting---the latter class representing a generalization of a new promising imputation algorithm called missForest. Performance of algorithms was assessed by ability to impute data accurately. Our findings reveal RF imputation to be generally robust with performance improving with increasing correlation. Performance was good under moderate to high missingness, and even (in certain cases) when data was missing not at random. version:2
arxiv-1701-05644 | Rare Disease Physician Targeting: A Factor Graph Approach | http://arxiv.org/abs/1701.05644 | id:1701.05644 author:Yong Cai, Yunlong Wang, Dong Dai category:stat.ML cs.LG  published:2017-01-19 summary:In rare disease physician targeting, a major challenge is how to identify physicians who are treating diagnosed or underdiagnosed rare diseases patients. Rare diseases have extremely low incidence rate. For a specified rare disease, only a small number of patients are affected and a fractional of physicians are involved. The existing targeting methodologies, such as segmentation and profiling, are developed under mass market assumption. They are not suitable for rare disease market where the target classes are extremely imbalanced. The authors propose a graphical model approach to predict targets by jointly modeling physician and patient features from different data spaces and utilizing the extra relational information. Through an empirical example with medical claim and prescription data, the proposed approach demonstrates better accuracy in finding target physicians. The graph representation also provides visual interpretability of relationship among physicians and patients. The model can be extended to incorporate more complex dependency structures. This article contributes to the literature of exploring the benefit of utilizing relational dependencies among entities in healthcare industry. version:1
arxiv-1701-05632 | The Internet as Quantitative Social Science Platform: Insights from a Trillion Observations | http://arxiv.org/abs/1701.05632 | id:1701.05632 author:Klaus Ackermann, Simon D Angus, Paul A Raschky category:q-fin.EC cs.CY cs.SI physics.soc-ph stat.ML  published:2017-01-19 summary:With the large-scale penetration of the internet, for the first time, humanity has become linked by a single, open, communications platform. Harnessing this fact, we report insights arising from a unified internet activity and location dataset of an unparalleled scope and accuracy drawn from over a trillion (1.5$\times 10^{12}$) observations of end-user internet connections, with temporal resolution of just 15min over 2006-2012. We first apply this dataset to the expansion of the internet itself over 1,647 urban agglomerations globally. We find that unique IP per capita counts reach saturation at approximately one IP per three people, and take, on average, 16.1 years to achieve; eclipsing the estimated 100- and 60- year saturation times for steam-power and electrification respectively. Next, we use intra-diurnal internet activity features to up-scale traditional over-night sleep observations, producing the first global estimate of over-night sleep duration in 645 cities over 7 years. We find statistically significant variation between continental, national and regional sleep durations including some evidence of global sleep duration convergence. Finally, we estimate the relationship between internet concentration and economic outcomes in 411 OECD regions and find that the internet's expansion is associated with negative or positive productivity gains, depending strongly on sectoral considerations. To our knowledge, our study is the first of its kind to use online/offline activity of the entire internet to infer social science insights, demonstrating the unparalleled potential of the internet as a social data-science platform. version:1
arxiv-1701-05625 | CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation | http://arxiv.org/abs/1701.05625 | id:1701.05625 author:Saeedeh Shekarpour, Valerie Shalin, Krishnaprasad Thirunarayan, Amit P. Sheth category:cs.CL  published:2017-01-19 summary:While the general analysis of named entities has received substantial research attention, the analysis of relations over named entities has not. In fact, a review of the literature on unstructured as well as structured data revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning and ontology engineering. In this paper, we present CEVO (i.e., a Comprehensive EVent Ontology) built on Levin's conceptual hierarchy of English verbs that categorizes verbs with the shared meaning and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases for demonstrating the benefits of this ontology on annotation tasks: 1) annotating relations in plain text, 2) annotating ontological properties and 3) linking textual relations to ontological properties. version:1
arxiv-1701-05616 | Holistic Interstitial Lung Disease Detection using Deep Convolutional Neural Networks: Multi-label Learning and Unordered Pooling | http://arxiv.org/abs/1701.05616 | id:1701.05616 author:Mingchen Gao, Ziyue Xu, Le Lu, Adam P. Harrison, Ronald M. Summers, Daniel J. Mollura category:cs.CV  published:2017-01-19 summary:Accurately predicting and detecting interstitial lung disease (ILD) patterns given any computed tomography (CT) slice without any pre-processing prerequisites, such as manually delineated regions of interest (ROIs), is a clinically desirable, yet challenging goal. The majority of existing work relies on manually-provided ILD ROIs to extract sampled 2D image patches from CT slices and, from there, performs patch-based ILD categorization. Acquiring manual ROIs is labor intensive and serves as a bottleneck towards fully-automated CT imaging ILD screening over large-scale populations. Furthermore, despite the considerable high frequency of more than one ILD pattern on a single CT slice, previous works are only designed to detect one ILD pattern per slice or patch. To tackle these two critical challenges, we present multi-label deep convolutional neural networks (CNNs) for detecting ILDs from holistic CT slices (instead of ROIs or sub-images). Conventional single-labeled CNN models can be augmented to cope with the possible presence of multiple ILD pattern labels, via 1) continuous-valued deep regression based robust norm loss functions or 2) a categorical objective as the sum of element-wise binary logistic losses. Our methods are evaluated and validated using a publicly available database of 658 patient CT scans under five-fold cross-validation, achieving promising performance on detecting four major ILD patterns: Ground Glass, Reticular, Honeycomb, and Emphysema. We also investigate the effectiveness of a CNN activation-based deep-feature encoding scheme using Fisher vector encoding, which treats ILD detection as spatially-unordered deep texture classification. version:1
arxiv-1701-05595 | Fast and Efficient Skin Detection for Facial Detection | http://arxiv.org/abs/1701.05595 | id:1701.05595 author:Mohammad Reza Mahmoodi category:cs.CV  published:2017-01-19 summary:In this paper, an efficient skin detection system is proposed. The algorithm is based on a very fast efficient pre-processing step utilizing the concept of ternary conversion in order to identify candidate windows and subsequently, a novel local two-stage diffusion method which has F-score accuracy of 0.5978 on SDD dataset. The pre-processing step has been proven to be useful to boost the speed of the system by eliminating 82% of an image in average. This is obtained by keeping the true positive rate above 98%. In addition, a novel segmentation algorithm is also designed to process candidate windows which is quantitatively and qualitatively proven to be very efficient in term of accuracy. The algorithm has been implemented in FPGA to obtain real-time processing speed. The system is designed fully pipeline and the inherent parallel structure of the algorithm is fully exploited to maximize the performance. The system is implemented on a Spartan-6 LXT45 Xilinx FPGA and it is capable of processing 98 frames of 640*480 24-bit color images per second. version:1
arxiv-1701-05593 | Parameter Selection Algorithm For Continuous Variables | http://arxiv.org/abs/1701.05593 | id:1701.05593 author:Peyman Tavallali, Marianne Razavi, Sean Brady category:stat.AP stat.ME stat.ML  published:2017-01-19 summary:In this article, we propose a new algorithm for supervised learning methods, by which one can both capture the non-linearity in data and also find the best subset model. To produce an enhanced subset of the original variables, an ideal selection method should have the potential of adding a supplementary level of regression analysis that would capture complex relationships in the data via mathematical transformation of the predictors and exploration of synergistic effects of combined variables. The method that we present here has the potential to produce an optimal subset of variables, rendering the overall process of model selection to be more efficient. The core objective of this paper is to introduce a new estimation technique for the classical least square regression framework. This new automatic variable transformation and model selection method could offer an optimal and stable model that minimizes the mean square error and variability, while combining all possible subset selection methodology and including variable transformations and interaction. Moreover, this novel method controls multicollinearity, leading to an optimal set of explanatory variables. version:1
arxiv-1701-05588 | High Performance Novel Skin Segmentation Algorithm for Images With Complex Background | http://arxiv.org/abs/1701.05588 | id:1701.05588 author:Mohammad Reza Mahmoodi category:cs.CV  published:2017-01-19 summary:Skin Segmentation is widely used in biometric applications such as face detection, face recognition, face tracking, and hand gesture recognition. However, several challenges such as nonlinear illumination, equipment effects, personal interferences, ethnicity variations, etc., are involved in detection process that result in the inefficiency of color based methods. Even though many ideas have already been proposed, the problem has not been satisfactorily solved yet. This paper introduces a technique that addresses some limitations of the previous works. The proposed algorithm consists of three main steps including initial seed generation of skin map, Otsu segmentation in color images, and finally a two-stage diffusion. The initial seed of skin pixels is provided based on the idea of ternary image as there are certain pixels in images which are associated to human complexion with very high probability. The Otsu segmentation is performed on several color channels in order to identify homogeneous regions. The result accompanying with the edge map of the image is utilized in two consecutive diffusion steps in order to annex initially unidentified skin pixels to the seed. Both quantitative and qualitative results demonstrate the effectiveness of the proposed system in compare with the state-of-the-art works. version:1
arxiv-1701-05581 | Leveraging Cognitive Features for Sentiment Analysis | http://arxiv.org/abs/1701.05581 | id:1701.05581 author:Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey, Pushpak Bhattacharyya category:cs.CL  published:2017-01-19 summary:Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features. We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs. version:1
arxiv-1701-05574 | Harnessing Cognitive Features for Sarcasm Detection | http://arxiv.org/abs/1701.05574 | id:1701.05574 author:Abhijit Mishra, Diptesh Kanojia, Seema Nagar, Kuntal Dey, Pushpak Bhattacharyya category:cs.CL  published:2017-01-19 summary:In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by his observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system. version:1
arxiv-1701-05573 | Poisson--Gamma Dynamical Systems | http://arxiv.org/abs/1701.05573 | id:1701.05573 author:Aaron Schein, Mingyuan Zhou, Hanna Wallach category:stat.ML cs.LG  published:2017-01-19 summary:We introduce a new dynamical system for sequentially observed multivariate count data. This model is based on the gamma--Poisson construction---a natural choice for count data---and relies on a novel Bayesian nonparametric prior that ties and shrinks the model parameters, thus avoiding overfitting. We present an efficient MCMC inference algorithm that advances recent work on augmentation schemes for inference in negative binomial models. Finally, we demonstrate the model's inductive bias using a variety of real-world data sets, showing that it exhibits superior predictive performance over other models and infers highly interpretable latent structure. version:1
arxiv-1701-05553 | Optimized Spatial Partitioning via Minimal Swarm Intelligence | http://arxiv.org/abs/1701.05553 | id:1701.05553 author:Casey Kneale, Dominic Poerio, Karl S. Booksh category:stat.ME cs.NE  published:2017-01-19 summary:Optimized spatial partitioning algorithms are the corner stone of many successful experimental designs and statistical methods. Of these algorithms, the Centroidal Voronoi Tessellation (CVT) is the most widely utilized. CVT based methods require global knowledge of spatial boundaries, do not readily allow for weighted regions, have challenging implementations, and are inefficiently extended to high dimensional spaces. We describe two simple partitioning schemes based on nearest and next nearest neighbor locations which easily incorporate these features at the slight expense of optimal placement. Several novel qualitative techniques which assess these partitioning schemes are also included. The feasibility of autonomous uninformed sensor networks utilizing these algorithms are considered. Some improvements in particle swarm optimizer results on multimodal test functions from partitioned initial positions in two space are also illustrated. Pseudo code for all of the novel algorithms depicted here-in is available in the supplementary information of this manuscript. version:1
arxiv-1701-05549 | Deep Neural Networks - A Brief History | http://arxiv.org/abs/1701.05549 | id:1701.05549 author:Krzysztof J. Cios category:cs.NE cs.CV cs.LG  published:2017-01-19 summary:Introduction to deep neural networks and their history. version:1
arxiv-1701-05524 | Synthetic to Real Adaptation with Deep Generative Correlation Alignment Networks | http://arxiv.org/abs/1701.05524 | id:1701.05524 author:Xingchao Peng, Kate Saenko category:cs.CV  published:2017-01-19 summary:Synthetic images rendered from 3D CAD models have been used in the past to augment training data for object recognition algorithms. However, the generated images are non-photorealistic and do not match real image statistics. This leads to a large domain discrepancy, causing models trained on synthetic data to perform poorly on real domains. Recent work has shown the great potential of deep convolutional neural networks to generate realistic images, but has not addressed synthetic-to-real domain adaptation. Inspired by these ideas, we propose the Deep Generative Correlation Alignment Network (DGCAN) to synthesize training images using a novel domain adaption algorithm. DGCAN leverages the L2 and the correlation alignment (CORAL) losses to minimize the domain discrepancy between generated and real images in deep feature space. The rendered results demonstrate that DGCAN can synthesize the object shape from 3D CAD models together with structured texture from a small amount of real background images. Experimentally, we show that training classifiers on the generated data can significantly boost performance when testing on the real image domain, improving upon several existing methods. version:1
arxiv-1701-05517 | PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications | http://arxiv.org/abs/1701.05517 | id:1701.05517 author:Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma category:cs.LG stat.ML  published:2017-01-19 summary:PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications. version:1
arxiv-1701-05512 | Fisher consistency for prior probability shift | http://arxiv.org/abs/1701.05512 | id:1701.05512 author:Dirk Tasche category:stat.ML cs.LG stat.CO 62C10  published:2017-01-19 summary:We introduce Fisher consistency in the sense of unbiasedness as a criterion to distinguish potentially suitable and unsuitable estimators of prior class probabilities in test datasets under prior probability and more general dataset shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted Classify & Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities. version:1
arxiv-1701-02301 | A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery | http://arxiv.org/abs/1701.02301 | id:1701.02301 author:Lingxiao Wang, Xiao Zhang, Quanquan Gu category:stat.ML  published:2017-01-09 summary:We propose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an appropriate initial estimator, our proposed algorithm performs projected gradient descent based on a novel semi-stochastic gradient specifically designed for low-rank matrix recovery. Based upon the mild restricted strong convexity and smoothness conditions, we derive a projected notion of the restricted Lipschitz continuous gradient property, and prove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an improved computational complexity. Moreover, our algorithm can be employed to both noiseless and noisy observations, where the optimal sample complexity and the minimax optimal statistical rate can be attained respectively. We further illustrate the superiority of our generic framework through several specific examples, both theoretically and experimentally. version:2
arxiv-1701-05498 | T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects | http://arxiv.org/abs/1701.05498 | id:1701.05498 author:Tomas Hodan, Pavel Haluza, Stepan Obdrzalek, Jiri Matas, Manolis Lourakis, Xenophon Zabulis category:cs.CV cs.AI cs.RO  published:2017-01-19 summary:We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less. version:1
arxiv-1701-05487 | Learning first-order definable concepts over structures of small degree | http://arxiv.org/abs/1701.05487 | id:1701.05487 author:Martin Grohe, Martin Ritzert category:cs.LG cs.LO  published:2017-01-19 summary:We consider a declarative framework for machine learning where concepts and hypotheses are defined by formulas of a logic over some background structure. We show that within this framework, concepts defined by first-order formulas over a background structure of at most polylogarithmic degree can be learned in polylogarithmic time in the "probably approximately correct" learning sense. version:1
arxiv-1701-05458 | Extreme value statistics for censored data with heavy tails under competing risks | http://arxiv.org/abs/1701.05458 | id:1701.05458 author:Julien Worms, Rym Worms category:math.ST stat.ML stat.TH  published:2017-01-19 summary:This paper addresses the problem of estimating, in the presence of random censoring as well as competing risks, the extreme value index of the (sub)-distribution function associated to one particular cause, in the heavy-tail case. Asymptotic normality of the proposed estimator (which has the form of an Aalen-Johansen integral, and is the first estimator proposed in this context) is established. A small simulation study exhibits its performances for finite samples. Estimation of extreme quantiles of the cumulative incidence function is also addressed. version:1
arxiv-1701-05432 | Higher-order Pooling of CNN Features via Kernel Linearization for Action Recognition | http://arxiv.org/abs/1701.05432 | id:1701.05432 author:Anoop Cherian, Piotr Koniusz, Stephen Gould category:cs.CV  published:2017-01-19 summary:Most successful deep learning algorithms for action recognition extend models designed for image-based tasks such as object recognition to video. Such extensions are typically trained for actions on single video frames or very short clips, and then their predictions from sliding-windows over the video sequence are pooled for recognizing the action at the sequence level. Usually this pooling step uses the first-order statistics of frame-level action predictions. In this paper, we explore the advantages of using higher-order correlations; specifically, we introduce Higher-order Kernel (HOK) descriptors generated from the late fusion of CNN classifier scores from all the frames in a sequence. To generate these descriptors, we use the idea of kernel linearization. Specifically, a similarity kernel matrix, which captures the temporal evolution of deep classifier scores, is first linearized into kernel feature maps. The HOK descriptors are then generated from the higher-order co-occurrences of these feature maps, and are then used as input to a video-level classifier. We provide experiments on two fine-grained action recognition datasets and show that our scheme leads to state-of-the-art results. version:1
arxiv-1701-05419 | Moving to VideoKifu: the last steps toward a fully automatic record-keeping of a Go game | http://arxiv.org/abs/1701.05419 | id:1701.05419 author:Mario Corsolini, Andrea Carta category:cs.CV  published:2017-01-19 summary:In a previous paper [ arXiv:1508.03269 ] we described the techniques we successfully employed for automatically reconstructing the whole move sequence of a Go game by means of a set of pictures. Now we describe how it is possible to reconstruct the move sequence by means of a video stream (which may be provided by an unattended webcam), possibly in real-time. Although the basic algorithms remain the same, we will discuss the new problems that arise when dealing with videos, with special care for the ones that could block a real-time analysis and require an improvement of our previous techniques or even a completely brand new approach. Eventually we present a number of preliminary but positive experimental results supporting the effectiveness of the software we are developing, built on the ideas here outlined. version:1
arxiv-1701-05412 | Block-wise Lensless Compressive Camera | http://arxiv.org/abs/1701.05412 | id:1701.05412 author:Xin Yuan, Gang Huang, Hong Jiang, Paul Wilford category:cs.CV  published:2017-01-19 summary:The existing lensless compressive camera ($\text{L}^2\text{C}^2$)~\cite{Huang13ICIP} suffers from low capture rates, resulting in low resolution images when acquired over a short time. In this work, we propose a new regime to mitigate these drawbacks. We replace the global-based compressive sensing used in the existing $\text{L}^2\text{C}^2$ by the local block (patch) based compressive sensing. We use a single sensor for each block, rather than for the entire image, thus forming a multiple but spatially parallel sensor $\text{L}^2\text{C}^2$. This new camera retains the advantages of existing $\text{L}^2\text{C}^2$ while leading to the following additional benefits: 1) Since each block can be very small, {\em e.g.}$~8\times 8$ pixels, we only need to capture $\sim 10$ measurements to achieve reasonable reconstruction. Therefore the capture time can be reduced significantly. 2) The coding patterns used in each block can be the same, therefore the sensing matrix is only of the block size compared to the entire image size in existing $\text{L}^2\text{C}^2$. This saves the memory requirement of the sensing matrix as well as speeds up the reconstruction. 3) Patch based image reconstruction is fast and since real time stitching algorithms exist, we can perform real time reconstruction. 4) These small blocks can be integrated to any desirable number, leading to ultra high resolution images while retaining fast capture rate and fast reconstruction. We develop multiple geometries of this block-wise $\text{L}^2\text{C}^2$ in this paper. We have built prototypes of the proposed block-wise $\text{L}^2\text{C}^2$ and demonstrated excellent results of real data. version:1
arxiv-1701-04926 | Agglomerative Info-Clustering | http://arxiv.org/abs/1701.04926 | id:1701.04926 author:Chung Chan, Ali Al-Bashabsheh, Qiaoqiao Zhou category:cs.IT cs.LG math.IT  published:2017-01-18 summary:An agglomerative clustering of random variables is proposed, where clusters of random variables sharing the maximum amount of multivariate mutual information are merged successively to form larger clusters. Compared to the previous info-clustering algorithms, the agglomerative approach allows the computation to stop earlier when clusters of desired size and accuracy are obtained. An efficient algorithm is also derived based on the submodularity of entropy and the duality between the principal sequence of partitions and the principal sequence for submodular functions. version:2
arxiv-1701-03655 | Dictionary Learning from Incomplete Data | http://arxiv.org/abs/1701.03655 | id:1701.03655 author:Valeriya Naumova, Karin Schnass category:cs.LG stat.ML  published:2017-01-13 summary:This paper extends the recently proposed and theoretically justified iterative thresholding and $K$ residual means algorithm ITKrM to learning dicionaries from incomplete/masked training data (ITKrMM). It further adapts the algorithm to the presence of a low rank component in the data and provides a strategy for recovering this low rank component again from incomplete data. Several synthetic experiments show the advantages of incorporating information about the corruption into the algorithm. Finally, image inpainting is considered as application example, which demonstrates the superior performance of ITKrMM in terms of speed at similar or better reconstruction quality compared to its closest dictionary learning counterpart. version:2
arxiv-1701-05384 | FusionSeg: Learning to combine motion and appearance for fully automatic segmention of generic objects in videos | http://arxiv.org/abs/1701.05384 | id:1701.05384 author:Suyog Dutt Jain, Bo Xiong, Kristen Grauman category:cs.CV  published:2017-01-19 summary:We propose an end-to-end learning framework for segmenting generic objects in videos. Our method learns to combine appearance and motion information to produce pixel level segmentation masks for all prominent objects in videos. We formulate this task as a structured prediction problem and design a two-stream fully convolutional neural network which fuses together motion and appearance in a unified framework. Since large-scale video datasets with pixel level segmentations are problematic, we show how to bootstrap weakly annotated videos together with existing image recognition datasets for training. Through experiments on three challenging video segmentation benchmarks, our method substantially improves the state-of-the-art for segmenting generic (unseen) objects. version:1
arxiv-1701-05377 | Profiling of OCR'ed Historical Texts Revisited | http://arxiv.org/abs/1701.05377 | id:1701.05377 author:Florian Fink, Klaus-U. Schulz, Uwe Springmann category:cs.CV cs.DL  published:2017-01-19 summary:In the absence of ground truth it is not possible to automatically determine the exact spectrum and occurrences of OCR errors in an OCR'ed text. Yet, for interactive postcorrection of OCR'ed historical printings it is extremely useful to have a statistical profile available that provides an estimate of error classes with associated frequencies, and that points to conjectured errors and suspicious tokens. The method introduced in Reffle (2013) computes such a profile, combining lexica, pattern sets and advanced matching techniques in a specialized Expectation Maximization (EM) procedure. Here we improve this method in three respects: First, the method in Reffle (2013) is not adaptive: user feedback obtained by actual postcorrection steps cannot be used to compute refined profiles. We introduce a variant of the method that is open for adaptivity, taking correction steps of the user into account. This leads to higher precision with respect to recognition of erroneous OCR tokens. Second, during postcorrection often new historical patterns are found. We show that adding new historical patterns to the linguistic background resources leads to a second kind of improvement, enabling even higher precision by telling historical spellings apart from OCR errors. Third, the method in Reffle (2013) does not make any active use of tokens that cannot be interpreted in the underlying channel model. We show that adding these uninterpretable tokens to the set of conjectured errors leads to a significant improvement of the recall for error detection, at the same time improving precision. version:1
arxiv-1701-05369 | Variational Dropout Sparsifies Deep Neural Networks | http://arxiv.org/abs/1701.05369 | id:1701.05369 author:Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov category:stat.ML cs.LG  published:2017-01-19 summary:We explore recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout. We extend variational dropout to the case when dropout rate is unknown and show that it can be found by optimizing evidence variational lower bound. We show that it is possible to assign and find individual dropout rates to each connection in DNN. Interestingly such assignment leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination (ARD) effect in empirical Bayes but has a number of advantages. We report up to 128 fold compression of popular architectures without a large loss of accuracy providing additional evidence to the fact that modern deep architectures are very redundant. version:1
arxiv-1701-05363 | Stochastic Subsampling for Factorizing Huge Matrices | http://arxiv.org/abs/1701.05363 | id:1701.05363 author:Arthur Mensch, Julien Mairal, Bertrand Thirion, Gael Varoquaux category:stat.ML cs.LG math.OC q-bio.NC  published:2017-01-19 summary:We present a matrix-factorization algorithm that scales to input matrices with both huge number of rows and columns. Learned factors may be sparse or dense and/or non-negative, which makes our algorithm suitable for dictionary learning, sparse component analysis, and non-negative matrix factorization. Our algorithm streams matrix columns while subsampling them to iteratively learn the matrix factors. At each iteration, the row dimension of a new sample is reduced by subsampling, resulting in lower time complexity compared to a simple streaming algorithm. Our method comes with convergence guarantees to reach a stationary point of the matrix-factorization problem. We demonstrate its efficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on patches extracted from hyperspectral images (103 GB). For both problems, which involve different penalties on rows and columns, we obtain significant speed-ups compared to state-of-the-art algorithms. version:1
arxiv-1701-05360 | 3D Face Morphable Models "In-the-Wild" | http://arxiv.org/abs/1701.05360 | id:1701.05360 author:James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis, Stefanos Zafeiriou category:cs.CV  published:2017-01-19 summary:3D Morphable Models (3DMMs) are powerful statistical models of 3D facial shape and texture, and among the state-of-the-art methods for reconstructing facial shape from single images. With the advent of new 3D sensors, many 3D facial datasets have been collected containing both neutral as well as expressive faces. However, all datasets are captured under controlled conditions. Thus, even though powerful 3D facial shape models can be learnt from such data, it is difficult to build statistical texture models that are sufficient to reconstruct faces captured in unconstrained conditions ("in-the-wild"). In this paper, we propose the first, to the best of our knowledge, "in-the-wild" 3DMM by combining a powerful statistical model of facial shape, which describes both identity and expression, with an "in-the-wild" texture model. We show that the employment of such an "in-the-wild" texture model greatly simplifies the fitting procedure, because there is no need to optimize with regards to the illumination parameters. Furthermore, we propose a new fast algorithm for fitting the 3DMM in arbitrary images. Finally, we have captured the first 3D facial database with relatively unconstrained conditions and report quantitative evaluations with state-of-the-art performance. Complementary qualitative reconstruction results are demonstrated on standard "in-the-wild" facial databases. An open source implementation of our technique is released as part of the Menpo Project. version:1
arxiv-1701-05349 | Pixel Objectness | http://arxiv.org/abs/1701.05349 | id:1701.05349 author:Suyog Dutt Jain, Bo Xiong, Kristen Grauman category:cs.CV  published:2017-01-19 summary:We propose an end-to-end learning framework for generating foreground object segmentations. Given a single novel image, our approach produces pixel-level masks for all "object-like" regions---even for object categories never seen during training. We formulate the task as a structured prediction problem of assigning foreground/background labels to all pixels, implemented using a deep fully convolutional network. Key to our idea is training with a mix of image-level object category examples together with relatively few images with boundary-level annotations. Our method substantially improves the state-of-the-art for foreground segmentation accuracy on the ImageNet and MIT Object Discovery datasets---with 19% improvements in some cases. Furthermore, with extensive evaluation on over 1 million images, we show it generalizes well to segment even object categories unseen in the foreground maps used for training. Finally, we demonstrate how our approach benefits image retrieval and image retargeting, both of which flourish when given our high-quality foreground maps. version:1
arxiv-1701-05343 | A Joint Framework for Argumentative Text Analysis Incorporating Domain Knowledge | http://arxiv.org/abs/1701.05343 | id:1701.05343 author:Zhongyu Wei, Chen Li, Yang Liu category:cs.CL  published:2017-01-19 summary:For argumentation mining, there are several sub-tasks such as argumentation component type classification, relation classification. Existing research tends to solve such sub-tasks separately, but ignore the close relation between them. In this paper, we present a joint framework incorporating logical relation between sub-tasks to improve the performance of argumentation structure generation. We design an objective function to combine the predictions from individual models for each sub-task and solve the problem with some constraints constructed from background knowledge. We evaluate our proposed model on two public corpora and the experiment results show that our model can outperform the baseline that uses a separate model significantly for each sub-task. Our model also shows advantages on component-related sub-tasks compared to a state-of-the-art joint model based on the evidence graph. version:1
arxiv-1701-05335 | On the Existence of Kernel Function for Kernel-Trick of k-Means | http://arxiv.org/abs/1701.05335 | id:1701.05335 author:Mieczysław A. Kłopotek category:cs.LG stat.ML  published:2017-01-19 summary:This paper corrects the proof of the Theorem 2 from the Gower's paper \cite[page 5]{Gower:1982}. The correction is needed in order to establish the existence of the kernel function used commonly in the kernel trick e.g. for $k$-means clustering algorithm, on the grounds of distance matrix. The scope of correction is explained in section 2. version:1
arxiv-1701-05334 | Fuzzy Ontology-Based Sentiment Analysis of Transportation and City Feature Reviews for Safe Traveling | http://arxiv.org/abs/1701.05334 | id:1701.05334 author:Farman Ali, D. Kwak, Pervez Khan, S. M. Riazul Islam, K. H. Kim, K. S. Kwak category:cs.AI cs.CL  published:2017-01-19 summary:Traffic congestion is rapidly increasing in urban areas, particularly in mega cities. To date, there exist a few sensor network based systems to address this problem. However, these techniques are not suitable enough in terms of monitoring an entire transportation system and delivering emergency services when needed. These techniques require real-time data and intelligent ways to quickly determine traffic activity from useful information. In addition, these existing systems and websites on city transportation and travel rely on rating scores for different factors (e.g., safety, low crime rate, cleanliness, etc.). These rating scores are not efficient enough to deliver precise information, whereas reviews or tweets are significant, because they help travelers and transportation administrators to know about each aspect of the city. However, it is difficult for travelers to read, and for transportation systems to process, all reviews and tweets to obtain expressive sentiments regarding the needs of the city. The optimum solution for this kind of problem is analyzing the information available on social network platforms and performing sentiment analysis. On the other hand, crisp ontology-based frameworks cannot extract blurred information from tweets and reviews; therefore, they produce inadequate results. In this regard, this paper proposes fuzzy ontology-based sentiment analysis and SWRL rule-based decision-making to monitor transportation activities and to make a city- feature polarity map for travelers. This system retrieves reviews and tweets related to city features and transportation activities. The feature opinions are extracted from these retrieved data, and then fuzzy ontology is used to determine the transportation and city-feature polarity. A fuzzy ontology and an intelligent system prototype are developed by using Prot\'eg\'e OWL and Java, respectively. version:1
arxiv-1701-05311 | Semantic Evolutionary Concept Distances for Effective Information Retrieval in Query Expansion | http://arxiv.org/abs/1701.05311 | id:1701.05311 author:Valentina Franzoni, Yuanxi Li, Clement H. C. Leung, Alfredo Milani category:cs.IR cs.AI cs.CL math.PR 68Uxx  37A50 H.3.5  published:2017-01-19 summary:In this work several semantic approaches to concept-based query expansion and reranking schemes are studied and compared with different ontology-based expansion methods in web document search and retrieval. In particular, we focus on concept-based query expansion schemes, where, in order to effectively increase the precision of web document retrieval and to decrease the users browsing time, the main goal is to quickly provide users with the most suitable query expansion. Two key tasks for query expansion in web document retrieval are to find the expansion candidates, as the closest concepts in web document domain, and to rank the expanded queries properly. The approach we propose aims at improving the expansion phase for better web document retrieval and precision. The basic idea is to measure the distance between candidate concepts using the PMING distance, a collaborative semantic proximity measure, i.e. a measure which can be computed by using statistical results from web search engine. Experiments show that the proposed technique can provide users with more satisfying expansion results and improve the quality of web document retrieval. version:1
arxiv-1701-05268 | Accurate Motion Estimation through Random Sample Aggregated Consensus | http://arxiv.org/abs/1701.05268 | id:1701.05268 author:Martin Rais, Gabriele Facciolo, Enric Meinhardt-Llopis, Jean-Michel Morel, Antoni Buades, Bartomeu Coll category:cs.CV  published:2017-01-19 summary:We reconsider the classic problem of estimating accurately a 2D transformation from point matches between images containing outliers. RANSAC discriminates outliers by randomly generating minimalistic sampled hypotheses and verifying their consensus over the input data. Its response is based on the single hypothesis that obtained the largest inlier support. In this article we show that the resulting accuracy can be improved by aggregating all generated hypotheses. This yields RANSAAC, a framework that improves systematically over RANSAC and its state-of-the-art variants by statistically aggregating hypotheses. To this end, we introduce a simple strategy that allows to rapidly average 2D transformations, leading to an almost negligible extra computational cost. We give practical applications on projective transforms and homography+distortion models and demonstrate a significant performance gain in both cases. version:1
arxiv-1701-05265 | Online Structure Learning for Sum-Product Networks with Gaussian Leaves | http://arxiv.org/abs/1701.05265 | id:1701.05265 author:Wilson Hsu, Agastya Kalra, Pascal Poupart category:stat.ML cs.LG  published:2017-01-19 summary:Sum-product networks have recently emerged as an attractive representation due to their dual view as a special type of deep neural network with clear semantics and a special type of probabilistic graphical model for which inference is always tractable. Those properties follow from some conditions (i.e., completeness and decomposability) that must be respected by the structure of the network. As a result, it is not easy to specify a valid sum-product network by hand and therefore structure learning techniques are typically used in practice. This paper describes the first online structure learning technique for continuous SPNs with Gaussian leaves. We also introduce an accompanying new parameter learning technique. version:1
arxiv-1701-00877 | On the Usability of Probably Approximately Correct Implication Bases | http://arxiv.org/abs/1701.00877 | id:1701.00877 author:Daniel Borchmann, Tom Hanika, Sergei Obiedkov category:cs.AI cs.LG cs.LO 03G10 68T27 F.4.1; I.2.6  published:2017-01-04 summary:We revisit the notion of probably approximately correct implication bases from the literature and present a first formulation in the language of formal concept analysis, with the goal to investigate whether such bases represent a suitable substitute for exact implication bases in practical use-cases. To this end, we quantitatively examine the behavior of probably approximately correct implication bases on artificial and real-world data sets and compare their precision and recall with respect to their corresponding exact implication bases. Using a small example, we also provide qualitative insight that implications from probably approximately correct bases can still represent meaningful knowledge from a given data set. version:2
arxiv-1701-03849 | Deep Neural Networks for Czech Multi-label Document Classification | http://arxiv.org/abs/1701.03849 | id:1701.03849 author:Ladislav Lenc, Pavel Král category:cs.CL  published:2017-01-13 summary:This paper is focused on automatic multi-label document classification of Czech text documents. The current approaches usually use some pre-processing which can have negative impact (loss of information, additional implementation work, etc). Therefore, we would like to omit it and use deep neural networks that learn from simple features. This choice was motivated by their successful usage in many other machine learning fields. Two different networks are compared: the first one is a standard multi-layer perceptron, while the second one is a popular convolutional network. The experiments on a Czech newspaper corpus show that both networks significantly outperform baseline method which uses a rich set of features with maximum entropy classifier. We have also shown that convolutional network gives the best results. version:2
arxiv-1701-05230 | Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index Models for Binary Outcomes | http://arxiv.org/abs/1701.05230 | id:1701.05230 author:Abhishek Chakrabortty, Matey Neykov, Raymond Carroll, Tianxi Cai category:stat.ME math.ST stat.ML stat.TH  published:2017-01-18 summary:We consider the recovery of regression coefficients, denoted by $\boldsymbol{\beta}_0$, for a single index model (SIM) relating a binary outcome $Y$ to a set of possibly high dimensional covariates $\boldsymbol{X}$, based on a large but 'unlabeled' dataset $\mathcal{U}$. On $\mathcal{U}$, we fully observe $\boldsymbol{X}$ and additionally a surrogate $S$ which, while not being strongly predictive of $Y$ throughout the entirety of its support, can forecast it with high accuracy when it assumes extreme values. Such datasets arise naturally in modern studies involving large databases such as electronic medical records (EMR) where $Y$, unlike $(\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies, an example of $Y$ and $S$ would be the true disease phenotype and the count of the associated diagnostic codes respectively. Assuming another SIM for $S$ given $\boldsymbol{X}$, we show that under sparsity assumptions, we can recover $\boldsymbol{\beta}_0$ proportionally by simply fitting a least squares LASSO estimator to the subset of the observed data on $(\boldsymbol{X}, S)$ restricted to the extreme sets of $S$ with $Y$ imputed using the surrogacy of $S$. We obtain sharp finite sample performance bounds for our estimator, including deterministic deviation bounds and probabilistic guarantees. We demonstrate the effectiveness of our approach through extensive simulations and by application to an EMR study conducted at the Partners HealthCare Systems. version:1
arxiv-1701-05228 | Recommendation under Capacity Constraints | http://arxiv.org/abs/1701.05228 | id:1701.05228 author:Konstantina Christakopoulou, Jaya Kawale, Arindam Banerjee category:stat.ML cs.IR cs.LG  published:2017-01-18 summary:In this paper, we investigate the common scenario where every candidate item for recommendation is characterized by a maximum capacity, i.e., number of seats in a Point-of-Interest or size of an item's inventory. Despite the prevalence of the task of recommending items under capacity constraints in a variety of settings, to the best of our knowledge, none of the known recommender methods is designed to respect capacity constraints. To close this gap, we extend two state-of-the art latent factor recommendation approaches: probabilistic matrix factorization (PMF) and geographical matrix factorization (GeoMF), to optimize for both prediction accuracy and expected item usage that respects the capacity constraints. We introduce the useful concepts of user propensity to listen and item capacity. Our experimental results in public datasets, both for the domain of item recommendation and Point-of-Interest recommendation, highlight the benefit of our method for the setting of recommendation under capacity constraints. version:1
arxiv-1701-05217 | Lipschitz Properties for Deep Convolutional Networks | http://arxiv.org/abs/1701.05217 | id:1701.05217 author:Radu Balan, Maneesh Singh, Dongmian Zou category:cs.LG math.FA 62M45 I.2.6  published:2017-01-18 summary:In this paper we discuss the stability properties of convolutional neural networks. Convolutional neural networks are widely used in machine learning. In classification they are mainly used as feature extractors. Ideally, we expect similar features when the inputs are from the same class. That is, we hope to see a small change in the feature vector with respect to a deformation on the input signal. This can be established mathematically, and the key step is to derive the Lipschitz properties. Further, we establish that the stability results can be extended for more general networks. We give a formula for computing the Lipschitz bound, and compare it with other methods to show it is closer to the optimal value. version:1
arxiv-1701-05159 | Temporal Overdrive Recurrent Neural Network | http://arxiv.org/abs/1701.05159 | id:1701.05159 author:Filippo Maria Bianchi, Michael Kampffmeyer, Enrico Maiorino, Robert Jenssen category:cs.NE math.DS  published:2017-01-18 summary:In this work we present a novel recurrent neural network architecture designed to model systems characterized by multiple characteristic timescales in their dynamics. The proposed network is composed by several recurrent groups of neurons that are trained to separately adapt to each timescale, in order to improve the system identification process. We test our framework on time series prediction tasks and we show some promising, preliminary results achieved on synthetic data. To evaluate the capabilities of our network, we compare the performance with several state-of-the-art recurrent architectures. version:1
arxiv-1701-05105 | Deep Learning Features at Scale for Visual Place Recognition | http://arxiv.org/abs/1701.05105 | id:1701.05105 author:Zetao Chen, Adam Jacobson, Niko Sunderhauf, Ben Upcroft, Lingqiao Liu, Chunhua Shen, Ian Reid, Michael Milford category:cs.CV cs.RO  published:2017-01-18 summary:The success of deep learning techniques in the computer vision domain has triggered a range of initial investigations into their utility for visual place recognition, all using generic features from networks that were trained for other types of recognition tasks. In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10% increase in performance over other place recognition algorithms and pre-trained CNNs. By analyzing the network responses and their differences from pre-trained networks, we provide insights into what a network learns when training for place recognition, and what these results signify for future research in this area. version:1
arxiv-1701-05130 | On the Performance of Network Parallel Training in Artificial Neural Networks | http://arxiv.org/abs/1701.05130 | id:1701.05130 author:Ludvig Ericson, Rendani Mbuvha category:cs.AI cs.NE cs.PF stat.ML  published:2017-01-18 summary:Artificial Neural Networks (ANNs) have received increasing attention in recent years with applications that span a wide range of disciplines including vital domains such as medicine, network security and autonomous transportation. However, neural network architectures are becoming increasingly complex and with an increasing need to obtain real-time results from such models, it has become pivotal to use parallelization as a mechanism for speeding up network training and deployment. In this work we propose an implementation of Network Parallel Training through Cannon's Algorithm for matrix multiplication. We show that increasing the number of processes speeds up training until the point where process communication costs become prohibitive; this point varies by network complexity. We also show through empirical efficiency calculations that the speedup obtained is superlinear. version:1
arxiv-1701-05121 | NMODE --- Neuro-MODule Evolution | http://arxiv.org/abs/1701.05121 | id:1701.05121 author:Keyan Ghazi-Zahedi category:cs.NE cs.RO  published:2017-01-18 summary:Modularisation, repetition, and symmetry are structural features shared by almost all biological neural networks. These features are very unlikely to be found by the means of structural evolution of artificial neural networks. This paper introduces NMODE, which is specifically designed to operate on neuro-modules. NMODE addresses a second problem in the context of evolutionary robotics, which is incremental evolution of complex behaviours for complex machines, by offering a way to interface neuro-modules. The scenario in mind is a complex walking machine, for which a locomotion module is evolved first, that is then extended by other modules in later stages. We show that NMODE is able to evolve a locomotion behaviour for a standard six-legged walking machine in approximately 10 generations and show how it can be used for incremental evolution of a complex walking machine. The entire source code used in this paper is publicly available through GitHub. version:1
arxiv-1701-05053 | Highly Efficient Hierarchical Online Nonlinear Regression Using Second Order Methods | http://arxiv.org/abs/1701.05053 | id:1701.05053 author:Burak C. Civek, Ibrahim Delibalta, Suleyman S. Kozat category:cs.LG  published:2017-01-18 summary:We introduce highly efficient online nonlinear regression algorithms that are suitable for real life applications. We process the data in a truly online manner such that no storage is needed, i.e., the data is discarded after being used. For nonlinear modeling we use a hierarchical piecewise linear approach based on the notion of decision trees where the space of the regressor vectors is adaptively partitioned based on the performance. As the first time in the literature, we learn both the piecewise linear partitioning of the regressor space as well as the linear models in each region using highly effective second order methods, i.e., Newton-Raphson Methods. Hence, we avoid the well known over fitting issues by using piecewise linear models, however, since both the region boundaries as well as the linear models in each region are trained using the second order methods, we achieve substantial performance compared to the state of the art. We demonstrate our gains over the well known benchmark data sets and provide performance results in an individual sequence manner guaranteed to hold without any statistical assumptions. Hence, the introduced algorithms address computational complexity issues widely encountered in real life applications while providing superior guaranteed performance in a strong deterministic sense. version:1
arxiv-1701-05013 | Transfer learning for multi-center classification of chronic obstructive pulmonary disease | http://arxiv.org/abs/1701.05013 | id:1701.05013 author:Veronika Cheplygina, Isabel Pino Peña, Jesper Holst Pedersen, David A. Lynch, Lauge Sørensen, Marleen de Bruijne category:cs.CV  published:2017-01-18 summary:Chronic obstructive pulmonary disease (COPD) is a lung disease which can be quantified using chest computed tomography (CT) scans. Recent studies have shown that COPD can be automatically diagnosed using weakly supervised learning of intensity and texture distributions. However, up till now such classifiers have only been evaluated on scans from a single domain, and it is unclear whether they would generalize across domains, such as different scanners or scanning protocols. To address this problem, we investigate classification of COPD in a multi-center dataset with a total of 803 scans from three different centers, four different scanners, with heterogenous subject distributions. Our method is based on Gaussian texture features, and a weighted logistic classifier, which increases the weights of samples similar to the test data. We show that Gaussian texture features outperform intensity features previously used in multi-center classification tasks. We also show that a weighting strategy based on a classifier that is trained to discriminate between scans from different domains, can further improve the results. To encourage further research into transfer learning methods for classification of COPD, upon acceptance of the paper we will release two feature datasets used in this study on http://TBA. version:1
arxiv-1701-05011 | Assessing User Expertise in Spoken Dialog System Interactions | http://arxiv.org/abs/1701.05011 | id:1701.05011 author:Eugénio Ribeiro, Fernando Batista, Isabel Trancoso, José Lopes, Ricardo Ribeiro, David Martins de Matos category:cs.CL H.1.2; I.2.1; I.2.7  published:2017-01-18 summary:Identifying the level of expertise of its users is important for a system since it can lead to a better interaction through adaptation techniques. Furthermore, this information can be used in offline processes of root cause analysis. However, not much effort has been put into automatically identifying the level of expertise of an user, especially in dialog-based interactions. In this paper we present an approach based on a specific set of task related features. Based on the distribution of the features among the two classes - Novice and Expert - we used Random Forests as a classification approach. Furthermore, we used a Support Vector Machine classifier, in order to perform a result comparison. By applying these approaches on data from a real system, Let's Go, we obtained preliminary results that we consider positive, given the difficulty of the task and the lack of competing approaches for comparison. version:1
arxiv-1701-05004 | Converting Cascade-Correlation Neural Nets into Probabilistic Generative Models | http://arxiv.org/abs/1701.05004 | id:1701.05004 author:Ardavan Salehi Nobandegani, Thomas R. Shultz category:q-bio.NC cs.AI cs.NE stat.ML  published:2017-01-18 summary:Humans are not only adept in recognizing what class an input instance belongs to (i.e., classification task), but perhaps more remarkably, they can imagine (i.e., generate) plausible instances of a desired class with ease, when prompted. Inspired by this, we propose a framework which allows transforming Cascade-Correlation Neural Networks (CCNNs) into probabilistic generative models, thereby enabling CCNNs to generate samples from a category of interest. CCNNs are a well-known class of deterministic, discriminative NNs, which autonomously construct their topology, and have been successful in giving accounts for a variety of psychological phenomena. Our proposed framework is based on a Markov Chain Monte Carlo (MCMC) method, called the Metropolis-adjusted Langevin algorithm, which capitalizes on the gradient information of the target distribution to direct its explorations towards regions of high probability, thereby achieving good mixing properties. Through extensive simulations, we demonstrate the efficacy of our proposed framework. version:1
arxiv-1701-05003 | Effective Multi-Query Expansions: Collaborative Deep Networks for Robust Landmark Retrieval | http://arxiv.org/abs/1701.05003 | id:1701.05003 author:Yang Wang, Xuemin Lin, Lin Wu, Wenjie Zhang category:cs.CV  published:2017-01-18 summary:Given a query photo issued by a user (q-user), the landmark retrieval is to return a set of photos with their landmarks similar to those of the query, while the existing studies on the landmark retrieval focus on exploiting geometries of landmarks for similarity matches between candidate photos and a query photo. We observe that the same landmarks provided by different users over social media community may convey different geometry information depending on the viewpoints and/or angles, and may subsequently yield very different results. In fact, dealing with the landmarks with \illshapes caused by the photography of q-users is often nontrivial and has seldom been studied. In this paper we propose a novel framework, namely multi-query expansions, to retrieve semantically robust landmarks by two steps. Firstly, we identify the top-$k$ photos regarding the latent topics of a query landmark to construct multi-query set so as to remedy its possible \illshape. For this purpose, we significantly extend the techniques of Latent Dirichlet Allocation. Then, motivated by the typical \emph{collaborative filtering} methods, we propose to learn a \emph{collaborative} deep networks based semantically, nonlinear and high-level features over the latent factor for landmark photo as the training set, which is formed by matrix factorization over \emph{collaborative} user-photo matrix regarding the multi-query set. The learned deep network is further applied to generate the features for all the other photos, meanwhile resulting into a compact multi-query set within such space. Extensive experiments are conducted on real-world social media data with both landmark photos together with their user information to show the superior performance over the existing methods. version:1
arxiv-1701-04968 | Multilayer Perceptron Algebra | http://arxiv.org/abs/1701.04968 | id:1701.04968 author:Zhao Peng category:stat.ML cs.LG  published:2017-01-18 summary:Artificial Neural Networks(ANN) has been phenomenally successful on various pattern recognition tasks. However, the design of neural networks rely heavily on the experience and intuitions of individual developers. In this article, the author introduces a mathematical structure called MLP algebra on the set of all Multilayer Perceptron Neural Networks(MLP), which can serve as a guiding principle to build MLPs accommodating to the particular data sets, and to build complex MLPs from simpler ones. version:1
arxiv-1701-04949 | A Deep Convolutional Auto-Encoder with Pooling - Unpooling Layers in Caffe | http://arxiv.org/abs/1701.04949 | id:1701.04949 author:Volodymyr Turchenko, Eric Chalmers, Artur Luczak category:cs.NE cs.CV cs.LG  published:2017-01-18 summary:This paper presents the development of several models of a deep convolutional auto-encoder in the Caffe deep learning framework and their experimental evaluation on the example of MNIST dataset. We have created five models of a convolutional auto-encoder which differ architecturally by the presence or absence of pooling and unpooling layers in the auto-encoder's encoder and decoder parts. Our results show that the developed models provide very good results in dimensionality reduction and unsupervised clustering tasks, and small classification errors when we used the learned internal code as an input of a supervised linear classifier and multi-layer perceptron. The best results were provided by a model where the encoder part contains convolutional and pooling layers, followed by an analogous decoder part with deconvolution and unpooling layers without the use of switch variables in the decoder part. The paper also discusses practical details of the creation of a deep convolutional auto-encoder in the very popular Caffe deep learning framework. We believe that our approach and results presented in this paper could help other researchers to build efficient deep neural network architectures in the future. version:1
arxiv-1701-04928 | Bringing Impressionism to Life with Neural Style Transfer in Come Swim | http://arxiv.org/abs/1701.04928 | id:1701.04928 author:Bhautik Joshi, Kristen Stewart, David Shapiro category:cs.CV I.3.3; I.4.0  published:2017-01-18 summary:Neural Style Transfer is a striking, recently-developed technique that uses neural networks to artistically redraw an image in the style of a source style image. This paper explores the use of this technique in a production setting, applying Neural Style Transfer to redraw key scenes in 'Come Swim' in the style of the impressionistic painting that inspired the film. We document how the technique can be driven within the framework of an iterative creative process to achieve a desired look, and propose a mapping of the broad parameter space to a key set of creative controls. We hope that this mapping can provide insights into priorities for future research. version:1
arxiv-1701-04925 | Action Recognition: From Static Datasets to Moving Robots | http://arxiv.org/abs/1701.04925 | id:1701.04925 author:Fahimeh Rezazadegan, Sareh Shirazi, Ben Upcroft, Michael Milford category:cs.RO cs.CV  published:2017-01-18 summary:Deep learning models have achieved state-of-the- art performance in recognizing human activities, but often rely on utilizing background cues present in typical computer vision datasets that predominantly have a stationary camera. If these models are to be employed by autonomous robots in real world environments, they must be adapted to perform independently of background cues and camera motion effects. To address these challenges, we propose a new method that firstly generates generic action region proposals with good potential to locate one human action in unconstrained videos regardless of camera motion and then uses action proposals to extract and classify effective shape and motion features by a ConvNet framework. In a range of experiments, we demonstrate that by actively proposing action regions during both training and testing, state-of-the-art or better performance is achieved on benchmarks. We show the outperformance of our approach compared to the state-of-the-art in two new datasets; one emphasizes on irrelevant background, the other highlights the camera motion. We also validate our action recognition method in an abnormal behavior detection scenario to improve workplace safety. The results verify a higher success rate for our method due to the ability of our system to recognize human actions regardless of environment and camera motion. version:1
arxiv-1701-04923 | Compression of Deep Neural Networks for Image Instance Retrieval | http://arxiv.org/abs/1701.04923 | id:1701.04923 author:Vijay Chandrasekhar, Jie Lin, Qianli Liao, Olivier Morère, Antoine Veillard, Lingyu Duan, Tomaso Poggio category:cs.CV  published:2017-01-18 summary:Image instance retrieval is the problem of retrieving images from a database which contain the same object. Convolutional Neural Network (CNN) based descriptors are becoming the dominant approach for generating {\it global image descriptors} for the instance retrieval problem. One major drawback of CNN-based {\it global descriptors} is that uncompressed deep neural network models require hundreds of megabytes of storage making them inconvenient to deploy in mobile applications or in custom hardware. In this work, we study the problem of neural network model compression focusing on the image instance retrieval task. We study quantization, coding, pruning and weight sharing techniques for reducing model size for the instance retrieval problem. We provide extensive experimental results on the trade-off between retrieval performance and model size for different types of networks on several data sets providing the most comprehensive study on this topic. We compress models to the order of a few MBs: two orders of magnitude smaller than the uncompressed models while achieving negligible loss in retrieval performance. version:1
arxiv-1701-04895 | Unknowable Manipulators: Social Network Curator Algorithms | http://arxiv.org/abs/1701.04895 | id:1701.04895 author:Samuel Albanie, Hillary Shakespeare, Tom Gunter category:cs.AI cs.SI stat.ML  published:2017-01-17 summary:For a social networking service to acquire and retain users, it must find ways to keep them engaged. By accurately gauging their preferences, it is able to serve them with the subset of available content that maximises revenue for the site. Without the constraints of an appropriate regulatory framework, we argue that a sufficiently sophisticated curator algorithm tasked with performing this process may choose to explore curation strategies that are detrimental to users. In particular, we suggest that such an algorithm is capable of learning to manipulate its users, for several qualitative reasons: 1. Access to vast quantities of user data combined with ongoing breakthroughs in the field of machine learning are leading to powerful but uninterpretable strategies for decision making at scale. 2. The availability of an effective feedback mechanism for assessing the short and long term user responses to curation strategies. 3. Techniques from reinforcement learning have allowed machines to learn automated and highly successful strategies at an abstract level, often resulting in non-intuitive yet nonetheless highly appropriate action selection. In this work, we consider the form that these strategies for user manipulation might take and scrutinise the role that regulation should play in the design of such systems. version:1
arxiv-1701-04889 | Efficient and Adaptive Linear Regression in Semi-Supervised Settings | http://arxiv.org/abs/1701.04889 | id:1701.04889 author:Abhishek Chakrabortty, Tianxi Cai category:stat.ME math.ST stat.ML stat.TH  published:2017-01-17 summary:We consider the linear regression problem under semi-supervised settings wherein the available data typically consists of: (i) a small or moderate sized 'labeled' data, and (ii) a much larger sized 'unlabeled' data. Such data arises naturally from settings where the outcome, unlike the covariates, is expensive to obtain, a frequent scenario in modern studies involving large databases like electronic medical records (EMR). Supervised estimators like the ordinary least squares (OLS) estimator utilize only the labeled data. It is often of interest to investigate if and when the unlabeled data can be exploited to improve estimation of the regression parameter in the adopted linear model. In this paper, we propose a class of 'Efficient and Adaptive Semi-Supervised Estimators' (EASE) to improve estimation efficiency. The EASE are two-step estimators adaptive to model mis-specification, leading to improved (optimal in some cases) efficiency under model mis-specification, and equal (optimal) efficiency under a linear model. This adaptive property, often unaddressed in the existing literature, is crucial for advocating 'safe' use of the unlabeled data. The construction of EASE primarily involves a flexible 'semi-non-parametric' imputation, including a smoothing step that works well even when the number of covariates is not small; and a follow up 'refitting' step along with a cross-validation (CV) strategy both of which have useful practical as well as theoretical implications towards addressing two important issues: under-smoothing and over-fitting. We establish asymptotic results including consistency, asymptotic normality and the adaptive properties of EASE. We also provide influence function expansions and a 'double' CV strategy for inference. The results are further validated through extensive simulations, followed by application to an EMR study on auto-immunity. version:1
arxiv-1701-04862 | Towards Principled Methods for Training Generative Adversarial Networks | http://arxiv.org/abs/1701.04862 | id:1701.04862 author:Martin Arjovsky, Léon Bottou category:stat.ML cs.LG  published:2017-01-17 summary:The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them. version:1
arxiv-1701-02797 | Full-reference image quality assessment-based B-mode ultrasound image similarity measure | http://arxiv.org/abs/1701.02797 | id:1701.02797 author:Kele Xu, Xi Liu, Hengxing Cai, Zhifeng Gao category:cs.CV  published:2017-01-10 summary:During the last decades, the number of new full-reference image quality assessment algorithms has been increasing drastically. Yet, despite of the remarkable progress that has been made, the medical ultrasound image similarity measurement remains largely unsolved due to a high level of speckle noise contamination. Potential applications of the ultrasound image similarity measurement seem evident in several aspects. To name a few, ultrasound imaging quality assessment, abnormal function region detection, etc. In this paper, a comparative study was made on full-reference image quality assessment methods for ultrasound image visual structural similarity measure. Moreover, based on the image similarity index, a generic ultrasound motion tracking re-initialization framework is given in this work. The experiments are conducted on synthetic data and real-ultrasound liver data and the results demonstrate that, with proposed similarity-based tracking re-initialization, the mean error of landmarks tracking can be decreased from 2 mm to about 1.5 mm in the ultrasound liver sequence. version:2
arxiv-1701-04831 | On the Equivalence of Restricted Boltzmann Machines and Tensor Network States | http://arxiv.org/abs/1701.04831 | id:1701.04831 author:Jing Chen, Song Cheng, Haidong Xie, Lei Wang, Tao Xiang category:cond-mat.str-el quant-ph stat.ML  published:2017-01-17 summary:Restricted Boltzmann machine (RBM) is one of the fundamental building blocks of deep learning. RBM finds wide applications in dimensional reduction, feature extraction, and recommender systems via modeling the probability distributions of a variety of input data including natural images, speech signals, and customer ratings, etc. We build a bridge between RBM and tensor network states (TNS) widely used in quantum many-body physics research. We devise efficient algorithms to translate an RBM into the commonly used TNS. Conversely, we give sufficient and necessary conditions to determine whether a TNS can be transformed into an RBM of given architectures. Revealing these general and constructive connections can cross-fertilize both deep learning and quantum-many body physics. Notably, by exploiting the entanglement entropy bound of TNS, we can rigorously quantify the expressive power of RBM on complex datasets. Insights into TNS and its entanglement capacity can guide the design of more powerful deep learning architectures. On the other hand, RBM can represent quantum many-body states with fewer parameters compared to TNS, which may allow more efficient classical simulations. version:1
arxiv-1701-04783 | Joint Deep Modeling of Users and Items Using Reviews for Recommendation | http://arxiv.org/abs/1701.04783 | id:1701.04783 author:Lei Zheng, Vahid Noroozi, Philip S. Yu category:cs.LG cs.IR  published:2017-01-17 summary:A large amount of information exists in reviews written by users. This source of information has been ignored by most of the current recommender systems while it can potentially alleviate the sparsity problem and improve the quality of recommendations. In this paper, we present a deep model to learn item properties and user behaviors jointly from review text. The proposed model, named Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel neural networks coupled in the last layers. One of the networks focuses on learning user behaviors exploiting reviews written by the user, and the other one learns item properties from the reviews written for the item. A shared layer is introduced on the top to couple these two networks together. The shared layer enables latent factors learned for users and items to interact with each other in a manner similar to factorization machine techniques. Experimental results demonstrate that DeepCoNN significantly outperforms all baseline recommender systems on a variety of datasets. version:1
arxiv-1701-04769 | Complex Event Recognition from Images with Few Training Examples | http://arxiv.org/abs/1701.04769 | id:1701.04769 author:Unaiza Ahsan, Chen Sun, James Hays, Irfan Essa category:cs.CV  published:2017-01-17 summary:We propose to leverage concept-level representations for complex event recognition in photographs given limited training examples. We introduce a novel framework to discover event concept attributes from the web and use that to extract semantic features from images and classify them into social event categories with few training examples. Discovered concepts include a variety of objects, scenes, actions and event sub-types, leading to a discriminative and compact representation for event images. Web images are obtained for each discovered event concept and we use (pretrained) CNN features to train concept classifiers. Extensive experiments on challenging event datasets demonstrate that our proposed method outperforms several baselines using deep CNN features directly in classifying images into events with limited training examples. We also demonstrate that our method achieves the best overall accuracy on a dataset with unseen event categories using a single training example. version:1
arxiv-1701-04752 | 3D Reconstruction of Simple Objects from A Single View Silhouette Image | http://arxiv.org/abs/1701.04752 | id:1701.04752 author:Xinhan Di, Pengqian Yu category:cs.CV  published:2017-01-17 summary:While recent deep neural networks have achieved promising results for 3D reconstruction from a single-view image, these rely on the availability of RGB textures in images and extra information as supervision. In this work, we propose novel stacked hierarchical networks and an end to end training strategy to tackle a more challenging task for the first time, 3D reconstruction from a single-view 2D silhouette image. We demonstrate that our model is able to conduct 3D reconstruction from a single-view silhouette image both qualitatively and quantitatively. Evaluation is performed using Shapenet for the single-view reconstruction and results are presented in comparison with a single network, to highlight the improvements obtained with the proposed stacked networks and the end to end training strategy. Furthermore, 3D re- construction in forms of IoU is compared with the state of art 3D reconstruction from a single-view RGB image, and the proposed model achieves higher IoU than the state of art of reconstruction from a single view RGB image. version:1
arxiv-1701-04743 | Computing Egomotion with Local Loop Closures for Egocentric Videos | http://arxiv.org/abs/1701.04743 | id:1701.04743 author:Suvam Patra, Himanshu Aggarwal, Himani Arora, Chetan Arora, Subhashis Banerjee category:cs.CV  published:2017-01-17 summary:Finding the camera pose is an important step in many egocentric video applications. It has been widely reported that, state of the art SLAM algorithms fail on egocentric videos. In this paper, we propose a robust method for camera pose estimation, designed specifically for egocentric videos. In an egocentric video, the camera views the same scene point multiple times as the wearer's head sweeps back and forth. We use this specific motion profile to perform short loop closures aligned with wearer's footsteps. For egocentric videos, depth estimation is usually noisy. In an important departure, we use 2D computations for rotation averaging which do not rely upon depth estimates. The two modification results in much more stable algorithm as is evident from our experiments on various egocentric video datasets for different egocentric applications. The proposed algorithm resolves a long standing problem in egocentric vision and unlocks new usage scenarios for future applications. version:1
arxiv-1701-04739 | Summoning Demons: The Pursuit of Exploitable Bugs in Machine Learning | http://arxiv.org/abs/1701.04739 | id:1701.04739 author:Rock Stevens, Octavian Suciu, Andrew Ruef, Sanghyun Hong, Michael Hicks, Tudor Dumitraş category:cs.CR cs.LG  published:2017-01-17 summary:Governments and businesses increasingly rely on data analytics and machine learning (ML) for improving their competitive edge in areas such as consumer satisfaction, threat intelligence, decision making, and product efficiency. However, by cleverly corrupting a subset of data used as input to a target's ML algorithms, an adversary can perturb outcomes and compromise the effectiveness of ML technology. While prior work in the field of adversarial machine learning has studied the impact of input manipulation on correct ML algorithms, we consider the exploitation of bugs in ML implementations. In this paper, we characterize the attack surface of ML programs, and we show that malicious inputs exploiting implementation bugs enable strictly more powerful attacks than the classic adversarial machine learning techniques. We propose a semi-automated technique, called steered fuzzing, for exploring this attack surface and for discovering exploitable bugs in machine learning programs, in order to demonstrate the magnitude of this threat. As a result of our work, we responsibly disclosed five vulnerabilities, established three new CVE-IDs, and illuminated a common insecure practice across many machine learning systems. Finally, we outline several research directions for further understanding and mitigating this threat. version:1
arxiv-1701-04722 | Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks | http://arxiv.org/abs/1701.04722 | id:1701.04722 author:Lars Mescheder, Sebastian Nowozin, Andreas Geiger category:cs.LG  published:2017-01-17 summary:Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement. version:1
arxiv-1701-03038 | Decoding with Finite-State Transducers on GPUs | http://arxiv.org/abs/1701.03038 | id:1701.03038 author:Arturo Argueta, David Chiang category:cs.CL cs.DC  published:2017-01-11 summary:Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST. version:2
arxiv-1701-04693 | Incremental Learning for Robot Perception through HRI | http://arxiv.org/abs/1701.04693 | id:1701.04693 author:Sepehr Valipour, Camilo Perez, Martin Jagersand category:cs.RO cs.HC cs.LG  published:2017-01-17 summary:Scene understanding and object recognition is a difficult to achieve yet crucial skill for robots. Recently, Convolutional Neural Networks (CNN), have shown success in this task. However, there is still a gap between their performance on image datasets and real-world robotics scenarios. We present a novel paradigm for incrementally improving a robot's visual perception through active human interaction. In this paradigm, the user introduces novel objects to the robot by means of pointing and voice commands. Given this information, the robot visually explores the object and adds images from it to re-train the perception module. Our base perception module is based on recent development in object detection and recognition using deep learning. Our method leverages state of the art CNNs from off-line batch learning, human guidance, robot exploration and incremental on-line learning. version:1
arxiv-1701-04674 | Human perception in computer vision | http://arxiv.org/abs/1701.04674 | id:1701.04674 author:Ron Dekel category:cs.CV q-bio.NC  published:2017-01-17 summary:Computer vision has made remarkable progress in recent years. Deep neural network (DNN) models optimized to identify objects in images exhibit unprecedented task-trained accuracy and, remarkably, some generalization ability: new visual problems can now be solved more easily based on previous learning. Biological vision (learned in life and through evolution) is also accurate and general-purpose. Is it possible that these different learning regimes converge to similar problem-dependent optimal computations? We therefore asked whether the human system-level computation of visual perception has DNN correlates and considered several anecdotal test cases. We found that perceptual sensitivity to image changes has DNN mid-computation correlates, while sensitivity to segmentation, crowding and shape has DNN end-computation correlates. Our results quantify the applicability of using DNN computation to estimate perceptual loss, and are consistent with the fascinating theoretical view that properties of human perception are a consequence of architecture-independent visual learning. version:1
arxiv-1701-04658 | Convolutional Oriented Boundaries: From Image Segmentation to High-Level Tasks | http://arxiv.org/abs/1701.04658 | id:1701.04658 author:Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Pablo Arbeláez, Luc Van Gool category:cs.CV  published:2017-01-17 summary:We present Convolutional Oriented Boundaries (COB), which produces multiscale oriented contours and region hierarchies starting from generic image classification Convolutional Neural Networks (CNNs). COB is computationally efficient, because it requires a single CNN forward pass for multi-scale contour detection and it uses a novel sparse boundary representation for hierarchical segmentation; it gives a significant leap in performance over the state-of-the-art, and it generalizes very well to unseen categories and datasets. Particularly, we show that learning to estimate not only contour strength but also orientation provides more accurate results. We perform extensive experiments for low-level applications on BSDS, PASCAL Context, PASCAL Segmentation, and NYUD to evaluate boundary detection performance, showing that COB provides state-of-the-art contours and region hierarchies in all datasets. We also evaluate COB on high-level tasks when coupled with multiple pipelines for object proposals, semantic contours, semantic segmentation, and object detection on various databases (MS-COCO, SBD, PASCAL VOC'07), showing that COB also improves the results for all tasks. version:1
arxiv-1701-04653 | Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods | http://arxiv.org/abs/1701.04653 | id:1701.04653 author:Marzieh Saeidi, Alessandro Venerandi, Licia Capra, Sebastian Riedel category:cs.CL cs.SI  published:2017-01-17 summary:In this paper, we investigate whether text from a Community Question Answering (QA) platform can be used to predict and describe real-world attributes. We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our results to the ones obtained from Twitter microblogs. Outcomes show that the correlation between the predicted demographic attributes using text from Yahoo! Answers discussions and the observed demographic attributes can reach an average Pearson correlation coefficient of \r{ho} = 0.54, slightly higher than the predictions obtained using Twitter data. Our qualitative analysis indicates that there is semantic relatedness between the highest correlated terms extracted from both datasets and their relative demographic attributes. Furthermore, the correlations highlight the different natures of the information contained in Yahoo! Answers and Twitter. While the former seems to offer a more encyclopedic content, the latter provides information related to the current sociocultural aspects or phenomena. version:1
arxiv-1701-04284 | Hierarchical Salient Object Detection for Assisted Grasping | http://arxiv.org/abs/1701.04284 | id:1701.04284 author:Dominik Alexander Klein, Boris Illing, Bastian Gaspers, Dirk Schulz, Armin Bernd Cremers category:cs.CV cs.RO  published:2017-01-16 summary:Visual scene decomposition into semantic entities is one of the major challenges when creating a reliable object grasping system. Recently, we introduced a bottom-up hierarchical clustering approach which is able to segment objects and parts in a scene. In this paper, we introduce a transform from such a segmentation into a corresponding, hierarchical saliency function. In comprehensive experiments we demonstrate its ability to detect salient objects in a scene. Furthermore, this hierarchical saliency defines a most salient corresponding region (scale) for every point in an image. Based on this, an easy-to-use pick and place manipulation system was developed and tested exemplarily. version:2
arxiv-1701-02898 | Modeling Retinal Ganglion Cell Population Activity with Restricted Boltzmann Machines | http://arxiv.org/abs/1701.02898 | id:1701.02898 author:Matteo Zanotto, Riccardo Volpi, Alessandro Maccione, Luca Berdondini, Diego Sona, Vittorio Murino category:cs.CV q-bio.NC  published:2017-01-11 summary:The retina is a complex nervous system which encodes visual stimuli before higher order processing occurs in the visual cortex. In this study we evaluated whether information about the stimuli received by the retina can be retrieved from the firing rate distribution of Retinal Ganglion Cells (RGCs), exploiting High-Density 64x64 MEA technology. To this end, we modeled the RGC population activity using mean-covariance Restricted Boltzmann Machines, latent variable models capable of learning the joint distribution of a set of continuous observed random variables and a set of binary unobserved random units. The idea was to figure out if binary latent states encode the regularities associated to different visual stimuli, as modes in the joint distribution. We measured the goodness of mcRBM encoding by calculating the Mutual Information between the latent states and the stimuli shown to the retina. Results show that binary states can encode the regularities associated to different stimuli, using both gratings and natural scenes as stimuli. We also discovered that hidden variables encode interesting properties of retinal activity, interpreted as population receptive fields. We further investigated the ability of the model to learn different modes in population activity by comparing results associated to a retina in normal conditions and after pharmacologically blocking GABA receptors (GABAC at first, and then also GABAA and GABAB). As expected, Mutual Information tends to decrease if we pharmacologically block receptors. We finally stress that the computational method described in this work could potentially be applied to any kind of neural data obtained through MEA technology, though different techniques should be applied to interpret the results. version:2
arxiv-1701-04600 | Faster K-Means Cluster Estimation | http://arxiv.org/abs/1701.04600 | id:1701.04600 author:Siddhesh Khandelwal, Amit Awekar category:cs.LG cs.IR  published:2017-01-17 summary:There has been considerable work on improving popular clustering algorithm `K-means' in terms of mean squared error (MSE) and speed, both. However, most of the k-means variants tend to compute distance of each data point to each cluster centroid for every iteration. We propose a fast heuristic to overcome this bottleneck with only marginal increase in MSE. We observe that across all iterations of K-means, a data point changes its membership only among a small subset of clusters. Our heuristic predicts such clusters for each data point by looking at nearby clusters after the first iteration of k-means. We augment well known variants of k-means with our heuristic to demonstrate effectiveness of our heuristic. For various synthetic and real-world datasets, our heuristic achieves speed-up of up-to 3 times when compared to efficient variants of k-means. version:1
arxiv-1701-04568 | Image Generation and Editing with Variational Info Generative AdversarialNetworks | http://arxiv.org/abs/1701.04568 | id:1701.04568 author:Mahesh Gorijala, Ambedkar Dukkipati category:cs.CV  published:2017-01-17 summary:Recently there has been an enormous interest in generative models for images in deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and Variational Auto-Encoder (VAE) have surfaced as two most prominent and popular models. While VAEs tend to produce excellent reconstructions but blurry samples, GANs generate sharp but slightly distorted images. In this paper we propose a new model called Variational InfoGAN (ViGAN). Our aim is two fold: (i) To generated new images conditioned on visual descriptions, and (ii) modify the image, by fixing the latent representation of image and varying the visual description. We evaluate our model on Labeled Faces in the Wild (LFW), celebA and a modified version of MNIST datasets and demonstrate the ability of our model to generate new images as well as to modify a given image by changing attributes. version:1
arxiv-1701-04540 | Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation | http://arxiv.org/abs/1701.04540 | id:1701.04540 author:Joy Egede, Michel Valstar, Brais Martinez category:cs.CV  published:2017-01-17 summary:Automatic continuous time, continuous value assessment of a patient's pain from face video is highly sought after by the medical profession. Despite the recent advances in deep learning that attain impressive results in many domains, pain estimation risks not being able to benefit from this due to the difficulty in obtaining data sets of considerable size. In this work we propose a combination of hand-crafted and deep-learned features that makes the most of deep learning techniques in small sample settings. Encoding shape, appearance, and dynamics, our method significantly outperforms the current state of the art, attaining a RMSE error of less than 1 point on a 16-level pain scale, whilst simultaneously scoring a 67.3% Pearson correlation coefficient between our predicted pain level time series and the ground truth. version:1
arxiv-1701-04532 | Multi-view Regularized Gaussian Processes | http://arxiv.org/abs/1701.04532 | id:1701.04532 author:Qiuyang Liu, Shiliang Sun category:stat.ML  published:2017-01-17 summary:Gaussian processes (GPs) have been proven to be powerful tools in various areas of machine learning. However, there are very few applications of GPs in the scenario of multi-view learning. In this paper, we present a new GP model for multi-view learning. Unlike existing methods, it combines multiple views by regularizing marginal likelihood with the consistency among the posterior distributions of latent functions from different views. Moreover, we give a general point selection scheme for multi-view learning and improve the proposed model by this criterion. Experimental results on multiple real world data sets have verified the effectiveness of the proposed model and witnessed the performance improvement through employing this novel point selection scheme. version:1
arxiv-1701-04520 | Systematic study of color spaces and components for the segmentation of sky/cloud images | http://arxiv.org/abs/1701.04520 | id:1701.04520 author:Soumyabrata Dev, Yee Hui Lee, Stefan Winkler category:cs.CV  published:2017-01-17 summary:Sky/cloud imaging using ground-based Whole Sky Imagers (WSI) is a cost-effective means to understanding cloud cover and weather patterns. The accurate segmentation of clouds in these images is a challenging task, as clouds do not possess any clear structure. Several algorithms using different color models have been proposed in the literature. This paper presents a systematic approach for the selection of color spaces and components for optimal segmentation of sky/cloud images. Using mainly principal component analysis (PCA) and fuzzy clustering for evaluation, we identify the most suitable color components for this task. version:1
arxiv-1701-04518 | Towards prediction of rapid intensification in tropical cyclones with recurrent neural networks | http://arxiv.org/abs/1701.04518 | id:1701.04518 author:Rohitash Chandra category:cs.LG stat.AP  published:2017-01-17 summary:The problem where a tropical cyclone intensifies dramatically within a short period of time is known as rapid intensification. This has been one of the major challenges for tropical weather forecasting. Recurrent neural networks have been promising for time series problems which makes them appropriate for rapid intensification. In this paper, recurrent neural networks are used to predict rapid intensification cases of tropical cyclones from the South Pacific and South Indian Ocean regions. A class imbalanced problem is encountered which makes it very challenging to achieve promising performance. A simple strategy was proposed to include more positive cases for detection where the false positive rate was slightly improved. The limitations of building an efficient system remains due to the challenges of addressing the class imbalance problem encountered for rapid intensification prediction. This motivates further research in using innovative machine learning methods. version:1
arxiv-1701-04516 | On The Construction of Extreme Learning Machine for Online and Offline One-Class Classification - An Expanded Toolbox | http://arxiv.org/abs/1701.04516 | id:1701.04516 author:Chandan Gautam, Aruna Tiwari, Qian Leng category:cs.LG stat.ML  published:2017-01-17 summary:One-Class Classification (OCC) has been prime concern for researchers and effectively employed in various disciplines. But, traditional methods based one-class classifiers are very time consuming due to its iterative process and various parameters tuning. In this paper, we present six OCC methods based on extreme learning machine (ELM) and Online Sequential ELM (OSELM). Our proposed classifiers mainly lie in two categories: reconstruction based and boundary based, which supports both types of learning viz., online and offline learning. Out of various proposed methods, four are offline and remaining two are online methods. Out of four offline methods, two methods perform random feature mapping and two methods perform kernel feature mapping. Kernel feature mapping based approaches have been tested with RBF kernel and online version of one-class classifiers are tested with both types of nodes viz., additive and RBF. It is well known fact that threshold decision is a crucial factor in case of OCC, so, three different threshold deciding criteria have been employed so far and analyses the effectiveness of one threshold deciding criteria over another. Further, these methods are tested on two artificial datasets to check there boundary construction capability and on eight benchmark datasets from different discipline to evaluate the performance of the classifiers. Our proposed classifiers exhibit better performance compared to ten traditional one-class classifiers and ELM based two one-class classifiers. Through proposed one-class classifiers, we intend to expand the functionality of the most used toolbox for OCC i.e. DD toolbox. All of our methods are totally compatible with all the present features of the toolbox. version:1
arxiv-1701-04508 | Online Learning with Regularized Kernel for One-class Classification | http://arxiv.org/abs/1701.04508 | id:1701.04508 author:Chandan Gautam, Aruna Tiwari, Sundaram Suresh, Kapil Ahuja category:cs.LG  published:2017-01-17 summary:This paper presents an online learning with regularized kernel based one-class extreme learning machine (ELM) classifier and is referred as online RK-OC-ELM. The baseline kernel hyperplane model considers whole data in a single chunk with regularized ELM approach for offline learning in case of one-class classification (OCC). Further, the basic hyper plane model is adapted in an online fashion from stream of training samples in this paper. Two frameworks viz., boundary and reconstruction are presented to detect the target class in online RKOC-ELM. Boundary framework based one-class classifier consists of single node output architecture and classifier endeavors to approximate all data to any real number. However, one-class classifier based on reconstruction framework is an autoencoder architecture, where output nodes are identical to input nodes and classifier endeavor to reconstruct input layer at the output layer. Both these frameworks employ regularized kernel ELM based online learning and consistency based model selection has been employed to select learning algorithm parameters. The performance of online RK-OC-ELM has been evaluated on standard benchmark datasets as well as on artificial datasets and the results are compared with existing state-of-the art one-class classifiers. The results indicate that the online learning one-class classifier is slightly better or same as batch learning based approaches. As, base classifier used for the proposed classifiers are based on the ELM, hence, proposed classifiers would also inherit the benefit of the base classifier i.e. it will perform faster computation compared to traditional autoencoder based one-class classifier. version:1
arxiv-1701-04503 | Deep Learning for Computational Chemistry | http://arxiv.org/abs/1701.04503 | id:1701.04503 author:Garrett B. Goh, Nathan O. Hodas, Abhinav Vishnu category:stat.ML cs.LG physics.chem-ph  published:2017-01-17 summary:The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry. Yet almost two decades later, we are now seeing a resurgence of interest in deep learning, a machine learning algorithm based on multilayer neural networks. Within the last few years, we have seen the transformative impact of deep learning in many domains, particularly in speech recognition and computer vision, to the extent that the majority of expert practitioners in those field are now regularly eschewing prior established models in favor of deep learning models. In this review, we provide an introductory overview into the theory of deep neural networks and their unique properties that distinguish them from traditional machine learning algorithms used in cheminformatics. By providing an overview of the variety of emerging applications of deep neural networks, we highlight its ubiquity and broad applicability to a wide range of challenges in the field, including QSAR, virtual screening, protein structure prediction, quantum chemistry, materials design and property prediction. In reviewing the performance of deep neural networks, we observed a consistent outperformance against non-neural networks state-of-the-art models across disparate research topics, and deep neural network based models often exceeded the "glass ceiling" expectations of their respective tasks. Coupled with the maturity of GPU-accelerated computing for training deep neural networks and the exponential growth of chemical data on which to train these networks on, we anticipate that deep learning algorithms will be a valuable tool for computational chemistry. version:1
arxiv-1701-04489 | Towards a New Interpretation of Separable Convolutions | http://arxiv.org/abs/1701.04489 | id:1701.04489 author:Tapabrata Ghosh category:cs.LG stat.ML  published:2017-01-16 summary:In recent times, the use of separable convolutions in deep convolutional neural network architectures has been explored. Several researchers, most notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in their deep architectures and have demonstrated state of the art or close to state of the art performance. However, the underlying mechanism of action of separable convolutions are still not fully understood. Although their mathematical definition is well understood as a depthwise convolution followed by a pointwise convolution, deeper interpretations such as the extreme Inception hypothesis (Chollet, 2016) have failed to provide a thorough explanation of their efficacy. In this paper, we propose a hybrid interpretation that we believe is a better model for explaining the efficacy of separable convolutions. version:1
arxiv-1701-02025 | Multi-level Representations for Fine-Grained Typing of Knowledge Base Entities | http://arxiv.org/abs/1701.02025 | id:1701.02025 author:Yadollah Yaghoobzadeh, Hinrich Schütze category:cs.CL cs.AI  published:2017-01-08 summary:Entities are essential elements of natural language. In this paper, we present methods for learning multi-level representations of entities on three complementary levels: character (character patterns in entity names extracted, e.g., by neural networks), word (embeddings of words in entity names) and entity (entity embeddings). We investigate state-of-the-art learning methods on each level and find large differences, e.g., for deep learning models, traditional ngram features and the subword model of fasttext (Bojanowski et al., 2016) on the character level; for word2vec (Mikolov et al., 2013) on the word level; and for the order-aware model wang2vec (Ling et al., 2015a) on the entity level. We confirm experimentally that each level of representation contributes complementary information and a joint representation of all three levels improves the existing embedding based baseline for fine-grained entity typing by a large margin. Additionally, we show that adding information from entity descriptions further improves multi-level representations of entities. version:2
arxiv-1701-04465 | The Incredible Shrinking Neural Network: New Perspectives on Learning Representations Through The Lens of Pruning | http://arxiv.org/abs/1701.04465 | id:1701.04465 author:Nikolas Wolfe, Aditya Sharma, Lukas Drude, Bhiksha Raj category:cs.NE cs.LG  published:2017-01-16 summary:How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many different techniques have been proposed to address this problem. In general, this is motivated by the idea that smaller models typically lead to better generalization. At the same time, the decision of what to prune and when to prune necessarily forces us to confront our assumptions about how neural networks actually learn to represent patterns in data. In this work we set out to test several long-held hypotheses about neural network learning representations and numerical approaches to pruning. To accomplish this we first reviewed the historical literature and derived a novel algorithm to prune whole neurons (as opposed to the traditional method of pruning weights) from optimally trained networks using a second-order Taylor method. We then set about testing the performance of our algorithm and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method based on the Skeletonization algorithm and an exhaustive brute-force serial pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. Our error analysis led us to question the validity of many widely-held assumptions behind pruning algorithms in general and the trade-offs we often make in the interest of reducing computational complexity. We discovered that there is a straightforward way, however expensive, to serially prune 40-70% of the neurons in a trained network with minimal effect on the learning representation and without any re-training. version:1
arxiv-1701-04455 | High-Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transition | http://arxiv.org/abs/1701.04455 | id:1701.04455 author:David Gamarnik, Ilias Zadik category:stat.ML math.PR math.ST stat.TH  published:2017-01-16 summary:We consider a sparse linear regression model Y=X\beta^{*}+W where X has a Gaussian entries, W is the noise vector with mean zero Gaussian entries, and \beta^{*} is a binary vector with support size (sparsity) k. Using a novel conditional second moment method we obtain a tight up to a multiplicative constant approximation of the optimal squared error \min_{\beta}\ Y-X\beta\ _{2}, where the minimization is over all k-sparse binary vectors \beta. The approximation reveals interesting structural properties of the underlying regression problem. In particular, a) We establish that n^*=2k\log p/\log (2k/\sigma^{2}+1) is a phase transition point with the following "all-or-nothing" property. When n exceeds n^{*}, (2k)^{-1}\ \beta_{2}-\beta^*\ _0\approx 0, and when n is below n^{*}, (2k)^{-1}\ \beta_{2}-\beta^*\ _0\approx 1, where \beta_2 is the optimal solution achieving the smallest squared error. With this we prove that n^{*} is the asymptotic threshold for recovering \beta^* information theoretically. b) We compute the squared error for an intermediate problem \min_{\beta}\ Y-X\beta\ _{2} where minimization is restricted to vectors \beta with \ \beta-\beta^{*}\ _0=2k \zeta, for \zeta\in [0,1]. We show that a lower bound part \Gamma(\zeta) of the estimate, which corresponds to the estimate based on the first moment method, undergoes a phase transition at three different thresholds, namely n_{\text{inf,1}}=\sigma^2\log p, which is information theoretic bound for recovering \beta^* when k=1 and \sigma is large, then at n^{*} and finally at n_{\text{LASSO/CS}}. c) We establish a certain Overlap Gap Property (OGP) on the space of all binary vectors \beta when n\le ck\log p for sufficiently small constant c. We conjecture that OGP is the source of algorithmic hardness of solving the minimization problem \min_{\beta}\ Y-X\beta\ _{2} in the regime n<n_{\text{LASSO/CS}}. version:1
arxiv-1701-04389 | Real-Time Energy Disaggregation of a Distribution Feeder's Demand Using Online Learning | http://arxiv.org/abs/1701.04389 | id:1701.04389 author:Gregory S. Ledva, Laura Balzano, Johanna L. Mathieu category:stat.ML math.OC  published:2017-01-16 summary:Though distribution system operators have been adding more sensors to their networks, they still often lack an accurate real-time picture of the behavior of distributed energy resources such as demand responsive electric loads and residential solar generation. Such information could improve system reliability, economic efficiency, and environmental impact. Rather than installing additional, costly sensing and communication infrastructure to obtain additional real-time information, it may be possible to use existing sensing capabilities and leverage knowledge about the system to reduce the need for new infrastructure. In this paper, we disaggregate a distribution feeder's demand measurements into two components: 1) the demand of a population of air conditioners, and 2) the demand of the remaining loads connected to the feeder. We use an online learning algorithm, Dynamic Fixed Share (DFS), that uses the real-time distribution feeder measurements as well as models generated from historical building- and device-level data. We develop two implementations of the algorithm and conduct simulations using real demand data from households and commercial buildings to investigate the effectiveness of the algorithm. Case studies demonstrate that DFS can effectively perform online disaggregation and the choice and construction of models included in the algorithm affects its accuracy, which is comparable to that of a set of Kalman filters. version:1
arxiv-1701-04383 | Automatic Knot Adjustment Using Dolphin Echolocation Algorithm for B-Spline Curve Approximation | http://arxiv.org/abs/1701.04383 | id:1701.04383 author:Hasan Ali Akyürek, Erkan Ülker, Barış Koçer category:cs.GR cs.NE  published:2017-01-16 summary:In this paper, a new approach to solve the cubic B-spline curve fitting problem is presented based on a meta-heuristic algorithm called " dolphin echolocation ". The method minimizes the proximity error value of the selected nodes that measured using the least squares method and the Euclidean distance method of the new curve generated by the reverse engineering. The results of the proposed method are compared with the genetic algorithm. As a result, this new method seems to be successful. version:1
arxiv-1701-00481 | Stochastic Variance-reduced Gradient Descent for Low-rank Matrix Recovery from Linear Measurements | http://arxiv.org/abs/1701.00481 | id:1701.00481 author:Xiao Zhang, Lingxiao Wang, Quanquan Gu category:stat.ML  published:2017-01-02 summary:We study the problem of estimating low-rank matrices from linear measurements (a.k.a., matrix sensing) through nonconvex optimization. We propose an efficient stochastic variance reduced gradient descent algorithm to solve a nonconvex optimization problem of matrix sensing. Our algorithm is applicable to both noisy and noiseless settings. In the case with noisy observations, we prove that our algorithm converges to the unknown low-rank matrix at a linear rate up to the minimax optimal statistical error. And in the noiseless setting, our algorithm is guaranteed to linearly converge to the unknown low-rank matrix and achieves exact recovery with optimal sample complexity. Most notably, the overall computational complexity of our proposed algorithm, which is defined as the iteration complexity times per iteration time complexity, is lower than the state-of-the-art algorithms based on gradient descent. Experiments on synthetic data corroborate the superiority of the proposed algorithm over the state-of-the-art algorithms. version:2
arxiv-1701-04355 | Classification of MRI data using Deep Learning and Gaussian Process-based Model Selection | http://arxiv.org/abs/1701.04355 | id:1701.04355 author:Hadrien Bertrand, Matthieu Perrot, Roberto Ardon, Isabelle Bloch category:cs.LG stat.ML  published:2017-01-16 summary:The classification of MRI images according to the anatomical field of view is a necessary task to solve when faced with the increasing quantity of medical images. In parallel, advances in deep learning makes it a suitable tool for computer vision problems. Using a common architecture (such as AlexNet) provides quite good results, but not sufficient for clinical use. Improving the model is not an easy task, due to the large number of hyper-parameters governing both the architecture and the training of the network, and to the limited understanding of their relevance. Since an exhaustive search is not tractable, we propose to optimize the network first by random search, and then by an adaptive search based on Gaussian Processes and Probability of Improvement. Applying this method on a large and varied MRI dataset, we show a substantial improvement between the baseline network and the final one (up to 20\% for the most difficult classes). version:1
arxiv-1701-04342 | Datenqualität in Regressionsproblemen | http://arxiv.org/abs/1701.04342 | id:1701.04342 author:Wolfgang Doneit, Ralf Mikut, Markus Reischl category:stat.ML  published:2017-01-16 summary:Regression models are increasingly built using datasets which do not follow a design of experiment. Instead, the data is e.g. gathered by an automated monitoring of a technical system. As a consequence, already the input data represents phenomena of the system and violates statistical assumptions of distributions. The input data can show correlations, clusters or other patterns. Further, the distribution of input data influences the reliability of regression models. We propose criteria to quantify typical phenomena of input data for regression and show their suitability with simulated benchmark datasets. ----- Regressionen werden zunehmend auf Datens\"atzen angewendet, deren Eingangsvektoren nicht durch eine statistische Versuchsplanung festgelegt wurden. Stattdessen werden die Daten beispielsweise durch die passive Beobachtung technischer Systeme gesammelt. Damit bilden bereits die Eingangsdaten Ph\"anomene des Systems ab und widersprechen statistischen Verteilungsannahmen. Die Verteilung der Eingangsdaten hat Einfluss auf die Zuverl\"assigkeit eines Regressionsmodells. Wir stellen deshalb Bewertungskriterien f\"ur einige typische Ph\"anomene in Eingangsdaten von Regressionen vor und zeigen ihre Funktionalit\"at anhand simulierter Benchmarkdatens\"atze. version:1
arxiv-1701-04292 | Semantic classifier approach to document classification | http://arxiv.org/abs/1701.04292 | id:1701.04292 author:Piotr Borkowski, Krzysztof Ciesielski, Mieczysław A. Kłopotek category:cs.IR cs.CL  published:2017-01-16 summary:In this paper we propose a new document classification method, bridging discrepancies (so-called semantic gap) between the training set and the application sets of textual data. We demonstrate its superiority over classical text classification approaches, including traditional classifier ensembles. The method consists in combining a document categorization technique with a single classifier or a classifier ensemble (SEMCOM algorithm - Committee with Semantic Categorizer). version:1
arxiv-1701-04290 | Machine Translation Approaches and Survey for Indian Languages | http://arxiv.org/abs/1701.04290 | id:1701.04290 author:Nadeem Jadoon Khan, Waqas Anwar, Nadir Durrani category:cs.CL  published:2017-01-16 summary:In this study, we present an analysis regarding the performance of the state-of-art Phrase-based Statistical Machine Translation (SMT) on multiple Indian languages. We report baseline systems on several language pairs. The motivation of this study is to promote the development of SMT and linguistic resources for these language pairs, as the current state-of-the-art is quite bleak due to sparse data resources. The success of an SMT system is contingent on the availability of a large parallel corpus. Such data is necessary to reliably estimate translation probabilities. We report the performance of baseline systems translating from Indian languages (Bengali, Guajarati, Hindi, Malayalam, Punjabi, Tamil, Telugu and Urdu) into English with average 10% accurate results for all the language pairs. version:1
arxiv-1701-04271 | Fast Rates for Empirical Risk Minimization of Strict Saddle Problems | http://arxiv.org/abs/1701.04271 | id:1701.04271 author:Alon Gonen, Shai Shalev-Shwartz category:cs.LG  published:2017-01-16 summary:We derive bounds on the sample complexity of empirical risk minimization (ERM) in the context of minimizing non-convex risks that admit the strict saddle property. Recent progress in non-convex optimization has yielded efficient algorithms for minimizing such functions. Our results imply that these efficient algorithms are statistically stable and also generalize well. In particular, we derive fast rates which resemble the bounds that are often attained in the strongly convex setting. We specify our bounds to Principal Component Analysis and Independent Component Analysis. Our results and techniques may pave the way for statistical analyses of additional strict saddle problems. version:1
arxiv-1701-04256 | Automatic Spatial Context-Sensitive Cloud/Cloud-Shadow Detection in Multi-Source Multi-Spectral Earth Observation Images: AutoCloud+ | http://arxiv.org/abs/1701.04256 | id:1701.04256 author:Andrea Baraldi category:cs.CV  published:2017-01-16 summary:The proposed Earth observation (EO) based value adding system (EO VAS), hereafter identified as AutoCloud+, consists of an innovative EO image understanding system (EO IUS) design and implementation capable of automatic spatial context sensitive cloud/cloud shadow detection in multi source multi spectral (MS) EO imagery, whether or not radiometrically calibrated, acquired by multiple platforms, either spaceborne or airborne, including unmanned aerial vehicles (UAVs). It is worth mentioning that the same EO IUS architecture is suitable for a large variety of EO based value adding products and services, including: (i) low level image enhancement applications, such as automatic MS image topographic correction, co registration, mosaicking and compositing, (ii) high level MS image land cover (LC) and LC change (LCC) classification and (iii) content based image storage/retrieval in massive multi source EO image databases (big data mining). version:1
arxiv-1701-04249 | Geometric features for voxel-based surface recognition | http://arxiv.org/abs/1701.04249 | id:1701.04249 author:Dmitry Yarotsky category:cs.CV cs.LG  published:2017-01-16 summary:We introduce a library of geometric voxel features for CAD surface recognition/retrieval tasks. Our features include local versions of the intrinsic volumes (the usual 3D volume, surface area, integrated mean and Gaussian curvature) and a few closely related quantities. We also compute Haar wavelet and statistical distribution features by aggregating raw voxel features. We apply our features to object classification on the ESB data set and demonstrate accurate results with a small number of shallow decision trees. version:1
arxiv-1701-04238 | Thompson Sampling For Stochastic Bandits with Graph Feedback | http://arxiv.org/abs/1701.04238 | id:1701.04238 author:Aristide C. Y. Tossou, Christos Dimitrakakis, Devdatt Dubhashi category:cs.LG cs.AI  published:2017-01-16 summary:We present a novel extension of Thompson Sampling for stochastic sequential decision problems with graph feedback, even when the graph structure itself is unknown and/or changing. We provide theoretical guarantees on the Bayesian regret of the algorithm, linking its performance to the underlying properties of the graph. Thompson Sampling has the advantage of being applicable without the need to construct complicated upper confidence bounds for different problems. We illustrate its performance through extensive experimental results on real and simulated networks with graph feedback. More specifically, we tested our algorithms on power law, planted partitions and Erdo's-Renyi graphs, as well as on graphs derived from Facebook and Flixster data. These all show that our algorithms clearly outperform related methods that employ upper confidence bounds, even if the latter use more information about the graph. version:1
arxiv-1701-04224 | Auxiliary Multimodal LSTM for Audio-visual Speech Recognition and Lipreading | http://arxiv.org/abs/1701.04224 | id:1701.04224 author:Chunlin Tian, Weijun Ji, Yuan Yuan category:cs.CV  published:2017-01-16 summary:The Aduio-visual Speech Recognition (AVSR) which employs both the video and audio information to do Automatic Speech Recognition (ASR) is one of the application of multimodal leaning making ASR system more robust and accuracy. The traditional models usually treated AVSR as inference or projection but strict prior limits its ability. As the revival of deep learning, Deep Neural Networks (DNN) becomes an important toolkit in many traditional classification tasks including ASR, image classification, natural language processing. Some DNN models were used in AVSR like Multimodal Deep Autoencoders (MDAEs), Multimodal Deep Belief Network (MDBN) and Multimodal Deep Boltzmann Machine (MDBM) that actually work better than traditional methods. However, such DNN models have several shortcomings: (1) They don't balance the modal fusion and temporal fusion, or even haven't temporal fusion; (2)The architecture of these models isn't end-to-end, the training and testing getting cumbersome. We propose a DNN model, Auxiliary Multimodal LSTM (am-LSTM), to overcome such weakness. The am-LSTM could be trained and tested in one time, alternatively easy to train and preventing overfitting automatically. The extensibility and flexibility are also take into consideration. The experiments shows that am-LSTM is much better than traditional methods and other DNN models in three datasets: AVLetters, AVLetters2, AVDigits. version:1
arxiv-1701-04222 | Achieving Privacy in the Adversarial Multi-Armed Bandit | http://arxiv.org/abs/1701.04222 | id:1701.04222 author:Aristide C. Y. Tossou, Christos Dimitrakakis category:cs.LG cs.AI cs.CR  published:2017-01-16 summary:In this paper, we improve the previously best known regret bound to achieve $\epsilon$-differential privacy in oblivious adversarial bandits from $\mathcal{O}{(T^{2/3}/\epsilon)}$ to $\mathcal{O}{(\sqrt{T} \ln T /\epsilon)}$. This is achieved by combining a Laplace Mechanism with EXP3. We show that though EXP3 is already differentially private, it leaks a linear amount of information in $T$. However, we can improve this privacy by relying on its intrinsic exponential mechanism for selecting actions. This allows us to reach $\mathcal{O}{(\sqrt{\ln T})}$-DP, with a regret of $\mathcal{O}{(T^{2/3})}$ that holds against an adaptive adversary, an improvement from the best known of $\mathcal{O}{(T^{3/4})}$. This is done by using an algorithm that run EXP3 in a mini-batch loop. Finally, we run experiments that clearly demonstrate the validity of our theoretical analysis. version:1
arxiv-1701-04210 | Bandwidth limited object recognition in high resolution imagery | http://arxiv.org/abs/1701.04210 | id:1701.04210 author:Laura Lopez-Fuentes, Andrew D. Bagdanov, Joost van de Weijer, Harald Skinnemoen category:cs.CV  published:2017-01-16 summary:This paper proposes a novel method to optimize bandwidth usage for object detection in critical communication scenarios. We develop two operating models of active information seeking. The first model identifies promising regions in low resolution imagery and progressively requests higher resolution regions on which to perform recognition of higher semantic quality. The second model identifies promising regions in low resolution imagery while simultaneously predicting the approximate location of the object of higher semantic quality. From this general framework, we develop a car recognition system via identification of its license plate and evaluate the performance of both models on a car dataset that we introduce. Results are compared with traditional JPEG compression and demonstrate that our system saves up to one order of magnitude of bandwidth while sacrificing little in terms of recognition performance. version:1
arxiv-1701-04207 | Sparse Kernel Canonical Correlation Analysis via $\ell_1$-regularization | http://arxiv.org/abs/1701.04207 | id:1701.04207 author:Xiaowei Zhang, Delin Chu, Li-Zhi Liao, Michael K. Ng category:stat.ML  published:2017-01-16 summary:Canonical correlation analysis (CCA) is a multivariate statistical technique for finding the linear relationship between two sets of variables. The kernel generalization of CCA named kernel CCA has been proposed to find nonlinear relations between datasets. Despite their wide usage, they have one common limitation that is the lack of sparsity in their solution. In this paper, we consider sparse kernel CCA and propose a novel sparse kernel CCA algorithm (SKCCA). Our algorithm is based on a relationship between kernel CCA and least squares. Sparsity of the dual transformations is introduced by penalizing the $\ell_{1}$-norm of dual vectors. Experiments demonstrate that our algorithm not only performs well in computing sparse dual transformations but also can alleviate the over-fitting problem of kernel CCA. version:1
arxiv-1701-04189 | Deep Memory Networks for Attitude Identification | http://arxiv.org/abs/1701.04189 | id:1701.04189 author:Cheng Li, Xiaoxiao Guo, Qiaozhu Mei category:cs.CL  published:2017-01-16 summary:We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral. Instead, we show that attitude identification can be solved with an end-to-end machine learning architecture, in which the two subtasks are interleaved by a deep memory network. In this way, signals produced in target detection provide clues for polarity classification, and reversely, the predicted polarity provides feedback to the identification of targets. Moreover, the treatments for the set of targets also influence each other -- the learned representations may share the same semantics for some targets but vary for others. The proposed deep memory network, the AttNet, outperforms methods that do not consider the interactions between the subtasks or those among the targets, including conventional machine learning methods and the state-of-the-art deep learning models. version:1
arxiv-1701-04185 | A Watermarking Technique Using Discrete Curvelet Transform for Security of Multiple Biometric Features | http://arxiv.org/abs/1701.04185 | id:1701.04185 author:Rohit M. Thanki, Ved Vyas Dwivedi, Komal R. Borisagar category:cs.MM cs.CR cs.CV  published:2017-01-16 summary:The robustness and security of the biometric watermarking approach can be improved by using a multiple watermarking. This multiple watermarking proposed for improving security of biometric features and data. When the imposter tries to create the spoofed biometric feature, the invisible biometric watermark features can provide appropriate protection to multimedia data. In this paper, a biometric watermarking technique with multiple biometric watermarks are proposed in which biometric features of fingerprint, face, iris and signature is embedded in the image. Before embedding, fingerprint, iris, face and signature features are extracted using Shen-Castan edge detection and Principal Component Analysis. These all biometric watermark features are embedded into various mid band frequency curvelet coefficients of host image. All four fingerprint features, iris features, facial features and signature features are the biometric characteristics of the individual and they are used for cross verification and copyright protection if any manipulation occurs. The proposed technique is fragile enough; features cannot be extracted from the watermarked image when an imposter tries to remove watermark features illegally. It can use for multiple copyright authentication and verification. version:1
