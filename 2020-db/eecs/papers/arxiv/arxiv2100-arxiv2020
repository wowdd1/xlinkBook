arxiv-1303-0095 | Label-dependent Feature Extraction in Social Networks for Node Classification | http://arxiv.org/abs/1303.0095 | id:1303.0095 author:Tomasz Kajdanowicz, Przemyslaw Kazienko, Piotr Doskocz category:cs.SI cs.LG 91D30  68T05  68T10 I.2.8; I.2.11  published:2013-03-01 summary:A new method of feature extraction in the social network for within-network classification is proposed in the paper. The method provides new features calculated by combination of both: network structure information and class labels assigned to nodes. The influence of various features on classification performance has also been studied. The experiments on real-world data have shown that features created owing to the proposed method can lead to significant improvement of classification accuracy. version:1
arxiv-1302-6764 | Categorizing Bugs with Social Networks: A Case Study on Four Open Source Software Communities | http://arxiv.org/abs/1302.6764 | id:1302.6764 author:Marcelo Serrano Zanetti, Ingo Scholtes, Claudio Juan Tessone, Frank Schweitzer category:cs.SE cs.LG cs.SI nlin.AO physics.soc-ph  published:2013-02-27 summary:Efficient bug triaging procedures are an important precondition for successful collaborative software engineering projects. Triaging bugs can become a laborious task particularly in open source software (OSS) projects with a large base of comparably inexperienced part-time contributors. In this paper, we propose an efficient and practical method to identify valid bug reports which a) refer to an actual software bug, b) are not duplicates and c) contain enough information to be processed right away. Our classification is based on nine measures to quantify the social embeddedness of bug reporters in the collaboration network. We demonstrate its applicability in a case study, using a comprehensive data set of more than 700,000 bug reports obtained from the Bugzilla installation of four major OSS communities, for a period of more than ten years. For those projects that exhibit the lowest fraction of valid bug reports, we find that the bug reporters' position in the collaboration network is a strong indicator for the quality of bug reports. Based on this finding, we develop an automated classification scheme that can easily be integrated into bug tracking platforms and analyze its performance in the considered OSS communities. A support vector machine (SVM) to identify valid bug reports based on the nine measures yields a precision of up to 90.3% with an associated recall of 38.9%. With this, we significantly improve the results obtained in previous case studies for an automated early identification of bugs that are eventually fixed. Furthermore, our study highlights the potential of using quantitative measures of social organization in collaborative software engineering. It also opens a broad perspective for the integration of social awareness in the design of support infrastructures. version:2
arxiv-1303-0018 | Sparse Shape Reconstruction | http://arxiv.org/abs/1303.0018 | id:1303.0018 author:Alireza Aghasi, Justin Romberg category:math.FA cs.CV math-ph math.DG math.MP  published:2013-02-28 summary:This paper introduces a new shape-based image reconstruction technique applicable to a large class of imaging problems formulated in a variational sense. Given a collection of shape priors (a shape dictionary), we define our problem as choosing the right elements and geometrically composing them through basic set operations to characterize desired regions in the image. This combinatorial problem can be relaxed and then solved using classical descent methods. The main component of this relaxation is forming certain compactly supported functions which we call "knolls", and reformulating the shape representation as a basis expansion in terms of such functions. To select suitable elements of the dictionary, our problem ultimately reduces to solving a nonlinear program with sparsity constraints. We provide a new sparse nonlinear reconstruction technique to approach this problem. The performance of proposed technique is demonstrated with some standard imaging problems including image segmentation, X-ray tomography and diffusive tomography. version:1
arxiv-1201-4777 | A probabilistic methodology for multilabel classification | http://arxiv.org/abs/1201.4777 | id:1201.4777 author:Alfonso E. Romero, Luis M. de Campos category:cs.AI cs.LG 68T37  68T10 I.2.6; I.2.3; H.3  published:2012-01-23 summary:Multilabel classification is a relatively recent subfield of machine learning. Unlike to the classical approach, where instances are labeled with only one category, in multilabel classification, an arbitrary number of categories is chosen to label an instance. Due to the problem complexity (the solution is one among an exponential number of alternatives), a very common solution (the binary method) is frequently used, learning a binary classifier for every category, and combining them all afterwards. The assumption taken in this solution is not realistic, and in this work we give examples where the decisions for all the labels are not taken independently, and thus, a supervised approach should learn those existing relationships among categories to make a better classification. Therefore, we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers, by using a combination procedure with a classifier trained on the co-occurrences of the labels. We show an exhaustive experimentation in three different standard corpora of labeled documents (Reuters-21578, Ohsumed-23 and RCV1), which present noticeable improvements in all of them, when using our methodology, in three probabilistic base classifiers. version:2
arxiv-1302-7283 | Source Separation using Regularized NMF with MMSE Estimates under GMM Priors with Online Learning for The Uncertainties | http://arxiv.org/abs/1302.7283 | id:1302.7283 author:Emad M. Grais, Hakan Erdogan category:cs.LG cs.NA  published:2013-02-28 summary:We propose a new method to enforce priors on the solution of the nonnegative matrix factorization (NMF). The proposed algorithm can be used for denoising or single-channel source separation (SCSS) applications. The NMF solution is guided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussian mixture prior models (GMM) for the source signal. In SCSS applications, the spectra of the observed mixed signal are decomposed as a weighted linear combination of trained basis vectors for each source using NMF. In this work, the NMF decomposition weight matrices are treated as a distorted image by a distortion operator, which is learned directly from the observed signals. The MMSE estimate of the weights matrix under GMM prior and log-normal distribution for the distortion is then found to improve the NMF decomposition results. The MMSE estimate is embedded within the optimization objective to form a novel regularized NMF cost function. The corresponding update rules for the new objectives are derived in this paper. Experimental results show that, the proposed regularized NMF algorithm improves the source separation performance compared with using NMF without prior or with other prior models. version:1
arxiv-1302-7280 | Bayesian Consensus Clustering | http://arxiv.org/abs/1302.7280 | id:1302.7280 author:Eric F. Lock, David B. Dunson category:stat.ML cs.LG  published:2013-02-28 summary:The task of clustering a set of objects based on multiple sources of data arises in several modern applications. We propose an integrative statistical model that permits a separate clustering of the objects for each data source. These separate clusterings adhere loosely to an overall consensus clustering, and hence they are not independent. We describe a computationally scalable Bayesian framework for simultaneous estimation of both the consensus clustering and the source-specific clusterings. We demonstrate that this flexible approach is more robust than joint clustering of all data sources, and is more powerful than clustering each data source separately. This work is motivated by the integrated analysis of heterogeneous biomedical data, and we present an application to subtype identification of breast cancer tumor samples using publicly available data from The Cancer Genome Atlas. Software is available at http://people.duke.edu/~el113/software.html. version:1
arxiv-1301-4767 | A Linear Time Active Learning Algorithm for Link Classification -- Full Version -- | http://arxiv.org/abs/1301.4767 | id:1301.4767 author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG cs.SI stat.ML  published:2013-01-21 summary:We present very efficient active learning algorithms for link classification in signed networks. Our algorithms are motivated by a stochastic model in which edge labels are obtained through perturbations of a initial sign assignment consistent with a two-clustering of the nodes. We provide a theoretical analysis within this model, showing that we can achieve an optimal (to whithin a constant factor) number of mistakes on any graph G = (V,E) such that E = \Omega( V ^{3/2}) by querying O( V ^{3/2}) edge labels. More generally, we show an algorithm that achieves optimality to within a factor of O(k) by querying at most order of V + ( V /k)^{3/2} edge labels. The running time of this algorithm is at most of order E + V \log V . version:2
arxiv-1301-4769 | A Correlation Clustering Approach to Link Classification in Signed Networks -- Full Version -- | http://arxiv.org/abs/1301.4769 | id:1301.4769 author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG cs.DS stat.ML  published:2013-01-21 summary:Motivated by social balance theory, we develop a theory of link classification in signed networks using the correlation clustering index as measure of label regularity. We derive learning bounds in terms of correlation clustering within three fundamental transductive learning settings: online, batch and active. Our main algorithmic contribution is in the active setting, where we introduce a new family of efficient link classifiers based on covering the input graph with small circuits. These are the first active algorithms for link classification with mistake bounds that hold for arbitrary signed networks. version:2
arxiv-1301-5160 | See the Tree Through the Lines: The Shazoo Algorithm -- Full Version -- | http://arxiv.org/abs/1301.5160 | id:1301.5160 author:Fabio Vitale, Nicolo Cesa-Bianchi, Claudio Gentile, Giovanni Zappella category:cs.LG  published:2013-01-22 summary:Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains. Since graph sparsification via spanning trees retains enough information while making the task much easier, trees are an important special case of this problem. Although it is known how to predict the nodes of an unweighted tree in a nearly optimal way, in the weighted case a fully satisfactory algorithm is not available yet. We fill this hole and introduce an efficient node predictor, Shazoo, which is nearly optimal on any weighted tree. Moreover, we show that Shazoo can be viewed as a common nontrivial generalization of both previous approaches for unweighted trees and weighted lines. Experiments on real-world datasets confirm that Shazoo performs well in that it fully exploits the structure of the input tree, and gets very close to (and sometimes better than) less scalable energy minimization methods. version:2
arxiv-1302-7180 | Fast Matching by 2 Lines of Code for Large Scale Face Recognition Systems | http://arxiv.org/abs/1302.7180 | id:1302.7180 author:Dong Yi, Zhen Lei, Yang Hu, Stan Z. Li category:cs.CV  published:2013-02-28 summary:In this paper, we propose a method to apply the popular cascade classifier into face recognition to improve the computational efficiency while keeping high recognition rate. In large scale face recognition systems, because the probability of feature templates coming from different subjects is very high, most of the matching pairs will be rejected by the early stages of the cascade. Therefore, the cascade can improve the matching speed significantly. On the other hand, using the nested structure of the cascade, we could drop some stages at the end of feature to reduce the memory and bandwidth usage in some resources intensive system while not sacrificing the performance too much. The cascade is learned by two steps. Firstly, some kind of prepared features are grouped into several nested stages. And then, the threshold of each stage is learned to achieve user defined verification rate (VR). In the paper, we take a landmark based Gabor+LDA face recognition system as baseline to illustrate the process and advantages of the proposed method. However, the use of this method is very generic and not limited in face recognition, which can be easily generalized to other biometrics as a post-processing module. Experiments on the FERET database show the good performance of our baseline and an experiment on a self-collected large scale database illustrates that the cascade can improve the matching speed significantly. version:1
arxiv-1302-7099 | Community Detection in Random Networks | http://arxiv.org/abs/1302.7099 | id:1302.7099 author:Ery Arias-Castro, Nicolas Verzelen category:math.ST stat.ML stat.TH  published:2013-02-28 summary:We formalize the problem of detecting a community in a network into testing whether in a given (random) graph there is a subgraph that is unusually dense. We observe an undirected and unweighted graph on N nodes. Under the null hypothesis, the graph is a realization of an Erd\"os-R\'enyi graph with probability p0. Under the (composite) alternative, there is a subgraph of n nodes where the probability of connection is p1 > p0. We derive a detection lower bound for detecting such a subgraph in terms of N, n, p0, p1 and exhibit a test that achieves that lower bound. We do this both when p0 is known and unknown. We also consider the problem of testing in polynomial-time. As an aside, we consider the problem of detecting a clique, which is intimately related to the planted clique problem. Our focus in this paper is in the quasi-normal regime where n p0 is either bounded away from zero, or tends to zero slowly. version:1
arxiv-1302-7096 | Using Artificial Intelligence Models in System Identification | http://arxiv.org/abs/1302.7096 | id:1302.7096 author:Wesam Elshamy category:cs.NE cs.SY 68T05  published:2013-02-28 summary:Artificial Intelligence (AI) techniques are known for its ability in tackling problems found to be unyielding to traditional mathematical methods. A recent addition to these techniques are the Computational Intelligence (CI) techniques which, in most cases, are nature or biologically inspired techniques. Different CI techniques found their way to many control engineering applications, including system identification, and the results obtained by many researchers were encouraging. However, most control engineers and researchers used the basic CI models as is or slightly modified them to match their needs. Henceforth, the merits of one model over the other was not clear, and full potential of these models was not exploited. In this research, Genetic Algorithm (GA) and Particle Swarm Optimization (PSO) methods, which are different CI techniques, are modified to best suit the multimodal problem of system identification. In the first case of GA, an extension to the basic algorithm, which is inspired from nature as well, was deployed by introducing redundant genetic material. This extension, which come in handy in living organisms, did not result in significant performance improvement to the basic algorithm. In the second case, the Clubs-based PSO (C-PSO) dynamic neighborhood structure was introduced to replace the basic static structure used in canonical PSO algorithms. This modification of the neighborhood structure resulted in significant performance of the algorithm regarding convergence speed, and equipped it with a tool to handle multimodal problems. To understand the suitability of different GA and PSO techniques in the problem of system identification, they were used in an induction motor's parameter identification problem. The results enforced previous conclusions and showed the superiority of PSO in general over the GA in such a multimodal problem. version:1
arxiv-1302-7088 | Continuous-time Infinite Dynamic Topic Models | http://arxiv.org/abs/1302.7088 | id:1302.7088 author:Wesam Elshamy category:cs.IR stat.AP stat.ML 68T10  published:2013-02-28 summary:Topic models are probabilistic models for discovering topical themes in collections of documents. In real world applications, these models provide us with the means of organizing what would otherwise be unstructured collections. They can help us cluster a huge collection into different topics or find a subset of the collection that resembles the topical theme found in an article at hand. The first wave of topic models developed were able to discover the prevailing topics in a big collection of documents spanning a period of time. It was later realized that these time-invariant models were not capable of modeling 1) the time varying number of topics they discover and 2) the time changing structure of these topics. Few models were developed to address this two deficiencies. The online-hierarchical Dirichlet process models the documents with a time varying number of topics. It varies the structure of the topics over time as well. However, it relies on document order, not timestamps to evolve the model over time. The continuous-time dynamic topic model evolves topic structure in continuous-time. However, it uses a fixed number of topics over time. In this dissertation, I present a model, the continuous-time infinite dynamic topic model, that combines the advantages of these two models 1) the online-hierarchical Dirichlet process, and 2) the continuous-time dynamic topic model. More specifically, the model I present is a probabilistic topic model that does the following: 1) it changes the number of topics over continuous time, and 2) it changes the topic structure over continuous-time. I compared the model I developed with the two other models with different setting values. The results obtained were favorable to my model and showed the need for having a model that has a continuous-time varying number of topics and topic structure. version:1
arxiv-1302-7082 | K Means Segmentation of Alzheimers Disease in PET scan datasets: An implementation | http://arxiv.org/abs/1302.7082 | id:1302.7082 author:A. Meena, K. Raja category:cs.CV cs.NE  published:2013-02-28 summary:The Positron Emission Tomography (PET) scan image requires expertise in the segmentation where clustering algorithm plays an important role in the automation process. The algorithm optimization is concluded based on the performance, quality and number of clusters extracted. This paper is proposed to study the commonly used K Means clustering algorithm and to discuss a brief list of toolboxes for reproducing and extending works presented in medical image analysis. This work is compiled using AForge .NET framework in windows environment and MATrix LABoratory (MATLAB 7.0.1) version:1
arxiv-1302-7080 | Parameter Identification of Induction Motor Using Modified Particle Swarm Optimization Algorithm | http://arxiv.org/abs/1302.7080 | id:1302.7080 author:Hassan M Emara, Wesam Elshamy, Ahmed Bahgat category:cs.NE 68T05  published:2013-02-28 summary:This paper presents a new technique for induction motor parameter identification. The proposed technique is based on a simple startup test using a standard V/F inverter. The recorded startup currents are compared to that obtained by simulation of an induction motor model. A Modified PSO optimization is used to find out the best model parameter that minimizes the sum square error between the measured and the simulated currents. The performance of the modified PSO is compared with other optimization methods including line search, conventional PSO and Genetic Algorithms. Simulation results demonstrate the ability of the proposed technique to capture the true values of the machine parameters and the superiority of the results obtained using the modified PSO over other optimization techniques. version:1
arxiv-1302-7069 | Learning Theory in the Arithmetic Hierarchy | http://arxiv.org/abs/1302.7069 | id:1302.7069 author:Achilles Beros category:math.LO cs.LG cs.LO 03D80  68Q32  published:2013-02-28 summary:We consider the arithmetic complexity of index sets of uniformly computably enumerable families learnable under different learning criteria. We determine the exact complexity of these sets for the standard notions of finite learning, learning in the limit, behaviorally correct learning and anomalous learning in the limit. In proving the $\Sigma_5^0$-completeness result for behaviorally correct learning we prove a result of independent interest; if a uniformly computably enumerable family is not learnable, then for any computable learner there is a $\Delta_2^0$ enumeration witnessing failure. version:1
arxiv-1302-7056 | KSU KDD: Word Sense Induction by Clustering in Topic Space | http://arxiv.org/abs/1302.7056 | id:1302.7056 author:Wesam Elshamy, Doina Caragea, William Hsu category:cs.CL cs.AI stat.AP stat.ML 68T05  published:2013-02-28 summary:We describe our language-independent unsupervised word sense induction system. This system only uses topic features to cluster different word senses in their global context topic space. Using unlabeled data, this system trains a latent Dirichlet allocation (LDA) topic model then uses it to infer the topics distribution of the test instances. By clustering these topics distributions in their topic space we cluster them into different senses. Our hypothesis is that closeness in topic space reflects similarity between different word senses. This system participated in SemEval-2 word sense induction and disambiguation task and achieved the second highest V-measure score among all other systems. version:1
arxiv-1302-7043 | Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization | http://arxiv.org/abs/1302.7043 | id:1302.7043 author:Evangelos E. Papalexakis, Tom M. Mitchell, Nicholas D. Sidiropoulos, Christos Faloutsos, Partha Pratim Talukdar, Brian Murphy category:stat.ML cs.LG  published:2013-02-28 summary:How can we correlate neural activity in the human brain as it responds to words, with behavioral data expressed as answers to questions about these same words? In short, we want to find latent variables, that explain both the brain activity, as well as the behavioral responses. We show that this is an instance of the Coupled Matrix-Tensor Factorization (CMTF) problem. We propose Scoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problem and produces a sparse latent low-rank subspace of the data. In our experiments, we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm for CMTF, along with a 5 fold increase in sparsity. Moreover, we extend Scoup-SMT to handle missing data without degradation of performance. We apply Scoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, human subjects) tensor and a (nouns, properties) matrix, with coupling along the nouns dimension. Scoup-SMT is able to find meaningful latent variables, as well as to predict brain activity with competitive accuracy. Finally, we demonstrate the generality of Scoup-SMT, by applying it on a Facebook dataset (users, friends, wall-postings); there, Scoup-SMT spots spammer-like anomalies. version:1
arxiv-1302-7039 | Content Based Image Retrieval System Using NOHIS-tree | http://arxiv.org/abs/1302.7039 | id:1302.7039 author:Mounira Taileb category:cs.IR cs.CV cs.DB H.3.3  published:2013-02-28 summary:Content-based image retrieval (CBIR) has been one of the most important research areas in computer vision. It is a widely used method for searching images in huge databases. In this paper we present a CBIR system called NOHIS-Search. The system is based on the indexing technique NOHIS-tree. The two phases of the system are described and the performance of the system is illustrated with the image database ImagEval. NOHIS-Search system was compared to other two CBIR systems; the first that using PDDP indexing algorithm and the second system is that using the sequential search. Results show that NOHIS-Search system outperforms the two other systems. version:1
arxiv-1302-6957 | Ensemble Sparse Models for Image Analysis | http://arxiv.org/abs/1302.6957 | id:1302.6957 author:Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan, Prasanna Sattigeri, Andreas Spanias category:cs.CV  published:2013-02-27 summary:Sparse representations with learned dictionaries have been successful in several image analysis applications. In this paper, we propose and analyze the framework of ensemble sparse models, and demonstrate their utility in image restoration and unsupervised clustering. The proposed ensemble model approximates the data as a linear combination of approximations from multiple \textit{weak} sparse models. Theoretical analysis of the ensemble model reveals that even in the worst-case, the ensemble can perform better than any of its constituent individual models. The dictionaries corresponding to the individual sparse models are obtained using either random example selection or boosted approaches. Boosted approaches learn one dictionary per round such that the dictionary learned in a particular round is optimized for the training examples having high reconstruction error in the previous round. Results with compressed recovery show that the ensemble representations lead to a better performance compared to using a single dictionary obtained with the conventional alternating minimization approach. The proposed ensemble models are also used for single image superresolution, and we show that they perform comparably to the recent approaches. In unsupervised clustering, experiments show that the proposed model performs better than baseline approaches in several standard datasets. version:1
arxiv-1302-6927 | Online Learning for Time Series Prediction | http://arxiv.org/abs/1302.6927 | id:1302.6927 author:Oren Anava, Elad Hazan, Shie Mannor, Ohad Shamir category:cs.LG  published:2013-02-27 summary:In this paper we address the problem of predicting a time series using the ARMA (autoregressive moving average) model, under minimal assumptions on the noise terms. Using regret minimization techniques, we develop effective online learning algorithms for the prediction problem, without assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we show that our algorithm's performances asymptotically approaches the performance of the best ARMA model in hindsight. version:1
arxiv-1302-6828 | Induction of Selective Bayesian Classifiers | http://arxiv.org/abs/1302.6828 | id:1302.6828 author:Pat Langley, Stephanie Sage category:cs.LG stat.ML  published:2013-02-27 summary:In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that c arries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research. version:1
arxiv-1302-6808 | Learning Gaussian Networks | http://arxiv.org/abs/1302.6808 | id:1302.6808 author:Dan Geiger, David Heckerman category:cs.AI cs.LG stat.ML  published:2013-02-27 summary:We describe algorithms for learning Bayesian networks from a combination of user knowledge and statistical data. The algorithms have two components: a scoring metric and a search procedure. The scoring metric takes a network structure, statistical data, and a user's prior knowledge, and returns a score proportional to the posterior probability of the network structure given the data. The search procedure generates networks for evaluation by the scoring metric. Previous work has concentrated on metrics for domains containing only discrete variables, under the assumption that data represents a multinomial sample. In this paper, we extend this work, developing scoring metrics for domains containing all continuous variables or a mixture of discrete and continuous variables, under the assumption that continuous data is sampled from a multivariate normal distribution. Our work extends traditional statistical approaches for identifying vanishing regression coefficients in that we identify two important assumptions, called event equivalence and parameter modularity, that when combined allow the construction of prior distributions for multivariate normal parameters from a single prior Bayesian network specified by a user. version:1
arxiv-1302-6777 | Ending-based Strategies for Part-of-speech Tagging | http://arxiv.org/abs/1302.6777 | id:1302.6777 author:Greg Adams, Beth Millar, Eric Neufeld, Tim Philip category:cs.CL  published:2013-02-27 summary:Probabilistic approaches to part-of-speech tagging rely primarily on whole-word statistics about word/tag combinations as well as contextual information. But experience shows about 4 per cent of tokens encountered in test sets are unknown even when the training set is as large as a million words. Unseen words are tagged using secondary strategies that exploit word features such as endings, capitalizations and punctuation marks. In this work, word-ending statistics are primary and whole-word statistics are secondary. First, a tagger was trained and tested on word endings only. Subsequent experiments added back whole-word statistics for the words occurring most frequently in the training set. As grew larger, performance was expected to improve, in the limit performing the same as word-based taggers. Surprisingly, the ending-based tagger initially performed nearly as well as the word-based tagger; in the best case, its performance significantly exceeded that of the word-based tagger. Lastly, and unexpectedly, an effect of negative returns was observed - as grew larger, performance generally improved and then declined. By varying factors such as ending length and tag-list strategy, we achieved a success rate of 97.5 percent. version:1
arxiv-1302-6766 | A bag-of-paths framework for network data analysis | http://arxiv.org/abs/1302.6766 | id:1302.6766 author:Kevin Françoisse, Ilkka Kivimäki, Amin Mantrach, Fabrice Rossi, Marco Saerens category:stat.ML  published:2013-02-27 summary:This work introduces a generic framework, called the bag-of-paths (BoP), that can be used for link and network data analysis. The primary application of this framework, investigated in this paper, is the definition of distance measures between nodes enjoying some nice properties. More precisely, let us assume a weighted directed graph G where a cost is associated to each arc. Within this context, consider a bag containing all the possible paths between pairs of nodes in G. Then, following, a probability distribution on this countable set of paths through the graph is defined by minimizing the total expected cost between all pairs of nodes while fixing the total relative entropy spread in the graph. This results in a Boltzmann distribution on the set of paths such that long (high-cost) paths have a low probability of being sampled from the bag, while short (low-cost) paths have a high probability of being sampled. Within this probabilistic framework, the BoP probabilities, P(s=i,e=j), of drawing a path starting from node i (s=i) and ending in node j (e=j) can easily be computed in closed form by a simple matrix inversion. Various applications of this framework are currently investigated, e.g., the definition of distance measures between the nodes of G, betweenness indexes, network criticality measures, edit distances, etc. As a first step, this paper describes the general BoP framework and introduces two families of distance measures between nodes. In addition to being a distance measure, one of these two quantities has the interesting property of interpolating between the shortest path and the commute cost distances. Experimental results on semi-supervised tasks show that these distance families are competitive with other state-of-the-art approaches. version:1
arxiv-1111-2546 | Accuracy guaranties for $\ell_1$ recovery of block-sparse signals | http://arxiv.org/abs/1111.2546 | id:1111.2546 author:Anatoli Juditsky, Fatma Kılınç Karzan, Arkadi Nemirovski, Boris Polyak category:math.ST math.OC stat.ML stat.TH  published:2011-11-10 summary:We introduce a general framework to handle structured models (sparse and block-sparse with possibly overlapping blocks). We discuss new methods for their recovery from incomplete observation, corrupted with deterministic and stochastic noise, using block-$\ell_1$ regularization. While the current theory provides promising bounds for the recovery errors under a number of different, yet mostly hard to verify conditions, our emphasis is on verifiable conditions on the problem parameters (sensing matrix and the block structure) which guarantee accurate recovery. Verifiability of our conditions not only leads to efficiently computable bounds for the recovery error but also allows us to optimize these error bounds with respect to the method parameters, and therefore construct estimators with improved statistical properties. To justify our approach, we also provide an oracle inequality, which links the properties of the proposed recovery algorithms and the best estimation performance. Furthermore, utilizing these verifiable conditions, we develop a computationally cheap alternative to block-$\ell_1$ minimization, the non-Euclidean Block Matching Pursuit algorithm. We close by presenting a numerical study to investigate the effect of different block regularizations and demonstrate the performance of the proposed recoveries. version:2
arxiv-1302-6677 | Taming the Curse of Dimensionality: Discrete Integration by Hashing and Optimization | http://arxiv.org/abs/1302.6677 | id:1302.6677 author:Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman category:cs.LG cs.AI stat.ML  published:2013-02-27 summary:Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection. version:1
arxiv-1302-6434 | Convex vs nonconvex approaches for sparse estimation: GLasso, Multiple Kernel Learning and Hyperparameter GLasso | http://arxiv.org/abs/1302.6434 | id:1302.6434 author:Aleksandr Y. Aravkin, James V. Burke, Alessandro Chiuso, Gianluigi Pillonetto category:stat.ML math.OC 62F35  65K10  47N30  published:2013-02-26 summary:The popular Lasso approach for sparse estimation can be derived via marginalization of a joint density associated with a particular stochastic model. A different marginalization of the same probabilistic model leads to a different non-convex estimator where hyperparameters are optimized. Extending these arguments to problems where groups of variables have to be estimated, we study a computational scheme for sparse estimation that differs from the Group Lasso. Although the underlying optimization problem defining this estimator is non-convex, an initialization strategy based on a univariate Bayesian forward selection scheme is presented. This also allows us to define an effective non-convex estimator where only one scalar variable is involved in the optimization process. Theoretical arguments, independent of the correctness of the priors entering the sparse model, are included to clarify the advantages of this non-convex technique in comparison with other convex estimators. Numerical experiments are also used to compare the performance of these approaches. version:2
arxiv-1302-6617 | Arriving on time: estimating travel time distributions on large-scale road networks | http://arxiv.org/abs/1302.6617 | id:1302.6617 author:Timothy Hunter, Aude Hofleitner, Jack Reilly, Walid Krichene, Jerome Thai, Anastasios Kouvelas, Pieter Abbeel, Alexandre Bayen category:cs.LG cs.AI  published:2013-02-26 summary:Most optimal routing problems focus on minimizing travel time or distance traveled. Oftentimes, a more useful objective is to maximize the probability of on-time arrival, which requires statistical distributions of travel times, rather than just mean values. We propose a method to estimate travel time distributions on large-scale road networks, using probe vehicle data collected from GPS. We present a framework that works with large input of data, and scales linearly with the size of the network. Leveraging the planar topology of the graph, the method computes efficiently the time correlations between neighboring streets. First, raw probe vehicle traces are compressed into pairs of travel times and number of stops for each traversed road segment using a `stop-and-go' algorithm developed for this work. The compressed data is then used as input for training a path travel time model, which couples a Markov model along with a Gaussian Markov random field. Finally, scalable inference algorithms are developed for obtaining path travel time distributions from the composite MM-GMRF model. We illustrate the accuracy and scalability of our model on a 505,000 road link network spanning the San Francisco Bay Area. version:1
arxiv-1302-6615 | PSO based Neural Networks vs. Traditional Statistical Models for Seasonal Time Series Forecasting | http://arxiv.org/abs/1302.6615 | id:1302.6615 author:Ratnadip Adhikari, R. K. Agrawal, Laxmi Kant category:cs.NE 68T05  published:2013-02-26 summary:Seasonality is a distinctive characteristic which is often observed in many practical time series. Artificial Neural Networks (ANNs) are a class of promising models for efficiently recognizing and forecasting seasonal patterns. In this paper, the Particle Swarm Optimization (PSO) approach is used to enhance the forecasting strengths of feedforward ANN (FANN) as well as Elman ANN (EANN) models for seasonal data. Three widely popular versions of the basic PSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here. The empirical analysis is conducted on three real-world seasonal time series. Results clearly show that each version of the PSO algorithm achieves notably better forecasting accuracies than the standard Backpropagation (BP) training method for both FANN and EANN models. The neural network forecasting results are also compared with those from the three traditional statistical models, viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters (HW) and Support Vector Machine (SVM). The comparison demonstrates that both PSO and BP based neural networks outperform SARIMA, HW and SVM models for all three time series datasets. The forecasting performances of ANNs are further improved through combining the outputs from the three PSO based models. version:1
arxiv-1302-6613 | An Introductory Study on Time Series Modeling and Forecasting | http://arxiv.org/abs/1302.6613 | id:1302.6613 author:Ratnadip Adhikari, R. K. Agrawal category:cs.LG stat.ML 68T01  published:2013-02-26 summary:Time series modeling and forecasting has fundamental importance to various practical domains. Thus a lot of active research works is going on in this subject during several years. Many important models have been proposed in literature for improving the accuracy and effectiveness of time series forecasting. The aim of this dissertation work is to present a concise description of some popular time series forecasting models used in practice, with their salient features. In this thesis, we have described three important classes of time series models, viz. the stochastic, neural networks and SVM based models, together with their inherent forecasting strengths and weaknesses. We have also discussed about the basic issues related to time series modeling, such as stationarity, parsimony, overfitting, etc. Our discussion about different time series models is supported by giving the experimental forecast results, performed on six real time series datasets. While fitting a model to a dataset, special care is taken to select the most parsimonious one. To evaluate forecast accuracy as well as to compare among different models fitted to a time series, we have used the five performance measures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of the six datasets, we have shown the obtained forecast diagram which graphically depicts the closeness between the original and forecasted observations. To have authenticity as well as clarity in our discussion about time series modeling and forecasting, we have taken the help of various published research works from reputed journals and some standard books. version:1
arxiv-1302-6523 | Sparse Frequency Analysis with Sparse-Derivative Instantaneous Amplitude and Phase Functions | http://arxiv.org/abs/1302.6523 | id:1302.6523 author:Yin Ding, Ivan W. Selesnick category:cs.LG  published:2013-02-26 summary:This paper addresses the problem of expressing a signal as a sum of frequency components (sinusoids) wherein each sinusoid may exhibit abrupt changes in its amplitude and/or phase. The Fourier transform of a narrow-band signal, with a discontinuous amplitude and/or phase function, exhibits spectral and temporal spreading. The proposed method aims to avoid such spreading by explicitly modeling the signal of interest as a sum of sinusoids with time-varying amplitudes. So as to accommodate abrupt changes, it is further assumed that the amplitude/phase functions are approximately piecewise constant (i.e., their time-derivatives are sparse). The proposed method is based on a convex variational (optimization) approach wherein the total variation (TV) of the amplitude functions are regularized subject to a perfect (or approximate) reconstruction constraint. A computationally efficient algorithm is derived based on convex optimization techniques. The proposed technique can be used to perform band-pass filtering that is relatively insensitive to narrow-band amplitude/phase jumps present in data, which normally pose a challenge (due to transients, leakage, etc.). The method is illustrated using both synthetic signals and human EEG data for the purpose of band-pass filtering and the estimation of phase synchrony indexes. version:1
arxiv-1302-6452 | A Conformal Prediction Approach to Explore Functional Data | http://arxiv.org/abs/1302.6452 | id:1302.6452 author:Jing Lei, Alessandro Rinaldo, Larry Wasserman category:stat.ML cs.LG  published:2013-02-26 summary:This paper applies conformal prediction techniques to compute simultaneous prediction bands and clustering trees for functional data. These tools can be used to detect outliers and clusters. Both our prediction bands and clustering trees provide prediction sets for the underlying stochastic process with a guaranteed finite sample behavior, under no distributional assumptions. The prediction sets are also informative in that they correspond to the high density region of the underlying process. While ordinary conformal prediction has high computational cost for functional data, we use the inductive conformal predictor, together with several novel choices of conformity scores, to simplify the computation. Our methods are illustrated on some real data examples. version:1
arxiv-1302-6426 | Segmentation of Alzheimers Disease in PET scan datasets using MATLAB | http://arxiv.org/abs/1302.6426 | id:1302.6426 author:A. Meena, K. Raja category:cs.NE  published:2013-02-26 summary:Positron Emission Tomography (PET) scan images are one of the bio medical imaging techniques similar to that of MRI scan images but PET scan images are helpful in finding the development of tumors.The PET scan images requires expertise in the segmentation where clustering plays an important role in the automation process.The segmentation of such images is manual to automate the process clustering is used.Clustering is commonly known as unsupervised learning process of n dimensional data sets are clustered into k groups so as to maximize the inter cluster similarity and to minimize the intra cluster similarity.This paper is proposed to implement the commonly used K Means and Fuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix LABoratory (MATLAB) and tested with sample PET scan image. The sample data is collected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image Processing and Visualization Tool (MIPAV) are used to compare the resultant images. version:1
arxiv-1302-6390 | The adaptive Gril estimator with a diverging number of parameters | http://arxiv.org/abs/1302.6390 | id:1302.6390 author:Mohammed El Anbari, Abdallah Mkhadri category:stat.ME cs.LG  published:2013-02-26 summary:We consider the problem of variables selection and estimation in linear regression model in situations where the number of parameters diverges with the sample size. We propose the adaptive Generalized Ridge-Lasso (\mbox{AdaGril}) which is an extension of the the adaptive Elastic Net. AdaGril incorporates information redundancy among correlated variables for model selection and estimation. It combines the strengths of the quadratic regularization and the adaptively weighted Lasso shrinkage. In this paper, we highlight the grouped selection property for AdaCnet method (one type of AdaGril) in the equal correlation case. Under weak conditions, we establish the oracle property of AdaGril which ensures the optimal large performance when the dimension is high. Consequently, it achieves both goals of handling the problem of collinearity in high dimension and enjoys the oracle property. Moreover, we show that AdaGril estimator achieves a Sparsity Inequality, i. e., a bound in terms of the number of non-zero components of the 'true' regression coefficient. This bound is obtained under a similar weak Restricted Eigenvalue (RE) condition used for Lasso. Simulations studies show that some particular cases of AdaGril outperform its competitors. version:1
arxiv-1302-6379 | Image-based Face Detection and Recognition: "State of the Art" | http://arxiv.org/abs/1302.6379 | id:1302.6379 author:Faizan Ahmad, Aaima Najam, Zeeshan Ahmed category:cs.CV  published:2013-02-26 summary:Face recognition from image or video is a popular topic in biometrics research. Many public places usually have surveillance cameras for video capture and these cameras have their significant value for security purpose. It is widely acknowledged that the face recognition have played an important role in surveillance system as it doesn't need the object's cooperation. The actual advantages of face based identification over other biometrics are uniqueness and acceptance. As human face is a dynamic object having high degree of variability in its appearance, that makes face detection a difficult problem in computer vision. In this field, accuracy and speed of identification is a main issue. The goal of this paper is to evaluate various face detection and recognition methods, provide complete solution for image based face detection and recognition with higher accuracy, better response rate as an initial step for video surveillance. Solution is proposed based on performed tests on various face rich databases in terms of subjects, pose, emotions, race and light. version:1
arxiv-1302-4242 | Metrics for Multivariate Dictionaries | http://arxiv.org/abs/1302.4242 | id:1302.4242 author:Sylvain Chevallier, Quentin Barthélemy, Jamal Atif category:cs.LG stat.ML K.3.2  published:2013-02-18 summary:Overcomplete representations and dictionary learning algorithms kept attracting a growing interest in the machine learning community. This paper addresses the emerging problem of comparing multivariate overcomplete representations. Despite a recurrent need to rely on a distance for learning or assessing multivariate overcomplete representations, no metrics in their underlying spaces have yet been proposed. Henceforth we propose to study overcomplete representations from the perspective of frame theory and matrix manifolds. We consider distances between multivariate dictionaries as distances between their spans which reveal to be elements of a Grassmannian manifold. We introduce Wasserstein-like set-metrics defined on Grassmannian spaces and study their properties both theoretically and numerically. Indeed a deep experimental study based on tailored synthetic datasetsand real EEG signals for Brain-Computer Interfaces (BCI) have been conducted. In particular, the introduced metrics have been embedded in clustering algorithm and applied to BCI Competition IV-2a for dataset quality assessment. Besides, a principled connection is made between three close but still disjoint research fields, namely, Grassmannian packing, dictionary learning and compressed sensing. version:2
arxiv-1302-6334 | Non-simplifying Graph Rewriting Termination | http://arxiv.org/abs/1302.6334 | id:1302.6334 author:Guillaume Bonfante, Bruno Guillaume category:cs.CL cs.CC cs.LO  published:2013-02-26 summary:So far, a very large amount of work in Natural Language Processing (NLP) rely on trees as the core mathematical structure to represent linguistic informations (e.g. in Chomsky's work). However, some linguistic phenomena do not cope properly with trees. In a former paper, we showed the benefit of encoding linguistic structures by graphs and of using graph rewriting rules to compute on those structures. Justified by some linguistic considerations, graph rewriting is characterized by two features: first, there is no node creation along computations and second, there are non-local edge modifications. Under these hypotheses, we show that uniform termination is undecidable and that non-uniform termination is decidable. We describe two termination techniques based on weights and we give complexity bound on the derivation length for these rewriting system. version:1
arxiv-1302-6315 | Rate-Distortion Bounds for an Epsilon-Insensitive Distortion Measure | http://arxiv.org/abs/1302.6315 | id:1302.6315 author:Kazuho Watanabe category:cs.IT cs.LG math.IT  published:2013-02-26 summary:Direct evaluation of the rate-distortion function has rarely been achieved when it is strictly greater than its Shannon lower bound. In this paper, we consider the rate-distortion function for the distortion measure defined by an epsilon-insensitive loss function. We first present the Shannon lower bound applicable to any source distribution with finite differential entropy. Then, focusing on the Laplacian and Gaussian sources, we prove that the rate-distortion functions of these sources are strictly greater than their Shannon lower bounds and obtain analytically evaluable upper bounds for the rate-distortion functions. Small distortion limit and numerical evaluation of the bounds suggest that the Shannon lower bound provides a good approximation to the rate-distortion function for the epsilon-insensitive distortion measure. version:1
arxiv-1302-6310 | Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining Techniques | http://arxiv.org/abs/1302.6310 | id:1302.6310 author:Adesesan . B Adeyemo, Adebola A. Oketola, Emmanuel O. Adetula, O. Osibanjo category:cs.NE  published:2013-02-26 summary:Industrial pollution is often considered to be one of the prime factors contributing to air, water and soil pollution. Sectoral pollution loads (ton/yr) into different media (i.e. air, water and land) in Lagos were estimated using Industrial Pollution Projected System (IPPS). These were further studied using Artificial neural Networks (ANNs), a data mining technique that has the ability of detecting and describing patterns in large data sets with variables that are non- linearly related. Time Lagged Recurrent Network (TLRN) appeared as the best Neural Network model among all the neural networks considered which includes Multilayer Perceptron (MLP) Network, Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF) Network and Recurrent Network (RN). TLRN modelled the data-sets better than the others in terms of the mean average error (MAE) (0.14), time (39 s) and linear correlation coefficient (0.84). The results showed that Artificial Neural Networks (ANNs) technique (i.e., Time Lagged Recurrent Network) is also applicable and effective in environmental assessment study. Keywords: Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial Pollution Projection System (IPPS), Pollution load, Pollution Intensity. version:1
arxiv-1008-0204 | Mixture decompositions of exponential families using a decomposition of their sample spaces | http://arxiv.org/abs/1008.0204 | id:1008.0204 author:Guido Montufar category:math.ST stat.ML stat.TH 52B05  60C05  62E17  published:2010-08-01 summary:We study the problem of finding the smallest $m$ such that every element of an exponential family can be written as a mixture of $m$ elements of another exponential family. We propose an approach based on coverings and packings of the face lattice of the corresponding convex support polytopes and results from coding theory. We show that $m=q^{N-1}$ is the smallest number for which any distribution of $N$ $q$-ary variables can be written as mixture of $m$ independent $q$-ary variables. Furthermore, we show that any distribution of $N$ binary variables is a mixture of $m = 2^{N-(k+1)}(1+ 1/(2^k-1))$ elements of the $k$-interaction exponential family. version:4
arxiv-1302-6210 | A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting | http://arxiv.org/abs/1302.6210 | id:1302.6210 author:Ratnadip Adhikari, R. K. Agrawal category:cs.NE cs.LG 68T05  published:2013-02-25 summary:Enhancing the robustness and accuracy of time series forecasting models is an active area of research. Recently, Artificial Neural Networks (ANNs) have found extensive applications in many practical forecasting problems. However, the standard backpropagation ANN training algorithm has some critical issues, e.g. it has a slow convergence rate and often converges to a local minimum, the complex pattern of error surfaces, lack of proper training parameters selection methods, etc. To overcome these drawbacks, various improved training methods have been developed in literature; but, still none of them can be guaranteed as the best for all problems. In this paper, we propose a novel weighted ensemble scheme which intelligently combines multiple training algorithms to increase the ANN forecast accuracies. The weight for each training algorithm is determined from the performance of the corresponding ANN model on the validation dataset. Experimental results on four important time series depicts that our proposed technique reduces the mentioned shortcomings of individual ANN training algorithms to a great extent. Also it achieves significantly better forecast accuracies than two other popular statistical models. version:1
arxiv-1207-3399 | Scaling of Model Approximation Errors and Expected Entropy Distances | http://arxiv.org/abs/1207.3399 | id:1207.3399 author:Guido F. Montufar, Johannes Rauh category:stat.ML 94A17  62B15  published:2012-07-14 summary:We compute the expected value of the Kullback-Leibler divergence to various fundamental statistical models with respect to canonical priors on the probability simplex. We obtain closed formulas for the expected model approximation errors, depending on the dimension of the models and the cardinalities of their sample spaces. For the uniform prior, the expected divergence from any model containing the uniform distribution is bounded by a constant $1-\gamma$, and for the models that we consider, this bound is approached if the state space is very large and the models' dimension does not grow too fast. For Dirichlet priors the expected divergence is bounded in a similar way, if the concentration parameters take reasonable values. These results serve as reference values for more complicated statistical models. version:2
arxiv-1302-6194 | Phoneme discrimination using $KS$-algebra II | http://arxiv.org/abs/1302.6194 | id:1302.6194 author:Ondrej Such, Lenka Mackovicova category:cs.SD cs.LG stat.ML I.2.7; I.5.4  published:2013-02-25 summary:$KS$-algebra consists of expressions constructed with four kinds operations, the minimum, maximum, difference and additively homogeneous generalized means. Five families of $Z$-classifiers are investigated on binary classification tasks between English phonemes. It is shown that the classifiers are able to reflect well known formant characteristics of vowels, while having very small Kolmogoroff's complexity. version:1
arxiv-1204-4227 | Estimating Unknown Sparsity in Compressed Sensing | http://arxiv.org/abs/1204.4227 | id:1204.4227 author:Miles E. Lopes category:cs.IT math.IT math.ST stat.ME stat.ML stat.TH  published:2012-04-19 summary:In the theory of compressed sensing (CS), the sparsity x _0 of the unknown signal x\in\R^p is commonly assumed to be a known parameter. However, it is typically unknown in practice. Due to the fact that many aspects of CS depend on knowing x _0, it is important to estimate this parameter in a data-driven way. A second practical concern is that x _0 is a highly unstable function of x. In particular, for real signals with entries not exactly equal to 0, the value x _0=p is not a useful description of the effective number of coordinates. In this paper, we propose to estimate a stable measure of sparsity s(x):= x _1^2/ x _2^2, which is a sharp lower bound on x _0. Our estimation procedure uses only a small number of linear measurements, does not rely on any sparsity assumptions, and requires very little computation. A confidence interval for s(x) is provided, and its width is shown to have no dependence on the signal dimension p. Moreover, this result extends naturally to the matrix recovery setting, where a soft version of matrix rank can be estimated with analogous guarantees. Finally, we show that the use of randomized measurements is essential to estimating s(x). This is accomplished by proving that the minimax risk for estimating s(x) with deterministic measurements is large when n<<p. version:2
arxiv-1302-6031 | Phoneme discrimination using KS algebra I | http://arxiv.org/abs/1302.6031 | id:1302.6031 author:Ondrej Such category:cs.SD cs.AI cs.NE I.2.7; I.5.2; I.5.4  published:2013-02-25 summary:In our work we define a new algebra of operators as a substitute for fuzzy logic. Its primary purpose is for construction of binary discriminators for phonemes based on spectral content. It is optimized for design of non-parametric computational circuits, and makes uses of 4 operations: $\min$, $\max$, the difference and generalized additively homogenuous means. version:1
arxiv-1302-6009 | On learning parametric-output HMMs | http://arxiv.org/abs/1302.6009 | id:1302.6009 author:Aryeh Kontorovich, Boaz Nadler, Roi Weiss category:cs.LG math.ST stat.ML stat.TH  published:2013-02-25 summary:We present a novel approach for learning an HMM whose outputs are distributed according to a parametric family. This is done by {\em decoupling} the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results. version:1
arxiv-1302-5985 | A Meta-Theory of Boundary Detection Benchmarks | http://arxiv.org/abs/1302.5985 | id:1302.5985 author:Xiaodi Hou, Alan Yuille, Christof Koch category:cs.CV  published:2013-02-25 summary:Human labeled datasets, along with their corresponding evaluation algorithms, play an important role in boundary detection. We here present a psychophysical experiment that addresses the reliability of such benchmarks. To find better remedies to evaluate the performance of any boundary detection algorithm, we propose a computational framework to remove inappropriate human labels and estimate the intrinsic properties of boundaries. version:1
arxiv-1302-5957 | Shape Characterization via Boundary Distortion | http://arxiv.org/abs/1302.5957 | id:1302.5957 author:Xavier Descombes, Serguei Komech category:cs.CV  published:2013-02-24 summary:In this paper, we derive new shape descriptors based on a directional characterization. The main idea is to study the behavior of the shape neighborhood under family of transformations. We obtain a description invariant with respect to rotation, reflection, translation and scaling. A well-defined metric is then proposed on the associated feature space. We show the continuity of this metric. Some results on shape retrieval are provided on two databases to show the accuracy of the proposed shape metric. version:1
arxiv-1302-5894 | Four Side Distance: A New Fourier Shape Signature | http://arxiv.org/abs/1302.5894 | id:1302.5894 author:Sonya Eini, Abdolah Chalechale category:cs.CV  published:2013-02-24 summary:Shape is one of the main features in content based image retrieval (CBIR). This paper proposes a new shape signature. In this technique, features of each shape are extracted based on four sides of the rectangle that covers the shape. The proposed technique is Fourier based and it is invariant to translation, scaling and rotation. The retrieval performance between some commonly used Fourier based signatures and the proposed four sides distance (FSD) signature has been tested using MPEG-7 database. Experimental results are shown that the FSD signature has better performance compared with those signatures. version:1
arxiv-1208-6268 | Authorship Identification in Bengali Literature: a Comparative Analysis | http://arxiv.org/abs/1208.6268 | id:1208.6268 author:Tanmoy Chakraborty category:cs.CL cs.IR  published:2012-08-30 summary:Stylometry is the study of the unique linguistic styles and writing behaviors of individuals. It belongs to the core task of text categorization like authorship identification, plagiarism detection etc. Though reasonable number of studies have been conducted in English language, no major work has been done so far in Bengali. In this work, We will present a demonstration of authorship identification of the documents written in Bengali. We adopt a set of fine-grained stylistic features for the analysis of the text and use them to develop two different models: statistical similarity model consisting of three measures and their combination, and machine learning model with Decision Tree, Neural Network and SVM. Experimental results show that SVM outperforms other state-of-the-art methods after 10-fold cross validations. We also validate the relative importance of each stylistic feature to show that some of them remain consistently significant in every model used in this experiment. version:4
arxiv-1111-6201 | Learning a Factor Model via Regularized PCA | http://arxiv.org/abs/1111.6201 | id:1111.6201 author:Yi-Hao Kao, Benjamin Van Roy category:cs.LG stat.ML  published:2011-11-26 summary:We consider the problem of learning a linear factor model. We propose a regularized form of principal component analysis (PCA) and demonstrate through experiments with synthetic and real data the superiority of resulting estimates to those produced by pre-existing factor analysis approaches. We also establish theoretical results that explain how our algorithm corrects the biases induced by conventional approaches. An important feature of our algorithm is that its computational requirements are similar to those of PCA, which enjoys wide use in large part due to its efficiency. version:4
arxiv-1302-5797 | Prediction by Random-Walk Perturbation | http://arxiv.org/abs/1302.5797 | id:1302.5797 author:Luc Devroye, Gábor Lugosi, Gergely Neu category:cs.LG  published:2013-02-23 summary:We propose a version of the follow-the-perturbed-leader online prediction algorithm in which the cumulative losses are perturbed by independent symmetric random walks. The forecaster is shown to achieve an expected regret of the optimal order O(sqrt(n log N)) where n is the time horizon and N is the number of experts. More importantly, it is shown that the forecaster changes its prediction at most O(sqrt(n log N)) times, in expectation. We also extend the analysis to online combinatorial optimization and show that even in this more general setting, the forecaster rarely switches between experts while having a regret of near-optimal order. version:1
arxiv-1302-5762 | Probabilistic Non-Local Means | http://arxiv.org/abs/1302.5762 | id:1302.5762 author:Yue Wu, Brian Tracey, Premkumar Natarajan, Joseph P. Noonan category:cs.CV stat.AP stat.CO  published:2013-02-23 summary:In this paper, we propose a so-called probabilistic non-local means (PNLM) method for image denoising. Our main contributions are: 1) we point out defects of the weight function used in the classic NLM; 2) we successfully derive all theoretical statistics of patch-wise differences for Gaussian noise; and 3) we employ this prior information and formulate the probabilistic weights truly reflecting the similarity between two noisy patches. The probabilistic nature of the new weight function also provides a theoretical basis to choose thresholds rejecting dissimilar patches for fast computations. Our simulation results indicate the PNLM outperforms the classic NLM and many NLM recent variants in terms of peak signal noise ratio (PSNR) and structural similarity (SSIM) index. Encouraging improvements are also found when we replace the NLM weights with the probabilistic weights in tested NLM variants. version:1
arxiv-1208-0848 | Learning Theory Approach to Minimum Error Entropy Criterion | http://arxiv.org/abs/1208.0848 | id:1208.0848 author:Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou category:cs.LG stat.ML 68T05  68Q32  62B10  published:2012-08-03 summary:We consider the minimum error entropy (MEE) criterion and an empirical risk minimization learning algorithm in a regression setting. A learning theory approach is presented for this MEE algorithm and explicit error bounds are provided in terms of the approximation ability and capacity of the involved hypothesis space when the MEE scaling parameter is large. Novel asymptotic analysis is conducted for the generalization error associated with Renyi's entropy and a Parzen window function, to overcome technical difficulties arisen from the essential differences between the classical least squares problems and the MEE setting. A semi-norm and the involved symmetrized least squares error are introduced, which is related to some ranking algorithms. version:2
arxiv-1302-5675 | Development of Yes/No Arabic Question Answering System | http://arxiv.org/abs/1302.5675 | id:1302.5675 author:Wafa N. Bdour, Natheer K. Gharaibeh category:cs.CL cs.IR 14J26  published:2013-02-22 summary:Developing Question Answering systems has been one of the important research issues because it requires insights from a variety of disciplines,including,Artificial Intelligence,Information Retrieval, Information Extraction,Natural Language Processing, and Psychology.In this paper we realize a formal model for a lightweight semantic based open domain yes/no Arabic question answering system based on paragraph retrieval with variable length. We propose a constrained semantic representation. Using an explicit unification framework based on semantic similarities and query expansion synonyms and antonyms.This frequently improves the precision of the system. Employing the passage retrieval system achieves a better precision by retrieving more paragraphs that contain relevant answers to the question; It significantly reduces the amount of text to be processed by the system. version:1
arxiv-1205-4476 | Soft Rule Ensembles for Statistical Learning | http://arxiv.org/abs/1205.4476 | id:1205.4476 author:Deniz Akdemir, Nicolas Heslot category:stat.ML cs.LG stat.AP  published:2012-05-21 summary:In this article supervised learning problems are solved using soft rule ensembles. We first review the importance sampling learning ensembles (ISLE) approach that is useful for generating hard rules. The soft rules are then obtained with logistic regression from the corresponding hard rules. In order to deal with the perfect separation problem related to the logistic regression, Firth's bias corrected likelihood is used. Various examples and simulation results show that soft rule ensembles can improve predictive performance over hard rule ensembles. version:3
arxiv-1302-5608 | Accelerated Linear SVM Training with Adaptive Variable Selection Frequencies | http://arxiv.org/abs/1302.5608 | id:1302.5608 author:Tobias Glasmachers, Ürün Dogan category:stat.ML cs.LG  published:2013-02-22 summary:Support vector machine (SVM) training is an active research area since the dawn of the method. In recent years there has been increasing interest in specialized solvers for the important case of linear models. The algorithm presented by Hsieh et al., probably best known under the name of the "liblinear" implementation, marks a major breakthrough. The method is analog to established dual decomposition algorithms for training of non-linear SVMs, but with greatly reduced computational complexity per update step. This comes at the cost of not keeping track of the gradient of the objective any more, which excludes the application of highly developed working set selection algorithms. We present an algorithmic improvement to this method. We replace uniform working set selection with an online adaptation of selection frequencies. The adaptation criterion is inspired by modern second order working set selection methods. The same mechanism replaces the shrinking heuristic. This novel technique speeds up training in some cases by more than an order of magnitude. version:1
arxiv-1111-6937 | Efficient Discovery of Association Rules and Frequent Itemsets through Sampling with Tight Performance Guarantees | http://arxiv.org/abs/1111.6937 | id:1111.6937 author:Matteo Riondato, Eli Upfal category:cs.DS cs.DB cs.LG H.2.8  published:2011-11-29 summary:The tasks of extracting (top-$K$) Frequent Itemsets (FI's) and Association Rules (AR's) are fundamental primitives in data mining and database applications. Exact algorithms for these problems exist and are widely used, but their running time is hindered by the need of scanning the entire dataset, possibly multiple times. High quality approximations of FI's and AR's are sufficient for most practical uses, and a number of recent works explored the application of sampling for fast discovery of approximate solutions to the problems. However, these works do not provide satisfactory performance guarantees on the quality of the approximation, due to the difficulty of bounding the probability of under- or over-sampling any one of an unknown number of frequent itemsets. In this work we circumvent this issue by applying the statistical concept of \emph{Vapnik-Chervonenkis (VC) dimension} to develop a novel technique for providing tight bounds on the sample size that guarantees approximation within user-specified parameters. Our technique applies both to absolute and to relative approximations of (top-$K$) FI's and AR's. The resulting sample size is linearly dependent on the VC-dimension of a range space associated with the dataset to be mined. The main theoretical contribution of this work is a proof that the VC-dimension of this range space is upper bounded by an easy-to-compute characteristic quantity of the dataset which we call \emph{d-index}, and is the maximum integer $d$ such that the dataset contains at least $d$ transactions of length at least $d$ such that no one of them is a superset of or equal to another. We show that this bound is strict for a large class of datasets. version:6
arxiv-1208-4384 | Iterative graph cuts for image segmentation with a nonlinear statistical shape prior | http://arxiv.org/abs/1208.4384 | id:1208.4384 author:Joshua C. Chang, Tom Chou category:cs.CV math.OC physics.data-an q-bio.QM stat.AP  published:2012-08-21 summary:Shape-based regularization has proven to be a useful method for delineating objects within noisy images where one has prior knowledge of the shape of the targeted object. When a collection of possible shapes is available, the specification of a shape prior using kernel density estimation is a natural technique. Unfortunately, energy functionals arising from kernel density estimation are of a form that makes them impossible to directly minimize using efficient optimization algorithms such as graph cuts. Our main contribution is to show how one may recast the energy functional into a form that is minimizable iteratively and efficiently using graph cuts. version:2
arxiv-1302-5565 | The Importance of Clipping in Neurocontrol by Direct Gradient Descent on the Cost-to-Go Function and in Adaptive Dynamic Programming | http://arxiv.org/abs/1302.5565 | id:1302.5565 author:Michael Fairbank category:cs.LG  published:2013-02-22 summary:In adaptive dynamic programming, neurocontrol and reinforcement learning, the objective is for an agent to learn to choose actions so as to minimise a total cost function. In this paper we show that when discretized time is used to model the motion of the agent, it can be very important to do "clipping" on the motion of the agent in the final time step of the trajectory. By clipping we mean that the final time step of the trajectory is to be truncated such that the agent stops exactly at the first terminal state reached, and no distance further. We demonstrate that when clipping is omitted, learning performance can fail to reach the optimum; and when clipping is done properly, learning performance can improve significantly. The clipping problem we describe affects algorithms which use explicit derivatives of the model functions of the environment to calculate a learning gradient. These include Backpropagation Through Time for Control, and methods based on Dual Heuristic Dynamic Programming. However the clipping problem does not significantly affect methods based on Heuristic Dynamic Programming, Temporal Differences or Policy Gradient Learning algorithms. Similarly, the clipping problem does not affect fixed-length finite-horizon problems. version:1
arxiv-1301-3533 | Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint | http://arxiv.org/abs/1301.3533 | id:1301.3533 author:Xanadu Halkias, Sebastien Paris, Herve Glotin category:cs.NE cs.LG stat.ML  published:2013-01-16 summary:Deep Belief Networks (DBN) have been successfully applied on popular machine learning tasks. Specifically, when applied on hand-written digit recognition, DBNs have achieved approximate accuracy rates of 98.8%. In an effort to optimize the data representation achieved by the DBN and maximize their descriptive power, recent advances have focused on inducing sparse constraints at each layer of the DBN. In this paper we present a theoretical approach for sparse constraints in the DBN using the mixed norm for both non-overlapping and overlapping groups. We explore how these constraints affect the classification accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES) and provide initial estimations of their usefulness by altering different parameters such as the group size and overlap percentage. version:2
arxiv-1302-5449 | Nonparametric Basis Pursuit via Sparse Kernel-based Learning | http://arxiv.org/abs/1302.5449 | id:1302.5449 author:Juan Andres Bazerque, Georgios B. Giannakis category:cs.LG cs.CV cs.IT math.IT stat.ML  published:2013-02-21 summary:Signal processing tasks as fundamental as sampling, reconstruction, minimum mean-square error interpolation and prediction can be viewed under the prism of reproducing kernel Hilbert spaces. Endowing this vantage point with contemporary advances in sparsity-aware modeling and processing, promotes the nonparametric basis pursuit advocated in this paper as the overarching framework for the confluence of kernel-based learning (KBL) approaches leveraging sparse linear regression, nuclear-norm regularization, and dictionary learning. The novel sparse KBL toolbox goes beyond translating sparse parametric approaches to their nonparametric counterparts, to incorporate new possibilities such as multi-kernel selection and matrix smoothing. The impact of sparse KBL to signal processing applications is illustrated through test cases from cognitive radio sensing, microarray data imputation, and network traffic prediction. version:1
arxiv-1302-5189 | Object Detection in Real Images | http://arxiv.org/abs/1302.5189 | id:1302.5189 author:Dilip K. Prasad category:cs.CV  published:2013-02-21 summary:Object detection and recognition are important problems in computer vision. Since these problems are meta-heuristic, despite a lot of research, practically usable, intelligent, real-time, and dynamic object detection/recognition methods are still unavailable. We propose a new object detection/recognition method, which improves over the existing methods in every stage of the object detection/recognition process. In addition to the usual features, we propose to use geometric shapes, like linear cues, ellipses and quadrangles, as additional features. The full potential of geometric cues is exploited by using them to extract other features in a robust, computationally efficient, and less meta-heuristic manner. We also propose a new hierarchical codebook, which provides good generalization and discriminative properties. The codebook enables fast multi-path inference mechanisms based on propagation of conditional likelihoods, that make it robust to occlusion and noise. It has the capability of dynamic learning. We also propose a new learning method that has generative and discriminative learning capabilities, does not need large and fully supervised training dataset, and is capable of online learning. The preliminary work of detecting geometric shapes in real images has been completed. This preliminary work is the focus of this report. Future path for realizing the proposed object detection/recognition method is also discussed in brief. version:1
arxiv-1302-5181 | Basic Classes of Grammars with Prohibition | http://arxiv.org/abs/1302.5181 | id:1302.5181 author:Mark Burgin category:cs.FL cs.CL  published:2013-02-21 summary:A practical tool for natural language modeling and development of human-machine interaction is developed in the context of formal grammars and languages. A new type of formal grammars, called grammars with prohibition, is introduced. Grammars with prohibition provide more powerful tools for natural language generation and better describe processes of language learning than the conventional formal grammars. Here we study relations between languages generated by different grammars with prohibition based on conventional types of formal grammars such as context-free or context sensitive grammars. Besides, we compare languages generated by different grammars with prohibition and languages generated by conventional formal grammars. In particular, it is demonstrated that they have essentially higher computational power and expressive possibilities in comparison with the conventional formal grammars. Thus, while conventional formal grammars are recursive and subrecursive algorithms, many classes of grammars with prohibition are superrecursive algorithms. Results presented in this work are aimed at the development of human-machine interaction, modeling natural languages, empowerment of programming languages, computer simulation, better software systems, and theory of recursion. version:1
arxiv-1302-5134 | Spectral Clustering with Unbalanced Data | http://arxiv.org/abs/1302.5134 | id:1302.5134 author:Jing Qian, Venkatesh Saligrama category:stat.ML  published:2013-02-20 summary:Spectral clustering (SC) and graph-based semi-supervised learning (SSL) algorithms are sensitive to how graphs are constructed from data. In particular if the data has proximal and unbalanced clusters these algorithms can lead to poor performance on well-known graphs such as $k$-NN, full-RBF, $\epsilon$-graphs. This is because the objectives such as Ratio-Cut (RCut) or normalized cut (NCut) attempt to tradeoff cut values with cluster sizes, which are not tailored to unbalanced data. We propose a novel graph partitioning framework, which parameterizes a family of graphs by adaptively modulating node degrees in a $k$-NN graph. We then propose a model selection scheme to choose sizable clusters which are separated by smallest cut values. Our framework is able to adapt to varying levels of unbalancedness of data and can be naturally used for small cluster detection. We theoretically justify our ideas through limit cut analysis. Unsupervised and semi-supervised experiments on synthetic and real data sets demonstrate the superiority of our method. version:1
arxiv-1302-5125 | High-Dimensional Probability Estimation with Deep Density Models | http://arxiv.org/abs/1302.5125 | id:1302.5125 author:Oren Rippel, Ryan Prescott Adams category:stat.ML cs.LG  published:2013-02-20 summary:One of the fundamental problems in machine learning is the estimation of a probability distribution from data. Many techniques have been proposed to study the structure of data, most often building around the assumption that observations lie on a lower-dimensional manifold of high probability. It has been more difficult, however, to exploit this insight to build explicit, tractable density models for high-dimensional data. In this paper, we introduce the deep density model (DDM), a new approach to density estimation. We exploit insights from deep learning to construct a bijective map to a representation space, under which the transformation of the distribution of the data is approximately factorized and has identical and known marginal densities. The simplicity of the latent distribution under the model allows us to feasibly explore it, and the invertibility of the map to characterize contraction of measure across it. This enables us to compute normalized densities for out-of-sample data. This combination of tractability and flexibility allows us to tackle a variety of probabilistic tasks on high-dimensional datasets, including: rapid computation of normalized densities at test-time without evaluating a partition function; generation of samples without MCMC; and characterization of the joint entropy of the data. version:1
arxiv-1302-4964 | Estimating Continuous Distributions in Bayesian Classifiers | http://arxiv.org/abs/1302.4964 | id:1302.4964 author:George H. John, Pat Langley category:cs.LG cs.AI stat.ML  published:2013-02-20 summary:When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. version:1
arxiv-1302-4949 | A Characterization of the Dirichlet Distribution with Application to Learning Bayesian Networks | http://arxiv.org/abs/1302.4949 | id:1302.4949 author:Dan Geiger, David Heckerman category:cs.AI cs.LG  published:2013-02-20 summary:We provide a new characterization of the Dirichlet distribution. This characterization implies that under assumptions made by several previous authors for learning belief networks, a Dirichlet prior on the parameters is inevitable. version:1
arxiv-1205-4133 | Constrained Overcomplete Analysis Operator Learning for Cosparse Signal Modelling | http://arxiv.org/abs/1205.4133 | id:1205.4133 author:Mehrdad Yaghoobi, Sangnam Nam, Remi Gribonval, Mike E. Davies category:math.NA cs.LG  published:2012-05-18 summary:We consider the problem of learning a low-dimensional signal model from a collection of training samples. The mainstream approach would be to learn an overcomplete dictionary to provide good approximations of the training samples using sparse synthesis coefficients. This famous sparse model has a less well known counterpart, in analysis form, called the cosparse analysis model. In this new model, signals are characterised by their parsimony in a transformed domain using an overcomplete (linear) analysis operator. We propose to learn an analysis operator from a training corpus using a constrained optimisation framework based on L1 optimisation. The reason for introducing a constraint in the optimisation framework is to exclude trivial solutions. Although there is no final answer here for which constraint is the most relevant constraint, we investigate some conventional constraints in the model adaptation field and use the uniformly normalised tight frame (UNTF) for this purpose. We then derive a practical learning algorithm, based on projected subgradients and Douglas-Rachford splitting technique, and demonstrate its ability to robustly recover a ground truth analysis operator, when provided with a clean training set, of sufficient size. We also find an analysis operator for images, using some noisy cosparse signals, which is indeed a more realistic experiment. As the derived optimisation problem is not a convex program, we often find a local minimum using such variational methods. Some local optimality conditions are derived for two different settings, providing preliminary theoretical support for the well-posedness of the learning problem under appropriate conditions. version:2
arxiv-1302-4874 | A Labeled Graph Kernel for Relationship Extraction | http://arxiv.org/abs/1302.4874 | id:1302.4874 author:Gonçalo Simões, Helena Galhardas, David Matos category:cs.CL cs.LG  published:2013-02-20 summary:In this paper, we propose an approach for Relationship Extraction (RE) based on labeled graph kernels. The kernel we propose is a particularization of a random walk kernel that exploits two properties previously studied in the RE literature: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions. We performed experiments on a dataset of protein-protein interactions and the results show that our approach obtains effectiveness values that are comparable with the state-of-the art kernel methods. Moreover, our approach is able to outperform the state-of-the-art kernels when combined with other kernel methods. version:1
arxiv-1302-4549 | Breaking the Small Cluster Barrier of Graph Clustering | http://arxiv.org/abs/1302.4549 | id:1302.4549 author:Nir Ailon, Yudong Chen, Xu Huan category:cs.LG stat.ML  published:2013-02-19 summary:This paper investigates graph clustering in the planted cluster model in the presence of {\em small clusters}. Traditional results dictate that for an algorithm to provably correctly recover the clusters, {\em all} clusters must be sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based recovery approach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones. Based on this result, we further devise an iterative algorithm to recover {\em almost all clusters} via a "peeling strategy", i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the {\em partial observation} setting, in which only a (chosen) part of the graph is observed.The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed). From a high level, this paper sheds novel insights on high-dimensional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxationsdoes the job. version:2
arxiv-1302-4814 | NLP and CALL: integration is working | http://arxiv.org/abs/1302.4814 | id:1302.4814 author:Georges Antoniadis, Sylviane Granger, Olivier Kraif, Claude Ponton, Virginie Zampa category:cs.CL  published:2013-02-20 summary:In the first part of this article, we explore the background of computer-assisted learning from its beginnings in the early XIXth century and the first teaching machines, founded on theories of learning, at the start of the XXth century. With the arrival of the computer, it became possible to offer language learners different types of language activities such as comprehension tasks, simulations, etc. However, these have limits that cannot be overcome without some contribution from the field of natural language processing (NLP). In what follows, we examine the challenges faced and the issues raised by integrating NLP into CALL. We hope to demonstrate that the key to success in integrating NLP into CALL is to be found in multidisciplinary work between computer experts, linguists, language teachers, didacticians and NLP specialists. version:1
arxiv-1302-4813 | Probabilistic Frame Induction | http://arxiv.org/abs/1302.4813 | id:1302.4813 author:Jackie Chi Kit Cheung, Hoifung Poon, Lucy Vanderwende category:cs.CL  published:2013-02-20 summary:In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, participants as latent topics and learns those frame and event transitions that best explain the text. The number of frames is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produced state-of-the-art results while substantially reducing engineering effort. version:1
arxiv-1302-4811 | Towards a Semantic-based Approach for Modeling Regulatory Documents in Building Industry | http://arxiv.org/abs/1302.4811 | id:1302.4811 author:Khalil Riad Bouzidi, Catherine Faron-Zucker, Bruno Fies, Olivier Corby, Le-Thanh Nhan category:cs.CL  published:2013-02-20 summary:Regulations in the Building Industry are becoming increasingly complex and involve more than one technical area. They cover products, components and project implementation. They also play an important role to ensure the quality of a building, and to minimize its environmental impact. In this paper, we are particularly interested in the modeling of the regulatory constraints derived from the Technical Guides issued by CSTB and used to validate Technical Assessments. We first describe our approach for modeling regulatory constraints in the SBVR language, and formalizing them in the SPARQL language. Second, we describe how we model the processes of compliance checking described in the CSTB Technical Guides. Third, we show how we implement these processes to assist industrials in drafting Technical Documents in order to acquire a Technical Assessment; a compliance report is automatically generated to explain the compliance or noncompliance of this Technical Documents. version:1
arxiv-1302-4784 | An Optical Watermarking Solution for Color Personal Identification Pictures | http://arxiv.org/abs/1302.4784 | id:1302.4784 author:Tan Yi-zhou, Liu Hai-bo, Huang Shui-hua, Sheng Ben-jian, Pan Zhong-ming category:cs.MM cs.CV physics.optics  published:2013-02-20 summary:This paper presents a new approach for embedding authentication information into image on printed materials based on optical projection technique. Our experimental setup consists of two parts, one is a common camera, and the other is a LCD projector, which project a pattern on personnel's body (especially on the face). The pattern, generated by a computer, act as the illumination light source with sinusoidal distribution and it is also the watermark signal. For a color image, the watermark is embedded into the blue channel. While we take pictures (256 *256 and 512*512, 567*390 pixels, respectively), an invisible mark is embedded directly into magnitude oefficients of Discrete Fourier transform (DFT) at exposure moment. Both optical an d digital correlation is suitable for detection of this type of watermark. The decoded watermark is a set of concentric circles or sectors in the DFT domain (middle frequencies region) which is robust to photographing, printing and scanning. The unlawful people modify or replace the original photograph, and make fake passport (drivers' license and so on). Experiments show, it is difficult to forge certificates in which a watermark was embedded by our projector-camera combination based on analogue watermark method rather than classical digital method. version:1
arxiv-1302-4773 | Optimal Discriminant Functions Based On Sampled Distribution Distance for Modulation Classification | http://arxiv.org/abs/1302.4773 | id:1302.4773 author:Paulo Urriza, Eric Rebeiz, Danijela Cabric category:stat.ML cs.LG cs.PF  published:2013-02-19 summary:In this letter, we derive the optimal discriminant functions for modulation classification based on the sampled distribution distance. The proposed method classifies various candidate constellations using a low complexity approach based on the distribution distance at specific testpoints along the cumulative distribution function. This method, based on the Bayesian decision criteria, asymptotically provides the minimum classification error possible given a set of testpoints. Testpoint locations are also optimized to improve classification performance. The method provides significant gains over existing approaches that also use the distribution of the signal features. version:1
arxiv-1302-4726 | An Ontology for Modelling and Supporting the Process of Authoring Technical Assessments | http://arxiv.org/abs/1302.4726 | id:1302.4726 author:Khalil Riad Bouzidi, Bruno Fies, Marc Bourdeau, Catherine Faron-Zucker, Nhan Le-Thanh category:cs.IR cs.CL cs.DL  published:2013-02-19 summary:In this paper, we present a semantic web approach for modelling the process of creating new technical and regulatory documents related to the Building sector. This industry, among other industries, is currently experiencing a phenomenal growth in its technical and regulatory texts. Therefore, it is urgent and crucial to improve the process of creating regulations by automating it as much as possible. We focus on the creation of particular technical documents issued by the French Scientific and Technical Centre for Building (CSTB), called Technical Assessments, and we propose services based on Semantic Web models and techniques for modelling the process of their creation. version:1
arxiv-1204-6160 | Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting | http://arxiv.org/abs/1204.6160 | id:1204.6160 author:José E. Chacón, Tarn Duong category:math.ST stat.ME stat.ML stat.TH 62G05  62H30  published:2012-04-27 summary:Important information concerning a multivariate data set, such as clusters and modal regions, is contained in the derivatives of the probability density function. Despite this importance, nonparametric estimation of higher order derivatives of the density functions have received only relatively scant attention. Kernel estimators of density functions are widely used as they exhibit excellent theoretical and practical properties, though their generalization to density derivatives has progressed more slowly due to the mathematical intractabilities encountered in the crucial problem of bandwidth (or smoothing parameter) selection. This paper presents the first fully automatic, data-based bandwidth selectors for multivariate kernel density derivative estimators. This is achieved by synthesizing recent advances in matrix analytic theory which allow mathematically and computationally tractable representations of higher order derivatives of multivariate vector valued functions. The theoretical asymptotic properties as well as the finite sample behaviour of the proposed selectors are studied. {In addition, we explore in detail the applications of the new data-driven methods for two other statistical problems: clustering and bump hunting. The introduced techniques are combined with the mean shift algorithm to develop novel automatic, nonparametric clustering procedures which are shown to outperform mixture-model cluster analysis and other recent nonparametric approaches in practice. Furthermore, the advantage of the use of smoothing parameters designed for density derivative estimation for feature significance analysis for bump hunting is illustrated with a real data example. version:3
arxiv-1209-1121 | Learning Manifolds with K-Means and K-Flats | http://arxiv.org/abs/1209.1121 | id:1209.1121 author:Guillermo D. Canas, Tomaso Poggio, Lorenzo Rosasco category:cs.LG stat.ML K.3.2  published:2012-09-05 summary:We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats, and analyze their performance. We extend previous results for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-flats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-flats, both the results and the mathematical tools are new. version:4
arxiv-1302-4673 | Good Recognition is Non-Metric | http://arxiv.org/abs/1302.4673 | id:1302.4673 author:Walter J. Scheirer, Michael J. Wilber, Michael Eckmann, Terrance E. Boult category:cs.CV  published:2013-02-19 summary:Recognition is the fundamental task of visual cognition, yet how to formalize the general recognition problem for computer vision remains an open issue. The problem is sometimes reduced to the simplest case of recognizing matching pairs, often structured to allow for metric constraints. However, visual recognition is broader than just pair matching -- especially when we consider multi-class training data and large sets of features in a learning context. What we learn and how we learn it has important implications for effective algorithms. In this paper, we reconsider the assumption of recognition as a pair matching test, and introduce a new formal definition that captures the broader context of the problem. Through a meta-analysis and an experimental assessment of the top algorithms on popular data sets, we gain a sense of how often metric properties are violated by good recognition algorithms. By studying these violations, useful insights come to light: we make the case that locally metric algorithms should leverage outside information to solve the general recognition problem. version:1
arxiv-1104-1990 | Adaptive Evolutionary Clustering | http://arxiv.org/abs/1104.1990 | id:1104.1990 author:Kevin S. Xu, Mark Kliger, Alfred O. Hero III category:cs.LG stat.ML I.5.3; H.3.3; G.3  published:2011-04-11 summary:In many practical applications of clustering, the objects to be clustered evolve over time, and a clustering result is desired at each time step. In such applications, evolutionary clustering typically outperforms traditional static clustering by producing clustering results that reflect long-term trends while being robust to short-term variations. Several evolutionary clustering algorithms have recently been proposed, often by adding a temporal smoothness penalty to the cost function of a static clustering method. In this paper, we introduce a different approach to evolutionary clustering by accurately tracking the time-varying proximities between objects followed by static clustering. We present an evolutionary clustering framework that adaptively estimates the optimal smoothing parameter using shrinkage estimation, a statistical approach that improves a naive estimate using additional information. The proposed framework can be used to extend a variety of static clustering algorithms, including hierarchical, k-means, and spectral clustering, into evolutionary clustering algorithms. Experiments on synthetic and real data sets indicate that the proposed framework outperforms static clustering and existing evolutionary clustering algorithms in many scenarios. version:3
arxiv-1302-4619 | Compactified Horizontal Visibility Graph for the Language Network | http://arxiv.org/abs/1302.4619 | id:1302.4619 author:D. V. Lande, A. A. Snarskii category:cs.CL cs.DS  published:2013-02-19 summary:A compactified horizontal visibility graph for the language network is proposed. It was found that the networks constructed in such way are scale free, and have a property that among the nodes with largest degrees there are words that determine not only a text structure communication, but also its informational structure. version:1
arxiv-1301-7738 | PyPLN: a Distributed Platform for Natural Language Processing | http://arxiv.org/abs/1301.7738 | id:1301.7738 author:Flávio Codeço Coelho, Renato Rocha Souza, Álvaro Justen, Flávio Amieiro, Heliana Mello category:cs.CL cs.IR  published:2013-01-31 summary:This paper presents a distributed platform for Natural Language Processing called PyPLN. PyPLN leverages a vast array of NLP and text processing open source tools, managing the distribution of the workload on a variety of configurations: from a single server to a cluster of linux servers. PyPLN is developed using Python 2.7.3 but makes it very easy to incorporate other softwares for specific tasks as long as a linux version is available. PyPLN facilitates analyses both at document and corpus level, simplifying management and publication of corpora and analytical results through an easy to use web interface. In the current (beta) release, it supports English and Portuguese languages with support to other languages planned for future releases. To support the Portuguese language PyPLN uses the PALAVRAS parser\citep{Bick2000}. Currently PyPLN offers the following features: Text extraction with encoding normalization (to UTF-8), part-of-speech tagging, token frequency, semantic annotation, n-gram extraction, word and sentence repertoire, and full-text search across corpora. The platform is licensed as GPL-v3. version:2
arxiv-1302-4519 | A Genetic Algorithm for Power-Aware Virtual Machine Allocation in Private Cloud | http://arxiv.org/abs/1302.4519 | id:1302.4519 author:Nguyen Quang-Hung, Pham Dac Nien, Nguyen Hoai Nam, Nguyen Huynh Tuong, Nam Thoai category:cs.NE cs.DC  published:2013-02-19 summary:Energy efficiency has become an important measurement of scheduling algorithm for private cloud. The challenge is trade-off between minimizing of energy consumption and satisfying Quality of Service (QoS) (e.g. performance or resource availability on time for reservation request). We consider resource needs in context of a private cloud system to provide resources for applications in teaching and researching. In which users request computing resources for laboratory classes at start times and non-interrupted duration in some hours in prior. Many previous works are based on migrating techniques to move online virtual machines (VMs) from low utilization hosts and turn these hosts off to reduce energy consumption. However, the techniques for migration of VMs could not use in our case. In this paper, a genetic algorithm for power-aware in scheduling of resource allocation (GAPA) has been proposed to solve the static virtual machine allocation problem (SVMAP). Due to limited resources (i.e. memory) for executing simulation, we created a workload that contains a sample of one-day timetable of lab hours in our university. We evaluate the GAPA and a baseline scheduling algorithm (BFD), which sorts list of virtual machines in start time (i.e. earliest start time first) and using best-fit decreasing (i.e. least increased power consumption) algorithm, for solving the same SVMAP. As a result, the GAPA algorithm obtains total energy consumption is lower than the baseline algorithm on simulated experimentation. version:1
arxiv-1302-4492 | Bilingual Terminology Extraction Using Multi-level Termhood | http://arxiv.org/abs/1302.4492 | id:1302.4492 author:Chengzhi Zhang, Dan Wu category:cs.CL  published:2013-02-19 summary:Purpose: Terminology is the set of technical words or expressions used in specific contexts, which denotes the core concept in a formal discipline and is usually applied in the fields of machine translation, information retrieval, information extraction and text categorization, etc. Bilingual terminology extraction plays an important role in the application of bilingual dictionary compilation, bilingual Ontology construction, machine translation and cross-language information retrieval etc. This paper addresses the issues of monolingual terminology extraction and bilingual term alignment based on multi-level termhood. Design/methodology/approach: A method based on multi-level termhood is proposed. The new method computes the termhood of the terminology candidate as well as the sentence that includes the terminology by the comparison of the corpus. Since terminologies and general words usually have differently distribution in the corpus, termhood can also be used to constrain and enhance the performance of term alignment when aligning bilingual terms on the parallel corpus. In this paper, bilingual term alignment based on termhood constraints is presented. Findings: Experiment results show multi-level termhood can get better performance than existing method for terminology extraction. If termhood is used as constrain factor, the performance of bilingual term alignment can be improved. version:1
arxiv-1302-4490 | Complex networks analysis of language complexity | http://arxiv.org/abs/1302.4490 | id:1302.4490 author:Diego R. Amancio, Sandra M. Aluisio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL cs.SI physics.data-an  published:2013-02-19 summary:Methods from statistical physics, such as those involving complex networks, have been increasingly used in quantitative analysis of linguistic phenomena. In this paper, we represented pieces of text with different levels of simplification in co-occurrence networks and found that topological regularity correlated negatively with textual complexity. Furthermore, in less complex texts the distance between concepts, represented as nodes, tended to decrease. The complex networks metrics were treated with multivariate pattern recognition techniques, which allowed us to distinguish between original texts and their simplified versions. For each original text, two simplified versions were generated manually with increasing number of simplification operations. As expected, distinction was easier for the strongly simplified versions, where the most relevant metrics were node strength, shortest paths and diversity. Also, the discrimination of complex texts was improved with higher hierarchical network metrics, thus pointing to the usefulness of considering wider contexts around the concepts. Though the accuracy rate in the distinction was not as high as in methods using deep linguistic knowledge, the complex network approach is still useful for a rapid screening of texts whenever assessing complexity is essential to guarantee accessibility to readers with limited reading ability version:1
arxiv-1302-4489 | Termhood-based Comparability Metrics of Comparable Corpus in Special Domain | http://arxiv.org/abs/1302.4489 | id:1302.4489 author:Sa Liu, Chengzhi Zhang category:cs.CL  published:2013-02-19 summary:Cross-Language Information Retrieval (CLIR) and machine translation (MT) resources, such as dictionaries and parallel corpora, are scarce and hard to come by for special domains. Besides, these resources are just limited to a few languages, such as English, French, and Spanish and so on. So, obtaining comparable corpora automatically for such domains could be an answer to this problem effectively. Comparable corpora, that the subcorpora are not translations of each other, can be easily obtained from web. Therefore, building and using comparable corpora is often a more feasible option in multilingual information processing. Comparability metrics is one of key issues in the field of building and using comparable corpus. Currently, there is no widely accepted definition or metrics method of corpus comparability. In fact, Different definitions or metrics methods of comparability might be given to suit various tasks about natural language processing. A new comparability, namely, termhood-based metrics, oriented to the task of bilingual terminology extraction, is proposed in this paper. In this method, words are ranked by termhood not frequency, and then the cosine similarities, calculated based on the ranking lists of word termhood, is used as comparability. Experiments results show that termhood-based metrics performs better than traditional frequency-based metrics. version:1
arxiv-1302-4471 | Word sense disambiguation via high order of learning in complex networks | http://arxiv.org/abs/1302.4471 | id:1302.4471 author:Thiago C. Silva, Diego R. Amancio category:physics.soc-ph cs.CL cs.SI physics.data-an  published:2013-02-18 summary:Complex networks have been employed to model many real systems and as a modeling tool in a myriad of applications. In this paper, we use the framework of complex networks to the problem of supervised classification in the word disambiguation task, which consists in deriving a function from the supervised (or labeled) training data of ambiguous words. Traditional supervised data classification takes into account only topological or physical features of the input data. On the other hand, the human (animal) brain performs both low- and high-level orders of learning and it has facility to identify patterns according to the semantic meaning of the input data. In this paper, we apply a hybrid technique which encompasses both types of learning in the field of word sense disambiguation and show that the high-level order of learning can really improve the accuracy rate of the model. This evidence serves to demonstrate that the internal structures formed by the words do present patterns that, generally, cannot be correctly unveiled by only traditional techniques. Finally, we exhibit the behavior of the model for different weights of the low- and high-level classifiers by plotting decision boundaries. This study helps one to better understand the effectiveness of the model. version:1
arxiv-1302-4465 | Unveiling the relationship between complex networks metrics and word senses | http://arxiv.org/abs/1302.4465 | id:1302.4465 author:Diego R. Amancio, Osvaldo N. Oliveira Jr., Luciano da F. Costa category:physics.soc-ph cs.CL cs.SI physics.data-an  published:2013-02-18 summary:The automatic disambiguation of word senses (i.e., the identification of which of the meanings is used in a given context for a word that has multiple meanings) is essential for such applications as machine translation and information retrieval, and represents a key step for developing the so-called Semantic Web. Humans disambiguate words in a straightforward fashion, but this does not apply to computers. In this paper we address the problem of Word Sense Disambiguation (WSD) by treating texts as complex networks, and show that word senses can be distinguished upon characterizing the local structure around ambiguous words. Our goal was not to obtain the best possible disambiguation system, but we nevertheless found that in half of the cases our approach outperforms traditional shallow methods. We show that the hierarchical connectivity and clustering of words are usually the most relevant features for WSD. The results reported here shine light on the relationship between semantic and structural parameters of complex networks. They also indicate that when combined with traditional techniques the complex network approach may be useful to enhance the discrimination of senses in large texts version:1
arxiv-1302-4383 | Explaining Zipf's Law via Mental Lexicon | http://arxiv.org/abs/1302.4383 | id:1302.4383 author:Armen E. Allahverdyan, Weibing Deng, Q. A. Wang category:physics.data-an cond-mat.stat-mech cs.CL  published:2013-02-18 summary:The Zipf's law is the major regularity of statistical linguistics that served as a prototype for rank-frequency relations and scaling laws in natural sciences. Here we show that the Zipf's law -- together with its applicability for a single text and its generalizations to high and low frequencies including hapax legomena -- can be derived from assuming that the words are drawn into the text with random probabilities. Their apriori density relates, via the Bayesian statistics, to general features of the mental lexicon of the author who produced the text. version:1
arxiv-1302-4343 | On Translation Invariant Kernels and Screw Functions | http://arxiv.org/abs/1302.4343 | id:1302.4343 author:Purushottam Kar, Harish Karnick category:math.FA cs.LG stat.ML  published:2013-02-18 summary:We explore the connection between Hilbertian metrics and positive definite kernels on the real line. In particular, we look at a well-known characterization of translation invariant Hilbertian metrics on the real line by von Neumann and Schoenberg (1941). Using this result we are able to give an alternate proof of Bochner's theorem for translation invariant positive definite kernels on the real line (Rudin, 1962). version:1
arxiv-1206-1106 | No More Pesky Learning Rates | http://arxiv.org/abs/1206.1106 | id:1206.1106 author:Tom Schaul, Sixin Zhang, Yann LeCun category:stat.ML cs.LG  published:2012-06-06 summary:The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. In our approach, learning rates can increase as well as decrease, making it suitable for non-stationary problems. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of SGD or other adaptive approaches with their best settings obtained through systematic search, and effectively removes the need for learning rate tuning. version:2
arxiv-1302-5645 | Role of temporal inference in the recognition of textual inference | http://arxiv.org/abs/1302.5645 | id:1302.5645 author:Djallel Bouneffouf category:cs.CL I.2.7  published:2013-02-18 summary:This project is a part of nature language processing and its aims to develop a system of recognition inference text-appointed TIMINF. This type of system can detect, given two portions of text, if a text is semantically deducted from the other. We focused on making the inference time in this type of system. For that we have built and analyzed a body built from questions collected through the web. This study has enabled us to classify different types of times inferences and for designing the architecture of TIMINF which seeks to integrate a module inference time in a detection system inference text. We also assess the performance of sorties TIMINF system on a test corpus with the same strategy adopted in the challenge RTE. version:1
arxiv-1302-4141 | Canonical dual solutions to nonconvex radial basis neural network optimization problem | http://arxiv.org/abs/1302.4141 | id:1302.4141 author:Vittorio Latorre, David Yang Gao category:cs.NE cs.LG stat.ML  published:2013-02-18 summary:Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in regression problems. One of their principal drawbacks is that the formulation corresponding to the training with the supervision of both the centers and the weights is a highly non-convex optimization problem, which leads to some fundamentally difficulties for traditional optimization theory and methods. This paper presents a generalized canonical duality theory for solving this challenging problem. We demonstrate that by sequential canonical dual transformations, the nonconvex optimization problem of the RBFNN can be reformulated as a canonical dual problem (without duality gap). Both global optimal solution and local extrema can be classified. Several applications to one of the most used Radial Basis Functions, the Gaussian function, are illustrated. Our results show that even for one-dimensional case, the global minimizer of the nonconvex problem may not be the best solution to the RBFNNs, and the canonical dual theory is a promising tool for solving general neural networks training problems. version:1
arxiv-1302-2277 | A Time Series Forest for Classification and Feature Extraction | http://arxiv.org/abs/1302.2277 | id:1302.2277 author:Houtao Deng, George Runger, Eugene Tuv, Martyanov Vladimir category:cs.LG  published:2013-02-09 summary:We propose a tree ensemble method, referred to as time series forest (TSF), for time series classification. TSF employs a combination of the entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain criterion improves the accuracy of TSF. TSF randomly samples features at each tree node and has a computational complexity linear in the length of a time series and can be built using parallel computing techniques such as multi-core computing used here. The temporal importance curve is also proposed to capture the important temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, deviation and slope outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping, is computationally efficient, and can provide insights into the temporal characteristics. version:2
arxiv-1302-4043 | A new scheme of signature extraction for iris authentication | http://arxiv.org/abs/1302.4043 | id:1302.4043 author:Belhassen Akrout, Imen Khanfir Kallel, Chokri Ben Amar category:cs.CV  published:2013-02-17 summary:Iris recognition, a relatively new biometric technology, has great advantages, such as variability, stability and security, thus is the most promising for high security environment. Iris recognition is proposed in this report. We describe some methods, the first one is based on grey level histogram to extract the pupil, the second is based on elliptic and parabolic HOUGH transformation to determinate the edge of iris, upper and lower eyelids, the third we used 2D Gabor Wavelets to encode the iris and finally we used the Hamming distance for authentication. version:1
arxiv-1211-1968 | Fourier-Bessel rotational invariant eigenimages | http://arxiv.org/abs/1211.1968 | id:1211.1968 author:Zhizhen Zhao, Amit Singer category:cs.CV  published:2012-11-08 summary:We present an efficient and accurate algorithm for principal component analysis (PCA) of a large set of two dimensional images, and, for each image, the set of its uniform rotations in the plane and its reflection. The algorithm starts by expanding each image, originally given on a Cartesian grid, in the Fourier-Bessel basis for the disk. Because the images are bandlimited in the Fourier domain, we use a sampling criterion to truncate the Fourier-Bessel expansion such that the maximum amount of information is preserved without the effect of aliasing. The constructed covariance matrix is invariant to rotation and reflection and has a special block diagonal structure. PCA is efficiently done for each block separately. This Fourier-Bessel based PCA detects more meaningful eigenimages and has improved denoising capability compared to traditional PCA for a finite number of noisy images. version:2
arxiv-1302-3979 | Gaussian Process Vine Copulas for Multivariate Dependence | http://arxiv.org/abs/1302.3979 | id:1302.3979 author:David Lopez-Paz, José Miguel Hernández-Lobato, Zoubin Ghahramani category:stat.ME stat.ML  published:2013-02-16 summary:Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function. Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables. In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data. version:1
arxiv-1212-1524 | Layer-wise learning of deep generative models | http://arxiv.org/abs/1212.1524 | id:1212.1524 author:Ludovic Arnold, Yann Ollivier category:cs.NE cs.LG stat.ML  published:2012-12-07 summary:When using deep, multi-layered architectures to build generative models of data, it is difficult to train all layers at once. We propose a layer-wise training procedure admitting a performance guarantee compared to the global optimum. It is based on an optimistic proxy of future performance, the best latent marginal. We interpret auto-encoders in this setting as generative models, by showing that they train a lower bound of this criterion. We test the new learning procedure against a state of the art method (stacked RBMs), and find it to improve performance. Both theory and experiments highlight the importance, when training deep architectures, of using an inference model (from data to hidden variables) richer than the generative model (from hidden variables to data). version:2
arxiv-1302-3956 | Clustering validity based on the most similarity | http://arxiv.org/abs/1302.3956 | id:1302.3956 author:Raheleh Namayandeh, Farzad Didehvar, Zahra Shojaei category:cs.LG stat.ML 68T05  published:2013-02-16 summary:One basic requirement of many studies is the necessity of classifying data. Clustering is a proposed method for summarizing networks. Clustering methods can be divided into two categories named model-based approaches and algorithmic approaches. Since the most of clustering methods depend on their input parameters, it is important to evaluate the result of a clustering algorithm with its different input parameters, to choose the most appropriate one. There are several clustering validity techniques based on inner density and outer density of clusters that represent different metrics to choose the most appropriate clustering independent of the input parameters. According to dependency of previous methods on the input parameters, one challenge in facing with large systems, is to complete data incrementally that effects on the final choice of the most appropriate clustering. Those methods define the existence of high intensity in a cluster, and low intensity among different clusters as the measure of choosing the optimal clustering. This measure has a tremendous problem, not availing all data at the first stage. In this paper, we introduce an efficient measure in which maximum number of repetitions for various initial values occurs. version:1
arxiv-1211-5063 | On the difficulty of training Recurrent Neural Networks | http://arxiv.org/abs/1211.5063 | id:1211.5063 author:Razvan Pascanu, Tomas Mikolov, Yoshua Bengio category:cs.LG  published:2012-11-21 summary:There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section. version:2
arxiv-1212-2126 | MAD-Bayes: MAP-based Asymptotic Derivations from Bayes | http://arxiv.org/abs/1212.2126 | id:1212.2126 author:Tamara Broderick, Brian Kulis, Michael I. Jordan category:stat.ML  published:2012-12-10 summary:The classical mixture of Gaussians model is related to K-means via small-variance asymptotics: as the covariances of the Gaussians tend to zero, the negative log-likelihood of the mixture of Gaussians model approaches the K-means objective, and the EM algorithm approaches the K-means algorithm. Kulis & Jordan (2012) used this observation to obtain a novel K-means-like algorithm from a Gibbs sampler for the Dirichlet process (DP) mixture. We instead consider applying small-variance asymptotics directly to the posterior in Bayesian nonparametric models. This framework is independent of any specific Bayesian inference algorithm, and it has the major advantage that it generalizes immediately to a range of models beyond the DP mixture. To illustrate, we apply our framework to the feature learning setting, where the beta process and Indian buffet process provide an appropriate Bayesian nonparametric prior. We obtain a novel objective function that goes beyond clustering to learn (and penalize new) groupings for which we relax the mutual exclusivity and exhaustivity assumptions of clustering. We demonstrate several other algorithms, all of which are scalable and simple to implement. Empirical results demonstrate the benefits of the new framework. version:2
arxiv-1302-3900 | Robust Image Segmentation in Low Depth Of Field Images | http://arxiv.org/abs/1302.3900 | id:1302.3900 author:Franz Graf, Hans-Peter Kriegel, Michael Weiler category:cs.CV  published:2013-02-15 summary:In photography, low depth of field (DOF) is an important technique to emphasize the object of interest (OOI) within an image. Thus, low DOF images are widely used in the application area of macro, portrait or sports photography. When viewing a low DOF image, the viewer implicitly concentrates on the regions that are sharper regions of the image and thus segments the image into regions of interest and non regions of interest which has a major impact on the perception of the image. Thus, a robust algorithm for the fully automatic detection of the OOI in low DOF images provides valuable information for subsequent image processing and image retrieval. In this paper we propose a robust and parameterless algorithm for the fully automatic segmentation of low DOF images. We compare our method with three similar methods and show the superior robustness even though our algorithm does not require any parameters to be set by hand. The experiments are conducted on a real world data set with high and low DOF images. version:1
arxiv-1302-3892 | Identifying trends in word frequency dynamics | http://arxiv.org/abs/1302.3892 | id:1302.3892 author:Eduardo G. Altmann, Zakary L. Whichard, Adilson E. Motter category:physics.soc-ph cond-mat.dis-nn cs.CL q-bio.PE  published:2013-02-15 summary:The word-stock of a language is a complex dynamical system in which words can be created, evolve, and become extinct. Even more dynamic are the short-term fluctuations in word usage by individuals in a population. Building on the recent demonstration that word niche is a strong determinant of future rise or fall in word frequency, here we introduce a model that allows us to distinguish persistent from temporary increases in frequency. Our model is illustrated using a 10^8-word database from an online discussion group and a 10^11-word collection of digitized books. The model reveals a strong relation between changes in word dissemination and changes in frequency. Aside from their implications for short-term word frequency dynamics, these observations are potentially important for language evolution as new words must survive in the short term in order to survive in the long term. version:1
arxiv-1302-1156 | A Non-Binary Associative Memory with Exponential Pattern Retrieval Capacity and Iterative Learning: Extended Results | http://arxiv.org/abs/1302.1156 | id:1302.1156 author:Amir Hesam Salavati, K. Raj Kumar, Amin Shokrollahi category:cs.NE  published:2013-02-05 summary:We consider the problem of neural association for a network of non-binary neurons. Here, the task is to first memorize a set of patterns using a network of neurons whose states assume values from a finite number of integer levels. Later, the same network should be able to recall previously memorized patterns from their noisy versions. Prior work in this area consider storing a finite number of purely random patterns, and have shown that the pattern retrieval capacities (maximum number of patterns that can be memorized) scale only linearly with the number of neurons in the network. In our formulation of the problem, we concentrate on exploiting redundancy and internal structure of the patterns in order to improve the pattern retrieval capacity. Our first result shows that if the given patterns have a suitable linear-algebraic structure, i.e. comprise a sub-space of the set of all possible patterns, then the pattern retrieval capacity is in fact exponential in terms of the number of neurons. The second result extends the previous finding to cases where the patterns have weak minor components, i.e. the smallest eigenvalues of the correlation matrix tend toward zero. We will use these minor components (or the basis vectors of the pattern null space) to both increase the pattern retrieval capacity and error correction capabilities. An iterative algorithm is proposed for the learning phase, and two simple neural update algorithms are presented for the recall phase. Using analytical results and simulations, we show that the proposed methods can tolerate a fair amount of errors in the input while being able to memorize an exponentially large number of patterns. version:2
arxiv-1209-1119 | Augment-and-Conquer Negative Binomial Processes | http://arxiv.org/abs/1209.1119 | id:1209.1119 author:Mingyuan Zhou, Lawrence Carin category:stat.ML stat.ME  published:2012-09-05 summary:By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters. version:2
arxiv-1302-3721 | Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection | http://arxiv.org/abs/1302.3721 | id:1302.3721 author:Joseph Mellor, Jonathan Shapiro category:cs.LG  published:2013-02-15 summary:Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective. version:1
arxiv-1302-3702 | A Fresnelet-Based Encryption of Medical Images using Arnold Transform | http://arxiv.org/abs/1302.3702 | id:1302.3702 author:Muhammad Nazeer, Bibi Nargis, Yasir Mehmood Malik, Dai-Gyoung Kim category:cs.CR cs.CV 68U10  68M11  published:2013-02-15 summary:Medical images are commonly stored in digital media and transmitted via Internet for certain uses. If a medical information image alters, this can lead to a wrong diagnosis which may create a serious health problem. Moreover, medical images in digital form can easily be modified by wiping off or adding small pieces of information intentionally for certain illegal purposes. Hence, the reliability of medical images is an important criterion in a hospital information system. In this paper, Fresnelet transform is employed along with appropriate handling of the Arnold transform and the discrete cosine transform to provide secure distribution of medical images. This method presents a new data hiding system in which steganography and cryptography are used to prevent unauthorized data access. The experimental results exhibit high imperceptibility for embedded images and significant encryption of information images. version:1
arxiv-1302-3700 | Density Ratio Hidden Markov Models | http://arxiv.org/abs/1302.3700 | id:1302.3700 author:John A. Quinn, Masashi Sugiyama category:stat.ML cs.LG  published:2013-02-15 summary:Hidden Markov models and their variants are the predominant sequential classification method in such domains as speech recognition, bioinformatics and natural language processing. Being generative rather than discriminative models, however, their classification performance is a drawback. In this paper we apply ideas from the field of density ratio estimation to bypass the difficult step of learning likelihood functions in HMMs. By reformulating inference and model fitting in terms of density ratios and applying a fast kernel-based estimation method, we show that it is possible to obtain a striking increase in discriminative performance while retaining the probabilistic qualities of the HMM. We demonstrate experimentally that this formulation makes more efficient use of training data than alternative approaches. version:1
arxiv-1302-3668 | Bio-inspired data mining: Treating malware signatures as biosequences | http://arxiv.org/abs/1302.3668 | id:1302.3668 author:Ajit Narayanan, Yi Chen category:cs.LG q-bio.QM stat.ML I.2.6  published:2013-02-15 summary:The application of machine learning to bioinformatics problems is well established. Less well understood is the application of bioinformatics techniques to machine learning and, in particular, the representation of non-biological data as biosequences. The aim of this paper is to explore the effects of giving amino acid representation to problematic machine learning data and to evaluate the benefits of supplementing traditional machine learning with bioinformatics tools and techniques. The signatures of 60 computer viruses and 60 computer worms were converted into amino acid representations and first multiply aligned separately to identify conserved regions across different families within each class (virus and worm). This was followed by a second alignment of all 120 aligned signatures together so that non-conserved regions were identified prior to input to a number of machine learning techniques. Differences in length between virus and worm signatures after the first alignment were resolved by the second alignment. Our first set of experiments indicates that representing computer malware signatures as amino acid sequences followed by alignment leads to greater classification and prediction accuracy. Our second set of experiments indicates that checking the results of data mining from artificial virus and worm data against known proteins can lead to generalizations being made from the domain of naturally occurring proteins to malware signatures. However, further work is needed to determine the advantages and disadvantages of different representations and sequence alignment methods for handling problematic machine learning data. version:1
arxiv-1302-3541 | An analysis of NK and generalized NK landscapes | http://arxiv.org/abs/1302.3541 | id:1302.3541 author:Jeffrey S. Buzas, Jeffrey Dinitz category:cs.NE  published:2013-02-14 summary:Simulated landscapes have been used for decades to evaluate search strategies whose goal is to find the landscape location with maximum fitness. Applications include modeling the capacity of enzymes to catalyze reactions and the clinical effectiveness of medical treatments. Understanding properties of landscapes is important for understanding search difficulty. This paper presents a novel and transparent characterization of NK landscapes. We prove that NK landscapes can be represented by parametric linear interaction models where model coefficients have meaningful interpretations. We derive the statistical properties of the model coefficients, providing insight into how the NK algorithm parses importance to main effects and interactions. An important insight derived from the linear model representation is that the rank of the linear model defined by the NK algorithm is correlated with the number of local optima, a strong determinant of landscape complexity and search difficulty. We show that the maximal rank for an NK landscape is achieved through epistatic interactions that form partially balanced incomplete block designs. Finally, an analytic expression representing the expected number of local optima on the landscape is derived, providing a way to quickly compute the expected number of local optima for very large landscapes. version:1
arxiv-1302-3407 | A consistent clustering-based approach to estimating the number of change-points in highly dependent time-series | http://arxiv.org/abs/1302.3407 | id:1302.3407 author:Azaden Khaleghi, Daniil Ryabko category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH  published:2013-02-14 summary:The problem of change-point estimation is considered under a general framework where the data are generated by unknown stationary ergodic process distributions. In this context, the consistent estimation of the number of change-points is provably impossible. However, it is shown that a consistent clustering method may be used to estimate the number of change points, under the additional constraint that the correct number of process distributions that generate the data is provided. This additional parameter has a natural interpretation in many real-world applications. An algorithm is proposed that estimates the number of change-points and locates the changes. The proposed algorithm is shown to be asymptotically consistent; its empirical evaluations are provided. version:1
arxiv-1302-3261 | Pavlov's dog associative learning demonstrated on synaptic-like organic transistors | http://arxiv.org/abs/1302.3261 | id:1302.3261 author:O. Bichler, W. Zhao, F. Alibart, S. Pleutin, S. Lenfant, D. Vuillaume, C. Gamrat category:q-bio.NC cond-mat.dis-nn cs.ET cs.NE  published:2013-02-13 summary:In this letter, we present an original demonstration of an associative learning neural network inspired by the famous Pavlov's dogs experiment. A single nanoparticle organic memory field effect transistor (NOMFET) is used to implement each synapse. We show how the physical properties of this dynamic memristive device can be used to perform low power write operations for the learning and implement short-term association using temporal coding and spike timing dependent plasticity based learning. An electronic circuit was built to validate the proposed learning scheme with packaged devices, with good reproducibility despite the complex synaptic-like dynamic of the NOMFET in pulse regime. version:1
arxiv-1302-3447 | Exact Methods for Multistage Estimation of a Binomial Proportion | http://arxiv.org/abs/1302.3447 | id:1302.3447 author:Zhengjia Chen, Xinjia Chen category:math.ST cs.LG cs.NA math.PR stat.TH  published:2013-02-13 summary:We first review existing sequential methods for estimating a binomial proportion. Afterward, we propose a new family of group sequential sampling schemes for estimating a binomial proportion with prescribed margin of error and confidence level. In particular, we establish the uniform controllability of coverage probability and the asymptotic optimality for such a family of sampling schemes. Our theoretical results establish the possibility that the parameters of this family of sampling schemes can be determined so that the prescribed level of confidence is guaranteed with little waste of samples. Analytic bounds for the cumulative distribution functions and expectations of sample numbers are derived. Moreover, we discuss the inherent connection of various sampling schemes. Numerical issues are addressed for improving the accuracy and efficiency of computation. Computational experiments are conducted for comparing sampling schemes. Illustrative examples are given for applications in clinical trials. version:1
arxiv-1211-7359 | Genetic braid optimization: A heuristic approach to compute quasiparticle braids | http://arxiv.org/abs/1211.7359 | id:1211.7359 author:Ross B. McDonald, Helmut G. Katzgraber category:quant-ph cond-mat.mes-hall cs.NE  published:2012-11-30 summary:In topologically-protected quantum computation, quantum gates can be carried out by adiabatically braiding two-dimensional quasiparticles, reminiscent of entangled world lines. Bonesteel et al. [Phys. Rev. Lett. 95, 140503 (2005)], as well as Leijnse and Flensberg [Phys. Rev. B 86, 104511 (2012)] recently provided schemes for computing quantum gates from quasiparticle braids. Mathematically, the problem of executing a gate becomes that of finding a product of the generators (matrices) in that set that approximates the gate best, up to an error. To date, efficient methods to compute these gates only strive to optimize for accuracy. We explore the possibility of using a generic approach applicable to a variety of braiding problems based on evolutionary (genetic) algorithms. The method efficiently finds optimal braids while allowing the user to optimize for the relative utilities of accuracy and/or length. Furthermore, when optimizing for error only, the method can quickly produce efficient braids. version:2
arxiv-1302-3155 | Morphological Analusis Of The Left Ventricular Eendocardial Surface Using A Bag-Of-Features Descriptor | http://arxiv.org/abs/1302.3155 | id:1302.3155 author:Anirban Mukhopadhyay, Zhen Qian, Suchendra M. Bhandarkar, Tianming Liu, Sarah Rinehart, Szilard Voros category:cs.CV  published:2013-02-13 summary:The limitations of conventional imaging techniques have hitherto precluded a thorough and formal investigation of the complex morphology of the left ventricular (LV) endocardial surface and its relation to the severity of Coronary Artery Disease (CAD). Recent developments in high-resolution Multirow-Detector Computed Tomography (MDCT) scanner technology have enabled the imaging of LV endocardial surface morphology in a single heart beat. Analysis of high-resolution Computed Tomography (CT) images from a 320-MDCT scanner allows the study of the relationship between percent Diameter Stenosis (DS) of the major coronary arteries and localization of the cardiac segments affected by coronary arterial stenosis. In this paper a novel approach for the analysis using a combination of rigid transformation-invariant shape descriptors and a more generalized isometry-invariant Bag-of-Features (BoF) descriptor, is proposed and implemented. The proposed approach is shown to be successful in identifying, localizing and quantifying the incidence and extent of CAD and thus, is seen to have a potentially significant clinical impact. Specifically, the association between the incidence and extent of CAD, determined via the percent DS measurements of the major coronary arteries, and the alterations in the endocardial surface morphology is formally quantified. A multivariate regression test performed on a strict leave-one-out basis are shown to exhibit a distinct pattern in terms of the correlation coefficient within the cardiac segments where the incidence of coronary arterial stenosis is localized. version:1
arxiv-1302-3590 | Bayesian Learning of Loglinear Models for Neural Connectivity | http://arxiv.org/abs/1302.3590 | id:1302.3590 author:Kathryn Blackmond Laskey, Laura Martignon category:cs.LG q-bio.NC stat.AP stat.ML  published:2013-02-13 summary:This paper presents a Bayesian approach to learning the connectivity structure of a group of neurons from data on configuration frequencies. A major objective of the research is to provide statistical tools for detecting changes in firing patterns with changing stimuli. Our framework is not restricted to the well-understood case of pair interactions, but generalizes the Boltzmann machine model to allow for higher order interactions. The paper applies a Markov Chain Monte Carlo Model Composition (MC3) algorithm to search over connectivity structures and uses Laplace's method to approximate posterior probabilities of structures. Performance of the methods was tested on synthetic data. The models were also applied to data obtained by Vaadia on multi-unit recordings of several neurons in the visual cortex of a rhesus monkey in two different attentional states. Results confirmed the experimenters' conjecture that different attentional states were associated with different interaction structures. version:1
arxiv-1302-3579 | On the Sample Complexity of Learning Bayesian Networks | http://arxiv.org/abs/1302.3579 | id:1302.3579 author:Nir Friedman, Zohar Yakhini category:cs.LG stat.ML  published:2013-02-13 summary:In recent years there has been an increasing interest in learning Bayesian networks from data. One of the most effective methods for learning such networks is based on the minimum description length (MDL) principle. Previous work has shown that this learning procedure is asymptotically successful: with probability one, it will converge to the target distribution, given a sufficient number of samples. However, the rate of this convergence has been hitherto unknown. In this work we examine the sample complexity of MDL based learning procedures for Bayesian networks. We show that the number of samples needed to learn an epsilon-close approximation (in terms of entropy distance) with confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog (1/delta)). This means that the sample complexity is a low-order polynomial in the error threshold and sub-linear in the confidence bound. We also discuss how the constants in this term depend on the complexity of the target distribution. Finally, we address questions of asymptotic minimality and propose a method for using the sample complexity results to speed up the learning process. version:1
arxiv-1302-3577 | Learning Bayesian Networks with Local Structure | http://arxiv.org/abs/1302.3577 | id:1302.3577 author:Nir Friedman, Moises Goldszmidt category:cs.AI cs.LG stat.ML  published:2013-02-13 summary:In this paper we examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability tables (CPTs), that quantify these networks. This increases the space of possible models, enabling the representation of CPTs with a variable number of parameters that depends on the learned local structures. The resulting learning procedure is capable of inducing models that better emulate the real complexity of the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures, as well as an empirical evaluation of the proposed method. This evaluation indicates that learning curves characterizing the procedure that exploits the local structure converge faster than these of the standard procedure. Our results also show that networks learned with local structure tend to be more complex (in terms of arcs), yet require less parameters. version:1
arxiv-1302-3566 | Learning Equivalence Classes of Bayesian Networks Structures | http://arxiv.org/abs/1302.3566 | id:1302.3566 author:David Maxwell Chickering category:cs.AI cs.LG stat.ML  published:2013-02-13 summary:Approaches to learning Bayesian networks from data typically combine a scoring function with a heuristic search procedure. Given a Bayesian network structure, many of the scoring functions derived in the literature return a score for the entire equivalence class to which the structure belongs. When using such a scoring function, it is appropriate for the heuristic search algorithm to search over equivalence classes of Bayesian networks as opposed to individual structures. We present the general formulation of a search space for which the states of the search correspond to equivalence classes of structures. Using this space, any one of a number of heuristic search algorithms can easily be applied. We compare greedy search performance in the proposed search space to greedy search performance in a search space for which the states correspond to individual Bayesian network structures. version:1
arxiv-1302-3556 | Object Recognition with Imperfect Perception and Redundant Description | http://arxiv.org/abs/1302.3556 | id:1302.3556 author:Claude Barrouil, Jerome Lemaire category:cs.CV cs.AI  published:2013-02-13 summary:This paper deals with a scene recognition system in a robotics contex. The general problem is to match images with <I>a priori</I> descriptions. A typical mission would consist in identifying an object in an installation with a vision system situated at the end of a manipulator and with a human operator provided description, formulated in a pseudo-natural language, and possibly redundant. The originality of this work comes from the nature of the description, from the special attention given to the management of imprecision and uncertainty in the interpretation process and from the way to assess the description redundancy so as to reinforce the overall matching likelihood. version:1
arxiv-1109-3336 | Sampled forms of functional PCA in reproducing kernel Hilbert spaces | http://arxiv.org/abs/1109.3336 | id:1109.3336 author:Arash A. Amini, Martin J. Wainwright category:math.ST stat.ML stat.TH  published:2011-09-15 summary:We consider the sampling problem for functional PCA (fPCA), where the simplest example is the case of taking time samples of the underlying functional components. More generally, we model the sampling operation as a continuous linear map from $\mathcal{H}$ to $\mathbb{R}^m$, where the functional components to lie in some Hilbert subspace $\mathcal{H}$ of $L^2$, such as a reproducing kernel Hilbert space of smooth functions. This model includes time and frequency sampling as special cases. In contrast to classical approach in fPCA in which access to entire functions is assumed, having a limited number m of functional samples places limitations on the performance of statistical procedures. We study these effects by analyzing the rate of convergence of an M-estimator for the subspace spanned by the leading components in a multi-spiked covariance model. The estimator takes the form of regularized PCA, and hence is computationally attractive. We analyze the behavior of this estimator within a nonasymptotic framework, and provide bounds that hold with high probability as a function of the number of statistical samples n and the number of functional samples m. We also derive lower bounds showing that the rates obtained are minimax optimal. version:2
arxiv-1302-3057 | Building a reordering system using tree-to-string hierarchical model | http://arxiv.org/abs/1302.3057 | id:1302.3057 author:Jacob Dlougach, Irina Galinskaya category:cs.CL  published:2013-02-13 summary:This paper describes our submission to the First Workshop on Reordering for Statistical Machine Translation. We have decided to build a reordering system based on tree-to-string model, using only publicly available tools to accomplish this task. With the provided training data we have built a translation model using Moses toolkit, and then we applied a chart decoder, implemented in Moses, to reorder the sentences. Even though our submission only covered English-Farsi language pair, we believe that the approach itself should work regardless of the choice of the languages, so we have also carried out the experiments for English-Italian and English-Urdu. For these language pairs we have noticed a significant improvement over the baseline in BLEU, Kendall-Tau and Hamming metrics. A detailed description is given, so that everyone can reproduce our results. Also, some possible directions for further improvements are discussed. version:1
arxiv-1302-3219 | An Efficient Dual Approach to Distance Metric Learning | http://arxiv.org/abs/1302.3219 | id:1302.3219 author:Chunhua Shen, Junae Kim, Fayao Liu, Lei Wang, Anton van den Hengel category:cs.LG  published:2013-02-13 summary:Distance metric learning is of fundamental interest in machine learning because the distance metric employed can significantly affect the performance of many learning methods. Quadratic Mahalanobis metric learning is a popular approach to the problem, but typically requires solving a semidefinite programming (SDP) problem, which is computationally expensive. Standard interior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with $D$ the dimension of input data), and can thus only practically solve problems exhibiting less than a few thousand variables. Since the number of variables is $D (D+1) / 2 $, this implies a limit upon the size of problem that can practically be solved of around a few hundred dimensions. The complexity of the popular quadratic Mahalanobis metric learning approach thus limits the size of problem to which metric learning can be applied. Here we propose a significantly more efficient approach to the metric learning problem based on the Lagrange dual formulation of the problem. The proposed formulation is much simpler to implement, and therefore allows much larger Mahalanobis metric learning problems to be solved. The time complexity of the proposed method is $O (D ^ 3) $, which is significantly lower than that of the SDP approach. Experiments on a variety of datasets demonstrate that the proposed method achieves an accuracy comparable to the state-of-the-art, but is applicable to significantly larger problems. We also show that the proposed method can be applied to solve more general Frobenius-norm regularized SDP problems approximately. version:1
arxiv-1110-3556 | Joint variable and rank selection for parsimonious estimation of high-dimensional matrices | http://arxiv.org/abs/1110.3556 | id:1110.3556 author:Florentina Bunea, Yiyuan She, Marten H. Wegkamp category:math.ST stat.ME stat.ML stat.TH  published:2011-10-17 summary:We propose dimension reduction methods for sparse, high-dimensional multivariate response regression models. Both the number of responses and that of the predictors may exceed the sample size. Sometimes viewed as complementary, predictor selection and rank reduction are the most popular strategies for obtaining lower-dimensional approximations of the parameter matrix in such models. We show in this article that important gains in prediction accuracy can be obtained by considering them jointly. We motivate a new class of sparse multivariate regression models, in which the coefficient matrix has low rank and zero rows or can be well approximated by such a matrix. Next, we introduce estimators that are based on penalized least squares, with novel penalties that impose simultaneous row and rank restrictions on the coefficient matrix. We prove that these estimators indeed adapt to the unknown matrix sparsity and have fast rates of convergence. We support our theoretical results with an extensive simulation study and two data analyses. version:4
arxiv-1302-2969 | Towards Identification of Relevant Variables in the observed Aerosol Optical Depth Bias between MODIS and AERONET observations | http://arxiv.org/abs/1302.2969 | id:1302.2969 author:N. K. Malakar, D. J. Lary, D. Gencaga, A. Albayrak, J. Wei category:stat.ML  published:2013-02-13 summary:Measurements made by satellite remote sensing, Moderate Resolution Imaging Spectroradiometer (MODIS), and globally distributed Aerosol Robotic Network (AERONET) are compared. Comparison of the two datasets measurements for aerosol optical depth values show that there are biases between the two data products. In this paper, we present a general framework towards identifying relevant set of variables responsible for the observed bias. We present a general framework to identify the possible factors influencing the bias, which might be associated with the measurement conditions such as the solar and sensor zenith angles, the solar and sensor azimuth, scattering angles, and surface reflectivity at the various measured wavelengths, etc. Specifically, we performed analysis for remote sensing Aqua-Land data set, and used machine learning technique, neural network in this case, to perform multivariate regression between the ground-truth and the training data sets. Finally, we used mutual information between the observed and the predicted values as the measure of similarity to identify the most relevant set of variables. The search is brute force method as we have to consider all possible combinations. The computations involves a huge number crunching exercise, and we implemented it by writing a job-parallel program. version:1
arxiv-1302-1611 | Bounded regret in stochastic multi-armed bandits | http://arxiv.org/abs/1302.1611 | id:1302.1611 author:Sébastien Bubeck, Vianney Perchet, Philippe Rigollet category:math.ST cs.LG stat.ML stat.TH 62L05  published:2013-02-06 summary:We study the stochastic multi-armed bandit problem when one knows the value $\mu^{(\star)}$ of an optimal arm, as a well as a positive lower bound on the smallest positive gap $\Delta$. We propose a new randomized policy that attains a regret {\em uniformly bounded over time} in this setting. We also prove several lower bounds, which show in particular that bounded regret is not possible if one only knows $\Delta$, and bounded regret of order $1/\Delta$ is not possible if one only knows $\mu^{(\star)}$ version:2
arxiv-1302-2672 | Competing With Strategies | http://arxiv.org/abs/1302.2672 | id:1302.2672 author:Wei Han, Alexander Rakhlin, Karthik Sridharan category:stat.ML cs.GT cs.LG  published:2013-02-12 summary:We study the problem of online learning with a notion of regret defined with respect to a set of strategies. We develop tools for analyzing the minimax rates and for deriving regret-minimization algorithms in this scenario. While the standard methods for minimizing the usual notion of regret fail, through our analysis we demonstrate existence of regret-minimization methods that compete with such sets of strategies as: autoregressive algorithms, strategies based on statistical models, regularized least squares, and follow the regularized leader strategies. In several cases we also derive efficient learning algorithms. version:1
arxiv-1302-2576 | The trace norm constrained matrix-variate Gaussian process for multitask bipartite ranking | http://arxiv.org/abs/1302.2576 | id:1302.2576 author:Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh category:cs.LG stat.ML  published:2013-02-11 summary:We propose a novel hierarchical model for multitask bipartite ranking. The proposed approach combines a matrix-variate Gaussian process with a generative model for task-wise bipartite ranking. In addition, we employ a novel trace constrained variational inference approach to impose low rank structure on the posterior matrix-variate Gaussian process. The resulting posterior covariance function is derived in closed form, and the posterior mean function is the solution to a matrix-variate regression with a novel spectral elastic net regularizer. Further, we show that variational inference for the trace constrained matrix-variate Gaussian process combined with maximum likelihood parameter estimation for the bipartite ranking model is jointly convex. Our motivating application is the prioritization of candidate disease genes. The goal of this task is to aid the identification of unobserved associations between human genes and diseases using a small set of observed associations as well as kernels induced by gene-gene interaction networks and disease ontologies. Our experimental results illustrate the performance of the proposed model on real world datasets. Moreover, we find that the resulting low rank solution improves the computational scalability of training and testing as compared to baseline models. version:1
arxiv-1302-2569 | Toric grammars: a new statistical approach to natural language modeling | http://arxiv.org/abs/1302.2569 | id:1302.2569 author:Olivier Catoni, Thomas Mainguy category:stat.ML cs.CL math.PR  published:2013-02-11 summary:We propose a new statistical model for computational linguistics. Rather than trying to estimate directly the probability distribution of a random sentence of the language, we define a Markov chain on finite sets of sentences with many finite recurrent communicating classes and define our language model as the invariant probability measures of the chain on each recurrent communicating class. This Markov chain, that we call a communication model, recombines at each step randomly the set of sentences forming its current state, using some grammar rules. When the grammar rules are fixed and known in advance instead of being estimated on the fly, we can prove supplementary mathematical properties. In particular, we can prove in this case that all states are recurrent states, so that the chain defines a partition of its state space into finite recurrent communicating classes. We show that our approach is a decisive departure from Markov models at the sentence level and discuss its relationships with Context Free Grammars. Although the toric grammars we use are closely related to Context Free Grammars, the way we generate the language from the grammar is qualitatively different. Our communication model has two purposes. On the one hand, it is used to define indirectly the probability distribution of a random sentence of the language. On the other hand it can serve as a (crude) model of language transmission from one speaker to another speaker through the communication of a (large) set of sentences. version:1
arxiv-1302-2552 | Selecting the State-Representation in Reinforcement Learning | http://arxiv.org/abs/1302.2552 | id:1302.2552 author:Odalric-Ambrym Maillard, Rémi Munos, Daniil Ryabko category:cs.LG  published:2013-02-11 summary:The problem of selecting the right state-representation in a reinforcement learning problem is considered. Several models (functions mapping past observations to a finite set) of the observations are given, and it is known that for at least one of these models the resulting state dynamics are indeed Markovian. Without knowing neither which of the models is the correct one, nor what are the probabilistic characteristics of the resulting MDP, it is required to obtain as much reward as the optimal policy for the correct model (or for the best of the correct models, if there are several). We propose an algorithm that achieves that, with a regret of order T^{2/3} where T is the horizon time. version:1
arxiv-1302-2550 | Online Regret Bounds for Undiscounted Continuous Reinforcement Learning | http://arxiv.org/abs/1302.2550 | id:1302.2550 author:Ronald Ortner, Daniil Ryabko category:cs.LG  published:2013-02-11 summary:We derive sublinear regret bounds for undiscounted reinforcement learning in continuous state space. The proposed algorithm combines state aggregation with the use of upper confidence bounds for implementing optimism in the face of uncertainty. Beside the existence of an optimal policy which satisfies the Poisson equation, the only assumptions made are Holder continuity of rewards and transition probabilities. version:1
arxiv-1302-2436 | Extracting useful rules through improved decision tree induction using information entropy | http://arxiv.org/abs/1302.2436 | id:1302.2436 author:Mohd Mahmood Ali, Mohd S Qaseem, Lakshmi Rajamani, A Govardhan category:cs.LG  published:2013-02-11 summary:Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. We suggest improvements to the existing C4.5 decision tree algorithm. In this paper attribute oriented induction (AOI) and relevance analysis are incorporated with concept hierarchys knowledge and HeightBalancePriority algorithm for construction of decision tree along with Multi level mining. The assignment of priorities to attributes is done by evaluating information entropy, at different levels of abstraction for building decision tree using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and explore the shortcomings of the decision trees generated by C4.5 classifier for education dataset and the results are compared with the proposed approach. version:1
arxiv-1210-2051 | Anomalous Vacillatory Learning | http://arxiv.org/abs/1210.2051 | id:1210.2051 author:Achilles Beros category:math.LO cs.LG cs.LO 03D80  68Q32  published:2012-10-07 summary:In 1986, Osherson, Stob and Weinstein asked whether two variants of anomalous vacillatory learning, TxtFex^*_* and TxtFext^*_*, could be distinguished. In both, a machine is permitted to vacillate between a finite number of hypotheses and to make a finite number of errors. TxtFext^*_*-learning requires that hypotheses output infinitely often must describe the same finite variant of the correct set, while TxtFex^*_*-learning permits the learner to vacillate between finitely many different finite variants of the correct set. In this paper we show that TxtFex^*_* \neq TxtFext^*_*, thereby answering the question posed by Osherson, \textit{et al}. We prove this in a strong way by exhibiting a family in TxtFex^*_2 \setminus {TxtFext}^*_*. version:2
arxiv-1302-2273 | Learning Universally Quantified Invariants of Linear Data Structures | http://arxiv.org/abs/1302.2273 | id:1302.2273 author:Pranav Garg, Christof Loding, P. Madhusudan, Daniel Neider category:cs.PL cs.FL cs.LG  published:2013-02-09 summary:We propose a new automaton model, called quantified data automata over words, that can model quantified invariants over linear data structures, and build poly-time active learning algorithms for them, where the learner is allowed to query the teacher with membership and equivalence queries. In order to express invariants in decidable logics, we invent a decidable subclass of QDAs, called elastic QDAs, and prove that every QDA has a unique minimally-over-approximating elastic QDA. We then give an application of these theoretically sound and efficient active learning algorithms in a passive learning framework and show that we can efficiently learn quantified linear data structure invariants from samples obtained from dynamic runs for a large class of programs. version:1
arxiv-1202-4482 | Metabolic cost as an organizing principle for cooperative learning | http://arxiv.org/abs/1202.4482 | id:1202.4482 author:David Balduzzi, Pedro A Ortega, Michel Besserve category:q-bio.NC cs.LG nlin.AO  published:2012-02-20 summary:This paper investigates how neurons can use metabolic cost to facilitate learning at a population level. Although decision-making by individual neurons has been extensively studied, questions regarding how neurons should behave to cooperate effectively remain largely unaddressed. Under assumptions that capture a few basic features of cortical neurons, we show that constraining reward maximization by metabolic cost aligns the information content of actions with their expected reward. Thus, metabolic cost provides a mechanism whereby neurons encode expected reward into their outputs. Further, aside from reducing energy expenditures, imposing a tight metabolic constraint also increases the accuracy of empirical estimates of rewards, increasing the robustness of distributed learning. Finally, we present two implementations of metabolically constrained learning that confirm our theoretical finding. These results suggest that metabolic cost may be an organizing principle underlying the neural code, and may also provide a useful guide to the design and analysis of other cooperating populations. version:2
arxiv-1104-2373 | Hybrid Deterministic-Stochastic Methods for Data Fitting | http://arxiv.org/abs/1104.2373 | id:1104.2373 author:Michael P. Friedlander, Mark Schmidt category:cs.NA cs.SY math.OC stat.ML  published:2011-04-13 summary:Many structured data-fitting applications require the solution of an optimization problem involving a sum over a potentially large number of measurements. Incremental gradient algorithms offer inexpensive iterations by sampling a subset of the terms in the sum. These methods can make great progress initially, but often slow as they approach a solution. In contrast, full-gradient methods achieve steady convergence at the expense of evaluating the full objective and gradient on each iteration. We explore hybrid methods that exhibit the benefits of both approaches. Rate-of-convergence analysis shows that by controlling the sample size in an incremental gradient algorithm, it is possible to maintain the steady convergence rates of full-gradient methods. We detail a practical quasi-Newton implementation based on this approach. Numerical experiments illustrate its potential benefits. version:4
arxiv-1301-7002 | Quadratic Basis Pursuit | http://arxiv.org/abs/1301.7002 | id:1301.7002 author:Henrik Ohlsson, Allen Y. Yang, Roy Dong, Michel Verhaegen, S. Shankar Sastry category:cs.IT math.IT stat.ML  published:2013-01-29 summary:In many compressive sensing problems today, the relationship between the measurements and the unknowns could be nonlinear. Traditional treatment of such nonlinear relationships have been to approximate the nonlinearity via a linear model and the subsequent un-modeled dynamics as noise. The ability to more accurately characterize nonlinear models has the potential to improve the results in both existing compressive sensing applications and those where a linear approximation does not suffice, e.g., phase retrieval. In this paper, we extend the classical compressive sensing framework to a second-order Taylor expansion of the nonlinearity. Using a lifting technique and a method we call quadratic basis pursuit, we show that the sparse signal can be recovered exactly when the sampling rate is sufficiently high. We further present efficient numerical algorithms to recover sparse signals in second-order nonlinear systems, which are considerably more difficult to solve than their linear counterparts in sparse optimization. version:2
arxiv-1302-2176 | Minimax Optimal Algorithms for Unconstrained Linear Optimization | http://arxiv.org/abs/1302.2176 | id:1302.2176 author:H. Brendan McMahan category:cs.LG  published:2013-02-08 summary:We design and analyze minimax-optimal algorithms for online linear optimization games where the player's choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. The standard benchmark is the loss of the best strategy chosen from a bounded comparator set. When the the comparison set and the adversary's gradients satisfy L_infinity bounds, we give the value of the game in closed form and prove it approaches sqrt(2T/pi) as T -> infinity. Interesting algorithms result when we consider soft constraints on the comparator, rather than restricting it to a bounded set. As a warmup, we analyze the game with a quadratic penalty. The value of this game is exactly T/2, and this value is achieved by perhaps the simplest online algorithm of all: unprojected gradient descent with a constant learning rate. We then derive a minimax-optimal algorithm for a much softer penalty function. This algorithm achieves good bounds under the standard notion of regret for any comparator point, without needing to specify the comparator set in advance. The value of this game converges to sqrt{e} as T ->infinity; we give a closed-form for the exact value as a function of T. The resulting algorithm is natural in unconstrained investment or betting scenarios, since it guarantees at worst constant loss, while allowing for exponential reward against an "easy" adversary. version:1
arxiv-1302-2131 | Data Mining of the Concept "End of the World" in Twitter Microblogs | http://arxiv.org/abs/1302.2131 | id:1302.2131 author:Bohdan Pavlyshenko category:cs.SI cs.CL cs.IR physics.soc-ph  published:2013-02-08 summary:This paper describes the analysis of quantitative characteristics of frequent sets and association rules in the posts of Twitter microblogs, related to the discussion of "end of the world", which was allegedly predicted on December 21, 2012 due to the Mayan calendar. Discovered frequent sets and association rules characterize semantic relations between the concepts of analyzed subjects.The support for some fequent sets reaches the global maximum before the expected event with some time delay. Such frequent sets may be considered as predictive markers that characterize the significance of expected events for blogosphere users. It was shown that time dynamics of confidence of some revealed association rules can also have predictive characteristics. Exceeding a certain threshold, it may be a signal for the corresponding reaction in the society during the time interval between the maximum and probable coming of an event. version:1
arxiv-1302-2068 | Efficiency for Regularization Parameter Selection in Penalized Likelihood Estimation of Misspecified Models | http://arxiv.org/abs/1302.2068 | id:1302.2068 author:Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff category:stat.ML  published:2013-02-08 summary:It has been shown that AIC-type criteria are asymptotically efficient selectors of the tuning parameter in non-concave penalized regression methods under the assumption that the population variance is known or that a consistent estimator is available. We relax this assumption to prove that AIC itself is asymptotically efficient and we study its performance in finite samples. In classical regression, it is known that AIC tends to select overly complex models when the dimension of the maximum candidate model is large relative to the sample size. Simulation studies suggest that AIC suffers from the same shortcomings when used in penalized regression. We therefore propose the use of the classical corrected AIC (AICc) as an alternative and prove that it maintains the desired asymptotic properties. To broaden our results, we further prove the efficiency of AIC for penalized likelihood methods in the context of generalized linear models with no dispersion parameter. Similar results exist in the literature but only for a restricted set of candidate models. By employing results from the classical literature on maximum-likelihood estimation in misspecified models, we are able to establish this result for a general set of candidate models. We use simulations to assess the performance of AIC and AICc, as well as that of other selectors, in finite samples for both SCAD-penalized and Lasso regressions and a real data example is considered. version:1
arxiv-1301-6630 | Political Disaffection: a case study on the Italian Twitter community | http://arxiv.org/abs/1301.6630 | id:1301.6630 author:Corrado Monti, Alessandro Rozza, Giovanni Zappella, Matteo Zignani, Adam Arvidsson, Monica Poletti category:cs.SI cs.LG physics.soc-ph  published:2013-01-28 summary:In our work we analyse the political disaffection or "the subjective feeling of powerlessness, cynicism, and lack of confidence in the political process, politicians, and democratic institutions, but with no questioning of the political regime" by exploiting Twitter data through machine learning techniques. In order to validate the quality of the time-series generated by the Twitter data, we highlight the relations of these data with political disaffection as measured by means of public opinion surveys. Moreover, we show that important political news of Italian newspapers are often correlated with the highest peaks of the produced time-series. version:2
arxiv-1302-1947 | A new compressive video sensing framework for mobile broadcast | http://arxiv.org/abs/1302.1947 | id:1302.1947 author:Chengbo Li, Hong Jiang, Paul Wilford, Yin Zhang, Mike Scheutzow category:cs.MM cs.CV cs.IT math.IT  published:2013-02-08 summary:A new video coding method based on compressive sampling is proposed. In this method, a video is coded using compressive measurements on video cubes. Video reconstruction is performed by minimization of total variation (TV) of the pixelwise DCT coefficients along the temporal direction. A new reconstruction algorithm is developed from TVAL3, an efficient TV minimization algorithm based on the alternating minimization and augmented Lagrangian methods. Video coding with this method is inherently scalable, and has applications in mobile broadcast. version:1
arxiv-1302-1942 | Surveillance Video Processing Using Compressive Sensing | http://arxiv.org/abs/1302.1942 | id:1302.1942 author:Hong Jiang, Wei Deng, Zuowei Shen category:cs.CV cs.IT math.IT  published:2013-02-08 summary:A compressive sensing method combined with decomposition of a matrix formed with image frames of a surveillance video into low rank and sparse matrices is proposed to segment the background and extract moving objects in a surveillance video. The video is acquired by compressive measurements, and the measurements are used to reconstruct the video by a low rank and sparse decomposition of matrix. The low rank component represents the background, and the sparse component is used to identify moving objects in the surveillance video. The decomposition is performed by an augmented Lagrangian alternating direction method. Experiments are carried out to demonstrate that moving objects can be reliably extracted with a small amount of measurements. version:1
arxiv-1207-3012 | Optimal rates for first-order stochastic convex optimization under Tsybakov noise condition | http://arxiv.org/abs/1207.3012 | id:1207.3012 author:Aaditya Ramdas, Aarti Singh category:cs.LG stat.ML  published:2012-07-12 summary:We focus on the problem of minimizing a convex function $f$ over a convex set $S$ given $T$ queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-like noise condition. Specifically, we prove that if $f$ grows at least as fast as $\ x-x^*_{f,S}\ ^\kappa$ around its minimum, for some $\kappa > 1$, then the optimal rate of learning $f(x^*_{f,S})$ is $\Theta(T^{-\frac{\kappa}{2\kappa-2}})$. The classic rate $\Theta(1/\sqrt T)$ for convex functions and $\Theta(1/T)$ for strongly convex functions are special cases of our result for $\kappa \rightarrow \infty$ and $\kappa=2$, and even faster rates are attained for $\kappa <2$. We also derive tight bounds for the complexity of learning $x_{f,S}^*$, where the optimal rate is $\Theta(T^{-\frac{1}{2\kappa-2}})$. Interestingly, these precise rates for convex optimization also characterize the complexity of active learning and our results further strengthen the connections between the two fields, both of which rely on feedback-driven queries. version:2
arxiv-1205-4698 | The Role of Weight Shrinking in Large Margin Perceptron Learning | http://arxiv.org/abs/1205.4698 | id:1205.4698 author:Constantinos Panagiotakopoulos, Petroula Tsampouka category:cs.LG  published:2012-05-21 summary:We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels. version:2
arxiv-1302-1789 | Lensless Compressive Sensing Imaging | http://arxiv.org/abs/1302.1789 | id:1302.1789 author:Gang Huang, Hong Jiang, Kim Matthews, Paul Wilford category:cs.CV cs.IT math.IT  published:2013-02-07 summary:In this paper, we propose a lensless compressive sensing imaging architecture. The architecture consists of two components, an aperture assembly and a sensor. No lens is used. The aperture assembly consists of a two dimensional array of aperture elements. The transmittance of each aperture element is independently controllable. The sensor is a single detection element, such as a single photo-conductive cell. Each aperture element together with the sensor defines a cone of a bundle of rays, and the cones of the aperture assembly define the pixels of an image. Each pixel value of an image is the integration of the bundle of rays in a cone. The sensor is used for taking compressive measurements. Each measurement is the integration of rays in the cones modulated by the transmittance of the aperture elements. A compressive sensing matrix is implemented by adjusting the transmittance of the individual aperture elements according to the values of the sensing matrix. The proposed architecture is simple and reliable because no lens is used. Furthermore, the sharpness of an image from our device is only limited by the resolution of the aperture assembly, but not affected by blurring due to defocus. The architecture can be used for capturing images of visible lights, and other spectra such as infrared, or millimeter waves. Such devices may be used in surveillance applications for detecting anomalies or extracting features such as speed of moving objects. Multiple sensors may be used with a single aperture assembly to capture multi-view images simultaneously. A prototype was built by using a LCD panel and a photoelectric sensor for capturing images of visible spectrum. version:1
arxiv-1302-1772 | An ANN-based Method for Detecting Vocal Fold Pathology | http://arxiv.org/abs/1302.1772 | id:1302.1772 author:Vahid Majidnezhad, Igor Kheidorov category:cs.LG cs.CV cs.SD  published:2013-02-07 summary:There are different algorithms for vocal fold pathology diagnosis. These algorithms usually have three stages which are Feature Extraction, Feature Reduction and Classification. While the third stage implies a choice of a variety of machine learning methods, the first and second stages play a critical role in performance and accuracy of the classification system. In this paper we present initial study of feature extraction and feature reduction in the task of vocal fold pathology diagnosis. A new type of feature vector, based on wavelet packet decomposition and Mel-Frequency-Cepstral-Coefficients (MFCCs), is proposed. Also Principal Component Analysis (PCA) is used for feature reduction. An Artificial Neural Network is used as a classifier for evaluating the performance of our proposed method. version:1
arxiv-1302-0439 | Correcting Camera Shake by Incremental Sparse Approximation | http://arxiv.org/abs/1302.0439 | id:1302.0439 author:Paul Shearer, Anna C. Gilbert, Alfred O. Hero III category:cs.CV cs.GR  published:2013-02-03 summary:The problem of deblurring an image when the blur kernel is unknown remains challenging after decades of work. Recently there has been rapid progress on correcting irregular blur patterns caused by camera shake, but there is still much room for improvement. We propose a new blind deconvolution method using incremental sparse edge approximation to recover images blurred by camera shake. We estimate the blur kernel first from only the strongest edges in the image, then gradually refine this estimate by allowing for weaker and weaker edges. Our method competes with the benchmark deblurring performance of the state-of-the-art while being significantly faster and easier to generalize. version:2
arxiv-1302-1733 | Feature Selection for Microarray Gene Expression Data using Simulated Annealing guided by the Multivariate Joint Entropy | http://arxiv.org/abs/1302.1733 | id:1302.1733 author:Fernando González, Lluís A. Belanche category:q-bio.QM cs.CE cs.LG stat.ML  published:2013-02-07 summary:In this work a new way to calculate the multivariate joint entropy is presented. This measure is the basis for a fast information-theoretic based evaluation of gene relevance in a Microarray Gene Expression data context. Its low complexity is based on the reuse of previous computations to calculate current feature relevance. The mu-TAFS algorithm --named as such to differentiate it from previous TAFS algorithms-- implements a simulated annealing technique specially designed for feature subset selection. The algorithm is applied to the maximization of gene subset relevance in several public-domain microarray data sets. The experimental results show a notoriously high classification performance and low size subsets formed by biologically meaningful genes. version:1
arxiv-1302-1700 | Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks | http://arxiv.org/abs/1302.1700 | id:1302.1700 author:Alessandro Giusti, Dan C. Cireşan, Jonathan Masci, Luca M. Gambardella, Jürgen Schmidhuber category:cs.CV cs.AI  published:2013-02-07 summary:Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present. version:1
arxiv-1302-1690 | A Fast Learning Algorithm for Image Segmentation with Max-Pooling Convolutional Networks | http://arxiv.org/abs/1302.1690 | id:1302.1690 author:Jonathan Masci, Alessandro Giusti, Dan Cireşan, Gabriel Fricout, Jürgen Schmidhuber category:cs.CV  published:2013-02-07 summary:We present a fast algorithm for training MaxPooling Convolutional Networks to segment images. This type of network yields record-breaking performance in a variety of tasks, but is normally trained on a computationally expensive patch-by-patch basis. Our new method processes each training image in a single pass, which is vastly more efficient. We validate the approach in different scenarios and report a 1500-fold speed-up. In an application to automated steel defect detection and segmentation, we obtain excellent performance with short training times. version:1
arxiv-1302-1649 | Eye-GUIDE (Eye-Gaze User Interface Design) Messaging for Physically-Impaired People | http://arxiv.org/abs/1302.1649 | id:1302.1649 author:Rommel Anacan, James Greggory Alcayde, Retchel Antegra, Leah Luna category:cs.HC cs.CV  published:2013-02-07 summary:Eye-GUIDE is an assistive communication tool designed for the paralyzed or physically impaired people who were unable to move parts of their bodies especially people whose communications are limited only to eye movements. The prototype consists of a camera and a computer. Camera captures images then it will be send to the computer, where the computer will be the one to interpret the data. Thus, Eye-GUIDE focuses on camera-based gaze tracking. The proponent designed the prototype to perform simple tasks and provides graphical user interface in order the paralyzed or physically impaired person can easily use it. version:1
arxiv-1302-1612 | Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering | http://arxiv.org/abs/1302.1612 | id:1302.1612 author:Hanane Froud, Abdelmonaime Lachkar, Said Alaoui Ouatik category:cs.IR cs.CL  published:2013-02-06 summary:Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance. version:1
arxiv-1205-3217 | A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With Application to Homicide Record Systems | http://arxiv.org/abs/1205.3217 | id:1205.3217 author:Mauricio Sadinle, Stephen E. Fienberg category:stat.AP stat.ME stat.ML stat.OT  published:2012-05-14 summary:We present a probabilistic method for linking multiple datafiles. This task is not trivial in the absence of unique identifiers for the individuals recorded. This is a common scenario when linking census data to coverage measurement surveys for census coverage evaluation, and in general when multiple record-systems need to be integrated for posterior analysis. Our method generalizes the Fellegi-Sunter theory for linking records from two datafiles and its modern implementations. The multiple record linkage goal is to classify the record K-tuples coming from K datafiles according to the different matching patterns. Our method incorporates the transitivity of agreement in the computation of the data used to model matching probabilities. We use a mixture model to fit matching probabilities via maximum likelihood using the EM algorithm. We present a method to decide the record K-tuples membership to the subsets of matching patterns and we prove its optimality. We apply our method to the integration of three Colombian homicide record systems and we perform a simulation study in order to explore the performance of the method under measurement error and different scenarios. The proposed method works well and opens some directions for future research. version:2
arxiv-1302-1572 | Lexical Access for Speech Understanding using Minimum Message Length Encoding | http://arxiv.org/abs/1302.1572 | id:1302.1572 author:Ian Thomas, Ingrid Zukerman, Jonathan Oliver, David Albrecht, Bhavani Raskutti category:cs.CL  published:2013-02-06 summary:The Lexical Access Problem consists of determining the intended sequence of words corresponding to an input sequence of phonemes (basic speech sounds) that come from a low-level phoneme recognizer. In this paper we present an information-theoretic approach based on the Minimum Message Length Criterion for solving the Lexical Access Problem. We model sentences using phoneme realizations seen in training, and word and part-of-speech information obtained from text corpora. We show results on multiple-speaker, continuous, read speech and discuss a heuristic using equivalence classes of similar sounding words which speeds up the recognition process without significant deterioration in recognition accuracy. version:1
arxiv-1302-1565 | Learning Bayesian Networks from Incomplete Databases | http://arxiv.org/abs/1302.1565 | id:1302.1565 author:Marco Ramoni, Paola Sebastiani category:cs.AI cs.LG  published:2013-02-06 summary:Bayesian approaches to learn the graphical structure of Bayesian Belief Networks (BBNs) from databases share the assumption that the database is complete, that is, no entry is reported as unknown. Attempts to relax this assumption involve the use of expensive iterative methods to discriminate among different structures. This paper introduces a deterministic method to learn the graphical structure of a BBN from a possibly incomplete database. Experimental evaluations show a significant robustness of this method and a remarkable independence of its execution time from the number of missing data. version:1
arxiv-1302-1552 | An Information-Theoretic Analysis of Hard and Soft Assignment Methods for Clustering | http://arxiv.org/abs/1302.1552 | id:1302.1552 author:Michael Kearns, Yishay Mansour, Andrew Y. Ng category:cs.LG stat.ML  published:2013-02-06 summary:Assignment methods are at the heart of many algorithms for unsupervised learning and clustering - in particular, the well-known K-means and Expectation-Maximization (EM) algorithms. In this work, we study several different methods of assignment, including the "hard" assignments used by K-means and the ?soft' assignments used by EM. While it is known that K-means minimizes the distortion on the data and EM maximizes the likelihood, little is known about the systematic differences of behavior between the two algorithms. Here we shed light on these differences via an information-theoretic analysis. The cornerstone of our results is a simple decomposition of the expected distortion, showing that K-means (and its extension for inferring general parametric densities from unlabeled sample data) must implicitly manage a trade-off between how similar the data assigned to each cluster are, and how the data are balanced among the clusters. How well the data are balanced is measured by the entropy of the partition defined by the hard assignments. In addition to letting us predict and verify systematic differences between K-means and EM on specific examples, the decomposition allows us to give a rather general argument showing that K ?means will consistently find densities with less "overlap" than EM. We also study a third natural assignment method that we call posterior assignment, that is close in spirit to the soft assignments of EM, but leads to a surprisingly different algorithm. version:1
arxiv-1302-1549 | Learning Belief Networks in Domains with Recursively Embedded Pseudo Independent Submodels | http://arxiv.org/abs/1302.1549 | id:1302.1549 author:Jun Hu, Yang Xiang category:cs.AI cs.LG  published:2013-02-06 summary:A pseudo independent (PI) model is a probabilistic domain model (PDM) where proper subsets of a set of collectively dependent variables display marginal independence. PI models cannot be learned correctly by many algorithms that rely on a single link search. Earlier work on learning PI models has suggested a straightforward multi-link search algorithm. However, when a domain contains recursively embedded PI submodels, it may escape the detection of such an algorithm. In this paper, we propose an improved algorithm that ensures the learning of all embedded PI submodels whose sizes are upper bounded by a predetermined parameter. We show that this improved learning capability only increases the complexity slightly beyond that of the previous algorithm. The performance of the new algorithm is demonstrated through experiment. version:1
arxiv-1302-1545 | Models and Selection Criteria for Regression and Classification | http://arxiv.org/abs/1302.1545 | id:1302.1545 author:David Heckerman, Christopher Meek category:cs.LG stat.ML  published:2013-02-06 summary:When performing regression or classification, we are interested in the conditional probability distribution for an outcome or class variable Y given a set of explanatoryor input variables X. We consider Bayesian models for this task. In particular, we examine a special class of models, which we call Bayesian regression/classification (BRC) models, that can be factored into independent conditional (y x) and input (x) models. These models are convenient, because the conditional model (the portion of the full model that we care about) can be analyzed by itself. We examine the practice of transforming arbitrary Bayesian models to BRC models, and argue that this practice is often inappropriate because it ignores prior knowledge that may be important for learning. In addition, we examine Bayesian methods for learning models from data. We discuss two criteria for Bayesian model selection that are appropriate for repression/classification: one described by Spiegelhalter et al. (1993), and another by Buntine (1993). We contrast these two criteria using the prequential framework of Dawid (1984), and give sufficient conditions under which the criteria agree. version:1
arxiv-1302-1542 | Learning Bayesian Nets that Perform Well | http://arxiv.org/abs/1302.1542 | id:1302.1542 author:Russell Greiner, Adam J. Grove, Dale Schuurmans category:cs.AI cs.LG  published:2013-02-06 summary:A Bayesian net (BN) is more than a succinct way to encode a probabilistic distribution; it also corresponds to a function used to answer queries. A BN can therefore be evaluated by the accuracy of the answers it returns. Many algorithms for learning BNs, however, attempt to optimize another criterion (usually likelihood, possibly augmented with a regularizing term), which is independent of the distribution of queries that are posed. This paper takes the "performance criteria" seriously, and considers the challenge of computing the BN whose performance - read "accuracy over the distribution of queries" - is optimal. We show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model. version:1
arxiv-1302-0435 | Parallel D2-Clustering: Large-Scale Clustering of Discrete Distributions | http://arxiv.org/abs/1302.0435 | id:1302.0435 author:Yu Zhang, James Z. Wang, Jia Li category:cs.LG cs.CV I.5.3; D.1.3  published:2013-02-02 summary:The discrete distribution clustering algorithm, namely D2-clustering, has demonstrated its usefulness in image classification and annotation where each object is represented by a bag of weighed vectors. The high computational complexity of the algorithm, however, limits its applications to large-scale problems. We present a parallel D2-clustering algorithm with substantially improved scalability. A hierarchical structure for parallel computing is devised to achieve a balance between the individual-node computation and the integration process of the algorithm. Additionally, it is shown that even with a single CPU, the hierarchical structure results in significant speed-up. Experiments on real-world large-scale image data, Youtube video data, and protein sequence data demonstrate the efficiency and wide applicability of the parallel D2-clustering algorithm. The loss in clustering accuracy is minor in comparison with the original sequential algorithm. version:2
arxiv-1302-1539 | Image Segmentation in Video Sequences: A Probabilistic Approach | http://arxiv.org/abs/1302.1539 | id:1302.1539 author:Nir Friedman, Stuart Russell category:cs.CV cs.AI  published:2013-02-06 summary:"Background subtraction" is an old technique for finding moving objects in a video sequence for example, cars driving on a freeway. The idea is that subtracting the current image from a timeaveraged background image will leave only nonstationary objects. It is, however, a crude approximation to the task of classifying each pixel of the current image; it fails with slow-moving objects and does not distinguish shadows from moving objects. The basic idea of this paper is that we can classify each pixel using a model of how that pixel looks when it is part of different classes. We learn a mixture-of-Gaussians classification model for each pixel using an unsupervised technique- an efficient, incremental version of EM. Unlike the standard image-averaging approach, this automatically updates the mixture component for each class according to likelihood of membership; hence slow-moving objects are handled perfectly. Our approach also identifies and eliminates shadows much more effectively than other techniques such as thresholding. Application of this method as part of the Roadwatch traffic surveillance project is expected to result in significant improvements in vehicle identification and tracking. version:1
arxiv-1302-1538 | Sequential Update of Bayesian Network Structure | http://arxiv.org/abs/1302.1538 | id:1302.1538 author:Nir Friedman, Moises Goldszmidt category:cs.AI cs.LG  published:2013-02-06 summary:There is an obvious need for improving the performance and accuracy of a Bayesian network as new data is observed. Because of errors in model construction and changes in the dynamics of the domains, we cannot afford to ignore the information in new data. While sequential update of parameters for a fixed structure can be accomplished using standard techniques, sequential update of network structure is still an open problem. In this paper, we investigate sequential update of Bayesian networks were both parameters and structure are expected to change. We introduce a new approach that allows for the flexible manipulation of the tradeoff between the quality of the learned networks and the amount of information that is maintained about past observations. We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesian networks, evaluate its effectiveness through an empirical study, and extend it to the case of missing data. version:1
arxiv-1302-1529 | Exploring Parallelism in Learning Belief Networks | http://arxiv.org/abs/1302.1529 | id:1302.1529 author:TongSheng Chu, Yang Xiang category:cs.AI cs.LG  published:2013-02-06 summary:It has been shown that a class of probabilistic domain models cannot be learned correctly by several existing algorithms which employ a single-link look ahead search. When a multi-link look ahead search is used, the computational complexity of the learning algorithm increases. We study how to use parallelism to tackle the increased complexity in learning such models and to speed up learning in large domains. An algorithm is proposed to decompose the learning task for parallel processing. A further task decomposition is used to balance load among processors and to increase the speed-up and efficiency. For learning from very large datasets, we present a regrouping of the available processors such that slow data access through file can be replaced by fast memory access. Our implementation in a parallel computer demonstrates the effectiveness of the algorithm. version:1
arxiv-1302-1519 | Update Rules for Parameter Estimation in Bayesian Networks | http://arxiv.org/abs/1302.1519 | id:1302.1519 author:Eric Bauer, Daphne Koller, Yoram Singer category:cs.LG stat.ML  published:2013-02-06 summary:This paper re-examines the problem of parameter estimation in Bayesian networks with missing values and hidden variables from the perspective of recent work in on-line learning [Kivinen & Warmuth, 1994]. We provide a unified framework for parameter estimation that encompasses both on-line learning, where the model is continuously adapted to new data cases as they arrive, and the more traditional batch learning, where a pre-accumulated set of samples is used in a one-time model selection process. In the batch case, our framework encompasses both the gradient projection algorithm and the EM algorithm for Bayesian networks. The framework also leads to new on-line and batch parameter update schemes, including a parameterized version of EM. We provide both empirical and theoretical results indicating that parameterized EM allows faster convergence to the maximum likelihood parameters than does standard EM. version:1
arxiv-1302-1380 | Towards the Rapid Development of a Natural Language Understanding Module | http://arxiv.org/abs/1302.1380 | id:1302.1380 author:Catarina Moreira, Ana Cristina Mendes, Luísa Coheur, Bruno Martins category:cs.CL  published:2013-02-06 summary:When developing a conversational agent, there is often an urgent need to have a prototype available in order to test the application with real users. A Wizard of Oz is a possibility, but sometimes the agent should be simply deployed in the environment where it will be used. Here, the agent should be able to capture as many interactions as possible and to understand how people react to failure. In this paper, we focus on the rapid development of a natural language understanding module by non experts. Our approach follows the learning paradigm and sees the process of understanding natural language as a classification problem. We test our module with a conversational agent that answers questions in the art domain. Moreover, we show how our approach can be used by a natural language interface to a cinema database. version:1
arxiv-1302-1326 | Cloud Computing framework for Computer Vision Research:An Introduction | http://arxiv.org/abs/1302.1326 | id:1302.1326 author:Yu Zhou category:cs.CV cs.DC  published:2013-02-06 summary:Cloud computing offers the potential to help scientists to process massive number of computing resources often required in machine learning application such as computer vision problems. This proposal would like to show that which benefits can be obtained from cloud in order to help medical image analysis users (including scientists, clinicians, and research institutes). As security and privacy of algorithms are important for most of algorithms inventors, these algorithms can be hidden in a cloud to allow the users to use the algorithms as a package without any access to see/change their inside. In another word, in the user part, users send their images to the cloud and configure the algorithm via an interface. In the cloud part, the algorithms are applied to this image and the results are returned back to the user. My proposal has two parts: (1) investigate the potential of cloud computing for computer vision problems and (2) study the components of a proposed cloud-based framework for medical image analysis application and develop them (depending on the length of the internship). The investigation part will involve a study on several aspects of the problem including security, usability (for medical end users of the service), appropriate programming abstractions for vision problems, scalability and resource requirements. In the second part of this proposal I am going to thoroughly study of the proposed framework components and their relations and develop them. The proposed cloud-based framework includes an integrated environment to enable scientists and clinicians to access to the previous and current medical image analysis algorithms using a handful user interface without any access to the algorithm codes and procedures. version:1
arxiv-1302-1300 | Kriging Interpolation Filter to Reduce High Density Salt and Pepper Noise | http://arxiv.org/abs/1302.1300 | id:1302.1300 author:Firas Ajil Jassim category:cs.CV  published:2013-02-06 summary:Image denoising is a critical issue in the field of digital image processing. This paper proposes a novel Salt & Pepper noise suppression by developing a Kriging Interpolation Filter (KIF) for image denoising. Gray-level images degraded with Salt & Pepper noise have been considered. A sequential search for noise detection was made using kXk window size to determine non-noisy pixels only. The non-noisy pixels are passed into Kriging interpolation method to predict their absent neighbor pixels that were noisy pixels at the first phase. The utilization of Kriging interpolation filter proves that it is very impressive to suppress high noise density. It has been found that Kriging Interpolation filter achieves noise reduction without loss of edges and detailed information. Comparisons with existing algorithms are done using quality metrics like PSNR and MSE to assess the proposed filter. version:1
arxiv-1205-1245 | Sparse group lasso and high dimensional multinomial classification | http://arxiv.org/abs/1205.1245 | id:1205.1245 author:Martin Vincent, Niels Richard Hansen category:stat.ML cs.LG stat.CO  published:2012-05-06 summary:The sparse group lasso optimization problem is solved using a coordinate gradient descent algorithm. The algorithm is applicable to a broad class of convex loss functions. Convergence of the algorithm is established, and the algorithm is used to investigate the performance of the multinomial sparse group lasso classifier. On three different real data examples the multinomial group lasso clearly outperforms multinomial lasso in terms of achieved classification error rate and in terms of including fewer features for the classification. The run-time of our sparse group lasso implementation is of the same order of magnitude as the multinomial lasso algorithm implemented in the R package glmnet. Our implementation scales well with the problem size. One of the high dimensional examples considered is a 50 class classification problem with 10k features, which amounts to estimating 500k parameters. The implementation is available as the R package msgl. version:2
arxiv-1302-1296 | Hybrid Image Segmentation using Discerner Cluster in FCM and Histogram Thresholding | http://arxiv.org/abs/1302.1296 | id:1302.1296 author:Firas Ajil Jassim category:cs.CV  published:2013-02-06 summary:Image thresholding has played an important role in image segmentation. This paper presents a hybrid approach for image segmentation based on the thresholding by fuzzy c-means (THFCM) algorithm for image segmentation. The goal of the proposed approach is to find a discerner cluster able to find an automatic threshold. The algorithm is formulated by applying the standard FCM clustering algorithm to the frequencies (y-values) on the smoothed histogram. Hence, the frequencies of an image can be used instead of the conventional whole data of image. The cluster that has the highest peak which represents the maximum frequency in the image histogram will play as an excellent role in determining a discerner cluster to the grey level image. Then, the pixels belong to the discerner cluster represent an object in the gray level histogram while the other clusters represent a background. Experimental results with standard test images have been obtained through the proposed approach (THFCM). version:1
arxiv-1302-1294 | Image Interpolation Using Kriging Technique for Spatial Data | http://arxiv.org/abs/1302.1294 | id:1302.1294 author:Firas Ajil Jassim, Fawzi Hasan Altaany category:cs.CV  published:2013-02-06 summary:Image interpolation has been used spaciously by customary interpolation techniques. Recently, Kriging technique has been widely implemented in simulation area and geostatistics for prediction. In this article, Kriging technique was used instead of the classical interpolation methods to predict the unknown points in the digital image array. The efficiency of the proposed technique was proven using the PSNR and compared with the traditional interpolation techniques. The results showed that Kriging technique is almost accurate as cubic interpolation and in some images Kriging has higher accuracy. A miscellaneous test images have been used to consolidate the proposed technique. version:1
arxiv-1302-1232 | When are the most informative components for inference also the principal components? | http://arxiv.org/abs/1302.1232 | id:1302.1232 author:Raj Rao Nadakuditi category:math.ST cs.DS cs.IT cs.LG math.IT math.PR stat.TH  published:2013-02-05 summary:Which components of the singular value decomposition of a signal-plus-noise data matrix are most informative for the inferential task of detecting or estimating an embedded low-rank signal matrix? Principal component analysis ascribes greater importance to the components that capture the greatest variation, i.e., the singular vectors associated with the largest singular values. This choice is often justified by invoking the Eckart-Young theorem even though that work addresses the problem of how to best represent a signal-plus-noise matrix using a low-rank approximation and not how to best_infer_ the underlying low-rank signal component. Here we take a first-principles approach in which we start with a signal-plus-noise data matrix and show how the spectrum of the noise-only component governs whether the principal or the middle components of the singular value decomposition of the data matrix will be the informative components for inference. Simply put, if the noise spectrum is supported on a connected interval, in a sense we make precise, then the use of the principal components is justified. When the noise spectrum is supported on multiple intervals, then the middle components might be more informative than the principal components. The end result is a proper justification of the use of principal components in the setting where the noise matrix is i.i.d. Gaussian and the identification of scenarios, generically involving heterogeneous noise models such as mixtures of Gaussians, where the middle components might be more informative than the principal components so that they may be exploited to extract additional processing gain. Our results show how the blind use of principal components can lead to suboptimal or even faulty inference because of phase transitions that separate a regime where the principal components are informative from a regime where they are uninformative. version:1
arxiv-1302-1143 | Evolvability Is Inevitable: Increasing Evolvability Without the Pressure to Adapt | http://arxiv.org/abs/1302.1143 | id:1302.1143 author:Joel Lehman, Kenneth O. Stanley category:cs.NE q-bio.PE  published:2013-02-05 summary:Why evolvability appears to have increased over evolutionary time is an important unresolved biological question. Unlike most candidate explanations, this paper proposes that increasing evolvability can result without any pressure to adapt. The insight is that if evolvability is heritable, then an unbiased drifting process across genotypes can still create a distribution of phenotypes biased towards evolvability, because evolvable organisms diffuse more quickly through the space of possible phenotypes. Furthermore, because phenotypic divergence often correlates with founding niches, niche founders may on average be more evolvable, which through population growth provides a genotypic bias towards evolvability. Interestingly, the combination of these two mechanisms can lead to increasing evolvability without any pressure to out-compete other organisms, as demonstrated through experiments with a series of simulated models. Thus rather than from pressure to adapt, evolvability may inevitably result from any drift through genotypic space combined with evolution's passive tendency to accumulate niches. version:1
arxiv-1302-1123 | Large Scale Distributed Acoustic Modeling With Back-off N-grams | http://arxiv.org/abs/1302.1123 | id:1302.1123 author:Ciprian Chelba, Peng Xu, Fernando Pereira, Thomas Richardson category:cs.CL 68T10 I.2.7  published:2013-02-05 summary:The paper revives an older approach to acoustic modeling that borrows from n-gram language modeling in an attempt to scale up both the amount of training data and model size (as measured by the number of parameters in the model), to approximately 100 times larger than current sizes used in automatic speech recognition. In such a data-rich setting, we can expand the phonetic context significantly beyond triphones, as well as increase the number of Gaussian mixture components for the context-dependent states that allow it. We have experimented with contexts that span seven or more context-independent phones, and up to 620 mixture components per state. Dealing with unseen phonetic contexts is accomplished using the familiar back-off technique used in language modeling due to implementation simplicity. The back-off acoustic model is estimated, stored and served using MapReduce distributed computing infrastructure. Speech recognition experiments are carried out in an N-best list rescoring framework for Google Voice Search. Training big models on large amounts of data proves to be an effective way to increase the accuracy of a state-of-the-art automatic speech recognition system. We use 87,000 hours of training data (speech along with transcription) obtained by filtering utterances in Voice Search logs on automatic speech recognition confidence. Models ranging in size between 20--40 million Gaussians are estimated using maximum likelihood training. They achieve relative reductions in word-error-rate of 11% and 6% when combined with first-pass models trained using maximum likelihood, and boosted maximum mutual information, respectively. Increasing the context size beyond five phones (quinphones) does not help. version:1
arxiv-1212-0927 | Two Algorithms for Finding $k$ Shortest Paths of a Weighted Pushdown Automaton | http://arxiv.org/abs/1212.0927 | id:1212.0927 author:Ke Wu, Philip Resnik category:cs.CL cs.DS cs.FL  published:2012-12-05 summary:We introduce efficient algorithms for finding the $k$ shortest paths of a weighted pushdown automaton (WPDA), a compact representation of a weighted set of strings with potential applications in parsing and machine translation. Both of our algorithms are derived from the same weighted deductive logic description of the execution of a WPDA using different search strategies. Experimental results show our Algorithm 2 adds very little overhead vs. the single shortest path algorithm, even with a large $k$. version:3
arxiv-1104-3621 | Distance Transform Gradient Density Estimation using the Stationary Phase Approximation | http://arxiv.org/abs/1104.3621 | id:1104.3621 author:Karthik S. Gurumoorthy, Anand Rangarajan category:stat.ML math.PR 42B10  41A60  published:2011-04-19 summary:The complex wave representation (CWR) converts unsigned 2D distance transforms into their corresponding wave functions. Here, the distance transform S(X) appears as the phase of the wave function \phi(X)---specifically, \phi(X)=exp(iS(X)/\tau where \tau is a free parameter. In this work, we prove a novel result using the higher-order stationary phase approximation: we show convergence of the normalized power spectrum (squared magnitude of the Fourier transform) of the wave function to the density function of the distance transform gradients as the free parameter \tau-->0. In colloquial terms, spatial frequencies are gradient histogram bins. Since the distance transform gradients have only orientation information (as their magnitudes are identically equal to one almost everywhere), as \tau-->0, the 2D Fourier transform values mainly lie on the unit circle in the spatial frequency domain. The proof of the result involves standard integration techniques and requires proper ordering of limits. Our mathematical relation indicates that the CWR of distance transforms is an intriguing, new representation. version:4
arxiv-1302-1007 | Image Denoising Using Interquartile Range Filter with Local Averaging | http://arxiv.org/abs/1302.1007 | id:1302.1007 author:Firas Ajil Jassim category:cs.CV  published:2013-02-05 summary:Image denoising is one of the fundamental problems in image processing. In this paper, a novel approach to suppress noise from the image is conducted by applying the interquartile range (IQR) which is one of the statistical methods used to detect outlier effect from a dataset. A window of size kXk was implemented to support IQR filter. Each pixel outside the IQR range of the kXk window is treated as noisy pixel. The estimation of the noisy pixels was obtained by local averaging. The essential advantage of applying IQR filter is to preserve edge sharpness better of the original image. A variety of test images have been used to support the proposed filter and PSNR was calculated and compared with median filter. The experimental results on standard test images demonstrate this filter is simpler and better performing than median filter. version:1
arxiv-1302-0974 | A Comparison of Relaxations of Multiset Cannonical Correlation Analysis and Applications | http://arxiv.org/abs/1302.0974 | id:1302.0974 author:Jan Rupnik, Primoz Skraba, John Shawe-Taylor, Sabrina Guettes category:cs.LG  published:2013-02-05 summary:Canonical correlation analysis is a statistical technique that is used to find relations between two sets of variables. An important extension in pattern analysis is to consider more than two sets of variables. This problem can be expressed as a quadratically constrained quadratic program (QCQP), commonly referred to Multi-set Canonical Correlation Analysis (MCCA). This is a non-convex problem and so greedy algorithms converge to local optima without any guarantees on global optimality. In this paper, we show that despite being highly structured, finding the optimal solution is NP-Hard. This motivates our relaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, can be solved reasonably efficiently and comes with both absolute and output-sensitive approximation quality. In addition to theoretical guarantees, we do an extensive comparison of the QCQP method and the SDP relaxation on a variety of synthetic and real world data. Finally, we present two useful extensions: we incorporate kernel methods and computing multiple sets of canonical vectors. version:1
arxiv-1302-0963 | RandomBoost: Simplified Multi-class Boosting through Randomization | http://arxiv.org/abs/1302.0963 | id:1302.0963 author:Sakrapee Paisitkriangkrai, Chunhua Shen, Qinfeng Shi, Anton van den Hengel category:cs.LG  published:2013-02-05 summary:We propose a novel boosting approach to multi-class classification problems, in which multiple classes are distinguished by a set of random projection matrices in essence. The approach uses random projections to alleviate the proliferation of binary classifiers typically required to perform multi-class classification. The result is a multi-class classifier with a single vector-valued parameter, irrespective of the number of classes involved. Two variants of this approach are proposed. The first method randomly projects the original data into new spaces, while the second method randomly projects the outputs of learned weak classifiers. These methods are not only conceptually simple but also effective and easy to implement. A series of experiments on synthetic, machine learning and visual recognition data sets demonstrate that our proposed methods compare favorably to existing multi-class boosting algorithms in terms of both the convergence rate and classification accuracy. version:1
arxiv-1302-0962 | Improved Accuracy of PSO and DE using Normalization: an Application to Stock Price Prediction | http://arxiv.org/abs/1302.0962 | id:1302.0962 author:Savinderjit Kaur, Veenu Mangat category:cs.NE cs.LG  published:2013-02-05 summary:Data Mining is being actively applied to stock market since 1980s. It has been used to predict stock prices, stock indexes, for portfolio management, trend detection and for developing recommender systems. The various algorithms which have been used for the same include ANN, SVM, ARIMA, GARCH etc. Different hybrid models have been developed by combining these algorithms with other algorithms like roughest, fuzzy logic, GA, PSO, DE, ACO etc. to improve the efficiency. This paper proposes DE-SVM model (Differential EvolutionSupport vector Machine) for stock price prediction. DE has been used to select best free parameters combination for SVM to improve results. The paper also compares the results of prediction with the outputs of SVM alone and PSO-SVM model (Particle Swarm Optimization). The effect of normalization of data on the accuracy of prediction has also been studied. version:1
arxiv-1302-0723 | Multi-Robot Informative Path Planning for Active Sensing of Environmental Phenomena: A Tale of Two Algorithms | http://arxiv.org/abs/1302.0723 | id:1302.0723 author:Nannan Cao, Kian Hsiang Low, John M. Dolan category:cs.LG cs.AI cs.MA cs.RO  published:2013-02-04 summary:A key problem of robotic environmental sensing and monitoring is that of active sensing: How can a team of robots plan the most informative observation paths to minimize the uncertainty in modeling and predicting an environmental phenomenon? This paper presents two principled approaches to efficient information-theoretic path planning based on entropy and mutual information criteria for in situ active sensing of an important broad class of widely-occurring environmental phenomena called anisotropic fields. Our proposed algorithms are novel in addressing a trade-off between active sensing performance and time efficiency. An important practical consequence is that our algorithms can exploit the spatial correlation structure of Gaussian process-based anisotropic fields to improve time efficiency while preserving near-optimal active sensing performance. We analyze the time complexity of our algorithms and prove analytically that they scale better than state-of-the-art algorithms with increasing planning horizon length. We provide theoretical guarantees on the active sensing performance of our algorithms for a class of exploration tasks called transect sampling, which, in particular, can be improved with longer planning time and/or lower spatial correlation along the transect. Empirical evaluation on real-world anisotropic field data shows that our algorithms can perform better or at least as well as the state-of-the-art algorithms while often incurring a few orders of magnitude less computational time, even when the field conditions are less favorable. version:2
arxiv-1203-1005 | Sparse Subspace Clustering: Algorithm, Theory, and Applications | http://arxiv.org/abs/1203.1005 | id:1203.1005 author:Ehsan Elhamifar, Rene Vidal category:cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML  published:2012-03-05 summary:In many real-world problems, we are dealing with collections of high-dimensional data, such as images, videos, text and web documents, DNA microarray data, and more. Often, high-dimensional data lie close to low-dimensional structures corresponding to several classes or categories the data belongs to. In this paper, we propose and study an algorithm, called Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of low-dimensional subspaces. The key idea is that, among infinitely many possible representations of a data point in terms of other points, a sparse representation corresponds to selecting a few points from the same subspace. This motivates solving a sparse optimization program whose solution is used in a spectral clustering framework to infer the clustering of data into subspaces. Since solving the sparse optimization program is in general NP-hard, we consider a convex relaxation and show that, under appropriate conditions on the arrangement of subspaces and the distribution of data, the proposed minimization program succeeds in recovering the desired sparse representations. The proposed algorithm can be solved efficiently and can handle data points near the intersections of subspaces. Another key advantage of the proposed algorithm with respect to the state of the art is that it can deal with data nuisances, such as noise, sparse outlying entries, and missing entries, directly by incorporating the model of the data into the sparse optimization program. We demonstrate the effectiveness of the proposed algorithm through experiments on synthetic data as well as the two real-world problems of motion segmentation and face clustering. version:3
arxiv-1302-2575 | Coded aperture compressive temporal imaging | http://arxiv.org/abs/1302.2575 | id:1302.2575 author:Patrick Llull, Xuejun Liao, Xin Yuan, Jianbo Yang, David Kittle, Lawrence Carin, Guillermo Sapiro, David J. Brady category:cs.CV cs.IT math.IT  published:2013-02-04 summary:We use mechanical translation of a coded aperture for code division multiple access compression of video. We present experimental results for reconstruction at 148 frames per coded snapshot. version:1
arxiv-1302-0895 | Exact Sparse Recovery with L0 Projections | http://arxiv.org/abs/1302.0895 | id:1302.0895 author:Ping Li, Cun-Hui Zhang category:stat.ML cs.IT cs.LG math.IT math.ST stat.TH  published:2013-02-04 summary:Many applications concern sparse signals, for example, detecting anomalies from the differences between consecutive images taken by surveillance cameras. This paper focuses on the problem of recovering a K-sparse signal x in N dimensions. In the mainstream framework of compressed sensing (CS), the vector x is recovered from M non-adaptive linear measurements y = xS, where S (of size N x M) is typically a Gaussian (or Gaussian-like) design matrix, through some optimization procedure such as linear programming (LP). In our proposed method, the design matrix S is generated from an $\alpha$-stable distribution with $\alpha\approx 0$. Our decoding algorithm mainly requires one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are "undetermined" in the previous iteration. Comparisons with two strong baselines, linear programming (LP) and orthogonal matching pursuit (OMP), demonstrate that our algorithm can be significantly faster in decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements, our algorithm can still reliably recover a significant portion of the nonzero coordinates. To provide the intuition for understanding our method, we also analyze the procedure by assuming an idealistic setting. Interestingly, when K=2, the "idealized" algorithm achieves exact recovery with merely 3 measurements, regardless of N. For general K, the required sample size of the "idealized" algorithm is about 5K. version:1
arxiv-1302-0870 | Centrality-constrained graph embedding | http://arxiv.org/abs/1302.0870 | id:1302.0870 author:Brian Baingana, Georgios B. Giannakis category:stat.ML cs.CV math.OC  published:2013-02-04 summary:Visual rendering of graphs is a key task in the mapping of complex network data. Although most graph drawing algorithms emphasize aesthetic appeal, certain applications such as travel-time maps place more importance on visualization of structural network properties. The present paper advocates a graph embedding approach with centrality considerations to comply with node hierarchy. The problem is formulated as one of constrained multi-dimensional scaling (MDS), and it is solved via block coordinate descent iterations with successive approximations and guaranteed convergence to a KKT point. In addition, a regularization term enforcing graph smoothness is incorporated with the goal of reducing edge crossings. Experimental results demonstrate that the algorithm converges, and can be used to efficiently embed large graphs on the order of thousands of nodes. version:1
arxiv-1302-0797 | Comparison of Ant-Inspired Gatherer Allocation Approaches using Memristor-Based Environmental Models | http://arxiv.org/abs/1302.0797 | id:1302.0797 author:Ella Gale, Ben de Lacy Costello, Andrew Adamatzky category:cs.NE  published:2013-02-04 summary:Memristors are used to compare three gathering techniques in an already-mapped environment where resource locations are known. The All Site model, which apportions gatherers based on the modeled memristance of that path, proves to be good at increasing overall efficiency and decreasing time to fully deplete an environment, however it only works well when the resources are of similar quality. The Leaf Cutter method, based on Leaf Cutter Ant behaviour, assigns all gatherers first to the best resource, and once depleted, uses the All Site model to spread them out amongst the rest. The Leaf Cutter model is better at increasing resource influx in the short-term and vastly out-performs the All Site model in a more varied environments. It is demonstrated that memristor based abstractions of gatherer models provide potential methods for both the comparison and implementation of agent controls. version:1
arxiv-1302-0785 | Beyond Markov Chains, Towards Adaptive Memristor Network-based Music Generation | http://arxiv.org/abs/1302.0785 | id:1302.0785 author:Ella Gale, Oliver Matthews, Ben de Lacy Costello, Andrew Adamatzky category:cs.ET cs.AI cs.NE cs.SD 68Txx  published:2013-02-04 summary:We undertook a study of the use of a memristor network for music generation, making use of the memristor's memory to go beyond the Markov hypothesis. Seed transition matrices are created and populated using memristor equations, and which are shown to generate musical melodies and change in style over time as a result of feedback into the transition matrix. The spiking properties of simple memristor networks are demonstrated and discussed with reference to applications of music making. The limitations of simulating composing memristor networks in von Neumann hardware is discussed and a hardware solution based on physical memristor properties is presented. version:1
arxiv-1212-3139 | Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles | http://arxiv.org/abs/1212.3139 | id:1212.3139 author:Aaron Gerow, Mark Keane category:cs.CL  published:2012-12-13 summary:Using a corpus of 17,000+ financial news reports (involving over 10M words), we perform an analysis of the argument-distributions of the UP and DOWN verbs used to describe movements of indices, stocks and shares. In Study 1 participants identified antonyms of these verbs in a free-response task and a matching task from which the most commonly identified antonyms were compiled. In Study 2, we determined whether the argument-distributions for the verbs in these antonym-pairs were sufficiently similar to predict the most frequently-identified antonym. Cosine similarity correlates moderately with the proportions of antonym-pairs identified by people (r = 0.31). More impressively, 87% of the time the most frequently-identified antonym is either the first- or second-most similar pair in the set of alternatives. The implications of these results for distributional approaches to determining metaphoric knowledge are discussed. version:2
arxiv-1302-0689 | Multi-scale Visual Attention & Saliency Modelling with Decision Theory | http://arxiv.org/abs/1302.0689 | id:1302.0689 author:Anh Cat Le Ngo, Li-Minn Ang, Guoping Qiu, Kah-Phooi Seng category:cs.CV  published:2013-02-04 summary:Bottom-up saliency, an early human visual processing, behaves like binary classification of interest and null hypothesis. Its discriminant power, mutual information of image features and class distribution, is closely related to saliency value by the well-known centre-surround theory. As classification accuracy very much depends on window sizes, the discriminant saliency (power) varies according to sampling scales. Discriminating power estimation in multi-scales framework needs integrating with wavelet transformation and then estimating statistical discrepancy of two consecutive scales (centre-surround windows) by Hidden Markov Tree (HMT) model. Finally, multi-scale discriminant saliency (MDIS) maps are combined by the maximum information rule to synthesize a final saliency map. All MDIS maps are evaluated with standard quantitative tools (NSS,LCC,AUC) on N.Bruce's database with ground truth data as eye-tracking locations ; as well assessed qualitatively by visual examination of individual cases. For evaluating MDIS against well-known AIM saliency method, simulations are needed and described in details with several interesting conclusions, drawn for further research directions. version:1
arxiv-1302-0540 | A game-theoretic framework for classifier ensembles using weighted majority voting with local accuracy estimates | http://arxiv.org/abs/1302.0540 | id:1302.0540 author:Harris V. Georgiou, Michael E. Mavroforakis category:cs.LG  published:2013-02-03 summary:In this paper, a novel approach for the optimal combination of binary classifiers is proposed. The classifier combination problem is approached from a Game Theory perspective. The proposed framework of adapted weighted majority rules (WMR) is tested against common rank-based, Bayesian and simple majority models, as well as two soft-output averaging rules. Experiments with ensembles of Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) and weighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate that this new adaptive WMR model, employing local accuracy estimators and the analytically computed optimal weights outperform all the other simple combination rules. version:1
arxiv-1108-1783 | An application of the stationary phase method for estimating probability densities of function derivatives | http://arxiv.org/abs/1108.1783 | id:1108.1783 author:Karthik S. Gurumoorthy, Anand Rangarajan, Arunava Banerjee category:stat.ML math.ST stat.TH  published:2011-08-08 summary:We prove a novel result wherein the density function of the gradients---corresponding to density function of the derivatives in one dimension---of a thrice differentiable function S (obtained via a random variable transformation of a uniformly distributed random variable) defined on a closed, bounded interval \Omega \subset R is accurately approximated by the normalized power spectrum of \phi=exp(iS/\tau) as the free parameter \tau-->0. The result is shown using the well known stationary phase approximation and standard integration techniques and requires proper ordering of limits. Experimental results provide anecdotal visual evidence corroborating the result. version:4
arxiv-1302-0446 | Sparse Camera Network for Visual Surveillance -- A Comprehensive Survey | http://arxiv.org/abs/1302.0446 | id:1302.0446 author:Mingli Song, Dachent Tao, Stephen J. Maybank category:cs.CV  published:2013-02-03 summary:Technological advances in sensor manufacture, communication, and computing are stimulating the development of new applications that are transforming traditional vision systems into pervasive intelligent camera networks. The analysis of visual cues in multi-camera networks enables a wide range of applications, from smart home and office automation to large area surveillance and traffic surveillance. While dense camera networks - in which most cameras have large overlapping fields of view - are well studied, we are mainly concerned with sparse camera networks. A sparse camera network undertakes large area surveillance using as few cameras as possible, and most cameras have non-overlapping fields of view with one another. The task is challenging due to the lack of knowledge about the topological structure of the network, variations in the appearance and motion of specific tracking targets in different views, and the difficulties of understanding composite events in the network. In this review paper, we present a comprehensive survey of recent research results to address the problems of intra-camera tracking, topological structure learning, target appearance modeling, and global activity understanding in sparse camera networks. A number of current open research issues are discussed. version:1
arxiv-1206-1270 | Factoring nonnegative matrices with linear programs | http://arxiv.org/abs/1206.1270 | id:1206.1270 author:Victor Bittorf, Benjamin Recht, Christopher Re, Joel A. Tropp category:math.OC cs.LG stat.ML  published:2012-06-06 summary:This paper describes a new approach, based on linear programming, for computing nonnegative matrix factorizations (NMFs). The key idea is a data-driven model for the factorization where the most salient features in the data are used to express the remaining features. More precisely, given a data matrix X, the algorithm identifies a matrix C such that X approximately equals CX and some linear constraints. The constraints are chosen to ensure that the matrix C selects features; these features can then be used to find a low-rank NMF of X. A theoretical analysis demonstrates that this approach has guarantees similar to those of the recent NMF algorithm of Arora et al. (2012). In contrast with this earlier work, the proposed method extends to more general noise models and leads to efficient, scalable algorithms. Experiments with synthetic and real datasets provide evidence that the new approach is also superior in practice. An optimized C++ implementation can factor a multigigabyte matrix in a matter of minutes. version:2
arxiv-1302-0406 | Generalization Guarantees for a Binary Classification Framework for Two-Stage Multiple Kernel Learning | http://arxiv.org/abs/1302.0406 | id:1302.0406 author:Purushottam Kar category:cs.LG stat.ML  published:2013-02-02 summary:We present generalization bounds for the TS-MKL framework for two stage multiple kernel learning. We also present bounds for sparse kernel learning formulations within the TS-MKL framework. version:1
arxiv-1302-0393 | Lambek vs. Lambek: Functorial Vector Space Semantics and String Diagrams for Lambek Calculus | http://arxiv.org/abs/1302.0393 | id:1302.0393 author:Bob Coecke, Edward Grefenstette, Mehrnoosh Sadrzadeh category:math.LO cs.CL math.CT 16B50  18A10  68T50  published:2013-02-02 summary:The Distributional Compositional Categorical (DisCoCat) model is a mathematical framework that provides compositional semantics for meanings of natural language sentences. It consists of a computational procedure for constructing meanings of sentences, given their grammatical structure in terms of compositional type-logic, and given the empirically derived meanings of their words. For the particular case that the meaning of words is modelled within a distributional vector space model, its experimental predictions, derived from real large scale data, have outperformed other empirically validated methods that could build vectors for a full sentence. This success can be attributed to a conceptually motivated mathematical underpinning, by integrating qualitative compositional type-logic and quantitative modelling of meaning within a category-theoretic mathematical framework. The type-logic used in the DisCoCat model is Lambek's pregroup grammar. Pregroup types form a posetal compact closed category, which can be passed, in a functorial manner, on to the compact closed structure of vector spaces, linear maps and tensor product. The diagrammatic versions of the equational reasoning in compact closed categories can be interpreted as the flow of word meanings within sentences. Pregroups simplify Lambek's previous type-logic, the Lambek calculus, which has been extensively used to formalise and reason about various linguistic phenomena. The apparent reliance of the DisCoCat on pregroups has been seen as a shortcoming. This paper addresses this concern, by pointing out that one may as well realise a functorial passage from the original type-logic of Lambek, a monoidal bi-closed category, to vector spaces, or to any other model of meaning organised within a monoidal bi-closed category. The corresponding string diagram calculus, due to Baez and Stay, now depicts the flow of word meanings. version:1
arxiv-1302-0386 | Fast Damage Recovery in Robotics with the T-Resilience Algorithm | http://arxiv.org/abs/1302.0386 | id:1302.0386 author:Sylvain Koos, Antoine Cully, Jean-Baptiste Mouret category:cs.RO cs.AI cs.LG  published:2013-02-02 summary:Damage recovery is critical for autonomous robots that need to operate for a long time without assistance. Most current methods are complex and costly because they require anticipating each potential damage in order to have a contingency plan ready. As an alternative, we introduce the T-resilience algorithm, a new algorithm that allows robots to quickly and autonomously discover compensatory behaviors in unanticipated situations. This algorithm equips the robot with a self-model and discovers new behaviors by learning to avoid those that perform differently in the self-model and in reality. Our algorithm thus does not identify the damaged parts but it implicitly searches for efficient behaviors that do not use them. We evaluate the T-Resilience algorithm on a hexapod robot that needs to adapt to leg removal, broken legs and motor failures; we compare it to stochastic local search, policy gradient and the self-modeling algorithm proposed by Bongard et al. The behavior of the robot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Using only 25 tests on the robot and an overall running time of 20 minutes, T-Resilience consistently leads to substantially better results than the other approaches. version:1
arxiv-1209-3318 | Hessian Schatten-Norm Regularization for Linear Inverse Problems | http://arxiv.org/abs/1209.3318 | id:1209.3318 author:Stamatios Lefkimmiatis, John Paul Ward, Michael Unser category:math.OC cs.CV cs.NA  published:2012-09-14 summary:We introduce a novel family of invariant, convex, and non-quadratic functionals that we employ to derive regularized solutions of ill-posed linear inverse imaging problems. The proposed regularizers involve the Schatten norms of the Hessian matrix, computed at every pixel of the image. They can be viewed as second-order extensions of the popular total-variation (TV) semi-norm since they satisfy the same invariance properties. Meanwhile, by taking advantage of second-order derivatives, they avoid the staircase effect, a common artifact of TV-based reconstructions, and perform well for a wide range of applications. To solve the corresponding optimization problems, we propose an algorithm that is based on a primal-dual formulation. A fundamental ingredient of this algorithm is the projection of matrices onto Schatten norm balls of arbitrary radius. This operation is performed efficiently based on a direct link we provide between vector projections onto $\ell_q$ norm balls and matrix projections onto Schatten norm balls. Finally, we demonstrate the effectiveness of the proposed methods through experimental results on several inverse imaging problems with real and simulated data. version:3
arxiv-1105-3828 | An Algorithmic Solution to the Five-Point Pose Problem Based on the Cayley Representation of Rotations | http://arxiv.org/abs/1105.3828 | id:1105.3828 author:Evgeniy Martyushev category:cs.CV  published:2011-05-19 summary:We give a new algorithmic solution to the well-known five-point relative pose problem. Our approach does not deal with the famous cubic constraint on an essential matrix. Instead, we use the Cayley representation of rotations in order to obtain a polynomial system from epipolar constraints. Solving that system, we directly get relative rotation and translation parameters of the cameras in terms of roots of a 10th degree polynomial. version:2
arxiv-1302-0324 | A New Constructive Method to Optimize Neural Network Architecture and Generalization | http://arxiv.org/abs/1302.0324 | id:1302.0324 author:Hou Muzhou, Moon Ho Lee category:cs.NE 41A99  65D15  published:2013-02-02 summary:In this paper, after analyzing the reasons of poor generalization and overfitting in neural networks, we consider some noise data as a singular value of a continuous function - jump discontinuity point. The continuous part can be approximated with the simplest neural networks, which have good generalization performance and optimal network architecture, by traditional algorithms such as constructive algorithm for feed-forward neural networks with incremental training, BP algorithm, ELM algorithm, various constructive algorithm, RBF approximation and SVM. At the same time, we will construct RBF neural networks to fit the singular value with every error in, and we prove that a function with jumping discontinuity points can be approximated by the simplest neural networks with a decay RBF neural networks in by each error, and a function with jumping discontinuity point can be constructively approximated by a decay RBF neural networks in by each error and the constructive part have no generalization influence to the whole machine learning system which will optimize neural network architecture and generalization performance, reduce the overfitting phenomenon by avoid fitting the noisy data. version:1
arxiv-1302-0315 | Sparse Multiple Kernel Learning with Geometric Convergence Rate | http://arxiv.org/abs/1302.0315 | id:1302.0315 author:Rong Jin, Tianbao Yang, Mehrdad Mahdavi category:cs.LG stat.ML  published:2013-02-01 summary:In this paper, we study the problem of sparse multiple kernel learning (MKL), where the goal is to efficiently learn a combination of a fixed small number of kernels from a large pool that could lead to a kernel classifier with a small prediction error. We develop an efficient algorithm based on the greedy coordinate descent algorithm, that is able to achieve a geometric convergence rate under appropriate conditions. The convergence rate is achieved by measuring the size of functional gradients by an empirical $\ell_2$ norm that depends on the empirical data distribution. This is in contrast to previous algorithms that use a functional norm to measure the size of gradients, which is independent from the data samples. We also establish a generalization error bound of the learned sparse kernel classifier using the technique of local Rademacher complexity. version:1
arxiv-1302-0256 | Regression shrinkage and grouping of highly correlated predictors with HORSES | http://arxiv.org/abs/1302.0256 | id:1302.0256 author:Woncheol Jang, Johan Lim, Nicole A. Lazar, Ji Meng Loh, Donghyeon Yu category:stat.ML 62J07  62P10  published:2013-02-01 summary:Identifying homogeneous subgroups of variables can be challenging in high dimensional data analysis with highly correlated predictors. We propose a new method called Hexagonal Operator for Regression with Shrinkage and Equality Selection, HORSES for short, that simultaneously selects positively correlated variables and identifies them as predictive clusters. This is achieved via a constrained least-squares problem with regularization that consists of a linear combination of an L_1 penalty for the coefficients and another L_1 penalty for pairwise differences of the coefficients. This specification of the penalty function encourages grouping of positively correlated predictors combined with a sparsity solution. We construct an efficient algorithm to implement the HORSES procedure. We show via simulation that the proposed method outperforms other variable selection methods in terms of prediction error and parsimony. The technique is demonstrated on two data sets, a small data set from analysis of soil in Appalachia, and a high dimensional data set from a near infrared (NIR) spectroscopy study, showing the flexibility of the methodology. version:1
arxiv-1210-5338 | Pairwise MRF Calibration by Perturbation of the Bethe Reference Point | http://arxiv.org/abs/1210.5338 | id:1210.5338 author:Cyril Furtlehner, Yufei Han, Jean-Marc Lasgouttes, Victorin Martin category:cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML  published:2012-10-19 summary:We investigate different ways of generating approximate solutions to the pairwise Markov random field (MRF) selection problem. We focus mainly on the inverse Ising problem, but discuss also the somewhat related inverse Gaussian problem because both types of MRF are suitable for inference tasks with the belief propagation algorithm (BP) under certain conditions. Our approach consists in to take a Bethe mean-field solution obtained with a maximum spanning tree (MST) of pairwise mutual information, referred to as the \emph{Bethe reference point}, for further perturbation procedures. We consider three different ways following this idea: in the first one, we select and calibrate iteratively the optimal links to be added starting from the Bethe reference point; the second one is based on the observation that the natural gradient can be computed analytically at the Bethe point; in the third one, assuming no local field and using low temperature expansion we develop a dual loop joint model based on a well chosen fundamental cycle basis. We indeed identify a subclass of planar models, which we refer to as \emph{Bethe-dual graph models}, having possibly many loops, but characterized by a singly connected dual factor graph, for which the partition function and the linear response can be computed exactly in respectively O(N) and $O(N^2)$ operations, thanks to a dual weight propagation (DWP) message passing procedure that we set up. When restricted to this subclass of models, the inverse Ising problem being convex, becomes tractable at any temperature. Experimental tests on various datasets with refined $L_0$ or $L_1$ regularization procedures indicate that these approaches may be competitive and useful alternatives to existing ones. version:2
arxiv-1301-2959 | Eléments pour une théorie des réseaux en phase d'apprentissage | http://arxiv.org/abs/1301.2959 | id:1301.2959 author:Jean Piniello category:nlin.AO cs.NE nlin.CD  published:2013-01-14 summary:This study deals with the evolution of the so called intelligent networks (insect society without leader, cells of an organism, brain...) during their apprenticeship period. The used formalism draws one's inspiration from the one of the Quantum field theory (Principle of stationary action, gauge fields, invariance by symmetry transformations...). After a recall of some definitions, we consider at first the free network, that is to say which does not exchange any information with outside. Then we study the evolution of the network connected with its environment, that is to say immersed into an information field created by this environment which so dictates to it the apprenticeship constraints. At that time, we obtain Lagrange equations which solutions describe the network evolution during the whole apprenticeship period. Finally, while proceeding with the same formalism inspiration, we suggest other study ways capable of evolving the knowledge in the considered scope. version:2
arxiv-1010-0771 | Genetic Algorithm for Mulicriteria Optimization of a Multi-Pickup and Delivery Problem with Time Windows | http://arxiv.org/abs/1010.0771 | id:1010.0771 author:Imen Harbaoui Dridi, Ryan Kammarti, Mekki Ksouri, Pierre Borne category:cs.NE  published:2010-10-05 summary:In This paper we present a genetic algorithm for mulicriteria optimization of a multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW is an optimization vehicles routing problem which must meet requests for transport between suppliers and customers satisfying precedence, capacity and time constraints. This paper purposes a brief literature review of the PDPTW, present an approach based on genetic algorithms and Pareto dominance method to give a set of satisfying solutions to the m-PDPTW minimizing total travel cost, total tardiness time and the vehicles number. version:2
arxiv-1302-0082 | Distribution-Free Distribution Regression | http://arxiv.org/abs/1302.0082 | id:1302.0082 author:Barnabas Poczos, Alessandro Rinaldo, Aarti Singh, Larry Wasserman category:stat.ML cs.LG math.ST stat.TH  published:2013-02-01 summary:`Distribution regression' refers to the situation where a response Y depends on a covariate P where P is a probability distribution. The model is Y=f(P) + mu where f is an unknown regression function and mu is a random error. Typically, we do not observe P directly, but rather, we observe a sample from P. In this paper we develop theory and methods for distribution-free versions of distribution regression. This means that we do not make distributional assumptions about the error term mu and covariate P. We prove that when the effective dimension is small enough (as measured by the doubling dimension), then the excess prediction risk converges to zero with a polynomial rate. version:1
arxiv-1302-0077 | Sparse MRI for motion correction | http://arxiv.org/abs/1302.0077 | id:1302.0077 author:Zai Yang, Cishen Zhang, Lihua Xie category:cs.CV physics.bio-ph physics.med-ph  published:2013-02-01 summary:MR image sparsity/compressibility has been widely exploited for imaging acceleration with the development of compressed sensing. A sparsity-based approach to rigid-body motion correction is presented for the first time in this paper. A motion is sought after such that the compensated MR image is maximally sparse/compressible among the infinite candidates. Iterative algorithms are proposed that jointly estimate the motion and the image content. The proposed method has a lot of merits, such as no need of additional data and loose requirement for the sampling sequence. Promising results are presented to demonstrate its performance. version:1
arxiv-1301-7745 | Equitability, mutual information, and the maximal information coefficient | http://arxiv.org/abs/1301.7745 | id:1301.7745 author:Justin B. Kinney, Gurinder S. Atwal category:q-bio.QM math.ST stat.ME stat.ML stat.TH  published:2013-01-31 summary:Reshef et al. recently proposed a new statistical measure, the "maximal information coefficient" (MIC), for quantifying arbitrary dependencies between pairs of stochastic quantities. MIC is based on mutual information, a fundamental quantity in information theory that is widely understood to serve this need. MIC, however, is not an estimate of mutual information. Indeed, it was claimed that MIC possesses a desirable mathematical property called "equitability" that mutual information lacks. This was not proven; instead it was argued solely through the analysis of simulated data. Here we show that this claim, in fact, is incorrect. First we offer mathematical proof that no (non-trivial) dependence measure satisfies the definition of equitability proposed by Reshef et al.. We then propose a self-consistent and more general definition of equitability that follows naturally from the Data Processing Inequality. Mutual information satisfies this new definition of equitability while MIC does not. Finally, we show that the simulation evidence offered by Reshef et al. was artifactual. We conclude that estimating mutual information is not only practical for many real-world applications, but also provides a natural solution to the problem of quantifying associations in large data sets. version:1
arxiv-1301-7661 | Fast non parametric entropy estimation for spatial-temporal saliency method | http://arxiv.org/abs/1301.7661 | id:1301.7661 author:Anh Cat Le Ngo, Guoping Qiu, Geoff Underwood, Kenneth Li-Minn Ang, Jasmine Kah-Phooi Seng category:cs.CV  published:2013-01-31 summary:This paper formulates bottom-up visual saliency as center surround conditional entropy and presents a fast and efficient technique for the computation of such a saliency map. It is shown that the new saliency formulation is consistent with self-information based saliency, decision-theoretic saliency and Bayesian definition of surprises but also faces the same significant computational challenge of estimating probability density in very high dimensional spaces with limited samples. We have developed a fast and efficient nonparametric method to make the practical implementation of these types of saliency maps possible. By aligning pixels from the center and surround regions and treating their location coordinates as random variables, we use a k-d partitioning method to efficiently estimating the center surround conditional entropy. We present experimental results on two publicly available eye tracking still image databases and show that the new technique is competitive with state of the art bottom-up saliency computational methods. We have also extended the technique to compute spatiotemporal visual saliency of video and evaluate the bottom-up spatiotemporal saliency against eye tracking data on a video taken onboard a moving vehicle with the driver's eye being tracked by a head mounted eye-tracker. version:1
arxiv-1301-7619 | Rank regularization and Bayesian inference for tensor completion and extrapolation | http://arxiv.org/abs/1301.7619 | id:1301.7619 author:Juan Andres Bazerque, Gonzalo Mateos, Georgios B. Giannakis category:cs.IT cs.LG math.IT stat.ML  published:2013-01-31 summary:A novel regularizer of the PARAFAC decomposition factors capturing the tensor's rank is proposed in this paper, as the key enabler for completion of three-way data arrays with missing entries. Set in a Bayesian framework, the tensor completion method incorporates prior information to enhance its smoothing and prediction capabilities. This probabilistic approach can naturally accommodate general models for the data distribution, lending itself to various fitting criteria that yield optimum estimates in the maximum-a-posteriori sense. In particular, two algorithms are devised for Gaussian- and Poisson-distributed data, that minimize the rank-regularized least-squares error and Kullback-Leibler divergence, respectively. The proposed technique is able to recover the "ground-truth'' tensor rank when tested on synthetic data, and to complete brain imaging and yeast gene expression datasets with 50% and 15% of missing entries respectively, resulting in recovery errors at -10dB and -15dB. version:1
arxiv-1202-1444 | Fully Automatic Expression-Invariant Face Correspondence | http://arxiv.org/abs/1202.1444 | id:1202.1444 author:Augusto Salazar, Stefanie Wuhrer, Chang Shu, Flavio Prieto category:cs.CV cs.GR  published:2012-02-07 summary:We consider the problem of computing accurate point-to-point correspondences among a set of human face scans with varying expressions. Our fully automatic approach does not require any manually placed markers on the scan. Instead, the approach learns the locations of a set of landmarks present in a database and uses this knowledge to automatically predict the locations of these landmarks on a newly available scan. The predicted landmarks are then used to compute point-to-point correspondences between a template model and the newly available scan. To accurately fit the expression of the template to the expression of the scan, we use as template a blendshape model. Our algorithm was tested on a database of human faces of different ethnic groups with strongly varying expressions. Experimental results show that the obtained point-to-point correspondence is both highly accurate and consistent for most of the tested 3D face models. version:2
arxiv-1209-1873 | Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization | http://arxiv.org/abs/1209.1873 | id:1209.1873 author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG math.OC  published:2012-09-10 summary:Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications. version:2
arxiv-1301-7411 | On the Geometry of Bayesian Graphical Models with Hidden Variables | http://arxiv.org/abs/1301.7411 | id:1301.7411 author:Raffaella Settimi, Jim Q. Smith category:cs.LG stat.ML  published:2013-01-30 summary:In this paper we investigate the geometry of the likelihood of the unknown parameters in a simple class of Bayesian directed graphs with hidden variables. This enables us, before any numerical algorithms are employed, to obtain certain insights in the nature of the unidentifiability inherent in such models, the way posterior densities will be sensitive to prior densities and the typical geometrical form these posterior densities might take. Many of these insights carry over into more complicated Bayesian networks with systematic missing data. version:1
arxiv-1301-7403 | A Multivariate Discretization Method for Learning Bayesian Networks from Mixed Data | http://arxiv.org/abs/1301.7403 | id:1301.7403 author:Stefano Monti, Gregory F. Cooper category:cs.AI cs.LG  published:2013-01-30 summary:In this paper we address the problem of discretization in the context of learning Bayesian networks (BNs) from data containing both continuous and discrete variables. We describe a new technique for <EM>multivariate</EM> discretization, whereby each continuous variable is discretized while taking into account its interaction with the other variables. The technique is based on the use of a Bayesian scoring metric that scores the discretization policy for a continuous variable given a BN structure and the observed data. Since the metric is relative to the BN structure currently being evaluated, the discretization of a variable needs to be dynamically adjusted as the BN structure changes. version:1
arxiv-1301-7393 | Mixture Representations for Inference and Learning in Boltzmann Machines | http://arxiv.org/abs/1301.7393 | id:1301.7393 author:Neil D. Lawrence, Christopher M. Bishop, Michael I. Jordan category:cs.LG stat.ML  published:2013-01-30 summary:Boltzmann machines are undirected graphical models with two-state stochastic variables, in which the logarithms of the clique potentials are quadratic functions of the node states. They have been widely studied in the neural computing literature, although their practical applicability has been limited by the difficulty of finding an effective learning algorithm. One well-established approach, known as mean field theory, represents the stochastic distribution using a factorized approximation. However, the corresponding learning algorithm often fails to find a good solution. We conjecture that this is due to the implicit uni-modality of the mean field approximation which is therefore unable to capture multi-modality in the true distribution. In this paper we use variational methods to approximate the stochastic distribution using multi-modal mixtures of factorized distributions. We present results for both inference and learning to demonstrate the effectiveness of this approach. version:1
arxiv-1301-7392 | Large Deviation Methods for Approximate Probabilistic Inference | http://arxiv.org/abs/1301.7392 | id:1301.7392 author:Michael Kearns, Lawrence Saul category:cs.LG stat.ML  published:2013-01-30 summary:We study two-layer belief networks of binary random variables in which the conditional probabilities Pr[childlparents] depend monotonically on weighted sums of the parents. In large networks where exact probabilistic inference is intractable, we show how to compute upper and lower bounds on many probabilities of interest. In particular, using methods from large deviation theory, we derive rigorous bounds on marginal probabilities such as Pr[children] and prove rates of convergence for the accuracy of our bounds as a function of network size. Our results apply to networks with generic transfer function parameterizations of the conditional probability tables, such as sigmoid and noisy-OR. They also explicitly illustrate the types of averaging behavior that can simplify the problem of inference in large networks. version:1
arxiv-1301-7390 | Hierarchical Mixtures-of-Experts for Exponential Family Regression Models with Generalized Linear Mean Functions: A Survey of Approximation and Consistency Results | http://arxiv.org/abs/1301.7390 | id:1301.7390 author:Wenxin Jiang, Martin A. Tanner category:cs.LG stat.ML  published:2013-01-30 summary:We investigate a class of hierarchical mixtures-of-experts (HME) models where exponential family regression models with generalized linear mean functions of the form psi(ga+fx^Tfgb) are mixed. Here psi(...) is the inverse link function. Suppose the true response y follows an exponential family regression model with mean function belonging to a class of smooth functions of the form psi(h(fx)) where h(...)in W_2^infty (a Sobolev class over [0,1]^{s}). It is shown that the HME probability density functions can approximate the true density, at a rate of O(m^{-2/s}) in L_p norm, and at a rate of O(m^{-4/s}) in Kullback-Leibler divergence. These rates can be achieved within the family of HME structures with no more than s-layers, where s is the dimension of the predictor fx. It is also shown that likelihood-based inference based on HME is consistent in recovering the truth, in the sense that as the sample size n and the number of experts m both increase, the mean square error of the predicted mean response goes to zero. Conditions for such results to hold are stated and discussed. version:1
arxiv-1301-7378 | Minimum Encoding Approaches for Predictive Modeling | http://arxiv.org/abs/1301.7378 | id:1301.7378 author:Peter D Grunwald, Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri category:cs.LG stat.ML  published:2013-01-30 summary:We analyze differences between two information-theoretically motivated approaches to statistical inference and model selection: the Minimum Description Length (MDL) principle, and the Minimum Message Length (MML) principle. Based on this analysis, we present two revised versions of MML: a pointwise estimator which gives the MML-optimal single parameter model, and a volumewise estimator which gives the MML-optimal region in the parameter space. Our empirical results suggest that with small data sets, the MDL approach yields more accurate predictions than the MML estimators. The empirical results also demonstrate that the revised MML estimators introduced here perform better than the original MML estimator suggested by Wallace and Freeman. version:1
arxiv-1301-7376 | Graphical Models and Exponential Families | http://arxiv.org/abs/1301.7376 | id:1301.7376 author:Dan Geiger, Christopher Meek category:cs.LG stat.ML  published:2013-01-30 summary:We provide a classification of graphical models according to their representation as subfamilies of exponential families. Undirected graphical models with no hidden variables are linear exponential families (LEFs), directed acyclic graphical models and chain graphs with no hidden variables, including Bayesian networks with several families of local distributions, are curved exponential families (CEFs) and graphical models with hidden variables are stratified exponential families (SEFs). An SEF is a finite union of CEFs satisfying a frontier condition. In addition, we illustrate how one can automatically generate independence and non-independence constraints on the distributions over the observable variables implied by a Bayesian network with hidden variables. The relevance of these results for model selection is examined. version:1
arxiv-1301-7375 | Learning by Transduction | http://arxiv.org/abs/1301.7375 | id:1301.7375 author:Alex Gammerman, Volodya Vovk, Vladimir Vapnik category:cs.LG stat.ML  published:2013-01-30 summary:We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed. version:1
arxiv-1301-7374 | Learning the Structure of Dynamic Probabilistic Networks | http://arxiv.org/abs/1301.7374 | id:1301.7374 author:Nir Friedman, Kevin Murphy, Stuart Russell category:cs.AI cs.LG  published:2013-01-30 summary:Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains. version:1
arxiv-1301-7373 | The Bayesian Structural EM Algorithm | http://arxiv.org/abs/1301.7373 | id:1301.7373 author:Nir Friedman category:cs.LG cs.AI stat.ML  published:2013-01-30 summary:In recent years there has been a flurry of works on learning Bayesian networks from data. One of the hard problems in this area is how to effectively learn the structure of a belief network from incomplete data- that is, in the presence of missing values or hidden variables. In a recent paper, I introduced an algorithm called Structural EM that combines the standard Expectation Maximization (EM) algorithm, which optimizes parameters, with structure search for model selection. That algorithm learns networks based on penalized likelihood scores, which include the BIC/MDL score and various approximations to the Bayesian score. In this paper, I extend Structural EM to deal directly with Bayesian model selection. I prove the convergence of the resulting algorithm and show how to apply it for learning a large class of probabilistic models, including Bayesian networks and some variants thereof. version:1
arxiv-1301-7363 | Empirical Analysis of Predictive Algorithms for Collaborative Filtering | http://arxiv.org/abs/1301.7363 | id:1301.7363 author:John S. Breese, David Heckerman, Carl Kadie category:cs.IR cs.LG  published:2013-01-30 summary:Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metrics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time. version:1
arxiv-1112-4258 | A geometric analysis of subspace clustering with outliers | http://arxiv.org/abs/1112.4258 | id:1112.4258 author:Mahdi Soltanolkotabi, Emmanuel J. Candés category:cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2011-12-19 summary:This paper considers the problem of clustering a collection of unlabeled data points assumed to lie near a union of lower-dimensional planes. As is common in computer vision or unsupervised learning applications, we do not know in advance how many subspaces there are nor do we have any information about their dimensions. We develop a novel geometric analysis of an algorithm named sparse subspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009 (2009) 2790-2797. IEEE], which significantly broadens the range of problems where it is provably effective. For instance, we show that SSC can recover multiple subspaces, each of dimension comparable to the ambient dimension. We also prove that SSC can correctly cluster data points even when the subspaces of interest intersect. Further, we develop an extension of SSC that succeeds when the data set is corrupted with possibly overwhelmingly many outliers. Underlying our analysis are clear geometric insights, which may bear on other sparse recovery problems. A numerical study complements our theoretical analysis and demonstrates the effectiveness of these methods. version:5
arxiv-1301-6939 | Multi-Step Regression Learning for Compositional Distributional Semantics | http://arxiv.org/abs/1301.6939 | id:1301.6939 author:Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, Marco Baroni category:cs.CL cs.LG 68T50  published:2013-01-29 summary:We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face. version:2
arxiv-1301-7118 | A note on selection stability: combining stability and prediction | http://arxiv.org/abs/1301.7118 | id:1301.7118 author:Yixin Fang, Junhui Wang, Wei Sun category:stat.ME stat.ML  published:2013-01-30 summary:Recently, many regularized procedures have been proposed for variable selection in linear regression, but their performance depends on the tuning parameter selection. Here a criterion for the tuning parameter selection is proposed, which combines the strength of both stability selection and cross-validation and therefore is referred as the prediction and stability selection (PASS). The selection consistency is established assuming the data generating model is a subset of the full model, and the small sample performance is demonstrated through some simulation studies where the assumption is either held or violated. version:1
arxiv-1301-7115 | Statistical mechanics of complex neural systems and high dimensional data | http://arxiv.org/abs/1301.7115 | id:1301.7115 author:Madhu Advani, Subhaneil Lahiri, Surya Ganguli category:q-bio.NC cond-mat.dis-nn stat.ML  published:2013-01-30 summary:Recent experimental advances in neuroscience have opened new vistas into the immense complexity of neuronal networks. This proliferation of data challenges us on two parallel fronts. First, how can we form adequate theoretical frameworks for understanding how dynamical network processes cooperate across widely disparate spatiotemporal scales to solve important computational problems? And second, how can we extract meaningful models of neuronal systems from high dimensional datasets? To aid in these challenges, we give a pedagogical review of a collection of ideas and theoretical methods arising at the intersection of statistical physics, computer science and neurobiology. We introduce the interrelated replica and cavity methods, which originated in statistical physics as powerful ways to quantitatively analyze large highly heterogeneous systems of many interacting degrees of freedom. We also introduce the closely related notion of message passing in graphical models, which originated in computer science as a distributed algorithm capable of solving large inference and optimization problems involving many coupled variables. We then show how both the statistical physics and computer science perspectives can be applied in a wide diversity of contexts to problems arising in theoretical neuroscience and data analysis. Along the way we discuss spin glasses, learning theory, illusions of structure in noise, random matrices, dimensionality reduction, and compressed sensing, all within the unified formalism of the replica method. Moreover, we review recent conceptual connections between message passing in graphical models, and neural computation and learning. Overall, these ideas illustrate how statistical physics and computer science might provide a lens through which we can uncover emergent computational functions buried deep within the dynamical complexities of neuronal networks. version:1
arxiv-1301-7047 | Link prediction for partially observed networks | http://arxiv.org/abs/1301.7047 | id:1301.7047 author:Yunpeng Zhao, Elizaveta Levina, Ji Zhu category:stat.ML cs.LG cs.SI  published:2013-01-29 summary:Link prediction is one of the fundamental problems in network analysis. In many applications, notably in genetics, a partially observed network may not contain any negative examples of absent edges, which creates a difficulty for many existing supervised learning approaches. We develop a new method which treats the observed network as a sample of the true network with different sampling rates for positive and negative examples. We obtain a relative ranking of potential links by their probabilities, utilizing information on node covariates as well as on network topology. Empirically, the method performs well under many settings, including when the observed network is sparse. We apply the method to a protein-protein interaction network and a school friendship network. version:1
arxiv-1205-5351 | Linearized Alternating Direction Method with Adaptive Penalty and Warm Starts for Fast Solving Transform Invariant Low-Rank Textures | http://arxiv.org/abs/1205.5351 | id:1205.5351 author:Xiang Ren, Zhouchen Lin category:cs.CV  published:2012-05-24 summary:Transform Invariant Low-rank Textures (TILT) is a novel and powerful tool that can effectively rectify a rich class of low-rank textures in 3D scenes from 2D images despite significant deformation and corruption. The existing algorithm for solving TILT is based on the alternating direction method (ADM). It suffers from high computational cost and is not theoretically guaranteed to converge to a correct solution. In this paper, we propose a novel algorithm to speed up solving TILT, with guaranteed convergence. Our method is based on the recently proposed linearized alternating direction method with adaptive penalty (LADMAP). To further reduce computation, warm starts are also introduced to initialize the variables better and cut the cost on singular value decomposition. Extensive experimental results on both synthetic and real data demonstrate that this new algorithm works much more efficiently and robustly than the existing algorithm. It could be at least five times faster than the previous method. version:2
arxiv-1205-4377 | Multi-Stage Classifier Design | http://arxiv.org/abs/1205.4377 | id:1205.4377 author:Kirill Trapeznikov, Venkatesh Saligrama, David Castanon category:cs.CV stat.ML  published:2012-05-20 summary:In many classification systems, sensing modalities have different acquisition costs. It is often {\it unnecessary} to use every modality to classify a majority of examples. We study a multi-stage system in a prediction time cost reduction setting, where the full data is available for training, but for a test example, measurements in a new modality can be acquired at each stage for an additional cost. We seek decision rules to reduce the average measurement acquisition cost. We formulate an empirical risk minimization problem (ERM) for a multi-stage reject classifier, wherein the stage $k$ classifier either classifies a sample using only the measurements acquired so far or rejects it to the next stage where more attributes can be acquired for a cost. To solve the ERM problem, we show that the optimal reject classifier at each stage is a combination of two binary classifiers, one biased towards positive examples and the other biased towards negative examples. We use this parameterization to construct stage-by-stage global surrogate risk, develop an iterative algorithm in the boosting framework and present convergence and generalization results. We test our work on synthetic, medical and explosives detection datasets. Our results demonstrate that substantial cost reduction without a significant sacrifice in accuracy is achievable. version:2
arxiv-1301-6952 | PyXNAT: XNAT in Python | http://arxiv.org/abs/1301.6952 | id:1301.6952 author:Yannick Schwartz, Alexis Barbot, Benjamin Thyreau, Vincent Frouin, Gaël Varoquaux, Aditya Siram, Daniel Marcus, Jean-Baptiste Poline category:cs.DB cs.CV q-bio.QM  published:2013-01-29 summary:As neuroimaging databases grow in size and complexity, the time researchers spend investigating and managing the data increases to the expense of data analysis. As a result, investigators rely more and more heavily on scripting using high-level languages to automate data management and processing tasks. For this, a structured and programmatic access to the data store is necessary. Web services are a first step toward this goal. They however lack in functionality and ease of use because they provide only low level interfaces to databases. We introduce here PyXNAT, a Python module that interacts with The Extensible Neuroimaging Archive Toolkit (XNAT) through native Python calls across multiple operating systems. The choice of Python enables PyXNAT to expose the XNAT Web Services and unify their features with a higher level and more expressive language. PyXNAT provides XNAT users direct access to all the scientific packages in Python. Finally PyXNAT aims to be efficient and easy to use, both as a backend library to build XNAT clients and as an alternative frontend from the command line. version:1
arxiv-1301-6944 | On the Consistency of the Bootstrap Approach for Support Vector Machines and Related Kernel Based Methods | http://arxiv.org/abs/1301.6944 | id:1301.6944 author:Andreas Christmann, Robert Hable category:stat.ML cs.LG  published:2013-01-29 summary:It is shown that bootstrap approximations of support vector machines (SVMs) based on a general convex and smooth loss function and on a general kernel are consistent. This result is useful to approximate the unknown finite sample distribution of SVMs by the bootstrap approach. version:1
arxiv-1301-6770 | An alternative text representation to TF-IDF and Bag-of-Words | http://arxiv.org/abs/1301.6770 | id:1301.6770 author:Zhixiang, Xu, Minmin Chen, Kilian Q. Weinberger, Fei Sha category:cs.IR cs.LG stat.ML  published:2013-01-28 summary:In text mining, information retrieval, and machine learning, text documents are commonly represented through variants of sparse Bag of Words (sBoW) vectors (e.g. TF-IDF). Although simple and intuitive, sBoW style representations suffer from their inherent over-sparsity and fail to capture word-level synonymy and polysemy. Especially when labeled data is limited (e.g. in document classification), or the text documents are short (e.g. emails or abstracts), many features are rarely observed within the training corpus. This leads to overfitting and reduced generalization accuracy. In this paper we propose Dense Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW document features. dCoT explicitly models absent words by removing and reconstructing random sub-sets of words in the unlabeled corpus. With this approach, dCoT learns to reconstruct frequent words from co-occurring infrequent words and maps the high dimensional sparse sBoW vectors into a low-dimensional dense representation. We show that the feature removal can be marginalized out and that the reconstruction can be solved for in closed-form. We demonstrate empirically, on several benchmark datasets, that dCoT features significantly improve the classification accuracy across several document classification tasks. version:1
arxiv-1301-4293 | Latent Relation Representations for Universal Schemas | http://arxiv.org/abs/1301.4293 | id:1301.4293 author:Sebastian Riedel, Limin Yao, Andrew McCallum category:cs.LG stat.ML  published:2013-01-18 summary:Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of pre-existing databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present a family of matrix factorization models that predict affinity between database tuples and relations. We show that this achieves substantially higher accuracy than the traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms state-of-the-art distant supervision systems. version:2
arxiv-1301-6626 | Discriminative Feature Selection for Uncertain Graph Classification | http://arxiv.org/abs/1301.6626 | id:1301.6626 author:Xiangnan Kong, Philip S. Yu, Xue Wang, Ann B. Ragin category:cs.LG cs.DB stat.ML  published:2013-01-28 summary:Mining discriminative features for graph data has attracted much attention in recent years due to its important role in constructing graph classifiers, generating graph indices, etc. Most measurement of interestingness of discriminative subgraph features are defined on certain graphs, where the structure of graph objects are certain, and the binary edges within each graph represent the "presence" of linkages among the nodes. In many real-world applications, however, the linkage structure of the graphs is inherently uncertain. Therefore, existing measurements of interestingness based upon certain graphs are unable to capture the structural uncertainty in these applications effectively. In this paper, we study the problem of discriminative subgraph feature selection from uncertain graphs. This problem is challenging and different from conventional subgraph mining problems because both the structure of the graph objects and the discrimination score of each subgraph feature are uncertain. To address these challenges, we propose a novel discriminative subgraph feature selection method, DUG, which can find discriminative subgraph features in uncertain graphs based upon different statistical measures including expectation, median, mode and phi-probability. We first compute the probability distribution of the discrimination scores for each subgraph feature based on dynamic programming. Then a branch-and-bound algorithm is proposed to search for discriminative subgraphs efficiently. Extensive experiments on various neuroimaging applications (i.e., Alzheimer's Disease, ADHD and HIV) have been performed to analyze the gain in performance by taking into account structural uncertainties in identifying discriminative subgraph features for graph classification. version:1
arxiv-1301-6324 | An improvement to k-nearest neighbor classifier | http://arxiv.org/abs/1301.6324 | id:1301.6324 author:T. Hitendra Sarma, P. Viswanath, D. Sai Koti Reddy, S. Sri Raghava category:cs.CV cs.LG stat.ML  published:2013-01-27 summary:K-Nearest neighbor classifier (k-NNC) is simple to use and has little design time like finding k values in k-nearest neighbor classifier, hence these are suitable to work with dynamically varying data-sets. There exists some fundamental improvements over the basic k-NNC, like weighted k-nearest neighbors classifier (where weights to nearest neighbors are given based on linear interpolation), using artificially generated training set called bootstrapped training set, etc. These improvements are orthogonal to space reduction and classification time reduction techniques, hence can be coupled with any of them. The paper proposes another improvement to the basic k-NNC where the weights to nearest neighbors are given based on Gaussian distribution (instead of linear interpolation as done in weighted k-NNC) which is also independent of any space reduction and classification time reduction technique. We formally show that our proposed method is closely related to non-parametric density estimation using a Gaussian kernel. We experimentally demonstrate using various standard data-sets that the proposed method is better than the existing ones in most cases. version:1
arxiv-1210-8442 | Linear-Nonlinear-Poisson Neuron Networks Perform Bayesian Inference On Boltzmann Machines | http://arxiv.org/abs/1210.8442 | id:1210.8442 author:Louis Yuanlong Shao category:cs.AI cs.NE q-bio.NC stat.ML  published:2012-10-31 summary:One conjecture in both deep learning and classical connectionist viewpoint is that the biological brain implements certain kinds of deep networks as its back-end. However, to our knowledge, a detailed correspondence has not yet been set up, which is important if we want to bridge between neuroscience and machine learning. Recent researches emphasized the biological plausibility of Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally plausible settings, the whole network is capable of representing any Boltzmann machine and performing a semi-stochastic Bayesian inference algorithm lying between Gibbs sampling and variational inference. version:3
arxiv-1104-0861 | Generalized double Pareto shrinkage | http://arxiv.org/abs/1104.0861 | id:1104.0861 author:Artin Armagan, David Dunson, Jaeyong Lee category:stat.ME math.ST stat.ML stat.TH  published:2011-04-05 summary:We propose a generalized double Pareto prior for Bayesian shrinkage estimation and inferences in linear models. The prior can be obtained via a scale mixture of Laplace or normal distributions, forming a bridge between the Laplace and Normal-Jeffreys' priors. While it has a spike at zero like the Laplace density, it also has a Student's $t$-like tail behavior. Bayesian computation is straightforward via a simple Gibbs sampling algorithm. We investigate the properties of the maximum a posteriori estimator, as sparse estimation plays an important role in many problems, reveal connections with some well-established regularization procedures, and show some asymptotic results. The performance of the prior is tested through simulations and an application. version:4
arxiv-1301-6277 | LA-LDA: A Limited Attention Topic Model for Social Recommendation | http://arxiv.org/abs/1301.6277 | id:1301.6277 author:Jeon-Hyung Kang, Kristina Lerman, Lise Getoor category:cs.SI cs.IR cs.LG  published:2013-01-26 summary:Social media users have finite attention which limits the number of incoming messages from friends they can process. Moreover, they pay more attention to opinions and recommendations of some friends more than others. In this paper, we propose LA-LDA, a latent topic model which incorporates limited, non-uniformly divided attention in the diffusion process by which opinions and information spread on the social network. We show that our proposed model is able to learn more accurate user models from users' social network and item adoption behavior than models which do not take limited attention into account. We analyze voting on news items on the social news aggregator Digg and show that our proposed model is better able to predict held out votes than alternative models. Our study demonstrates that psycho-socially motivated models have better ability to describe and predict observed behavior than models which only consider topics. version:1
arxiv-1301-5686 | Transfer Topic Modeling with Ease and Scalability | http://arxiv.org/abs/1301.5686 | id:1301.5686 author:Jeon-Hyung Kang, Jun Ma, Yan Liu category:cs.CL cs.LG stat.ML  published:2013-01-24 summary:The increasing volume of short texts generated on social media sites, such as Twitter or Facebook, creates a great demand for effective and efficient topic modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it is not optimal due to its weakness in handling short texts with fast-changing topics and scalability concerns. In this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting and result interpretation. Specifically, we develop Transfer Hierarchical LDA (thLDA) model, which incorporates the label information from other domains via informative priors. In addition, we develop a parallel implementation of our model for large-scale applications. We demonstrate the effectiveness of our thLDA model on both a microblogging dataset and standard text collections including AP and RCV1 datasets. version:2
arxiv-1211-4410 | Mixture Gaussian Process Conditional Heteroscedasticity | http://arxiv.org/abs/1211.4410 | id:1211.4410 author:Emmanouil A. Platanios, Sotirios P. Chatzis category:cs.LG stat.ML  published:2012-11-19 summary:Generalized autoregressive conditional heteroscedasticity (GARCH) models have long been considered as one of the most successful families of approaches for volatility modeling in financial return series. In this paper, we propose an alternative approach based on methodologies widely used in the field of statistical machine learning. Specifically, we propose a novel nonparametric Bayesian mixture of Gaussian process regression models, each component of which models the noise variance process that contaminates the observed data as a separate latent Gaussian process driven by the observed data. This way, we essentially obtain a mixture Gaussian process conditional heteroscedasticity (MGPCH) model for volatility modeling in financial return series. We impose a nonparametric prior with power-law nature over the distribution of the model mixture components, namely the Pitman-Yor process prior, to allow for better capturing modeled data distributions with heavy tails and skewness. Finally, we provide a copula- based approach for obtaining a predictive posterior for the covariances over the asset returns modeled by means of a postulated MGPCH model. We evaluate the efficacy of our approach in a number of benchmark scenarios, and compare its performance to state-of-the-art methodologies. version:4
arxiv-1301-3530 | The Neural Representation Benchmark and its Evaluation on Brain and Machine | http://arxiv.org/abs/1301.3530 | id:1301.3530 author:Charles F. Cadieu, Ha Hong, Dan Yamins, Nicolas Pinto, Najib J. Majaj, James J. DiCarlo category:cs.NE cs.CV cs.LG q-bio.NC  published:2013-01-15 summary:A key requirement for the development of effective learning representations is their evaluation and comparison to representations we know to be effective. In natural sensory domains, the community has viewed the brain as a source of inspiration and as an implicit benchmark for success. However, it has not been possible to directly test representational learning algorithms directly against the representations contained in neural systems. Here, we propose a new benchmark for visual representations on which we have directly tested the neural representation in multiple visual cortical areas in macaque (utilizing data from [Majaj et al., 2012]), and on which any computer vision algorithm that produces a feature space can be tested. The benchmark measures the effectiveness of the neural or machine representation by computing the classification loss on the ordered eigendecomposition of a kernel matrix [Montavon et al., 2011]. In our analysis we find that the neural representation in visual area IT is superior to visual area V4. In our analysis of representational learning algorithms, we find that three-layer models approach the representational performance of V4 and the algorithm in [Le et al., 2012] surpasses the performance of V4. Impressively, we find that a recent supervised algorithm [Krizhevsky et al., 2012] achieves performance comparable to that of IT for an intermediate level of image variation difficulty, and surpasses IT at a higher difficulty level. We believe this result represents a major milestone: it is the first learning algorithm we have found that exceeds our current estimate of IT representation performance. We hope that this benchmark will assist the community in matching the representational performance of visual cortex and will serve as an initial rallying point for further correspondence between representations derived in brains and machines. version:2
arxiv-1301-6058 | Weighted Last-Step Min-Max Algorithm with Improved Sub-Logarithmic Regret | http://arxiv.org/abs/1301.6058 | id:1301.6058 author:Edward Moroshko, Koby Crammer category:cs.LG  published:2013-01-25 summary:In online learning the performance of an algorithm is typically compared to the performance of a fixed function from some class, with a quantity called regret. Forster proposed a last-step min-max algorithm which was somewhat simpler than the algorithm of Vovk, yet with the same regret. In fact the algorithm he analyzed assumed that the choices of the adversary are bounded, yielding artificially only the two extreme cases. We fix this problem by weighing the examples in such a way that the min-max problem will be well defined, and provide analysis with logarithmic regret that may have better multiplicative factor than both bounds of Forster and Vovk. We also derive a new bound that may be sub-logarithmic, as a recent bound of Orabona et.al, but may have better multiplicative factor. Finally, we analyze the algorithm in a weak-type of non-stationary setting, and show a bound that is sub-linear if the non-stationarity is sub-linear as well. version:1
arxiv-1203-6178 | Statistical Mechanics of Dictionary Learning | http://arxiv.org/abs/1203.6178 | id:1203.6178 author:Ayaka Sakata, Yoshiyuki Kabashima category:cond-mat.dis-nn cond-mat.stat-mech cs.IT cs.LG math.IT  published:2012-03-28 summary:Finding a basis matrix (dictionary) by which objective signals are represented sparsely is of major relevance in various scientific and technological fields. We consider a problem to learn a dictionary from a set of training signals. We employ techniques of statistical mechanics of disordered systems to evaluate the size of the training set necessary to typically succeed in the dictionary learning. The results indicate that the necessary size is much smaller than previously estimated, which theoretically supports and/or encourages the use of dictionary learning in practical situations. version:3
arxiv-1301-6027 | Explorative Data Analysis for Changes in Neural Activity | http://arxiv.org/abs/1301.6027 | id:1301.6027 author:Duncan A. J. Blythe, Frank C. Meinecke, Paul von Buenau, Klaus-Robert Mueller category:q-bio.QM stat.ML  published:2013-01-25 summary:Neural recordings are nonstationary time series, i.e. their properties typically change over time. Identifying specific changes, e.g. those induced by a learning task, can shed light on the underlying neural processes. However, such changes of interest are often masked by strong unrelated changes, which can be of physiological origin or due to measurement artifacts. We propose a novel algorithm for disentangling such different causes of non-stationarity and in this manner enable better neurophysiological interpretation for a wider set of experimental paradigms. A key ingredient is the repeated application of Stationary Subspace Analysis (SSA) using different temporal scales. The usefulness of our explorative approach is demonstrated in simulations, theory and EEG experiments with 80 Brain-Computer-Interfacing (BCI) subjects. version:1
arxiv-1301-5898 | Phase Diagram and Approximate Message Passing for Blind Calibration and Dictionary Learning | http://arxiv.org/abs/1301.5898 | id:1301.5898 author:Florent Krzakala, Marc Mézard, Lenka Zdeborová category:cs.IT cond-mat.stat-mech cs.LG math.IT  published:2013-01-24 summary:We consider dictionary learning and blind calibration for signals and matrices created from a random ensemble. We study the mean-squared error in the limit of large signal dimension using the replica method and unveil the appearance of phase transitions delimiting impossible, possible-but-hard and possible inference regions. We also introduce an approximate message passing algorithm that asymptotically matches the theoretical performance, and show through numerical tests that it performs very well, for the calibration problem, for tractable system sizes. version:1
arxiv-1210-3210 | Fitness Landscape-Based Characterisation of Nature-Inspired Algorithms | http://arxiv.org/abs/1210.3210 | id:1210.3210 author:Matthew Crossley, Andy Nisbet, Martyn Amos category:cs.NE  published:2012-10-11 summary:A significant challenge in nature-inspired algorithmics is the identification of specific characteristics of problems that make them harder (or easier) to solve using specific methods. The hope is that, by identifying these characteristics, we may more easily predict which algorithms are best-suited to problems sharing certain features. Here, we approach this problem using fitness landscape analysis. Techniques already exist for measuring the "difficulty" of specific landscapes, but these are often designed solely with evolutionary algorithms in mind, and are generally specific to discrete optimisation. In this paper we develop an approach for comparing a wide range of continuous optimisation algorithms. Using a fitness landscape generation technique, we compare six different nature-inspired algorithms and identify which methods perform best on landscapes exhibiting specific features. version:2
arxiv-1301-5734 | Reinforcement learning from comparisons: Three alternatives is enough, two is not | http://arxiv.org/abs/1301.5734 | id:1301.5734 author:Benoit Laslier, Jean-Francois Laslier category:math.OC cs.LG math.PR  published:2013-01-24 summary:The paper deals with the problem of finding the best alternatives on the basis of pairwise comparisons when these comparisons need not be transitive. In this setting, we study a reinforcement urn model. We prove convergence to the optimal solution when reinforcement of a winning alternative occurs each time after considering three random alternatives. The simpler process, which reinforces the winner of a random pair does not always converges: it may cycle. version:1
arxiv-1301-5584 | Improved Cheeger's Inequality: Analysis of Spectral Partitioning Algorithms through Higher Order Spectral Gap | http://arxiv.org/abs/1301.5584 | id:1301.5584 author:Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Shayan Oveis Gharan, Luca Trevisan category:cs.DS cs.DM math.CO math.SP stat.ML  published:2013-01-23 summary:Let \phi(G) be the minimum conductance of an undirected graph G, and let 0=\lambda_1 <= \lambda_2 <=... <= \lambda_n <= 2 be the eigenvalues of the normalized Laplacian matrix of G. We prove that for any graph G and any k >= 2, \phi(G) = O(k) \lambda_2 / \sqrt{\lambda_k}, and this performance guarantee is achieved by the spectral partitioning algorithm. This improves Cheeger's inequality, and the bound is optimal up to a constant factor for any k. Our result shows that the spectral partitioning algorithm is a constant factor approximation algorithm for finding a sparse cut if \lambda_k$ is a constant for some constant k. This provides some theoretical justification to its empirical performance in image segmentation and clustering problems. We extend the analysis to other graph partitioning problems, including multi-way partition, balanced separator, and maximum cut. version:1
arxiv-1301-5582 | Multi-Class Detection and Segmentation of Objects in Depth | http://arxiv.org/abs/1301.5582 | id:1301.5582 author:Cheng Zhang, Hedvig Kjellstrom category:cs.CV cs.RO  published:2013-01-23 summary:The quality of life of many people could be improved by autonomous humanoid robots in the home. To function in the human world, a humanoid household robot must be able to locate itself and perceive the environment like a human; scene perception, object detection and segmentation, and object spatial localization in 3D are fundamental capabilities for such humanoid robots. This paper presents a 3D multi-class object detection and segmentation method. The contributions are twofold. Firstly, we present a multi-class detection method, where a minimal joint codebook is learned in a principled manner. Secondly, we incorporate depth information using RGB-D imagery, which increases the robustness of the method and gives the 3D location of objects -- necessary since the robot reasons in 3D space. Experiments show that the multi-class extension improves the detection efficiency with respect to the number of classes and the depth extension improves the detection robustness and give sufficient natural 3D location of the objects. version:1
arxiv-1301-6738 | Approximate Learning in Complex Dynamic Bayesian Networks | http://arxiv.org/abs/1301.6738 | id:1301.6738 author:Raffaella Settimi, Jim Q. Smith, A. S. Gargoum category:cs.LG stat.ML  published:2013-01-23 summary:In this paper we extend the work of Smith and Papamichail (1999) and present fast approximate Bayesian algorithms for learning in complex scenarios where at any time frame, the relationships between explanatory state space variables can be described by a Bayesian network that evolve dynamically over time and the observations taken are not necessarily Gaussian. It uses recent developments in approximate Bayesian forecasting methods in combination with more familiar Gaussian propagation algorithms on junction trees. The procedure for learning state parameters from data is given explicitly for common sampling distributions and the methodology is illustrated through a real application. The efficiency of the dynamic approximation is explored by using the Hellinger divergence measure and theoretical bounds for the efficacy of such a procedure are discussed. version:1
arxiv-1301-6731 | Variational Learning in Mixed-State Dynamic Graphical Models | http://arxiv.org/abs/1301.6731 | id:1301.6731 author:Vladimir Pavlovic, Brendan J. Frey, Thomas S. Huang category:cs.LG stat.ML  published:2013-01-23 summary:Many real-valued stochastic time-series are locally linear (Gassian), but globally non-linear. For example, the trajectory of a human hand gesture can be viewed as a linear dynamic system driven by a nonlinear dynamic system that represents muscle actions. We present a mixed-state dynamic graphical model in which a hidden Markov model drives a linear dynamic system. This combination allows us to model both the discrete and continuous causes of trajectories such as human gestures. The number of computations needed for exact inference is exponential in the sequence length, so we derive an approximate variational inference technique that can also be used to learn the parameters of the discrete and continuous models. We show how the mixed-state model and the variational technique can be used to classify human hand gestures made with a computer mouse. version:1
arxiv-1301-6730 | Accelerating EM: An Empirical Study | http://arxiv.org/abs/1301.6730 | id:1301.6730 author:Luis E. Ortiz, Leslie Pack Kaelbling category:cs.LG stat.ML  published:2013-01-23 summary:Many applications require that we learn the parameters of a model from data. EM is a method used to learn the parameters of probabilistic models for which the data for some of the variables in the models is either missing or hidden. There are instances in which this method is slow to converge. Therefore, several accelerations have been proposed to improve the method. None of the proposed acceleration methods are theoretically dominant and experimental comparisons are lacking. In this paper, we present the different proposed accelerations and try to compare them experimentally. From the results of the experiments, we argue that some acceleration of EM is always possible, but that which acceleration is superior depends on properties of the problem. version:1
arxiv-1301-6727 | Learning Bayesian Networks with Restricted Causal Interactions | http://arxiv.org/abs/1301.6727 | id:1301.6727 author:Julian R. Neil, Chris S. Wallace, Kevin B. Korb category:cs.AI cs.LG stat.ML  published:2013-01-23 summary:A major problem for the learning of Bayesian networks (BNs) is the exponential number of parameters needed for conditional probability tables. Recent research reduces this complexity by modeling local structure in the probability tables. We examine the use of log-linear local models. While log-linear models in this context are not new (Whittaker, 1990; Buntine, 1991; Neal, 1992; Heckerman and Meek, 1997), for structure learning they are generally subsumed under a naive Bayes model. We describe an alternative interpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric for structure learning of networks exhibiting causal independence, which we term first-order networks (FONs). We also investigate local model selection on a node-by-node basis. version:1
arxiv-1301-6726 | Learning Bayesian Networks from Incomplete Data with Stochastic Search Algorithms | http://arxiv.org/abs/1301.6726 | id:1301.6726 author:James W. Myers, Kathryn Blackmond Laskey, Tod S. Levitt category:cs.AI cs.LG  published:2013-01-23 summary:This paper describes stochastic search approaches, including a new stochastic algorithm and an adaptive mutation operator, for learning Bayesian networks from incomplete data. This problem is characterized by a huge solution space with a highly multimodal landscape. State-of-the-art approaches all involve using deterministic approaches such as the expectation-maximization algorithm. These approaches are guaranteed to find local maxima, but do not explore the landscape for other modes. Our approach evolves structure and the missing data. We compare our stochastic algorithms and show they all produce accurate results. version:1
arxiv-1301-6725 | Loopy Belief Propagation for Approximate Inference: An Empirical Study | http://arxiv.org/abs/1301.6725 | id:1301.6725 author:Kevin Murphy, Yair Weiss, Michael I. Jordan category:cs.AI cs.LG  published:2013-01-23 summary:Recently, researchers have demonstrated that loopy belief propagation - the use of Pearls polytree algorithm IN a Bayesian network WITH loops OF error- correcting codes.The most dramatic instance OF this IS the near Shannon - limit performance OF Turbo Codes codes whose decoding algorithm IS equivalent TO loopy belief propagation IN a chain - structured Bayesian network. IN this paper we ask : IS there something special about the error - correcting code context, OR does loopy propagation WORK AS an approximate inference schemeIN a more general setting? We compare the marginals computed using loopy propagation TO the exact ones IN four Bayesian network architectures, including two real - world networks : ALARM AND QMR.We find that the loopy beliefs often converge AND WHEN they do, they give a good approximation TO the correct marginals.However,ON the QMR network, the loopy beliefs oscillated AND had no obvious relationship TO the correct posteriors. We present SOME initial investigations INTO the cause OF these oscillations, AND show that SOME simple methods OF preventing them lead TO the wrong results. version:1
arxiv-1301-6724 | A Variational Approximation for Bayesian Networks with Discrete and Continuous Latent Variables | http://arxiv.org/abs/1301.6724 | id:1301.6724 author:Kevin Murphy category:cs.AI cs.LG stat.ML  published:2013-01-23 summary:We show how to use a variational approximation to the logistic function to perform approximate inference in Bayesian networks containing discrete nodes with continuous parents. Essentially, we convert the logistic function to a Gaussian, which facilitates exact inference, and then iteratively adjust the variational parameters to improve the quality of the approximation. We demonstrate experimentally that this approximation is faster and potentially more accurate than sampling. We also introduce a simple new technique for handling evidence, which allows us to handle arbitrary distributions on observed nodes, as well as achieving a significant speedup in networks with discrete variables of large cardinality. version:1
arxiv-1301-6723 | A Bayesian Network Classifier that Combines a Finite Mixture Model and a Naive Bayes Model | http://arxiv.org/abs/1301.6723 | id:1301.6723 author:Stefano Monti, Gregory F. Cooper category:cs.LG cs.AI stat.ML  published:2013-01-23 summary:In this paper we present a new Bayesian network model for classification that combines the naive-Bayes (NB) classifier and the finite-mixture (FM) classifier. The resulting classifier aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance, both in terms of accuracy and in terms of calibration of the estimated probabilities. The proposed classifier is obtained by superimposing a finite mixture model on the set of feature variables of a naive Bayes model. We present experimental results that compare the predictive performance on real datasets of the new classifier with the predictive performance of the NB classifier and the FM classifier. version:1
arxiv-1301-6710 | On Supervised Selection of Bayesian Networks | http://arxiv.org/abs/1301.6710 | id:1301.6710 author:Petri Kontkanen, Petri Myllymaki, Tomi Silander, Henry Tirri category:cs.LG stat.ML  published:2013-01-23 summary:Given a set of possible models (e.g., Bayesian network structures) and a data sample, in the unsupervised model selection problem the task is to choose the most accurate model with respect to the domain joint probability distribution. In contrast to this, in supervised model selection it is a priori known that the chosen model will be used in the future for prediction tasks involving more ``focused' predictive distributions. Although focused predictive distributions can be produced from the joint probability distribution by marginalization, in practice the best model in the unsupervised sense does not necessarily perform well in supervised domains. In particular, the standard marginal likelihood score is a criterion for the unsupervised task, and, although frequently used for supervised model selection also, does not perform well in such tasks. In this paper we study the performance of the marginal likelihood score empirically in supervised Bayesian network selection tasks by using a large number of publicly available classification data sets, and compare the results to those obtained by alternative model selection criteria, including empirical crossvalidation methods, an approximation of a supervised marginal likelihood measure, and a supervised version of Dawids prequential(predictive sequential) principle.The results demonstrate that the marginal likelihood score does NOT perform well FOR supervised model selection, WHILE the best results are obtained BY using Dawids prequential r napproach. version:1
arxiv-1301-6705 | Probabilistic Latent Semantic Analysis | http://arxiv.org/abs/1301.6705 | id:1301.6705 author:Thomas Hofmann category:cs.LG cs.IR stat.ML  published:2013-01-23 summary:Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments. version:1
arxiv-1301-6701 | Multi-objects association in perception of dynamical situation | http://arxiv.org/abs/1301.6701 | id:1301.6701 author:Dominique Gruyer, Veronique Berge-Cherfaoui category:cs.AI cs.CV  published:2013-01-23 summary:In current perception systems applied to the rebuilding of the environment for intelligent vehicles, the part reserved to object association for the tracking is increasingly significant. This allows firstly to follow the objects temporal evolution and secondly to increase the reliability of environment perception. We propose in this communication the development of a multi-objects association algorithm with ambiguity removal entering into the design of such a dynamic perception system for intelligent vehicles. This algorithm uses the belief theory and data modelling with fuzzy mathematics in order to be able to handle inaccurate as well as uncertain information due to imperfect sensors. These theories also allow the fusion of numerical as well as symbolic data. We develop in this article the problem of matching between known and perceived objects. This makes it possible to update a dynamic environment map for a vehicle. The belief theory will enable us to quantify the belief in the association of each perceived object with each known object. Conflicts can appear in the case of object appearance or disappearance, or in the case of a confused situation or bad perception. These conflicts are removed or solved using an assignment algorithm, giving a solution called the " best " and so ensuring the tracking of some objects present in our environment. version:1
arxiv-1301-6696 | Learning Bayesian Network Structure from Massive Datasets: The "Sparse Candidate" Algorithm | http://arxiv.org/abs/1301.6696 | id:1301.6696 author:Nir Friedman, Iftach Nachman, Dana Pe'er category:cs.LG cs.AI stat.ML  published:2013-01-23 summary:Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a statistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of instances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the parents of each variable to belong to a small subset of candidates. We then search for a network that satisfies these constraints. The learned network is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. version:1
arxiv-1301-6695 | Data Analysis with Bayesian Networks: A Bootstrap Approach | http://arxiv.org/abs/1301.6695 | id:1301.6695 author:Nir Friedman, Moises Goldszmidt, Abraham Wyner category:cs.LG cs.AI stat.ML  published:2013-01-23 summary:In recent years there has been significant progress in algorithms and methods for inducing Bayesian networks from data. However, in complex data analysis problems, we need to go beyond being satisfied with inducing networks with high scores. We need to provide confidence measures on features of these networks: Is the existence of an edge between two nodes warranted? Is the Markov blanket of a given node robust? Can we say something about the ordering of the variables? We should be able to address these questions, even when the amount of data is not enough to induce a high scoring network. In this paper we propose Efron's Bootstrap as a computationally efficient approach for answering these questions. In addition, we propose to use these confidence measures to induce better structures from the data, and to detect the presence of latent variables. version:1
arxiv-1301-6690 | Model-Based Bayesian Exploration | http://arxiv.org/abs/1301.6690 | id:1301.6690 author:Richard Dearden, Nir Friedman, David Andre category:cs.AI cs.LG  published:2013-01-23 summary:Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information - the expected improvement in future decision quality arising from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper we investigate ways of representing and reasoning about this uncertainty in algorithms where the system attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q-values based on these. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. version:1
arxiv-1301-6688 | Learning Polytrees | http://arxiv.org/abs/1301.6688 | id:1301.6688 author:Sanjoy Dasgupta category:cs.AI cs.LG  published:2013-01-23 summary:We consider the task of learning the maximum-likelihood polytree from data. Our first result is a performance guarantee establishing that the optimal branching (or Chow-Liu tree), which can be computed very easily, constitutes a good approximation to the best polytree. We then show that it is not possible to do very much better, since the learning problem is NP-hard even to approximately solve within some constant factor. version:1
arxiv-1301-6684 | Comparing Bayesian Network Classifiers | http://arxiv.org/abs/1301.6684 | id:1301.6684 author:Jie Cheng, Russell Greiner category:cs.LG cs.AI stat.ML  published:2013-01-23 summary:In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN augmented Naive-Bayes and general BNs, where the latter two are learned using two variants of a conditional-independence (CI) based BN-learning algorithm. Experimental results show the obtained classifiers, learned using the CI based algorithms, are competitive with (or superior to) the best known classifiers, based on both Bayesian networks and other formalisms; and that the computational time for learning and using these classifiers is relatively small. Moreover, these results also suggest a way to learn yet more effective classifiers; we demonstrate empirically that this new algorithm does work as expected. Collectively, these results argue that BN classifiers deserve more attention in machine learning and data mining communities. version:1
arxiv-1301-6683 | Discovering the Hidden Structure of Complex Dynamic Systems | http://arxiv.org/abs/1301.6683 | id:1301.6683 author:Xavier Boyen, Nir Friedman, Daphne Koller category:cs.AI cs.LG  published:2013-01-23 summary:Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no expert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial computational aspects of learning the structure of dynamic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the Structural Expectation Maximization (SEM) algorithm. The main computational cost of the SEM algorithm is the gathering of expected sufficient statistics. We propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden variable typically results in a violation of the Markov property. Thus, our algorithm searches for such violations in the data, and introduces hidden variables to explain them. We provide empirical results showing that the algorithm is able to learn the dynamics of complex systems in a computationally tractable way. version:1
arxiv-1301-6677 | Relative Loss Bounds for On-line Density Estimation with the Exponential Family of Distributions | http://arxiv.org/abs/1301.6677 | id:1301.6677 author:Katy S. Azoury, Manfred K. Warmuth category:cs.LG stat.ML  published:2013-01-23 summary:We consider on-line density estimation with a parameterized density from the exponential family. The on-line algorithm receives one example at a time and maintains a parameter that is essentially an average of the past examples. After receiving an example the algorithm incurs a loss which is the negative log-likelihood of the example w.r.t. the past parameter of the algorithm. An off-line algorithm can choose the best parameter based on all the examples. We prove bounds on the additional total loss of the on-line algorithm over the total loss of the off-line algorithm. These relative loss bounds hold for an arbitrary sequence of examples. The goal is to design algorithms with the best possible relative loss bounds. We use a certain divergence to derive and analyze the algorithms. This divergence is a relative entropy between two exponential distributions. version:1
arxiv-1301-6676 | Inferring Parameters and Structure of Latent Variable Models by Variational Bayes | http://arxiv.org/abs/1301.6676 | id:1301.6676 author:Hagai Attias category:cs.LG stat.ML  published:2013-01-23 summary:Current methods for learning graphical models with latent variables and a fixed structure estimate optimal values for the model parameters. Whereas this approach usually produces overfitting and suboptimal generalization performance, carrying out the Bayesian program of computing the full posterior distributions over the parameters remains a difficult problem. Moreover, learning the structure of models with latent variables, for which the Bayesian approach is crucial, is yet a harder problem. In this paper I present the Variational Bayes framework, which provides a solution to these problems. This approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner without resorting to sampling methods. Unlike in the Laplace approximation, these posteriors are generally non-Gaussian and no Hessian needs to be computed. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. I demonstrate that this algorithm can be applied to a large class of models in several domains, including unsupervised clustering and blind source separation. version:1
arxiv-1301-5491 | ChESS - Quick and Robust Detection of Chess-board Features | http://arxiv.org/abs/1301.5491 | id:1301.5491 author:Stuart Bennett, Joan Lasenby category:cs.CV  published:2013-01-23 summary:Localization of chess-board vertices is a common task in computer vision, underpinning many applications, but relatively little work focusses on designing a specific feature detector that is fast, accurate and robust. In this paper the `Chess-board Extraction by Subtraction and Summation' (ChESS) feature detector, designed to exclusively respond to chess-board vertices, is presented. The method proposed is robust against noise, poor lighting and poor contrast, requires no prior knowledge of the extent of the chess-board pattern, is computationally very efficient, and provides a strength measure of detected features. Such a detector has significant application both in the key field of camera calibration, as well as in Structured Light 3D reconstruction. Evidence is presented showing its robustness, accuracy, and efficiency in comparison to other commonly used detectors both under simulation and in experimental 3D reconstruction of flat plate and cylindrical objects version:1
arxiv-1301-5488 | Multi-class Generalized Binary Search for Active Inverse Reinforcement Learning | http://arxiv.org/abs/1301.5488 | id:1301.5488 author:Francisco Melo, Manuel Lopes category:cs.LG cs.AI stat.ML  published:2013-01-23 summary:This paper addresses the problem of learning a task from demonstration. We adopt the framework of inverse reinforcement learning, where tasks are represented in the form of a reward function. Our contribution is a novel active learning algorithm that enables the learning agent to query the expert for more informative demonstrations, thus leading to more sample-efficient learning. For this novel algorithm (Generalized Binary Search for Inverse Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample complexity and illustrate its applicability on several different tasks. To our knowledge, GBS-IRL is the first active IRL algorithm with provable sample complexity bounds. We also discuss our method in light of other existing methods in the literature and its general applicability in multi-class classification problems. Finally, motivated by recent work on learning from demonstration in robots, we also discuss how different forms of human feedback can be integrated in a transparent manner in our learning framework. version:1
arxiv-1301-5451 | Spread spectrum compressed sensing MRI using chirp radio frequency pulses | http://arxiv.org/abs/1301.5451 | id:1301.5451 author:Xiaobo Qu, Ying Chen, Xiaoxing Zhuang, Zhiyu Yan, Di Guo, Zhong Chen category:cs.CV math.OC physics.med-ph  published:2013-01-23 summary:Compressed sensing has shown great potential in reducing data acquisition time in magnetic resonance imaging (MRI). Recently, a spread spectrum compressed sensing MRI method modulates an image with a quadratic phase. It performs better than the conventional compressed sensing MRI with variable density sampling, since the coherence between the sensing and sparsity bases are reduced. However, spread spectrum in that method is implemented via a shim coil which limits its modulation intensity and is not convenient to operate. In this letter, we propose to apply chirp (linear frequency-swept) radio frequency pulses to easily control the spread spectrum. To accelerate the image reconstruction, an alternating direction algorithm is modified by exploiting the complex orthogonality of the quadratic phase encoding. Reconstruction on the acquired data demonstrates that more image features are preserved using the proposed approach than those of conventional CS-MRI. version:1
arxiv-1301-5332 | Online Learning with Pairwise Loss Functions | http://arxiv.org/abs/1301.5332 | id:1301.5332 author:Yuyang Wang, Roni Khardon, Dmitry Pechyony, Rosie Jones category:stat.ML cs.LG  published:2013-01-22 summary:Efficient online learning with pairwise loss functions is a crucial component in building large-scale learning system that maximizes the area under the Receiver Operator Characteristic (ROC) curve. In this paper we investigate the generalization performance of online learning algorithms with pairwise loss functions. We show that the existing proof techniques for generalization bounds of online algorithms with a univariate loss can not be directly applied to pairwise losses. In this paper, we derive the first result providing data-dependent bounds for the average risk of the sequence of hypotheses generated by an arbitrary online learner in terms of an easily computable statistic, and show how to extract a low risk hypothesis from the sequence. We demonstrate the generality of our results by applying it to two important problems in machine learning. First, we analyze two online algorithms for bipartite ranking; one being a natural extension of the perceptron algorithm and the other using online convex optimization. Secondly, we provide an analysis for the risk bound for an online algorithm for supervised metric learning. version:1
arxiv-1211-0632 | Stochastic ADMM for Nonsmooth Optimization | http://arxiv.org/abs/1211.0632 | id:1211.0632 author:Hua Ouyang, Niao He, Alexander Gray category:cs.LG math.OC stat.ML  published:2012-11-03 summary:We present a stochastic setting for optimization problems with nonsmooth convex separable objective functions over linear equality constraints. To solve such problems, we propose a stochastic Alternating Direction Method of Multipliers (ADMM) algorithm. Our algorithm applies to a more general class of nonsmooth convex functions that does not necessarily have a closed-form solution by minimizing the augmented function directly. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic functions: $O(1/\sqrt{t})$ for convex functions and $O(\log t/t)$ for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM algorithm, for the first time, in terms of both the objective value and the feasibility violation. version:2
arxiv-1301-5112 | Active Learning on Trees and Graphs | http://arxiv.org/abs/1301.5112 | id:1301.5112 author:Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella category:cs.LG stat.ML  published:2013-01-22 summary:We investigate the problem of active learning on a given tree whose nodes are assigned binary labels in an adversarial way. Inspired by recent results by Guillory and Bilmes, we characterize (up to constant factors) the optimal placement of queries so to minimize the mistakes made on the non-queried nodes. Our query selection algorithm is extremely efficient, and the optimal number of mistakes on the non-queried nodes is achieved by a simple and efficient mincut classifier. Through a simple modification of the query selection algorithm we also show optimality (up to constant factors) with respect to the trade-off between number of queries and number of mistakes on non-queried nodes. By using spanning trees, our algorithms can be efficiently applied to general graphs, although the problem of finding optimal and efficient active learning algorithms for general graphs remains open. Towards this end, we provide a lower bound on the number of mistakes made on arbitrary graphs by any active learning algorithm using a number of queries which is up to a constant fraction of the graph size. version:1
arxiv-1301-5088 | Piecewise Linear Multilayer Perceptrons and Dropout | http://arxiv.org/abs/1301.5088 | id:1301.5088 author:Ian J. Goodfellow category:stat.ML cs.LG  published:2013-01-22 summary:We propose a new type of hidden layer for a multilayer perceptron, and demonstrate that it obtains the best reported performance for an MLP on the MNIST dataset. version:1
arxiv-1301-4944 | Evaluation of a Supervised Learning Approach for Stock Market Operations | http://arxiv.org/abs/1301.4944 | id:1301.4944 author:Marcelo S. Lauretto, Barbara B. C. Silva, Pablo M. Andrade category:stat.ML cs.LG stat.AP  published:2013-01-21 summary:Data mining methods have been widely applied in financial markets, with the purpose of providing suitable tools for prices forecasting and automatic trading. Particularly, learning methods aim to identify patterns in time series and, based on such patterns, to recommend buy/sell operations. The objective of this work is to evaluate the performance of Random Forests, a supervised learning method based on ensembles of decision trees, for decision support in stock markets. Preliminary results indicate good rates of successful operations and good rates of return per operation, providing a strong motivation for further research in this topic. version:1
arxiv-1301-4917 | Dirichlet draws are sparse with high probability | http://arxiv.org/abs/1301.4917 | id:1301.4917 author:Matus Telgarsky category:cs.LG math.PR stat.ML  published:2013-01-21 summary:This note provides an elementary proof of the folklore fact that draws from a Dirichlet distribution (with parameters less than 1) are typically sparse (most coordinates are small). version:1
arxiv-1301-4862 | Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots | http://arxiv.org/abs/1301.4862 | id:1301.4862 author:Adrien Baranes, Pierre-Yves Oudeyer category:cs.LG cs.AI cs.CV cs.NE cs.RO  published:2013-01-21 summary:We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy pa- rameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot. version:1
arxiv-1208-2112 | Inverse Reinforcement Learning with Gaussian Process | http://arxiv.org/abs/1208.2112 | id:1208.2112 author:Qifeng Qiao, Peter A. Beling category:cs.LG  published:2012-08-10 summary:We present new algorithms for inverse reinforcement learning (IRL, or inverse optimal control) in convex optimization settings. We argue that finite-space IRL can be posed as a convex quadratic program under a Bayesian inference framework with the objective of maximum a posterior estimation. To deal with problems in large or even infinite state space, we propose a Gaussian process model and use preference graphs to represent observations of decision trajectories. Our method is distinguished from other approaches to IRL in that it makes no assumptions about the form of the reward function and yet it retains the promise of computationally manageable implementations for potential real-world applications. In comparison with an establish algorithm on small-scale numerical problems, our method demonstrated better accuracy in apprenticeship learning and a more robust dependence on the number of observations. version:2
arxiv-1301-4753 | Pattern Matching for Self- Tuning of MapReduce Jobs | http://arxiv.org/abs/1301.4753 | id:1301.4753 author:Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya category:cs.DC cs.AI cs.LG  published:2013-01-21 summary:In this paper, we study CPU utilization time patterns of several MapReduce applications. After extracting running patterns of several applications, they are saved in a reference database to be later used to tweak system parameters to efficiently execute unknown applications in future. To achieve this goal, CPU utilization patterns of new applications are compared with the already known ones in the reference database to find/predict their most probable execution patterns. Because of different patterns lengths, the Dynamic Time Warping (DTW) is utilized for such comparison; a correlation analysis is then applied to DTWs outcomes to produce feasible similarity patterns. Three real applications (WordCount, Exim Mainlog parsing and Terasort) are used to evaluate our hypothesis in tweaking system parameters in executing similar applications. Results were very promising and showed effectiveness of our approach on pseudo-distributed MapReduce platforms. version:1
arxiv-1301-3614 | Joint Space Neural Probabilistic Language Model for Statistical Machine Translation | http://arxiv.org/abs/1301.3614 | id:1301.3614 author:Tsuyoshi Okita category:cs.CL  published:2013-01-16 summary:A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM. version:2
arxiv-1301-4662 | Recurrent Neural Network Method in Arabic Words Recognition System | http://arxiv.org/abs/1301.4662 | id:1301.4662 author:Yusuf Perwej category:cs.NE  published:2013-01-20 summary:The recognition of unconstrained handwriting continues to be a difficult task for computers despite active research for several decades. This is because handwritten text offers great challenges such as character and word segmentation, character recognition, variation between handwriting styles, different character size and no font constraints as well as the background clarity. In this paper primarily discussed Online Handwriting Recognition methods for Arabic words which being often used among then across the Middle East and North Africa people. Because of the characteristic of the whole body of the Arabic words, namely connectivity between the characters, thereby the segmentation of An Arabic word is very difficult. We introduced a recurrent neural network to online handwriting Arabic word recognition. The key innovation is a recently produce recurrent neural networks objective function known as connectionist temporal classification. The system consists of an advanced recurrent neural network with an output layer designed for sequence labeling, partially combined with a probabilistic language model. Experimental results show that unconstrained Arabic words achieve recognition rates about 79%, which is significantly higher than the about 70% using a previously developed hidden markov model based recognition system. version:1
arxiv-1301-3753 | Switched linear encoding with rectified linear autoencoders | http://arxiv.org/abs/1301.3753 | id:1301.3753 author:Leif Johnson, Craig Corcoran category:cs.LG  published:2013-01-16 summary:Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions. version:2
arxiv-1301-4558 | Lip Localization and Viseme Classification for Visual Speech Recognition | http://arxiv.org/abs/1301.4558 | id:1301.4558 author:Salah Werda, Walid Mahdi, Abdelmajid Ben Hamadou category:cs.CV  published:2013-01-19 summary:The need for an automatic lip-reading system is ever increasing. Infact, today, extraction and reliable analysis of facial movements make up an important part in many multimedia systems such as videoconference, low communication systems, lip-reading systems. In addition, visual information is imperative among people with special needs. We can imagine, for example, a dependent person ordering a machine with an easy lip movement or by a simple syllable pronunciation. Moreover, people with hearing problems compensate for their special needs by lip-reading as well as listening to the person with whome they are talking. version:1
arxiv-1207-4417 | Penalty Constraints and Kernelization of M-Estimation Based Fuzzy C-Means | http://arxiv.org/abs/1207.4417 | id:1207.4417 author:Jingwei Liu, Meizhi Xu category:cs.CV stat.CO  published:2012-07-18 summary:A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm is proposed with iterative reweighted least squares (IRLS) algorithm, and penalty constraint and kernelization extensions of MFCM algorithms are also developed. Introducing penalty information to the object functions of MFCM algorithms, the spatially constrained fuzzy C-means (SFCM) is extended to penalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean distance with kernel method, the MFCM and pMFCM algorithms are extended to kernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms. The performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in three tasks: pattern recognition on 10 standard data sets from UCI Machine Learning databases, noise image segmentation performances on a synthetic image, a magnetic resonance brain image (MRI), and image segmentation of a standard images from Berkeley Segmentation Dataset and Benchmark. The experimental results demonstrate the effectiveness of our proposed algorithms in pattern recognition and image segmentation. version:2
arxiv-1205-5075 | Efficient Sparse Group Feature Selection via Nonconvex Optimization | http://arxiv.org/abs/1205.5075 | id:1205.5075 author:Shuo Xiang, Xiaotong Shen, Jieping Ye category:cs.LG stat.ML  published:2012-05-23 summary:Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) statistically, we introduce a nonconvex sparse group feature selection model which can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved; (2) computationally, we propose an efficient algorithm that is applicable to large-scale problems. Numerical results suggest that the proposed nonconvex method compares favorably against its competitors on synthetic data and real-world applications, thus achieving desired goal of delivering high performance. version:2
arxiv-1301-4432 | Language learning from positive evidence, reconsidered: A simplicity-based approach | http://arxiv.org/abs/1301.4432 | id:1301.4432 author:Anne S. Hsu, Nick Chater, Paul M. B. Vitányi category:cs.CL  published:2013-01-18 summary:Children learn their native language by exposure to their linguistic and communicative environment, but apparently without requiring that their mistakes are corrected. Such learning from positive evidence has been viewed as raising logical problems for language acquisition. In particular, without correction, how is the child to recover from conjecturing an over-general grammar, which will be consistent with any sentence that the child hears? There have been many proposals concerning how this logical problem can be dissolved. Here, we review recent formal results showing that the learner has sufficient data to learn successfully from positive evidence, if it favours the simplest encoding of the linguistic input. Results include the ability to learn a linguistic prediction, grammaticality judgements, language production, and form-meaning mappings. The simplicity approach can also be scaled-down to analyse the ability to learn a specific linguistic constructions, and is amenable to empirical test as a framework for describing human language acquisition. version:1
arxiv-1301-4377 | Multiple models of Bayesian networks applied to offline recognition of Arabic handwritten city names | http://arxiv.org/abs/1301.4377 | id:1301.4377 author:Mohamed Ali Mahjoub, Nabil Ghanmy, Khlifia jayech, Ikram Miled category:cs.CV  published:2013-01-18 summary:In this paper we address the problem of offline Arabic handwriting word recognition. Off-line recognition of handwritten words is a difficult task due to the high variability and uncertainty of human writing. The majority of the recent systems are constrained by the size of the lexicon to deal with and the number of writers. In this paper, we propose an approach for multi-writers Arabic handwritten words recognition using multiple Bayesian networks. First, we cut the image in several blocks. For each block, we compute a vector of descriptors. Then, we use K-means to cluster the low-level features including Zernik and Hu moments. Finally, we apply four variants of Bayesian networks classifiers (Na\"ive Bayes, Tree Augmented Na\"ive Bayes (TAN), Forest Augmented Na\"ive Bayes (FAN) and DBN (dynamic bayesian network) to classify the whole image of tunisian city name. The results demonstrate FAN and DBN outperform good recognition rates version:1
arxiv-1112-5505 | A Study on Using Uncertain Time Series Matching Algorithms in MapReduce Applications | http://arxiv.org/abs/1112.5505 | id:1112.5505 author:Nikzad Babaii Rizvandi, Javid Taheri, Albert Y. Zomaya, Reza Moraveji category:cs.DC cs.AI cs.LG cs.PF  published:2011-12-23 summary:In this paper, we study CPU utilization time patterns of several Map-Reduce applications. After extracting running patterns of several applications, the patterns with their statistical information are saved in a reference database to be later used to tweak system parameters to efficiently execute unknown applications in future. To achieve this goal, CPU utilization patterns of new applications along with its statistical information are compared with the already known ones in the reference database to find/predict their most probable execution patterns. Because of different patterns lengths, the Dynamic Time Warping (DTW) is utilized for such comparison; a statistical analysis is then applied to DTWs' outcomes to select the most suitable candidates. Moreover, under a hypothesis, another algorithm is proposed to classify applications under similar CPU utilization patterns. Three widely used text processing applications (WordCount, Distributed Grep, and Terasort) and another application (Exim Mainlog parsing) are used to evaluate our hypothesis in tweaking system parameters in executing similar applications. Results were very promising and showed effectiveness of our approach on 5-node Map-Reduce platform version:5
arxiv-1111-2085 | Ag-dependent (in silico) approach implies a deterministic kinetics for homeostatic memory cell turnover | http://arxiv.org/abs/1111.2085 | id:1111.2085 author:Alexandre de Castro category:q-bio.CB cs.NE  published:2011-11-09 summary:Verhulst-like mathematical modeling has been used to investigate several complex biological issues, such as immune memory equilibrium and cell-mediated immunity in mammals. The regulation mechanisms of both these processes are still not sufficiently understood. In a recent paper, Choo et al. [J. Immunol., v. 185, pp. 3436-44, 2010], used an Ag-independent approach to quantitatively analyze memory cell turnover from some empirical data, and concluded that immune homeostasis behaves stochastically, rather than deterministically. In the paper here presented, we use an in silico Ag-dependent approach to simulate the process of antigenic mutation and study its implications for memory dynamics. Our results have suggested a deterministic kinetics for homeostatic equilibrium, what contradicts the Choo et al. findings. Accordingly, our calculations are an indication that a more extensive empirical protocol for studying the homeostatic turnover should be considered. version:3
arxiv-1204-6703 | A Spectral Algorithm for Latent Dirichlet Allocation | http://arxiv.org/abs/1204.6703 | id:1204.6703 author:Animashree Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade, Yi-Kai Liu category:cs.LG stat.ML  published:2012-04-30 summary:The problem of topic modeling can be seen as a generalization of the clustering problem, in that it posits that observations are generated due to multiple latent factors (e.g., the words in each document are generated as a mixture of several active topics, as opposed to just one). This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic probability vectors (the distributions over words for each topic), when only the words are observed and the corresponding topics are hidden. We provide a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of mixture models, including the popular latent Dirichlet allocation (LDA) model. For LDA, the procedure correctly recovers both the topic probability vectors and the prior over the topics, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, termed Excess Correlation Analysis (ECA), is based on a spectral decomposition of low order moments (third and fourth order) via two singular value decompositions (SVDs). Moreover, the algorithm is scalable since the SVD operations are carried out on $k\times k$ matrices, where $k$ is the number of latent factors (e.g. the number of topics), rather than in the $d$-dimensional observed space (typically $d \gg k$). version:4
arxiv-1301-4194 | Financial Portfolio Optimization: Computationally guided agents to investigate, analyse and invest!? | http://arxiv.org/abs/1301.4194 | id:1301.4194 author:Ankit Dangi category:q-fin.PM cs.CE cs.NE q-fin.CP stat.ML  published:2013-01-17 summary:Financial portfolio optimization is a widely studied problem in mathematics, statistics, financial and computational literature. It adheres to determining an optimal combination of weights associated with financial assets held in a portfolio. In practice, it faces challenges by virtue of varying math. formulations, parameters, business constraints and complex financial instruments. Empirical nature of data is no longer one-sided; thereby reflecting upside and downside trends with repeated yet unidentifiable cyclic behaviours potentially caused due to high frequency volatile movements in asset trades. Portfolio optimization under such circumstances is theoretically and computationally challenging. This work presents a novel mechanism to reach an optimal solution by encoding a variety of optimal solutions in a solution bank to guide the search process for the global investment objective formulation. It conceptualizes the role of individual solver agents that contribute optimal solutions to a bank of solutions, a super-agent solver that learns from the solution bank, and, thus reflects a knowledge-based computationally guided agents approach to investigate, analyse and reach to optimal solution for informed investment decisions. Conceptual understanding of classes of solver agents that represent varying problem formulations and, mathematically oriented deterministic solvers along with stochastic-search driven evolutionary and swarm-intelligence based techniques for optimal weights are discussed. Algorithmic implementation is presented by an enhanced neighbourhood generation mechanism in Simulated Annealing algorithm. A framework for inclusion of heuristic knowledge and human expertise from financial literature related to investment decision making process is reflected via introduction of controlled perturbation strategies using a decision matrix for neighbourhood generation. version:1
arxiv-1301-4171 | Affinity Weighted Embedding | http://arxiv.org/abs/1301.4171 | id:1301.4171 author:Jason Weston, Ron Weiss, Hector Yee category:cs.IR cs.LG stat.ML  published:2013-01-17 summary:Supervised (linear) embedding models like Wsabie and PSI have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration's features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results. version:1
arxiv-1301-4157 | On the Product Rule for Classification Problems | http://arxiv.org/abs/1301.4157 | id:1301.4157 author:Marcelo Cicconet category:cs.LG cs.CV stat.ML  published:2013-01-17 summary:We discuss theoretical aspects of the product rule for classification problems in supervised machine learning for the case of combining classifiers. We show that (1) the product rule arises from the MAP classifier supposing equivalent priors and conditional independence given a class; (2) under some conditions, the product rule is equivalent to minimizing the sum of the squared distances to the respective centers of the classes related with different features, such distances being weighted by the spread of the classes; (3) observing some hypothesis, the product rule is equivalent to concatenating the vectors of features. version:1
arxiv-1301-4144 | Non-parametric Bayesian modelling of digital gene expression data | http://arxiv.org/abs/1301.4144 | id:1301.4144 author:Dimitrios V. Vavoulis, Julian Gough category:q-bio.QM q-bio.GN stat.AP stat.ML  published:2013-01-17 summary:Next-generation sequencing technologies provide a revolutionary tool for generating gene expression data. Starting with a fixed RNA sample, they construct a library of millions of differentially abundant short sequence tags or "reads", which constitute a fundamentally discrete measure of the level of gene expression. A common limitation in experiments using these technologies is the low number or even absence of biological replicates, which complicates the statistical analysis of digital gene expression data. Analysis of this type of data has often been based on modified tests originally devised for analysing microarrays; both these and even de novo methods for the analysis of RNA-seq data are plagued by the common problem of low replication. We propose a novel, non-parametric Bayesian approach for the analysis of digital gene expression data. We begin with a hierarchical model for modelling over-dispersed count data and a blocked Gibbs sampling algorithm for inferring the posterior distribution of model parameters conditional on these counts. The algorithm compensates for the problem of low numbers of biological replicates by clustering together genes with tag counts that are likely sampled from a common distribution and using this augmented sample for estimating the parameters of this distribution. The number of clusters is not decided a priori, but it is inferred along with the remaining model parameters. We demonstrate the ability of this approach to model biological data with high fidelity by applying the algorithm on a public dataset obtained from cancerous and non-cancerous neural tissues. version:1
arxiv-1210-0805 | Robust PCA and subspace tracking from incomplete observations using L0-surrogates | http://arxiv.org/abs/1210.0805 | id:1210.0805 author:Clemens Hage, Martin Kleinsteuber category:stat.ML  published:2012-10-02 summary:Many applications in data analysis rely on the decomposition of a data matrix into a low-rank and a sparse component. Existing methods that tackle this task use the nuclear norm and L1-cost functions as convex relaxations of the rank constraint and the sparsity measure, respectively, or employ thresholding techniques. We propose a method that allows for reconstructing and tracking a subspace of upper-bounded dimension from incomplete and corrupted observations. It does not require any a priori information about the number of outliers. The core of our algorithm is an intrinsic Conjugate Gradient method on the set of orthogonal projection matrices, the so-called Grassmannian. Non-convex sparsity measures are used for outlier detection, which leads to improved performance in terms of robustly recovering and tracking the low-rank matrix. In particular, our approach can cope with more outliers and with an underlying matrix of higher rank than other state-of-the-art methods. version:2
arxiv-1301-4096 | Evolutionary Algorithms and Dynamic Programming | http://arxiv.org/abs/1301.4096 | id:1301.4096 author:Benjamin Doerr, Anton Eremeev, Frank Neumann, Madeleine Theile, Christian Thyssen category:cs.NE cs.DS  published:2013-01-17 summary:Recently, it has been proven that evolutionary algorithms produce good results for a wide range of combinatorial optimization problems. Some of the considered problems are tackled by evolutionary algorithms that use a representation which enables them to construct solutions in a dynamic programming fashion. We take a general approach and relate the construction of such algorithms to the development of algorithms using dynamic programming techniques. Thereby, we give general guidelines on how to develop evolutionary algorithms that have the additional ability of carrying out dynamic programming steps. Finally, we show that for a wide class of the so-called DP-benevolent problems (which are known to admit FPTAS) there exists a fully polynomial-time randomized approximation scheme based on an evolutionary algorithm. version:1
arxiv-1301-0534 | Follow the Leader If You Can, Hedge If You Must | http://arxiv.org/abs/1301.0534 | id:1301.0534 author:Steven de Rooij, Tim van Erven, Peter D. Grünwald, Wouter M. Koolen category:cs.LG stat.ML  published:2013-01-03 summary:Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that guarantees constant regret in the stochastic setting, but has terrible performance for worst-case data. Other hedging strategies have better worst-case guarantees but may perform much worse than FTL if the data are not maximally adversarial. We introduce the FlipFlop algorithm, which is the first method that provably combines the best of both worlds. As part of our construction, we develop AdaHedge, which is a new way of dynamically tuning the learning rate in Hedge without using the doubling trick. AdaHedge refines a method by Cesa-Bianchi, Mansour and Stoltz (2007), yielding slightly improved worst-case guarantees. By interleaving AdaHedge and FTL, the FlipFlop algorithm achieves regret within a constant factor of the FTL regret, without sacrificing AdaHedge's worst-case guarantees. AdaHedge and FlipFlop do not need to know the range of the losses in advance; moreover, unlike earlier methods, both have the intuitive property that the issued weights are invariant under rescaling and translation of the losses. The losses are also allowed to be negative, in which case they may be interpreted as gains. version:2
