arxiv-1606-01161 | Exploiting Multi-typed Treebanks for Parsing with Deep Multi-task Learning | http://arxiv.org/abs/1606.01161 | id:1606.01161 author:Jiang Guo, Wanxiang Che, Haifeng Wang, Ting Liu category:cs.CL  published:2016-06-03 summary:Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain syntactic knowledge that is potential to benefit each other. This paper presents an universal framework for exploiting these multi-typed treebanks to improve parsing with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Multiple treebanks are trained jointly and interacted with multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models. version:1
arxiv-1606-01160 | Robust Ensemble Clustering Using Probability Trajectories | http://arxiv.org/abs/1606.01160 | id:1606.01160 author:Dong Huang, Jian-Huang Lai, Chang-Dong Wang category:stat.ML cs.LG  published:2016-06-03 summary:Although many successful ensemble clustering approaches have been developed in recent years, there are still two limitations to most of the existing approaches. First, they mostly overlook the issue of uncertain links, which may mislead the overall consensus process. Second, they generally lack the ability to incorporate global information to refine the local links. To address these two limitations, in this paper, we propose a novel ensemble clustering approach based on sparse graph representation and probability trajectory analysis. In particular, we present the elite neighbor selection strategy to identify the uncertain links by locally adaptive thresholds and build a sparse graph with a small number of probably reliable links. We argue that a small number of probably reliable links can lead to significantly better consensus results than using all graph links regardless of their reliability. The random walk process driven by a new transition probability matrix is utilized to explore the global information in the graph. We derive a novel and dense similarity measure from the sparse graph by analyzing the probability trajectories of the random walkers, based on which two consensus functions are further proposed. Experimental results on multiple real-world datasets demonstrate the effectiveness and efficiency of our approach. version:1
arxiv-1602-06410 | Semidefinite Programs for Exact Recovery of a Hidden Community | http://arxiv.org/abs/1602.06410 | id:1602.06410 author:Bruce Hajek, Yihong Wu, Jiaming Xu category:stat.ML cs.IT cs.SI math.IT math.ST stat.TH  published:2016-02-20 summary:We study a semidefinite programming (SDP) relaxation of the maximum likelihood estimation for exactly recovering a hidden community of cardinality $K$ from an $n \times n$ symmetric data matrix $A$, where for distinct indices $i,j$, $A_{ij} \sim P$ if $i, j$ are both in the community and $A_{ij} \sim Q$ otherwise, for two known probability distributions $P$ and $Q$. We identify a sufficient condition and a necessary condition for the success of SDP for the general model. For both the Bernoulli case ($P={{\rm Bern}}(p)$ and $Q={{\rm Bern}}(q)$ with $p>q$) and the Gaussian case ($P=\mathcal{N}(\mu,1)$ and $Q=\mathcal{N}(0,1)$ with $\mu>0$), which correspond to the problem of planted dense subgraph recovery and submatrix localization respectively, the general results lead to the following findings: (1) If $K=\omega( n /\log n)$, SDP attains the information-theoretic recovery limits with sharp constants; (2) If $K=\Theta(n/\log n)$, SDP is order-wise optimal, but strictly suboptimal by a constant factor; (3) If $K=o(n/\log n)$ and $K \to \infty$, SDP is order-wise suboptimal. The same critical scaling for $K$ is found to hold, up to constant factors, for the performance of SDP on the stochastic block model of $n$ vertices partitioned into multiple communities of equal size $K$. A key ingredient in the proof of the necessary condition is a construction of a primal feasible solution based on random perturbation of the true cluster matrix. version:2
arxiv-1606-01151 | Privacy Protection for Natural Language: Neural Generative Models for Synthetic Text Data | http://arxiv.org/abs/1606.01151 | id:1606.01151 author:Alexander G. Ororbia II, Fridolin Linder, Joshua Snoke category:cs.CL  published:2016-06-03 summary:Redaction has been the most common approach to protecting text data, but synthetic data presents a potentially more reliable alternative for disclosure control. By producing new sample values which closely follow the original sample distribution but do not contain real values, privacy protection can be improved while utility from the data for specific purposes is maintained. We extend the synthetic data approach to natural language by developing a neural generative model for such data. We find that the synthetic models outperform simple redaction on both comparative risk and utility. version:1
arxiv-1606-01141 | On Valid Optimal Assignment Kernels and Applications to Graph Classification | http://arxiv.org/abs/1606.01141 | id:1606.01141 author:Nils M. Kriege, Pierre-Louis Giscard, Richard C. Wilson category:cs.LG stat.ML  published:2016-06-03 summary:The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel. version:1
arxiv-1511-06709 | Improving Neural Machine Translation Models with Monolingual Data | http://arxiv.org/abs/1511.06709 | id:1511.06709 author:Rico Sennrich, Barry Haddow, Alexandra Birch category:cs.CL  published:2015-11-20 summary:Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German. version:4
arxiv-1606-01128 | Difference of Convex Functions Programming Applied to Control with Expert Data | http://arxiv.org/abs/1606.01128 | id:1606.01128 author:Bilal Piot, Matthieu Geist, Olivier Pietquin category:math.OC cs.LG stat.ML  published:2016-06-03 summary:This paper shows how Difference of Convex functions (DC) programming can improve the performance of some Reinforcement Learning (RL) algorithms using expert data and Learning from Demonstrations (LfD) algorithms. This is principally due to the fact that the norm of the Optimal Bellman Residual (OBR), which is one of the main component of the algorithms considered, is DC. The slight performance improvement is shown on two algorithms, namely Reward-regularized Classification for Apprenticeship Learning (RCAL) and Reinforcement Learning with Expert Demonstrations (RLED), through experiments on generic Markov Decision Processes (MDP) called Garnets. version:1
arxiv-1603-09260 | Degrees of Freedom in Deep Neural Networks | http://arxiv.org/abs/1603.09260 | id:1603.09260 author:Tianxiang Gao, Vladimir Jojic category:cs.LG stat.ML  published:2016-03-30 summary:In this paper, we explore degrees of freedom in deep sigmoidal neural networks. We show that the degrees of freedom in these models is related to the expected optimism, which is the expected difference between test error and training error. We provide an efficient Monte-Carlo method to estimate the degrees of freedom for multi-class classification methods. We show degrees of freedom are lower than the parameter count in a simple XOR network. We extend these results to neural nets trained on synthetic and real data, and investigate impact of network's architecture and different regularization choices. The degrees of freedom in deep networks are dramatically smaller than the number of parameters, in some real datasets several orders of magnitude. Further, we observe that for fixed number of parameters, deeper networks have less degrees of freedom exhibiting a regularization-by-depth. version:2
arxiv-1606-01111 | Property-driven State-Space Coarsening for Continuous Time Markov Chains | http://arxiv.org/abs/1606.01111 | id:1606.01111 author:Michalis Michaelides, Dimitrios Milios, Jane Hillston, Guido Sanguinetti category:cs.SY stat.ML  published:2016-06-03 summary:Dynamical systems with large state-spaces are often expensive to thoroughly explore experimentally. Coarse-graining methods aim to define simpler systems which are more amenable to analysis and exploration; most current methods, however, focus on a priori state aggregation based on similarities in transition rates, which is not necessarily reflected in similar behaviours at the level of trajectories. We propose a way to coarsen the state-space of a system which optimally preserves the satisfaction of a set of logical specifications about the system's trajectories. Our approach is based on Gaussian Process emulation and Multi-Dimensional Scaling, a dimensionality reduction technique which optimally preserves distances in non-Euclidean spaces. We show how to obtain low-dimensional visualisations of the system's state-space from the perspective of properties' satisfaction, and how to define macro-states which behave coherently with respect to the specifications. Our approach is illustrated on a non-trivial running example, showing promising performance and high computational efficiency. version:1
arxiv-1606-01102 | Acquisition of Visual Features Through Probabilistic Spike-Timing-Dependent Plasticity | http://arxiv.org/abs/1606.01102 | id:1606.01102 author:Amirhossein Tavanaei, Timothee Masquelier, Anthony S Maida category:cs.NE  published:2016-06-03 summary:This paper explores modifications to a feedforward five-layer spiking convolutional network (SCN) of the ventral visual stream [Masquelier, T., Thorpe, S., Unsupervised learning of visual features through spike timing dependent plasticity. PLoS Computational Biology, 3(2), 247-257]. The original model showed that a spike-timing-dependent plasticity (STDP) learning algorithm embedded in an appropriately selected SCN could perform unsupervised feature discovery. The discovered features where interpretable and could effectively be used to perform rapid binary decisions in a classifier. In order to study the robustness of the previous results, the present research examines the effects of modifying some of the components of the original model. For improved biological realism, we replace the original non-leaky integrate-and-fire neurons with Izhikevich-like neurons. We also replace the original STDP rule with a novel rule that has a probabilistic interpretation. The probabilistic STDP slightly but significantly improves the performance for both types of model neurons. Use of the Izhikevich-like neuron was not found to improve performance although performance was still comparable to the IF neuron. This shows that the model is robust enough to handle more biologically realistic neurons. We also conclude that the underlying reasons for stable performance in the model are preserved despite the overt changes to the explicit components of the model. version:1
arxiv-1606-01100 | Learning under Distributed Weak Supervision | http://arxiv.org/abs/1606.01100 | id:1606.01100 author:Martin Rajchl, Matthew C. H. Lee, Franklin Schrans, Alice Davidson, Jonathan Passerat-Palmbach, Giacomo Tarroni, Amir Alansary, Ozan Oktay, Bernhard Kainz, Daniel Rueckert category:cs.CV  published:2016-06-03 summary:The availability of training data for supervision is a frequently encountered bottleneck of medical image analysis methods. While typically established by a clinical expert rater, the increase in acquired imaging data renders traditional pixel-wise segmentations less feasible. In this paper, we examine the use of a crowdsourcing platform for the distribution of super-pixel weak annotation tasks and collect such annotations from a crowd of non-expert raters. The crowd annotations are subsequently used for training a fully convolutional neural network to address the problem of fetal brain segmentation in T2-weighted MR images. Using this approach we report encouraging results compared to highly targeted, fully supervised methods and potentially address a frequent problem impeding image analysis research. version:1
arxiv-1602-09013 | Beyond CCA: Moment Matching for Multi-View Models | http://arxiv.org/abs/1602.09013 | id:1602.09013 author:Anastasia Podosinnikova, Francis Bach, Simon Lacoste-Julien category:stat.ML cs.LG  published:2016-02-29 summary:We introduce three novel semi-parametric extensions of probabilistic canonical correlation analysis with identifiability guarantees. We consider moment matching techniques for estimation in these models. For that, by drawing explicit links between the new models and a discrete version of independent component analysis (DICA), we first extend the DICA cumulant tensors to the new discrete version of CCA. By further using a close connection with independent component analysis, we introduce generalized covariance matrices, which can replace the cumulant tensors in the moment matching framework, and, therefore, improve sample complexity and simplify derivations and algorithms significantly. As the tensor power method or orthogonal joint diagonalization are not applicable in the new setting, we use non-orthogonal joint diagonalization techniques for matching the cumulants. We demonstrate performance of the proposed models and estimation techniques on experiments with both synthetic and real datasets. version:2
arxiv-1605-00482 | Compositional Sentence Representation from Character within Large Context Text | http://arxiv.org/abs/1605.00482 | id:1605.00482 author:Geonmin Kim, Hwaran Lee, Jisu Choi, Soo-young Lee category:cs.CL  published:2016-05-02 summary:This paper describes a Hierarchical Composition Recurrent Network (HCRN) consisting of a 3-level hierarchy of compositional models: character, word and sentence. This model is designed to overcome two problems of representing a sentence on the basis of a constituent word sequence. The first is a data-sparsity problem in word embedding, and the other is a no usage of inter-sentence dependency. In the HCRN, word representations are built from characters, thus resolving the data-sparsity problem, and inter-sentence dependency is embedded into sentence representation at the level of sentence composition. We adopt a hierarchy-wise learning scheme in order to alleviate the optimization difficulties of learning deep hierarchical recurrent network in end-to-end fashion. The HCRN was quantitatively and qualitatively evaluated on a dialogue act classification task. Especially, sentence representations with an inter-sentence dependency are able to capture both implicit and explicit semantics of sentence, significantly improving performance. In the end, the HCRN achieved state-of-the-art performance with a test error rate of 22.7% for dialogue act classification on the SWBD-DAMSL database. version:3
arxiv-1510-02125 | Resolving References to Objects in Photographs using the Words-As-Classifiers Model | http://arxiv.org/abs/1510.02125 | id:1510.02125 author:David Schlangen, Sina Zarriess, Casey Kennington category:cs.CL  published:2015-10-07 summary:A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The "words as classifiers" model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small number of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of types and for which referring expressions are available. Using a pre-trained convolutional neural network to extract image features, and augmenting these with in-picture positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible. version:3
arxiv-1602-07905 | Thompson Sampling is Asymptotically Optimal in General Environments | http://arxiv.org/abs/1602.07905 | id:1602.07905 author:Jan Leike, Tor Lattimore, Laurent Orseau, Marcus Hutter category:cs.LG cs.AI stat.ML  published:2016-02-25 summary:We discuss a variant of Thompson sampling for nonparametric reinforcement learning in a countable classes of general stochastic environments. These environments can be non-Markov, non-ergodic, and partially observable. We show that Thompson sampling learns the environment class in the sense that (1) asymptotically its value converges to the optimal value in mean and (2) given a recoverability assumption regret is sublinear. version:2
arxiv-1606-01042 | Machine Learning for E-mail Spam Filtering: Review,Techniques and Trends | http://arxiv.org/abs/1606.01042 | id:1606.01042 author:Alexy Bhowmick, Shyamanta M. Hazarika category:cs.LG cs.CR  published:2016-06-03 summary:We present a comprehensive review of the most effective content-based e-mail spam filtering techniques. We focus primarily on Machine Learning-based spam filters and their variants, and report on a broad review ranging from surveying the relevant ideas, efforts, effectiveness, and the current progress. The initial exposition of the background examines the basics of e-mail spam filtering, the evolving nature of spam, spammers playing cat-and-mouse with e-mail service providers (ESPs), and the Machine Learning front in fighting spam. We conclude by measuring the impact of Machine Learning-based filters and explore the promising offshoots of latest developments. version:1
arxiv-1602-07576 | Group Equivariant Convolutional Networks | http://arxiv.org/abs/1602.07576 | id:1602.07576 author:Taco S. Cohen, Max Welling category:cs.LG stat.ML  published:2016-02-24 summary:We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. version:3
arxiv-1303-0727 | A Sharp Bound on the Computation-Accuracy Tradeoff for Majority Voting Ensembles | http://arxiv.org/abs/1303.0727 | id:1303.0727 author:Miles E. Lopes category:math.PR cs.SI math.ST stat.ML stat.TH  published:2013-03-04 summary:When random forests are used for binary classification, an ensemble of $t=1,2,\dots$ randomized classifiers is generated, and the predictions of the classifiers are aggregated by majority vote. Due to the randomness in the algorithm, there is a natural tradeoff between statistical performance and computational cost. On one hand, as $t$ increases, the (random) prediction error of the ensemble tends to decrease and stabilize. On the other hand, larger ensembles require greater computational cost for training and making new predictions. The present work offers a new approach for quantifying this tradeoff: Given a fixed training set $\mathcal{D}$, let the random variables $\text{Err}_{t,0}$ and $\text{Err}_{t,1}$ denote the class-wise prediction error rates of a randomly generated ensemble of size $t$. As $t\to\infty$, we provide a general bound on the "algorithmic variance", $\text{var}(\text{Err}_{t,l} \mathcal{D})\leq \frac{f_l(1/2)^2}{4t}+o(\frac{1}{t})$, where $l\in\{0,1\}$, and $f_l$ is a density function that arises from the ensemble method. Conceptually, this result is somewhat surprising, because $\text{var}(\text{Err}_{t,l} \mathcal{D})$ describes how $\text{Err}_{t,l}$ varies over repeated runs of the algorithm, and yet, the formula leads to a method for bounding $\text{var}(\text{Err}_{t,l} \mathcal{D})$ with a single ensemble. The bound is also sharp in the sense that it is attained by an explicit family of randomized classifiers. With regard to the task of estimating $f_l(1/2)$, the presence of the ensemble leads to a unique twist on the classical setup of non-parametric density estimation --- wherein the effects of sample size and computational cost are intertwined. In particular, we propose an estimator for $f_l(1/2)$, and derive an upper bound on its MSE that matches "standard optimal non-parametric rates" when $t$ is sufficiently large. version:2
arxiv-1606-01021 | Automatic Separation of Compound Figures in Scientific Articles | http://arxiv.org/abs/1606.01021 | id:1606.01021 author:Mario Taschwer, Oge Marques category:cs.CV cs.MM  published:2016-06-03 summary:Content-based analysis and retrieval of digital images found in scientific articles is often hindered by images consisting of multiple subfigures (compound figures). We address this problem by proposing a method to automatically classify and separate compound figures, which consists of two main steps: (i) a supervised compound figure classifier (CFC) discriminates between compound and non-compound figures using task-specific image features; and (ii) an image processing algorithm is applied to predicted compound images to perform compound figure separation (CFS). Our CFC approach is shown to achieve state-of-the-art classification performance on a published dataset. Our CFS algorithm shows superior separation accuracy on two different datasets compared to other known automatic approaches. Finally, we propose a method to evaluate the effectiveness of the CFC-CFS process chain and use it to optimize the misclassification loss of CFC for maximal effectiveness in the process chain. version:1
arxiv-1604-07751 | Compressive phase-only filtering at extreme compression rates | http://arxiv.org/abs/1604.07751 | id:1604.07751 author:David Pastor-Calle, Anna Pastuszczak, Michal Mikolajczyk, Rafal Kotynski category:cs.CV physics.optics  published:2016-04-26 summary:We introduce an efficient method for the reconstruction of the correlation between a compressively measured image and a phase-only filter (POF). The proposed method is based on two properties of POF: such filtering is a unitary circulant transform, and the correlation plane it produces is usually sparse. The method can be applied to the measurements from single-pixel detectors, which opens a way for adding some functionalities of an optical correlator to these kind of cameras at no additional computational expense. We show that images measured at extremely high compression rates may still contain sufficient information for target classification and localization. version:3
arxiv-1504-05843 | Exploit Bounding Box Annotations for Multi-label Object Recognition | http://arxiv.org/abs/1504.05843 | id:1504.05843 author:Hao Yang, Joey Tianyi Zhou, Yu Zhang, Bin-Bin Gao, Jianxin Wu, Jianfei Cai category:cs.CV cs.LG  published:2015-04-22 summary:Convolutional neural networks (CNNs) have shown great performance as general feature representations for object recognition applications. However, for multi-label images that contain multiple objects from different categories, scales and locations, global CNN features are not optimal. In this paper, we incorporate local information to enhance the feature discriminative power. In particular, we first extract object proposals from each image. With each image treated as a bag and object proposals extracted from it treated as instances, we transform the multi-label recognition problem into a multi-class multi-instance learning problem. Then, in addition to extracting the typical CNN feature representation from each proposal, we propose to make use of ground-truth bounding box annotations (strong labels) to add another level of local information by using nearest-neighbor relationships of local regions to form a multi-view pipeline. The proposed multi-view multi-instance framework utilizes both weak and strong labels effectively, and more importantly it has the generalization ability to even boost the performance of unseen categories by partial strong labels from other categories. Our framework is extensively compared with state-of-the-art hand-crafted feature based methods and CNN based methods on two multi-label benchmark datasets. The experimental results validate the discriminative power and the generalization ability of the proposed framework. With strong labels, our framework is able to achieve state-of-the-art results in both datasets. version:2
arxiv-1606-01873 | Optically lightweight tracking of objects around a corner | http://arxiv.org/abs/1606.01873 | id:1606.01873 author:Jonathan Klein, Christoph Peters, Jaime Martín, Martin Laurenzis, Matthias B. Hullin category:cs.CV cs.GR physics.optics I.3.7; I.3.8; I.4.8  published:2016-06-03 summary:The observation of objects located in inaccessible regions is a recurring challenge in a wide variety of important applications. Recent work has shown that indirect diffuse light reflections can be used to reconstruct objects and two-dimensional (2D) patterns around a corner. However, these prior methods always require some specialized setup involving either ultrafast detectors or narrowband light sources. Here we show that occluded objects can be tracked in real time using a standard 2D camera and a laser pointer. Unlike previous methods based on the backprojection approach, we formulate the problem in an analysis-by-synthesis sense. By repeatedly simulating light transport through the scene, we determine the set of object parameters that most closely fits the measured intensity distribution. We experimentally demonstrate that this approach is capable of following the translation of unknown objects, and translation and orientation of a known object, in real time. version:1
arxiv-1606-01001 | On Recognizing Transparent Objects in Domestic Environments Using Fusion of Multiple Sensor Modalities | http://arxiv.org/abs/1606.01001 | id:1606.01001 author:Alexander Hagg, Frederik Hegger, Paul Plöger category:cs.CV  published:2016-06-03 summary:Current object recognition methods fail on object sets that include both diffuse, reflective and transparent materials, although they are very common in domestic scenarios. We show that a combination of cues from multiple sensor modalities, including specular reflectance and unavailable depth information, allows us to capture a larger subset of household objects by extending a state of the art object recognition method. This leads to a significant increase in robustness of recognition over a larger set of commonly used objects. version:1
arxiv-1606-00985 | A Graph-Based Semi-Supervised k Nearest-Neighbor Method for Nonlinear Manifold Distributed Data Classification | http://arxiv.org/abs/1606.00985 | id:1606.00985 author:Enmei Tu, Yaqian Zhang, Lin Zhu, Jie Yang, Nikola Kasabov category:cs.LG stat.ML  published:2016-06-03 summary:$k$ Nearest Neighbors ($k$NN) is one of the most widely used supervised learning algorithms to classify Gaussian distributed data, but it does not achieve good results when it is applied to nonlinear manifold distributed data, especially when a very limited amount of labeled samples are available. In this paper, we propose a new graph-based $k$NN algorithm which can effectively handle both Gaussian distributed data and nonlinear manifold distributed data. To achieve this goal, we first propose a constrained Tired Random Walk (TRW) by constructing an $R$-level nearest-neighbor strengthened tree over the graph, and then compute a TRW matrix for similarity measurement purposes. After this, the nearest neighbors are identified according to the TRW matrix and the class label of a query point is determined by the sum of all the TRW weights of its nearest neighbors. To deal with online situations, we also propose a new algorithm to handle sequential samples based a local neighborhood reconstruction. Comparison experiments are conducted on both synthetic data sets and real-world data sets to demonstrate the validity of the proposed new $k$NN algorithm and its improvements to other version of $k$NN algorithms. Given the widespread appearance of manifold structures in real-world problems and the popularity of the traditional $k$NN algorithm, the proposed manifold version $k$NN shows promising potential for classifying manifold-distributed data. version:1
arxiv-1603-00954 | Training Input-Output Recurrent Neural Networks through Spectral Methods | http://arxiv.org/abs/1603.00954 | id:1603.00954 author:Hanie Sedghi, Anima Anandkumar category:cs.LG cs.NE stat.ML  published:2016-03-03 summary:We consider the problem of training input-output recurrent neural networks (RNN) for sequence labeling tasks. We propose a novel spectral approach for learning the network parameters. It is based on decomposition of the cross-moment tensor between the output and a non-linear transformation of the input, based on score functions. We guarantee consistent learning with polynomial sample and computational complexity under transparent conditions such as non-degeneracy of model parameters, polynomial activations for the neurons, and a Markovian evolution of the input sequence. We also extend our results to Bidirectional RNN which uses both previous and future information to output the label at each time point, and is employed in many NLP tasks such as POS tagging. version:3
arxiv-1606-00979 | Question Answering over Knowledge Base with Neural Attention Combining Global Knowledge Information | http://arxiv.org/abs/1606.00979 | id:1606.00979 author:Yuanzhe Zhang, Kang Liu, Shizhu He, Guoliang Ji, Zhanyi Liu, Hua Wu, Jun Zhao category:cs.IR cs.AI cs.CL cs.NE  published:2016-06-03 summary:With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. And it also alleviates the out of vocabulary (OOV) problem, which helps the attention model to represent the question more precisely. The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach. version:1
arxiv-1606-00972 | Synthesizing Dynamic Textures and Sounds by Spatial-Temporal Generative ConvNet | http://arxiv.org/abs/1606.00972 | id:1606.00972 author:Jianwen Xie, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.CV cs.LG cs.NE  published:2016-06-03 summary:Dynamic textures are spatial-temporal processes that exhibit statistical stationarity or stochastic repetitiveness in the temporal dimension. In this paper, we study the problem of modeling and synthesizing dynamic textures using a generative version of the convolution neural network (ConvNet or CNN) that consists of multiple layers of spatial-temporal filters to capture the spatial-temporal patterns in the dynamic textures. We show that such spatial-temporal generative ConvNet can synthesize realistic dynamic textures. We also apply the temporal generative ConvNet to the one-dimensional sound data, and show that the model can synthesize realistic natural and man-made sounds. The videos and sounds can be found at http://www.stat.ucla.edu/~jxie/STGConvNet/STGConvNet.html version:1
arxiv-1606-00968 | Smooth Imitation Learning for Online Sequence Prediction | http://arxiv.org/abs/1606.00968 | id:1606.00968 author:Hoang M. Le, Andrew Kang, Yisong Yue, Peter Carr category:cs.LG  published:2016-06-03 summary:We study the problem of smooth imitation learning for online sequence prediction, where the goal is to train a policy that can smoothly imitate demonstrated behavior in a dynamic and continuous environment in response to online, sequential context input. Since the mapping from context to behavior is often complex, we take a learning reduction approach to reduce smooth imitation learning to a regression problem using complex function classes that are regularized to ensure smoothness. We present a learning meta-algorithm that achieves fast and stable convergence to a good policy. Our approach enjoys several attractive properties, including being fully deterministic, employing an adaptive learning rate that can provably yield larger policy improvements compared to previous approaches, and the ability to ensure stable convergence. Our empirical results demonstrate significant performance gains over previous approaches. version:1
arxiv-1606-00128 | Self-Paced Learning: an Implicit Regularization Perspective | http://arxiv.org/abs/1606.00128 | id:1606.00128 author:Yanbo Fan, Ran He, Jian Liang, Bao-Gang Hu category:cs.LG cs.CV  published:2016-06-01 summary:Self-paced learning (SPL) mimics the cognitive mechanism of humans and animals that gradually learns from easy to hard samples. One key issue in SPL is to obtain better weighting strategy that is determined by the minimizer functions. Existing methods usually pursue this by artificially designing the explicit form of regularizers. In this paper, we focus on the minimizer functions, and study a group of new regularizers, named self-paced implicit regularizers that are derived from convex conjugacy. Based on the multiplicative form of half-quadratic optimization, convex and non-convex functions induced minimizer functions for the implicit regularizers are developed. And a general framework (named SPL-IR) for SPL is developed accordingly. We further analyze the relation between SPLIR and half-quadratic optimization. We implement SPL-IR to matrix factorization and multi-view clustering. Experimental results on both synthetic and real-world databases corroborate our ideas and demonstrate the effectiveness of implicit regularizers. version:2
arxiv-1512-04011 | L1-Regularized Distributed Optimization: A Communication-Efficient Primal-Dual Framework | http://arxiv.org/abs/1512.04011 | id:1512.04011 author:Virginia Smith, Simone Forte, Michael I. Jordan, Martin Jaggi category:cs.LG  published:2015-12-13 summary:Despite the importance of sparsity in many large-scale applications, there are few methods for distributed optimization of sparsity-inducing objectives. In this paper, we present a communication-efficient framework for L1-regularized optimization in the distributed environment. By viewing classical objectives in a more general primal-dual setting, we develop a new class of methods that can be efficiently distributed and applied to common sparsity-inducing models, such as Lasso, sparse logistic regression, and elastic net-regularized problems. We provide theoretical convergence guarantees for our framework, and demonstrate its efficiency and flexibility with a thorough experimental comparison on Amazon EC2. Our proposed framework yields speedups of up to 50x as compared to current state-of-the-art methods for distributed L1-regularized optimization. version:2
arxiv-1606-00931 | Deep Survival: A Deep Cox Proportional Hazards Network | http://arxiv.org/abs/1606.00931 | id:1606.00931 author:Jared Katzman, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, Yuval Kluger category:stat.ML cs.NE  published:2016-06-02 summary:Previous research has shown that neural networks can model survival data in situations in which some patients' death times are unknown, e.g. right-censored. However, neural networks have rarely been shown to outperform their linear counterparts such as the Cox proportional hazards model. In this paper, we run simulated experiments and use real survival data to build upon the risk-regression architecture proposed by Faraggi and Simon. We demonstrate that our model, DeepSurv, not only works as well as the standard linear Cox proportional hazards model but actually outperforms it in predictive ability on survival data with linear and nonlinear risk functions. We then show that the neural network can also serve as a recommender system by including a categorical variable representing a treatment group. This can be used to provide personalized treatment recommendations based on an individual's calculated risk. We provide an open source Python module that implements these methods in order to advance research on deep learning and survival analysis. version:1
arxiv-1606-00930 | Comparison of 14 different families of classification algorithms on 115 binary datasets | http://arxiv.org/abs/1606.00930 | id:1606.00930 author:Jacques Wainer category:cs.LG cs.CV  published:2016-06-02 summary:We tested 14 very different classification algorithms (random forest, gradient boosting machines, SVM - linear, polynomial, and RBF - 1-hidden-layer neural nets, extreme learning machines, k-nearest neighbors and a bagging of knn, naive Bayes, learning vector quantization, elastic net logistic regression, sparse linear discriminant analysis, and a boosting of linear classifiers) on 115 real life binary datasets. We followed the Demsar analysis and found that the three best classifiers (random forest, gbm and RBF SVM) are not significantly different from each other. We also discuss that a change of less then 0.0112 in the error rate should be considered as an irrelevant change, and used a Bayesian ANOVA analysis to conclude that with high probability the differences between these three classifiers is not of practical consequence. We also verified the execution time of "standard implementations" of these algorithms and concluded that RBF SVM is the fastest (significantly so) both in training time and in training plus testing time. version:1
arxiv-1606-00925 | Convolutional Imputation of Matrix Network | http://arxiv.org/abs/1606.00925 | id:1606.00925 author:Qingyun Sun, Mengyuan Yan, Stephen Boyd category:cs.LG stat.ML  published:2016-06-02 summary:A matrix network is a family of matrices, where the relationship between them is modeled as a weighted graph. Each node represents a matrix, and the weight on each edge represents the similarity between the two matrices. Suppose that we observe a few entries of each matrix with noise, and the fraction of entries we observe varies from matrix to matrix. Even worse, a subset of matrices in this family may be completely unobserved. How can we recover the entire matrix network from noisy and incomplete observations? One motivating example is the cold start problem, where we need to do inference on new users or items that come with no information. To recover this network of matrices, we propose a structural assumption that the matrix network can be approximated by generalized convolution of low rank matrices living on the same network. We propose an iterative imputation algorithm to complete the matrix network. This algorithm is efficient for large scale applications and is guaranteed to accurately recover all matrices, as long as there are enough observations accumulated over the network. version:1
arxiv-1606-00917 | Towards a Job Title Classification System | http://arxiv.org/abs/1606.00917 | id:1606.00917 author:Faizan Javed, Matt McNair, Ferosh Jacob, Meng Zhao category:cs.LG cs.AI  published:2016-06-02 summary:Document classification for text, images and other applicable entities has long been a focus of research in academia and also finds application in many industrial settings. Amidst a plethora of approaches to solve such problems, machine-learning techniques have found success in a variety of scenarios. In this paper we discuss the design of a machine learning-based semi-supervised job title classification system for the online job recruitment domain currently in production at CareerBuilder.com and propose enhancements to it. The system leverages a varied collection of classification as well clustering algorithms. These algorithms are encompassed in an architecture that facilitates leveraging existing off-the-shelf machine learning tools and techniques while keeping into consideration the challenges of constructing a scalable classification system for a large taxonomy of categories. As a continuously evolving system that is still under development we first discuss the existing semi-supervised classification system which is composed of both clustering and classification components in a proximity-based classifier setup and results of which are already used across numerous products at CareerBuilder. We then elucidate our long-term goals for job title classification and propose enhancements to the existing system in the form of a two-stage coarse and fine level classifier augmentation to construct a cascade of hierarchical vertical classifiers. Preliminary results are presented using experimental evaluation on real world industrial data. version:1
arxiv-1606-00915 | DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs | http://arxiv.org/abs/1606.00915 | id:1606.00915 author:Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille category:cs.CV  published:2016-06-02 summary:In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7% mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online. version:1
arxiv-1606-00911 | Distributed Cooperative Decision-Making in Multiarmed Bandits: Frequentist and Bayesian Algorithms | http://arxiv.org/abs/1606.00911 | id:1606.00911 author:Peter Landgren, Vaibhav Srivastava, Naomi Ehrich Leonard category:cs.SY cs.LG math.OC  published:2016-06-02 summary:We study distributed cooperative decision-making under the explore-exploit tradeoff in the multiarmed bandit (MAB) problem. We extend the state-of-the-art frequentist and Bayesian algorithms for single-agent MAB problems to cooperative distributed algorithms for multi-agent MAB problems in which agents communicate according to a fixed network graph. We rely on a running consensus algorithm for each agent's estimation of mean rewards from its own rewards and the estimated rewards of its neighbors. We prove the performance of these algorithms and show that they asymptotically recover the performance of a centralized agent. Further, we rigorously characterize the influence of the communication graph structure on the decision-making performance of the group. version:1
arxiv-1605-03468 | A constrained L1 minimization approach for estimating multiple Sparse Gaussian or Nonparanormal Graphical Models | http://arxiv.org/abs/1605.03468 | id:1605.03468 author:Beilun Wang, Ritambhara Singh, Yanjun Qi category:cs.LG stat.ML  published:2016-05-11 summary:The flood of multi-context measurement data from many scientific domains have created an urgent need to reconstruct context-specific variable networks, that can significantly simplify network-driven studies. Computationally, this problem can be formulated as jointly estimating multiple different, but related, sparse Undirected Graphical Models (UGM) from samples aggregated across several tasks. Previous joint-UGM studies could not fully address the challenge since they mostly focus on Gaussian Graphical Models (GGM) and have used likelihood-based formulations to push multiple estimated networks toward a common pattern. Differently, we propose a novel approach, SIMULE (learning Shared and Individual parts of MULtiple graphs Explicitly) to solve multi-task UGM using a l1 constrained optimization. SIMULE can handle both multivariate Gaussian and multivariate Nonparanormal data (greatly relaxing the normality assumption most real data do not follow). SIMULE is cast as independent subproblems of linear programming that can be solved efficiently. It automatically infers specific dependencies that are unique to each context as well as shared substructures preserved among all the contexts. Theoretically we prove that SIMULE achieves a consistent estimation at rate O(log(Kp)/ntot) (not been proved before). On four synthetic datasets and two real datasets, SIMULE shows significant improvements over state-of-the-art multi-sGGM and single-UGM baselines. version:3
arxiv-1606-00906 | Nonlinear Statistical Learning with Truncated Gaussian Graphical Models | http://arxiv.org/abs/1606.00906 | id:1606.00906 author:Qinliang Su, Xuejun Liao, Changyou Chen, Lawrence Carin category:stat.ML  published:2016-06-02 summary:We introduce the truncated Gaussian graphical model (TGGM) as a novel framework for designing statistical models for nonlinear learning. A TGGM is a Gaussian graphical model (GGM) with a subset of variables truncated to be nonnegative. The truncated variables are assumed latent and integrated out to induce a marginal model. We show that the variables in the marginal model are non-Gaussian distributed and their expected relations are nonlinear. We use expectation-maximization to break the inference of the nonlinear model into a sequence of TGGM inference problems, each of which is efficiently solved by using the properties and numerical methods of multivariate Gaussian distributions. We use the TGGM to design models for nonlinear regression and classification, with the performances of these models demonstrated on extensive benchmark datasets and compared to state-of-the-art competing results. version:1
arxiv-1602-05205 | Primal-Dual Rates and Certificates | http://arxiv.org/abs/1602.05205 | id:1602.05205 author:Celestine Dünner, Simone Forte, Martin Takáč, Martin Jaggi category:cs.LG math.OC 90C46  90C25  49N15 G.1.6  published:2016-02-16 summary:We propose an algorithm-independent framework to equip existing optimization methods with primal-dual certificates. Such certificates and corresponding rate of convergence guarantees are important for practitioners to diagnose progress, in particular in machine learning applications. We obtain new primal-dual convergence rates, e.g., for the Lasso as well as many L1, Elastic Net, group Lasso and TV-regularized problems. The theory applies to any norm-regularized generalized linear model. Our approach provides efficiently computable duality gaps which are globally defined, without modifying the original problems in the region of interest. version:2
arxiv-1606-00897 | Multi-Organ Cancer Classification and Survival Analysis | http://arxiv.org/abs/1606.00897 | id:1606.00897 author:Stefan Bauer, Nicolas Carion, Peter Schüffler, Thomas Fuchs, Peter Wild, Joachim M. Buhmann category:q-bio.QM cs.LG q-bio.TO stat.ML  published:2016-06-02 summary:Accurate and robust cell nuclei classification is the cornerstone for a wider range of tasks in digital and Computational Pathology. However, most machine learning systems require extensive labeling from expert pathologists for each individual problem at hand, with no or limited abilities for knowledge transfer between datasets and organ sites. In this paper we implement and evaluate a variety of deep neural network models and model ensembles for nuclei classification in renal cell cancer (RCC) and prostate cancer (PCa). We propose a convolutional neural network system based on residual learning which significantly improves over the state-of-the-art in cell nuclei classification. However, the main thrust of our work is to demonstrate for the first time the models ability to transfer the learned concepts from one organ type to another, with robust performance. Finally, we show that the combination of tissue types during training increases not only classification accuracy but also overall survival analysis. The best model, trained on combined data of RCC and PCa, exhibits optimal performance on PCa classification and better survival group stratification than an expert pathologist ($p=0.006$). All code, image data and expert labels are made publicly available to serve as benchmark for the community for future research into computational pathology. version:1
arxiv-1604-04393 | Unsupervised Image Segmentation using the Deffuant-Weisbuch Model from Social Dynamics | http://arxiv.org/abs/1604.04393 | id:1604.04393 author:Subhradeep Kayal category:cs.CV  published:2016-04-15 summary:Unsupervised image segmentation algorithms aim at identifying disjoint homogeneous regions in an image, and have been subject to considerable attention in the machine vision community. In this paper, a popular theoretical model with it's origins in statistical physics and social dynamics, known as the Deffuant-Weisbuch model, is applied to the image segmentation problem. The Deffuant-Weisbuch model has been found to be useful in modelling the evolution of a closed system of interacting agents characterised by their opinions or beliefs, leading to the formation of clusters of agents who share a similar opinion or belief at steady state. In the context of image segmentation, this paper considers a pixel as an agent and it's colour property as it's opinion, with opinion updates as per the Deffuant-Weisbuch model. Apart from applying the basic model to image segmentation, this paper incorporates adjacency and neighbourhood information in the model, which factors in the local similarity and smoothness properties of images. Convergence is reached when the number of unique pixel opinions, i.e., the number of colour centres, matches the pre-specified number of clusters. Experiments are performed on a set of images from the Berkeley Image Segmentation Dataset and the results are analysed both qualitatively and quantitatively, which indicate that this simple and intuitive method is promising for image segmentation. To the best of the knowledge of the author, this is the first work where a theoretical model from statistical physics and social dynamics has been successfully applied to image processing. version:3
arxiv-1606-00868 | Unified Framework for Quantification | http://arxiv.org/abs/1606.00868 | id:1606.00868 author:Aykut Firat category:cs.LG  published:2016-06-02 summary:Quantification is the machine learning task of estimating test-data class proportions that are not necessarily similar to those in training. Apart from its intrinsic value as an aggregate statistic, quantification output can also be used to optimize classifier probabilities, thereby increasing classification accuracy. We unify major quantification approaches under a constrained multi-variate regression framework, and use mathematical programming to estimate class proportions for different loss functions. With this modeling approach, we extend existing binary-only quantification approaches to multi-class settings as well. We empirically verify our unified framework by experimenting with several multi-class datasets including the Stanford Sentiment Treebank and CIFAR-10. version:1
arxiv-1606-00856 | Sequential Principal Curves Analysis | http://arxiv.org/abs/1606.00856 | id:1606.00856 author:Valero Laparra, Jesus Malo category:stat.ML cs.LG  published:2016-06-02 summary:This work includes all the technical details of the Sequential Principal Curves Analysis (SPCA) in a single document. SPCA is an unsupervised nonlinear and invertible feature extraction technique. The identified curvilinear features can be interpreted as a set of nonlinear sensors: the response of each sensor is the projection onto the corresponding feature. Moreover, it can be easily tuned for different optimization criteria; e.g. infomax, error minimization, decorrelation; by choosing the right way to measure distances along each curvilinear feature. Even though proposed in [Laparra et al. Neural Comp. 12] and shown to work in multiple modalities in [Laparra and Malo Frontiers Hum. Neuro. 15], the SPCA framework has its original roots in the nonlinear ICA algorithm in [Malo and Gutierrez Network 06]. Later on, the SPCA philosophy for nonlinear generalization of PCA originated substantially faster alternatives at the cost of introducing different constraints in the model. Namely, the Principal Polynomial Analysis (PPA) [Laparra et al. IJNS 14], and the Dimensionality Reduction via Regression (DRR) [Laparra et al. IEEE TGRS 15]. This report illustrates the reasons why we developed such family and is the appropriate technical companion for the missing details in [Laparra et al., NeCo 12, Laparra and Malo, Front.Hum.Neuro. 15]. See also the data, code and examples in the dedicated sites http://isp.uv.es/spca.html and http://isp.uv.es/after effects.html version:1
arxiv-1606-00850 | Face Detection with End-to-End Integration of a ConvNet and a 3D Model | http://arxiv.org/abs/1606.00850 | id:1606.00850 author:Yunzhu Li, Benyuan Sun, Tianfu Wu, Yizhou Wang, Wen Gao category:cs.CV  published:2016-06-02 summary:This paper presents a method of integrating a ConvNet and a 3D model in an end-to-end multi-task discriminative learning fashion for face detection in the wild. In training, we assume a 3D mean face model and the facial key-point annotation of each face image are available, our ConvNet learns to predict (i) face bounding box proposals via estimating the 3D transformation (rotation and translation) of the mean face model as well as (ii) the facial key-points for each face instance. It addresses two issues in the state-of-the-art generic object detection ConvNets (e.g., faster R-CNN \cite{FasterRCNN}) by adapting it for face detection: (i) One is to eliminate the heuristic design of predefined anchor boxes in the region proposals network (RPN) by exploiting a 3D mean face model. (ii) The other is to replace the generic RoI (Region-of-Interest) pooling layer with a "configuration pooling" layer, which respects the underlying object configurations based on the predicted facial key-points, hence, it is more semantics driven. The multi-task loss consists of three terms: the classification Softmax loss and the location smooth $l_1$-losses \cite{FastRCNN} of both the facial key-points and the face bounding boxes. In experiments, our ConvNet is trained on the AFLW dataset \cite{AFLW} only and tested on the FDDB benchmark \cite{FDDB} and the AFW benchmark \cite{AFW}. The results show that the proposed method achieves very competitive state-of-the-art performance in the two benchmarks. version:1
arxiv-1606-00832 | High Dimensional Multivariate Regression and Precision Matrix Estimation via Nonconvex Optimization | http://arxiv.org/abs/1606.00832 | id:1606.00832 author:Jinghui Chen, Quanquan Gu category:stat.ML  published:2016-06-02 summary:We propose a nonconvex estimator for joint multivariate regression and precision matrix estimation in the high dimensional regime, under sparsity constraints. A gradient descent algorithm with hard thresholding is developed to solve the nonconvex estimator, and it attains a linear rate of convergence to the true regression coefficients and precision matrix simultaneously, up to the statistical error. Compared with existing methods along this line of research, which have little theoretical guarantee, the proposed algorithm not only is computationally much more efficient with provable convergence guarantee, but also attains the optimal finite sample statistical rate up to a logarithmic factor. Thorough experiments on both synthetic and real datasets back up our theory. version:1
arxiv-1606-00825 | Training a Hidden Markov Model with a Bayesian Spiking Neural Network | http://arxiv.org/abs/1606.00825 | id:1606.00825 author:Amirhossein Tavanaei, Anthony S Maida category:cs.NE  published:2016-06-02 summary:It is of some interest to understand how statistically based mechanisms for signal processing might be integrated with biologically motivated mechanisms such as neural networks. This paper explores a novel hybrid approach for classifying segments of sequential data, such as individual spoken works. The approach combines a hidden Markov model (HMM) with a spiking neural network (SNN). The HMM, consisting of states and transitions, forms a fixed backbone with nonadaptive transition probabilities. The SNN, however, implements a biologically based Bayesian computation that derives from the spike timing-dependent plasticity (STDP) learning rule. The emission (observation) probabilities of the HMM are represented in the SNN and trained with the STDP rule. A separate SNN, each with the same architecture, is associated with each of the states of the HMM. Because of the STDP training, each SNN implements an expectation maximization algorithm to learn the emission probabilities for one HMM state. The model was studied on synthesized spike-train data and also on spoken word data. Preliminary results suggest its performance compares favorably with other biologically motivated approaches. Because of the model's uniqueness and initial promise, it warrants further study. It provides some new ideas on how the brain might implement the equivalent of an HMM in a neural circuit. version:1
arxiv-1606-00822 | Unifying Geometric Features and Facial Action Units for Improved Performance of Facial Expression Analysis | http://arxiv.org/abs/1606.00822 | id:1606.00822 author:Mehdi Ghayoumi, Arvind K Bansal category:cs.CV cs.HC  published:2016-06-02 summary:Previous approaches to model and analyze facial expression analysis use three different techniques: facial action units, geometric features and graph based modelling. However, previous approaches have treated these technique separately. There is an interrelationship between these techniques. The facial expression analysis is significantly improved by utilizing these mappings between major geometric features involved in facial expressions and the subset of facial action units whose presence or absence are unique to a facial expression. This paper combines dimension reduction techniques and image classification with search space pruning achieved by this unique subset of facial action units to significantly prune the search space. The performance results on the publicly facial expression database shows an improvement in performance by 70% over time while maintaining the emotion recognition correctness. version:1
arxiv-1606-00813 | Generalized Root Models: Beyond Pairwise Graphical Models for Univariate Exponential Families | http://arxiv.org/abs/1606.00813 | id:1606.00813 author:David I. Inouye, Pradeep Ravikumar, Inderjit S. Dhillon category:stat.ML  published:2016-06-02 summary:We present a novel k-way high-dimensional graphical model called the Generalized Root Model (GRM) that explicitly models dependencies between variable sets of size k > 2---where k = 2 is the standard pairwise graphical model. This model is based on taking the k-th root of the original sufficient statistics of any univariate exponential family with positive sufficient statistics, including the Poisson and exponential distributions. As in the recent work with square root graphical (SQR) models [Inouye et al. 2016]---which was restricted to pairwise dependencies---we give the conditions of the parameters that are needed for normalization using the radial conditionals similar to the pairwise case [Inouye et al. 2016]. In particular, we show that the Poisson GRM has no restrictions on the parameters and the exponential GRM only has a restriction akin to negative definiteness. We develop a simple but general learning algorithm based on L1-regularized node-wise regressions. We also present a general way of numerically approximating the log partition function and associated derivatives of the GRM univariate node conditionals---in contrast to [Inouye et al. 2016], which only provided algorithm for estimating the exponential SQR. To illustrate GRM, we model word counts with a Poisson GRM and show the associated k-sized variable sets. We finish by discussing methods for reducing the parameter space in various situations. version:1
arxiv-1606-00802 | A Spiking Network that Learns to Extract Spike Signatures from Speech Signals | http://arxiv.org/abs/1606.00802 | id:1606.00802 author:Amirhossein Tavanaei, Anthony S Maida category:cs.NE  published:2016-06-02 summary:Spiking neural networks (SNNs) with adaptive synapses reflect core properties of biological neural networks. Speech recognition, as an application involving audio coding and dynamic learning, provides a good test problem to study SNN functionality. We present a novel and efficient method that learns to convert a speech signal into a spike train signature. The signature is distinguishable from signatures for other speech signals, representing different words, thereby enabling digit recognition and discrimination in devices that use only spiking neurons. The method uses a small SNN consisting of Izhikevich neurons equipped with spike timing dependent plasticity (STDP) and biologically realistic synapses. This approach introduces an efficient, fast, and multi-speaker strategy without error-feedback training, although it does require supervised training. The new simulation results produce discriminative spike train patterns for spoken digits in which highly correlated spike trains belong to the same category and low correlated patterns belong to different categories. The signatures can be used as a set of feature maps for classification. Our experiments compare two simple classifiers for spoken digit recognition in both clean and noisy environments. version:1
arxiv-1606-00800 | Multi-View Treelet Transform | http://arxiv.org/abs/1606.00800 | id:1606.00800 author:Brian A. Mitchell, Linda R. Petzold category:stat.ML cs.CV cs.SI q-bio.NC  published:2016-06-02 summary:Current multi-view factorization methods make assumptions that are not acceptable for many kinds of data, and in particular, for graphical data with hierarchical structure. At the same time, current hierarchical methods work only in the single-view setting. We generalize the Treelet Transform to the Multi-View Treelet Transform (MVTT) to allow for the capture of hierarchical structure when multiple views are avilable. Further, we show how this generalization is consistent with the existing theory and how it might be used in denoising empirical networks and in computing the shared response of functional brain data. version:1
arxiv-1606-00787 | Prior Swapping for Data-Independent Inference | http://arxiv.org/abs/1606.00787 | id:1606.00787 author:Willie Neiswanger, Eric Xing category:stat.ML cs.AI cs.LG stat.CO stat.ME  published:2016-06-02 summary:While Bayesian methods are praised for their ability to incorporate useful prior knowledge, in practice, priors that allow for computationally convenient or tractable inference are more commonly used. In this paper, we investigate the following question: for a given model, is it possible to use any convenient prior to infer a false posterior, and afterwards, given some true prior of interest, quickly transform this result into the true posterior? We present a procedure to carry out this task: given an inferred false posterior and true prior, our algorithm generates samples from the true posterior. This transformation procedure, which we call "prior swapping" works for arbitrary priors. Notably, its cost is independent of data size. It therefore allows us, in some cases, to apply significantly less-costly inference procedures to more-sophisticated models than previously possible. It also lets us quickly perform any additional inferences, such as with updated priors or for many different hyperparameter settings, without touching the data. We prove that our method can generate asymptotically exact samples, and demonstrate it empirically on a number of models and priors. version:1
arxiv-1412-6565 | On the robustness of learning in games with stochastically perturbed payoff observations | http://arxiv.org/abs/1412.6565 | id:1412.6565 author:Mario Bravo, Panayotis Mertikopoulos category:math.OC cs.GT math.PR stat.ML  published:2014-12-20 summary:Motivated by the scarcity of accurate payoff feedback in practical applications of game theory, we examine a class of learning dynamics where players adjust their choices based on past payoff observations that are subject to noise and random disturbances. First, in the single-player case (corresponding to an agent trying to adapt to an arbitrarily changing environment), we show that the stochastic dynamics under study lead to no regret almost surely, irrespective of the noise level in the player's observations. In the multi-player case, we find that dominated strategies become extinct and we show that strict Nash equilibria are stochastically stable and attracting; conversely, if a state is stable or attracting with positive probability, then it is a Nash equilibrium. Finally, we provide an averaging principle for 2-player games, and we show that in zero-sum games with an interior equilibrium, time averages converge to Nash equilibrium for any noise level. version:2
arxiv-1606-00739 | Stochastic Structured Prediction under Bandit Feedback | http://arxiv.org/abs/1606.00739 | id:1606.00739 author:Artem Sokolov, Julia Kreutzer, Stefan Riezler category:cs.CL cs.LG  published:2016-06-02 summary:Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We introduce stochastic approximation algorithms that apply this learning scenario to probabilistic structured prediction, with a focus on asymptotic convergence and ease of elicitability of feedback. We present simulation experiments for complex natural language processing tasks, showing fastest empirical convergence and smallest empirical variance for stochastic optimization of a non-convex pairwise preference learning objective compared to stochastic optimization of related non-convex and convex objectives. version:1
arxiv-1602-03476 | Conditional Dependence via Shannon Capacity: Axioms, Estimators and Applications | http://arxiv.org/abs/1602.03476 | id:1602.03476 author:Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath category:cs.IT cs.LG math.IT stat.ML  published:2016-02-10 summary:We conduct an axiomatic study of the problem of estimating the strength of a known causal relationship between a pair of variables. We propose that an estimate of causal strength should be based on the conditional distribution of the effect given the cause (and not on the driving distribution of the cause), and study dependence measures on conditional distributions. Shannon capacity, appropriately regularized, emerges as a natural measure under these axioms. We examine the problem of calculating Shannon capacity from the observed samples and propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate its consistency. Finally, we demonstrate an application to single-cell flow-cytometry, where the proposed estimators significantly reduce sample complexity. version:3
arxiv-1606-00720 | Differentially Private Gaussian Processes | http://arxiv.org/abs/1606.00720 | id:1606.00720 author:Michael Thomas Smith, Max Zwiessele, Neil D. Lawrence category:stat.ML cs.LG 60G15 G.3  published:2016-06-02 summary:A major challenge for machine learning is increasing the availability of data while respecting the privacy of individuals. Differential privacy is a framework which allows algorithms to have provable privacy guarantees. Gaussian processes are a widely used approach for dealing with uncertainty in functions. This paper explores differentially private mechanisms for Gaussian processes. We compare binning and adding noise before regression with adding noise post-regression. For the former we develop a new kernel for use with binned data. For the latter we show that using inducing inputs allows us to reduce the scale of the added perturbation. We find that, for the datasets used, adding noise to a binned dataset has superior accuracy. Together these methods provide a starter toolkit for combining differential privacy and Gaussian processes. version:1
arxiv-1606-00709 | f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization | http://arxiv.org/abs/1606.00709 | id:1606.00709 author:Sebastian Nowozin, Botond Cseke, Ryota Tomioka category:stat.ML cs.LG stat.ME  published:2016-06-02 summary:Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models. version:1
arxiv-1606-00704 | Adversarially Learned Inference | http://arxiv.org/abs/1606.00704 | id:1606.00704 author:Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, Aaron Courville category:stat.ML cs.LG  published:2016-06-02 summary:We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network that is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with other recent approaches on the semi-supervised SVHN task. version:1
arxiv-1605-01133 | Deep Motif: Visualizing Genomic Sequence Classifications | http://arxiv.org/abs/1605.01133 | id:1605.01133 author:Jack Lanchantin, Ritambhara Singh, Zeming Lin, Yanjun Qi category:cs.LG  published:2016-05-04 summary:This paper applies a deep convolutional/highway MLP framework to classify genomic sequences on the transcription factor binding site task. To make the model understandable, we propose an optimization driven strategy to extract "motifs", or symbolic patterns which visualize the positive class learned by the network. We show that our system, Deep Motif (DeMo), extracts motifs that are similar to, and in some cases outperform the current well known motifs. In addition, we find that a deeper model consisting of multiple convolutional and highway layers can outperform a single convolutional and fully connected layer in the previous state-of-the-art. version:2
arxiv-1605-02783 | Estimacion de carga muscular mediante imagenes | http://arxiv.org/abs/1605.02783 | id:1605.02783 author:Leandro Abraham, Facundo Bromberg, Raymundo Forradellas category:cs.CV  published:2016-05-09 summary:Un problema de gran interes en disciplinas como la ocupacional, ergonomica y deportiva, es la medicion de variables biomecanicas involucradas en el movimiento humano (como las fuerzas musculares internas y torque de articulaciones). Actualmente este problema se resuelve en un proceso de dos pasos. Primero capturando datos con dispositivos poco pr\'acticos, intrusivos y costosos. Luego estos datos son usados como entrada en modelos complejos para obtener las variables biomecanicas como salida. El presente trabajo representa una alternativa automatizada, no intrusiva y economica al primer paso, proponiendo la captura de estos datos a traves de imagenes. En trabajos futuros la idea es automatizar todo el proceso de calculo de esas variables. En este trabajo elegimos un caso particular de medicion de variables biomecanicas: el problema de estimar el nivel discreto de carga muscular que estan ejerciendo los musculos de un brazo. Para estimar a partir de imagenes estaticas del brazo ejerciendo la fuerza de sostener la carga, el nivel de la misma, realizamos un proceso de clasificacion. Nuestro enfoque utiliza Support Vector Machines para clasificacion, combinada con una etapa de pre-procesamiento que extrae caracter{\i}sticas visuales utilizando variadas tecnicas (Bag of Keypoints, Local Binary Patterns, Histogramas de Color, Momentos de Contornos) En los mejores casos (Local Binary Patterns y Momentos de Contornos) obtenemos medidas de performance en la clasificacion (Precision, Recall, F-Measure y Accuracy) superiores al 90 %. version:2
arxiv-1605-07669 | On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems | http://arxiv.org/abs/1605.07669 | id:1605.07669 author:Pei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-Barahona, Stefan Ultes, David Vandyke, Tsung-Hsien Wen, Steve Young category:cs.CL cs.LG  published:2016-05-24 summary:The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user's intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning. version:2
arxiv-1606-00182 | On the Troll-Trust Model for Edge Sign Prediction in Social Networks | http://arxiv.org/abs/1606.00182 | id:1606.00182 author:Géraud Le Falher, Nicolò Cesa-Bianchi, Claudio Gentille, Fabio Vitale category:cs.LG cs.SI  published:2016-06-01 summary:In the problem of edge sign classification, we are given a directed graph (representing an online social network), and our task is to predict the binary labels of the edges (i.e., the positive or negative nature of the social relationships). Many successful heuristics for this problem are based on the troll-trust features, estimating on each node the fraction of outgoing and incoming positive edges. We show that these heuristics can be understood, and rigorously analyzed, as approximators to the Bayes optimal classifier for a simple probabilistic model of the edge labels. We then show that the maximum likelihood estimator of this model approximately corresponds to the predictions of a label propagation algorithm run on a transformed version of the original social graph. Extensive experiments on a number of real-world datasets show that this algorithm is competitive against state-of-the-art classifiers in terms of both prediction performance and scalability. Finally, we show that troll-trust features can also be used to derive online learning algorithms which have theoretical guarantees even when edges are adversarially labeled. version:2
arxiv-1606-00668 | Unified Scalable Equivalent Formulations for Schatten Quasi-Norms | http://arxiv.org/abs/1606.00668 | id:1606.00668 author:Fanhua Shang, Yuanyuan Liu, James Cheng category:cs.IT math.IT math.OC stat.ML  published:2016-06-02 summary:The Schatten quasi-norm can be used to bridge the gap between the nuclear norm and rank function. However, most existing algorithms are too slow or even impractical for large-scale problems, due to the SVD or EVD of the whole matrix in each iteration. In this paper, we rigorously prove that for any p, p1, p2>0 satisfying 1/p=1/p1+1/p2, the Schatten-p quasi-norm is equivalent to the minimization of the product of the Schatten-p1 norm (or quasi-norm) and Schatten-p2 norm (or quasi-norm) of its two factor matrices. Then we prove the equivalence relationship between the product formula of the Schatten quasi-norm and its sum formula for the two cases of p1 and p2. Especially, when p>1/2, there is an equivalence between the Schatten-p quasi-norm (or norm) of any matrix and the Schatten-2p norms of its two factor matrices. That is, various Schatten-p quasi-norm minimization problems with p>1/2 can be transformed into the one only involving the smooth, convex norms of two factor matrices, which can lead to simpler and more efficient algorithms than conventional methods. We further extend the theoretical results of two factor matrices to the cases of three and more factor matrices, from which we can see that for any 0<p<1, the Schatten-p quasi-norm is the minimization of the mean to the power of p3+1 of the Schatten-(p3+1)p norms of all factor matrices, where p3 denotes the largest integer not exceeding 1/p. In other words, for any 0<p<1, the Schatten-p quasi-norm minimization can be transformed into an optimization problem only involving the smooth, convex norms of multiple factor matrices. In addition, we present some representative examples for two and three factor matrices. The bi-nuclear and Frobenius/nuclear quasi-norms defined in [1] and the tri-nuclear quasi-norm defined in [2] are three important special cases. version:1
arxiv-1606-00656 | Forecasting Framework for Open Access Time Series in Energy | http://arxiv.org/abs/1606.00656 | id:1606.00656 author:Gergo Barta, Gabor Nagy, Gabor Simon, Gyozo Papp category:stat.AP stat.ML  published:2016-06-02 summary:In this paper we propose a framework for automated forecasting of energy-related time series using open access data from European Network of Transmission System Operators for Electricity (ENTSO-E). The framework provides forecasts for various European countries using publicly available historical data only. Our solution was benchmarked using the actual load data and the country provided estimates (where available). We conclude that the proposed system can produce timely forecasts with comparable prediction accuracy in a number of cases. We also investigate the probabilistic case of forecasting - that is, providing a probability distribution rather than a simple point forecast - and incorporate it into a web based API that provides quick and easy access to reliable forecasts. version:1
arxiv-1606-00625 | Storytelling of Photo Stream with Bidirectional Multi-thread Recurrent Neural Network | http://arxiv.org/abs/1606.00625 | id:1606.00625 author:Yu Liu, Jianlong Fu, Tao Mei, Chang Wen Chen category:cs.CV  published:2016-06-02 summary:Visual storytelling aims to generate human-level narrative language (i.e., a natural paragraph with multiple sentences) from a photo streams. A typical photo story consists of a global timeline with multi-thread local storylines, where each storyline occurs in one different scene. Such complex structure leads to large content gaps at scene transitions between consecutive photos. Most existing image/video captioning methods can only achieve limited performance, because the units in traditional recurrent neural networks (RNN) tend to "forget" the previous state when the visual sequence is inconsistent. In this paper, we propose a novel visual storytelling approach with Bidirectional Multi-thread Recurrent Neural Network (BMRNN). First, based on the mined local storylines, a skip gated recurrent unit (sGRU) with delay control is proposed to maintain longer range visual information. Second, by using sGRU as basic units, the BMRNN is trained to align the local storylines into the global sequential timeline. Third, a new training scheme with a storyline-constrained objective function is proposed by jointly considering both global and local matches. Experiments on three standard storytelling datasets show that the BMRNN model outperforms the state-of-the-art methods. version:1
arxiv-1606-00611 | Autoconvolution for Unsupervised Feature Learning | http://arxiv.org/abs/1606.00611 | id:1606.00611 author:Boris Knyazev, Erhardt Barth, Thomas Martinetz category:cs.CV cs.LG cs.NE  published:2016-06-02 summary:In visual recognition tasks, supervised learning shows excellent performance. On the other hand, unsupervised learning exploits cheap unlabeled data and can help to solve the same tasks more efficiently. We show that the recursive autoconvolutional operator, adopted from physics, boosts existing unsupervised methods to learn more powerful filters. We use a well established multilayer convolutional network and train filters layer-wise. To build a stronger classifier, we design a very light committee of SVM models. The total number of trainable parameters is also greatly reduced by using shared filters in higher layers. We evaluate our networks on the MNIST, CIFAR-10 and STL-10 benchmarks and report several state of the art results among other unsupervised methods. version:1
arxiv-1603-02160 | Bayesian Learning of Kernel Embeddings | http://arxiv.org/abs/1603.02160 | id:1603.02160 author:Seth Flaxman, Dino Sejdinovic, John P. Cunningham, Sarah Filippi category:stat.ML  published:2016-03-07 summary:Kernel methods are one of the mainstays of machine learning, but the problem of kernel learning remains challenging, with only a few heuristics and very little theory. This is of particular importance in methods based on estimation of kernel mean embeddings of probability measures. For characteristic kernels, which include most commonly used ones, the kernel mean embedding uniquely determines its probability measure, so it can be used to design a powerful statistical testing framework, which includes nonparametric two-sample and independence tests. In practice, however, the performance of these tests can be very sensitive to the choice of kernel and its lengthscale parameters. To address this central issue, we propose a new probabilistic model for kernel mean embeddings, the Bayesian Kernel Embedding model, combining a Gaussian process prior over the Reproducing Kernel Hilbert Space containing the mean embedding with a conjugate likelihood function, thus yielding a closed form posterior over the mean embedding. The posterior mean of our model is closely related to recently proposed shrinkage estimators for kernel mean embeddings, while the posterior uncertainty is a new, interesting feature with various possible applications. Critically for the purposes of kernel learning, our model gives a simple, closed form marginal pseudolikelihood of the observed data given the kernel hyperparameters. This marginal pseudolikelihood can either be optimized to inform the hyperparameter choice or fully Bayesian inference can be used. version:2
arxiv-1606-00602 | Variance-Reduced Proximal Stochastic Gradient Descent for Non-convex Composite optimization | http://arxiv.org/abs/1606.00602 | id:1606.00602 author:Xiyu Yu, Dacheng Tao category:stat.ML cs.LG cs.NA  published:2016-06-02 summary:Here we study non-convex composite optimization: first, a finite-sum of smooth but non-convex functions, and second, a general function that admits a simple proximal mapping. Most research on stochastic methods for composite optimization assumes convexity or strong convexity of each function. In this paper, we extend this problem into the non-convex setting using variance reduction techniques, such as prox-SVRG and prox-SAGA. We prove that, with a constant step size, both prox-SVRG and prox-SAGA are suitable for non-convex composite optimization, and help the problem converge to a stationary point within $O(1/\epsilon)$ iterations. That is similar to the convergence rate seen with the state-of-the-art RSAG method and faster than stochastic gradient descent. Our analysis is also extended into the min-batch setting, which linearly accelerates the convergence. To the best of our knowledge, this is the first analysis of convergence rate of variance-reduced proximal stochastic gradient for non-convex composite optimization. version:1
arxiv-1606-00601 | On the performance of different mutation operators of a subpopulation-based genetic algorithm for multi-robot task allocation problems | http://arxiv.org/abs/1606.00601 | id:1606.00601 author:Chun Liu, Andreas Kroll category:cs.NE  published:2016-06-02 summary:The performance of different mutation operators is usually evaluated in conjunc-tion with specific parameter settings of genetic algorithms and target problems. Most studies focus on the classical genetic algorithm with different parameters or on solving unconstrained combinatorial optimization problems such as the traveling salesman problems. In this paper, a subpopulation-based genetic al-gorithm that uses only mutation and selection is developed to solve multi-robot task allocation problems. The target problems are constrained combinatorial optimization problems, and are more complex if cooperative tasks are involved as these introduce additional spatial and temporal constraints. The proposed genetic algorithm can obtain better solutions than classical genetic algorithms with tournament selection and partially mapped crossover. The performance of different mutation operators in solving problems without/with cooperative tasks is evaluated. The results imply that inversion mutation performs better than others when solving problems without cooperative tasks, and the swap-inversion combination performs better than others when solving problems with cooperative tasks. version:1
arxiv-1606-00589 | Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection | http://arxiv.org/abs/1606.00589 | id:1606.00589 author:Katharina Kann, Hinrich Schütze category:cs.CL  published:2016-06-02 summary:Morphological reinflection is the task of generating a target form given a source form, a source tag and a target tag. We propose a new way of modeling this task with neural encoder-decoder models. Our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results, making encoder-decoder models applicable to morphological reinflection even for low-resource languages. We further present a new automatic correction method for the outputs based on edit trees. version:1
arxiv-1511-05864 | Fast Saddle-Point Algorithm for Generalized Dantzig Selector and FDR Control with the Ordered l1-Norm | http://arxiv.org/abs/1511.05864 | id:1511.05864 author:Sangkyun Lee, Damian Brzyski, Malgorzata Bogdan category:stat.ML math.OC  published:2015-11-18 summary:In this paper we propose a primal-dual proximal extragradient algorithm to solve the generalized Dantzig selector (GDS) estimation problem, based on a new convex-concave saddle-point (SP) reformulation. Our new formulation makes it possible to adopt recent developments in saddle-point optimization, to achieve the optimal $O(1/k)$ rate of convergence. Compared to the optimal non-SP algorithms, ours do not require specification of sensitive parameters that affect algorithm performance or solution quality. We also provide a new analysis showing a possibility of local acceleration to achieve the rate of $O(1/k^2)$ in special cases even without strong convexity or strong smoothness. As an application, we propose a GDS equipped with the ordered $\ell_1$-norm, showing its false discovery rate control properties in variable selection. Algorithm performance is compared between ours and other alternatives, including the linearized ADMM, Nesterov's smoothing, Nemirovski's mirror-prox, and the accelerated hybrid proximal extragradient techniques. version:3
arxiv-1606-00577 | Source-LDA: Enhancing probabilistic topic models using prior knowledge sources | http://arxiv.org/abs/1606.00577 | id:1606.00577 author:Justin Wood category:cs.CL cs.IR cs.LG  published:2016-06-02 summary:A popular approach to topic modeling involves extracting co-occurring n-grams of a corpus into semantic themes. The set of n-grams in a theme represents an underlying topic, but most topic modeling approaches are not able to label these sets of words with a single n-gram. Such labels are useful for topic identification in summarization systems. This paper introduces a novel approach to labeling a group of n-grams comprising an individual topic. The approach taken is to complement the existing topic distributions over words with a known distribution based on a predefined set of topics. This is done by integrating existing labeled knowledge sources representing known potential topics into the probabilistic topic model. These knowledge sources are translated into a distribution and used to set the hyperparameters of the Dirichlet generated distribution over words. In the inference these modified distributions guide the convergence of the latent topics to conform with the complementary distributions. This approach ensures that the topic inference process is consistent with existing knowledge. The label assignment from the complementary knowledge sources are then transferred to the latent topics of the corpus. The results show both accurate label assignment to topics as well as improved topic generation than those obtained using various labeling approaches of Latent Dirichlet allocation (LDA) when compared by pointwise mutual information (PMI) assessment. version:1
arxiv-1606-00575 | Ensemble-Compression: A New Method for Parallel Training of Deep Neural Networks | http://arxiv.org/abs/1606.00575 | id:1606.00575 author:Shizhao Sun, Wei Chen, Tie-Yan Liu category:cs.DC cs.LG cs.NE  published:2016-06-02 summary:In recent year, parallel implementations have been used to speed up the training of deep neural networks (DNN). Typically, the parameters of the local models are periodically communicated and averaged to get a global model until the training curve converges (denoted as MA-DNN). However, since DNN is a highly non-convex model, the global model obtained by averaging parameters does not have guarantee on its performance improvement over the local models and might even be worse than the average performance of the local models, which leads to the slow-down of convergence and the decrease of the final performance. To tackle this problem, we propose a new parallel training method called \emph{Ensemble-Compression} (denoted as EC-DNN). Specifically, we propose to aggregate the local models by ensemble, i.e., the outputs of the local models are averaged instead of the parameters. Considering that the widely used loss functions are convex to the output of the model, the performance of the global model obtained in this way is guaranteed to be at least as good as the average performance of local models. However, the size of the global model will increase after each ensemble and may explode after multiple rounds of ensembles. Thus, we conduct model compression after each ensemble, to ensure the size of the global model to be the same as the local models. We conducted experiments on a benchmark dataset. The experimental results demonstrate that our proposed EC-DNN can stably achieve better performance than MA-DNN. version:1
arxiv-1602-00133 | SCOPE: Scalable Composite Optimization for Learning on Spark | http://arxiv.org/abs/1602.00133 | id:1602.00133 author:Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, Wu-Jun Li category:stat.ML cs.LG  published:2016-01-30 summary:Many machine learning models, such as logistic regression~(LR) and support vector machine~(SVM), can be formulated as composite optimization problems. Recently, many distributed stochastic optimization~(DSO) methods have been proposed to solve the large-scale composite optimization problems, which have shown better performance than traditional batch methods. However, most of these DSO methods are not scalable enough. In this paper, we propose a novel DSO method, called \underline{s}calable \underline{c}omposite \underline{op}timization for l\underline{e}arning~({SCOPE}), and implement it on the fault-tolerant distributed platform \mbox{Spark}. SCOPE is both computation-efficient and communication-efficient. Theoretical analysis shows that SCOPE is convergent with linear convergence rate when the objective function is convex. Furthermore, empirical results on real datasets show that SCOPE can outperform other state-of-the-art distributed learning methods on Spark, including both batch learning methods and DSO methods. version:4
arxiv-1509-01817 | On collapsed representation of hierarchical Completely Random Measures | http://arxiv.org/abs/1509.01817 | id:1509.01817 author:Gaurav Pandey, Ambedkar Dukkipati category:math.ST cs.LG stat.TH  published:2015-09-06 summary:The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures~(CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topic-modelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP. version:2
arxiv-1511-06393 | Fixed Point Quantization of Deep Convolutional Networks | http://arxiv.org/abs/1511.06393 | id:1511.06393 author:Darryl D. Lin, Sachin S. Talathi, V. Sreekanth Annapureddy category:cs.LG  published:2015-11-19 summary:In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we propose a quantizer design for fixed point implementation of DCNs. We formulate and solve an optimization problem to identify optimal fixed point bit-width allocation across DCN layers. Our experiments show that in comparison to equal bit-width settings, the fixed point DCNs with optimized bit width allocation offer >20% reduction in the model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark. version:3
arxiv-1606-00546 | Forecasting wind power - Modeling periodic and non-linear effects under conditional heteroscedasticity | http://arxiv.org/abs/1606.00546 | id:1606.00546 author:Florian Ziel, Carsten Croonenbroeck, Daniel Ambach category:stat.AP stat.CO stat.ML stat.OT 62P12  62M10  62J07 G.3  published:2016-06-02 summary:In this article we present an approach that enables joint wind speed and wind power forecasts for a wind park. We combine a multivariate seasonal time varying threshold autoregressive moving average (TVARMA) model with a power threshold generalized autoregressive conditional heteroscedastic (power-TGARCH) model. The modeling framework incorporates diurnal and annual periodicity modeling by periodic B-splines, conditional heteroscedasticity and a complex autoregressive structure with non-linear impacts. In contrast to usually time-consuming estimation approaches as likelihood estimation, we apply a high-dimensional shrinkage technique. We utilize an iteratively re-weighted least absolute shrinkage and selection operator (lasso) technique. It allows for conditional heteroscedasticity, provides fast computing times and guarantees a parsimonious and regularized specification, even though the parameter space may be vast. We are able to show that our approach provides accurate forecasts of wind power at a turbine-specific level for forecasting horizons of up to 48 h (short- to medium-term forecasts). version:1
arxiv-1606-00540 | Multi-pretrained Deep Neural Network | http://arxiv.org/abs/1606.00540 | id:1606.00540 author:Zhen Hu, Zhuyin Xue, Tong Cui, Shiqiang Zong, Chenglong He category:cs.NE cs.LG  published:2016-06-02 summary:Pretraining is widely used in deep neutral network and one of the most famous pretraining models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. The experimental results show that DBN get a better initial model. However the model converges to a relatively worse model after the finetuning process. Yet after pretrained by SDA for the second time the model converges to a better model if finetuned. version:1
arxiv-1606-00538 | Dictionary Learning for Robotic Grasp Recognition and Detection | http://arxiv.org/abs/1606.00538 | id:1606.00538 author:Ludovic Trottier, Philippe Giguère, Brahim Chaib-draa category:cs.RO cs.CV  published:2016-06-02 summary:The ability to grasp ordinary and potentially never-seen objects is an important feature in both domestic and industrial robotics. For a system to accomplish this, it must autonomously identify grasping locations by using information from various sensors, such as Microsoft Kinect 3D camera. Despite numerous progress, significant work still remains to be done in this field. To this effect, we propose a dictionary learning and sparse representation (DLSR) framework for representing RGBD images from 3D sensors in the context of determining such good grasping locations. In contrast to previously proposed approaches that relied on sophisticated regularization or very large datasets, the derived perception system has a fast training phase and can work with small datasets. It is also theoretically founded for dealing with masked-out entries, which are common with 3D sensors. We contribute by presenting a comparative study of several DLSR approach combinations for recognizing and detecting grasp candidates on the standard Cornell dataset. Importantly, experimental results show a performance improvement of 1.69% in detection and 3.16% in recognition over current state-of-the-art convolutional neural network (CNN). Even though nowadays most popular vision-based approach is CNN, this suggests that DLSR is also a viable alternative with interesting advantages that CNN has not. version:1
arxiv-1605-09522 | Kernel Mean Embedding of Distributions: A Review and Beyonds | http://arxiv.org/abs/1605.09522 | id:1605.09522 author:Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Bernhard Schölkopf category:stat.ML cs.LG  published:2016-05-31 summary:A Hilbert space embedding of distributions---in short, kernel mean embedding---has recently emerged as a powerful machinery for probabilistic modeling, statistical inference, machine learning, and causal discovery. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It gave rise to a great deal of research and novel applications of positive definite kernels. The goal of this survey is to give a comprehensive review of existing works and recent advances in this research area, and to discuss some of the most challenging issues and open problems that could potentially lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, group anomaly detection, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way using the new representation of distributions in RKHS. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions. version:2
arxiv-1511-03328 | Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform | http://arxiv.org/abs/1511.03328 | id:1511.03328 author:Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy, Alan L. Yuille category:cs.CV  published:2015-11-10 summary:Deep convolutional neural networks (CNNs) are the backbone of state-of-art semantic image segmentation systems. Recent work has shown that complementing CNNs with fully-connected conditional random fields (CRFs) can significantly enhance their object localization accuracy, yet dense CRF inference is computationally expensive. We propose replacing the fully-connected CRF with domain transform (DT), a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Domain transform filtering is several times faster than dense CRF inference and we show that it yields comparable semantic segmentation results, accurately capturing object boundaries. Importantly, our formulation allows learning the reference edge map from intermediate CNN features instead of using the image gradient magnitude as in standard DT filtering. This produces task-specific edges in an end-to-end trainable system optimizing the target semantic segmentation quality. version:2
arxiv-1511-03339 | Attention to Scale: Scale-aware Semantic Image Segmentation | http://arxiv.org/abs/1511.03339 | id:1511.03339 author:Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille category:cs.CV  published:2015-11-10 summary:Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014. version:2
arxiv-1606-00061 | Hierarchical Question-Image Co-Attention for Visual Question Answering | http://arxiv.org/abs/1606.00061 | id:1606.00061 author:Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh category:cs.CV cs.CL  published:2016-05-31 summary:A number of recent works have proposed attention models for Visual Question Answering (VQA) that generate spatial maps highlighting image regions relevant to answering the question. In this paper, we argue that in addition to modeling "where to look" or visual attention, it is equally important to model "what words to listen to" or question attention. We present a novel co-attention model for VQA that jointly reasons about image and question attention. In addition, our model reasons about the question and consequently the image via the co-attention mechanism in a hierarchical fashion via a novel 1-dimensional convolution neural networks (CNN) model. Our final model outperforms all reported methods, improving the state-of-the-art on the VQA dataset from 60.4% to 62.1%, and from 61.6% to 65.4% on the COCO-QA dataset. version:2
arxiv-1403-2660 | Robust and Scalable Bayes via a Median of Subset Posterior Measures | http://arxiv.org/abs/1403.2660 | id:1403.2660 author:Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, David B. Dunson category:math.ST cs.DC cs.LG stat.TH  published:2014-03-11 summary:We propose a novel approach to Bayesian analysis that is provably robust to outliers in the data and often has computational advantages over standard methods. Our technique is based on splitting the data into non-overlapping subgroups, evaluating the posterior distribution given each independent subgroup, and then combining the resulting measures. The main novelty of our approach is the proposed aggregation step, which is based on the evaluation of a median in the space of probability measures equipped with a suitable collection of distances that can be quickly and efficiently evaluated in practice. We present both theoretical and numerical evidence illustrating the improvements achieved by our method. version:3
arxiv-1511-06411 | Training Deep Neural Networks via Direct Loss Minimization | http://arxiv.org/abs/1511.06411 | id:1511.06411 author:Yang Song, Alexander G. Schwing, Richard S. Zemel, Raquel Urtasun category:cs.LG  published:2015-11-19 summary:Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on metrics specific to the application. In this paper we propose a direct loss minimization approach to train deep neural networks, which provably minimizes the application-specific loss function. This is often non-trivial, since these functions are neither smooth nor decomposable and thus are not amenable to optimization with standard gradient-based methods. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we develop a novel dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection, especially in the presence of label noise. version:2
arxiv-1606-00511 | Large Scale Distributed Hessian-Free Optimization for Deep Neural Network | http://arxiv.org/abs/1606.00511 | id:1606.00511 author:Xi He, Dheevatsa Mudigere, Mikhail Smelyanskiy, Martin Takáč category:cs.LG cs.DC math.OC  published:2016-06-02 summary:Training deep neural network is a high dimensional and a highly non-convex optimization problem. Stochastic gradient descent (SGD) algorithm and it's variations are the current state-of-the-art solvers for this task. However, due to non-covexity nature of the problem, it was observed that SGD slows down near saddle point. Recent empirical work claim that by detecting and escaping saddle point efficiently, it's more likely to improve training performance. With this objective, we revisit Hessian-free optimization method for deep networks. We also develop its distributed variant and demonstrate superior scaling potential to SGD, which allows more efficiently utilizing larger computing resources thus enabling large models and faster time to obtain desired solution. Furthermore, unlike truncated Newton method (Marten's HF) that ignores negative curvature information by using na\"ive conjugate gradient method and Gauss-Newton Hessian approximation information - we propose a novel algorithm to explore negative curvature direction by solving the sub-problem with stabilized bi-conjugate method involving possible indefinite stochastic Hessian information. We show that these techniques accelerate the training process for both the standard MNIST dataset and also the TIMIT speech recognition problem, demonstrating robust performance with upto an order of magnitude larger batch sizes. This increased scaling potential is illustrated with near linear speed-up on upto 16 CPU nodes for a simple 4-layer network. version:1
arxiv-1602-07726 | Adaptive Learning with Robust Generalization Guarantees | http://arxiv.org/abs/1602.07726 | id:1602.07726 author:Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, Zhiwei Steven Wu category:cs.DS cs.LG  published:2016-02-24 summary:The traditional notion of generalization---i.e., learning a hypothesis whose empirical error is close to its true error---is surprisingly brittle. As has recently been noted in [DFH+15b], even if several algorithms have this guarantee in isolation, the guarantee need not hold if the algorithms are composed adaptively. In this paper, we study three notions of generalization---increasing in strength---that are robust to postprocessing and amenable to adaptive composition, and examine the relationships between them. We call the weakest such notion Robust Generalization. A second, intermediate, notion is the stability guarantee known as differential privacy. The strongest guarantee we consider we call Perfect Generalization. We prove that every hypothesis class that is PAC learnable is also PAC learnable in a robustly generalizing fashion, with almost the same sample complexity. It was previously known that differentially private algorithms satisfy robust generalization. In this paper, we show that robust generalization is a strictly weaker concept, and that there is a learning task that can be carried out subject to robust generalization guarantees, yet cannot be carried out subject to differential privacy. We also show that perfect generalization is a strictly stronger guarantee than differential privacy, but that, nevertheless, many learning tasks can be carried out subject to the guarantees of perfect generalization. version:2
arxiv-1603-00622 | PLATO: Policy Learning using Adaptive Trajectory Optimization | http://arxiv.org/abs/1603.00622 | id:1603.00622 author:Gregory Kahn, Tianhao Zhang, Sergey Levine, Pieter Abbeel category:cs.LG  published:2016-03-02 summary:Policy search can in principle acquire complex strategies for control of robots and other autonomous systems. When the policy is trained to process raw sensory inputs, such as images and depth maps, it can acquire a strategy that combines perception and control. However, effectively processing such complex inputs requires an expressive policy class, such as a large neural network. These high-dimensional policies are difficult to train, especially when training must be done for safety-critical systems. We propose PLATO, an algorithm that trains complex control policies with supervised learning, using model-predictive control (MPC) to generate the supervision. PLATO uses an adaptive training method to modify the behavior of MPC to gradually match the learned policy, in order to generate training samples at states that are likely to be visited by the policy while avoiding highly undesirable on-policy actions. We prove that this type of adaptive MPC expert produces supervision that leads to good long-horizon performance of the resulting policy. We also empirically demonstrate that MPC can still avoid dangerous on-policy actions in unexpected situations during training. Our empirical results on a set of challenging simulated aerial vehicle tasks demonstrate that, compared to prior methods, PLATO learns faster, experiences substantially fewer catastrophic failures (crashes) during training, and often converges to a better policy. version:2
arxiv-1606-00499 | Generalizing and Hybridizing Count-based and Neural Language Models | http://arxiv.org/abs/1606.00499 | id:1606.00499 author:Graham Neubig, Chris Dyer category:cs.CL  published:2016-06-01 summary:Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches. version:1
arxiv-1606-00496 | On the equivalence between Kolmogorov-Smirnov and ROC curve metrics for binary classification | http://arxiv.org/abs/1606.00496 | id:1606.00496 author:Paulo J. L. Adeodato, Sílvio B. Melo category:cs.AI cs.CV I.2; I.5.2  published:2016-06-01 summary:Binary decisions are very common in artificial intelligence. Applying a threshold on the continuous score gives the human decider the power to control the operating point to separate the two classes. The classifier,s discriminating power is measured along the continuous range of the score by the Area Under the ROC curve (AUC_ROC) in most application fields. Only finances uses the poor single point metric maximum Kolmogorov-Smirnov (KS) distance. This paper proposes the Area Under the KS curve (AUC_KS) for performance assessment and proves AUC_ROC = 0.5 + AUC_KS, as a simpler way to calculate the AUC_ROC. That is even more important for ROC averaging in ensembles of classifiers or n fold cross-validation. The proof is geometrically inspired on rotating all KS curve to make it lie on the top of the ROC chance diagonal. On the practical side, the independent variable on the abscissa on the KS curve simplifies the calculation of the AUC_ROC. On the theoretical side, this research gives insights on probabilistic interpretations of classifiers assessment and integrates the existing body of knowledge of the information theoretical ROC approach with the proposed statistical approach based on the thoroughly known KS distribution. version:1
arxiv-1606-00474 | A 3D Face Modelling Approach for Pose-Invariant Face Recognition in a Human-Robot Environment | http://arxiv.org/abs/1606.00474 | id:1606.00474 author:Michael Grupp, Philipp Kopp, Patrik Huber, Matthias Rätsch category:cs.CV cs.HC cs.RO 68T45  68T40  68T10  published:2016-06-01 summary:Face analysis techniques have become a crucial component of human-machine interaction in the fields of assistive and humanoid robotics. However, the variations in head-pose that arise naturally in these environments are still a great challenge. In this paper, we present a real-time capable 3D face modelling framework for 2D in-the-wild images that is applicable for robotics. The fitting of the 3D Morphable Model is based exclusively on automatically detected landmarks. After fitting, the face can be corrected in pose and transformed back to a frontal 2D representation that is more suitable for face recognition. We conduct face recognition experiments with non-frontal images from the MUCT database and uncontrolled, in the wild images from the PaSC database, the most challenging face recognition database to date, showing an improved performance. Finally, we present our SCITOS G5 robot system, which incorporates our framework as a means of image pre-processing for face analysis. version:1
arxiv-1606-00451 | Graph-Guided Banding of the Covariance Matrix | http://arxiv.org/abs/1606.00451 | id:1606.00451 author:Jacob Bien category:stat.ME math.ST stat.CO stat.ML stat.TH  published:2016-06-01 summary:Regularization has become a primary tool for developing reliable estimators of the covariance matrix in high-dimensional settings. To curb the curse of dimensionality, numerous methods assume that the population covariance (or inverse covariance) matrix is sparse, while making no particular structural assumptions on the desired pattern of sparsity. A highly-related, yet complementary, literature studies the specific setting in which the measured variables have a known ordering, in which case a banded population matrix is often assumed. While the banded approach is conceptually and computationally easier than asking for "patternless sparsity," it is only applicable in very specific situations (such as when data are measured over time or one-dimensional space). This work proposes a generalization of the notion of bandedness that greatly expands the range of problems in which banded estimators apply. We develop convex regularizers occupying the broad middle ground between the former approach of "patternless sparsity" and the latter reliance on having a known ordering. Our framework defines bandedness with respect to a known graph on the measured variables. Such a graph is available in diverse situations, and we provide a theoretical, computational, and applied treatment of two new estimators. version:1
arxiv-1606-00411 | Temporal Topic Modeling to Assess Associations between News Trends and Infectious Disease Outbreaks | http://arxiv.org/abs/1606.00411 | id:1606.00411 author:Saurav Ghosh, Prithwish Chakraborty, Elaine O. Nsoesie, Emily Cohn, Sumiko R. Mekaru, John S. Brownstein, Naren Ramakrishnan category:cs.SI cs.CL cs.IR stat.ML  published:2016-06-01 summary:In retrospective assessments, internet news reports have been shown to capture early reports of unknown infectious disease transmission prior to official laboratory confirmation. In general, media interest and reporting peaks and wanes during the course of an outbreak. In this study, we quantify the extent to which media interest during infectious disease outbreaks is indicative of trends of reported incidence. We introduce an approach that uses supervised temporal topic models to transform large corpora of news articles into temporal topic trends. The key advantages of this approach include, applicability to a wide range of diseases, and ability to capture disease dynamics - including seasonality, abrupt peaks and troughs. We evaluated the method using data from multiple infectious disease outbreaks reported in the United States of America (U.S.), China and India. We noted that temporal topic trends extracted from disease-related news reports successfully captured the dynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue outbreaks in India (2013) and China (2014). Our observations also suggest that efficient modeling of temporal topic trends using time-series regression techniques can estimate disease case counts with increased precision before official reports by health organizations. version:1
arxiv-1511-03243 | Black-box $α$-divergence Minimization | http://arxiv.org/abs/1511.03243 | id:1511.03243 author:José Miguel Hernández-Lobato, Yingzhen Li, Mark Rowland, Daniel Hernández-Lobato, Thang Bui, Richard E. Turner category:stat.ML  published:2015-11-10 summary:Black-box alpha (BB-$\alpha$) is a new approximate inference method based on the minimization of $\alpha$-divergences. BB-$\alpha$ scales to large datasets because it can be implemented using stochastic gradient descent. BB-$\alpha$ can be applied to complex probabilistic models with little effort since it only requires as input the likelihood function and its gradients. These gradients can be easily obtained using automatic differentiation. By changing the divergence parameter $\alpha$, the method is able to interpolate between variational Bayes (VB) ($\alpha \rightarrow 0$) and an algorithm similar to expectation propagation (EP) ($\alpha = 1$). Experiments on probit regression and neural network regression and classification problems show that BB-$\alpha$ with non-standard settings of $\alpha$, such as $\alpha = 0.5$, usually produces better predictions than with $\alpha \rightarrow 0$ (VB) or $\alpha = 1$ (EP). version:3
arxiv-1606-00399 | Scaling Submodular Maximization via Pruned Submodularity Graphs | http://arxiv.org/abs/1606.00399 | id:1606.00399 author:Tianyi Zhou, Hua Ouyang, Yi Chang, Jeff Bilmes, Carlos Guestrin category:cs.LG math.CO stat.ML  published:2016-06-01 summary:We propose a new random pruning method (called "submodular sparsification (SS)") to reduce the cost of submodular maximization. The pruning is applied via a "submodularity graph" over the $n$ ground elements, where each directed edge is associated with a pairwise dependency defined by the submodular function. In each step, SS prunes a $1-1/\sqrt{c}$ (for $c>1$) fraction of the nodes using weights on edges computed based on only a small number ($O(\log n)$) of randomly sampled nodes. The algorithm requires $\log_{\sqrt{c}}n$ steps with a small and highly parallelizable per-step computation. An accuracy-speed tradeoff parameter $c$, set as $c = 8$, leads to a fast shrink rate $\sqrt{2}/4$ and small iteration complexity $\log_{2\sqrt{2}}n$. Analysis shows that w.h.p., the greedy algorithm on the pruned set of size $O(\log^2 n)$ can achieve a guarantee similar to that of processing the original dataset. In news and video summarization tasks, SS is able to substantially reduce both computational costs and memory usage, while maintaining (or even slightly exceeding) the quality of the original (and much more costly) greedy algorithm. version:1
arxiv-1606-00398 | Short Communication on QUIST: A Quick Clustering Algorithm | http://arxiv.org/abs/1606.00398 | id:1606.00398 author:Sherenaz W. Al-Haj Baddar category:cs.LG stat.ML 68T05  published:2016-06-01 summary:In this short communication we introduce the quick clustering algorithm (QUIST), an efficient hierarchical clustering algorithm based on sorting. QUIST is a poly-logarithmic divisive clustering algorithm that does not assume the number of clusters, and/or the cluster size to be known ahead of time. It is also insensitive to the original ordering of the input. version:1
arxiv-1606-00389 | Stream Clipper: Scalable Submodular Maximization on Stream | http://arxiv.org/abs/1606.00389 | id:1606.00389 author:Tianyi Zhou, Jeff Bilmes category:stat.ML cs.LG math.CO  published:2016-06-01 summary:Applying submodular maximization in the streaming setting is nontrivial because the commonly used greedy algorithm exceeds the fixed memory and computational limits typically needed during stream processing. We introduce a new algorithm, called stream clipper, that uses two thresholds to select elements either into a solution set $S$ or an extra buffer $B$. The output is achieved by a greedy algorithm that starts from $S$ and then, if needed, greedily adds elements from $B$. Swapping elements out of $S$ may also be triggered lazily for further improvements, and elements may also be removed from $B$ (and corresponding thresholds adjusted) in order to keep memory use bounded by a constant. Although the worst-case approximation factor does not outperform the previous worst-case of $1/2$, stream clipper can perform better than $1/2$ depending on the order of the elements in the stream. We develop the idea of an "order complexity" to characterize orders on which an approximation factor of $1-\alpha$ can be achieved. In news and video summarization tasks, stream clipper significantly outperforms other streaming methods. It shows similar performance to the greedy algorithm but with less computation and memory costs. version:1
arxiv-1511-02799 | Neural Module Networks | http://arxiv.org/abs/1511.02799 | id:1511.02799 author:Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein category:cs.CV cs.CL cs.LG cs.NE  published:2015-11-09 summary:Visual question answering is fundamentally compositional in nature---a question like "where is the dog?" shares substructure with questions like "what color is the dog?" and "where is the cat?" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes. version:3
arxiv-1605-08062 | A PAC RL Algorithm for Episodic POMDPs | http://arxiv.org/abs/1605.08062 | id:1605.08062 author:Zhaohan Daniel Guo, Shayan Doroudi, Emma Brunskill category:cs.LG cs.AI stat.ML  published:2016-05-25 summary:Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in method of moments for latent variable model estimation. version:2
arxiv-1602-03828 | Community Recovery in Graphs with Locality | http://arxiv.org/abs/1602.03828 | id:1602.03828 author:Yuxin Chen, Govinda Kamath, Changho Suh, David Tse category:cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH  published:2016-02-11 summary:Motivated by applications in domains such as social networks and computational biology, we study the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all nodes pairs, as in most existing models. We present an algorithm that runs nearly linearly in the number of measurements and which achieves the information theoretic limit for exact recovery. version:3
arxiv-1606-00373 | Deeper Depth Prediction with Fully Convolutional Residual Networks | http://arxiv.org/abs/1606.00373 | id:1606.00373 author:Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, Nassir Navab category:cs.CV  published:2016-06-01 summary:This paper addresses the problem of estimating the depth map of a scene given a single RGB image. To model the ambiguous mapping between monocular images and depth maps, we leverage on deep learning capabilities and present a fully convolutional architecture encompassing residual learning. The proposed model is deeper than the current state of the art, but contains fewer parameters and requires less training data, while still outperforming all current CNN approaches aimed at the same task. We further present a novel way to efficiently learn feature map up-sampling within the network. For optimization we introduce the reverse Huber loss, particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. The predictions are given by a single architecture, trained end-to-end, that does not rely on post-processing techniques, such as CRFs or other additional refinement steps. version:1
arxiv-1606-00372 | Conversational Contextual Cues: The Case of Personalization and History for Response Ranking | http://arxiv.org/abs/1606.00372 | id:1606.00372 author:Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun-hsuan Sung, Brian Strope, Ray Kurzweil category:cs.CL cs.LG  published:2016-06-01 summary:We investigate the task of modeling open-domain, multi-turn, unstructured, multi-participant, conversational dialogue. We specifically study the effect of incorporating different elements of the conversation. Unlike previous efforts, which focused on modeling messages and responses, we extend the modeling to long context and participant's history. Our system does not rely on handwritten rules or engineered features; instead, we train deep neural networks on a large conversational dataset. In particular, we exploit the structure of Reddit comments and posts to extract 2.1 billion messages and 133 million conversations. We evaluate our models on the task of predicting the next response in a conversation, and we find that modeling both context and participants improves prediction accuracy. version:1
arxiv-1606-00370 | Decoding Emotional Experience through Physiological Signal Processing | http://arxiv.org/abs/1606.00370 | id:1606.00370 author:Maria S. Perez-Rosero, Behnaz Rezaei, Murat Akcakaya, Sarah Ostadabbas category:cs.HC cs.LG  published:2016-06-01 summary:There is an increasing consensus among re- searchers that making a computer emotionally intelligent with the ability to decode human affective states would allow a more meaningful and natural way of human-computer interactions (HCIs). One unobtrusive and non-invasive way of recognizing human affective states entails the exploration of how physiological signals vary under different emotional experiences. In particular, this paper explores the correlation between autonomically-mediated changes in multimodal body signals and discrete emotional states. In order to fully exploit the information in each modality, we have provided an innovative classification approach for three specific physiological signals including Electromyogram (EMG), Blood Volume Pressure (BVP) and Galvanic Skin Response (GSR). These signals are analyzed as inputs to an emotion recognition paradigm based on fusion of a series of weak learners. Our proposed classification approach showed 88.1% recognition accuracy, which outperformed the conventional Support Vector Machine (SVM) classifier with 17% accuracy improvement. Furthermore, in order to avoid information redundancy and the resultant over-fitting, a feature reduction method is proposed based on a correlation analysis to optimize the number of features required for training and validating each weak learner. Results showed that despite the feature space dimensionality reduction from 27 to 18 features, our methodology preserved the recognition accuracy of about 85.0%. This reduction in complexity will get us one step closer towards embedding this human emotion encoder in the wireless and wearable HCI platforms. version:1
arxiv-1605-07129 | Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries | http://arxiv.org/abs/1605.07129 | id:1605.07129 author:Stanislav Minsker category:math.ST stat.ML stat.TH  published:2016-05-23 summary:Estimation of the covariance matrix has attracted a lot of attention of the statistical research community over the years, partially due to important applications such as Principal Component Analysis. However, frequently used empirical covariance estimator (and its modifications) is very sensitive to outliers in the data. As P. J. Huber wrote in 1964, "...This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): what happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance..." Motivated by this question, we develop a new estimator of the (element-wise) mean of a random matrix, which includes covariance estimation problem as a special case. Assuming that the entries of a matrix possess only finite second moment, this new estimator admits sub-Gaussian or sub-exponential concentration around the unknown mean in the operator norm. We will explain the key ideas behind our construction, as well as applications to covariance estimation and matrix completion problems. version:2
arxiv-1602-06725 | Variational inference for Monte Carlo objectives | http://arxiv.org/abs/1602.06725 | id:1602.06725 author:Andriy Mnih, Danilo J. Rezende category:cs.LG stat.ML  published:2016-02-22 summary:Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizing a lower bound on the log-likelihood, using samples from the variational posterior to compute the required gradients. Recently, Burda et al. (2016) have derived a tighter lower bound using a multi-sample importance sampling estimate of the likelihood and showed that optimizing it yields models that use more of their capacity and achieve higher likelihoods. This development showed the importance of such multi-sample objectives and explained the success of several related approaches. We extend the multi-sample approach to discrete latent variables and analyze the difficulty encountered when estimating the gradients involved. We then develop the first unbiased gradient estimator designed for importance-sampled objectives and evaluate it at training generative and structured output prediction models. The resulting estimator, which is based on low-variance per-sample learning signals, is both simpler and more effective than the NVIL estimator proposed for the single-sample variational objective, and is competitive with the currently used biased estimators. version:2
arxiv-1602-04741 | Delay and Cooperation in Nonstochastic Bandits | http://arxiv.org/abs/1602.04741 | id:1602.04741 author:Nicolo' Cesa-Bianchi, Claudio Gentile, Yishay Mansour, Alberto Minora category:cs.LG  published:2016-02-15 summary:We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than $d$ hops to arrive, where $d$ is a delay parameter. We introduce \textsc{Exp3-Coop}, a cooperative version of the {\sc Exp3} algorithm and prove that with $K$ actions and $N$ agents the average per-agent regret after $T$ rounds is at most of order $\sqrt{\bigl(d+1 + \tfrac{K}{N}\alpha_{\le d}\bigr)(T\ln K)}$, where $\alpha_{\le d}$ is the independence number of the $d$-th power of the connected communication graph $G$. We then show that for any connected graph, for $d=\sqrt{K}$ the regret bound is $K^{1/4}\sqrt{T}$, strictly better than the minimax regret $\sqrt{KT}$ for noncooperating agents. More informed choices of $d$ lead to bounds which are arbitrarily close to the full information minimax regret $\sqrt{T\ln K}$ when $G$ is dense. When $G$ has sparse components, we show that a variant of \textsc{Exp3-Coop}, allowing agents to choose their parameters according to their centrality in $G$, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay. version:2
arxiv-1606-00313 | Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits | http://arxiv.org/abs/1606.00313 | id:1606.00313 author:Vasilis Syrgkanis, Haipeng Luo, Akshay Krishnamurthy, Robert E. Schapire category:cs.LG  published:2016-06-01 summary:We give an oracle-based algorithm for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order $O((KT)^{\frac{2}{3}}(\log N)^{\frac{1}{3}})$, where $K$ is the number of actions, $T$ is the number of iterations and $N$ is the number of baseline policies. Our result is the first to break the $O(T^{\frac{3}{4}})$ barrier that is achieved by recently introduced algorithms. Breaking this barrier was left as a major open problem. Our analysis is based on the recent relaxation based approach of (Rakhlin and Sridharan, 2016). version:1
arxiv-1606-00305 | Improving Deep Neural Network with Multiple Parametric Exponential Linear Units | http://arxiv.org/abs/1606.00305 | id:1606.00305 author:Yang Li, Chunxiao Fan, Yong Li, Qiong Wu category:cs.CV  published:2016-06-01 summary:Activation function is crucial to the recent successes of neural network. In this paper, we propose a new activation function that generalizes and unifies the rectified and exponential linear units. The proposed method, named MPELU, has the advantages of PReLU and ELU. We show that on the CIFAR-10 dataset, MPELU network converges fast and improves the performance for image classification. On the ImageNet dataset, MPELU provides better generalization capability than PReLU. Furthermore, we put forward a way of initialization suitable for exponential linear units. To the best of our knowledge, the proposed initialization is the first one for exponential-linear-unit networks. Experiments prove that our initialization is helpful to train very deep exponential-linear-unit networks. version:1
arxiv-1602-04589 | Optimal Best Arm Identification with Fixed Confidence | http://arxiv.org/abs/1602.04589 | id:1602.04589 author:Aurélien Garivier, Emilie Kaufmann category:math.ST cs.LG stat.ML stat.TH  published:2016-02-15 summary:We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the `Track-and-Stop' strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis. version:2
arxiv-1606-00298 | Automatic tagging using deep convolutional neural networks | http://arxiv.org/abs/1606.00298 | id:1606.00298 author:Keunwoo Choi, George Fazekas, Mark Sandler category:cs.SD cs.LG  published:2016-06-01 summary:We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data. version:1
arxiv-1606-00294 | Improved Parsing for Argument-Clusters Coordination | http://arxiv.org/abs/1606.00294 | id:1606.00294 author:Jessica Ficler, Yoav Goldberg category:cs.CL  published:2016-06-01 summary:Syntactic parsers perform poorly in prediction of Argument-Cluster Coordination (ACC). We change the PTB representation of ACC to be more suitable for learning by a statistical PCFG parser, affecting 125 trees in the training set. Training on the modified trees yields a slight improvement in EVALB scores on sections 22 and 23. The main evaluation is on a corpus of 4th grade science exams, in which ACC structures are prevalent. On this corpus, we obtain an impressive x2.7 improvement in recovering ACC structures compared to a parser trained on the original PTB trees. version:1
arxiv-1606-00282 | Multi-Label Zero-Shot Learning via Concept Embedding | http://arxiv.org/abs/1606.00282 | id:1606.00282 author:Ubai Sandouk, Ke Chen category:cs.LG 68T05 I.2.6  published:2016-06-01 summary:Zero Shot Learning (ZSL) enables a learning model to classify instances of an unseen class during training. While most research in ZSL focuses on single-label classification, few studies have been done in multi-label ZSL, where an instance is associated with a set of labels simultaneously, due to the difficulty in modeling complex semantics conveyed by a set of labels. In this paper, we propose a novel approach to multi-label ZSL via concept embedding learned from collections of public users' annotations of multimedia. Thanks to concept embedding, multi-label ZSL can be done by efficiently mapping an instance input features onto the concept embedding space in a similar manner used in single-label ZSL. Moreover, our semantic learning model is capable of embedding an out-of-vocabulary label by inferring its meaning from its co-occurring labels. Thus, our approach allows both seen and unseen labels during the concept embedding learning to be used in the aforementioned instance mapping, which makes multi-label ZSL more flexible and suitable for real applications. Experimental results of multi-label ZSL on images and music tracks suggest that our approach outperforms a state-of-the-art multi-label ZSL model and can deal with a scenario involving out-of-vocabulary labels without re-training the semantics learning model. version:1
arxiv-1606-00265 | Finding Singular Features | http://arxiv.org/abs/1606.00265 | id:1606.00265 author:Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:stat.ME stat.ML  published:2016-06-01 summary:We present a method for finding high density, low-dimensional structures in noisy point clouds. These structures are sets with zero Lebesgue measure with respect to the $D$-dimensional ambient space and belong to a $d<D$ dimensional space. We call them "singular features." Hunting for singular features corresponds to finding unexpected or unknown structures hidden in point clouds belonging to $\R^D$. Our method outputs well defined sets of dimensions $d<D$. Unlike spectral clustering, the method works well in the presence of noise. We show how to find singular features by first finding ridges in the estimated density, followed by a filtering step based on the eigenvalues of the Hessian of the density. version:1
arxiv-1606-00253 | On a Topic Model for Sentences | http://arxiv.org/abs/1606.00253 | id:1606.00253 author:Georgios Balikas, Massih-Reza Amini, Marianne Clausel category:cs.CL cs.IR cs.LG  published:2016-06-01 summary:Probabilistic topic models are generative models that describe the content of documents by discovering the latent topics underlying them. However, the structure of the textual input, and for instance the grouping of words in coherent text spans such as sentences, contains much information which is generally lost with these models. In this paper, we propose sentenceLDA, an extension of LDA whose goal is to overcome this limitation by incorporating the structure of the text in the generative and inference processes. We illustrate the advantages of sentenceLDA by comparing it with LDA using both intrinsic (perplexity) and extrinsic (text classification) evaluation tasks on different text collections. version:1
arxiv-1601-06733 | Long Short-Term Memory-Networks for Machine Reading | http://arxiv.org/abs/1601.06733 | id:1601.06733 author:Jianpeng Cheng, Li Dong, Mirella Lapata category:cs.CL cs.NE  published:2016-01-25 summary:Machine reading, the automatic understanding of text, remains a challenging task of great value for NLP applications. We propose a machine reader which processes text incrementally from left to right, while linking the current word to previous words stored in memory and implicitly discovering lexical dependencies facilitating understanding. The reader is equipped with a Long Short-Term Memory architecture, which differs from previous work in that it has a memory tape (instead of a memory cell) for adaptively storing past information without severe information compression. We also integrate our reader with a new attention mechanism in encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art. version:6
arxiv-1506-07868 | The local convexity of solving systems of quadratic equations | http://arxiv.org/abs/1506.07868 | id:1506.07868 author:Chris D. White, Sujay Sanghavi, Rachel Ward category:math.NA math.OC stat.ML  published:2015-06-25 summary:This paper considers the recovery of a rank $r$ positive semidefinite matrix $X X^T\in\mathbb{R}^{n\times n}$ from $m$ scalar measurements of the form $y_i := a_i^T X X^T a_i$ (i.e., quadratic measurements of $X$). Such problems arise in a variety of applications, including covariance sketching of high-dimensional data streams, quadratic regression, quantum state tomography, among others. A natural approach to this problem is to minimize the loss function $f(U) = \sum_i (y_i - a_i^TUU^Ta_i)^2$ which has an entire manifold of solutions given by $\{XO\}_{O\in\mathcal{O}_r}$ where $\mathcal{O}_r$ is the orthogonal group of $r\times r$ orthogonal matrices; this is {\it non-convex} in the $n\times r$ matrix $U$, but methods like gradient descent are simple and easy to implement (as compared to semidefinite relaxation approaches). In this paper we show that once we have $m \geq C nr \log^2(n)$ samples from isotropic gaussian $a_i$, with high probability {\em (a)} this function admits a dimension-independent region of {\em local strong convexity} on lines perpendicular to the solution manifold, and {\em (b)} with an additional polynomial factor of $r$ samples, a simple spectral initialization will land within the region of convexity with high probability. Together, this implies that gradient descent with initialization (but no re-sampling) will converge linearly to the correct $X$, up to an orthogonal transformation. We believe that this general technique (local convexity reachable by spectral initialization) should prove applicable to a broader class of nonconvex optimization problems. version:5
arxiv-1606-00235 | Clustering with phylogenetic tools in astrophysics | http://arxiv.org/abs/1606.00235 | id:1606.00235 author:Didier Fraix-Burnet category:astro-ph.IM math.ST q-bio.QM stat.ML stat.TH  published:2016-06-01 summary:Phylogenetic approaches are finding more and more applications outside the field of biology. Astrophysics is no exception since an overwhelming amount of multivariate data has appeared in the last twenty years or so. In particular, the diversification of galaxies throughout the evolution of the Universe quite naturally invokes phylogenetic approaches. We have demonstrated that Maximum Parsimony brings useful astrophysical results, and we now proceed toward the analyses of large datasets for galaxies. In this talk I present how we solve the major difficulties for this goal: the choice of the parameters, their discretization, and the analysis of a high number of objects with an unsupervised NP-hard classification technique like cladistics. 1. Introduction How do the galaxy form, and when? How did the galaxy evolve and transform themselves to create the diversity we observe? What are the progenitors to present-day galaxies? To answer these big questions, observations throughout the Universe and the physical modelisation are obvious tools. But between these, there is a key process, without which it would be impossible to extract some digestible information from the complexity of these systems. This is classification. One century ago, galaxies were discovered by Hubble. From images obtained in the visible range of wavelengths, he synthetised his observations through the usual process: classification. With only one parameter (the shape) that is qualitative and determined with the eye, he found four categories: ellipticals, spirals, barred spirals and irregulars. This is the famous Hubble classification. He later hypothetized relationships between these classes, building the Hubble Tuning Fork. The Hubble classification has been refined, notably by de Vaucouleurs, and is still used as the only global classification of galaxies. Even though the physical relationships proposed by Hubble are not retained any more, the Hubble Tuning Fork is nearly always used to represent the classification of the galaxy diversity under its new name the Hubble sequence (e.g. Delgado-Serrano, 2012). Its success is impressive and can be understood by its simplicity, even its beauty, and by the many correlations found between the morphology of galaxies and their other properties. And one must admit that there is no alternative up to now, even though both the Hubble classification and diagram have been recognised to be unsatisfactory. Among the most obvious flaws of this classification, one must mention its monovariate, qualitative, subjective and old-fashioned nature, as well as the difficulty to characterise the morphology of distant galaxies. The first two most significant multivariate studies were by Watanabe et al. (1985) and Whitmore (1984). Since the year 2005, the number of studies attempting to go beyond the Hubble classification has increased largely. Why, despite of this, the Hubble classification and its sequence are still alive and no alternative have yet emerged (Sandage, 2005)? My feeling is that the results of the multivariate analyses are not easily integrated into a one-century old practice of modeling the observations. In addition, extragalactic objects like galaxies, stellar clusters or stars do evolve. Astronomy now provides data on very distant objects, raising the question of the relationships between those and our present day nearby galaxies. Clearly, this is a phylogenetic problem. Astrocladistics 1 aims at exploring the use of phylogenetic tools in astrophysics (Fraix-Burnet et al., 2006a,b). We have proved that Maximum Parsimony (or cladistics) can be applied in astrophysics and provides a new exploration tool of the data (Fraix-Burnet et al., 2009, 2012, Cardone \& Fraix-Burnet, 2013). As far as the classification of galaxies is concerned, a larger number of objects must now be analysed. In this paper, I version:1
arxiv-1606-00226 | Crowdsourcing: Low complexity, Minimax Optimal Algorithms | http://arxiv.org/abs/1606.00226 | id:1606.00226 author:Thomas Bonald, Richard Combes category:stat.ML cs.HC cs.LG cs.SI  published:2016-06-01 summary:We consider the problem of accurately estimating the reliability of workers based on noisy labels they provide, which is a fundamental question in crowdsourcing. We propose a novel lower bound on the minimax estimation error which applies to any estimation procedure. We further propose Triangular Estimation (TE), an algorithm for estimating the reliability of workers. TE has low complexity, may be implemented in a streaming setting when labels are provided by workers in real time, and does not rely on an iterative procedure. We further prove that TE is minimax optimal and matches our lower bound. We conclude by assessing the performance of TE and other state-of-the-art algorithms on both synthetic and real-world data sets. version:1
arxiv-1606-00219 | Hyperspectral Subspace Identification Using SURE | http://arxiv.org/abs/1606.00219 | id:1606.00219 author:Behnood Rasti, Magnus O. Ulfarsson, Johannes R. Sveinsson category:cs.CV  published:2016-06-01 summary:Remote sensing hyperspectral sensors collect large volumes of high dimensional spectral and spatial data. However, due to spectral and spatial redundancy the true hyperspectral signal lies on a subspace of much lower dimension than the original data. The identification of the signal subspace is a very important first step for most hyperspectral algorithms. In this paper we investigate the important problem of identifying the hyperspectral signal subspace by minimizing the mean squared error (MSE) between the true signal and an estimate of the signal. Since the MSE is uncomputable in practice, due to its dependency on the true signal, we propose a method based on the Stein's unbiased risk estimator (SURE) that provides an unbiased estimate of the MSE. The resulting method is simple and fully automatic and we evaluate it using both simulated and real hyperspectral data sets. Experimental results shows that our proposed method compares well to recent state-of-the-art subspace identification methods. version:1
arxiv-1606-00210 | Exploiting N-Best Hypotheses to Improve an SMT Approach to Grammatical Error Correction | http://arxiv.org/abs/1606.00210 | id:1606.00210 author:Duc Tam Hoang, Shamil Chollampatt, Hwee Tou Ng category:cs.CL  published:2016-06-01 summary:Grammatical error correction (GEC) is the task of detecting and correcting grammatical errors in texts written by second language learners. The statistical machine translation (SMT) approach to GEC, in which sentences written by second language learners are translated to grammatically correct sentences, has achieved state-of-the-art accuracy. However, the SMT approach is unable to utilize global context. In this paper, we propose a novel approach to improve the accuracy of GEC, by exploiting the n-best hypotheses generated by an SMT approach. Specifically, we build a classifier to score the edits in the n-best hypotheses. The classifier can be used to select appropriate edits or re-rank the n-best hypotheses. We apply these methods to a state-of-the-art GEC system that uses the SMT approach. Our experiments show that our methods achieve statistically significant improvements in accuracy over the best published results on a benchmark test dataset on GEC. version:1
arxiv-1606-00189 | Neural Network Translation Models for Grammatical Error Correction | http://arxiv.org/abs/1606.00189 | id:1606.00189 author:Shamil Chollampatt, Kaveh Taghipour, Hwee Tou Ng category:cs.CL  published:2016-06-01 summary:Phrase-based statistical machine translation (SMT) systems have previously been used for the task of grammatical error correction (GEC) to achieve state-of-the-art accuracy. The superiority of SMT systems comes from their ability to learn text transformations from erroneous to corrected text, without explicitly modeling error types. However, phrase-based SMT systems suffer from limitations of discrete word representation, linear mapping, and lack of global context. In this paper, we address these limitations by using two different yet complementary neural network models, namely a neural network global lexicon model and a neural network joint model. These neural networks can generalize better by using continuous space representation of words and learn non-linear mappings. Moreover, they can leverage contextual information from the source sentence more effectively. By adding these two components, we achieve statistically significant improvement in accuracy for grammatical error correction over a state-of-the-art GEC system. version:1
arxiv-1606-00185 | A Survey on Learning to Hash | http://arxiv.org/abs/1606.00185 | id:1606.00185 author:Jingdong Wang, Ting Zhang, Jingkuan Song, Nicu Sebe, Heng Tao Shen category:cs.CV  published:2016-06-01 summary:Nearest neighbor search is a problem of finding the data points from the feature space such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper, we present a comprehensive survey of the learning to hash algorithms, and categorize them according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization, and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation protocols, and the general performance analysis and point out that the quantization algorithms perform superiorly in terms of search accuracy, search time cost, and space cost. Finally, we introduce a few emerging topics. version:1
arxiv-1606-00166 | Multiview Rectification of Folded Documents | http://arxiv.org/abs/1606.00166 | id:1606.00166 author:Shaodi You, Yasuyuki Matsushita, Sudipta Sinha, Yusuke Bou, Katsushi Ikeuchi category:cs.CV  published:2016-06-01 summary:Digitally unwrapping images of paper sheets is crucial for accurate document scanning and text recognition. This paper presents a method for automatically rectifying curved or folded paper sheets from a few images captured from multiple viewpoints. Prior methods either need expensive 3D scanners or model deformable surfaces using over-simplified parametric representations. In contrast, our method uses regular images and is based on general developable surface models that can represent a wide variety of paper deformations. Our main contribution is a new robust rectification method based on ridge-aware 3D reconstruction of a paper sheet and unwrapping the reconstructed surface using properties of developable surfaces via $\ell_1$ conformal mapping. We present results on several examples including book pages, folded letters and shopping receipts. version:1
arxiv-1606-00157 | Hamiltonian synaptic sampling in a model for reward-gated network plasticity | http://arxiv.org/abs/1606.00157 | id:1606.00157 author:Zhaofei Yu, David Kappel, Robert Legenstein, Sen Song, Feng Chen, Wolfgang Maass category:cs.NE q-bio.NC  published:2016-06-01 summary:Experimental data show that synaptic connections are subject to stochastic processes, and that neural codes drift on larger time scales. These data suggest to consider besides maximum likelihood learning also sampling models for network plasticity (synaptic sampling), where the current network connectivity and parameter values are viewed as a sample from a Markov chain, whose stationary distribution captures the invariant properties of network plasticity. However convergence to this stationary distribution may be rather slow if synaptic sampling carries out Langevin sampling. We show here that data on the molecular basis of synaptic plasticity, specifically on the role of CaMKII in its activated form, support a substantially more efficient Hamiltonian sampling of network configurations. We apply this new conceptual and mathematical framework to the analysis of reward-gated network plasticity, and show in a concrete example based on experimental data that Hamiltonian sampling speeds up the convergence to well-functioning network configurations. We also show that a regulation of the temperature of the sampling process provides a link between reinforcement learning and global network optimization through simulated annealing. version:1
arxiv-1603-09048 | Cost-sensitive Label Embedding for Multi-label Classification | http://arxiv.org/abs/1603.09048 | id:1603.09048 author:Kuan-Hao Huang, Hsuan-Tien Lin category:cs.LG stat.ML  published:2016-03-30 summary:Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better performance. Different real-world applications evaluate performance by different cost functions of interest. Current LE algorithms often aim to optimize one specific cost function, but they can suffer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost function of interest into account. The proposed algorithm, cost-sensitive label embedding with multidimensional scaling (CLEMS), approximates the cost information with the distances of the embedded vectors using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asymmetric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. Theoretical results justify that CLEMS achieves the cost-sensitivity and extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions. version:3
arxiv-1606-00151 | Mapping and Localization from Planar Markers | http://arxiv.org/abs/1606.00151 | id:1606.00151 author:Rafael Muñoz-Salinas, Manuel J. Marín-Jimenez, Enrique Yeguas-Bolivar, Rafael Medina-Carnicer category:cs.CV  published:2016-06-01 summary:Squared planar markers are a popular tool for fast, accurate and robust camera localization, but its use is frequently limited to a single marker, or at most, to a small set of them for which their relative pose is known beforehand. Mapping and localization from a large set of planar markers is yet a scarcely treated problem in favour of keypoint-based approaches. However, while keypoint detectors are not robust to rapid motion, large changes in viewpoint, or significant changes in appearance, fiducial markers can be robustly detected under a wider range of conditions. This paper proposes a novel method to simultaneously solve the problems of mapping and localization from a set of squared planar markers. First, a quiver of pairwise relative marker poses is created, from which an initial pose graph is obtained. The pose graph may contain small pairwise pose errors, that when propagated, leads to large errors. Thus, we distribute the rotational and translational error along the basis cycles of the graph so as to obtain a corrected pose graph. Finally, we perform a global pose optimization by minimizing the reprojection errors of the planar markers in all observed frames. The experiments conducted show that our method performs better than Structure from Motion and visual SLAM techniques. version:1
arxiv-1606-00142 | Model selection consistency from the perspective of generalization ability and VC theory with an application to Lasso | http://arxiv.org/abs/1606.00142 | id:1606.00142 author:Ning Xu, Jian Hong, Timothy C. G. Fisher category:stat.ML q-fin.EC stat.CO  published:2016-06-01 summary:Model selection is difficult to analyse yet theoretically and empirically important, especially for high-dimensional data analysis. Recently the least absolute shrinkage and selection operator (Lasso) has been applied in the statistical and econometric literature. Consis- tency of Lasso has been established under various conditions, some of which are difficult to verify in practice. In this paper, we study model selection from the perspective of generalization ability, under the framework of structural risk minimization (SRM) and Vapnik-Chervonenkis (VC) theory. The approach emphasizes the balance between the in-sample and out-of-sample fit, which can be achieved by using cross-validation to select a penalty on model complexity. We show that an exact relationship exists between the generalization ability of a model and model selection consistency. By implementing SRM and the VC inequality, we show that Lasso is L2-consistent for model selection under assumptions similar to those imposed on OLS. Furthermore, we derive a probabilistic bound for the distance between the penalized extremum estimator and the extremum estimator without penalty, which is dominated by overfitting. We also propose a new measurement of overfitting, GR2, based on generalization ability, that converges to zero if model selection is consistent. Using simulations, we demonstrate that the proposed CV-Lasso algorithm performs well in terms of model selection and overfitting control. version:1
arxiv-1606-00136 | Efficiently Bounding Optimal Solutions after Small Data Modification in Large-Scale Empirical Risk Minimization | http://arxiv.org/abs/1606.00136 | id:1606.00136 author:Hiroyuki Hanada, Atsushi Shibagaki, Jun Sakuma, Ichiro Takeuchi category:stat.ML cs.LG  published:2016-06-01 summary:We study large-scale classification problems in changing environments where a small part of the dataset is modified, and the effect of the data modification must be quickly incorporated into the classifier. When the entire dataset is large, even if the amount of the data modification is fairly small, the computational cost of re-training the classifier would be prohibitively large. In this paper, we propose a novel method for efficiently incorporating such a data modification effect into the classifier without actually re-training it. The proposed method provides bounds on the unknown optimal classifier with the cost only proportional to the size of the data modification. We demonstrate through numerical experiments that the proposed method provides sufficiently tight bounds with negligible computational costs, especially when a small part of the dataset is modified in a large-scale classification problem. version:1
arxiv-1605-09653 | A Comprehensive Evaluation and Benchmark for Person Re-Identification: Features, Metrics, and Datasets | http://arxiv.org/abs/1605.09653 | id:1605.09653 author:Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels Rates-Borras, Octavia Camps, Richard J. Radke category:cs.CV  published:2016-05-31 summary:Person re-identification (re-id) is a critical problem in video analytics applications such as security and surveillance. The public release of several datasets and code for vision algorithms has facilitated rapid progress in this area over the last few years. However, directly comparing re-id algorithms reported in the literature has become difficult since a wide variety of features, experimental protocols, and evaluation metrics are employed. In order to address this need, we present an extensive review and performance evaluation of single- and multi-shot re-id algorithms. The experimental protocol incorporates the most recent advances in both feature extraction and metric learning. To ensure a fair comparison, all of the approaches were implemented using a unified code library that includes 6 feature extraction algorithms and 21 metric learning and ranking techniques. All approaches were evaluated using a new large-scale dataset that closely mimics a real-world problem setting, in addition to 13 other publicly available datasets: VIPeR, GRID, CAVIAR, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK03, RAiD, iLIDSVID, HDA+ and Market1501. The evaluation codebase and results will be made publicly available for community use. version:2
arxiv-1406-1837 | A Credit Assignment Compiler for Joint Prediction | http://arxiv.org/abs/1406.1837 | id:1406.1837 author:Kai-Wei Chang, He He, Hal Daumé III, John Langford, Stephane Ross category:cs.LG  published:2014-06-07 summary:Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time. version:5
arxiv-1606-00118 | Gene-Gene association for Imaging Genetics Data using Robust Kernel Canonical Correlation Analysis | http://arxiv.org/abs/1606.00118 | id:1606.00118 author:Md ashad Alam, Osamu Komori, Yu-Ping Wang category:stat.ML  published:2016-06-01 summary:In genome-wide interaction studies, to detect gene-gene interactions, most methods are divided into two folds: single nucleotide polymorphisms (SNP) based and gene-based methods. Basically, the methods based on the gene are more effective than the methods based on a single SNP. Recent years, while the kernel canonical correlation analysis (Classical kernel CCA) based U statistic (KCCU) has proposed to detect the nonlinear relationship between genes. To estimate the variance in KCCU, they have used resampling based methods which are highly computationally intensive. In addition, classical kernel CCA is not robust to contaminated data. We, therefore, first discuss robust kernel mean element, the robust kernel covariance, and cross-covariance operators. Second, we propose a method based on influence function to estimate the variance of the KCCU. Third, we propose a nonparametric robust KCCU method based on robust kernel CCA, which is designed for contaminated data and less sensitive to noise than classical kernel CCA. Finally, we investigate the proposed methods to synthesized data and imaging genetic data set. Based on gene ontology and pathway analysis, the synthesized and genetics analysis demonstrate that the proposed robust method shows the superior performance of the state-of-the-art methods. version:1
arxiv-1606-00113 | Identifying Outliers using Influence Function of Multiple Kernel Canonical Correlation Analysis | http://arxiv.org/abs/1606.00113 | id:1606.00113 author:Md Ashad Alam, Yu-Ping Wang category:stat.ML  published:2016-06-01 summary:Imaging genetic research has essentially focused on discovering unique and co-association effects, but typically ignoring to identify outliers or atypical objects in genetic as well as non-genetics variables. Identifying significant outliers is an essential and challenging issue for imaging genetics and multiple sources data analysis. Therefore, we need to examine for transcription errors of identified outliers. First, we address the influence function (IF) of kernel mean element, kernel covariance operator, kernel cross-covariance operator, kernel canonical correlation analysis (kernel CCA) and multiple kernel CCA. Second, we propose an IF of multiple kernel CCA, which can be applied for more than two datasets. Third, we propose a visualization method to detect influential observations of multiple sources of data based on the IF of kernel CCA and multiple kernel CCA. Finally, the proposed methods are capable of analyzing outliers of subjects usually found in biomedical applications, in which the number of dimension is large. To examine the outliers, we use the stem-and-leaf display. Experiments on both synthesized and imaging genetics data (e.g., SNP, fMRI, and DNA methylation) demonstrate that the proposed visualization can be applied effectively. version:1
arxiv-1606-00110 | OpenSalicon: An Open Source Implementation of the Salicon Saliency Model | http://arxiv.org/abs/1606.00110 | id:1606.00110 author:Christopher Thomas category:cs.CV  published:2016-06-01 summary:In this technical report, we present our publicly downloadable implementation of the SALICON saliency model. At the time of this writing, SALICON is one of the top performing saliency models on the MIT 300 fixation prediction dataset which evaluates how well an algorithm is able to predict where humans would look in a given image. Recently, numerous models have achieved state-of-the-art performance on this benchmark, but none of the top 5 performing models (including SALICON) are available for download. To address this issue, we have created a publicly downloadable implementation of the SALICON model. It is our hope that our model will engender further research in visual attention modeling by providing a baseline for comparison of other algorithms and a platform for extending this implementation. The model we provide supports both training and testing, enabling researchers to quickly fine-tune the model on their own dataset. We also provide a pre-trained model and code for those users who only need to generate saliency maps for images without training their own model. version:1
arxiv-1606-00103 | A Comparative Study of Blending Algorithms for Realtime Panoramic Video Stitching | http://arxiv.org/abs/1606.00103 | id:1606.00103 author:Zhe Zhu, Jiaming Lu, Songhai Zhang, Ralph Martin, Hantao Liu, Shimin Hu category:cs.CV  published:2016-06-01 summary:Panoramic video stitching consists of two major steps: remapping each candidate video stream to its final position and compositing them to generate seamless results. Given videos captured with cameras in fixed relative positions, the remapping step can be done directly using a precomputed look-up table. Greater challenges lie in the more time-consuming composition step. Real world applications typically use blending to perform composition; the performance of the whole system largely depends on the efficiency of the blending algorithm. In this paper, we provide in-depth analysis of the application of several state-of-the-art image blending techniques to realtime panoramic video stitching, as realtime panoramic video stitching enables near-immediate broadcast. Test videos were captured under various conditions, and stitched using various blending methods. Both computational efficiency and quality of composition results were evaluated. Source code and test videos are all publicly available. version:1
arxiv-1602-08405 | We don't need no bounding-boxes: Training object class detectors using only human verification | http://arxiv.org/abs/1602.08405 | id:1602.08405 author:Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio Ferrari category:cs.CV  published:2016-02-26 summary:Training object class detectors typically requires a large set of images in which objects are annotated by bounding-boxes. However, manually drawing bounding-boxes is very time consuming. We propose a new scheme for training object detectors which only requires annotators to verify bounding-boxes produced automatically by the learning algorithm. Our scheme iterates between re-training the detector, re-localizing objects in the training images, and human verification. We use the verification signal both to improve re-training and to reduce the search space for re-localisation, which makes these steps different to what is normally done in a weakly supervised setting. Extensive experiments on PASCAL VOC 2007 show that (1) using human verification to update detectors and reduce the search space leads to the rapid production of high-quality bounding-box annotations; (2) our scheme delivers detectors performing almost as good as those trained in a fully supervised setting, without ever drawing any bounding-box; (3) as the verification task is very quick, our scheme substantially reduces total annotation time by a factor 6x-9x. version:2
arxiv-1606-00094 | Boda-RTC: Productive Generation of Portable, Efficient Code for Convolutional Neural Networks on Mobile Computing Platforms | http://arxiv.org/abs/1606.00094 | id:1606.00094 author:Matthew Moskewicz, Forrest Iandola, Kurt Keutzer category:cs.DC cs.MS cs.NE  published:2016-06-01 summary:The popularity of neural networks (NNs) spans academia, industry, and popular culture. In particular, convolutional neural networks (CNNs) have been applied to many image based machine learning tasks and have yielded strong results. The availability of hardware/software systems for efficient training and deployment of large and/or deep CNN models has been, and continues to be, an important consideration for the field. Early systems for NN computation focused on leveraging existing dense linear algebra techniques and libraries. Current approaches use low-level machine specific programming and/or closed-source, purpose-built vendor libraries. In this work, we present an open source system that, compared to existing approaches, achieves competitive computational speed while achieving higher portability. We achieve this by targeting the vendor-neutral OpenCL platform using a code-generation approach. We argue that our approach allows for both: (1) the rapid development of new computational kernels for existing hardware targets, and (2) the rapid tuning of existing computational kernels for new hardware targets. Results are presented for a case study of targeting the Qualcomm Snapdragon 820 mobile computing platform for CNN deployment. version:1
arxiv-1508-05038 | Seeing Behind the Camera: Identifying the Authorship of a Photograph | http://arxiv.org/abs/1508.05038 | id:1508.05038 author:Christopher Thomas, Adriana Kovashka category:cs.CV  published:2015-08-20 summary:We introduce the novel problem of identifying the photographer behind a photograph. To explore the feasibility of current computer vision techniques to address this problem, we created a new dataset of over 180,000 images taken by 41 well-known photographers. Using this dataset, we examined the effectiveness of a variety of features (low and high-level, including CNN features) at identifying the photographer. We also trained a new deep convolutional neural network for this task. Our results show that high-level features greatly outperform low-level features. We provide qualitative results using these learned models that give insight into our method's ability to distinguish between photographers, and allow us to draw interesting conclusions about what specific photographers shoot. We also demonstrate two applications of our method. version:3
arxiv-1606-01175 | Infant directed speech is consistent with teaching | http://arxiv.org/abs/1606.01175 | id:1606.01175 author:Baxter S. Eaves Jr., Naomi H. Feldman, Thomas L. Griffiths, Patrick Shafto category:cs.LG  published:2016-06-01 summary:Infant-directed speech (IDS) has distinctive properties that differ from adult-directed speech (ADS). Why it has these properties -- and whether they are intended to facilitate language learning -- is matter of contention. We argue that much of this disagreement stems from lack of a formal, guiding theory of how phonetic categories should best be taught to infant-like learners. In the absence of such a theory, researchers have relied on intuitions about learning to guide the argument. We use a formal theory of teaching, validated through experiments in other domains, as the basis for a detailed analysis of whether IDS is well-designed for teaching phonetic categories. Using the theory, we generate ideal data for teaching phonetic categories in English. We qualitatively compare the simulated teaching data with human IDS, finding that the teaching data exhibit many features of IDS, including some that have been taken as evidence IDS is not for teaching. The simulated data reveal potential pitfalls for experimentalists exploring the role of IDS in language learning. Focusing on different formants and phoneme sets leads to different conclusions, and the benefit of the teaching data to learners is not apparent until a sufficient number of examples have been provided. Finally, we investigate transfer of IDS to learning ADS. The teaching data improves classification of ADS data, but only for the learner they were generated to teach; not universally across all classes of learner. This research offers a theoretically-grounded framework that empowers experimentalists to systematically evaluate whether IDS is for teaching. version:1
arxiv-1411-4389 | Long-term Recurrent Convolutional Networks for Visual Recognition and Description | http://arxiv.org/abs/1411.4389 | id:1411.4389 author:Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell category:cs.CV  published:2014-11-17 summary:Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep"' in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized. version:4
arxiv-1606-00068 | Quantifying the probable approximation error of probabilistic inference programs | http://arxiv.org/abs/1606.00068 | id:1606.00068 author:Marco F Cusumano-Towner, Vikash K Mansinghka category:cs.AI cs.LG stat.ML  published:2016-05-31 summary:This paper introduces a new technique for quantifying the approximation error of a broad class of probabilistic inference programs, including ones based on both variational and Monte Carlo approaches. The key idea is to derive a subjective bound on the symmetrized KL divergence between the distribution achieved by an approximate inference program and its true target distribution. The bound's validity (and subjectivity) rests on the accuracy of two auxiliary probabilistic programs: (i) a "reference" inference program that defines a gold standard of accuracy and (ii) a "meta-inference" program that answers the question "what internal random choices did the original approximate inference program probably make given that it produced a particular result?" The paper includes empirical results on inference problems drawn from linear regression, Dirichlet process mixture modeling, HMMs, and Bayesian networks. The experiments show that the technique is robust to the quality of the reference inference program and that it can detect implementation bugs that are not apparent from predictive performance. version:1
arxiv-1602-03146 | DCM Bandits: Learning to Rank with Multiple Clicks | http://arxiv.org/abs/1602.03146 | id:1602.03146 author:Sumeet Katariya, Branislav Kveton, Csaba Szepesvári, Zheng Wen category:cs.LG stat.ML  published:2016-02-09 summary:A search engine recommends to the user a list of web pages. The user examines this list, from the first page to the last, and clicks on all attractive pages until the user is satisfied. This behavior of the user can be described by the dependent click model (DCM). We propose DCM bandits, an online learning variant of the DCM where the goal is to maximize the probability of recommending satisfactory items, such as web pages. The main challenge of our learning problem is that we do not observe which attractive item is satisfactory. We propose a computationally-efficient learning algorithm for solving our problem, dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable assumptions; and also prove a matching lower bound up to logarithmic factors. We evaluate our algorithm on synthetic and real-world problems, and show that it performs well even when our model is misspecified. This work presents the first practical and regret-optimal online algorithm for learning to rank with multiple clicks in a cascade-like click model. version:2
arxiv-1606-00025 | Implementing a Reverse Dictionary, based on word definitions, using a Node-Graph Architecture | http://arxiv.org/abs/1606.00025 | id:1606.00025 author:Sushrut Thorat, Varad Choudhari category:cs.CL I.2.7  published:2016-05-31 summary:In this paper, we outline an approach to build graph based reverse dictionaries using word definitions. A reverse dictionary takes a phrase as an input and outputs a list of words semantically similar to that phrase, and is a solution to the Tip of the Tongue problem. We use a distance based similarity measure, computed on the graph, to assess the similarity between a word and the input phrase. We compare the performance of our approach with the Onelook Reverse Dictionary and a distributional semantics method based on word2vec, and show that our approach is much better than the distributional semantics method, and as good as Onelook's. version:1
arxiv-1606-00021 | Texture Synthesis Using Shallow Convolutional Networks with Random Filters | http://arxiv.org/abs/1606.00021 | id:1606.00021 author:Ivan Ustyuzhaninov, Wieland Brendel, Leon A. Gatys, Matthias Bethge category:cs.CV  published:2016-05-31 summary:Here we demonstrate that the feature space of random shallow convolutional neural networks (CNNs) can serve as a surprisingly good model of natural textures. Patches from the same texture are consistently classified as being more similar then patches from different textures. Samples synthesized from the model capture spatial correlations on scales much larger then the receptive field size, and sometimes even rival or surpass the perceptual quality of state of the art texture models (but show less variability). The current state of the art in parametric texture synthesis relies on the multi-layer feature space of deep CNNs that were trained on natural images. Our finding suggests that such optimized multi-layer feature spaces are not imperative for texture modeling. Instead, much simpler shallow and convolutional networks can serve as the basis for novel texture synthesis algorithms. version:1
arxiv-1605-07583 | Provably Useful Kernel Matrix Approximation in Linear Time | http://arxiv.org/abs/1605.07583 | id:1605.07583 author:Cameron Musco, Christopher Musco category:cs.LG cs.DS stat.ML  published:2016-05-24 summary:We give the first algorithms for kernel matrix approximation that run in time linear in the number of data points and output an approximation which gives provable guarantees when used in many downstream learning tasks, including kernel principal component analysis, kernel $k$-means clustering, kernel ridge regression, and kernel canonical correlation analysis. Our methods require just $\tilde O(n\cdot k)$ kernel evaluations and $\tilde O(n \cdot k^2)$ additional runtime, where $n$ is the number of training data points and $k$ is a target rank or effective dimensionality parameter. These runtimes are significantly sub-linear in the size of the $n \times n$ kernel matrix and apply to any kernel matrix, without assuming regularity or incoherence conditions. The algorithms are based on a ridge leverage score Nystr\"om sampling scheme (RLS-Nystr\"om) which was recently shown to yield strong kernel approximations, but which had no efficient implementation. We address this shortcoming by introducing fast recursive sampling methods for RLS-Nystr\"om, while at the same time proving extended approximation guarantees for this promising new method. version:2
arxiv-1605-09782 | Adversarial Feature Learning | http://arxiv.org/abs/1605.09782 | id:1605.09782 author:Jeff Donahue, Philipp Krähenbühl, Trevor Darrell category:cs.LG cs.AI cs.CV cs.NE stat.ML  published:2016-05-31 summary:The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing generators learn to "linearize semantics" in the latent space of such models. Intuitively, such latent spaces may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning. version:1
arxiv-1605-09332 | Parametric Exponential Linear Unit for Deep Convolutional Neural Networks | http://arxiv.org/abs/1605.09332 | id:1605.09332 author:Ludovic Trottier, Philippe Giguère, Brahim Chaib-draa category:cs.LG cs.CV cs.NE  published:2016-05-30 summary:The activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning. version:2
arxiv-1605-09774 | Asynchrony begets Momentum, with an Application to Deep Learning | http://arxiv.org/abs/1605.09774 | id:1605.09774 author:Ioannis Mitliagkas, Ce Zhang, Stefan Hadjis, Christopher Ré category:stat.ML cs.DC cs.LG math.OC  published:2016-05-31 summary:Asynchronous methods are widely used in deep learning, but have limited theoretical justification when applied to non-convex problems. We give a simple argument that running stochastic gradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration. Our result does not assume convexity of the objective function, so is applicable to deep learning systems. We observe that a standard queuing model of asynchrony results in a form of momentum that is commonly used by deep learning practitioners. This forges a link between queuing theory and asynchrony in deep learning systems, which could be useful for systems builders. For convolutional neural networks, we experimentally validate that the degree of asynchrony directly correlates with the momentum, confirming our main result. Since asynchrony has better hardware efficiency, this result may shed light on when asynchronous execution is more efficient for deep learning systems. version:1
arxiv-1502-03473 | Collaborative Filtering Bandits | http://arxiv.org/abs/1502.03473 | id:1502.03473 author:Shuai Li, Alexandros Karatzoglou, Claudio Gentile category:cs.LG cs.AI stat.ML  published:2015-02-11 summary:Classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, we investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Our algorithm takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. We provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. We also provide a regret analysis within a standard linear stochastic noise setting. version:7
arxiv-1605-09759 | Fast Zero-Shot Image Tagging | http://arxiv.org/abs/1605.09759 | id:1605.09759 author:Yang Zhang, Boqing Gong, Mubarak Shah category:cs.CV  published:2016-05-31 summary:The well-known word analogy experiments show that the recent word vectors capture fine-grained linguistic regularities in words by linear vector offsets, but it is unclear how well the simple vector offsets can encode visual regularities over words. We study a particular image-word relevance relation in this paper. Our results show that the word vectors of relevant tags for a given image rank ahead of the irrelevant tags, along a principal direction in the word vector space. Inspired by this observation, we propose to solve image tagging by estimating the principal direction for an image. Particularly, we exploit linear mappings and nonlinear deep neural networks to approximate the principal direction from an input image. We arrive at a quite versatile tagging model. It runs fast given a test image, in constant time w.r.t.\ the training set size. It not only gives superior performance for the conventional tagging task on the NUS-WIDE dataset, but also outperforms competitive baselines on annotating images with previously unseen tags version:1
arxiv-1605-09757 | Towards ontology driven learning of visual concept detectors | http://arxiv.org/abs/1605.09757 | id:1605.09757 author:Sanchit Arora, Chuck Cho, Paul Fitzpatrick, Francois Scharffe category:cs.IR cs.AI cs.CV  published:2016-05-31 summary:The maturity of deep learning techniques has led in recent years to a breakthrough in object recognition in visual media. While for some specific benchmarks, neural techniques seem to match if not outperform human judgement, challenges are still open for detecting arbitrary concepts in arbitrary videos. In this paper, we propose a system that combines neural techniques, a large scale visual concepts ontology, and an active learning loop, to provide on the fly model learning of arbitrary concepts. We give an overview of the system as a whole, and focus on the central role of the ontology for guiding and bootstrapping the learning of new concepts, improving the recall of concept detection, and, on the user end, providing semantic search on a library of annotated videos. version:1
arxiv-1605-09735 | Information Theoretically Aided Reinforcement Learning for Embodied Agents | http://arxiv.org/abs/1605.09735 | id:1605.09735 author:Guido Montufar, Keyan Ghazi-Zahedi, Nihat Ay category:cs.AI cs.RO math.OC stat.ML 68T05  68T40  published:2016-05-31 summary:Reinforcement learning for embodied agents is a challenging problem. The accumulated reward to be optimized is often a very rugged function, and gradient methods are impaired by many local optimizers. We demonstrate, in an experimental setting, that incorporating an intrinsic reward can smoothen the optimization landscape while preserving the global optimizers of interest. We show that policy gradient optimization for locomotion in a complex morphology is significantly improved when supplementing the extrinsic reward by an intrinsic reward defined in terms of the mutual information of time consecutive sensor readings. version:1
arxiv-1605-05894 | Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages | http://arxiv.org/abs/1605.05894 | id:1605.05894 author:Muhammad Imran, Prasenjit Mitra, Carlos Castillo category:cs.CL cs.CY cs.SI  published:2016-05-19 summary:Microblogging platforms such as Twitter provide active communication channels during mass convergence and emergency events such as earthquakes, typhoons. During the sudden onset of a crisis situation, affected people post useful information on Twitter that can be used for situational awareness and other humanitarian disaster response efforts, if processed timely and effectively. Processing social media information pose multiple challenges such as parsing noisy, brief and informal messages, learning information categories from the incoming stream of messages and classifying them into different classes among others. One of the basic necessities of many of these tasks is the availability of data, in particular human-annotated data. In this paper, we present human-annotated Twitter corpora collected during 19 different crises that took place between 2013 and 2015. To demonstrate the utility of the annotations, we train machine learning classifiers. Moreover, we publish first largest word2vec word embeddings trained on 52 million crisis-related tweets. To deal with tweets language issues, we present human-annotated normalized lexical resources for different lexical variations. version:2
arxiv-1602-06291 | Contextual LSTM (CLSTM) models for Large scale NLP tasks | http://arxiv.org/abs/1602.06291 | id:1602.06291 author:Shalini Ghosh, Oriol Vinyals, Brian Strope, Scott Roy, Tom Dean, Larry Heck category:cs.CL  published:2016-02-19 summary:Documents exhibit sequential structure at multiple levels of abstraction (e.g., sentences, paragraphs, sections). These abstractions constitute a natural hierarchy for representing the context in which to infer the meaning of words and larger fragments of text. In this paper, we present CLSTM (Contextual LSTM), an extension of the recurrent neural network LSTM (Long-Short Term Memory) model, where we incorporate contextual features (e.g., topics) into the model. We evaluate CLSTM on three specific NLP tasks: word prediction, next sentence selection, and sentence topic prediction. Results from experiments run on two corpora, English documents in Wikipedia and a subset of articles from a recent snapshot of English Google News, indicate that using both words and topics as features improves performance of the CLSTM models over baseline LSTM models for these tasks. For example on the next sentence selection task, we get relative accuracy improvements of 21% for the Wikipedia dataset and 18% for the Google News dataset. This clearly demonstrates the significant benefit of using context appropriately in natural language (NL) tasks. This has implications for a wide variety of NL applications like question answering, sentence completion, paraphrase generation, and next utterance prediction in dialog systems. version:2
arxiv-1605-09721 | CYCLADES: Conflict-free Asynchronous Machine Learning | http://arxiv.org/abs/1605.09721 | id:1605.09721 author:Xinghao Pan, Maximilian Lam, Stephen Tu, Dimitris Papailiopoulos, Ce Zhang, Michael I. Jordan, Kannan Ramchandran, Chris Re, Benjamin Recht category:stat.ML cs.DC cs.DS cs.LG math.OC  published:2016-05-31 summary:We present CYCLADES, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. CYCLADES is asynchronous during shared model updates, and requires no memory locking mechanisms, similar to HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts during the parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms. Due to its inherent conflict-free nature and cache locality, our multi-core implementation of CYCLADES consistently outperforms HOGWILD!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to the HOGWILD! implementation of SGD, and up to 5x gains over asynchronous implementations of variance reduction algorithms. version:1
arxiv-1605-09016 | Semi-supervised Zero-Shot Learning by a Clustering-based Approach | http://arxiv.org/abs/1605.09016 | id:1605.09016 author:Seyed Mohsen Shojaee, Mahdieh Soleymani Baghshah category:cs.CV  published:2016-05-29 summary:In some of object recognition problems, labeled data may not be available for all categories. Zero-shot learning utilizes auxiliary information (also called signatures) describing each category in order to find a classifier that can recognize samples from categories with no labeled instance. In this paper, we propose a novel semi-supervised zero-shot learning method that works on an embedding space corresponding to abstract deep visual features. We seek a linear transformation on signatures to map them onto the visual features, such that the mapped signatures of the seen classes are close to labeled samples of the corresponding classes and unlabeled data are also close to the mapped signatures of one of the unseen classes. We use the idea that the rich deep visual features provide a representation space in which samples of each class are usually condensed in a cluster. The effectiveness of the proposed method is demonstrated through extensive experiments on four public benchmarks improving the state-of-the-art prediction accuracy on three of them. version:2
arxiv-1603-02501 | Mixture Proportion Estimation via Kernel Embedding of Distributions | http://arxiv.org/abs/1603.02501 | id:1603.02501 author:Harish G. Ramaswamy, Clayton Scott, Ambuj Tewari category:cs.LG stat.ML  published:2016-03-08 summary:Mixture proportion estimation (MPE) is the problem of estimating the weight of a component distribution in a mixture, given samples from the mixture and component. This problem constitutes a key part in many "weakly supervised learning" problems like learning with positive and unlabelled samples, learning with label noise, anomaly detection and crowdsourcing. While there have been several methods proposed to solve this problem, to the best of our knowledge no efficient algorithm with a proven convergence rate towards the true proportion exists for this problem. We fill this gap by constructing a provably correct algorithm for MPE, and derive convergence rates under certain assumptions on the distribution. Our method is based on embedding distributions onto an RKHS, and implementing it only requires solving a simple convex quadratic programming problem a few times. We run our algorithm on several standard classification datasets, and demonstrate that it performs comparably to or better than other algorithms on most datasets. version:2
arxiv-1605-09696 | Generalized Multi-view Embedding for Visual Recognition and Cross-modal Retrieval | http://arxiv.org/abs/1605.09696 | id:1605.09696 author:Guanqun Cao, Alexandros Iosifidis, Ke Chen, Moncef Gabbouj category:cs.CV cs.LG  published:2016-05-31 summary:In this paper, the problem of multi-view embedding from different visual cues and modalities is considered. We propose a unified solution for subspace learning methods using the Rayleigh quotient, which is extensible for multiple views, supervised learning, and non-linear embeddings. Numerous methods including Canonical Correlation Analysis, Partial Least Sqaure regression and Linear Discriminant Analysis are studied using specific intrinsic and penalty graphs within the same framework. Non-linear extensions based on kernels and (deep) neural networks are derived, achieving better performance than the linear ones. Moreover, a novel variant of multi-view Linear Discriminant Analysis is proposed by taking the view difference into consideration. We demonstrate the effectiveness of the proposed multi-view embedding methods on visual object recognition and cross-modal image retrieval, and obtain superior results in both applications compared to related methods. version:1
arxiv-1605-09674 | Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks | http://arxiv.org/abs/1605.09674 | id:1605.09674 author:Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, Pieter Abbeel category:cs.LG cs.AI cs.RO stat.ML  published:2016-05-31 summary:Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards. version:1
arxiv-1605-09658 | An Iterative Smoothing Algorithm for Regression with Structured Sparsity | http://arxiv.org/abs/1605.09658 | id:1605.09658 author:Fouad Hadj-Selem, Tommy Lofstedt, Vincent Frouin, Vincent Guillemot, Edouard Duchesnay category:stat.ML  published:2016-05-31 summary:High-dimensional prediction models are increasingly used to analyze biological data such as neuroimaging of genetic data sets. However, classical penalized algorithms yield to dense solutions that are difficult to interpret without arbitrary thresholding. Alternatives based on sparsity-inducing penalties suffer from coefficient instability. Complex structured sparsity-inducing penalties are a promising approach to force the solution to adhere to some domain-specific constraints and thus offering new perspectives in biomarker identification. We propose a generic optimization framework that can combine any smooth convex loss function with: (i) penalties whose proximal operator is known and (ii) with a large range of complex, non-smooth convex structured penalties such as total variation, or overlapping group lasso. Although many papers have addressed a similar goal, few have tackled it in such a generic way and in the context of high-dimensional data. The proposed continuation algorithm, called \textit{CONESTA}, dynamically smooths the complex penalties to avoid the computation of proximal operators, that are either not known or expensive to compute. The decreasing sequence of smoothing parameters is dynamically adapted, using the duality gap, in order to maintain the optimal convergence speed towards any globally desired precision with duality gap guarantee. First, we demonstrate, on both simulated data and on experimental MRI data, that CONESTA outperforms the excessive gap method, ADMM, proximal gradient smoothing (without continuation) and inexact FISTA in terms of convergence speed and/or precision of the solution. Second, on the experimental MRI data set, we establish the superiority of structured sparsity-inducing penalties ($\ell_1$ and total variation) over non-structured methods in terms of the recovery of meaningful and stable groups of predictive variables. version:1
arxiv-1604-06057 | Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation | http://arxiv.org/abs/1604.06057 | id:1604.06057 author:Tejas D. Kulkarni, Karthik R. Narasimhan, Ardavan Saeedi, Joshua B. Tenenbaum category:cs.LG cs.AI cs.CV cs.NE stat.ML  published:2016-04-20 summary:Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'. version:2
arxiv-1605-09646 | Average-case Hardness of RIP Certification | http://arxiv.org/abs/1605.09646 | id:1605.09646 author:Tengyao Wang, Quentin Berthet, Yaniv Plan category:cs.LG cs.CC math.ST stat.ML stat.TH  published:2016-05-31 summary:The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models. It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it. These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime. Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs. version:1
arxiv-1605-09619 | Horizontally Scalable Submodular Maximization | http://arxiv.org/abs/1605.09619 | id:1605.09619 author:Mario Lucic, Olivier Bachem, Morteza Zadimoghaddam, Andreas Krause category:stat.ML cs.DC cs.DM cs.LG  published:2016-05-31 summary:A variety of large-scale machine learning problems can be cast as instances of constrained submodular maximization. Existing approaches for distributed submodular maximization have a critical drawback: The capacity - number of instances that can fit in memory - must grow with the data set size. In practice, while one can provision many machines, the capacity of each machine is limited by physical constraints. We propose a truly scalable approach for distributed submodular maximization under fixed capacity. The proposed framework applies to a broad class of algorithms and constraints and provides theoretical guarantees on the approximation factor for any available capacity. We empirically evaluate the proposed algorithm on a variety of data sets and demonstrate that it achieves performance competitive with the centralized greedy solution. version:1
arxiv-1605-09612 | The use of deep learning in image segmentation, classification and detection | http://arxiv.org/abs/1605.09612 | id:1605.09612 author:M. S. Badea, I. I. Felea, L. M. Florea, C. Vertan category:cs.CV  published:2016-05-31 summary:Recent years have shown that deep learned neural networks are a valuable tool in the field of computer vision. This paper addresses the use of two different kinds of network architectures, namely LeNet and Network in Network (NiN). They will be compared in terms of both performance and computational efficiency by addressing the classification and detection problems. In this paper, multiple databases will be used to test the networks. One of them contains images depicting burn wounds from pediatric cases, another one contains an extensive number of art images and other facial databases were used for facial keypoints detection. version:1
arxiv-1605-09196 | Forest Floor Visualizations of Random Forests | http://arxiv.org/abs/1605.09196 | id:1605.09196 author:Soeren H. Welling, Hanne H. F. Refsgaard, Per B. Brockhoff, Line H. Clemmensen category:stat.ML cs.LG  published:2016-05-30 summary:We propose a novel methodology, forest floor, to visualize and interpret random forest (RF) models. RF is a popular and useful tool for non-linear multi-variate classification and regression, which yields a good trade-off between robustness (low variance) and adaptiveness (low bias). Direct interpretation of a RF model is difficult, as the explicit ensemble model of hundreds of deep trees is complex. Nonetheless, it is possible to visualize a RF model fit by its mapping from feature space to prediction space. Hereby the user is first presented with the overall geometrical shape of the model structure, and when needed one can zoom in on local details. Dimensional reduction by projection is used to visualize high dimensional shapes. The traditional method to visualize RF model structure, partial dependence plots, achieve this by averaging multiple parallel projections. We suggest to first use feature contributions, a method to decompose trees by splitting features, and then subsequently perform projections. The advantages of forest floor over partial dependence plots is that interactions are not masked by averaging. As a consequence, it is possible to locate interactions, which are not visualized in a given projection. Furthermore, we introduce: a goodness-of-visualization measure, use of colour gradients to identify interactions and an out-of-bag cross validated variant of feature contributions. version:2
arxiv-1509-01404 | Coordinate Descent Methods for Symmetric Nonnegative Matrix Factorization | http://arxiv.org/abs/1509.01404 | id:1509.01404 author:Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon category:cs.NA cs.CV cs.LG math.OC stat.ML  published:2015-09-04 summary:Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix factorization (symNMF) is the problem of finding a nonnegative matrix $H$, usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMF can be used for data analysis and in particular for various clustering tasks. In this paper, we propose simple and very efficient coordinate descent schemes to solve this problem, and that can handle large and sparse input matrices. The effectiveness of our methods is illustrated on synthetic and real-world data sets, and we show that they perform favorably compared to recent state-of-the-art methods. version:2
arxiv-1509-00651 | Manipulated Object Proposal: A Discriminative Object Extraction and Feature Fusion Framework for First-Person Daily Activity Recognition | http://arxiv.org/abs/1509.00651 | id:1509.00651 author:Changzhi Luo, Bingbing Ni, Jun Yuan, Jianfeng Wang, Shuicheng Yan, Meng Wang category:cs.CV  published:2015-09-02 summary:Detecting and recognizing objects interacting with humans lie in the center of first-person (egocentric) daily activity recognition. However, due to noisy camera motion and frequent changes in viewpoint and scale, most of the previous egocentric action recognition methods fail to capture and model highly discriminative object features. In this work, we propose a novel pipeline for first-person daily activity recognition, aiming at more discriminative object feature representation and object-motion feature fusion. Our object feature extraction and representation pipeline is inspired by the recent success of object hypotheses and deep convolutional neural network based detection frameworks. Our key contribution is a simple yet effective manipulated object proposal generation scheme. This scheme leverages motion cues such as motion boundary and motion magnitude (in contrast, camera motion is usually considered as "noise" for most previous methods) to generate a more compact and discriminative set of object proposals, which are more closely related to the objects which are being manipulated. Then, we learn more discriminative object detectors from these manipulated object proposals based on region-based convolutional neural network (R-CNN). Meanwhile, we develop a network based feature fusion scheme which better combines object and motion features. We show in experiments that the proposed framework significantly outperforms the state-of-the-art recognition performance on a challenging first-person daily activity benchmark. version:3
arxiv-1605-09593 | Controlling Exploration Improves Training for Deep Neural Networks | http://arxiv.org/abs/1605.09593 | id:1605.09593 author:Yasutoshi Ida, Yasuhiro Fujiwara, Sotetsu Iwamura category:cs.LG cs.AI stat.ML  published:2016-05-31 summary:Stochastic optimization methods are widely used for training of deep neural networks. However, it is still a challenging research problem to achieve effective training by using stochastic optimization methods. This is due to the difficulties in finding good parameters on a loss function that have many saddle points. In this paper, we propose a stochastic optimization method called STDProp for effective training of deep neural networks. Its key idea is to effectively explore parameters on a complex surface of a loss function. We additionally develop momentum version of STDProp. While our approaches are easy to implement with high memory efficiency, it is more effective than other practical stochastic optimization methods for deep neural networks. version:1
arxiv-1511-05212 | Binary embeddings with structured hashed projections | http://arxiv.org/abs/1511.05212 | id:1511.05212 author:Anna Choromanska, Krzysztof Choromanski, Mariusz Bojarski, Tony Jebara, Sanjiv Kumar, Yann LeCun category:cs.LG  published:2015-11-16 summary:We consider the hashing mechanism for constructing binary embeddings, that involves pseudo-random projections followed by nonlinear (sign function) mappings. The pseudo-random projection is described by a matrix, where not all entries are independent random variables but instead a fixed "budget of randomness" is distributed across the matrix. Such matrices can be efficiently stored in sub-quadratic or even linear space, provide reduction in randomness usage (i.e. number of required random values), and very often lead to computational speed ups. We prove several theoretical results showing that projections via various structured matrices followed by nonlinear mappings accurately preserve the angular distance between input high-dimensional vectors. To the best of our knowledge, these results are the first that give theoretical ground for the use of general structured matrices in the nonlinear setting. In particular, they generalize previous extensions of the Johnson-Lindenstrauss lemma and prove the plausibility of the approach that was so far only heuristically confirmed for some special structured matrices. Consequently, we show that many structured matrices can be used as an efficient information compression mechanism. Our findings build a better understanding of certain deep architectures, which contain randomly weighted and untrained layers, and yet achieve high performance on different learning tasks. We empirically verify our theoretical findings and show the dependence of learning via structured hashed projections on the performance of neural network as well as nearest neighbor classifier. version:4
arxiv-1605-09584 | A Sparse Representation of Complete Local Binary Pattern Histogram for Human Face Recognition | http://arxiv.org/abs/1605.09584 | id:1605.09584 author:Mawloud Guermoui, Mohamed L. Mekhalfi category:cs.CV  published:2016-05-31 summary:Human face recognition has been a long standing problem in computer vision and pattern recognition. Facial analysis can be viewed as a two-fold problem, namely (i) facial representation, and (ii) classification. So far, many face representations have been proposed, a well-known method is the Local Binary Pattern (LBP), which has witnessed a growing interest. In this respect, we treat in this paper the issues of face representation as well as classification in a novel manner. On the one hand, we use a variant to LBP, so-called Complete Local Binary Pattern (CLBP), which differs from the basic LBP by coding a given local region using a given central pixel and Sing_ Magnitude difference. Subsequently, most of LBPbased descriptors use a fixed grid to code a given facial image, which technique is, in most cases, not robust to pose variation and misalignment. To cope with such issue, a representative Multi-Resolution Histogram (MH) decomposition is adopted in our work. On the other hand, having the histograms of the considered images extracted, we exploit their sparsity to construct a so-called Sparse Representation Classifier (SRC) for further face classification. Experimental results have been conducted on ORL face database, and pointed out the superiority of our scheme over other popular state-of-the-art techniques. version:1
arxiv-1605-09582 | Model-driven Simulations for Deep Convolutional Neural Networks | http://arxiv.org/abs/1605.09582 | id:1605.09582 author:V S R Veeravasarapu, Constantin Rothkopf, Visvanathan Ramesh category:cs.CV  published:2016-05-31 summary:The use of simulated virtual environments to train deep convolutional neural networks (CNN) is a currently active practice to reduce the (real)data-hungriness of the deep CNN models, especially in application domains in which large scale real data and/or groundtruth acquisition is difficult or laborious. Recent approaches have attempted to harness the capabilities of existing video games, animated movies to provide training data with high precision groundtruth. However, a stumbling block is in how one can certify generalization of the learned models and their usefulness in real world data sets. This opens up fundamental questions such as: What is the role of photorealism of graphics simulations in training CNN models? Are the trained models valid in reality? What are possible ways to reduce the performance bias? In this work, we begin to address theses issues systematically in the context of urban semantic understanding with CNNs. Towards this end, we (a) propose a simple probabilistic urban scene model, (b) develop a parametric rendering tool to synthesize the data with groundtruth, followed by (c) a systematic exploration of the impact of level-of-realism on the generality of the trained CNN model to real world; and domain adaptation concepts to minimize the performance bias. version:1
arxiv-1605-09564 | Determining the Characteristic Vocabulary for a Specialized Dictionary using Word2vec and a Directed Crawler | http://arxiv.org/abs/1605.09564 | id:1605.09564 author:Gregory Grefenstette, Lawrence Muchemi category:cs.CL cs.AI cs.IR  published:2016-05-31 summary:Specialized dictionaries are used to understand concepts in specific domains, especially where those concepts are not part of the general vocabulary, or having meanings that differ from ordinary languages. The first step in creating a specialized dictionary involves detecting the characteristic vocabulary of the domain in question. Classical methods for detecting this vocabulary involve gathering a domain corpus, calculating statistics on the terms found there, and then comparing these statistics to a background or general language corpus. Terms which are found significantly more often in the specialized corpus than in the background corpus are candidates for the characteristic vocabulary of the domain. Here we present two tools, a directed crawler, and a distributional semantics package, that can be used together, circumventing the need of a background corpus. Both tools are available on the web. version:1
arxiv-1605-09559 | Modeling Photographic Composition via Triangles | http://arxiv.org/abs/1605.09559 | id:1605.09559 author:Zihan Zhou, Siqiong He, Jia Li, James Z. Wang category:cs.CV  published:2016-05-31 summary:The capacity of automatically modeling photographic composition is valuable for many real-world machine vision applications such as digital photography, image retrieval, image understanding, and image aesthetics assessment. The triangle technique is among those indispensable composition methods on which professional photographers often rely. This paper proposes a system that can identify prominent triangle arrangements in two major categories of photographs: natural or urban scenes, and portraits. For the natural or urban scene pictures, the focus is on the effect of linear perspective. For portraits, we carefully examine the positioning of human subjects in a photo. We show that line analysis is highly advantageous for modeling composition in both categories. Based on the detected triangles, new mathematical descriptors for composition are formulated and used to retrieve similar images. Leveraging the rich source of high aesthetics photos online, similar approaches can potentially be incorporated in future smart cameras to enhance a person's photo composition skills. version:1
arxiv-1605-09553 | Attention Correctness in Neural Image Captioning | http://arxiv.org/abs/1605.09553 | id:1605.09553 author:Chenxi Liu, Junhua Mao, Fei Sha, Alan Yuille category:cs.CV cs.CL cs.LG  published:2016-05-31 summary:Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the "correctness" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for how well the attention maps align with human judgment, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality. version:1
arxiv-1605-09546 | Semantic-Aware Depth Super-Resolution in Outdoor Scenes | http://arxiv.org/abs/1605.09546 | id:1605.09546 author:Miaomiao Liu, Mathieu Salzmann, Xuming He category:cs.CV  published:2016-05-31 summary:While depth sensors are becoming increasingly popular, their spatial resolution often remains limited. Depth super-resolution therefore emerged as a solution to this problem. Despite much progress, state-of-the-art techniques suffer from two drawbacks: (i) they rely on the assumption that intensity edges coincide with depth discontinuities, which, unfortunately, is only true in controlled environments; and (ii) they typically exploit the availability of high-resolution training depth maps, which can often not be acquired in practice due to the sensors' limitations. By contrast, here, we introduce an approach to performing depth super-resolution in more challenging conditions, such as in outdoor scenes. To this end, we first propose to exploit semantic information to better constrain the super-resolution process. In particular, we design a co-sparse analysis model that learns filters from joint intensity, depth and semantic information. Furthermore, we show how low-resolution training depth maps can be employed in our learning strategy. We demonstrate the benefits of our approach over state-of-the-art depth super-resolution methods on two outdoor scene datasets. version:1
arxiv-1605-09533 | Robust Deep-Learning-Based Road-Prediction for Augmented Reality Navigation Systems | http://arxiv.org/abs/1605.09533 | id:1605.09533 author:Matthias Limmer, Julian Forster, Dennis Baudach, Florian Schüle, Roland Schweiger, Hendrik P. A. Lensch category:cs.CV cs.LG cs.RO  published:2016-05-31 summary:This paper proposes an approach that predicts the road course from camera sensors leveraging deep learning techniques. Road pixels are identified by training a multi-scale convolutional neural network on a large number of full-scene-labeled night-time road images including adverse weather conditions. A framework is presented that applies the proposed approach to longer distance road course estimation, which is the basis for an augmented reality navigation application. In this framework long range sensor data (radar) and data from a map database are fused with short range sensor data (camera) to produce a precise longitudinal and lateral localization and road course estimation. The proposed approach reliably detects roads with and without lane markings and thus increases the robustness and availability of road course estimations and augmented reality navigation. Evaluations on an extensive set of high precision ground truth data taken from a differential GPS and an inertial measurement unit show that the proposed approach reaches state-of-the-art performance without the limitation of requiring existing lane markings. version:1
arxiv-1605-09527 | Biconvex Relaxation for Semidefinite Programming in Computer Vision | http://arxiv.org/abs/1605.09527 | id:1605.09527 author:Sohil Shah, Abhay Kumar, David Jacobs, Christoph Studer, Tom Goldstein category:cs.CV cs.NA math.NA math.OC  published:2016-05-31 summary:Semidefinite programming is an indispensable tool in computer vision, but general-purpose solvers for semidefinite programs are often too slow and memory intensive for large-scale problems. We propose a general framework to approximately solve large-scale semidefinite problems (SDPs) at low complexity. Our approach, referred to as biconvex relaxation (BCR), transforms a general SDP into a specific biconvex optimization problem, which can then be solved in the original, low-dimensional variable space at low complexity. The resulting biconvex problem is solved using an efficient alternating minimization (AM) procedure. Since AM has the potential to get stuck in local minima, we propose a general initialization scheme that enables BCR to start close to a global optimum - this is key for our algorithm to quickly converge to optimal or near-optimal solutions. We showcase the efficacy of our approach on three applications in computer vision, namely segmentation, co-segmentation, and manifold metric learning. BCR achieves solution quality comparable to state-of-the-art SDP methods with speedups between 4X and 35X. At the same time, BCR handles a more general set of SDPs than previous approaches, which are more specialized. version:1
arxiv-1605-09526 | Intention from Motion | http://arxiv.org/abs/1605.09526 | id:1605.09526 author:Andrea Zunino, Jacopo Cavazza, Atesh Koul, Andrea Cavallo, Cristina Becchio, Vittorio Murino category:cs.CV  published:2016-05-31 summary:In this paper, we propose Intention from Motion, a new paradigm for action prediction where, without using any contextual information, we can predict human intentions all originating from the same motor act, non specific of the following performed action. To this purpose, we have designed a new multimodal dataset consisting of a set of motion capture marker 3D data and 2D video sequences where, by only analysing very similar movements in both training and test phases, we are able to predict the underlying intention, i.e., the future, never observed, action. We also present an extensive three-fold experimental evaluation as a baseline, customizing state-of-the-art techniques for 3D and 2D data analysis, and proposing fusion methods to combine the two types of available data. This work constitutes a proof of concept for this new paradigm as we empirically prove the affordability of predicting intentions by analysing motion patterns and without considering any additional contextual cues. version:1
arxiv-1605-09507 | Deep convolutional neural networks for predominant instrument recognition in polyphonic music | http://arxiv.org/abs/1605.09507 | id:1605.09507 author:Yoonchang Han, Jaehun Kim, Kyogu Lee category:cs.SD cs.CV cs.LG cs.NE  published:2016-05-31 summary:Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms. version:1
arxiv-1602-03264 | A Theory of Generative ConvNet | http://arxiv.org/abs/1602.03264 | id:1602.03264 author:Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.LG  published:2016-02-10 summary:We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the categories is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns. version:3
arxiv-1605-09499 | Extreme Stochastic Variational Inference: Distributed and Asynchronous | http://arxiv.org/abs/1605.09499 | id:1605.09499 author:Parameswaran Raman, Jiong Zhang, Hsiang-Fu Yu, Shihao Ji, S. V. N. Vishwanathan category:stat.ML  published:2016-05-31 summary:We propose extreme stochastic variational inference (ESVI), which allows multiple processors to simultaneously and asynchronously perform variational inference updates. Moreover, by using a classic owner computes paradigm, our algorithm can be made lock-free. ESVI exhibits data and model parallelism, that is, each processor only needs access to a subset of the data and a subset of the parameters. In our experiments we show that our new algorithm outperforms a straightforward strategy for parallelizing variational inference, which requires bulk synchronization after every iteration. version:1
arxiv-1605-09477 | A Neural Autoregressive Approach to Collaborative Filtering | http://arxiv.org/abs/1605.09477 | id:1605.09477 author:Yin Zheng, Bangsheng Tang, Wenkui Ding, Hanning Zhou category:cs.IR cs.LG stat.ML  published:2016-05-31 summary:This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance. version:1
arxiv-1605-09466 | Bayesian optimization under mixed constraints with a slack-variable augmented Lagrangian | http://arxiv.org/abs/1605.09466 | id:1605.09466 author:Victor Picheny, Robert B. Gramacy, Stefan M. Wild, Sebastien Le Digabel category:stat.CO stat.ML  published:2016-05-31 summary:An augmented Lagrangian (AL) can convert a constrained optimization problem into a sequence of simpler (e.g., unconstrained) problems, which are then usually solved with local solvers. Recently, surrogate-based Bayesian optimization (BO) sub-solvers have been successfully deployed in the AL framework for a more global search in the presence of inequality constraints; however, a drawback was that expected improvement (EI) evaluations relied on Monte Carlo. Here we introduce an alternative slack variable AL, and show that in this formulation the EI may be evaluated with library routines. The slack variables furthermore facilitate equality as well as inequality constraints, and mixtures thereof. We show how our new slack "ALBO" compares favorably to the original. Its superiority over conventional alternatives is reinforced on several mixed constraint examples. version:1
arxiv-1605-09459 | Scalable and Optimal Generalized Canonical Correlation Analysis via Alternating Optimization | http://arxiv.org/abs/1605.09459 | id:1605.09459 author:Xiao Fu, Kejun Huang, Mingyi Hong, Nicholas D. Sidiropoulos, Anthony Man-Cho So category:stat.ML  published:2016-05-31 summary:This paper considers generalized (multiview) canonical correlation analysis (GCCA) for large-scale datasets. A memory-efficient and computationally lightweight algorithm is proposed for the classic MAX-VAR GCCA formulation, which is gaining renewed interest in various applications, such as speech recognition and natural language processing. The MAX-VAR GCCA problem can be solved optimally via eigen-decomposition of a matrix that compounds the (whitened) correlation matrices of the views. However, this route can easily lead to memory explosion and a heavy computational burden when the size of the views becomes large. Instead, we propose an alternating optimization (AO)-based algorithm, which avoids instantiating the correlation matrices of the views and thus can achieve substantial saving in memory. The algorithm also maintains data sparsity, which can be exploited to alleviate the computational burden. Consequently, the proposed algorithm is highly scalable. Despite the non-convexity of the MAX-VAR GCCA problem, the proposed iterative algorithm is shown to converge to a globally optimal solution under certain mild conditions. The proposed framework ensures global convergence even when the subproblems are inexactly solved, which can further reduce the complexity in practice. Simulations and large-scale word embedding tasks are employed to showcase the effectiveness of the proposed algorithm. version:1
arxiv-1605-09458 | Training Auto-encoders Effectively via Eliminating Task-irrelevant Input Variables | http://arxiv.org/abs/1605.09458 | id:1605.09458 author:Hui Shen, Dehua Li, Hong Wu, Zhaoxiang Zang category:cs.LG  published:2016-05-31 summary:Auto-encoders are often used as building blocks of deep network classifier to learn feature extractors, but task-irrelevant information in the input data may lead to bad extractors and result in poor generalization performance of the network. In this paper,via dropping the task-irrelevant input variables the performance of auto-encoders can be obviously improved .Specifically, an importance-based variable selection method is proposed to aim at finding the task-irrelevant input variables and dropping them.It firstly estimates importance of each variable,and then drops the variables with importance value lower than a threshold. In order to obtain better performance, the method can be employed for each layer of stacked auto-encoders. Experimental results show that when combined with our method the stacked denoising auto-encoders achieves significantly improved performance on three challenging datasets. version:1
arxiv-1605-09452 | Latent Bi-constraint SVM for Video-based Object Recognition | http://arxiv.org/abs/1605.09452 | id:1605.09452 author:Yang Liu, Minh Hoai, Mang Shao, Tae-Kyun Kim category:cs.CV  published:2016-05-31 summary:We address the task of recognizing objects from video input. This important problem is relatively unexplored, compared with image-based object recognition. To this end, we make the following contributions. First, we introduce two comprehensive datasets for video-based object recognition. Second, we propose Latent Bi-constraint SVM (LBSVM), a maximum-margin framework for video-based object recognition. LBSVM is based on Structured-Output SVM, but extends it to handle noisy video data and ensure consistency of the output decision throughout time. We apply LBSVM to recognize office objects and museum sculptures, and we demonstrate its benefits over image-based, set-based, and other video-based object recognition. version:1
arxiv-1605-09451 | Quantitative Analysis of Saliency Models | http://arxiv.org/abs/1605.09451 | id:1605.09451 author:Flora Ponjou Tasse, Jiří Kosinka, Neil Anthony Dodgson category:cs.GR cs.CV  published:2016-05-31 summary:Previous saliency detection research required the reader to evaluate performance qualitatively, based on renderings of saliency maps on a few shapes. This qualitative approach meant it was unclear which saliency models were better, or how well they compared to human perception. This paper provides a quantitative evaluation framework that addresses this issue. In the first quantitative analysis of 3D computational saliency models, we evaluate four computational saliency models and two baseline models against ground-truth saliency collected in previous work. version:1
arxiv-1605-09444 | A Novel Fault Classification Scheme Based on Least Square SVM | http://arxiv.org/abs/1605.09444 | id:1605.09444 author:Harishchandra Dubey, A. K. Tiwari, Nandita, P. K. Ray, S. R. Mohanty, Nand Kishor category:cs.SY cs.LG  published:2016-05-30 summary:This paper presents a novel approach for fault classification and section identification in a series compensated transmission line based on least square support vector machine. The current signal corresponding to one-fourth of the post fault cycle is used as input to proposed modular LS-SVM classifier. The proposed scheme uses four binary classifier; three for selection of three phases and fourth for ground detection. The proposed classification scheme is found to be accurate and reliable in presence of noise as well. The simulation results validate the efficacy of proposed scheme for accurate classification of fault in a series compensated transmission line. version:1
arxiv-1605-09441 | Blind Modulation Classification based on MLP and PNN | http://arxiv.org/abs/1605.09441 | id:1605.09441 author:Harishchandra Dubey, Nandita, Ashutosh Kumar Tiwari category:cs.CV cs.IT math.IT  published:2016-05-30 summary:In this work, a pattern recognition system is investigated for blind automatic classification of digitally modulated communication signals. The proposed technique is able to discriminate the type of modulation scheme which is eventually used for demodulation followed by information extraction. The proposed system is composed of two subsystems namely feature extraction sub-system (FESS) and classifier sub-system (CSS). The FESS consists of continuous wavelet transform (CWT) for feature generation and principal component analysis (PCA) for selection of the feature subset which is rich in discriminatory information. The CSS uses the selected features to accurately classify the modulation class of the received signal. The proposed technique uses probabilistic neural network (PNN) and multilayer perceptron forward neural network (MLPFN) for comparative study of their recognition ability. PNN have been found to perform better in terms of classification accuracy as well as testing and training time than MLPFN. The proposed approach is robust to presence of phase offset and additive Gaussian noise. version:1
arxiv-1605-09432 | Evaluating Crowdsourcing Participants in the Absence of Ground-Truth | http://arxiv.org/abs/1605.09432 | id:1605.09432 author:Ramanathan Subramanian, Romer Rosales, Glenn Fung, Jennifer Dy category:cs.HC cs.LG  published:2016-05-30 summary:Given a supervised/semi-supervised learning scenario where multiple annotators are available, we consider the problem of identification of adversarial or unreliable annotators. version:1
arxiv-1602-07415 | Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling | http://arxiv.org/abs/1602.07415 | id:1602.07415 author:Christopher De Sa, Kunle Olukotun, Christopher Ré category:cs.LG  published:2016-02-24 summary:Gibbs sampling is a Markov chain Monte Carlo technique commonly used for estimating marginal distributions. To speed up Gibbs sampling, there has recently been interest in parallelizing it by executing asynchronously. While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: bias and mixing time. We show experimentally that our theoretical results match practical outcomes. version:2
arxiv-1511-02386 | Hierarchical Variational Models | http://arxiv.org/abs/1511.02386 | id:1511.02386 author:Rajesh Ranganath, Dustin Tran, David M. Blei category:stat.ML cs.LG stat.CO stat.ME  published:2015-11-07 summary:Black box variational inference allows researchers to easily prototype and evaluate an array of models. Recent advances allow such algorithms to scale to high dimensions. However, a central question remains: How to specify an expressive variational distribution that maintains efficient computation? To address this, we develop hierarchical variational models (HVMs). HVMs augment a variational approximation with a prior on its parameters, which allows it to capture complex structure for both discrete and continuous latent variables. The algorithm we develop is black box, can be used for any HVM, and has the same computational efficiency as the original approximation. We study HVMs on a variety of deep discrete latent variable models. HVMs generalize other expressive variational distributions and maintains higher fidelity to the posterior. version:2
arxiv-1412-1193 | New insights and perspectives on the natural gradient method | http://arxiv.org/abs/1412.1193 | id:1412.1193 author:James Martens category:cs.LG stat.ML  published:2014-12-03 summary:Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of approximate 2nd-order optimization method, where the Fisher information matrix used to compute the natural gradient direction can be viewed as an approximation of the Hessian. This perspective turns out to have significant implications for how to design a practical and robust version of the method. Among our various other contributions is a thorough analysis of the convergence speed of natural gradient descent and more general stochastic methods, a critical examination of the oft-used "empirical" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by the method, which we show still holds for certain other choices of the curvature matrix, but notably not the Hessian. version:7
arxiv-1605-09410 | End-to-End Instance Segmentation and Counting with Recurrent Attention | http://arxiv.org/abs/1605.09410 | id:1605.09410 author:Mengye Ren, Richard S. Zemel category:cs.LG cs.CV  published:2016-05-30 summary:While convolutional neural networks have gained impressive success recently in solving structured prediction problems such as semantic segmentation, it remains a challenge to differentiate individual object instances in the scene. Instance segmentation is very important in a variety of applications, such as autonomous driving, image captioning, and visual question answering. Techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end recurrent neural network (RNN) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations. The network is jointly trained to sequentially produce regions of interest as well as a dominant object segmentation within each region. The proposed model achieves state-of-the-art results on the CVPPP leaf segmentation dataset and KITTI vehicle segmentation dataset. version:1
arxiv-1605-09370 | Unsupervised Discovery of El Nino Using Causal Feature Learning on Microlevel Climate Data | http://arxiv.org/abs/1605.09370 | id:1605.09370 author:Krzysztof Chalupka, Tobias Bischoff, Pietro Perona, Frederick Eberhardt category:stat.ML cs.AI cs.LG physics.ao-ph  published:2016-05-30 summary:We show that the climate phenomena of El Nino and La Nina arise naturally as states of macro-variables when our recent causal feature learning framework (Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind (ZW) and sea surface temperatures (SST) taken over the equatorial band of the Pacific Ocean. The method identifies these unusual climate states on the basis of the relation between ZW and SST patterns without any input about past occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering the SST fields while disregarding their relationship with ZW patterns, or (ii) clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the degree to which our method supports a causal interpretation and use a low-dimensional toy example to explain its success over other clustering approaches. Finally, we propose a new robust and scalable alternative to our original algorithm (Chalupka 2016), which circumvents the need for high-dimensional density learning. version:1
arxiv-1604-08145 | Laser light-field fusion for wide-field lensfree on-chip phase contrast nanoscopy | http://arxiv.org/abs/1604.08145 | id:1604.08145 author:Farnoud Kazemzadeh, Alexander Wong category:physics.optics cs.CV physics.ins-det  published:2016-04-27 summary:Wide-field lensfree on-chip microscopy, which leverages holography principles to capture interferometric light-field encodings without lenses, is an emerging imaging modality with widespread interest given the large field-of-view compared to lens-based techniques. In this study, we introduce the idea of laser light-field fusion for lensfree on-chip phase contrast nanoscopy, where interferometric laser light-field encodings acquired using an on-chip setup with laser pulsations at different wavelengths are fused to produce marker-free phase contrast images of superior quality with resolving power more than five times below the pixel pitch of the sensor array and more than 40% beyond the diffraction limit. As a proof of concept, we demonstrate, for the first time, a wide-field lensfree on-chip instrument successfully detecting 300 nm particles, resulting in a numerical aperture of 1.1, across a large field-of-view of $\sim$ 30 mm$^2$ without any specialized or intricate sample preparation, or the use of synthetic aperture- or shift-based techniques. version:2
arxiv-1605-09351 | Review of Fall Detection Techniques: A Data Availability Perspective | http://arxiv.org/abs/1605.09351 | id:1605.09351 author:Shehroz S. Khan, Jesse Hoey category:cs.LG  published:2016-05-30 summary:A fall is an abnormal activity that occurs rarely; however, missing to identify falls can have serious health and safety implications on an individual. Due to the rarity of occurrence of falls, there may be insufficient or no training data available for them. Therefore, standard supervised machine learning methods may not be directly applied to handle this problem. In this paper, we present a taxonomy for the study of fall detection from the perspective of availability of fall data. The proposed taxonomy is independent of the type of sensors used and specific feature extraction/selection methods. The taxonomy identifies different categories of classification methods for the study of fall detection based on the availability of their data during training the classifiers. Then, we present a comprehensive literature review within those categories and identify the approach of treating a fall as an abnormal activity to be a plausible research direction. We conclude our paper by discussing several open research problems in the field and pointers for future research. version:1
arxiv-1605-09346 | Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs | http://arxiv.org/abs/1605.09346 | id:1605.09346 author:Anton Osokin, Jean-Baptiste Alayrac, Isabella Lukasewitz, Puneet K. Dokania, Simon Lacoste-Julien category:cs.LG math.OC stat.ML G.1.6; I.2.6  published:2016-05-30 summary:In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets. version:1
arxiv-1310-5764 | Minimax Optimal Convergence Rates for Estimating Ground Truth from Crowdsourced Labels | http://arxiv.org/abs/1310.5764 | id:1310.5764 author:Chao Gao, Dengyong Zhou category:stat.ML math.ST stat.TH  published:2013-10-22 summary:Crowdsourcing has become a primary means for label collection in many real-world machine learning applications. A classical method for inferring the true labels from the noisy labels provided by crowdsourcing workers is Dawid-Skene estimator. In this paper, we prove convergence rates of a projected EM algorithm for the Dawid-Skene estimator. The revealed exponent in the rate of convergence is shown to be optimal via a lower bound argument. Our work resolves the long standing issue of whether Dawid-Skene estimator has sound theoretical guarantees besides its good performance observed in practice. In addition, a comparative study with majority voting illustrates both advantages and pitfalls of the Dawid-Skene estimator. version:6
arxiv-1605-09336 | Learning the image processing pipeline | http://arxiv.org/abs/1605.09336 | id:1605.09336 author:Haomiao Jiang, Qiyuan Tian, Joyce Farrell, Brian Wandell category:cs.CV  published:2016-05-30 summary:Many creative ideas are being proposed for image sensor designs, and these may be useful in applications ranging from consumer photography to computer vision. To understand and evaluate each new design, we must create a corresponding image processing pipeline that transforms the sensor data into a form that is appropriate for the application. The need to design and optimize these pipelines is time-consuming and costly. We explain a method that combines machine learning and image systems simulation that automates the pipeline design. The approach is based on a new way of thinking of the image processing pipeline as a large collection of local linear filters. We illustrate how the method has been used to design pipelines for novel sensor architectures in consumer photography applications. version:1
arxiv-1512-08836 | Learning to Filter with Predictive State Inference Machines | http://arxiv.org/abs/1512.08836 | id:1512.08836 author:Wen Sun, Arun Venkatraman, Byron Boots, J. Andrew Bagnell category:cs.LG  published:2015-12-30 summary:Latent state space models are a fundamental and widely used tool for modeling dynamical systems. However, they are difficult to learn from data and learned models often lack performance guarantees on inference tasks such as filtering and prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE (PSIM), a data-driven method that considers the inference procedure on a dynamical system as a composition of predictors. The key idea is that rather than first learning a latent state space model, and then using the learned model for inference, PSIM directly learns predictors for inference in predictive state space. We provide theoretical guarantees for inference, in both realizable and agnostic settings, and showcase practical performance on a variety of simulated and real world robotics benchmarks. version:2
arxiv-1605-09299 | k2-means for fast and accurate large scale clustering | http://arxiv.org/abs/1605.09299 | id:1605.09299 author:Eirikur Agustsson, Radu Timofte, Luc Van Gool category:cs.LG cs.CV  published:2016-05-30 summary:We propose k^2-means, a new clustering method which efficiently copes with large numbers of clusters and achieves low energy solutions. k^2-means builds upon the standard k-means (Lloyd's algorithm) and combines a new strategy to accelerate the convergence with a new low time complexity divisive initialization. The accelerated convergence is achieved through only looking at k_n nearest clusters and using triangle inequality bounds in the assignment step while the divisive initialization employs an optimal 2-clustering along a direction. The worst-case time complexity per iteration of our k^2-means is O(nk_nd+k^2d), where d is the dimension of the n data points and k is the number of clusters and usually n << k << k_n. Compared to k-means' O(nkd) complexity, our k^2-means complexity is significantly lower, at the expense of slightly increasing the memory complexity by O(nk_n+k^2). In our extensive experiments k^2-means is order(s) of magnitude faster than standard methods in computing accurate clusterings on several standard datasets and settings with hundreds of clusters and high dimensional data. Moreover, the proposed divisive initialization generally leads to clustering energies comparable to those achieved with the standard k-means++ initialization, while being significantly faster. version:1
arxiv-1605-09293 | Internal Guidance for Satallax | http://arxiv.org/abs/1605.09293 | id:1605.09293 author:Michael Färber, Chad Brown category:cs.LO cs.AI cs.LG  published:2016-05-30 summary:We propose a new internal guidance method for automated theorem provers based on the given-clause algorithm. Our method influences the choice of unprocessed clauses using positive and negative examples from previous proofs. To this end, we present an efficient scheme for Naive Bayesian classification by generalising label occurrences to types with monoid structure. This makes it possible to extend existing fast classifiers, which consider only positive examples, with negative ones. We implement the method in the higher-order logic prover Satallax, where we modify the delay with which propositions are processed. We evaluated our method on a simply-typed higher-order logic version of the Flyspeck project, where it solves 26% more problems than Satallax without internal guidance. version:1
arxiv-1509-04072 | Robust Gaussian Filtering using a Pseudo Measurement | http://arxiv.org/abs/1509.04072 | id:1509.04072 author:Manuel Wüthrich, Cristina Garcia Cifuentes, Sebastian Trimpe, Franziska Meier, Jeannette Bohg, Jan Issac, Stefan Schaal category:stat.ML cs.SY  published:2015-09-14 summary:Many sensors, such as range, sonar, radar, GPS and visual devices, produce measurements which are contaminated by outliers. This problem can be addressed by using fat-tailed sensor models, which account for the possibility of outliers. Unfortunately, all estimation algorithms belonging to the family of Gaussian filters (such as the widely-used extended Kalman filter and unscented Kalman filter) are inherently incompatible with such fat-tailed sensor models. The contribution of this paper is to show that any Gaussian filter can be made compatible with fat-tailed sensor models by applying one simple change: Instead of filtering with the physical measurement, we propose to filter with a pseudo measurement obtained by applying a feature function to the physical measurement. We derive such a feature function which is optimal under some conditions. Simulation results show that the proposed method can effectively handle measurement outliers and allows for robust filtering in both linear and nonlinear systems. version:3
arxiv-1605-01278 | A Bayesian Approach to Policy Recognition and State Representation Learning | http://arxiv.org/abs/1605.01278 | id:1605.01278 author:Adrian Šošić, Abdelhak M. Zoubir, Heinz Koeppl category:stat.ML cs.LG cs.SY math.DS math.PR  published:2016-05-04 summary:Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by an expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered situations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a deterministic optimal ground truth policy or require direct monitoring of the expert's controls, which limits their practical use as part of a general system identification framework. In this work, we consider the LfD problem in a more general setting where we allow for arbitrary stochastic expert policies, without reasoning about the quality of the demonstrations. In particular, we focus on the problem of policy recognition, which is to extract a system's latent control policy from observed system behavior. Following a Bayesian methodology allows us to consider various sources of uncertainty about the expert behavior, including the latent expert controls, to model the full posterior distribution of expert controllers. Further, we show that the same methodology can be applied in a nonparametric context to reason about the complexity of the state representation used by the expert and to learn task-appropriate partitionings of the system state space. version:2
arxiv-1605-07678 | An Analysis of Deep Neural Network Models for Practical Applications | http://arxiv.org/abs/1605.07678 | id:1605.07678 author:Alfredo Canziani, Adam Paszke, Eugenio Culurciello category:cs.CV  published:2016-05-24 summary:Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) fully connected layers are largely inefficient for smaller batches of images; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint are an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs. version:2
arxiv-1511-01259 | Transforming Wikipedia into an Ontology-based Information Retrieval Search Engine for Local Experts using a Third-Party Taxonomy | http://arxiv.org/abs/1511.01259 | id:1511.01259 author:Gregory Grefenstette, Karima Rafes category:cs.IR cs.CL  published:2015-11-04 summary:Wikipedia is widely used for finding general information about a wide variety of topics. Its vocation is not to provide local information. For example, it provides plot, cast, and production information about a given movie, but not showing times in your local movie theatre. Here we describe how we can connect local information to Wikipedia, without altering its content. The case study we present involves finding local scientific experts. Using a third-party taxonomy, independent from Wikipedia's category hierarchy, we index information connected to our local experts, present in their activity reports, and we re-index Wikipedia content using the same taxonomy. The connections between Wikipedia pages and local expert reports are stored in a relational database, accessible through as public SPARQL endpoint. A Wikipedia gadget (or plugin) activated by the interested user, accesses the endpoint as each Wikipedia page is accessed. An additional tab on the Wikipedia page allows the user to open up a list of teams of local experts associated with the subject matter in the Wikipedia page. The technique, though presented here as a way to identify local experts, is generic, in that any third party taxonomy, can be used in this to connect Wikipedia to any non-Wikipedia data source. version:2
arxiv-1605-09232 | Tradeoffs between Convergence Speed and Reconstruction Accuracy in Inverse Problems | http://arxiv.org/abs/1605.09232 | id:1605.09232 author:Raja Giryes, Yonina C. Eldar, Alex M. Bronstein, Guillermo Sapiro category:cs.NA cs.LG cs.NE math.OC stat.ML 65B99  90C59  published:2016-05-30 summary:Solving inverse problems with iterative algorithms such as stochastic gradient descent is a popular technique, especially for large data. In applications, due to time constraints, the number of iterations one may apply is usually limited, consequently limiting the accuracy achievable by certain methods. Given a reconstruction error one is willing to tolerate, an important question is whether it is possible to modify the original iterations to obtain a faster convergence to a minimizer with the allowed error. Relying on recent recovery techniques developed for settings in which the desired signal belongs to some low-dimensional set, we show that using a coarse estimate of this set leads to faster convergence to an error related to the accuracy of the set approximation. Our theory ties to recent advances in sparse recovery, compressed sensing and deep learning. In particular, it provides an explanation for the successful approximation of the ISTA solution by neural networks with layers representing iterations. version:1
arxiv-1605-09227 | Learning Combinatorial Functions from Pairwise Comparisons | http://arxiv.org/abs/1605.09227 | id:1605.09227 author:Maria-Florina Balcan, Ellen Vitercik, Colin White category:cs.LG cs.DS  published:2016-05-30 summary:A large body of work in machine learning has focused on the problem of learning a close approximation to an underlying combinatorial function, given a small set of labeled examples. However, for real-valued functions, cardinal labels might not be accessible, or it may be difficult for an expert to consistently assign real-valued labels over the entire set of examples. For instance, it is notoriously hard for consumers to reliably assign values to bundles of merchandise. Instead, it might be much easier for a consumer to report which of two bundles she likes better. With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support. version:1
arxiv-1605-09221 | Deep Reinforcement Learning Radio Control and Signal Detection with KeRLym, a Gym RL Agent | http://arxiv.org/abs/1605.09221 | id:1605.09221 author:Timothy J. O'Shea, T. Charles Clancy category:cs.LG  published:2016-05-30 summary:This paper presents research in progress investigating the viability and adaptation of reinforcement learning using deep neural network based function approximation for the task of radio control and signal detection in the wireless domain. We demonstrate a successful initial method for radio control which allows naive learning of search without the need for expert features, heuristics, or search strategies. We also introduce Kerlym, an open Keras based reinforcement learning agent collection for OpenAI's Gym. version:1
arxiv-1605-09211 | Going Deeper for Multilingual Visual Sentiment Detection | http://arxiv.org/abs/1605.09211 | id:1605.09211 author:Brendan Jou, Shih-Fu Chang category:cs.MM cs.CL cs.CV  published:2016-05-30 summary:This technical report details several improvements to the visual concept detector banks built on images from the Multilingual Visual Sentiment Ontology (MVSO). The detector banks are trained to detect a total of 9,918 sentiment-biased visual concepts from six major languages: English, Spanish, Italian, French, German and Chinese. In the original MVSO release, adjective-noun pair (ANP) detectors were trained for the six languages using an AlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a more extensive set of experiments, parameter tuning, and training runs, we detail and release higher accuracy models for detecting ANPs across six languages from the same image pool and setting as in the original release using a more modern architecture, GoogLeNet, providing comparable or better performance with reduced network parameter cost. In addition, since the image pool in MVSO can be corrupted by user noise from social interactions, we partitioned out a sub-corpus of MVSO images based on tag-restricted queries for higher fidelity labels. We show that as a result of these higher fidelity labels, higher performing AlexNet-styled ANP detectors can be trained using the tag-restricted image subset as compared to the models in full corpus. We release all these newly trained models for public research use along with the list of tag-restricted images from the MVSO dataset. version:1
arxiv-1605-09136 | Hyperspectral Image Classification with Support Vector Machines on Kernel Distribution Embeddings | http://arxiv.org/abs/1605.09136 | id:1605.09136 author:Gianni Franchi, Jesus Angulo, Dino Sejdinovic category:cs.CV cs.LG stat.ML  published:2016-05-30 summary:We propose a novel approach for pixel classification in hyperspectral images, leveraging on both the spatial and spectral information in the data. The introduced method relies on a recently proposed framework for learning on distributions -- by representing them with mean elements in reproducing kernel Hilbert spaces (RKHS) and formulating a classification algorithm therein. In particular, we associate each pixel to an empirical distribution of its neighbouring pixels, a judicious representation of which in an RKHS, in conjunction with the spectral information contained in the pixel itself, give a new explicit set of features that can be fed into a suite of standard classification techniques -- we opt for a well-established framework of support vector machines (SVM). Furthermore, the computational complexity is reduced via random Fourier features formalism. We study the consistency and the convergence rates of the proposed method and the experiments demonstrate strong performance on hyperspectral data with gains in comparison to the state-of-the-art results. version:1
arxiv-1605-09131 | Classification under Streaming Emerging New Classes: A Solution using Completely Random Trees | http://arxiv.org/abs/1605.09131 | id:1605.09131 author:Xin Mu, Kai Ming Ting, Zhi-Hua Zhou category:cs.LG  published:2016-05-30 summary:This paper investigates an important problem in stream mining, i.e., classification under streaming emerging new classes or SENC. The common approach is to treat it as a classification problem and solve it using either a supervised learner or a semi-supervised learner. We propose an alternative approach by using unsupervised learning as the basis to solve this problem. The SENC problem can be decomposed into three sub problems: detecting emerging new classes, classifying for known classes, and updating models to enable classification of instances of the new class and detection of more emerging new classes. The proposed method employs completely random trees which have been shown to work well in unsupervised learning and supervised learning independently in the literature. This is the first time, as far as we know, that completely random trees are used as a single common core to solve all three sub problems: unsupervised learning, supervised learning and model update in data streams. We show that the proposed unsupervised-learning-focused method often achieves significantly better outcomes than existing classification-focused methods. version:1
arxiv-1605-06950 | A Sub-Quadratic Exact Medoid Algorithm | http://arxiv.org/abs/1605.06950 | id:1605.06950 author:James Newling, François Fleuret category:stat.ML cs.DS cs.LG  published:2016-05-23 summary:We present a new algorithm, trimed, for obtaining the medoid of a set, that is the element of the set which minimises the mean distance to all other elements. The algorithm is shown to have, under weak assumptions, complexity O(N^(3/2)) in R^d where N is the set size, making it the first sub-quadratic exact medoid algorithm for d>1. Experiments show that it performs very well on spatial network data, frequently requiring two orders of magnitude fewer distances than state-of-the-art approximate algorithms. We show how trimed can be used as a component in an accelerated K-medoids algorithm, and how it can be relaxed to obtain further computational gains with an only minor loss in quality. version:2
arxiv-1602-02934 | Fast Mini-Batch K-Means by Nesting | http://arxiv.org/abs/1602.02934 | id:1602.02934 author:James Newling, François Fleuret category:stat.ML cs.LG  published:2016-02-09 summary:A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t+1. Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100 times earlier than the standard mini-batch algorithm. version:3
arxiv-1605-09128 | Control of Memory, Active Perception, and Action in Minecraft | http://arxiv.org/abs/1605.09128 | id:1605.09128 author:Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee category:cs.AI cs.CV cs.LG  published:2016-05-30 summary:In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures. version:1
arxiv-1603-06186 | The Multiscale Laplacian Graph Kernel | http://arxiv.org/abs/1603.06186 | id:1603.06186 author:Risi Kondor, Horace Pan category:stat.ML  published:2016-03-20 summary:Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character. In contrast, by building a hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG kernels) that we define in this paper can account for structure at a range of different scales. At the heart of the MLG construction is another new graph kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels to subgraphs recursively. To make the MLG kernel computationally feasible, we also introduce a randomized projection procedure, similar to the Nystr\"om method, but for RKHS operators. version:2
arxiv-1605-09116 | Image segmentation based on the hybrid total variation model and the K-means clustering strategy | http://arxiv.org/abs/1605.09116 | id:1605.09116 author:Baoli Shi, Zhi-Feng Pang, Jing Xu category:math.OC cs.CV  published:2016-05-30 summary:The performance of image segmentation highly relies on the original inputting image. When the image is contaminated by some noises or blurs, we can not obtain the efficient segmentation result by using direct segmentation methods. In order to efficiently segment the contaminated image, this paper proposes a two step method based on the hybrid total variation model with a box constraint and the K-means clustering method. In the first step, the hybrid model is based on the weighted convex combination between the total variation functional and the high-order total variation as the regularization term to obtain the original clustering data. In order to deal with non-smooth regularization term, we solve this model by employing the alternating split Bregman method. Then, in the second step, the segmentation can be obtained by thresholding this clustering data into different phases, where the thresholds can be given by using the K-means clustering method. Numerical comparisons show that our proposed model can provide more efficient segmentation results dealing with the noise image and blurring image. version:1
arxiv-1605-09114 | ParMAC: distributed optimisation of nested functions, with application to learning binary autoencoders | http://arxiv.org/abs/1605.09114 | id:1605.09114 author:Miguel Á. Carreira-Perpiñán, Mehdi Alizadeh category:cs.LG cs.DC cs.NE math.OC stat.ML  published:2016-05-30 summary:Many powerful machine learning models are based on the composition of multiple processing layers, such as deep nets, which gives rise to nonconvex objective functions. A general, recent approach to optimise such "nested" functions is the method of auxiliary coordinates (MAC). MAC introduces an auxiliary coordinate for each data point in order to decouple the nested model into independent submodels. This decomposes the optimisation into steps that alternate between training single layers and updating the coordinates. It has the advantage that it reuses existing single-layer algorithms, introduces parallelism, and does not need to use chain-rule gradients, so it works with nondifferentiable layers. With large-scale problems, or when distributing the computation is necessary for faster training, the dataset may not fit in a single machine. It is then essential to limit the amount of communication between machines so it does not obliterate the benefit of parallelism. We describe a general way to achieve this, ParMAC. ParMAC works on a cluster of processing machines with a circular topology and alternates two steps until convergence: one step trains the submodels in parallel using stochastic updates, and the other trains the coordinates in parallel. Only submodel parameters, no data or coordinates, are ever communicated between machines. ParMAC exhibits high parallelism, low communication overhead, and facilitates data shuffling, load balancing, fault tolerance and streaming data processing. We study the convergence of ParMAC and propose a theoretical model of its runtime and parallel speedup. We develop ParMAC to learn binary autoencoders for fast, approximate image retrieval. We implement it in MPI in a distributed system and demonstrate nearly perfect speedups in a 128-processor cluster with a training set of 100 million high-dimensional points. version:1
arxiv-1605-08361 | No bad local minima: Data independent training error guarantees for multilayer neural networks | http://arxiv.org/abs/1605.08361 | id:1605.08361 author:Daniel Soudry, Yair Carmon category:stat.ML cs.LG cs.NE  published:2016-05-26 summary:We use smoothed analysis techniques to provide guarantees on the training loss of Multilayer Neural Networks (MNNs) at differentiable local minima. Specifically, we examine MNNs with piecewise linear activation functions, quadratic loss and a single output, under mild over-parametrization. We prove that for a MNN with one hidden layer, the training error is zero at every differentiable local minimum, for almost every dataset and dropout-like noise realization. We then extend these results to the case of more than one hidden layer. Our theoretical guarantees assume essentially nothing on the training data, and are verified numerically. These results suggest why the highly non-convex loss of such MNNs can be easily optimized using local updates (e.g., stochastic gradient descent), as observed empirically. version:2
arxiv-1605-09090 | Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention | http://arxiv.org/abs/1605.09090 | id:1605.09090 author:Yang Liu, Chengjie Sun, Lei Lin, Xiaolong Wang category:cs.CL  published:2016-05-30 summary:In this paper, we proposed a sentence encoding-based model for recognizing text entailment. In our approach, the encoding of sentence is a two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate a first-stage sentence representation. Secondly, attention mechanism was employed to replace average pooling on the same sentence for better representations. Instead of using target sentence to attend words in source sentence, we utilized the sentence's first-stage representation to attend words appeared in itself, which is called "Inner-Attention" in our paper . Experiments conducted on Stanford Natural Language Inference (SNLI) Corpus has proved the effectiveness of "Inner-Attention" mechanism. With less number of parameters, our model outperformed the existing best sentence encoding-based approach by a large margin. version:1
arxiv-1510-08896 | Robust Shift-and-Invert Preconditioning: Faster and More Sample Efficient Algorithms for Eigenvector Computation | http://arxiv.org/abs/1510.08896 | id:1510.08896 author:Chi Jin, Sham M. Kakade, Cameron Musco, Praneeth Netrapalli, Aaron Sidford category:cs.DS cs.LG math.NA math.OC  published:2015-10-29 summary:We provide faster algorithms and improved sample complexities for approximating the top eigenvector of a matrix. Offline Setting: Given an $n \times d$ matrix $A$, we show how to compute an $\epsilon$ approximate top eigenvector in time $\tilde O ( [nnz(A) + \frac{d \cdot sr(A)}{gap^2}]\cdot \log 1/\epsilon )$ and $\tilde O([\frac{nnz(A)^{3/4} (d \cdot sr(A))^{1/4}}{\sqrt{gap}}]\cdot \log1/\epsilon )$. Here $sr(A)$ is the stable rank and $gap$ is the multiplicative eigenvalue gap. By separating the $gap$ dependence from $nnz(A)$ we improve on the classic power and Lanczos methods. We also improve prior work using fast subspace embeddings and stochastic optimization, giving significantly improved dependencies on $sr(A)$ and $\epsilon$. Our second running time improves this further when $nnz(A) \le \frac{d\cdot sr(A)}{gap^2}$. Online Setting: Given a distribution $D$ with covariance matrix $\Sigma$ and a vector $x_0$ which is an $O(gap)$ approximate top eigenvector for $\Sigma$, we show how to refine to an $\epsilon$ approximation using $\tilde O(\frac{v(D)}{gap^2} + \frac{v(D)}{gap \cdot \epsilon})$ samples from $D$. Here $v(D)$ is a natural variance measure. Combining our algorithm with previous work to initialize $x_0$, we obtain a number of improved sample complexity and runtime results. For general distributions, we achieve asymptotically optimal accuracy as a function of sample size as the number of samples grows large. Our results center around a robust analysis of the classic method of shift-and-invert preconditioning to reduce eigenvector computation to approximately solving a sequence of linear systems. We then apply fast SVRG based approximate system solvers to achieve our claims. We believe our results suggest the general effectiveness of shift-and-invert based approaches and imply that further computational gains may be reaped in practice. version:2
arxiv-1605-09088 | The Bayesian Linear Information Filtering Problem | http://arxiv.org/abs/1605.09088 | id:1605.09088 author:Bangrui Chen, Peter Frazier category:cs.LG  published:2016-05-30 summary:We present a Bayesian sequential decision-making formulation of the information filtering problem, in which an algorithm presents items (news articles, scientific papers, tweets) arriving in a stream, and learns relevance from user feedback on presented items. We model user preferences using a Bayesian linear model, similar in spirit to a Bayesian linear bandit. We compute a computational upper bound on the value of the optimal policy, which allows computing an optimality gap for implementable policies. We then use this analysis as motivation in introducing a pair of new Decompose-Then-Decide (DTD) heuristic policies, DTD-Dynamic-Programming (DTD-DP) and DTD-Upper-Confidence-Bound (DTD-UCB). We compare DTD-DP and DTD-UCB against several benchmarks on real and simulated data, demonstrating significant improvement, and show that the achieved performance is close to the upper bound. version:1
arxiv-1603-01431 | Normalization Propagation: A Parametric Technique for Removing Internal Covariate Shift in Deep Networks | http://arxiv.org/abs/1603.01431 | id:1603.01431 author:Devansh Arpit, Yingbo Zhou, Bhargava U. Kota, Venu Govindaraju category:stat.ML cs.LG  published:2016-03-04 summary:While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- \textit{Internal Covariate Shift}-- the current solution has certain drawbacks. For instance, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate due to shifting parameter values (especially during initial training epochs). Another fundamental problem with BN is that it cannot be used with batch-size $ 1 $ during training. We address these drawbacks of BN by proposing a non-adaptive normalization technique for removing covariate shift, that we call \textit{Normalization Propagation}. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers. version:4
arxiv-1605-09085 | Stochastic Function Norm Regularization of Deep Networks | http://arxiv.org/abs/1605.09085 | id:1605.09085 author:Amal Rannen Triki, Matthew B. Blaschko category:cs.LG cs.CV stat.ML  published:2016-05-30 summary:Deep neural networks have had an enormous impact on image analysis. State-of-the-art training methods, based on weight decay and DropOut, result in impressive performance when a very large training set is available. However, they tend to have large problems overfitting to small data sets. Indeed, the available regularization methods deal with the complexity of the network function only indirectly. In this paper, we study the feasibility of directly using the $L_2$ function norm for regularization. Two methods to integrate this new regularization in the stochastic backpropagation are proposed. Moreover, the convergence of these new algorithms is studied. We finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets (MNIST and CIFAR10). The obtained results demonstrate very clear improvement, especially in the context of small sample regimes with data laying in a low dimensional manifold. Source code of the method can be found at \url{https://github.com/AmalRT/DNN_Reg}. version:1
arxiv-1605-09082 | One-Pass Learning with Incremental and Decremental Features | http://arxiv.org/abs/1605.09082 | id:1605.09082 author:Chenping Hou, Zhi-Hua Zhou category:cs.LG  published:2016-05-30 summary:In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically. version:1
arxiv-1605-09081 | Understanding Convolutional Neural Networks | http://arxiv.org/abs/1605.09081 | id:1605.09081 author:Jayanth Koushik category:stat.ML cs.AI cs.CV cs.LG cs.NE  published:2016-05-30 summary:Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a variety of machine learning tasks. However, their mathematical properties and behavior are quite poorly understood. There is some work, in the form of a framework, for analyzing the operations that they perform. The goal of this project is to present key results from this theory, and provide intuition for why CNNs work. version:1
arxiv-1605-09068 | A budget-constrained inverse classification framework for smooth classifiers | http://arxiv.org/abs/1605.09068 | id:1605.09068 author:Michael T. Lash, Qihang Lin, W. Nick Street, Jennifer G. Robinson category:cs.LG stat.ML H.2.8  published:2016-05-29 summary:Inverse classification is the process of manipulating an instance such that it is more likely to conform to a specific class. Past methods that address such a problem have shortcomings. Greedy methods make changes that are overly radical, often relying on data that is strictly discrete. Other methods rely on certain data points, the presence of which cannot be guaranteed. In this paper we propose a general framework and method that overcomes these and other limitations. The formulation of our method uses any differentiable classification function. We demonstrate the method by using Gaussian kernel SVMs. We constrain the inverse classification to occur on features that can actually be changed, each of which incurs an individual cost. We further subject such changes to fall within a certain level of cumulative change (budget). Our framework can also accommodate the estimation of features whose values change as a consequence of actions taken (indirectly changeable features). Furthermore, we propose two methods for specifying feature-value ranges that result in different algorithmic behavior. We apply our method, and a proposed sensitivity analysis-based benchmark method, to two freely available datasets: Student Performance, from the UCI Machine Learning Repository and a real-world cardiovascular disease dataset. The results obtained demonstrate the validity and benefits of our framework and method. version:1
arxiv-1605-09066 | Distributed Asynchronous Stochastic Dual Coordinate Ascent without Duality | http://arxiv.org/abs/1605.09066 | id:1605.09066 author:Zhouyuan Huo, Heng Huang category:cs.LG  published:2016-05-29 summary:In this paper, we propose new Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA), and provide the proof of convergence rate for two cases: the individual loss is convex and the individual loss is non-convex but its expected loss is convex. Stochastic Dual Coordinate Ascent (SDCA) model is a popular method and often has better performances than stochastic gradient descent methods in solving regularized convex loss minimization problems. Dual-Free Stochastic Dual Coordinate Ascent method is a variation of SDCA, and can be applied to non-convex problem when its dual problem is meaningless. We extend Dual-Free Stochastic Dual Coordinate Ascent method to the distributed mode with considering the star network in this paper. version:1
arxiv-1605-09062 | Predicting Personal Traits from Facial Images using Convolutional Neural Networks Augmented with Facial Landmark Information | http://arxiv.org/abs/1605.09062 | id:1605.09062 author:Yoad Lewenberg, Yoram Bachrach, Sukrit Shankar, Antonio Criminisi category:cs.CV  published:2016-05-29 summary:We consider the task of predicting various traits of a person given an image of their face. We estimate both objective traits, such as gender, ethnicity and hair-color; as well as subjective traits, such as the emotion a person expresses or whether he is humorous or attractive. For sizeable experimentation, we contribute a new Face Attributes Dataset (FAD), having roughly 200,000 attribute labels for the above traits, for over 10,000 facial images. Due to the recent surge of research on Deep Convolutional Neural Networks (CNNs), we begin by using a CNN architecture for estimating facial attributes and show that they indeed provide an impressive baseline performance. To further improve performance, we propose a novel approach that incorporates facial landmark information for input images as an additional channel, helping the CNN learn better attribute-specific features so that the landmarks across various training images hold correspondence. We empirically analyse the performance of our method, showing consistent improvement over the baseline across traits. version:1
arxiv-1603-06807 | Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus | http://arxiv.org/abs/1603.06807 | id:1603.06807 author:Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio category:cs.CL cs.AI cs.LG cs.NE  published:2016-03-22 summary:Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions. version:2
arxiv-1605-09049 | Recycling Randomness with Structure for Sublinear time Kernel Expansions | http://arxiv.org/abs/1605.09049 | id:1605.09049 author:Krzysztof Choromanski, Vikas Sindhwani category:cs.LG cs.NA stat.ML  published:2016-05-29 summary:We propose a scheme for recycling Gaussian random vectors into structured matrices to approximate various kernel functions in sublinear time via random embeddings. Our framework includes the Fastfood construction as a special case, but also extends to Circulant, Toeplitz and Hankel matrices, and the broader family of structured matrices that are characterized by the concept of low-displacement rank. We introduce notions of coherence and graph-theoretic structural constants that control the approximation quality, and prove unbiasedness and low-variance properties of random feature maps that arise within our framework. For the case of low-displacement matrices, we show how the degree of structure and randomness can be controlled to reduce statistical variance at the cost of increased computation and storage requirements. Empirical results strongly support our theory and justify the use of a broader family of structured matrices for scaling up kernel methods using random features. version:1
arxiv-1603-05629 | Discriminative Embeddings of Latent Variable Models for Structured Data | http://arxiv.org/abs/1603.05629 | id:1603.05629 author:Hanjun Dai, Bo Dai, Le Song category:cs.LG  published:2016-03-17 summary:Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are $10,000$ times smaller, while at the same time achieving the state-of-the-art predictive performance. version:3
arxiv-1604-05129 | Memory shapes time perception and intertemporal choices | http://arxiv.org/abs/1604.05129 | id:1604.05129 author:Pedro A. Ortega, Naftali Tishby category:q-bio.NC cs.AI stat.ML  published:2016-04-18 summary:There is a consensus that human and non-human subjects experience temporal distortions in many stages of their perceptual and decision-making systems. Similarly, intertemporal choice research has shown that decision-makers undervalue future outcomes relative to immediate ones. Here we combine techniques from information theory and artificial intelligence to show how both temporal distortions and intertemporal choice preferences can be explained as a consequence of the coding efficiency of sensorimotor representation. In particular, the model implies that interactions that constrain future behavior are perceived as being both longer in duration and more valuable. Furthermore, using simulations of artificial agents, we investigate how memory constraints enforce a renormalization of the perceived timescales. Our results show that qualitatively different discount functions, such as exponential and hyperbolic discounting, arise as a consequence of an agent's probabilistic model of the world. version:2
arxiv-1602-02867 | Value Iteration Networks | http://arxiv.org/abs/1602.02867 | id:1602.02867 author:Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel category:cs.AI cs.LG cs.NE stat.ML  published:2016-02-09 summary:We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within. VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning. Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation. We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task. We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains. version:2
arxiv-1605-09042 | MCMC assisted by Belief Propagaion | http://arxiv.org/abs/1605.09042 | id:1605.09042 author:Sungsoo Ahn, Michael Chertkov, Jinwoo Shin category:stat.ML cs.AI cs.DS  published:2016-05-29 summary:Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus (LC) approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes. version:1
arxiv-1605-09026 | Singular ridge regression with homoscedastic residuals: generalization error with estimated parameters | http://arxiv.org/abs/1605.09026 | id:1605.09026 author:Lyudmila Grigoryeva, Juan-Pablo Ortega category:stat.ML stat.ME  published:2016-05-29 summary:This paper characterizes the conditional distribution properties of the finite sample ridge regression estimator and uses that result to evaluate total regression and generalization errors that incorporate the inaccuracies committed at the time of parameter estimation. The paper provides explicit formulas for those errors. Unlike other classical references in this setup, our results take place in a fully singular setup that does not assume the existence of a solution for the non-regularized regression problem. In exchange, we invoke a conditional homoscedasticity hypothesis on the regularized regression residuals that is crucial in our developments. version:1
arxiv-1603-00988 | Learning Functions: When Is Deep Better Than Shallow | http://arxiv.org/abs/1603.00988 | id:1603.00988 author:Hrushikesh Mhaskar, Qianli Liao, Tomaso Poggio category:cs.LG  published:2016-03-03 summary:While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower number of training parameters as well as VC-dimension. This theorem settles an old conjecture by Bengio on the role of depth in networks. We then define a general class of scalable, shift-invariant algorithms to show a simple and natural set of requirements that justify deep convolutional networks. version:4
arxiv-1605-00788 | Online Learning of Commission Avoidant Portfolio Ensembles | http://arxiv.org/abs/1605.00788 | id:1605.00788 author:Guy Uziel, Ran El-Yaniv category:cs.AI cs.LG  published:2016-05-03 summary:We present a novel online ensemble learning strategy for portfolio selection. The new strategy controls and exploits any set of commission-oblivious portfolio selection algorithms. The strategy handles transaction costs using a novel commission avoidance mechanism. We prove a logarithmic regret bound for our strategy with respect to optimal mixtures of the base algorithms. Numerical examples validate the viability of our method and show significant improvement over the state-of-the-art. version:2
arxiv-1504-01362 | A New Approach to Building the Interindustry Input--Output Table | http://arxiv.org/abs/1504.01362 | id:1504.01362 author:Ryohei Hisano category:stat.ML  published:2015-04-06 summary:We present a new approach to estimating the interdependence of industries in an economy by applying data science solutions. By exploiting interfirm buyer--seller network data, we show that the problem of estimating the interdependence of industries is similar to the problem of uncovering the latent block structure in network science literature. To estimate the underlying structure with greater accuracy, we propose an extension of the sparse block model that incorporates node textual information and an unbounded number of industries and interactions among them. The latter task is accomplished by extending the well-known Chinese restaurant process to two dimensions. Inference is based on collapsed Gibbs sampling, and the model is evaluated on both synthetic and real-world datasets. We show that the proposed model improves in predictive accuracy and successfully provides a satisfactory solution to the motivated problem. We also discuss issues that affect the future performance of this approach. version:7
arxiv-1605-09004 | Tight (Lower) Bounds for the Fixed Budget Best Arm Identification Bandit Problem | http://arxiv.org/abs/1605.09004 | id:1605.09004 author:Alexandra Carpentier, Andrea Locatelli category:stat.ML cs.LG  published:2016-05-29 summary:We consider the problem of \textit{best arm identification} with a \textit{fixed budget $T$}, in the $K$-armed stochastic bandit setting, with arms distribution defined on $[0,1]$. We prove that any bandit strategy, for at least one bandit problem characterized by a complexity $H$, will misidentify the best arm with probability lower bounded by $$\exp\Big(-\frac{T}{\log(K)H}\Big),$$ where $H$ is the sum for all sub-optimal arms of the inverse of the squared gaps. Our result disproves formally the general belief - coming from results in the fixed confidence setting - that there must exist an algorithm for this problem whose probability of error is upper bounded by $\exp(-T/H)$. This also proves that some existing strategies based on the Successive Rejection of the arms are optimal - closing therefore the current gap between upper and lower bounds for the fixed budget best arm identification problem. version:1
arxiv-1605-05034 | LIME: A Method for Low-light IMage Enhancement | http://arxiv.org/abs/1605.05034 | id:1605.05034 author:Xiaojie Guo category:cs.CV  published:2016-05-17 summary:When one captures images in low-light conditions, the images often suffer from low visibility. This poor quality may significantly degrade the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a very simple and effective method, named as LIME, to enhance low-light images. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G and B channels (Max-RGB). Further, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging real-world low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts. version:2
arxiv-1605-08988 | On Explore-Then-Commit Strategies | http://arxiv.org/abs/1605.08988 | id:1605.08988 author:Aurélien Garivier, Emilie Kaufmann, Tor Lattimore category:math.ST cs.LG stat.TH  published:2016-05-29 summary:We study the problem of minimising regret in two-armed bandit problems with Gaussian rewards. Our objective is to use this simple setting to illustrate that strategies based on an exploration phase (up to a stopping time) followed by exploitation are necessarily suboptimal. The results hold regardless of whether or not the difference in means between the two arms is known. Besides the main message, we also refine existing deviation inequalities, which allow us to design fully sequential strategies with finite-time regret guarantees that are (a) asymptotically optimal as the horizon grows and (b) order-optimal in the minimax sense. Furthermore we provide empirical evidence that the theory also holds in practice and discuss extensions to non-gaussian and multiple-armed case. version:1
arxiv-1506-06343 | Mining Mid-level Visual Patterns with Deep CNN Activations | http://arxiv.org/abs/1506.06343 | id:1506.06343 author:Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2015-06-21 summary:The purpose of mid-level visual element discovery is to find clusters of image patches that are both representative and discriminative. Here we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). We observe that a fully-connected CNN activation extracted from an image patch typically possesses two appealing properties that enable its seamless integration with pattern mining techniques. The marriage between CNN activations and association rule mining, a well-known pattern mining technique in the literature, leads to fast and effective discovery of representative and discriminative patterns from a huge number of image patches. When we retrieve and visualize image patches with the same pattern, surprisingly, they are not only visually similar but also semantically consistent, and thus give rise to a mid-level visual element in our work. Given the patterns and retrieved mid-level visual elements, we propose two methods to generate image feature representations for each. The first method is to use the patterns as codewords in a dictionary, similar to the Bag-of-Visual-Words model, we compute a Bag-of-Patterns representation. The second one relies on the retrieved mid-level visual elements to construct a Bag-of-Elements representation. We evaluate the two encoding methods on scene and object classification tasks, and demonstrate that our approach outperforms or matches recent works using CNN activations for these tasks. version:3
arxiv-1603-04733 | Structured and Efficient Variational Deep Learning with Matrix Gaussian Posteriors | http://arxiv.org/abs/1603.04733 | id:1603.04733 author:Christos Louizos, Max Welling category:stat.ML cs.LG  published:2016-03-15 summary:We introduce a variational Bayesian neural network where the parameters are governed via a probability distribution on random matrices. Specifically, we employ a matrix variate Gaussian \cite{gupta1999matrix} parameter posterior distribution where we explicitly model the covariance among the input and output dimensions of each layer. Furthermore, with approximate covariance matrices we can achieve a more efficient way to represent those correlations that is also cheaper than fully factorized parameter posteriors. We further show that with the "local reprarametrization trick" \cite{kingma2015variational} on this posterior distribution we arrive at a Gaussian Process \cite{rasmussen2006gaussian} interpretation of the hidden units in each layer and we, similarly with \cite{gal2015dropout}, provide connections with deep Gaussian processes. We continue in taking advantage of this duality and incorporate "pseudo-data" \cite{snelson2005sparse} in our model, which in turn allows for more efficient sampling while maintaining the properties of the original model. The validity of the proposed approach is verified through extensive experiments. version:4
arxiv-1602-07764 | Reinforcement Learning of POMDPs using Spectral Methods | http://arxiv.org/abs/1602.07764 | id:1602.07764 author:Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar category:cs.AI cs.LG cs.NA math.OC stat.ML  published:2016-02-25 summary:We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound with respect to the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces. version:2
arxiv-1411-4419 | Automatic Subspace Learning via Principal Coefficients Embedding | http://arxiv.org/abs/1411.4419 | id:1411.4419 author:Xi Peng, Jiwen Lu, Rui Yan, Zhang Yi category:cs.CV  published:2014-11-17 summary:In this paper, we address two challenging problems in unsupervised subspace learning: 1) how to automatically identify the feature dimension of the learned subspace (i.e., automatic subspace learning), and 2) how to learn the underlying subspace in the presence of Gaussian noise (i.e., robust subspace learning). We show that these two problems can be simultaneously solved by proposing a new method (called principal coefficients embedding, PCE). For a given data set $\mathbf{D}\in \mathds{R}^{m\times n}$, PCE recovers a clean data set $\mathbf{D}_{0}\in \mathds{R}^{m\times n}$ from $\mathbf{D}$ and simultaneously learns a global reconstruction relation $\mathbf{C}\in \mathbf{R}^{n\times n}$ of $\mathbf{D}_{0}$. By preserving $\mathbf{C}$ into an $m^{\prime}$-dimensional space, the proposed method obtains a projection matrix that can capture the latent manifold structure of $\mathbf{D}_{0}$, where $m^{\prime}\ll m$ is automatically determined by the rank of $\mathbf{C}$ with theoretical guarantees. PCE has three advantages: 1) it can automatically determine the feature dimension even though data are sampled from a union of multiple linear subspaces in presence of the Gaussian noise; 2) Although the objective function of PCE only considers the Gaussian noise, experimental results show that it is robust to the non-Gaussian noise (\textit{e.g.}, random pixel corruption) and real disguises; 3) Our method has a closed-form solution and can be calculated very fast. Extensive experimental results show the superiority of PCE on a range of databases with respect to the classification accuracy, robustness and efficiency. version:4
arxiv-1605-08961 | A simple and provable algorithm for sparse diagonal CCA | http://arxiv.org/abs/1605.08961 | id:1605.08961 author:Megasthenis Asteris, Anastasios Kyrillidis, Oluwasanmi Koyejo, Russell Poldrack category:stat.ML cs.DS cs.IT math.IT math.OC stat.ME  published:2016-05-29 summary:Given two sets of variables, derived from a common set of samples, sparse Canonical Correlation Analysis (CCA) seeks linear combinations of a small number of variables in each set, such that the induced canonical variables are maximally correlated. Sparse CCA is NP-hard. We propose a novel combinatorial algorithm for sparse diagonal CCA, i.e., sparse CCA under the additional assumption that variables within each set are standardized and uncorrelated. Our algorithm operates on a low rank approximation of the input data and its computational complexity scales linearly with the number of input variables. It is simple to implement, and parallelizable. In contrast to most existing approaches, our algorithm administers precise control on the sparsity of the extracted canonical vectors, and comes with theoretical data-dependent global approximation guarantees, that hinge on the spectrum of the input data. Finally, it can be straightforwardly adapted to other constrained variants of CCA enforcing structure beyond sparsity. We empirically evaluate the proposed scheme and apply it on a real neuroimaging dataset to investigate associations between brain activity and behavior measurements. version:1
arxiv-1602-05110 | Generating images with recurrent adversarial networks | http://arxiv.org/abs/1602.05110 | id:1602.05110 author:Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, Roland Memisevic category:cs.LG cs.CV  published:2016-02-16 summary:Gatys et al. (2015) showed that optimizing pixels to match features in a convolutional network with respect reference image features is a way to render images of high visual quality. We show that unrolling this gradient-based optimization yields a recurrent computation that creates images by incrementally adding onto a visual "canvas". We propose a recurrent generative model inspired by this view, and show that it can be trained using adversarial training to generate very good image samples. We also propose a way to quantitatively compare adversarial networks by having the generators and discriminators of these networks compete against each other. version:4
arxiv-1603-01354 | End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF | http://arxiv.org/abs/1603.01354 | id:1603.01354 author:Xuezhe Ma, Eduard Hovy category:cs.LG cs.CL stat.ML  published:2016-03-04 summary:State-of-the-art sequence labeling systems traditionally require large amounts of task-specific knowledge in the form of hand-crafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- and character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data pre-processing, thus making it applicable to a wide range of sequence labeling tasks. We evaluate our system on two data sets for two sequence labeling tasks --- Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain state-of-the-art performance on both the two data --- 97.55\% accuracy for POS tagging and 91.21\% F1 for NER. version:5
arxiv-1106-1379 | A Unified Framework for Approximating and Clustering Data | http://arxiv.org/abs/1106.1379 | id:1106.1379 author:Dan Feldman, Michael Langberg category:cs.LG  published:2011-06-07 summary:Given a set $F$ of $n$ positive functions over a ground set $X$, we consider the problem of computing $x^*$ that minimizes the expression $\sum_{f\in F}f(x)$, over $x\in X$. A typical application is \emph{shape fitting}, where we wish to approximate a set $P$ of $n$ elements (say, points) by a shape $x$ from a (possibly infinite) family $X$ of shapes. Here, each point $p\in P$ corresponds to a function $f$ such that $f(x)$ is the distance from $p$ to $x$, and we seek a shape $x$ that minimizes the sum of distances from each point in $P$. In the $k$-clustering variant, each $x\in X$ is a tuple of $k$ shapes, and $f(x)$ is the distance from $p$ to its closest shape in $x$. Our main result is a unified framework for constructing {\em coresets} and {\em approximate clustering} for such general sets of functions. To achieve our results, we forge a link between the classic and well defined notion of $\varepsilon$-approximations from the theory of PAC Learning and VC dimension, to the relatively new (and not so consistent) paradigm of coresets, which are some kind of "compressed representation" of the input set $F$. Using traditional techniques, a coreset usually implies an LTAS (linear time approximation scheme) for the corresponding optimization problem, which can be computed in parallel, via one pass over the data, and using only polylogarithmic space (i.e, in the streaming model). We show how to generalize the results of our framework for squared distances (as in $k$-mean), distances to the $q$th power, and deterministic constructions. version:4
arxiv-1605-08933 | Interaction Pursuit with Feature Screening and Selection | http://arxiv.org/abs/1605.08933 | id:1605.08933 author:Yingying Fan, Yinfei Kong, Daoji Li, Jinchi Lv category:stat.ME stat.ML  published:2016-05-28 summary:Understanding how features interact with each other is of paramount importance in many scientific discoveries and contemporary applications. Yet interaction identification becomes challenging even for a moderate number of covariates. In this paper, we suggest an efficient and flexible procedure, called the interaction pursuit (IP), for interaction identification in ultra-high dimensions. The suggested method first reduces the number of interactions and main effects to a moderate scale by a new feature screening approach, and then selects important interactions and main effects in the reduced feature space using regularization methods. Compared to existing approaches, our method screens interactions separately from main effects and thus can be more effective in interaction screening. Under a fairly general framework, we establish that for both interactions and main effects, the method enjoys the sure screening property in screening and oracle inequalities in selection. Our method and theoretical results are supported by several simulation and real data examples. version:1
arxiv-1411-1810 | Variational Tempering | http://arxiv.org/abs/1411.1810 | id:1411.1810 author:Stephan Mandt, James McInerney, Farhan Abrol, Rajesh Ranganath, David Blei category:stat.ML cs.LG  published:2014-11-07 summary:Variational inference (VI) combined with data subsampling enables approximate posterior inference over large data sets, but suffers from poor local optima. We first formulate a deterministic annealing approach for the generic class of conditionally conjugate exponential family models. This approach uses a decreasing temperature parameter which deterministically deforms the objective during the course of the optimization. A well-known drawback to this annealing approach is the choice of the cooling schedule. We therefore introduce variational tempering, a variational algorithm that introduces a temperature latent variable to the model. In contrast to related work in the Markov chain Monte Carlo literature, this algorithm results in adaptive annealing schedules. Lastly, we develop local variational tempering, which assigns a latent temperature to each data point; this allows for dynamic annealing that varies across data. Compared to the traditional VI, all proposed approaches find improved predictive likelihoods on held-out data. version:4
arxiv-1511-00352 | Spatial Semantic Scan: Jointly Detecting Subtle Events and their Spatial Footprint | http://arxiv.org/abs/1511.00352 | id:1511.00352 author:Abhinav Maurya category:cs.LG cs.CL stat.ML  published:2015-11-02 summary:Many methods have been proposed for detecting emerging events in text streams using topic modeling. However, these methods have shortcomings that make them unsuitable for rapid detection of locally emerging events on massive text streams. We describe Spatially Compact Semantic Scan (SCSS) that has been developed specifically to overcome the shortcomings of current methods in detecting new spatially compact events in text streams. SCSS employs alternating optimization between using semantic scan to estimate contrastive foreground topics in documents, and discovering spatial neighborhoods with high occurrence of documents containing the foreground topics. We evaluate our method on Emergency Department chief complaints dataset (ED dataset) to verify the effectiveness of our method in detecting real-world disease outbreaks from free-text ED chief complaint data. version:3
arxiv-1605-08912 | A Riemannian Framework for Statistical Analysis of Topological Persistence Diagrams | http://arxiv.org/abs/1605.08912 | id:1605.08912 author:Rushil Anirudh, Vinay Venkataraman, Karthikeyan Natesan Ramamurthy, Pavan Turaga category:math.AT cs.CG cs.CV math.DG math.ST stat.TH  published:2016-05-28 summary:Topological data analysis is becoming a popular way to study high dimensional feature spaces without any contextual clues or assumptions. This paper concerns itself with one popular topological feature, which is the number of $d-$dimensional holes in the dataset, also known as the Betti$-d$ number. The persistence of the Betti numbers over various scales is encoded into a persistence diagram (PD), which indicates the birth and death times of these holes as scale varies. A common way to compare PDs is by a point-to-point matching, which is given by the $n$-Wasserstein metric. However, a big drawback of this approach is the need to solve correspondence between points before computing the distance; for $n$ points, the complexity grows according to $\mathcal{O}($n$^3)$. Instead, we propose to use an entirely new framework built on Riemannian geometry, that models PDs as 2D probability density functions that are represented in the square-root framework on a Hilbert Sphere. The resulting space is much more intuitive with closed form expressions for common operations. The distance metric is 1) correspondence-free and also 2) independent of the number of points in the dataset. The complexity of computing distance between PDs now grows according to $\mathcal{O}(K^2)$, for a $K \times K$ discretization of $[0,1]^2$. This also enables the use of existing machinery in differential geometry towards statistical analysis of PDs such as computing the mean, geodesics, classification etc. We report competitive results with the Wasserstein metric, at a much lower computational load, indicating the favorable properties of the proposed approach. version:1
arxiv-1605-08900 | Aspect Level Sentiment Classification with Deep Memory Network | http://arxiv.org/abs/1605.08900 | id:1605.08900 author:Duyu Tang, Bing Qin, Ting Liu category:cs.CL  published:2016-05-28 summary:We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. version:1
arxiv-1605-08889 | Beyond Majority Voting: Generating Evaluation Scales using Item Response Theory | http://arxiv.org/abs/1605.08889 | id:1605.08889 author:John Lalor, Hao Wu, Hong Yu category:cs.CL  published:2016-05-28 summary:We introduce Item Response Theory (IRT) from psychometrics as an alternative to majority voting to create an IRT gold standard ($GS_{IRT}$). IRT describes characteristics of individual items in $GS_{IRT}$ - their difficulty and discriminating power - and is able to account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we evaluated IRT's model-fitting of a majority vote gold standard designed for Recognizing Textual Entailment (RTE), denoted as $GS_{RTE}$. By collecting human responses and fitting our IRT model, we found that up to 31% of $GS_{RTE}$ were not useful in building $GS_{IRT}$ for RTE. In addition, we found low inter-annotator agreement for some items in $GS_{RTE}$ suggesting that more work is needed for creating intelligent gold-standards. version:1
arxiv-1605-08882 | Optimal Learning for Multi-pass Stochastic Gradient Methods | http://arxiv.org/abs/1605.08882 | id:1605.08882 author:Junhong Lin, Lorenzo Rosasco category:cs.LG math.OC stat.ML  published:2016-05-28 summary:We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases. version:1
arxiv-1605-08881 | Sparse Coding and Counting for Robust Visual Tracking | http://arxiv.org/abs/1605.08881 | id:1605.08881 author:Risheng Liu, Jing Wang, Yiyang Wang, Zhixun Su, Yu Cai category:cs.CV  published:2016-05-28 summary:In this paper, we propose a novel sparse coding and counting method under Bayesian framwork for visual tracking. In contrast to existing methods, the proposed method employs the combination of L0 and L1 norm to regularize the linear coefficients of incrementally updated linear basis. The sparsity constraint enables the tracker to effectively handle difficult challenges, such as occlusion or image corruption. To achieve realtime processing, we propose a fast and efficient numerical algorithm for solving the proposed model. Although it is an NP-hard problem, the proposed accelerated proximal gradient (APG) approach is guaranteed to converge to a solution quickly. Besides, we provide a closed solution of combining L0 and L1 regularized representation to obtain better sparsity. Experimental results on challenging video sequences demonstrate that the proposed method achieves state-of-the-art results both in accuracy and speed. version:1
arxiv-1605-08872 | Online Bayesian Collaborative Topic Regression | http://arxiv.org/abs/1605.08872 | id:1605.08872 author:Chenghao Liu, Tao Jin, Steven C. H. Hoi, Peilin Zhao, Jianling Sun category:cs.LG cs.IR  published:2016-05-28 summary:Collaborative Topic Regression (CTR) combines ideas of probabilistic matrix factorization (PMF) and topic modeling (e.g., LDA) for recommender systems, which has gained increasing successes in many applications. Despite enjoying many advantages, the existing CTR algorithms have some critical limitations. First of all, they are often designed to work in a batch learning manner, making them unsuitable to deal with streaming data or big data in real-world recommender systems. Second, the document-specific topic proportions of LDA are fed to the downstream PMF, but not reverse, which is sub-optimal as the rating information is not exploited in discovering the low-dimensional representation of documents and thus can result in a sub-optimal representation for prediction. In this paper, we propose a novel scheme of Online Bayesian Collaborative Topic Regression (OBCTR) which is efficient and scalable for learning from data streams. Particularly, we {\it jointly} optimize the combined objective function of both PMF and LDA in an online learning fashion, in which both PMF and LDA tasks can be reinforced each other during the online learning process. Our encouraging experimental results on real-world data validate the effectiveness of the proposed method. version:1
arxiv-1604-00642 | Multi-objective design of quantum circuits using genetic programming | http://arxiv.org/abs/1604.00642 | id:1604.00642 author:Moein Sarvaghad-Moghaddam category:cs.ET cs.NE  published:2016-04-03 summary:Quantum computing is a new way of data processing based on the concept of quantum mechanics. Quantum circuit design is a process of converting a quantum gate to a series of basic gates and is divided into two general categories based on the decomposition and composition. In the second group, using evolutionary algorithms and especially genetic algorithms, multiplication of matrix gates was used to achieve the final characteristic of quantum circuit. Genetic programming is a subfield of evolutionary computing in which computer programs evolve to solve studied problems. In past research that has been done in the field of quantum circuits design, only one cost metrics (usually quantum cost) has been investigated. In this paper for the first time, a multi-objective approach has been provided to design quantum circuits using genetic programming that considers the depth and the cost of nearest neighbor metrics in addition to quantum cost metric. Another innovation of this article is the use of two-step fitness function and taking into account the equivalence of global phase in quantum gates. The results show that the proposed method is able to find a good answer in a short time. version:2
arxiv-1605-08857 | Video Key Frame Extraction using Entropy value as Global and Local Feature | http://arxiv.org/abs/1605.08857 | id:1605.08857 author:Siddu P Algur, Vivek R category:cs.CV  published:2016-05-28 summary:Key frames play an important role in video annotation. It is one of the widely used methods for video abstraction as this will help us for processing a large set of video data with sufficient content representation in faster way. In this paper a novel approach for key-frame extraction using entropy value is proposed. The proposed approach classifies frames based on entropy values as global feature and selects frame from each class as representative key-frame. It also eliminates redundant frames from selected key-frames using entropy value as local feature. Evaluation of the approach on several video clips has been presented. Results show that the algorithm is successful in helping annotators automatically identify video key-frames. version:1
arxiv-1605-08856 | A Channelized Binning Method for Extraction of Dominant Color Pixel Value | http://arxiv.org/abs/1605.08856 | id:1605.08856 author:Siddu P Algur, N H Ayachit, Vivek R category:cs.CV  published:2016-05-28 summary:The Color is one of the most important and easily identifiable features for describing the visual content. The MPEG standard has developed a number of descriptors that covers different aspects of the visual content. The Dominant color descriptor is one of them. This paper proposes a channelized binning approach a novel method for extraction of the dominant color pixel value which is a variant of the dominant color descriptor. The Channelized binning method treats the problem as a statistical problem and tries to avoid color quantization and interpolation guessing of number and centroid of dominant colors. Channelized binning is an iterative approach which automatically estimates the number of dominant pixel values and their centroids. It operates on 24 bit full RGB color space, by considering one color channel at a time and hence avoiding the color quantization. Results show that the proposed method can successfully extract dominant color pixel values. version:1
arxiv-1512-01904 | Gauss quadrature for matrix inverse forms with applications | http://arxiv.org/abs/1512.01904 | id:1512.01904 author:Chengtao Li, Suvrit Sra, Stefanie Jegelka category:stat.ML cs.NA  published:2015-12-07 summary:We present a framework for accelerating a spectrum of machine learning algorithms that require computation of bilinear inverse forms $u^\top A^{-1}u$, where $A$ is a positive definite matrix and $u$ a given vector. Our framework is built on Gauss-type quadrature and easily scales to large, sparse matrices. Further, it allows retrospective computation of lower and upper bounds on $u^\top A^{-1}u$, which in turn accelerates several algorithms. We prove that these bounds tighten iteratively and converge at a linear (geometric) rate. To our knowledge, ours is the first work to demonstrate these key properties of Gauss-type quadrature, which is a classical and deeply studied topic. We illustrate empirical consequences of our results by using quadrature to accelerate machine learning tasks involving determinantal point processes and submodular optimization, and observe tremendous speedups in several instances. version:2
arxiv-1603-06052 | Fast DPP Sampling for Nyström with Application to Kernel Methods | http://arxiv.org/abs/1603.06052 | id:1603.06052 author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:cs.LG  published:2016-03-19 summary:The Nystr\"om method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystr\"om using Determinantal Point Processes (DPPs), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via DPPs guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of DPPsampling, we show that (under certain conditions) Markov chain DPP sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of DPP-based landmark selection compared with existing approaches. version:2
arxiv-1602-07416 | Learning to Generate with Memory | http://arxiv.org/abs/1602.07416 | id:1602.07416 author:Chongxuan Li, Jun Zhu, Bo Zhang category:cs.LG cs.CV  published:2016-02-24 summary:Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation. version:2
arxiv-1605-08838 | Dueling Bandits with Dependent Arms | http://arxiv.org/abs/1605.08838 | id:1605.08838 author:Bangrui Chen, Peter Frazier category:cs.LG  published:2016-05-28 summary:We consider online content recommendation with implicit feedback through pairwise comparisons. We study a new formulation of the dueling bandit problems in which arms are dependent and regret occurs when neither pulled arm is optimal. We propose a new algorithm, Comparing The Best (CTB), with computational requirements appropriate for problems with few arms, and a variation of this algorithm whose computation scales to problems with many arms. We show both algorithms have constant expected cumulative regret. We demonstrate through numerical experiments on simulated and real dataset that these algorithms improve significantly over existing algorithms in the setting we study. version:1
arxiv-1605-08833 | Muffled Semi-Supervised Learning | http://arxiv.org/abs/1605.08833 | id:1605.08833 author:Akshay Balsubramani, Yoav Freund category:cs.LG stat.ML  published:2016-05-28 summary:We explore a novel approach to semi-supervised learning. This approach is contrary to the common approach in that the unlabeled examples serve to "muffle," rather than enhance, the guidance provided by the labeled examples. We provide several variants of the basic algorithm and show experimentally that they can achieve significantly higher AUC than boosted trees, random forests and logistic regression when unlabeled examples are available. version:1
arxiv-1506-02975 | Stagewise Learning for Sparse Clustering of Discretely-Valued Data | http://arxiv.org/abs/1506.02975 | id:1506.02975 author:Vincent Zhao, Steven W. Zucker category:stat.ML cs.LG q-bio.QM  published:2015-06-09 summary:The performance of EM in learning mixtures of product distributions often depends on the initialization. This can be problematic in crowdsourcing and other applications, e.g. when a small number of 'experts' are diluted by a large number of noisy, unreliable participants. We develop a new EM algorithm that is driven by these experts. In a manner that differs from other approaches, we start from a single mixture class. The algorithm then develops the set of 'experts' in a stagewise fashion based on a mutual information criterion. At each stage EM operates on this subset of the players, effectively regularizing the E rather than the M step. Experiments show that stagewise EM outperforms other initialization techniques for crowdsourcing and neurosciences applications, and can guide a full EM to results comparable to those obtained knowing the exact distribution. version:2
arxiv-1605-08831 | Weighted Residuals for Very Deep Networks | http://arxiv.org/abs/1605.08831 | id:1605.08831 author:Falong Shen, Gang Zeng category:cs.CV  published:2016-05-28 summary:Deep residual networks have recently shown appealing performance on many challenging computer vision tasks. However, the original residual structure still has some defects making it difficult to converge on very deep networks. In this paper, we introduce a weighted residual network to address the incompatibility between \texttt{ReLU} and element-wise addition and the deep network initialization problem. The weighted residual network is able to learn to combine residuals from different layers effectively and efficiently. The proposed models enjoy a consistent improvement over accuracy and convergence with increasing depths from 100+ layers to 1000+ layers. Besides, the weighted residual networks have little more computation and GPU memory burden than the original residual networks. The networks are optimized by projected stochastic gradient descent. Experiments on CIFAR-10 have shown that our algorithm has a \emph{faster convergence speed} than the original residual networks and reaches a \emph{high accuracy} at 95.3\% with a 1192-layer model. version:1
arxiv-1509-01618 | Efficient Sampling for k-Determinantal Point Processes | http://arxiv.org/abs/1509.01618 | id:1509.01618 author:Chengtao Li, Stefanie Jegelka, Suvrit Sra category:cs.LG  published:2015-09-04 summary:Determinantal Point Processes (DPPs) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete $k$-DPPs. Our method takes advantage of the diversity property of subsets sampled from a DPP, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches. version:2
arxiv-1602-04484 | Fundamental Differences between Dropout and Weight Decay in Deep Networks | http://arxiv.org/abs/1602.04484 | id:1602.04484 author:David P. Helmbold, Philip M. Long category:cs.LG cs.AI cs.NE math.ST stat.ML stat.TH  published:2016-02-14 summary:We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress. version:3
arxiv-1602-04485 | Benefits of depth in neural networks | http://arxiv.org/abs/1602.04485 | id:1602.04485 author:Matus Telgarsky category:cs.LG cs.NE stat.ML  published:2016-02-14 summary:For any positive integer $k$, there exist neural networks with $\Theta(k^3)$ layers, $\Theta(1)$ nodes per layer, and $\Theta(1)$ distinct parameters which can not be approximated by networks with $\mathcal{O}(k)$ layers unless they are exponentially large --- they must possess $\Omega(2^k)$ nodes. This result is proved here for a class of nodes termed "semi-algebraic gates" which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: $\Omega(2^{k^3})$ total tree nodes are required). version:2
arxiv-1511-06014 | Regret Analysis of the Finite-Horizon Gittins Index Strategy for Multi-Armed Bandits | http://arxiv.org/abs/1511.06014 | id:1511.06014 author:Tor Lattimore category:cs.LG math.ST stat.ML stat.TH  published:2015-11-18 summary:I analyse the frequentist regret of the famous Gittins index strategy for multi-armed bandits with Gaussian noise and a finite horizon. Remarkably it turns out that this approach leads to finite-time regret guarantees comparable to those available for the popular UCB algorithm. Along the way I derive finite-time bounds on the Gittins index that are asymptotically exact and may be of independent interest. I also discuss some computational issues and present experimental results suggesting that a particular version of the Gittins index strategy is a modest improvement on existing algorithms with finite-time regret guarantees such as UCB and Thompson sampling. version:3
arxiv-1603-00550 | Synthesized Classifiers for Zero-Shot Learning | http://arxiv.org/abs/1603.00550 | id:1603.00550 author:Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, Fei Sha category:cs.CV  published:2016-03-02 summary:Given semantic descriptions of object classes, zero-shot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the semantic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of "phantom" object classes whose coordinates live in both the semantic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve optimal discriminative performance. We demonstrate superior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, including the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes. version:3
arxiv-1411-5649 | No-Regret Learnability for Piecewise Linear Losses | http://arxiv.org/abs/1411.5649 | id:1411.5649 author:Arthur Flajolet, Patrick Jaillet category:cs.LG  published:2014-11-20 summary:In the convex optimization approach to online regret minimization, many methods have been developed to guarantee a $O(\sqrt{T})$ bound on regret for subdifferentiable convex loss functions with bounded subgradients, by using a reduction to linear loss functions. This suggests that linear loss functions tend to be the hardest ones to learn against, regardless of the underlying decision spaces. We investigate this question in a systematic fashion looking at the interplay between the set of possible moves for both the decision maker and the adversarial environment. This allows us to highlight sharp distinctive behaviors about the learnability of piecewise linear loss functions. On the one hand, when the decision set of the decision maker is a polyhedron, we establish $\Omega(\sqrt{T})$ lower bounds on regret for a large class of piecewise linear loss functions with important applications in online linear optimization, repeated zero-sum Stackelberg games, online prediction with side information, and online two-stage optimization. On the other hand, we exhibit $O(\log{T})$ learning rates, achieved by the Follow-The-Leader algorithm, in online linear optimization when the boundary of the decision maker's decision set is curved and when $0$ does not lie in the convex hull of the environment's decision set. These results hold in a completely adversarial setting. version:4
arxiv-1605-08803 | Density estimation using Real NVP | http://arxiv.org/abs/1605.08803 | id:1605.08803 author:Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio category:cs.LG cs.AI cs.NE stat.ML  published:2016-05-27 summary:Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations. version:1
arxiv-1605-08798 | Asymptotic Analysis of Objectives based on Fisher Information in Active Learning | http://arxiv.org/abs/1605.08798 | id:1605.08798 author:Jamshid Sourati, Murat Akcakaya, Todd K. Leen, Deniz Erdogmus, Jennifer G. Dy category:stat.ML cs.LG  published:2016-05-27 summary:Obtaining labels can be costly and time-consuming. Active learning allows a learning algorithm to intelligently query samples to be labeled for efficient learning. Fisher information ratio (FIR) has been used as an objective for selecting queries in active learning. However, little is known about the theory behind the use of FIR for active learning. There is a gap between the underlying theory and the motivation of its usage in practice. In this paper, we attempt to fill this gap and provide a rigorous framework for analyzing the existing FIR-based active learning methods. In particular, we show that FIR can be asymptotically viewed as an upper bound of the expected variance of the log-likelihood ratio. Additionally, our analysis suggests a unifying framework that not only enables us to make theoretical comparisons among the existing querying methods based on FIR, but also gives more insight into the development of new active learning approaches based on this objective. version:1
arxiv-1508-06464 | SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter | http://arxiv.org/abs/1508.06464 | id:1508.06464 author:Osamu Hirose, Shotaro Kawaguchi, Terumasa Tokunaga, Yu Toyoshima, Takayuki Teramoto, Sayuri Kuge, Takeshi Ishihara, Yuichi Iino, Ryo Yoshida category:cs.CV  published:2015-08-26 summary:Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: (i) cells are imaged as globular-like objects, (ii) it is difficult to distinguish cells based only on shape and size, (iii) the number of imaged cells ranges in several hundreds, (iv) moves of nearly-located cells are strongly correlated and (v) cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. (2014). version:3
arxiv-1605-08764 | Stacking With Auxiliary Features | http://arxiv.org/abs/1605.08764 | id:1605.08764 author:Nazneen Fatema Rajani, Raymond J. Mooney category:cs.CL cs.CV cs.LG  published:2016-05-27 summary:Ensembling methods are well known for improving prediction accuracy. However, they are limited in the sense that they cannot discriminate among component models effectively. In this paper, we propose stacking with auxiliary features that learns to fuse relevant information from multiple systems to improve performance. Auxiliary features enable the stacker to rely on systems that not just agree on an output but also the provenance of the output. We demonstrate our approach on three very different and difficult problems -- the Cold Start Slot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet object detection tasks. We obtain new state-of-the-art results on the first two tasks and substantial improvements on the detection task, thus verifying the power and generality of our approach. version:1
arxiv-1604-06778 | Benchmarking Deep Reinforcement Learning for Continuous Control | http://arxiv.org/abs/1604.06778 | id:1604.06778 author:Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel category:cs.LG cs.AI cs.RO  published:2016-04-22 summary:Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers. version:3
arxiv-1506-01972 | Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives | http://arxiv.org/abs/1506.01972 | id:1506.01972 author:Zeyuan Allen-Zhu, Yang Yuan category:cs.LG cs.DS math.OC stat.ML  published:2015-06-05 summary:Many classical algorithms are found until several years later to outlive the confines in which they were conceived, and continue to be relevant in unforeseen settings. In this paper, we show that SVRG is one such method: being originally designed for strongly convex objectives, it is also very robust in non-strongly convex or sum-of-non-convex settings. More precisely, we provide new analysis to improve the state-of-the-art running times in both settings by either applying SVRG or its novel variant. Since non-strongly convex objectives include important examples such as Lasso or logistic regression, and sum-of-non-convex objectives include famous examples such as stochastic PCA and is even believed to be related to training deep neural nets, our results also imply better performances in these applications. version:3
arxiv-1512-09103 | Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling | http://arxiv.org/abs/1512.09103 | id:1512.09103 author:Zeyuan Allen-Zhu, Zheng Qu, Peter Richtárik, Yang Yuan category:math.OC cs.DS math.NA stat.ML  published:2015-12-30 summary:Accelerated coordinate descent is widely used in optimization due to its cheap per-iteration cost and scalability to large-scale problems. Up to a primal-dual transformation, it is also the same as accelerated stochastic gradient descent that is one of the central methods used in machine learning. In this paper, we improve the best known running time of accelerated coordinate descent by a factor up to $\sqrt{n}$. Our improvement is based on a clean, novel non-uniform sampling that selects each coordinate with a probability proportional to the square root of its smoothness parameter. Our proof technique also deviates from the classical estimation sequence technique used in prior work. Our speed-up applies to important problems such as empirical risk minimization and solving linear systems, both in theory and in practice. version:3
arxiv-1509-05009 | On the Expressive Power of Deep Learning: A Tensor Analysis | http://arxiv.org/abs/1509.05009 | id:1509.05009 author:Nadav Cohen, Or Sharir, Amnon Shashua category:cs.NE cs.LG cs.NA stat.ML  published:2015-09-16 summary:It has long been conjectured that hypotheses spaces suitable for data that is compositional in nature, such as text or images, may be more efficiently represented with deep hierarchical networks than with shallow ones. Despite the vast empirical evidence supporting this belief, theoretical justifications to date are limited. In particular, they do not account for the locality, sharing and pooling constructs of convolutional networks, the most successful deep learning architecture to date. In this work we derive a deep network architecture based on arithmetic circuits that inherently employs locality, sharing and pooling. An equivalence between the networks and hierarchical tensor factorizations is established. We show that a shallow network corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to Hierarchical Tucker decomposition. Using tools from measure theory and matrix algebra, we prove that besides a negligible set, all functions that can be implemented by a deep network of polynomial size, require exponential size in order to be realized (or even approximated) by a shallow network. Since log-space computation transforms our networks into SimNets, the result applies directly to a deep learning architecture demonstrating promising empirical performance. The construction and theory developed in this paper shed new light on various practices and ideas employed by the deep learning community. version:3
arxiv-1605-03631 | EEF: Exponentially Embedded Families with Class-Specific Features for Classification | http://arxiv.org/abs/1605.03631 | id:1605.03631 author:Bo Tang, Steven Kay, Haibo He, Paul M. Baggenstoss category:stat.ML cs.LG  published:2016-05-11 summary:In this letter, we present a novel exponentially embedded families (EEF) based classification method, in which the probability density function (PDF) on raw data is estimated from the PDF on features. With the PDF construction, we show that class-specific features can be used in the proposed classification method, instead of a common feature subset for all classes as used in conventional approaches. We apply the proposed EEF classifier for text categorization as a case study and derive an optimal Bayesian classification rule with class-specific feature selection based on the Information Gain (IG) score. The promising performance on real-life data sets demonstrates the effectiveness of the proposed approach and indicates its wide potential applications. version:2
arxiv-1602-04129 | Learning may need only a few bits of synaptic precision | http://arxiv.org/abs/1602.04129 | id:1602.04129 author:Carlo Baldassi, Federica Gerace, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:cond-mat.dis-nn q-bio.NC stat.ML  published:2016-02-12 summary:Learning in neural networks poses peculiar challenges when using discretized rather then continuous synaptic states. The choice of discrete synapses is motivated by biological reasoning and experiments, and possibly by hardware implementation considerations as well. In this paper we extend a previous large deviations analysis which unveiled the existence of peculiar dense regions in the space of synaptic states which accounts for the possibility of learning efficiently in networks with binary synapses. We extend the analysis to synapses with multiple states and generally more plausible biological features. The results clearly indicate that the overall qualitative picture is unchanged with respect to the binary case, and very robust to variation of the details of the model. We also provide quantitative results which suggest that the advantages of increasing the synaptic precision (i.e.~the number of internal synaptic states) rapidly vanish after the first few bits, and therefore that, for practical applications, only few bits may be needed for near-optimal performance, consistently with recent biological findings. Finally, we demonstrate how the theoretical analysis can be exploited to design efficient algorithmic search strategies. version:2
arxiv-1602-00061 | Spectrum Estimation from Samples | http://arxiv.org/abs/1602.00061 | id:1602.00061 author:Weihao Kong, Gregory Valiant category:cs.LG stat.ML  published:2016-01-30 summary:We consider the problem of approximating the set of eigenvalues of the covariance matrix of a multivariate distribution (equivalently, the problem of approximating the "population spectrum"), given access to samples drawn from the distribution. The eigenvalues of the covariance of a distribution contain basic information about the distribution, including the presence or lack of structure in the distribution, the effective dimensionality of the distribution, and the applicability of higher-level machine learning and multivariate statistical tools. We consider this fundamental recovery problem in the regime where the number of samples is comparable, or even sublinear in the dimensionality of the distribution in question. First, we propose a theoretically optimal and computationally efficient algorithm for recovering the moments of the eigenvalues of the population covariance matrix. We then leverage this accurate moment recovery, via a Wasserstein distance argument, to show that the vector of eigenvalues can be accurately recovered. Specifically, we show that our eigenvalue reconstruction algorithm is asymptotically consistent as the dimensionality of the distribution and sample size tend towards infinity, even in the sublinear sample regime where the ratio of the sample size to the dimensionality tends to zero. In addition to our theoretical results, we show that our approach performs well in practice for a broad range of distributions and sample sizes. version:2
arxiv-1604-03930 | Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis | http://arxiv.org/abs/1604.03930 | id:1604.03930 author:Rong Ge, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford category:cs.LG math.OC stat.ML  published:2016-04-13 summary:This paper considers the problem of canonical-correlation analysis (CCA) (Hotelling, 1936) and, more broadly, the generalized eigenvector problem for a pair of symmetric matrices. These are two fundamental problems in data analysis and scientific computing with numerous applications in machine learning and statistics (Shi and Malik, 2000; Hardoon et al., 2004; Witten et al., 2009). We provide simple iterative algorithms, with improved runtimes, for solving these problems that are globally linearly convergent with moderate dependencies on the condition numbers and eigenvalue gaps of the matrices involved. We obtain our results by reducing CCA to the top-$k$ generalized eigenvector problem. We solve this problem through a general framework that simply requires black box access to an approximate linear system solver. Instantiating this framework with accelerated gradient descent we obtain a running time of $O(\frac{z k \sqrt{\kappa}}{\rho} \log(1/\epsilon) \log \left(k\kappa/\rho\right))$ where $z$ is the total number of nonzero entries, $\kappa$ is the condition number and $\rho$ is the relative eigenvalue gap of the appropriate matrices. Our algorithm is linear in the input size and the number of components $k$ up to a $\log(k)$ factor. This is essential for handling large-scale matrices that appear in practice. To the best of our knowledge this is the first such algorithm with global linear convergence. We hope that our results prompt further research and ultimately improve the practical running time for performing these important data analysis procedures on large data sets. version:2
arxiv-1403-4544 | On the Sensitivity of the Lasso to the Number of Predictor Variables | http://arxiv.org/abs/1403.4544 | id:1403.4544 author:Cheryl J. Flynn, Clifford M. Hurvich, Jeffrey S. Simonoff category:stat.ML  published:2014-03-18 summary:The Lasso is a computationally efficient regression regularization procedure that can produce sparse estimators when the number of predictors (p) is large. Oracle inequalities provide probability loss bounds for the Lasso estimator at a deterministic choice of the regularization parameter. These bounds tend to zero if p is appropriately controlled, and are thus commonly cited as theoretical justification for the Lasso and its ability to handle high-dimensional settings. Unfortunately, in practice the regularization parameter is not selected to be a deterministic quantity, but is instead chosen using a random, data-dependent procedure. To address this shortcoming of previous theoretical work, we study the loss of the Lasso estimator when tuned optimally for prediction. Assuming orthonormal predictors and a sparse true model, we prove that the probability that the best possible predictive performance of the Lasso deteriorates as p increases is positive and can be arbitrarily close to one given a sufficiently high signal to noise ratio and sufficiently large p. We further demonstrate empirically that the amount of deterioration in performance can be far worse than the oracle inequalities suggest and provide a real data example where deterioration is observed. version:3
arxiv-1605-07144 | Actively Learning Hemimetrics with Applications to Eliciting User Preferences | http://arxiv.org/abs/1605.07144 | id:1605.07144 author:Adish Singla, Sebastian Tschiatschek, Andreas Krause category:stat.ML cs.LG  published:2016-05-23 summary:Motivated by an application of eliciting users' preferences, we investigate the problem of learning hemimetrics, i.e., pairwise distances among a set of $n$ items that satisfy triangle inequalities and non-negativity constraints. In our application, the (asymmetric) distances quantify private costs a user incurs when substituting one item by another. We aim to learn these distances (costs) by asking the users whether they are willing to switch from one item to another for a given incentive offer. Without exploiting structural constraints of the hemimetric polytope, learning the distances between each pair of items requires $\Theta(n^2)$ queries. We propose an active learning algorithm that substantially reduces this sample complexity by exploiting the structural constraints on the version space of hemimetrics. Our proposed algorithm achieves provably-optimal sample complexity for various instances of the task. For example, when the items are embedded into $K$ tight clusters, the sample complexity of our algorithm reduces to $O(n K)$. Extensive experiments on a restaurant recommendation data set support the conclusions of our theoretical analysis. version:2
arxiv-1605-08722 | An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits | http://arxiv.org/abs/1605.08722 | id:1605.08722 author:Peter Auer, Chao-Kai Chiang category:cs.LG  published:2016-05-27 summary:We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is $O(K\sqrt{n \log n})$ and against stochastic bandits the pseudo-regret is $O(\sum_i (\log n)/\Delta_i)$. We also show that no algorithm with $O(\log n)$ pseudo-regret against stochastic bandits can achieve $\tilde{O}(\sqrt{n})$ expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show $\tilde{O}(\sqrt{n})$ expected adversarial regret with $O((\log n)^2)$ stochastic pseudo-regret. version:1
arxiv-1506-02159 | Riemannian preconditioning for tensor completion | http://arxiv.org/abs/1506.02159 | id:1506.02159 author:Hiroyuki Kasai, Bamdev Mishra category:cs.NA cs.LG math.OC  published:2015-06-06 summary:We propose a novel Riemannian preconditioning approach for the tensor completion problem with rank constraint. A Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop a preconditioned nonlinear conjugate gradient algorithm for the problem. To this end, concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithm robustly outperforms state-of-the-art algorithms across different problem instances encompassing various synthetic and real-world datasets. version:2
arxiv-1605-04951 | Viziometrics: Analyzing Visual Information in the Scientific Literature | http://arxiv.org/abs/1605.04951 | id:1605.04951 author:Po-shen Lee, Jevin D. West, Bill Howe category:cs.SI cs.CV cs.DL cs.IR  published:2016-05-16 summary:Scientific results are communicated visually in the literature through diagrams, visualizations, and photographs. These information-dense objects have been largely ignored in bibliometrics and scientometrics studies when compared to citations and text. In this paper, we use techniques from computer vision and machine learning to classify more than 8 million figures from PubMed into 5 figure types and study the resulting patterns of visual information as they relate to impact. We find that the distribution of figures and figure types in the literature has remained relatively constant over time, but can vary widely across field and topic. Remarkably, we find a significant correlation between scientific impact and the use of visual information, where higher impact papers tend to include more diagrams, and to a lesser extent more plots and photographs. To explore these results and other ways of extracting this visual information, we have built a visual browser to illustrate the concept and explore design alternatives for supporting viziometric analysis and organizing visual information. We use these results to articulate a new research agenda -- viziometrics -- to study the organization and presentation of visual information in the scientific literature. version:2
arxiv-1603-00448 | Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization | http://arxiv.org/abs/1603.00448 | id:1603.00448 author:Chelsea Finn, Sergey Levine, Pieter Abbeel category:cs.LG cs.AI cs.RO  published:2016-03-01 summary:Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency. version:3
arxiv-1604-08859 | The Z-loss: a shift and scale invariant classification loss belonging to the Spherical Family | http://arxiv.org/abs/1604.08859 | id:1604.08859 author:Alexandre de Brébisson, Pascal Vincent category:cs.LG cs.AI stat.ML  published:2016-04-29 summary:Despite being the standard loss function to train multi-class neural networks, the log-softmax has two potential limitations. First, it involves computations that scale linearly with the number of output classes, which can restrict the size of problems we are able to tackle with current hardware. Second, it remains unclear how close it matches the task loss such as the top-k error rate or other non-differentiable evaluation metrics which we aim to optimize ultimately. In this paper, we introduce an alternative classification loss function, the Z-loss, which is designed to address these two issues. Unlike the log-softmax, it has the desirable property of belonging to the spherical loss family (Vincent et al., 2015), a class of loss functions for which training can be performed very efficiently with a complexity independent of the number of output classes. We show experimentally that it significantly outperforms the other spherical loss functions previously investigated. Furthermore, we show on a word language modeling task that it also outperforms the log-softmax with respect to certain ranking scores, such as top-k scores, suggesting that the Z-loss has the flexibility to better match the task loss. These qualities thus makes the Z-loss an appealing candidate to train very efficiently large output networks such as word-language models or other extreme classification problems. On the One Billion Word (Chelba et al., 2014) dataset, we are able to train a model with the Z-loss 40 times faster than the log-softmax and more than 4 times faster than the hierarchical softmax. version:2
arxiv-1605-08680 | Achieving stable subspace clustering by post-processing generic clustering results | http://arxiv.org/abs/1605.08680 | id:1605.08680 author:Duc-Son Pham, Ognjen Arandjelovic, Svetha Venkatesh category:cs.CV  published:2016-05-27 summary:We propose an effective subspace selection scheme as a post-processing step to improve results obtained by sparse subspace clustering (SSC). Our method starts by the computation of stable subspaces using a novel random sampling scheme. Thus constructed preliminary subspaces are used to identify the initially incorrectly clustered data points and then to reassign them to more suitable clusters based on their goodness-of-fit to the preliminary model. To improve the robustness of the algorithm, we use a dominant nearest subspace classification scheme that controls the level of sensitivity against reassignment. We demonstrate that our algorithm is convergent and superior to the direct application of a generic alternative such as principal component analysis. On several popular datasets for motion segmentation and face clustering pervasively used in the sparse subspace clustering literature the proposed method is shown to reduce greatly the incidence of clustering errors while introducing negligible disturbance to the data points already correctly clustered. version:1
arxiv-1605-08675 | Boosting Question Answering by Deep Entity Recognition | http://arxiv.org/abs/1605.08675 | id:1605.08675 author:Piotr Przybyła category:cs.CL  published:2016-05-27 summary:In this paper an open-domain factoid question answering system for Polish, RAFAEL, is presented. The system goes beyond finding an answering sentence; it also extracts a single string, corresponding to the required entity. Herein the focus is placed on different approaches to entity recognition, essential for retrieving information matching question constraints. Apart from traditional approach, including named entity recognition (NER) solutions, a novel technique, called Deep Entity Recognition (DeepER), is introduced and implemented. It allows a comprehensive search of all forms of entity references matching a given WordNet synset (e.g. an impressionist), based on a previously assembled entity library. It has been created by analysing the first sentences of encyclopaedia entries and disambiguation and redirect pages. DeepER also provides automatic evaluation, which makes possible numerous experiments, including over a thousand questions from a quiz TV show answered on the grounds of Polish Wikipedia. The final results of a manual evaluation on a separate question set show that the strength of DeepER approach lies in its ability to answer questions that demand answers beyond the traditional categories of named entities. version:1
arxiv-1605-08671 | An optimal algorithm for the Thresholding Bandit Problem | http://arxiv.org/abs/1605.08671 | id:1605.08671 author:Andrea Locatelli, Maurilio Gutzeit, Alexandra Carpentier category:stat.ML cs.LG  published:2016-05-27 summary:We study a specific \textit{combinatorial pure exploration stochastic bandit problem} where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and \textit{for a fixed time horizon}. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with \textit{fixed budget} for which optimal strategies are constructed. version:1
arxiv-1605-02226 | Neural Autoregressive Distribution Estimation | http://arxiv.org/abs/1605.02226 | id:1605.02226 author:Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, Hugo Larochelle category:cs.LG  published:2016-05-07 summary:We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE. version:3
arxiv-1605-08636 | PAC-Bayesian Theory Meets Bayesian Inference | http://arxiv.org/abs/1605.08636 | id:1605.08636 author:Pascal Germain, Francis Bach, Alexandre Lacoste, Simon Lacoste-Julien category:stat.ML cs.LG  published:2016-05-27 summary:We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks. version:1
arxiv-1605-08233 | Stochastic Variance Reduced Riemannian Eigensolver | http://arxiv.org/abs/1605.08233 | id:1605.08233 author:Zhiqiang Xu, Yiping Ke category:cs.LG stat.ML  published:2016-05-26 summary:We study the stochastic Riemannian gradient algorithm for matrix eigen-decomposition. The state-of-the-art stochastic Riemannian algorithm requires the learning rate to decay to zero and thus suffers from slow convergence and sub-optimal solutions. In this paper, we address this issue by deploying the variance reduction (VR) technique of stochastic gradient descent (SGD). The technique was originally developed to solve convex problems in the Euclidean space. We generalize it to Riemannian manifolds and realize it to solve the non-convex eigen-decomposition problem. We are the first to propose and analyze the generalization of SVRG to Riemannian manifolds. Specifically, we propose the general variance reduction form, SVRRG, in the framework of the stochastic Riemannian gradient optimization. It's then specialized to the problem with eigensolvers and induces the SVRRG-EIGS algorithm. We provide a novel and elegant theoretical analysis on this algorithm. The theory shows that a fixed learning rate can be used in the Riemannian setting with an exponential global convergence rate guaranteed. The theoretical results make a significant improvement over existing studies, with the effectiveness empirically verified. version:2
arxiv-1605-08618 | Variational Bayesian Inference for Hidden Markov Models With Multivariate Gaussian Output Distributions | http://arxiv.org/abs/1605.08618 | id:1605.08618 author:Christian Gruhl, Bernhard Sick category:cs.LG stat.ML  published:2016-05-27 summary:Hidden Markov Models (HMM) have been used for several years in many time series analysis or pattern recognitions tasks. HMM are often trained by means of the Baum-Welch algorithm which can be seen as a special variant of an expectation maximization (EM) algorithm. Second-order training techniques such as Variational Bayesian Inference (VI) for probabilistic models regard the parameters of the probabilistic models as random variables and define distributions over these distribution parameters, hence the name of this technique. VI can also bee regarded as a special case of an EM algorithm. In this article, we bring both together and train HMM with multivariate Gaussian output distributions with VI. The article defines the new training technique for HMM. An evaluation based on some case studies and a comparison to related approaches is part of our ongoing work. version:1
