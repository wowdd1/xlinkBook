arxiv-1505-05354 | DropSample: A New Training Method to Enhance Deep Convolutional Neural Networks for Large-Scale Unconstrained Handwritten Chinese Character Recognition | http://arxiv.org/abs/1505.05354 | id:1505.05354 author:Weixin Yang, Lianwen Jin, Dacheng Tao, Zecheng Xie, Ziyong Feng category:cs.CV  published:2015-05-20 summary:Inspired by the theory of Leitners learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher probability of being selected as training data in the next iteration; in contrast, well-trained and well-recognized samples with very high confidence will have a lower probability of being involved in the next training iteration and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 97.33%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature. version:1
arxiv-1505-05338 | Algorithmic Analysis of Edge Ranking and Profiling for MTF Determination of an Imaging System | http://arxiv.org/abs/1505.05338 | id:1505.05338 author:Poorna Banerjee Dasgupta category:cs.CV  published:2015-05-20 summary:Edge detection is one of the most principal techniques for detecting discontinuities in the gray levels of image pixels. The Modulation Transfer Function (MTF) is one of the main criteria for assessing imaging quality and is a parameter frequently used for measuring the sharpness of an imaging system. In order to determine the MTF, it is essential to determine the best edge from the target image so that an edge profile can be developed and then the line spread function and hence the MTF, can be computed accordingly. For regular image sizes, the human visual system is adept enough to identify suitable edges from the image. But considering huge image datasets, such as those obtained from satellites, the image size may range in few gigabytes and in such a case, manual inspection of images for determination of the best suitable edge is not plausible and hence, edge profiling tasks have to be automated. This paper presents a novel, yet simple, algorithm for edge ranking and detection from image data-sets for MTF computation, which is ideal for automation on vectorised graphical processing units. version:1
arxiv-1505-05286 | Measuring Visibility using Atmospheric Transmission and Digital Surface Model | http://arxiv.org/abs/1505.05286 | id:1505.05286 author:Jean-Philippe Andreu, Stefan Mayer, Karlheinz Gutjahr, Harald Ganster category:cs.CV  published:2015-05-20 summary:Reliable and exact assessment of visibility is essential for safe air traffic. In order to overcome the drawbacks of the currently subjective reports from human observers, we present an approach to automatically derive visibility measures by means of image processing. It first exploits image based estimation of the atmospheric transmission describing the portion of the light that is not scattered by atmospheric phenomena (e.g., haze, fog, smoke) and reaches the camera. Once the atmospheric transmission is estimated, a 3D representation of the vicinity (digital surface model: DMS) is used to compute depth measurements for the haze-free pixels and then derive a global visibility estimation for the airport. Results on foggy images demonstrate the validity of the proposed method. version:1
arxiv-1505-04028 | Biometric Matching and Fusion System for Fingerprints from Non-Distal Phalanges | http://arxiv.org/abs/1505.04028 | id:1505.04028 author:Mehmet Kayaoglu, Berkay Topcu, Umut Uludag category:cs.CV  published:2015-05-15 summary:Market research indicates that fingerprints are still the most popular biometric modality for personal authentication. Even with the onset of new modalities (e.g. vein matching), many applications within different domains (e-ID, banking, border control...) and geographies rely on fingerprints obtained from the distal phalanges (a.k.a. sections, digits) of the human hand structure. Motivated by the problem of poor quality distal fingerprint images affecting a non-trivial portion of the population (which decreases associated authentication accuracy), we designed and tested a multifinger, multiphalanx fusion scheme, that combines minutiae matching scores originating from non-distal (ie. middle and proximal) phalanges based on (i) simple sum fusion, (ii) NFIQ image-quality-based fusion, and (iii) phalanx-type-based fusion. Utilizing a medium-size (50 individuals, 400 unique fingers, 1600 distinct images) database collected in our laboratory with a commercial optical fingerprint sensor, and a commercial minutiae extractor & matcher (without any modification), allowed us to simulate a real-world fingerprint authentication setting. Detailed analyses including ROC curves with statistical confidence intervals show that the proposed system can be a viable alternative for cases where (i) distal phalanx images are not usable (e.g. due to missing digits, or low quality finger surface due to manual labor), and (ii) switching to a new biometric modality (e.g. iris) is not possible due to economical or infrastructure limits. Further, we show that when distal phalanx images are in fact usable, combining them with images from other phalanges increases accuracy as well. version:2
arxiv-1404-0979 | Kernel-Based Adaptive Online Reconstruction of Coverage Maps With Side Information | http://arxiv.org/abs/1404.0979 | id:1404.0979 author:Martin Kasparick, Renato L. G. Cavalcante, Stefan Valentin, Slawomir Stanczak, Masahiro Yukawa category:cs.NI cs.LG stat.ML  published:2014-04-03 summary:In this paper, we address the problem of reconstructing coverage maps from path-loss measurements in cellular networks. We propose and evaluate two kernel-based adaptive online algorithms as an alternative to typical offline methods. The proposed algorithms are application-tailored extensions of powerful iterative methods such as the adaptive projected subgradient method and a state-of-the-art adaptive multikernel method. Assuming that the moving trajectories of users are available, it is shown how side information can be incorporated in the algorithms to improve their convergence performance and the quality of the estimation. The complexity is significantly reduced by imposing sparsity-awareness in the sense that the algorithms exploit the compressibility of the measurement data to reduce the amount of data which is saved and processed. Finally, we present extensive simulations based on realistic data to show that our algorithms provide fast, robust estimates of coverage maps in real-world scenarios. Envisioned applications include path-loss prediction along trajectories of mobile users as a building block for anticipatory buffering or traffic offloading. version:3
arxiv-1505-05254 | Live Video Synopsis for Multiple Cameras | http://arxiv.org/abs/1505.05254 | id:1505.05254 author:Yedid Hoshen, Shmuel Peleg category:cs.CV  published:2015-05-20 summary:Video surveillance cameras generate most of recorded video, and there is far more recorded video than operators can watch. Much progress has recently been made using summarization of recorded video, but such techniques do not have much impact on live video surveillance. We assume a camera hierarchy where a Master camera observes the decision-critical region, and one or more Slave cameras observe regions where past activity is important for making the current decision. We propose that when people appear in the live Master camera, the Slave cameras will display their past activities, and the operator could use past information for real-time decision making. The basic units of our method are action tubes, representing objects and their trajectories over time. Our object-based method has advantages over frame based methods, as it can handle multiple people, multiple activities for each person, and can address re-identification uncertainty. version:1
arxiv-1504-05319 | Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word: The Impact of Word Representation on Sequence Labelling Tasks | http://arxiv.org/abs/1504.05319 | id:1504.05319 author:Lizhen Qu, Gabriela Ferraro, Liyuan Zhou, Weiwei Hou, Nathan Schneider, Timothy Baldwin category:cs.CL  published:2015-04-21 summary:Word embeddings -- distributed word representations that can be learned from unlabelled data -- have been shown to have high utility in many natural language processing applications. In this paper, we perform an extrinsic evaluation of five popular word embedding methods in the context of four sequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE identification. A particular focus of the paper is analysing the effects of task-based updating of word representations. We show that when using word embeddings as features, as few as several hundred training instances are sufficient to achieve competitive results, and that word embeddings lead to improvements over OOV words and out of domain. Perhaps more surprisingly, our results indicate there is little difference between the different word embedding methods, and that simple Brown clusters are often competitive with word embeddings across all tasks we consider. version:2
arxiv-1502-02206 | Learning to Search Better Than Your Teacher | http://arxiv.org/abs/1502.02206 | id:1502.02206 author:Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé III, John Langford category:cs.LG stat.ML  published:2015-02-08 summary:Methods for learning to search for structured prediction typically imitate a reference policy, with existing theoretical guarantees demonstrating low regret compared to that reference. This is unsatisfactory in many applications where the reference policy is suboptimal and the goal of learning is to improve upon it. Can learning to search work even when the reference is poor? We provide a new learning to search algorithm, LOLS, which does well relative to the reference policy, but additionally guarantees low regret compared to deviations from the learned policy: a local-optimality guarantee. Consequently, LOLS can improve upon the reference policy, unlike previous algorithms. This enables us to develop structured contextual bandits, a partial information structured prediction setting with many potential applications. version:2
arxiv-1505-05240 | Benchmarking KAZE and MCM for Multiclass Classification | http://arxiv.org/abs/1505.05240 | id:1505.05240 author:Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall category:cs.CV cs.IR  published:2015-05-20 summary:In this paper, we propose a novel approach for feature generation by appropriately fusing KAZE and SIFT features. We then use this feature set along with Minimal Complexity Machine(MCM) for object classification. We show that KAZE and SIFT features are complementary. Experimental results indicate that an elementary integration of these techniques can outperform the state-of-the-art approaches. version:1
arxiv-1505-05232 | Multi-scale recognition with DAG-CNNs | http://arxiv.org/abs/1505.05232 | id:1505.05232 author:Songfan Yang, Deva Ramanan category:cs.CV  published:2015-05-20 summary:We explore multi-scale convolutional neural nets (CNNs) for image classification. Contemporary approaches extract features from a single output layer. By extracting features from multiple layers, one can simultaneously reason about high, mid, and low-level features during classification. The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to learn a set of multiscale features that can be effectively shared between coarse and fine-grained classification tasks. While fine-tuning such models helps performance, we show that even "off-the-self" multiscale features perform quite well. We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks (SUN397, MIT67, and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets, our results reduce the lowest previously-reported error by 23.9% and 9.5%, respectively. version:1
arxiv-1505-05231 | Bounds on the Minimax Rate for Estimating a Prior over a VC Class from Independent Learning Tasks | http://arxiv.org/abs/1505.05231 | id:1505.05231 author:Liu Yang, Steve Hanneke, Jaime Carbonell category:cs.LG  published:2015-05-20 summary:We study the optimal rates of convergence for estimating a prior distribution over a VC class from a sequence of independent data sets respectively labeled by independent target functions sampled from the prior. We specifically derive upper and lower bounds on the optimal rates under a smoothness condition on the correct prior, with the number of samples per data set equal the VC dimension. These results have implications for the improvements achievable via transfer learning. We additionally extend this setting to real-valued function, where we establish consistency of an estimator for the prior, and discuss an additional application to a preference elicitation problem in algorithmic economics. version:1
arxiv-1505-05229 | Variable subset selection via GA and information complexity in mixtures of Poisson and negative binomial regression models | http://arxiv.org/abs/1505.05229 | id:1505.05229 author:T. J. Massaro, H. Bozdogan category:stat.ML stat.ME  published:2015-05-20 summary:Count data, for example the number of observed cases of a disease in a city, often arise in the fields of healthcare analytics and epidemiology. In this paper, we consider performing regression on multivariate data in which our outcome is a count. Specifically, we derive log-likelihood functions for finite mixtures of regression models involving counts that come from a Poisson distribution, as well as a negative binomial distribution when the counts are significantly overdispersed. Within our proposed modeling framework, we carry out optimal component selection using the information criteria scores AIC, BIC, CAIC, and ICOMP. We demonstrate applications of our approach on simulated data, as well as on a real data set of HIV cases in Tennessee counties from the year 2010. Finally, using a genetic algorithm within our framework, we perform variable subset selection to determine the covariates that are most responsible for categorizing Tennessee counties. This leads to some interesting insights into the traits of counties that have high HIV counts. version:1
arxiv-1505-05225 | Image aesthetic evaluation using paralleled deep convolution neural network | http://arxiv.org/abs/1505.05225 | id:1505.05225 author:Guo Lihua, Li Fudi category:cs.CV cs.MM 68U10 I.4.7  published:2015-05-20 summary:Image aesthetic evaluation has attracted much attention in recent years. Image aesthetic evaluation methods heavily depend on the effective aesthetic feature. Traditional meth-ods always extract hand-crafted features. However, these hand-crafted features are always designed to adapt particu-lar datasets, and extraction of them needs special design. Rather than extracting hand-crafted features, an automati-cally learn of aesthetic features based on deep convolutional neural network (DCNN) is first adopt in this paper. As we all know, when the training dataset is given, the DCNN architecture with high complexity may meet the over-fitting problem. On the other side, the DCNN architecture with low complexity would not efficiently extract effective features. For these reasons, we further propose a paralleled convolutional neural network (PDCNN) with multi-level structures to automatically adapt to the training dataset. Experimental results show that our proposed PDCNN architecture achieves better performance than other traditional methods. version:1
arxiv-1505-05216 | Convergence Analysis of Policy Iteration | http://arxiv.org/abs/1505.05216 | id:1505.05216 author:Ali Heydari category:cs.SY math.OC stat.ML  published:2015-05-20 summary:Adaptive optimal control of nonlinear dynamic systems with deterministic and known dynamics under a known undiscounted infinite-horizon cost function is investigated. Policy iteration scheme initiated using a stabilizing initial control is analyzed in solving the problem. The convergence of the iterations and the optimality of the limit functions, which follows from the established uniqueness of the solution to the Bellman equation, are the main results of this study. Furthermore, a theoretical comparison between the speed of convergence of policy iteration versus value iteration is presented. Finally, the convergence results are extended to the case of multi-step look-ahead policy iteration. version:1
arxiv-1505-05215 | Learning with a Drifting Target Concept | http://arxiv.org/abs/1505.05215 | id:1505.05215 author:Steve Hanneke, Varun Kanade, Liu Yang category:cs.LG  published:2015-05-20 summary:We study the problem of learning in the presence of a drifting target concept. Specifically, we provide bounds on the error rate at a given time, given a learner with access to a history of independent samples labeled according to a target concept that can change on each round. One of our main contributions is a refinement of the best previous results for polynomial-time algorithms for the space of linear separators under a uniform distribution. We also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. Some of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated bounds on the error rates. version:1
arxiv-1401-8126 | Extrinsic Methods for Coding and Dictionary Learning on Grassmann Manifolds | http://arxiv.org/abs/1401.8126 | id:1401.8126 author:Mehrtash Harandi, Richard Hartley, Chunhua Shen, Brian Lovell, Conrad Sanderson category:cs.LG cs.CV stat.ML  published:2014-01-31 summary:Sparsity-based representations have recently led to notable results in various visual recognition tasks. In a separate line of research, Riemannian manifolds have been shown useful for dealing with features and models that do not lie in Euclidean spaces. With the aim of building a bridge between the two realms, we address the problem of sparse coding and dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping. This in turn enables us to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we propose closed-form solutions for learning a Grassmann dictionary, atom by atom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann sparse coding and dictionary learning algorithms through embedding into Hilbert spaces. Experiments on several classification tasks (gender recognition, gesture classification, scene analysis, face recognition, action recognition and dynamic texture classification) show that the proposed approaches achieve considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelized Affine Hull Method and graph-embedding Grassmann discriminant analysis. version:2
arxiv-1505-05212 | Barcode Annotations for Medical Image Retrieval: A Preliminary Investigation | http://arxiv.org/abs/1505.05212 | id:1505.05212 author:Hamid R. Tizhoosh category:cs.CV  published:2015-05-19 summary:This paper proposes to generate and to use barcodes to annotate medical images and/or their regions of interest such as organs, tumors and tissue types. A multitude of efficient feature-based image retrieval methods already exist that can assign a query image to a certain image class. Visual annotations may help to increase the retrieval accuracy if combined with existing feature-based classification paradigms. Whereas with annotations we usually mean textual descriptions, in this paper barcode annotations are proposed. In particular, Radon barcodes (RBC) are introduced. As well, local binary patterns (LBP) and local Radon binary patterns (LRBP) are implemented as barcodes. The IRMA x-ray dataset with 12,677 training images and 1,733 test images is used to verify how barcodes could facilitate image retrieval. version:1
arxiv-1505-05208 | oASIS: Adaptive Column Sampling for Kernel Matrix Approximation | http://arxiv.org/abs/1505.05208 | id:1505.05208 author:Raajen Patel, Thomas A. Goldstein, Eva L. Dyer, Azalia Mirhoseini, Richard G. Baraniuk category:stat.ML cs.LG G.1.0; G.4  published:2015-05-19 summary:Kernel matrices (e.g. Gram or similarity matrices) are essential for many state-of-the-art approaches to classification, clustering, and dimensionality reduction. For large datasets, the cost of forming and factoring such kernel matrices becomes intractable. To address this challenge, we introduce a new adaptive sampling algorithm called Accelerated Sequential Incoherence Selection (oASIS) that samples columns without explicitly computing the entire kernel matrix. We provide conditions under which oASIS is guaranteed to exactly recover the kernel matrix with an optimal number of columns selected. Numerical experiments on both synthetic and real-world datasets demonstrate that oASIS achieves performance comparable to state-of-the-art adaptive sampling methods at a fraction of the computational cost. The low runtime complexity of oASIS and its low memory footprint enable the solution of large problems that are simply intractable using other adaptive methods. version:1
arxiv-1505-05190 | Image Reconstruction from Bag-of-Visual-Words | http://arxiv.org/abs/1505.05190 | id:1505.05190 author:Hiroharu Kato, Tatsuya Harada category:cs.CV cs.AI  published:2015-05-19 summary:The objective of this work is to reconstruct an original image from Bag-of-Visual-Words (BoVW). Image reconstruction from features can be a means of identifying the characteristics of features. Additionally, it enables us to generate novel images via features. Although BoVW is the de facto standard feature for image recognition and retrieval, successful image reconstruction from BoVW has not been reported yet. What complicates this task is that BoVW lacks the spatial information for including visual words. As described in this paper, to estimate an original arrangement, we propose an evaluation function that incorporates the naturalness of local adjacency and the global position, with a method to obtain related parameters using an external image database. To evaluate the performance of our method, we reconstruct images of objects of 101 kinds. Additionally, we apply our method to analyze object classifiers and to generate novel images via BoVW. version:1
arxiv-1307-8229 | Posterior Contraction Rates of the Phylogenetic Indian Buffet Processes | http://arxiv.org/abs/1307.8229 | id:1307.8229 author:Mengjie Chen, Chao Gao, Hongyu Zhao category:stat.ML math.ST q-bio.QM stat.AP stat.TH  published:2013-07-31 summary:By expressing prior distributions as general stochastic processes, nonparametric Bayesian methods provide a flexible way to incorporate prior knowledge and constrain the latent structure in statistical inference. The Indian buffet process (IBP) is such an example that can be used to define a prior distribution on infinite binary features, where the exchangeability among subjects is assumed. The phylogenetic Indian buffet process (pIBP), a derivative of IBP, enables the modeling of non-exchangeability among subjects through a stochastic process on a rooted tree, which is similar to that used in phylogenetics, to describe relationships among the subjects. In this paper, we study the theoretical properties of IBP and pIBP under a binary factor model. We establish the posterior contraction rates for both IBP and pIBP and substantiate the theoretical results through simulation studies. This is the first work addressing the frequentist property of the posterior behaviors of IBP and pIBP. We also demonstrated its practical usefulness by applying pIBP prior to a real data example arising in the field of cancer genomics where the exchangeability among subjects is violated. version:2
arxiv-1504-07468 | Non-Gaussian Discriminative Factor Models via the Max-Margin Rank-Likelihood | http://arxiv.org/abs/1504.07468 | id:1504.07468 author:Xin Yuan, Ricardo Henao, Ephraim L. Tsalik, Raymond J. Langley, Lawrence Carin category:stat.ML  published:2015-04-28 summary:We consider the problem of discriminative factor analysis for data that are in general non-Gaussian. A Bayesian model based on the ranks of the data is proposed. We first introduce a new {\em max-margin} version of the rank-likelihood. A discriminative factor model is then developed, integrating the max-margin rank-likelihood and (linear) Bayesian support vector machines, which are also built on the max-margin principle. The discriminative factor model is further extended to the {\em nonlinear} case through mixtures of local linear classifiers, via Dirichlet processes. Fully local conjugacy of the model yields efficient inference with both Markov Chain Monte Carlo and variational Bayes approaches. Extensive experiments on benchmark and real data demonstrate superior performance of the proposed model and its potential for applications in computational biology. version:3
arxiv-1505-05117 | Vector-Space Markov Random Fields via Exponential Families | http://arxiv.org/abs/1505.05117 | id:1505.05117 author:Wesley Tansey, Oscar Hernan Madrid Padilla, Arun Sai Suggala, Pradeep Ravikumar category:stat.ML  published:2015-05-19 summary:We present Vector-Space Markov Random Fields (VS-MRFs), a novel class of undirected graphical models where each variable can belong to an arbitrary vector space. VS-MRFs generalize a recent line of work on scalar-valued, uni-parameter exponential family and mixed graphical models, thereby greatly broadening the class of exponential families available (e.g., allowing multinomial and Dirichlet distributions). Specifically, VS-MRFs are the joint graphical model distributions where the node-conditional distributions belong to generic exponential families with general vector space domains. We also present a sparsistent $M$-estimator for learning our class of MRFs that recovers the correct set of edges with high probability. We validate our approach via a set of synthetic data experiments as well as a real-world case study of over four million foods from the popular diet tracking app MyFitnessPal. Our results demonstrate that our algorithm performs well empirically and that VS-MRFs are capable of capturing and highlighting interesting structure in complex, real-world data. All code for our algorithm is open source and publicly available. version:1
arxiv-1501-02565 | EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow | http://arxiv.org/abs/1501.02565 | id:1501.02565 author:Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV  published:2015-01-12 summary:We propose a novel approach for optical flow estimation , targeted at large displacements with significant oc-clusions. It consists of two steps: i) dense matching by edge-preserving interpolation from a sparse set of matches; ii) variational energy minimization initialized with the dense matches. The sparse-to-dense interpolation relies on an appropriate choice of the distance, namely an edge-aware geodesic distance. This distance is tailored to handle occlusions and motion boundaries -- two common and difficult issues for optical flow computation. We also propose an approximation scheme for the geodesic distance to allow fast computation without loss of performance. Subsequent to the dense interpolation step, standard one-level variational energy minimization is carried out on the dense matches to obtain the final flow estimation. The proposed approach, called Edge-Preserving Interpolation of Correspondences (EpicFlow) is fast and robust to large displacements. It significantly outperforms the state of the art on MPI-Sintel and performs on par on Kitti and Middlebury. version:2
arxiv-1410-6460 | Markov Chain Monte Carlo and Variational Inference: Bridging the Gap | http://arxiv.org/abs/1410.6460 | id:1410.6460 author:Tim Salimans, Diederik P. Kingma, Max Welling category:stat.CO stat.ML  published:2014-10-23 summary:Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results. version:4
arxiv-1505-04984 | Risk and Regret of Hierarchical Bayesian Learners | http://arxiv.org/abs/1505.04984 | id:1505.04984 author:Jonathan H. Huggins, Joshua B. Tenenbaum category:cs.LG stat.ML  published:2015-05-19 summary:Common statistical practice has shown that the full power of Bayesian methods is not realized until hierarchical priors are used, as these allow for greater "robustness" and the ability to "share statistical strength." Yet it is an ongoing challenge to provide a learning-theoretically sound formalism of such notions that: offers practical guidance concerning when and how best to utilize hierarchical models; provides insights into what makes for a good hierarchical prior; and, when the form of the prior has been chosen, can guide the choice of hyperparameter settings. We present a set of analytical tools for understanding hierarchical priors in both the online and batch learning settings. We provide regret bounds under log-loss, which show how certain hierarchical models compare, in retrospect, to the best single model in the model class. We also show how to convert a Bayesian log-loss regret bound into a Bayesian risk bound for any bounded loss, a result which may be of independent interest. Risk and regret bounds for Student's $t$ and hierarchical Gaussian priors allow us to formalize the concepts of "robustness" and "sharing statistical strength." Priors for feature selection are investigated as well. Our results suggest that the learning-theoretic benefits of using hierarchical priors can often come at little cost on practical problems. version:1
arxiv-1505-04966 | Multi-task additive models with shared transfer functions based on dictionary learning | http://arxiv.org/abs/1505.04966 | id:1505.04966 author:Alhussein Fawzi, Mathieu Sinn, Pascal Frossard category:stat.ML cs.LG  published:2015-05-19 summary:Additive models form a widely popular class of regression models which represent the relation between covariates and response variables as the sum of low-dimensional transfer functions. Besides flexibility and accuracy, a key benefit of these models is their interpretability: the transfer functions provide visual means for inspecting the models and identifying domain-specific relations between inputs and outputs. However, in large-scale problems involving the prediction of many related tasks, learning independently additive models results in a loss of model interpretability, and can cause overfitting when training data is scarce. We introduce a novel multi-task learning approach which provides a corpus of accurate and interpretable additive models for a large number of related forecasting tasks. Our key idea is to share transfer functions across models in order to reduce the model complexity and ease the exploration of the corpus. We establish a connection with sparse dictionary learning and propose a new efficient fitting algorithm which alternates between sparse coding and transfer function updates. The former step is solved via an extension of Orthogonal Matching Pursuit, whose properties are analyzed using a novel recovery condition which extends existing results in the literature. The latter step is addressed using a traditional dictionary update rule. Experiments on real-world data demonstrate that our approach compares favorably to baseline methods while yielding an interpretable corpus of models, revealing structure among the individual tasks and being more robust when training data is scarce. Our framework therefore extends the well-known benefits of additive models to common regression settings possibly involving thousands of tasks. version:1
arxiv-1401-4529 | General factorization framework for context-aware recommendations | http://arxiv.org/abs/1401.4529 | id:1401.4529 author:Balázs Hidasi, Domonkos Tikk category:cs.IR cs.LG  published:2014-01-18 summary:Context-aware recommendation algorithms focus on refining recommendations by considering additional information, available to the system. This topic has gained a lot of attention recently. Among others, several factorization methods were proposed to solve the problem, although most of them assume explicit feedback which strongly limits their real-world applicability. While these algorithms apply various loss functions and optimization strategies, the preference modeling under context is less explored due to the lack of tools allowing for easy experimentation with various models. As context dimensions are introduced beyond users and items, the space of possible preference models and the importance of proper modeling largely increases. In this paper we propose a General Factorization Framework (GFF), a single flexible algorithm that takes the preference model as an input and computes latent feature matrices for the input dimensions. GFF allows us to easily experiment with various linear models on any context-aware recommendation task, be it explicit or implicit feedback based. The scaling properties makes it usable under real life circumstances as well. We demonstrate the framework's potential by exploring various preference models on a 4-dimensional context-aware problem with contexts that are available for almost any real life datasets. We show in our experiments -- performed on five real life, implicit feedback datasets -- that proper preference modelling significantly increases recommendation accuracy, and previously unused models outperform the traditional ones. Novel models in GFF also outperform state-of-the-art factorization algorithms. We also extend the method to be fully compliant to the Multidimensional Dataspace Model, one of the most extensive data models of context-enriched data. Extended GFF allows the seamless incorporation of information into the fac[truncated] version:2
arxiv-1505-04925 | High Performance Offline Handwritten Chinese Character Recognition Using GoogLeNet and Directional Feature Maps | http://arxiv.org/abs/1505.04925 | id:1505.04925 author:Zhuoyao Zhong, Lianwen Jin, Zecheng Xie category:cs.CV  published:2015-05-19 summary:Just like its great success in solving many computer vision problems, the convolutional neural networks (CNN) provided new end-to-end approach to handwritten Chinese character recognition (HCCR) with very promising results in recent years. However, previous CNNs so far proposed for HCCR were neither deep enough nor slim enough. We show in this paper that, a deeper architecture can benefit HCCR a lot to achieve higher performance, meanwhile can be designed with less parameters. We also show that the traditional feature extraction methods, such as Gabor or gradient feature maps, are still useful for enhancing the performance of CNN. We design a streamlined version of GoogLeNet [13], which was original proposed for image classification in recent years with very deep architecture, for HCCR (denoted as HCCR-GoogLeNet). The HCCR-GoogLeNet we used is 19 layers deep but involves with only 7.26 million parameters. Experiments were conducted using the ICDAR 2013 offline HCCR competition dataset. It has been shown that with the proper incorporation with traditional directional feature maps, the proposed single and ensemble HCCR-GoogLeNet models achieve new state of the art recognition accuracy of 96.35% and 96.74%, respectively, outperforming previous best result with significant gap. version:1
arxiv-1505-04922 | Character-level Chinese Writer Identification using Path Signature Feature, DropStroke and Deep CNN | http://arxiv.org/abs/1505.04922 | id:1505.04922 author:Weixin Yang, Lianwen Jin, Manfei Liu category:cs.CV  published:2015-05-19 summary:Most existing online writer-identification systems require that the text content is supplied in advance and rely on separately designed features and classifiers. The identifications are based on lines of text, entire paragraphs, or entire documents; however, these materials are not always available. In this paper, we introduce a path-signature feature to an end-to-end text-independent writer-identification system with a deep convolutional neural network (DCNN). Because deep models require a considerable amount of data to achieve good performance, we propose a data-augmentation method named DropStroke to enrich personal handwriting. Experiments were conducted on online handwritten Chinese characters from the CASIA-OLHWDB1.0 dataset, which consists of 3,866 classes from 420 writers. For each writer, we only used 200 samples for training and the remaining 3,666. The results reveal that the path-signature feature is useful for writer identification, and the proposed DropStroke technique enhances the generalization and significantly improves performance. version:1
arxiv-1505-04873 | Have a Look at What I See | http://arxiv.org/abs/1505.04873 | id:1505.04873 author:Lior Talker, Yael Moses, Ilan Shimshoni category:cs.CV  published:2015-05-19 summary:We propose a method for guiding a photographer to rotate her/his smartphone camera to obtain an image that overlaps with another image of the same scene. The other image is taken by another photographer from a different viewpoint. Our method is applicable even when the images do not have overlapping fields of view. Straightforward applications of our method include sharing attention to regions of interest for social purposes, or adding missing images to improve structure for motion results. Our solution uses additional images of the scene, which are often available since many people use their smartphone cameras regularly. These images may be available online from other photographers who are present at the scene. Our method avoids 3D scene reconstruction; it relies instead on a new representation that consists of the spatial orders of the scene points on two axes, x and y. This representation allows a sequence of points to be chosen efficiently and projected onto the photographers images, using epipolar point transfer. Overlaying these epipolar lines on the live preview of the camera produces a convenient interface to guide the user. The method was tested on challenging datasets of images and succeeded in guiding a photographer from one view to a non-overlapping destination view. version:1
arxiv-1505-04868 | Action Recognition with Trajectory-Pooled Deep-Convolutional Descriptors | http://arxiv.org/abs/1505.04868 | id:1505.04868 author:Limin Wang, Yu Qiao, Xiaoou Tang category:cs.CV  published:2015-05-19 summary:Visual features are of vital importance for human action understanding in videos. This paper presents a new video representation, called trajectory-pooled deep-convolutional descriptor (TDD), which shares the merits of both hand-crafted features and deep-learned features. Specifically, we utilize deep architectures to learn discriminative convolutional feature maps, and conduct trajectory-constrained pooling to aggregate these convolutional features into effective descriptors. To enhance the robustness of TDDs, we design two normalization methods to transform convolutional feature maps, namely spatiotemporal normalization and channel normalization. The advantages of our features come from (i) TDDs are automatically learned and contain high discriminative capacity compared with those hand-crafted features; (ii) TDDs take account of the intrinsic characteristics of temporal dimension and introduce the strategies of trajectory-constrained sampling and pooling for aggregating deep-learned features. We conduct experiments on two challenging datasets: HMDB51 and UCF101. Experimental results show that TDDs outperform previous hand-crafted features and deep-learned features. Our method also achieves superior performance to the state of the art on these datasets (HMDB51 65.9%, UCF101 91.5%). version:1
arxiv-1406-1822 | Logarithmic Time Online Multiclass prediction | http://arxiv.org/abs/1406.1822 | id:1406.1822 author:Anna Choromanska, John Langford category:cs.LG  published:2014-06-06 summary:We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications. version:13
arxiv-1410-5329 | Naive Bayes and Text Classification I - Introduction and Theory | http://arxiv.org/abs/1410.5329 | id:1410.5329 author:Sebastian Raschka category:cs.LG  published:2014-10-16 summary:Naive Bayes classifiers, a family of classifiers that are based on the popular Bayes' probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification and disease prediction. In this article, we will look at the main concepts of naive Bayes classification in the context of document categorization. version:3
arxiv-1501-07320 | Tensor Factorization via Matrix Factorization | http://arxiv.org/abs/1501.07320 | id:1501.07320 author:Volodymyr Kuleshov, Arun Tejasvi Chaganty, Percy Liang category:cs.LG stat.ML  published:2015-01-29 summary:Tensor factorization arises in many machine learning applications, such knowledge base modeling and parameter estimation in latent variable models. However, numerical methods for tensor factorization have not reached the level of maturity of matrix factorization methods. In this paper, we propose a new method for CP tensor factorization that uses random projections to reduce the problem to simultaneous matrix diagonalization. Our method is conceptually simple and also applies to non-orthogonal and asymmetric tensors of arbitrary order. We prove that a small number random projections essentially preserves the spectral information in the tensor, allowing us to remove the dependence on the eigengap that plagued earlier tensor-to-matrix reductions. Experimentally, our method outperforms existing tensor factorization methods on both simulated data and two real datasets. version:2
arxiv-1502-02398 | Towards a Learning Theory of Cause-Effect Inference | http://arxiv.org/abs/1502.02398 | id:1502.02398 author:David Lopez-Paz, Krikamol Muandet, Bernhard Schölkopf, Ilya Tolstikhin category:stat.ML math.PR math.ST stat.TH  published:2015-02-09 summary:We pose causal inference as the problem of learning to classify probability distributions. In particular, we assume access to a collection $\{(S_i,l_i)\}_{i=1}^n$, where each $S_i$ is a sample drawn from the probability distribution of $X_i \times Y_i$, and $l_i$ is a binary label indicating whether "$X_i \to Y_i$" or "$X_i \leftarrow Y_i$". Given these data, we build a causal inference rule in two steps. First, we featurize each $S_i$ using the kernel mean embedding associated with some characteristic kernel. Second, we train a binary classifier on such embeddings to distinguish between causal directions. We present generalization bounds showing the statistical consistency and learning rates of the proposed approach, and provide a simple implementation that achieves state-of-the-art cause-effect inference. Furthermore, we extend our ideas to infer causal relationships between more than two variables. version:2
arxiv-1505-04824 | An Asynchronous Mini-Batch Algorithm for Regularized Stochastic Optimization | http://arxiv.org/abs/1505.04824 | id:1505.04824 author:Hamid Reza Feyzmahdavian, Arda Aytekin, Mikael Johansson category:math.OC cs.SY stat.ML  published:2015-05-18 summary:Mini-batch optimization has proven to be a powerful paradigm for large-scale learning. However, the state of the art parallel mini-batch algorithms assume synchronous operation or cyclic update orders. When worker nodes are heterogeneous (due to different computational capabilities or different communication delays), synchronous and cyclic operations are inefficient since they will leave workers idle waiting for the slower nodes to complete their computations. In this paper, we propose an asynchronous mini-batch algorithm for regularized stochastic optimization problems with smooth loss functions that eliminates idle waiting and allows workers to run at their maximal update rates. We show that by suitably choosing the step-size values, the algorithm achieves a rate of the order $O(1/\sqrt{T})$ for general convex regularization functions, and the rate $O(1/T)$ for strongly convex regularization functions, where $T$ is the number of iterations. In both cases, the impact of asynchrony on the convergence rate of our algorithm is asymptotically negligible, and a near-linear speedup in the number of workers can be expected. Theoretical results are confirmed in real implementations on a distributed computing infrastructure. version:1
arxiv-1412-2684 | HyperSpectral classification with adaptively weighted L1-norm regularization and spatial postprocessing | http://arxiv.org/abs/1412.2684 | id:1412.2684 author:Victor Stefan Aldea, M. O. Ahmad, W. E. Lynch category:math.OC cs.CV  published:2014-12-08 summary:Sparse regression methods have been proven effective in a wide range of signal processing problems such as image compression, speech coding, channel equalization, linear regression and classification. In this paper we develop a new method of hyperspectral image classification based on the sparse unmixing algorithm SUnSAL for which a pixel adaptive L1-norm regularization term is introduced. To further enhance class separability, the algorithm is kernelized using a RBF kernel and the final results are improved by a combination of spatial pre and post-processing operations. We show that our method is competitive with state of the art algorithms such as SVM-CK, KLR-CK, KSOMP and KSSP. version:2
arxiv-1505-04803 | Predicting Important Objects for Egocentric Video Summarization | http://arxiv.org/abs/1505.04803 | id:1505.04803 author:Yong Jae Lee, Kristen Grauman category:cs.CV  published:2015-05-18 summary:We present a video summarization approach for egocentric or "wearable" camera data. Given hours of video, the proposed method produces a compact storyboard summary of the camera wearer's day. In contrast to traditional keyframe selection techniques, the resulting summary focuses on the most important objects and people with which the camera wearer interacts. To accomplish this, we develop region cues indicative of high-level saliency in egocentric video---such as the nearness to hands, gaze, and frequency of occurrence---and learn a regressor to predict the relative importance of any new region based on these cues. Using these predictions and a simple form of temporal event detection, our method selects frames for the storyboard that reflect the key object-driven happenings. We adjust the compactness of the final summary given either an importance selection criterion or a length budget; for the latter, we design an efficient dynamic programming solution that accounts for importance, visual uniqueness, and temporal displacement. Critically, the approach is neither camera-wearer-specific nor object-specific; that means the learned importance metric need not be trained for a given user or context, and it can predict the importance of objects and people that have never been seen previously. Our results on two egocentric video datasets show the method's promise relative to existing techniques for saliency and summarization. version:1
arxiv-1505-04778 | On the tightness of an SDP relaxation of k-means | http://arxiv.org/abs/1505.04778 | id:1505.04778 author:Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar category:cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH  published:2015-05-18 summary:Recently, Awasthi et al. introduced an SDP relaxation of the $k$-means problem in $\mathbb R^m$. In this work, we consider a random model for the data points in which $k$ balls of unit radius are deterministically distributed throughout $\mathbb R^m$, and then in each ball, $n$ points are drawn according to a common rotationally invariant probability distribution. For any fixed ball configuration and probability distribution, we prove that the SDP relaxation of the $k$-means problem exactly recovers these planted clusters with probability $1-e^{-\Omega(n)}$ provided the distance between any two of the ball centers is $>2+\epsilon$, where $\epsilon$ is an explicit function of the configuration of the ball centers, and can be arbitrarily small when $m$ is large. version:1
arxiv-1502-02763 | Cascading Bandits: Learning to Rank in the Cascade Model | http://arxiv.org/abs/1502.02763 | id:1502.02763 author:Branislav Kveton, Csaba Szepesvari, Zheng Wen, Azin Ashkan category:cs.LG stat.ML  published:2015-02-10 summary:A search engine usually outputs a list of $K$ web pages. The user examines this list, from the first web page to the last, and chooses the first attractive page. This model of user behavior is known as the cascade model. In this paper, we propose cascading bandits, a learning variant of the cascade model where the objective is to identify $K$ most attractive items. We formulate our problem as a stochastic combinatorial partial monitoring problem. We propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We also prove gap-dependent upper bounds on the regret of these algorithms and derive a lower bound on the regret in cascading bandits. The lower bound matches the upper bound of CascadeKL-UCB up to a logarithmic factor. We experiment with our algorithms on several problems. The algorithms perform surprisingly well even when our modeling assumptions are violated. version:2
arxiv-1505-02475 | Foundational principles for large scale inference: Illustrations through correlation mining | http://arxiv.org/abs/1505.02475 | id:1505.02475 author:Alfred O. Hero, Bala Rajaratnam category:math.ST stat.ML stat.TH  published:2015-05-11 summary:When can reliable inference be drawn in the "Big Data" context? This paper presents a framework for answering this fundamental question in the context of correlation mining, with implications for general large scale inference. In large scale data applications like genomics, connectomics, and eco-informatics the dataset is often variable-rich but sample-starved: a regime where the number $n$ of acquired samples (statistical replicates) is far fewer than the number $p$ of observed variables (genes, neurons, voxels, or chemical constituents). Much of recent work has focused on understanding the computational complexity of proposed methods for "Big Data." Sample complexity however has received relatively less attention, especially in the setting when the sample size $n$ is fixed, and the dimension $p$ grows without bound. To address this gap, we develop a unified statistical framework that explicitly quantifies the sample complexity of various inferential tasks. Sampling regimes can be divided into several categories: 1) the classical asymptotic regime where the variable dimension is fixed and the sample size goes to infinity; 2) the mixed asymptotic regime where both variable dimension and sample size go to infinity at comparable rates; 3) the purely high dimensional asymptotic regime where the variable dimension goes to infinity and the sample size is fixed. Each regime has its niche but only the latter regime applies to exa-scale data dimension. We illustrate this high dimensional framework for the problem of correlation mining, where it is the matrix of pairwise and partial correlations among the variables that are of interest. We demonstrate various regimes of correlation mining based on the unifying perspective of high dimensional learning rates and sample complexity for different structured covariance models and different inference tasks. version:2
arxiv-1412-0744 | Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from the Literature | http://arxiv.org/abs/1412.0744 | id:1412.0744 author:Artemy Kolchinsky, Anália Lourenço, Heng-Yi Wu, Lang Li, Luis M. Rocha category:stat.ML cs.IR q-bio.QM H.2.8; H.3.1; J.3  published:2014-12-02 summary:Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a subject of intense scientific interest. Biomedical literature mining can aid DDI research by extracting evidence for large numbers of potential interactions from published literature and clinical databases. Though DDI is investigated in domains ranging in scale from intracellular biochemistry to human populations, literature mining has not been used to extract specific types of experimental evidence, which are reported differently for distinct experimental goals. We focus on pharmacokinetic evidence for DDI, essential for identifying causal mechanisms of putative interactions and as input for further pharmacological and pharmaco-epidemiology investigations. We used manually curated corpora of PubMed abstracts and annotated sentences to evaluate the efficacy of literature mining on two tasks: first, identifying PubMed abstracts containing pharmacokinetic evidence of DDIs; second, extracting sentences containing such evidence from abstracts. We implemented a text mining pipeline and evaluated it using several linear classifiers and a variety of feature transforms. The most important textual features in the abstract and sentence classification tasks were analyzed. We also investigated the performance benefits of using features derived from PubMed metadata fields, various publicly available named entity recognizers, and pharmacokinetic dictionaries. Several classifiers performed very well in distinguishing relevant and irrelevant abstracts (reaching F1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65, iAUC~=0.83). We found that word bigram features were important for achieving optimal classifier performance and that features derived from Medical Subject Headings (MeSH) terms significantly improved abstract classification. ... version:2
arxiv-1505-04141 | WhittleSearch: Interactive Image Search with Relative Attribute Feedback | http://arxiv.org/abs/1505.04141 | id:1505.04141 author:Adriana Kovashka, Devi Parikh, Kristen Grauman category:cs.CV  published:2015-05-15 summary:We propose a novel mode of feedback for image search, where a user describes which properties of exemplar images should be adjusted in order to more closely match his/her mental model of the image sought. For example, perusing image results for a query "black shoes", the user might state, "Show me shoe images like these, but sportier." Offline, our approach first learns a set of ranking functions, each of which predicts the relative strength of a nameable attribute in an image (e.g., sportiness). At query time, the system presents the user with a set of exemplar images, and the user relates them to his/her target image with comparative statements. Using a series of such constraints in the multi-dimensional attribute space, our method iteratively updates its relevance function and re-ranks the database of images. To determine which exemplar images receive feedback from the user, we present two variants of the approach: one where the feedback is user-initiated and another where the feedback is actively system-initiated. In either case, our approach allows a user to efficiently "whittle away" irrelevant portions of the visual feature space, using semantic language to precisely communicate her preferences to the system. We demonstrate our technique for refining image search for people, products, and scenes, and we show that it outperforms traditional binary relevance feedback in terms of search speed and accuracy. In addition, the ordinal nature of relative attributes helps make our active approach efficient -- both computationally for the machine when selecting the reference images, and for the user by requiring less user interaction than conventional passive and active methods. version:2
arxiv-1505-04637 | Ensemble of Example-Dependent Cost-Sensitive Decision Trees | http://arxiv.org/abs/1505.04637 | id:1505.04637 author:Alejandro Correa Bahnsen, Djamila Aouada, Bjorn Ottersten category:cs.LG  published:2015-05-18 summary:Several real-world classification problems are example-dependent cost-sensitive in nature, where the costs due to misclassification vary between examples and not only within classes. However, standard classification methods do not take these costs into account, and assume a constant cost of misclassification errors. In previous works, some methods that take into account the financial costs into the training of different algorithms have been proposed, with the example-dependent cost-sensitive decision tree algorithm being the one that gives the highest savings. In this paper we propose a new framework of ensembles of example-dependent cost-sensitive decision-trees. The framework consists in creating different example-dependent cost-sensitive decision trees on random subsamples of the training set, and then combining them using three different combination approaches. Moreover, we propose two new cost-sensitive combination approaches; cost-sensitive weighted voting and cost-sensitive stacking, the latter being based on the cost-sensitive logistic regression method. Finally, using five different databases, from four real-world applications: credit card fraud detection, churn modeling, credit scoring and direct marketing, we evaluate the proposed method against state-of-the-art example-dependent cost-sensitive techniques, namely, cost-proportionate sampling, Bayes minimum risk and cost-sensitive decision trees. The results show that the proposed algorithms have better results for all databases, in the sense of higher savings. version:1
arxiv-1505-04636 | Graph Partitioning via Parallel Submodular Approximation to Accelerate Distributed Machine Learning | http://arxiv.org/abs/1505.04636 | id:1505.04636 author:Mu Li, Dave G. Andersen, Alexander J. Smola category:cs.DC cs.AI cs.LG  published:2015-05-18 summary:Distributed computing excels at processing large scale data, but the communication cost for synchronizing the shared parameters may slow down the overall performance. Fortunately, the interactions between parameter and data in many problems are sparse, which admits efficient partition in order to reduce the communication overhead. In this paper, we formulate data placement as a graph partitioning problem. We propose a distributed partitioning algorithm. We give both theoretical guarantees and a highly efficient implementation. We also provide a highly efficient implementation of the algorithm and demonstrate its promising results on both text datasets and social networks. We show that the proposed algorithm leads to 1.6x speedup of a state-of-the-start distributed machine learning system by eliminating 90\% of the network communication. version:1
arxiv-1505-04627 | Simple regret for infinitely many armed bandits | http://arxiv.org/abs/1505.04627 | id:1505.04627 author:Alexandra Carpentier, Michal Valko category:cs.LG stat.ML  published:2015-05-18 summary:We consider a stochastic bandit problem with infinitely many arms. In this setting, the learner has no chance of trying all the arms even once and has to dedicate its limited number of samples only to a certain number of arms. All previous algorithms for this setting were designed for minimizing the cumulative regret of the learner. In this paper, we propose an algorithm aiming at minimizing the simple regret. As in the cumulative regret setting of infinitely many armed bandits, the rate of the simple regret will depend on a parameter $\beta$ characterizing the distribution of the near-optimal arms. We prove that depending on $\beta$, our algorithm is minimax optimal either up to a multiplicative constant or up to a $\log(n)$ factor. We also provide extensions to several important cases: when $\beta$ is unknown, in a natural setting where the near-optimal arms have a small variance, and in the case of unknown time horizon. version:1
arxiv-1505-04618 | Fractally-organized Connectionist Networks: Conjectures and Preliminary Results | http://arxiv.org/abs/1505.04618 | id:1505.04618 author:Vincenzo De Florio category:cs.NE  published:2015-05-18 summary:A strict interpretation of connectionism mandates complex networks of simple components. The question here is, is this simplicity to be interpreted in absolute terms? I conjecture that absolute simplicity might not be an essential attribute of connectionism, and that it may be effectively exchanged with a requirement for relative simplicity, namely simplicity with respect to the current organizational level. In this paper I provide some elements to the analysis of the above question. In particular I conjecture that fractally organized connectionist networks may provide a convenient means to achive what Leibniz calls an "art of complication", namely an effective way to encapsulate complexity and practically extend the applicability of connectionism to domains such as sociotechnical system modeling and design. Preliminary evidence to my claim is brought by considering the design of the software architecture designed for the telemonitoring service of Flemish project "Little Sister". version:1
arxiv-1505-04617 | Joint Representation Classification for Collective Face Recognition | http://arxiv.org/abs/1505.04617 | id:1505.04617 author:Liping Wang, Songcan Chen category:cs.CV math.OC  published:2015-05-18 summary:Sparse representation based classification (SRC) is popularly used in many applications such as face recognition, and implemented in two steps: representation coding and classification. For a given set of testing images, SRC codes every image over the base images as a sparse representation then classifies it to the class with the least representation error. This scheme utilizes an individual representation rather than the collective one to classify such a set of images, doing so obviously ignores the correlation among the given images. In this paper, a joint representation classification (JRC) for collective face recognition is proposed. JRC takes the correlation of multiple images as well as a single representation into account. Under the assumption that the given face images are generally related to each other, JRC codes all the testing images over the base images simultaneously to facilitate recognition. To this end, the testing inputs are aligned into a matrix and the joint representation coding is formulated to a generalized $l_{2,q}-l_{2,p}$-minimization problem. To uniformly solve the induced optimization problems for any $q\in[1,2]$ and $p\in (0,2]$, an iterative quadratic method (IQM) is developed. IQM is proved to be a strict descent algorithm with convergence to the optimal solution. Moreover, a more practical IQM is proposed for large-scale case. Experimental results on three public databases show that the JRC with practical IQM no only saves much computational cost but also achieves better performance in collective face recognition than the state-of-the-arts. version:1
arxiv-1501-04826 | Entailment Among Probabilistic Implications | http://arxiv.org/abs/1501.04826 | id:1501.04826 author:Albert Atserias, José L. Balcázar category:cs.LO cs.DB cs.LG  published:2015-01-20 summary:We study a natural variant of the implicational fragment of propositional logic. Its formulas are pairs of conjunctions of positive literals, related together by an implicational-like connective; the semantics of this sort of implication is defined in terms of a threshold on a conditional probability of the consequent, given the antecedent: we are dealing with what the data analysis community calls confidence of partial implications or association rules. Existing studies of redundancy among these partial implications have characterized so far only entailment from one premise and entailment from two premises. By exploiting a previously noted alternative view of this entailment in terms of linear programming duality, we characterize exactly the cases of entailment from arbitrary numbers of premises. As a result, we obtain decision algorithms of better complexity; additionally, for each potential case of entailment, we identify a critical confidence threshold and show that it is, actually, intrinsic to each set of premises and antecedent of the conclusion. version:2
arxiv-1407-1974 | Learning Discriminative Stein Kernel for SPD Matrices and Its Applications | http://arxiv.org/abs/1407.1974 | id:1407.1974 author:Jianjia Zhang, Lei Wang, Luping Zhou, Wanqing Li category:cs.CV  published:2014-07-08 summary:Stein kernel has recently shown promising performance on classifying images represented by symmetric positive definite (SPD) matrices. It evaluates the similarity between two SPD matrices through their eigenvalues. In this paper, we argue that directly using the original eigenvalues may be problematic because: i) Eigenvalue estimation becomes biased when the number of samples is inadequate, which may lead to unreliable kernel evaluation; ii) More importantly, eigenvalues only reflect the property of an individual SPD matrix. They are not necessarily optimal for computing Stein kernel when the goal is to discriminate different sets of SPD matrices. To address the two issues in one shot, we propose a discriminative Stein kernel, in which an extra parameter vector is defined to adjust the eigenvalues of the input SPD matrices. The optimal parameter values are sought by optimizing a proxy of classification performance. To show the generality of the proposed method, three different kernel learning criteria that are commonly used in the literature are employed respectively as a proxy. A comprehensive experimental study is conducted on a variety of image classification tasks to compare our proposed discriminative Stein kernel with the original Stein kernel and other commonly used methods for evaluating the similarity between SPD matrices. The experimental results demonstrate that, the discriminative Stein kernel can attain greater discrimination and better align with classification tasks by altering the eigenvalues. This makes it produce higher classification performance than the original Stein kernel and other commonly used methods. version:3
arxiv-1505-04597 | U-Net: Convolutional Networks for Biomedical Image Segmentation | http://arxiv.org/abs/1505.04597 | id:1505.04597 author:Olaf Ronneberger, Philipp Fischer, Thomas Brox category:cs.CV  published:2015-05-18 summary:There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net . version:1
arxiv-1505-04585 | Global Variational Method for Fingerprint Segmentation by Three-part Decomposition | http://arxiv.org/abs/1505.04585 | id:1505.04585 author:Duy Hoang Thai, Carsten Gottschlich category:cs.CV  published:2015-05-18 summary:Verifying an identity claim by fingerprint recognition is a commonplace experience for millions of people in their daily life, e.g. for unlocking a tablet computer or smartphone. The first processing step after fingerprint image acquisition is segmentation, i.e. dividing a fingerprint image into a foreground region which contains the relevant features for the comparison algorithm, and a background region. We propose a novel segmentation method by global three-part decomposition (G3PD). Based on global variational analysis, the G3PD method decomposes a fingerprint image into cartoon, texture and noise parts. After decomposition, the foreground region is obtained from the non-zero coefficients in the texture image using morphological processing. The segmentation performance of the G3PD method is compared to five state-of-the-art methods on a benchmark which comprises manually marked ground truth segmentation for 10560 images. Performance evaluations show that the G3PD method consistently outperforms existing methods in terms of segmentation accuracy. version:1
arxiv-1503-01640 | BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation | http://arxiv.org/abs/1503.01640 | id:1503.01640 author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV  published:2015-03-05 summary:Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called BoxSup, produces competitive results supervised by boxes only, on par with strong baselines fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further unleashes the power of deep convolutional networks and yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT. version:2
arxiv-1505-04548 | Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM | http://arxiv.org/abs/1505.04548 | id:1505.04548 author:Michael Milford, Hanme Kim, Michael Mangan, Stefan Leutenegger, Tom Stone, Barbara Webb, Andrew Davison category:cs.RO cs.CV  published:2015-05-18 summary:Event-based cameras offer much potential to the fields of robotics and computer vision, in part due to their large dynamic range and extremely high "frame rates". These attributes make them, at least in theory, particularly suitable for enabling tasks like navigation and mapping on high speed robotic platforms under challenging lighting conditions, a task which has been particularly challenging for traditional algorithms and camera sensors. Before these tasks become feasible however, progress must be made towards adapting and innovating current RGB-camera-based algorithms to work with event-based cameras. In this paper we present ongoing research investigating two distinct approaches to incorporating event-based cameras for robotic navigation: the investigation of suitable place recognition / loop closure techniques, and the development of efficient neural implementations of place recognition techniques that enable the possibility of place recognition using event-based cameras at very high frame rates using neuromorphic computing hardware. version:1
arxiv-1505-04518 | Emergence-focused design in complex system simulation | http://arxiv.org/abs/1505.04518 | id:1505.04518 author:Chris Marriott, Jobran Chebib category:q-bio.PE cs.AI cs.NE  published:2015-05-18 summary:Emergence is a phenomenon taken for granted in science but also still not well understood. We have developed a model of artificial genetic evolution intended to allow for emergence on genetic, population and social levels. We present the details of the current state of our environment, agent, and reproductive models. In developing our models we have relied on a principle of using non-linear systems to model as many systems as possible including mutation and recombination, gene-environment interaction, agent metabolism, agent survival, resource gathering and sexual reproduction. In this paper we review the genetic dynamics that have emerged in our system including genotype-phenotype divergence, genetic drift, pseudogenes, and gene duplication. We conclude that emergence-focused design in complex system simulation is necessary to reproduce the multilevel emergence seen in the natural world. version:1
arxiv-1505-04502 | Reproducible Evaluation of Pan-Tilt-Zoom Tracking | http://arxiv.org/abs/1505.04502 | id:1505.04502 author:Gengjie Chen, Pierre-Luc St-Charles, Wassim Bouachir, Thomas Joeisseint, Guillaume-Alexandre Bilodeau, Robert Bergevin category:cs.CV  published:2015-05-18 summary:Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in computer vision for many years. However, it is very difficult to assess the progress that has been made on this topic because there is no standard evaluation methodology. The difficulty in evaluating PTZ tracking algorithms arises from their dynamic nature. In contrast to other forms of tracking, PTZ tracking involves both locating the target in the image and controlling the motors of the camera to aim it so that the target stays in its field of view. This type of tracking can only be performed online. In this paper, we propose a new evaluation framework based on a virtual PTZ camera. With this framework, tracking scenarios do not change for each experiment and we are able to replicate online PTZ camera control and behavior including camera positioning delays, tracker processing delays, and numerical zoom. We tested our evaluation framework with the Camshift tracker to show its viability and to establish baseline results. version:1
arxiv-1504-02531 | HEp-2 Cell Image Classification with Deep Convolutional Neural Networks | http://arxiv.org/abs/1504.02531 | id:1504.02531 author:Zhimin Gao, Lei Wang, Luping Zhou, Jianjia Zhang category:cs.CV  published:2015-04-10 summary:Efficient Human Epithelial-2 (HEp-2) cell image classification can facilitate the diagnosis of many autoimmune diseases. This paper presents an automatic framework for this classification task, by utilizing the deep convolutional neural networks (CNNs) which have recently attracted intensive attention in visual recognition. This paper elaborates the important components of this framework, discusses multiple key factors that impact the efficiency of training a deep CNN, and systematically compares this framework with the well-established image classification models in the literature. Experiments on benchmark datasets show that i) the proposed framework can effectively outperform existing models by properly applying data augmentation; ii) our CNN-based framework demonstrates excellent adaptability across different datasets, which is highly desirable for classification under varying laboratory settings. Our system is ranked high in the cell image classification competition hosted by ICPR 2014. version:2
arxiv-1505-04474 | Visual Semantic Role Labeling | http://arxiv.org/abs/1505.04474 | id:1505.04474 author:Saurabh Gupta, Jitendra Malik category:cs.CV  published:2015-05-17 summary:In this paper we introduce the problem of Visual Semantic Role Labeling: given an image we want to detect people doing actions and localize the objects of interaction. Classical approaches to action recognition either study the task of action classification at the image or video clip level or at best produce a bounding box around the person doing the action. We believe such an output is inadequate and a complete understanding can only come when we are able to associate objects in the scene to the different semantic roles of the action. To enable progress towards this goal, we annotate a dataset of 16K people instances in 10K images with actions they are doing and associate objects in the scene with different semantic roles for each action. Finally, we provide a set of baseline algorithms for this task and analyze error modes providing directions for future work. version:1
arxiv-1505-04467 | Exploring Nearest Neighbor Approaches for Image Captioning | http://arxiv.org/abs/1505.04467 | id:1505.04467 author:Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, C. Lawrence Zitnick category:cs.CV  published:2015-05-17 summary:We explore a variety of nearest neighbor baseline approaches for image captioning. These approaches find a set of nearest neighbor images in the training set from which a caption may be borrowed for the query image. We select a caption for the query image by finding the caption that best represents the "consensus" of the set of candidate captions gathered from the nearest neighbor images. When measured by automatic evaluation metrics on the MS COCO caption evaluation server, these approaches perform as well as many recent approaches that generate novel captions. However, human studies show that a method that generates novel captions is still preferred over the nearest neighbor approach. version:1
arxiv-1505-04427 | The Best of Both Worlds: Combining Data-independent and Data-driven Approaches for Action Recognition | http://arxiv.org/abs/1505.04427 | id:1505.04427 author:Zhenzhong Lan, Dezhong Yao, Ming Lin, Shoou-I Yu, Alexander Hauptmann category:cs.CV  published:2015-05-17 summary:Motivated by the success of data-driven convolutional neural networks (CNNs) in object recognition on static images, researchers are working hard towards developing CNN equivalents for learning video features. However, learning video features globally has proven to be quite a challenge due to its high dimensionality, the lack of labelled data and the difficulty in processing large-scale video data. Therefore, we propose to leverage effective techniques from both data-driven and data-independent approaches to improve action recognition system. Our contribution is three-fold. First, we propose a two-stream Stacked Convolutional Independent Subspace Analysis (ConvISA) architecture to show that unsupervised learning methods can significantly boost the performance of traditional local features extracted from data-independent models. Second, we demonstrate that by learning on video volumes detected by Improved Dense Trajectory (IDT), we can seamlessly combine our novel local descriptors with hand-crafted descriptors. Thus we can utilize available feature enhancing techniques developed for hand-crafted descriptors. Finally, similar to multi-class classification framework in CNNs, we propose a training-free re-ranking technique that exploits the relationship among action classes to improve the overall performance. Our experimental results on four benchmark action recognition datasets show significantly improved performance. version:1
arxiv-1505-04424 | Improved Microaneurysm Detection using Deep Neural Networks | http://arxiv.org/abs/1505.04424 | id:1505.04424 author:Mrinal Haloi category:cs.CV 68T45  published:2015-05-17 summary:In this work, we propose a novel microaneurysm (MA) detection for early dieabetic ratinopathy screening using color fundus images. Since MA usually the first lesions to appear as a indicator of diabetic retinopathy, accurate detection of MA is necessary for treatment. Each pixel of the image is classified as either MA or non-MA using deep neural network with dropout training procedure using maxout activation function. No preprocessing step or manual feature extraction is required. Substantial improvements over standard MA detection method based on pipeline of preprocessing, feature extraction, classification followed by postprocessing is achieved. The presented method is evaluated in publicly available Retinopathy Online Challenge (ROC) and Diaretdb1v2 database and achieved state-of-the-art accuracy. version:1
arxiv-1505-04420 | CCG Parsing and Multiword Expressions | http://arxiv.org/abs/1505.04420 | id:1505.04420 author:Miryam de Lhoneux category:cs.CL  published:2015-05-17 summary:This thesis presents a study about the integration of information about Multiword Expressions (MWEs) into parsing with Combinatory Categorial Grammar (CCG). We build on previous work which has shown the benefit of adding information about MWEs to syntactic parsing by implementing a similar pipeline with CCG parsing. More specifically, we collapse MWEs to one token in training and test data in CCGbank, a corpus which contains sentences annotated with CCG derivations. Our collapsing algorithm however can only deal with MWEs when they form a constituent in the data which is one of the limitations of our approach. We study the effect of collapsing training and test data. A parsing effect can be obtained if collapsed data help the parser in its decisions and a training effect can be obtained if training on the collapsed data improves results. We also collapse the gold standard and show that our model significantly outperforms the baseline model on our gold standard, which indicates that there is a training effect. We show that the baseline model performs significantly better on our gold standard when the data are collapsed before parsing than when the data are collapsed after parsing which indicates that there is a parsing effect. We show that these results can lead to improved performance on the non-collapsed standard benchmark although we fail to show that it does so significantly. We conclude that despite the limited settings, there are noticeable improvements from using MWEs in parsing. We discuss ways in which the incorporation of MWEs into parsing can be improved and hypothesize that this will lead to more substantial results. We finally show that turning the MWE recognition part of the pipeline into an experimental part is a useful thing to do as we obtain different results with different recognizers. version:1
arxiv-1407-2433 | Identifying Cover Songs Using Information-Theoretic Measures of Similarity | http://arxiv.org/abs/1407.2433 | id:1407.2433 author:Peter Foster, Simon Dixon, Anssi Klapuri category:cs.IR cs.LG stat.ML  published:2014-07-09 summary:This paper investigates methods for quantifying similarity between audio signals, specifically for the task of of cover song detection. We consider an information-theoretic approach, where we compute pairwise measures of predictability between time series. We compare discrete-valued approaches operating on quantised audio features, to continuous-valued approaches. In the discrete case, we propose a method for computing the normalised compression distance, where we account for correlation between time series. In the continuous case, we propose to compute information-based measures of similarity as statistics of the prediction error between time series. We evaluate our methods on two cover song identification tasks using a data set comprised of 300 Jazz standards and using the Million Song Dataset. For both datasets, we observe that continuous-valued approaches outperform discrete-valued approaches. We consider approaches to estimating the normalised compression distance (NCD) based on string compression and prediction, where we observe that our proposed normalised compression distance with alignment (NCDA) improves average performance over NCD, for sequential compression algorithms. Finally, we demonstrate that continuous-valued distances may be combined to improve performance with respect to baseline approaches. Using a large-scale filter-and-refine approach, we demonstrate state-of-the-art performance for cover song identification using the Million Song Dataset. version:3
arxiv-1505-04382 | Robust Visual Knowledge Transfer via EDA | http://arxiv.org/abs/1505.04382 | id:1505.04382 author:Lei Zhang, David Zhang category:cs.CV  published:2015-05-17 summary:We address the problem of visual knowledge adaptation by leveraging labeled patterns from the source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. We introduce a new semi-supervised cross-domain network learning framework, referred to as Extreme Domain Adaptation (EDA), that allows us to simultaneously learn a category transformation and an extreme classifier by minimizing the L(2,1)-norm of the output weights and the learning error, in which the network output weights can be analytically determined. The unlabeled target data, as useful knowledge, is also learned as a fidelity term by minimizing the matching error between the extreme classifier and a base classifier to guarantee the stability during cross domain learning, into which many existing classifiers can be readily incorporated as base classifiers. Additionally, a manifold regularization with Laplacian graph is incorporated into EDA, such that it is beneficial to semi-supervised learning. Under the EDA, we also propose an extensive model learned with multiple views. Experiments on three visual data sets for video event recognition and object recognition, respectively, demonstrate that our EDA outperforms existing cross-domain learning methods. version:1
arxiv-1505-04373 | Evolutionary Cost-sensitive Extreme Learning Machine and Subspace Extension | http://arxiv.org/abs/1505.04373 | id:1505.04373 author:Lei Zhang, David Zhang category:cs.CV  published:2015-05-17 summary:Conventional extreme learning machines solve a Moore-Penrose generalized inverse of hidden layer activated matrix and analytically determine the output weights to achieve generalized performance, by assuming the same loss from different types of misclassification. The assumption may not hold in cost-sensitive recognition tasks, such as face recognition based access control system, where misclassifying a stranger as a family member and allowed to enter the house may result in more serious disaster than misclassifying a family member as a stranger and not allowed to enter. Though recent cost-sensitive learning can reduce the total loss with a given cost matrix that quantifies how severe one type of mistake against another type of mistake, in many realistic cases the cost matrix is unknown to users. Motivated by these concerns, this paper proposes an evolutionary cost-sensitive extreme learning machine (ECSELM), with the following merits: 1) to our best knowledge, it is the first proposal of cost-sensitive ELM; 2) it well addresses the open issue of how to define the cost matrix in cost-sensitive learning tasks; 3) an evolutionary backtracking search algorithm is induced for adaptive cost matrix optimization. Extensively, an ECSLDA method is generalized by coupling with cost-sensitive subspace learning. Experiments in a variety of cost-sensitive tasks well demonstrate the efficiency and effectiveness of the proposed approaches, specifically, 5%~10% improvements in classification are obtained on several datasets compared with ELMs; the computational efficiency is also 10 times faster than cost-sensitive subspace learning methods. version:1
arxiv-1505-04369 | Shrinkage degree in $L_2$-re-scale boosting for regression | http://arxiv.org/abs/1505.04369 | id:1505.04369 author:Lin Xu, Shaobo Lin, Yao Wang, Zongben Xu category:cs.LG  published:2015-05-17 summary:Re-scale boosting (RBoosting) is a variant of boosting which can essentially improve the generalization performance of boosting learning. The key feature of RBoosting lies in introducing a shrinkage degree to re-scale the ensemble estimate in each gradient-descent step. Thus, the shrinkage degree determines the performance of RBoosting. The aim of this paper is to develop a concrete analysis concerning how to determine the shrinkage degree in $L_2$-RBoosting. We propose two feasible ways to select the shrinkage degree. The first one is to parameterize the shrinkage degree and the other one is to develope a data-driven approach of it. After rigorously analyzing the importance of the shrinkage degree in $L_2$-RBoosting learning, we compare the pros and cons of the proposed methods. We find that although these approaches can reach the same learning rates, the structure of the final estimate of the parameterized approach is better, which sometimes yields a better generalization capability when the number of sample is finite. With this, we recommend to parameterize the shrinkage degree of $L_2$-RBoosting. To this end, we present an adaptive parameter-selection strategy for shrinkage degree and verify its feasibility through both theoretical analysis and numerical verification. The obtained results enhance the understanding of RBoosting and further give guidance on how to use $L_2$-RBoosting for regression tasks. version:1
arxiv-1505-04366 | Learning Deconvolution Network for Semantic Segmentation | http://arxiv.org/abs/1505.04366 | id:1505.04366 author:Hyeonwoo Noh, Seunghoon Hong, Bohyung Han category:cs.CV  published:2015-05-17 summary:We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained with no external data through ensemble with the fully convolutional network. version:1
arxiv-1505-04364 | Salient Structure Detection by Context-Guided Visual Search | http://arxiv.org/abs/1505.04364 | id:1505.04364 author:Kai-Fu Yang, Hui Li, Chao-Yi Li, Yong-Jie Li category:cs.CV  published:2015-05-17 summary:We define the task of salient structure (SS) detection to unify the saliency-related tasks like fixation prediction, salient object detection, and other detection of structures of interest. In this study, we propose a unified framework for SS detection by modeling the two-pathway-based guided search strategy of biological vision. Firstly, context-based spatial prior (CBSP) is extracted based on the layout of edges in the given scene along a fast visual pathway, called non-selective pathway. This is a rough and non-selective estimation of the locations where the potential SSs present. Secondly, another flow of local feature extraction is executed in parallel along the selective pathway. Finally, Bayesian inference is used to integrate local cues guided by CBSP, and to predict the exact locations of SSs in the input scene. The proposed model is invariant to size and features of objects. Experimental results on four datasets (two fixation prediction datasets and two salient object datasets) demonstrate that our system achieves competitive performance for SS detection (i.e., both the tasks of fixation prediction and salient object detection) comparing to the state-of-the-art methods. version:1
arxiv-1505-04363 | Local identifiability of $l_1$-minimization dictionary learning: a sufficient and almost necessary condition | http://arxiv.org/abs/1505.04363 | id:1505.04363 author:Siqi Wu, Bin Yu category:stat.ML  published:2015-05-17 summary:We study the theoretical properties of learning a dictionary from a set of $N$ signals $\mathbf x_i\in \mathbb R^K$ for $i=1,...,N$ via $l_1$-minimization. We assume that the signals $\mathbf x_i$'s are generated as $i.i.d.$ random linear combinations of the $K$ atoms from a complete reference dictionary $\mathbf D_0 \in \mathbb R^{K\times K}$. For the random linear coefficients, we consider two generative models: the $s$-sparse Gaussian model with $s = 1,..,K$, and the Bernoulli($p$)-Gaussian model with $p\in (0,1]$. First, for the population case and under each of the two generative models, we establish a sufficient and almost necessary condition for the reference dictionary $\mathbf D_0$ to be locally identifiable, i.e. a local minimum of the expected $l_1$-norm objective function. Our condition covers both the sparse and dense cases of signal generation, and significantly improves the sufficient condition by Gribonval and Schnass (2010). It fully describes the interaction between the collinearity of dictionary atoms $\mathbf M_0 = \mathbf D_0^T\mathbf D_0$, and the sparsity parameter $s$ or $p$ of the random coefficients in achieving local identifiability. We also provide sharp and easy-to-compute lower and upper bounds for the quantities involved in our conditions. With these bounds, we show that local identifiability is possible with sparsity level $s$ or $pK$ up to the order $O(\mu^{-2})$ for a complete $\mu$-coherent reference dictionary, i.e. a dictionary with maximum absolute collinearity $\mu = \max_{i\neq j} \mathbf M_0[i,j] $. Moreover, our local identifiability results also translate to the finite sample case with high probability provided that the number of signals $N$ scales as $O(K\log K)$. version:1
arxiv-1505-04357 | Evolving Spiking Networks with Variable Resistive Memories | http://arxiv.org/abs/1505.04357 | id:1505.04357 author:Gerard David Howard, Larry Bull, Ben de Lacy Costello, Andrew Adamatzky, Ella Gale category:cs.NE  published:2015-05-17 summary:Neuromorphic computing is a brainlike information processing paradigm that requires adaptive learning mechanisms. A spiking neuro-evolutionary system is used for this purpose; plastic resistive memories are implemented as synapses in spiking neural networks. The evolutionary design process exploits parameter self-adaptation and allows the topology and synaptic weights to be evolved for each network in an autonomous manner. Variable resistive memories are the focus of this research; each synapse has its own conductance profile which modifies the plastic behaviour of the device and may be altered during evolution. These variable resistive networks are evaluated on a noisy robotic dynamic-reward scenario against two static resistive memories and a system containing standard connections only. Results indicate that the extra behavioural degrees of freedom available to the networks incorporating variable resistive memories enable them to outperform the comparative synapse types. version:1
arxiv-1404-4997 | Tight bounds for learning a mixture of two gaussians | http://arxiv.org/abs/1404.4997 | id:1404.4997 author:Moritz Hardt, Eric Price category:cs.LG cs.DS stat.ML  published:2014-04-19 summary:We consider the problem of identifying the parameters of an unknown mixture of two arbitrary $d$-dimensional gaussians from a sequence of independent random samples. Our main results are upper and lower bounds giving a computationally efficient moment-based estimator with an optimal convergence rate, thus resolving a problem introduced by Pearson (1894). Denoting by $\sigma^2$ the variance of the unknown mixture, we prove that $\Theta(\sigma^{12})$ samples are necessary and sufficient to estimate each parameter up to constant additive error when $d=1.$ Our upper bound extends to arbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$ using a novel---yet simple---dimensionality reduction technique. We further identify several interesting special cases where the sample complexity is notably smaller than our optimal worst-case bound. For instance, if the means of the two components are separated by $\Omega(\sigma)$ the sample complexity reduces to $O(\sigma^2)$ and this is again optimal. Our results also apply to learning each component of the mixture up to small error in total variation distance, where our algorithm gives strong improvements in sample complexity over previous work. We also extend our lower bound to mixtures of $k$ Gaussians, showing that $\Omega(\sigma^{6k-2})$ samples are necessary to estimate each parameter up to constant additive error. version:3
arxiv-1501-07645 | Hyper-parameter optimization of Deep Convolutional Networks for object recognition | http://arxiv.org/abs/1501.07645 | id:1501.07645 author:Sachin S. Talathi category:cs.CV cs.LG  published:2015-01-30 summary:Recently sequential model based optimization (SMBO) has emerged as a promising hyper-parameter optimization strategy in machine learning. In this work, we investigate SMBO to identify architecture hyper-parameters of deep convolution networks (DCNs) object recognition. We propose a simple SMBO strategy that starts from a set of random initial DCN architectures to generate new architectures, which on training perform well on a given dataset. Using the proposed SMBO strategy we are able to identify a number of DCN architectures that produce results that are comparable to state-of-the-art results on object recognition benchmarks. version:2
arxiv-1505-04343 | Provably Correct Active Sampling Algorithms for Matrix Column Subset Selection with Missing Data | http://arxiv.org/abs/1505.04343 | id:1505.04343 author:Yining Wang, Aarti Singh category:stat.ML cs.LG  published:2015-05-17 summary:We consider the problem of matrix column subset selection, which selects a subset of columns from an input matrix such that the input can be well approximated by the span of the selected columns. Column subset selection has been applied to numerous real-world data applications such as population genetics summarization, electronic circuits testing and recommendation systems. In many applications the complete data matrix is unavailable and one needs to select representative columns by inspecting only a small portion of the input matrix. In this paper we propose the first provably correct column subset selection algorithms for partially observed data matrices. Our proposed algorithms exhibit different merits and drawbacks in terms of statistical accuracy, computational efficiency, sample complexity and sampling schemes, which provides a nice exploration of the tradeoff between these desired properties for column subset selection. The proposed methods employ the idea of feedback driven sampling and are inspired by several sampling schemes previously introduced for low-rank matrix approximation tasks [DMM08, FKV04, DV06, KS14]. Our analysis shows that two of the proposed algorithms enjoy a relative error bound, which is preferred for column subset selection and matrix approximation purposes. We also demonstrate through both theoretical and empirical analysis the power of feedback driven sampling compared to uniform random sampling on input matrices with highly correlated columns. version:1
arxiv-1302-3567 | Efficient Approximations for the Marginal Likelihood of Incomplete Data Given a Bayesian Network | http://arxiv.org/abs/1302.3567 | id:1302.3567 author:David Maxwell Chickering, David Heckerman category:cs.LG cs.AI stat.ML  published:2013-02-13 summary:We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that the CS measure is the most accurate. version:2
arxiv-1302-3580 | Asymptotic Model Selection for Directed Networks with Hidden Variables | http://arxiv.org/abs/1302.3580 | id:1302.3580 author:Dan Geiger, David Heckerman, Christopher Meek category:cs.LG cs.AI stat.ML  published:2013-02-13 summary:We extend the Bayesian Information Criterion (BIC), an asymptotic approximation for the marginal likelihood, to Bayesian networks with hidden variables. This approximation can be used to select models given large samples of data. The standard BIC as well as our extension punishes the complexity of a model according to the dimension of its parameters. We argue that the dimension of a Bayesian network with hidden variables is the rank of the Jacobian matrix of the transformation between the parameters of the network and the parameters of the observable variables. We compute the dimensions of several networks including the naive Bayes model with a hidden root node. version:2
arxiv-1302-1561 | Structure and Parameter Learning for Causal Independence and Causal Interaction Models | http://arxiv.org/abs/1302.1561 | id:1302.1561 author:Christopher Meek, David Heckerman category:cs.AI cs.LG  published:2013-02-06 summary:This paper discusses causal independence models and a generalization of these models called causal interaction models. Causal interaction models are models that have independent mechanisms where a mechanism can have several causes. In addition to introducing several particular types of causal interaction models, we show how we can apply the Bayesian approach to learning causal interaction models obtaining approximate posterior distributions for the models and obtain MAP and ML estimates for the parameters. We illustrate the approach with a simulation study of learning model posteriors. version:2
arxiv-1302-1528 | A Bayesian Approach to Learning Bayesian Networks with Local Structure | http://arxiv.org/abs/1302.1528 | id:1302.1528 author:David Maxwell Chickering, David Heckerman, Christopher Meek category:cs.LG cs.AI stat.ML  published:2013-02-06 summary:Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability that is, the Bayesian score of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function. version:2
arxiv-1301-7415 | Learning Mixtures of DAG Models | http://arxiv.org/abs/1301.7415 | id:1301.7415 author:Bo Thiesson, Christopher Meek, David Maxwell Chickering, David Heckerman category:cs.LG cs.AI stat.ML  published:2013-01-30 summary:We describe computationally efficient methods for learning mixtures in which each component is a directed acyclic graphical model (mixtures of DAGs or MDAGs). We argue that simple search-and-score algorithms are infeasible for a variety of problems, and introduce a feasible approach in which parameter and structure search is interleaved and expected data is treated as real data. Our approach can be viewed as a combination of (1) the Cheeseman--Stutz asymptotic approximation for model posterior probability and (2) the Expectation--Maximization algorithm. We evaluate our procedure for selecting among MDAGs on synthetic and real examples. version:2
arxiv-1301-7401 | An Experimental Comparison of Several Clustering and Initialization Methods | http://arxiv.org/abs/1301.7401 | id:1301.7401 author:Marina Meila, David Heckerman category:cs.LG stat.ML  published:2013-01-30 summary:We examine methods for clustering in high dimensions. In the first part of the paper, we perform an experimental comparison between three batch clustering algorithms: the Expectation-Maximization (EM) algorithm, a winner take all version of the EM algorithm reminiscent of the K-means algorithm, and model-based hierarchical agglomerative clustering. We learn naive-Bayes models with a hidden root node, using high-dimensional discrete-variable data sets (both real and synthetic). We find that the EM algorithm significantly outperforms the other methods, and proceed to investigate the effect of various initialization schemes on the final solution produced by the EM algorithm. The initializations that we consider are (1) parameters sampled from an uninformative prior, (2) random perturbations of the marginal distribution of the data, and (3) the output of hierarchical agglomerative clustering. Although the methods are substantially different, they lead to learned models that are strikingly similar in quality. version:2
arxiv-1301-7382 | Inferring Informational Goals from Free-Text Queries: A Bayesian Approach | http://arxiv.org/abs/1301.7382 | id:1301.7382 author:David Heckerman, Eric J. Horvitz category:cs.IR cs.AI cs.CL  published:2013-01-30 summary:People using consumer software applications typically do not use technical jargon when querying an online database of help topics. Rather, they attempt to communicate their goals with common words and phrases that describe software functionality in terms of structure and objects they understand. We describe a Bayesian approach to modeling the relationship between words in a user's query for assistance and the informational goals of the user. After reviewing the general method, we describe several extensions that center on integrating additional distinctions and structure about language usage and user goals into the Bayesian models. version:2
arxiv-1301-6685 | Fast Learning from Sparse Data | http://arxiv.org/abs/1301.6685 | id:1301.6685 author:David Maxwell Chickering, David Heckerman category:cs.LG stat.ML  published:2013-01-23 summary:We describe two techniques that significantly improve the running time of several standard machine-learning algorithms when data is sparse. The first technique is an algorithm that effeciently extracts one-way and two-way counts--either real or expected-- from discrete data. Extracting such counts is a fundamental step in learning algorithms for constructing a variety of models including decision trees, decision graphs, Bayesian networks, and naive-Bayes clustering models. The second technique is an algorithm that efficiently performs the E-step of the EM algorithm (i.e. inference) when applied to a naive-Bayes clustering model. Using real-world data sets, we demonstrate a dramatic decrease in running time for algorithms that incorporate these techniques. version:2
arxiv-1301-6697 | Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions | http://arxiv.org/abs/1301.6697 | id:1301.6697 author:Dan Geiger, David Heckerman category:cs.LG stat.ML  published:2013-01-23 summary:We show that the only parameter prior for complete Gaussian DAG models that satisfies global parameter independence, complete model equivalence, and some weak regularity assumptions, is the normal-Wishart distribution. Our analysis is based on the following new characterization of the Wishart distribution: let W be an n x n, n >= 3, positive-definite symmetric matrix of random variables and f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only if W_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for every block partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similar characterizations of the normal and normal-Wishart distributions are provided as well. We also show how to construct a prior for every DAG model over X from the prior of a single regression model. version:2
arxiv-1301-0600 | An MDP-based Recommender System | http://arxiv.org/abs/1301.0600 | id:1301.0600 author:Guy Shani, Ronen I. Brafman, David Heckerman category:cs.LG cs.AI cs.IR  published:2012-12-12 summary:Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system. version:2
arxiv-1206-3298 | Continuous Time Dynamic Topic Models | http://arxiv.org/abs/1206.3298 | id:1206.3298 author:Chong Wang, David Blei, David Heckerman category:cs.IR cs.LG stat.ML  published:2012-06-13 summary:In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a "topic" is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction. version:2
arxiv-0911-0054 | Learning Exponential Families in High-Dimensions: Strong Convexity and Sparsity | http://arxiv.org/abs/0911.0054 | id:0911.0054 author:Sham M. Kakade, Ohad Shamir, Karthik Sridharan, Ambuj Tewari category:cs.LG stat.ML I.2.6  published:2009-10-31 summary:The versatility of exponential families, along with their attendant convexity properties, make them a popular and effective statistical model. A central issue is learning these models in high-dimensions, such as when there is some sparsity pattern of the optimal parameter. This work characterizes a certain strong convexity property of general exponential families, which allow their generalization ability to be quantified. In particular, we show how this property can be used to analyze generic exponential families under L_1 regularization. version:2
arxiv-1505-04313 | A type-theoretical approach to Universal Grammar | http://arxiv.org/abs/1505.04313 | id:1505.04313 author:Erkki Luuk category:cs.CL math.LO 03  published:2015-05-16 summary:The idea of Universal Grammar (UG) as the hypothetical linguistic structure shared by all human languages harkens back at least to the 13th century. The best known modern elaborations of the idea are due to Chomsky. Following a devastating critique from theoretical, typological and field linguistics, these elaborations, the idea of UG itself and the more general idea of language universals stand untenable and are largely abandoned. The proposal tackles the hypothetical contents of UG using dependent and polymorphic type theory in a framework very different from the Chomskyan ones. We introduce a type logic for a precise, universal and parsimonious representation of natural language morphosyntax and compositional semantics. The logic handles grammatical ambiguity (with polymorphic types), selectional restrictions and diverse kinds of anaphora (with dependent types), and features a partly universal set of morphosyntactic types (by the Curry-Howard isomorphism). version:1
arxiv-1505-04286 | Robust Real-time Extraction of Fiducial Facial Feature Points using Haar-like Features | http://arxiv.org/abs/1505.04286 | id:1505.04286 author:Harry Commin category:cs.CV  published:2015-05-16 summary:In this paper, we explore methods of robustly extracting fiducial facial feature points - an important process for numerous facial image processing tasks. We consider various methods to first detect face, then facial features and finally salient facial feature points. Colour-based models are analysed and their overall unsuitability for this task is summarised. The bulk of the report is then dedicated to proposing a learning-based method centred on the Viola-Jones algorithm. The specific difficulties and considerations relating to feature point detection are laid out in this context and a novel approach is established to address these issues. On a sequence of clear and unobstructed face images, our proposed system achieves average detection rates of over 90%. Then, using a more varied sample dataset, we identify some possible areas for future development of our system. version:1
arxiv-1505-04260 | The color of smiling: computational synaesthesia of facial expressions | http://arxiv.org/abs/1505.04260 | id:1505.04260 author:Vittorio Cuculo, Raffaella Lanzarotti, Giuseppe Boccignone category:cs.CV  published:2015-05-16 summary:This note gives a preliminary account of the transcoding or rechanneling problem between different stimuli as it is of interest for the natural interaction or affective computing fields. By the consideration of a simple example, namely the color response of an affective lamp to a sensed facial expression, we frame the problem within an information- theoretic perspective. A full justification in terms of the Information Bottleneck principle promotes a latent affective space, hitherto surmised as an appealing and intuitive solution, as a suitable mediator between the different stimuli. version:1
arxiv-1505-04243 | A New Perspective on Boosting in Linear Regression via Subgradient Optimization and Relatives | http://arxiv.org/abs/1505.04243 | id:1505.04243 author:Robert M. Freund, Paul Grigas, Rahul Mazumder category:math.ST cs.LG math.OC stat.ML stat.TH 62J05  62J07  90C25  published:2015-05-16 summary:In this paper we analyze boosting algorithms in linear regression from a new perspective: that of modern first-order methods in convex optimization. We show that classic boosting algorithms in linear regression, namely the incremental forward stagewise algorithm (FS$_\varepsilon$) and least squares boosting (LS-Boost($\varepsilon$)), can be viewed as subgradient descent to minimize the loss function defined as the maximum absolute correlation between the features and residuals. We also propose a modification of FS$_\varepsilon$ that yields an algorithm for the Lasso, and that may be easily extended to an algorithm that computes the Lasso path for different values of the regularization parameter. Furthermore, we show that these new algorithms for the Lasso may also be interpreted as the same master algorithm (subgradient descent), applied to a regularized version of the maximum absolute correlation loss function. We derive novel, comprehensive computational guarantees for several boosting algorithms in linear regression (including LS-Boost($\varepsilon$) and FS$_\varepsilon$) by using techniques of modern first-order methods in convex optimization. Our computational guarantees inform us about the statistical properties of boosting algorithms. In particular they provide, for the first time, a precise theoretical description of the amount of data-fidelity and regularization imparted by running a boosting algorithm with a prespecified learning rate for a fixed but arbitrary number of iterations, for any dataset. version:1
arxiv-1412-7725 | Automatic Photo Adjustment Using Deep Neural Networks | http://arxiv.org/abs/1412.7725 | id:1412.7725 author:Zhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris, Yizhou Yu category:cs.CV cs.GR cs.LG  published:2014-12-24 summary:Photo retouching enables photographers to invoke dramatic visual impressions by artistically enhancing their photos through stylistic color and tone adjustments. However, it is also a time-consuming and challenging task that requires advanced skills beyond the abilities of casual photographers. Using an automated algorithm is an appealing alternative to manual work but such an algorithm faces many hurdles. Many photographic styles rely on subtle adjustments that depend on the image content and even its semantics. Further, these adjustments are often spatially varying. Because of these characteristics, existing automatic algorithms are still limited and cover only a subset of these challenges. Recently, deep machine learning has shown unique abilities to address hard problems that resisted machine algorithms for long. This motivated us to explore the use of deep learning in the context of photo editing. In this paper, we explain how to formulate the automatic photo adjustment problem in a way suitable for this approach. We also introduce an image descriptor that accounts for the local semantics of an image. Our experiments demonstrate that our deep learning formulation applied using these descriptors successfully capture sophisticated photographic styles. In particular and unlike previous techniques, it can model local adjustments that depend on the image semantics. We show on several examples that this yields results that are qualitatively and quantitatively better than previous work. version:2
arxiv-1410-0736 | HD-CNN: Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition | http://arxiv.org/abs/1410.0736 | id:1410.0736 author:Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste, Wei Di, Yizhou Yu category:cs.CV cs.LG cs.NE  published:2014-10-03 summary:In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65%, 3.1% and 1.1%, respectively. version:4
arxiv-1505-04215 | An Analysis of Active Learning With Uniform Feature Noise | http://arxiv.org/abs/1505.04215 | id:1505.04215 author:Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman category:stat.ML cs.AI cs.LG math.ST stat.TH  published:2015-05-15 summary:In active learning, the user sequentially chooses values for feature $X$ and an oracle returns the corresponding label $Y$. In this paper, we consider the effect of feature noise in active learning, which could arise either because $X$ itself is being measured, or it is corrupted in transmission to the oracle, or the oracle returns the label of a noisy version of the query point. In statistics, feature noise is known as "errors in variables" and has been studied extensively in non-active settings. However, the effect of feature noise in active learning has not been studied before. We consider the well-known Berkson errors-in-variables model with additive uniform noise of width $\sigma$. Our simple but revealing setting is that of one-dimensional binary classification setting where the goal is to learn a threshold (point where the probability of a $+$ label crosses half). We deal with regression functions that are antisymmetric in a region of size $\sigma$ around the threshold and also satisfy Tsybakov's margin condition around the threshold. We prove minimax lower and upper bounds which demonstrate that when $\sigma$ is smaller than the minimiax active/passive noiseless error derived in \cite{CN07}, then noise has no effect on the rates and one achieves the same noiseless rates. For larger $\sigma$, the \textit{unflattening} of the regression function on convolution with uniform noise, along with its local antisymmetry around the threshold, together yield a behaviour where noise \textit{appears} to be beneficial. Our key result is that active learning can buy significant improvement over a passive strategy even in the presence of feature noise. version:1
arxiv-1505-04214 | Algorithmic Connections Between Active Learning and Stochastic Convex Optimization | http://arxiv.org/abs/1505.04214 | id:1505.04214 author:Aaditya Ramdas, Aarti Singh category:cs.LG cs.AI math.OC stat.ML  published:2015-05-15 summary:Interesting theoretical associations have been established by recent papers between the fields of active learning and stochastic convex optimization due to the common role of feedback in sequential querying mechanisms. In this paper, we continue this thread in two parts by exploiting these relations for the first time to yield novel algorithms in both fields, further motivating the study of their intersection. First, inspired by a recent optimization algorithm that was adaptive to unknown uniform convexity parameters, we present a new active learning algorithm for one-dimensional thresholds that can yield minimax rates by adapting to unknown noise parameters. Next, we show that one can perform $d$-dimensional stochastic minimization of smooth uniformly convex functions when only granted oracle access to noisy gradient signs along any coordinate instead of real-valued gradients, by using a simple randomized coordinate descent procedure where each line search can be solved by $1$-dimensional active learning, provably achieving the same error convergence rate as having the entire real-valued gradient. Combining these two parts yields an algorithm that solves stochastic convex optimization of uniformly convex and smooth functions using only noisy gradient signs by repeatedly performing active learning, achieves optimal rates and is adaptive to all unknown convexity and smoothness parameters. version:1
arxiv-1505-04197 | Arabic Inquiry-Answer Dialogue Acts Annotation Schema | http://arxiv.org/abs/1505.04197 | id:1505.04197 author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL  published:2015-05-15 summary:We present an annotation schema as part of an effort to create a manually annotated corpus for Arabic dialogue language understanding including spoken dialogue and written "chat" dialogue for inquiry-answer domain. The proposed schema handles mainly the request and response acts that occurs frequently in inquiry-answer debate conversations expressing request services, suggests, and offers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues. version:1
arxiv-1412-5675 | Stabilizing Value Iteration with and without Approximation Errors | http://arxiv.org/abs/1412.5675 | id:1412.5675 author:Ali Heydari category:cs.SY math.OC stat.ML  published:2014-12-17 summary:Adaptive optimal control using value iteration (VI) initiated from a stabilizing policy is theoretically analyzed in various aspects including the continuity of the result, the stability of the system operated using any single/constant resulting control policy, the stability of the system operated using the evolving/time-varying control policy, the convergence of the algorithm, and the optimality of the limit function. Afterwards, the effect of presence of approximation errors in the involved function approximation processes is incorporated and another set of results for boundedness of the approximate VI as well as stability of the system operated under the results for both cases of applying a single policy or an evolving policy are derived. A feature of the presented results is providing estimations of the region of attraction so that if the initial condition is within the region, the whole trajectory will remain inside it and hence, the function approximation results will be reliable. version:2
arxiv-1412-6095 | Theoretical and Numerical Analysis of Approximate Dynamic Programming with Approximation Errors | http://arxiv.org/abs/1412.6095 | id:1412.6095 author:Ali Heydari category:cs.SY cs.LG math.OC stat.ML  published:2014-12-18 summary:This study is aimed at answering the famous question of how the approximation errors at each iteration of Approximate Dynamic Programming (ADP) affect the quality of the final results considering the fact that errors at each iteration affect the next iteration. To this goal, convergence of Value Iteration scheme of ADP for deterministic nonlinear optimal control problems with undiscounted cost functions is investigated while considering the errors existing in approximating respective functions. The boundedness of the results around the optimal solution is obtained based on quantities which are known in a general optimal control problem and assumptions which are verifiable. Moreover, since the presence of the approximation errors leads to the deviation of the results from optimality, sufficient conditions for stability of the system operated by the result obtained after a finite number of value iterations, along with an estimation of its region of attraction, are derived in terms of a calculable upper bound of the control approximation error. Finally, the process of implementation of the method on an orbital maneuver problem is investigated through which the assumptions made in the theoretical developments are verified and the sufficient conditions are applied for guaranteeing stability and near optimality. version:3
arxiv-1505-04150 | Reinforcement Learning applied to Single Neuron | http://arxiv.org/abs/1505.04150 | id:1505.04150 author:Zhipeng Wang, Mingbo Cai category:cs.AI cs.NE  published:2015-05-15 summary:This paper extends the reinforcement learning ideas into the multi-agents system, which is far more complicated than the previously studied single-agent system. We studied two different multi-agents systems. One is the fully-connected neural network consists of multiple single neurons. Another one is the simplified mechanical arm system which is controlled by multiple neurons. We suppose that each neuron is like an agent and it can do Gibbs sampling of the posterior probability of stimulus features. The policy is optimized in a way that the cumulative global rewards are maximized. The algorithm for the second system is based on the same idea but we incorporate the physics model into the constraints. The simulation results show that for the first system our algorithm converges well. For the second system it does not converge well in a reasonable simulation time length. In summary, we took the initial endeavor to study the reinforcement learning for multi-agents system. The computational complexity is always an issue and significant amount of works have to be done in order to better understand the problem. version:1
arxiv-1505-04143 | Dense Semantic Correspondence where Every Pixel is a Classifier | http://arxiv.org/abs/1505.04143 | id:1505.04143 author:Hilton Bristow, Jack Valmadre, Simon Lucey category:cs.CV  published:2015-05-15 summary:Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms. Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ. Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and (ii) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train. We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment. version:1
arxiv-1505-04137 | Consistent Algorithms for Multiclass Classification with a Reject Option | http://arxiv.org/abs/1505.04137 | id:1505.04137 author:Harish G. Ramaswamy, Ambuj Tewari, Shivani Agarwal category:cs.LG stat.ML  published:2015-05-15 summary:We consider the problem of $n$-class classification ($n\geq 2$), where the classifier can choose to abstain from making predictions at a given cost, say, a factor $\alpha$ of the cost of misclassification. Designing consistent algorithms for such $n$-class classification problems with a `reject option' is the main goal of this paper, thereby extending and generalizing previously known results for $n=2$. We show that the Crammer-Singer surrogate and the one vs all hinge loss, albeit with a different predictor than the standard argmax, yield consistent algorithms for this problem when $\alpha=\frac{1}{2}$. More interestingly, we design a new convex surrogate that is also consistent for this problem when $\alpha=\frac{1}{2}$ and operates on a much lower dimensional space ($\log(n)$ as opposed to $n$). We also generalize all three surrogates to be consistent for any $\alpha\in[0, \frac{1}{2}]$. version:1
arxiv-1504-02756 | Discrimination and characterization of Parkinsonian rest tremors by analyzing long-term correlations and multifractal signatures | http://arxiv.org/abs/1504.02756 | id:1504.02756 author:Lorenzo Livi, Alireza Sadeghian, Hamid Sadeghian category:physics.med-ph cs.CV physics.data-an  published:2015-04-10 summary:In this paper, we analyze 48 signals of rest tremor velocity related to 12 distinct subjects affected by Parkinson's disease. The subjects belong to two different groups, formed by four and eight subjects with, respectively, high- and low-amplitude rest tremors. Each subject is tested in four settings, given by combining the use of deep brain stimulation and L-DOPA medication. We develop two main feature-based representations of such signals, which are obtained by considering (i) the long-term correlations and multifractal properties, and (ii) the power spectra. The feature-based representations are initially utilized for the purpose of characterizing the subjects under different settings. In agreement with previous studies, we show that deep brain stimulation does not significantly characterize neither of the two groups, regardless of the adopted representation. On the other hand, the medication effect yields statistically significant differences in both high- and low-amplitude tremor groups. We successively test several different instances of the two feature-based representations of the signals in the setting of supervised classification and (nonlinear) feature transformation. We consider three different classification problems, involving the recognition of (i) the presence of medication, (ii) the use of deep brain stimulation, and (iii) the membership to the high- and low-amplitude tremor groups. Classification results show that the use of medication can be discriminated with higher accuracy, considering many of the feature-based representations. Notably, we show that the best results are obtained with a parsimonious, two-dimensional representation encoding the long-term correlations and multifractal character of the signals. version:2
arxiv-1505-04123 | Margins, Kernels and Non-linear Smoothed Perceptrons | http://arxiv.org/abs/1505.04123 | id:1505.04123 author:Aaditya Ramdas, Javier Peña category:cs.LG cs.AI cs.NA math.OC  published:2015-05-15 summary:We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes - the Perceptron (primal) and Von-Neumann (dual) algorithms. We cast our problem as one of maximizing the regularized normalized hard-margin ($\rho$) in an RKHS and %use the Representer Theorem to rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel's (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of $\tfrac{\sqrt {\log n}}{\rho}$ given $n$ separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is $\tfrac1{\rho^2}$. When no such classifier exists, we prove a version of Gordan's separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in $\min\{\tfrac{\sqrt n}{ \rho }, \tfrac{\sqrt n}{\epsilon}\}$ iterations with a perfect separator in the RKHS if the primal is feasible or a dual $\epsilon$-certificate of near-infeasibility. version:1
arxiv-1505-04117 | Discovering Attribute Shades of Meaning with the Crowd | http://arxiv.org/abs/1505.04117 | id:1505.04117 author:Adriana Kovashka, Kristen Grauman category:cs.CV  published:2015-05-15 summary:To learn semantic attributes, existing methods typically train one discriminative model for each word in a vocabulary of nameable properties. However, this "one model per word" assumption is problematic: while a word might have a precise linguistic definition, it need not have a precise visual definition. We propose to discover shades of attribute meaning. Given an attribute name, we use crowdsourced image labels to discover the latent factors underlying how different annotators perceive the named concept. We show that structure in those latent factors helps reveal shades, that is, interpretations for the attribute shared by some group of annotators. Using these shades, we train classifiers to capture the primary (often subtle) variants of the attribute. The resulting models are both semantic and visually precise. By catering to users' interpretations, they improve attribute prediction accuracy on novel images. Shades also enable more successful attribute-based image search, by providing robust personalized models for retrieving multi-attribute query results. They are widely applicable to tasks that involve describing visual content, such as zero-shot category learning and organization of photo collections. version:1
arxiv-1505-04097 | MCODE: Multivariate Conditional Outlier Detection | http://arxiv.org/abs/1505.04097 | id:1505.04097 author:Charmgil Hong, Milos Hauskrecht category:cs.AI cs.LG stat.ML  published:2015-05-15 summary:Outlier detection aims to identify unusual data instances that deviate from expected patterns. The outlier detection is particularly challenging when outliers are context dependent and when they are defined by unusual combinations of multiple outcome variable values. In this paper, we develop and study a new conditional outlier detection approach for multivariate outcome spaces that works by (1) transforming the conditional detection to the outlier detection problem in a new (unconditional) space and (2) defining outlier scores by analyzing the data in the new space. Our approach relies on the classifier chain decomposition of the multi-dimensional classification problem that lets us transform the output space into a probability vector, one probability for each dimension of the output space. Outlier scores applied to these transformed vectors are then used to detect the outliers. Experiments on multiple multi-dimensional classification problems with the different outlier injection rates show that our methodology is robust and able to successfully identify outliers when outliers are either sparse (manifested in one or very few dimensions) or dense (affecting multiple dimensions). version:1
arxiv-1505-04085 | Optimal Low-Rank Tensor Recovery from Separable Measurements: Four Contractions Suffice | http://arxiv.org/abs/1505.04085 | id:1505.04085 author:Parikshit Shah, Nikhil Rao, Gongguo Tang category:stat.ML cs.IT cs.LG math.IT math.OC  published:2015-05-15 summary:Tensors play a central role in many modern machine learning and signal processing applications. In such applications, the target tensor is usually of low rank, i.e., can be expressed as a sum of a small number of rank one tensors. This motivates us to consider the problem of low rank tensor recovery from a class of linear measurements called separable measurements. As specific examples, we focus on two distinct types of separable measurement mechanisms (a) Random projections, where each measurement corresponds to an inner product of the tensor with a suitable random tensor, and (b) the completion problem where measurements constitute revelation of a random set of entries. We present a computationally efficient algorithm, with rigorous and order-optimal sample complexity results (upto logarithmic factors) for tensor recovery. Our method is based on reduction to matrix completion sub-problems and adaptation of Leurgans' method for tensor decomposition. We extend the methodology and sample complexity results to higher order tensors, and experimentally validate our theoretical results. version:1
arxiv-1410-3351 | Coarse Ricci curvature with applications to manifold learning problem | http://arxiv.org/abs/1410.3351 | id:1410.3351 author:Antonio G. Ache, Micah W. Warren category:math.DG cs.LG math.MG stat.ML 53  published:2014-10-13 summary:Consider a sample of $n$ points taken i.i.d from a submanifold of Euclidean space. This defines a metric measure space. We show that there is an explicit set of scales $t_{n}\rightarrow0$ such that a coarse Ricci curvature at scale $t_{n}$ on this metric measure space converges almost surely to the coarse Ricci curvature of the underlying manifold. version:2
arxiv-1505-04073 | Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices | http://arxiv.org/abs/1505.04073 | id:1505.04073 author:Jie Wang, Jieping Ye category:cs.LG  published:2015-05-15 summary:Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule---that is based on the dual projection onto convex sets (DPC)---to quickly identify the inactive features---that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features---especially for high dimensional data---which leads to a speedup up to several orders of magnitude. version:1
arxiv-1505-04058 | A Real Time Facial Expression Classification System Using Local Binary Patterns | http://arxiv.org/abs/1505.04058 | id:1505.04058 author:S. L. Happy, Anjith George, Aurobinda Routray category:cs.CV  published:2015-05-15 summary:Facial expression analysis is one of the popular fields of research in human computer interaction (HCI). It has several applications in next generation user interfaces, human emotion analysis, behavior and cognitive modeling. In this paper, a facial expression classification algorithm is proposed which uses Haar classifier for face detection purpose, Local Binary Patterns (LBP) histogram of different block sizes of a face image as feature vectors and classifies various facial expressions using Principal Component Analysis (PCA). The algorithm is implemented in real time for expression classification since the computational complexity of the algorithm is small. A customizable approach is proposed for facial expression analysis, since the various expressions and intensity of expressions vary from person to person. The system uses grayscale frontal face images of a person to classify six basic emotions namely happiness, sadness, disgust, fear, surprise and anger. version:1
arxiv-1505-04055 | A Video Database of Human Faces under Near Infra-Red Illumination for Human Computer Interaction Aplications | http://arxiv.org/abs/1505.04055 | id:1505.04055 author:S L Happy, Anirban Dasgupta, Anjith George, Aurobinda Routray category:cs.CV  published:2015-05-15 summary:Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting. version:1
arxiv-1505-04030 | Robust Facial Expression Classification Using Shape and Appearance Features | http://arxiv.org/abs/1505.04030 | id:1505.04030 author:S. L. Happy, Aurobinda Routray category:cs.CV  published:2015-05-15 summary:Facial expression recognition has many potential applications which has attracted the attention of researchers in the last decade. Feature extraction is one important step in expression analysis which contributes toward fast and accurate expression recognition. This paper represents an approach of combining the shape and appearance features to form a hybrid feature vector. We have extracted Pyramid of Histogram of Gradients (PHOG) as shape descriptors and Local Binary Patterns (LBP) as appearance features. The proposed framework involves a novel approach of extracting hybrid features from active facial patches. The active facial patches are located on the face regions which undergo a major change during different expressions. After detection of facial landmarks, the active patches are localized and hybrid features are calculated from these patches. The use of small parts of face instead of the whole face for extracting features reduces the computational cost and prevents the over-fitting of the features for classification. By using linear discriminant analysis, the dimensionality of the feature is reduced which is further classified by using the support vector machine (SVM). The experimental results on two publicly available databases show promising accuracy in recognizing all expression classes. version:1
arxiv-1505-04026 | Automatic Facial Expression Recognition Using Features of Salient Facial Patches | http://arxiv.org/abs/1505.04026 | id:1505.04026 author:S L Happy, Aurobinda Routray category:cs.CV  published:2015-05-15 summary:Extraction of discriminative features from salient facial patches plays a vital role in effective facial expression recognition. The accurate detection of facial landmarks improves the localization of the salient patches on face images. This paper proposes a novel framework for expression recognition by using appearance features of selected facial patches. A few prominent facial patches, depending on the position of facial landmarks, are extracted which are active during emotion elicitation. These active patches are further processed to obtain the salient patches which contain discriminative features for classification of each pair of expressions, thereby selecting different facial patches as salient for different pair of expression classes. One-against-one classification method is adopted using these features. In addition, an automated learning-free facial landmark detection technique has been proposed, which achieves similar performances as that of other state-of-art landmark detection methods, yet requires significantly less execution time. The proposed method is found to perform well consistently in different resolutions, hence, providing a solution for expression recognition in low resolution images. Experiments on CK+ and JAFFE facial expression databases show the effectiveness of the proposed system. version:1
arxiv-1505-03358 | An Image is Worth More than a Thousand Favorites: Surfacing the Hidden Beauty of Flickr Pictures | http://arxiv.org/abs/1505.03358 | id:1505.03358 author:Rossano Schifanella, Miriam Redi, Luca Aiello category:cs.SI cs.CV cs.CY cs.MM  published:2015-05-13 summary:The dynamics of attention in social media tend to obey power laws. Attention concentrates on a relatively small number of popular items and neglecting the vast majority of content produced by the crowd. Although popularity can be an indication of the perceived value of an item within its community, previous research has hinted to the fact that popularity is distinct from intrinsic quality. As a result, content with low visibility but high quality lurks in the tail of the popularity distribution. This phenomenon can be particularly evident in the case of photo-sharing communities, where valuable photographers who are not highly engaged in online social interactions contribute with high-quality pictures that remain unseen. We propose to use a computer vision method to surface beautiful pictures from the immense pool of near-zero-popularity items, and we test it on a large dataset of creative-commons photos on Flickr. By gathering a large crowdsourced ground truth of aesthetics scores for Flickr images, we show that our method retrieves photos whose median perceived beauty score is equal to the most popular ones, and whose average is lower by only 1.5%. version:2
arxiv-1308-6069 | Compound Poisson Processes, Latent Shrinkage Priors and Bayesian Nonconvex Penalization | http://arxiv.org/abs/1308.6069 | id:1308.6069 author:Zhihua Zhang, Jin Li category:stat.ML stat.ME  published:2013-08-28 summary:In this paper we discuss Bayesian nonconvex penalization for sparse learning problems. We explore a nonparametric formulation for latent shrinkage parameters using subordinators which are one-dimensional L\'{e}vy processes. We particularly study a family of continuous compound Poisson subordinators and a family of discrete compound Poisson subordinators. We exemplify four specific subordinators: Gamma, Poisson, negative binomial and squared Bessel subordinators. The Laplace exponents of the subordinators are Bernstein functions, so they can be used as sparsity-inducing nonconvex penalty functions. We exploit these subordinators in regression problems, yielding a hierarchical model with multiple regularization parameters. We devise ECME (Expectation/Conditional Maximization Either) algorithms to simultaneously estimate regression coefficients and regularization parameters. The empirical evaluation of simulated data shows that our approach is feasible and effective in high-dimensional data analysis. version:3
arxiv-1303-4805 | Ensembling classification models based on phalanxes of variables with applications in drug discovery | http://arxiv.org/abs/1303.4805 | id:1303.4805 author:Jabed H. Tomal, William J. Welch, Ruben H. Zamar category:stat.ML stat.CO  published:2013-03-20 summary:Statistical detection of a rare class of objects in a two-class classification problem can pose several challenges. Because the class of interest is rare in the training data, there is relatively little information in the known class response labels for model building. At the same time the available explanatory variables are often moderately high dimensional. In the four assays of our drug-discovery application, compounds are active or not against a specific biological target, such as lung cancer tumor cells, and active compounds are rare. Several sets of chemical descriptor variables from computational chemistry are available to classify the active versus inactive class; each can have up to thousands of variables characterizing molecular structure of the compounds. The statistical challenge is to make use of the richness of the explanatory variables in the presence of scant response information. Our algorithm divides the explanatory variables into subsets adaptively and passes each subset to a base classifier. The various base classifiers are then ensembled to produce one model to rank new objects by their estimated probabilities of belonging to the rare class of interest. The essence of the algorithm is to choose the subsets such that variables in the same group work well together; we call such groups phalanxes. version:4
arxiv-1505-03932 | Using Ensemble Models in the Histological Examination of Tissue Abnormalities | http://arxiv.org/abs/1505.03932 | id:1505.03932 author:Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin category:cs.CV cs.CE cs.LG H.2.8; I.5.3; J.3  published:2015-05-15 summary:Classification models for the automatic detection of abnormalities on histological samples do exists, with an active debate on the cost associated with false negative diagnosis (underdiagnosis) and false positive diagnosis (overdiagnosis). Current models tend to underdiagnose, failing to recognize a potentially fatal disease. The objective of this study is to investigate the possibility of automatically identifying abnormalities in tissue samples through the use of an ensemble model on data generated by histological examination and to minimize the number of false negative cases. version:1
arxiv-1602-07264 | A Multivariate Biomarker for Parkinson's Disease | http://arxiv.org/abs/1602.07264 | id:1602.07264 author:Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum, Tamba Lamin category:cs.LG H.2.8; I.5.3; J.3  published:2015-05-15 summary:In this study, we executed a genomic analysis with the objective of selecting a set of genes (possibly small) that would help in the detection and classification of samples from patients affected by Parkinson Disease. We performed a complete data analysis and during the exploratory phase, we selected a list of differentially expressed genes. Despite their association with the diseased state, we could not use them as a biomarker tool. Therefore, our research was extended to include a multivariate analysis approach resulting in the identification and selection of a group of 20 genes that showed a clear potential in detecting and correctly classify Parkinson Disease samples even in the presence of other neurodegenerative disorders. version:1
arxiv-1505-03917 | General Riemannian SOM | http://arxiv.org/abs/1505.03917 | id:1505.03917 author:Jascha A. Schewtschenko category:cs.NE  published:2015-05-14 summary:Kohonen's Self-Organizing Maps (SOMs) have proven to be a successful data-reduction method to identify the intrinsic lower-dimensional sub-manifold of a data set that is scattered in the higher-dimensional feature space. Motivated by the possibly non-Euclidian nature of the feature space and of the intrinsic geometry of the data set, we extend the definition of classic SOMs to obtain the General Riemannian SOM (GRiSOM). We additionally provide an implementation as a proof-of-concept for geometries with constant curvature. We furthermore perform the analytic and numerical analysis of the stability limits of certain (GRi)SOM configurations covering the different possible regular tessellation of the map space in each geometry. A deviation between the numerical and analytic stability limit has been observed for the square and hexagonal Euclidean maps for very small neighbourhoods in the map space as well as agreement in case of longer-ranged relations between the map nodes. version:1
arxiv-1505-03906 | Training generative neural networks via Maximum Mean Discrepancy optimization | http://arxiv.org/abs/1505.03906 | id:1505.03906 author:Gintare Karolina Dziugaite, Daniel M. Roy, Zoubin Ghahramani category:stat.ML cs.LG  published:2015-05-14 summary:We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD. version:1
arxiv-1505-03898 | Pinball Loss Minimization for One-bit Compressive Sensing | http://arxiv.org/abs/1505.03898 | id:1505.03898 author:Xiaolin Huang, Lei Shi, Ming Yan, Johan A. K. Suykens category:cs.IT math.IT math.NA math.OC stat.ML  published:2015-05-14 summary:The one-bit quantization can be implemented by one single comparator, which operates at low power and a high rate. Hence one-bit compressive sensing (\emph{1bit-CS}) becomes very attractive in signal processing. When the measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The existing loss functions include the hinge loss and the linear loss. Though 1bit-CS can be regarded as a binary classification problem because a one-bit measurement only provides the sign information, the choice of the hinge loss over the linear loss in binary classification is not true for 1bit-CS. Many experiments show that the linear loss performs better than the hinge loss for 1bit-CS. Motivated by this observation, we consider the pinball loss, which provides a bridge between the hinge loss and the linear loss. Using this bridge, two 1bit-CS models and two corresponding algorithms are proposed. Pinball loss iterative hard thresholding improves the performance of the binary iterative hard theresholding proposed in [6] and is suitable for the case when the sparsity of the true signal is given. Elastic-net pinball support vector machine generalizes the passive model proposed in [11] and is suitable for the case when the sparsity of the true signal is not given. A fast dual coordinate ascent algorithm is proposed to solve the elastic-net pinball support vector machine problem, and its convergence is proved. The numerical experiments demonstrate that the pinball loss, as a trade-off between the hinge loss and the linear loss, improves the existing 1bit-CS models with better performances. version:1
arxiv-1312-7651 | Petuum: A New Platform for Distributed Machine Learning on Big Data | http://arxiv.org/abs/1312.7651 | id:1312.7651 author:Eric P. Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, Yaoliang Yu category:stat.ML cs.LG cs.SY  published:2013-12-30 summary:What is a systematic way to efficiently apply a wide spectrum of advanced ML programs to industrial scale problems, using Big Models (up to 100s of billions of parameters) on Big Data (up to terabytes or petabytes)? Modern parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized graph-based execution that relies on graph representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of ML programs at scale. We propose a general-purpose framework that systematically addresses data- and model-parallel challenges in large-scale ML, by observing that many ML programs are fundamentally optimization-centric and admit error-tolerant, iterative-convergent algorithmic solutions. This presents unique opportunities for an integrative system design, such as bounded-error network synchronization and dynamic scheduling based on ML program structure. We demonstrate the efficacy of these system designs versus well-known implementations of modern ML algorithms, allowing ML programs to run in much less time and at considerably larger model sizes, even on modestly-sized compute clusters. version:2
arxiv-1505-03891 | Task-Based Optimization of Computed Tomography Imaging Systems | http://arxiv.org/abs/1505.03891 | id:1505.03891 author:Adrian A. Sanchez category:physics.med-ph cs.CV  published:2015-05-14 summary:The goal of this thesis is to provide a framework for the use of task-based metrics of image quality to aid in the design, implementation, and evaluation of CT image reconstruction algorithms and CT systems in general. We support the view that task-based metrics of image quality can be useful in guiding the algorithm design and implementation process in order to yield images of objectively superior quality and higher utility for a given task. Further, we believe that metrics such as the Hotelling observer (HO) SNR can be used as summary scalar metrics of image quality for the evaluation of images produced by novel reconstruction algorithms. In this work, we aim to construct a concise and versatile formalism for image reconstruction algorithm design, implementation, and assessment. The bulk of the work focuses on linear analytical algorithms, specifically the ubiquitous filtered back-projection (FBP) algorithm. However, due to the demonstrated importance of optimization-based algorithms in a wide variety of CT applications, we devote one chapter to the characterization of noise properties in TV-based iterative reconstruction, as the understanding of image statistics in optimization-based reconstruction is the limiting factor in applying HO metrics. version:1
arxiv-1505-03873 | Improving Image Classification with Location Context | http://arxiv.org/abs/1505.03873 | id:1505.03873 author:Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, Lubomir Bourdev category:cs.CV  published:2015-05-14 summary:With the widespread availability of cellphones and cameras that have GPS capabilities, it is common for images being uploaded to the Internet today to have GPS coordinates associated with them. In addition to research that tries to predict GPS coordinates from visual features, this also opens up the door to problems that are conditioned on the availability of GPS coordinates. In this work, we tackle the problem of performing image classification with location context, in which we are given the GPS coordinates for images in both the train and test phases. We explore different ways of encoding and extracting features from the GPS coordinates, and show how to naturally incorporate these features into a Convolutional Neural Network (CNN), the current state-of-the-art for most image classification and recognition problems. We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework. To evaluate our model and to help promote research in this area, we identify a set of location-sensitive concepts and annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has GPS coordinates with these concepts, which we make publicly available. By leveraging location context, we are able to achieve almost a 7% gain in mean average precision. version:1
arxiv-1406-1827 | Recursive Neural Networks Can Learn Logical Semantics | http://arxiv.org/abs/1406.1827 | id:1406.1827 author:Samuel R. Bowman, Christopher Potts, Christopher D. Manning category:cs.CL cs.LG cs.NE  published:2014-06-06 summary:Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models---plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)---can correctly learn to identify logical relationships such as entailment and contradiction using these representations. In our first set of experiments, we generate artificial data from a logical grammar and use it to evaluate the models' ability to learn to handle basic relational reasoning, recursive structures, and quantification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. version:4
arxiv-1505-03840 | Non-unique games over compact groups and orientation estimation in cryo-EM | http://arxiv.org/abs/1505.03840 | id:1505.03840 author:Afonso S. Bandeira, Yutong Chen, Amit Singer category:cs.CV cs.DS math.OC  published:2015-05-14 summary:Let $\mathcal{G}$ be a compact group and let $f_{ij} \in L^2(\mathcal{G})$. We define the Non-Unique Games (NUG) problem as finding $g_1,\dots,g_n \in \mathcal{G}$ to minimize $\sum_{i,j=1}^n f_{ij} \left( g_i g_j^{-1}\right)$. We devise a relaxation of the NUG problem to a semidefinite program (SDP) by taking the Fourier transform of $f_{ij}$ over $\mathcal{G}$, which can then be solved efficiently. The NUG framework can be seen as a generalization of the little Grothendieck problem over the orthogonal group and the Unique Games problem and includes many practically relevant problems, such as the maximum likelihood estimator} to registering bandlimited functions over the unit sphere in $d$-dimensions and orientation estimation in cryo-Electron Microscopy. version:1
arxiv-1505-03832 | Parametric Regression on the Grassmannian | http://arxiv.org/abs/1505.03832 | id:1505.03832 author:Yi Hong, Nikhil Singh, Roland Kwitt, Nuno Vasconcelos, Marc Niethammer category:cs.CV  published:2015-05-14 summary:We address the problem of fitting parametric curves on the Grassmann manifold for the purpose of intrinsic parametric regression. As customary in the literature, we start from the energy minimization formulation of linear least-squares in Euclidean spaces and generalize this concept to general nonflat Riemannian manifolds, following an optimal-control point of view. We then specialize this idea to the Grassmann manifold and demonstrate that it yields a simple, extensible and easy-to-implement solution to the parametric regression problem. In fact, it allows us to extend the basic geodesic model to (1) a time-warped variant and (2) cubic splines. We demonstrate the utility of the proposed solution on different vision problems, such as shape regression as a function of age, traffic-speed estimation and crowd-counting from surveillance video clips. Most notably, these problems can be conveniently solved within the same framework without any specifically-tailored steps along the processing pipeline. version:1
arxiv-1505-03825 | Unsupervised Object Discovery and Tracking in Video Collections | http://arxiv.org/abs/1505.03825 | id:1505.03825 author:Suha Kwak, Minsu Cho, Ivan Laptev, Jean Ponce, Cordelia Schmid category:cs.CV  published:2015-05-14 summary:This paper addresses the problem of automatically localizing dominant objects as spatio-temporal tubes in a noisy collection of videos with minimal or even no supervision. We formulate the problem as a combination of two complementary processes: discovery and tracking. The first one establishes correspondences between prominent regions across videos, and the second one associates successive similar object regions within the same video. Interestingly, our algorithm also discovers the implicit topology of frames associated with instances of the same object class across different videos, a role normally left to supervisory information in the form of class labels in conventional image and video understanding methods. Indeed, as demonstrated by our experiments, our method can handle video collections featuring multiple object classes, and substantially outperforms the state of the art in colocalization, even though it tackles a broader problem with much less supervision. version:1
arxiv-1412-1732 | Statistical models and regularization strategies in statistical image reconstruction of low-dose X-ray CT: a survey | http://arxiv.org/abs/1412.1732 | id:1412.1732 author:Hao Zhang, Jing Wang, Jianhua Ma, Hongbing Lu, Zhengrong Liang category:physics.med-ph cs.CV  published:2014-12-04 summary:Statistical image reconstruction (SIR) methods have shown potential to substantially improve the image quality of low-dose X-ray computed tomography (CT) as compared to the conventional filtered back-projection (FBP) method for various clinical tasks. According to the maximum a posterior (MAP) estimation, the SIR methods can be typically formulated by an objective function consisting of two terms: (1) data-fidelity (or equivalently, data-fitting or data-mismatch) term modeling the statistics of projection measurements, and (2) regularization (or equivalently, prior or penalty) term reflecting prior knowledge or expectation on the characteristics of the image to be reconstructed. Existing SIR methods for low-dose CT can be divided into two groups: (1) those that use calibrated transmitted photon counts (before log-transform) with penalized maximum likelihood (pML) criterion, and (2) those that use calibrated line-integrals (after log-transform) with penalized weighted least-squares (PWLS) criterion. Accurate statistical modeling of the projection measurements is a prerequisite for SIR, while the regularization term in the objective function also plays a critical role for successful image reconstruction. This paper reviews several statistical models on CT projection measurements and various regularization strategies incorporating prior knowledge or expected properties of the image to be reconstructed, which together formulate the objective function of the SIR methods for low-dose X-ray CT. version:3
arxiv-1505-03795 | Fast and numerically stable circle fit | http://arxiv.org/abs/1505.03795 | id:1505.03795 author:Houssam Abdul-Rahman, Nikolai Chernov category:cs.CV  published:2015-05-14 summary:We develop a new algorithm for fitting circles that does not have drawbacks commonly found in existing circle fits. Our fit achieves ultimate accuracy (to machine precision), avoids divergence, and is numerically stable even when fitting circles get arbitrary large. Lastly, our algorithm takes less than 10 iterations to converge, on average. version:1
arxiv-1505-03783 | Rank diversity of languages: Generic behavior in computational linguistics | http://arxiv.org/abs/1505.03783 | id:1505.03783 author:Germinal Cocho, Jorge Flores, Carlos Gershenson, Carlos Pineda, Sergio Sánchez category:cs.CL  published:2015-05-14 summary:Statistical studies of languages have focused on the rank-frequency distribution of words. Instead, we introduce here a measure of how word ranks change in time and call this distribution \emph{rank diversity}. We calculate this diversity for books published in six European languages since 1800, and find that it follows a universal lognormal distribution. Based on the mean and standard deviation associated with the lognormal distribution, we define three different word regimes of languages: "heads" consist of words which almost do not change their rank in time, "bodies" are words of general use, while "tails" are comprised by context-specific words and vary their rank considerably in time. The heads and bodies reflect the size of language cores identified by linguists for basic communication. We propose a Gaussian random walk model which reproduces the rank variation of words in time and thus the diversity. Rank diversity of words can be understood as the result of random variations in rank, where the size of the variation depends on the rank itself. We find that the core size is similar for all languages studied. version:1
arxiv-1505-03493 | Modified Hausdorff Fractal Dimension (MHFD) | http://arxiv.org/abs/1505.03493 | id:1505.03493 author:Reza Farrahi Moghaddam, Mohamed Cheriet category:cs.CV  published:2015-05-13 summary:The Hausdorff fractal dimension has been a fast-to-calculate method to estimate complexity of fractal shapes. In this work, a modified version of this fractal dimension is presented in order to make it more robust when applied in estimating complexity of non-fractal images. The modified Hausdorff fractal dimension stands on two features that weaken the requirement of presence of a shape and also reduce the impact of the noise possibly presented in the input image. The new algorithm has been evaluated on a set of images of different character with promising performance. version:2
arxiv-1505-03703 | A PCA-Based Convolutional Network | http://arxiv.org/abs/1505.03703 | id:1505.03703 author:Yanhai Gan, Jun Liu, Junyu Dong, Guoqiang Zhong category:cs.LG cs.CV cs.NE  published:2015-05-14 summary:In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-the-art deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks. version:1
arxiv-1505-03205 | Leveraging Image based Prior for Visual Place Recognition | http://arxiv.org/abs/1505.03205 | id:1505.03205 author:Tsukamoto Taisho, Tanaka Kanji category:cs.CV  published:2015-05-13 summary:In this study, we propose a novel scene descriptor for visual place recognition. Unlike popular bag-of-words scene descriptors which rely on a library of vector quantized visual features, our proposed descriptor is based on a library of raw image data, such as publicly available photo collections from Google StreetView and Flickr. The library images need not to be associated with spatial information regarding the viewpoint and orientation of the scene. As a result, these images are cheaper than the database images; in addition, they are readily available. Our proposed descriptor directly mines the image library to discover landmarks (i.e., image patches) that suitably match an input query/database image. The discovered landmarks are then compactly described by their pose and shape (i.e., library image ID, bounding boxes) and used as a compact discriminative scene descriptor for the input image. We evaluate the effectiveness of our scene description framework by comparing its performance to that of previous approaches. version:2
arxiv-1505-03597 | Looking outside of the Box: Object Detection and Localization with Multi-scale Patterns | http://arxiv.org/abs/1505.03597 | id:1505.03597 author:Eshed Ohn-Bar, M. M. Trivedi category:cs.CV  published:2015-05-14 summary:Detection and localization of objects at multiple scales often involves sliding a single scale template in order to score windows at different scales independently. Nonetheless, multi-scale visual information at a given image location is highly correlated. This fundamental insight allows us to generalize the traditional multi-scale sliding window technique by jointly considering image features at all scales in order to detect and localize objects. Two max-margin approaches are studied for learning the multi-scale templates and leveraging the highly structured multi-scale information which would have been ignored if a single-scale template was used. The multi-scale formulation is shown to significantly improve general detection performance (measured on the PASCAL VOC dataset). The experimental analysis shows the method to be effective with different visual features, both HOG and CNN. Surprisingly, for a given window in a specific scale, visual information from windows at the same image location but other scales (`out-of-scale' information) contains most of the discriminative information for detection. version:1
arxiv-1505-03581 | CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research | http://arxiv.org/abs/1505.03581 | id:1505.03581 author:Ali Borji, Laurent Itti category:cs.CV  published:2015-05-14 summary:Saliency modeling has been an active research area in computer vision for about two decades. Existing state of the art models perform very well in predicting where people look in natural scenes. There is, however, the risk that these models may have been overfitting themselves to available small scale biased datasets, thus trapping the progress in a local minimum. To gain a deeper insight regarding current issues in saliency modeling and to better gauge progress, we recorded eye movements of 120 observers while they freely viewed a large number of naturalistic and artificial images. Our stimuli includes 4000 images; 200 from each of 20 categories covering different types of scenes such as Cartoons, Art, Objects, Low resolution images, Indoor, Outdoor, Jumbled, Random, and Line drawings. We analyze some basic properties of this dataset and compare some successful models. We believe that our dataset opens new challenges for the next generation of saliency models and helps conduct behavioral studies on bottom-up visual attention. version:1
arxiv-1505-03578 | Vanishing Point Attracts Eye Movements in Scene Free-viewing | http://arxiv.org/abs/1505.03578 | id:1505.03578 author:Ali Borji, Mengyang Feng, Huchuan Lu category:cs.CV  published:2015-05-14 summary:Eye movements are crucial in understanding complex scenes. By predicting where humans look in natural scenes, we can understand how they percieve scenes and priotriaze information for further high-level processing. Here, we study the effect of a particular type of scene structural information known as vanishing point and show that human gaze is attracted to vanishing point regions. We then build a combined model of traditional saliency and vanishing point channel that outperforms state of the art saliency models. version:1
arxiv-1505-03511 | Bootstrapped Adaptive Threshold Selection for Statistical Model Selection and Estimation | http://arxiv.org/abs/1505.03511 | id:1505.03511 author:Kristofer E. Bouchard category:stat.ML  published:2015-05-13 summary:A central goal of neuroscience is to understand how activity in the nervous system is related to features of the external world, or to features of the nervous system itself. A common approach is to model neural responses as a weighted combination of external features, or vice versa. The structure of the model weights can provide insight into neural representations. Often, neural input-output relationships are sparse, with only a few inputs contributing to the output. In part to account for such sparsity, structured regularizers are incorporated into model fitting optimization. However, by imposing priors, structured regularizers can make it difficult to interpret learned model parameters. Here, we investigate a simple, minimally structured model estimation method for accurate, unbiased estimation of sparse models based on Bootstrapped Adaptive Threshold Selection followed by ordinary least-squares refitting (BoATS). Through extensive numerical investigations, we show that this method often performs favorably compared to L1 and L2 regularizers. In particular, for a variety of model distributions and noise levels, BoATS more accurately recovers the parameters of sparse models, leading to more parsimonious explanations of outputs. Finally, we apply this method to the task of decoding human speech production from ECoG recordings. version:1
arxiv-1505-03505 | On a spatial-temporal decomposition of the optical flow | http://arxiv.org/abs/1505.03505 | id:1505.03505 author:Aniello Raffale Patrone, Otmar Scherzer category:cs.CV  published:2015-05-13 summary:In this paper we present the first variational spatial-temporal decomposition algorithm for computation of the optical flow of a dynamic sequence. We consider several applications, such as the extraction of temporal motion patterns of different scales and motion detection in dynamic sequences under varying illumination conditions, such as they appear for instance in psychological flickering experiments. In order to take into account variable illumination conditions we review the derivation, and modify, the optical flow equation. Concerning the numerical implementation, we propose a relaxation approach for the adapted model such that the resulting optimality condition is an integro-differential equation, which is numerically solved by a fixed point iteration. For comparison purposes we use the standard time dependent optical flow algorithm from Weickert-Schn\"orr, which in contrast to our method, constitutes in solving a spatial-temporal differential equation. version:1
arxiv-1505-01147 | Prediction and Quantification of Individual Athletic Performance | http://arxiv.org/abs/1505.01147 | id:1505.01147 author:Duncan A. J. Blythe, Franz J. Király category:stat.AP stat.ML  published:2015-05-05 summary:We provide scientific foundations for athletic performance prediction on an individual level, exposing the phenomenology of individual athletic running performance in the form of a low-rank model dominated by an individual power law. We present, evaluate, and compare a selection of methods for prediction of individual running performance, including our own, \emph{local matrix completion} (LMC), which we show to perform best. We also show that many documented phenomena in quantitative sports science, such as the form of scoring tables, the success of existing prediction methods including Riegel's formula, the Purdy points scheme, the power law for world records performances and the broken power law for world record speeds may be explained on the basis of our findings in a unified way. version:2
arxiv-1505-03489 | A Review Paper: Noise Models in Digital Image Processing | http://arxiv.org/abs/1505.03489 | id:1505.03489 author:Ajay Kumar Boyat, Brijendra Kumar Joshi category:cs.CV  published:2015-05-13 summary:Noise is always presents in digital images during image acquisition, coding, transmission, and processing steps. Noise is very difficult to remove it from the digital images without the prior knowledge of noise model. That is why, review of noise models are essential in the study of image denoising techniques. In this paper, we express a brief overview of various noise models. These noise models can be selected by analysis of their origin. In this way, we present a complete and quantitative analysis of noise models available in digital images. version:1
arxiv-1505-01658 | A Survey of Predictive Modelling under Imbalanced Distributions | http://arxiv.org/abs/1505.01658 | id:1505.01658 author:Paula Branco, Luis Torgo, Rita Ribeiro category:cs.LG I.2.6  published:2015-05-07 summary:Many real world data mining applications involve obtaining predictive models using data sets with strongly imbalanced distributions of the target variable. Frequently, the least common values of this target variable are associated with events that are highly relevant for end users (e.g. fraud detection, unusual returns on stock markets, anticipation of catastrophes, etc.). Moreover, the events may have different costs and benefits, which when associated with the rarity of some of them on the available training data creates serious problems to predictive modelling techniques. This paper presents a survey of existing techniques for handling these important applications of predictive analytics. Although most of the existing work addresses classification tasks (nominal target variables), we also describe methods designed to handle similar problems within regression tasks (numeric target variables). In this survey we discuss the main challenges raised by imbalanced distributions, describe the main approaches to these problems, propose a taxonomy of these methods and refer to some related problems within predictive modelling. version:2
arxiv-1505-03365 | MRF Optimization by Graph Approximation | http://arxiv.org/abs/1505.03365 | id:1505.03365 author:Wonsik Kim, Kyoung Mu Lee category:cs.CV  published:2015-05-13 summary:Graph cuts-based algorithms have achieved great success in energy minimization for many computer vision applications. These algorithms provide approximated solutions for multi-label energy functions via move-making approach. This approach fuses the current solution with a proposal to generate a lower-energy solution. Thus, generating the appropriate proposals is necessary for the success of the move-making approach. However, not much research efforts has been done on the generation of "good" proposals, especially for non-metric energy functions. In this paper, we propose an application-independent and energy-based approach to generate "good" proposals. With these proposals, we present a graph cuts-based move-making algorithm called GA-fusion (fusion with graph approximation-based proposals). Extensive experiments support that our proposal generation is effective across different classes of energy functions. The proposed algorithm outperforms others both on real and synthetic problems. version:1
arxiv-1505-03352 | A Vision Based System for Monitoring the Loss of Attention in Automotive Drivers | http://arxiv.org/abs/1505.03352 | id:1505.03352 author:Anirban Dasgupta, Anjith George, S. L. Happy, Aurobinda Routray category:cs.CV  published:2015-05-13 summary:On board monitoring of the alertness level of an automotive driver has been a challenging research in transportation safety and management. In this paper, we propose a robust real time embedded platform to monitor the loss of attention of the driver during day as well as night driving conditions. The PERcentage of eye CLOSure (PERCLOS) has been used as the indicator of the alertness level. In this approach, the face is detected using Haar like features and tracked using a Kalman Filter. The Eyes are detected using Principal Component Analysis (PCA) during day time and the block Local Binary Pattern (LBP) features during night. Finally the eye state is classified as open or closed using Support Vector Machines(SVM). In plane and off plane rotations of the drivers face have been compensated using Affine and Perspective Transformation respectively. Compensation in illumination variation is carried out using Bi Histogram Equalization (BHE). The algorithm has been cross validated using brain signals and finally been implemented on a Single Board Computer (SBC) having Intel Atom processor, 1 GB RAM, 1.66 GHz clock, x86 architecture, Windows Embedded XP operating system. The system is found to be robust under actual driving conditions. version:1
arxiv-1503-05214 | Analysis of PCA Algorithms in Distributed Environments | http://arxiv.org/abs/1503.05214 | id:1503.05214 author:Tarek Elgamal, Mohamed Hefeeda category:cs.DC cs.LG cs.NA  published:2015-03-17 summary:Classical machine learning algorithms often face scalability bottlenecks when they are applied to large-scale data. Such algorithms were designed to work with small data that is assumed to fit in the memory of one machine. In this report, we analyze different methods for computing an important machine learing algorithm, namely Principal Component Analysis (PCA), and we comment on its limitations in supporting large datasets. The methods are analyzed and compared across two important metrics: time complexity and communication complexity. We consider the worst-case scenarios for both metrics, and we identify the software libraries that implement each method. The analysis in this report helps researchers and engineers in (i) understanding the main bottlenecks for scalability in different PCA algorithms, (ii) choosing the most appropriate method and software library for a given application and data set characteristics, and (iii) designing new scalable PCA algorithms. version:2
arxiv-1408-3693 | Stability and Performance Limits of Adaptive Primal-Dual Networks | http://arxiv.org/abs/1408.3693 | id:1408.3693 author:Zaid J. Towfic, Ali H. Sayed category:math.OC cs.DC cs.LG cs.MA  published:2014-08-16 summary:This work studies distributed primal-dual strategies for adaptation and learning over networks from streaming data. Two first-order methods are considered based on the Arrow-Hurwicz (AH) and augmented Lagrangian (AL) techniques. Several revealing results are discovered in relation to the performance and stability of these strategies when employed over adaptive networks. The conclusions establish that the advantages that these methods have for deterministic optimization problems do not necessarily carry over to stochastic optimization problems. It is found that they have narrower stability ranges and worse steady-state mean-square-error performance than primal methods of the consensus and diffusion type. It is also found that the AH technique can become unstable under a partial observation model, while the other techniques are able to recover the unknown under this scenario. A method to enhance the performance of AL strategies is proposed by tying the selection of the step-size to their regularization parameter. It is shown that this method allows the AL algorithm to approach the performance of consensus and diffusion strategies but that it remains less stable than these other strategies. version:3
arxiv-1505-03344 | A Framework for Fast Face and Eye Detection | http://arxiv.org/abs/1505.03344 | id:1505.03344 author:Anjith George, Anirban Dasgupta, Aurobinda Routray category:cs.CV  published:2015-05-13 summary:Face detection is an essential step in many computer vision applications like surveillance, tracking, medical analysis, facial expression analysis etc. Several approaches have been made in the direction of face detection. Among them, Haar-like features based method is a robust method. In spite of the robustness, Haar - like features work with some limitations. However, with some simple modifications in the algorithm, its performance can be made faster and more robust. The present work refers to the increase in speed of operation of the original algorithm by down sampling the frames and its analysis with different scale factors. It also discusses the detection of tilted faces using an affine transformation of the input image. version:1
arxiv-1505-03257 | Optimal linear estimation under unknown nonlinear transform | http://arxiv.org/abs/1505.03257 | id:1505.03257 author:Xinyang Yi, Zhaoran Wang, Constantine Caramanis, Han Liu category:stat.ML cs.IT math.IT  published:2015-05-13 summary:Linear regression studies the problem of estimating a model parameter $\beta^* \in \mathbb{R}^p$, from $n$ observations $\{(y_i,\mathbf{x}_i)\}_{i=1}^n$ from linear model $y_i = \langle \mathbf{x}_i,\beta^* \rangle + \epsilon_i$. We consider a significant generalization in which the relationship between $\langle \mathbf{x}_i,\beta^* \rangle$ and $y_i$ is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. This model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover $\beta^*$ in settings (i.e., classes of link function $f$) where previous algorithms fail. In general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between $y_i$ and $\langle \mathbf{x}_i,\beta^* \rangle$. We also consider the high dimensional setting where $\beta^*$ is sparse ,and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where $p \gg n$. For a broad class of link functions between $\langle \mathbf{x}_i,\beta^* \rangle$ and $y_i$, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes. version:1
arxiv-1505-03239 | Feature selection using Fisher's ratio technique for automatic speech recognition | http://arxiv.org/abs/1505.03239 | id:1505.03239 author:Sarika Hegde, K. K. Achary, Surendra Shetty category:cs.CL  published:2015-05-13 summary:Automatic Speech Recognition involves mainly two steps; feature extraction and classification . Mel Frequency Cepstral Coefficient is used as one of the prominent feature extraction techniques in ASR. Usually, the set of all 12 MFCC coefficients is used as the feature vector in the classification step. But the question is whether the same or improved classification accuracy can be achieved by using a subset of 12 MFCC as feature vector. In this paper, Fisher's ratio technique is used for selecting a subset of 12 MFCC coefficients that contribute more in discriminating a pattern. The selected coefficients are used in classification with Hidden Markov Model algorithm. The classification accuracies that we get by using 12 coefficients and by using the selected coefficients are compared. version:1
arxiv-1505-03236 | Hybrid data clustering approach using K-Means and Flower Pollination Algorithm | http://arxiv.org/abs/1505.03236 | id:1505.03236 author:R. Jensi, G. Wiselin Jiji category:cs.LG cs.IR cs.NE  published:2015-05-13 summary:Data clustering is a technique for clustering set of objects into known number of groups. Several approaches are widely applied to data clustering so that objects within the clusters are similar and objects in different clusters are far away from each other. K-Means, is one of the familiar center based clustering algorithms since implementation is very easy and fast convergence. However, K-Means algorithm suffers from initialization, hence trapped in local optima. Flower Pollination Algorithm (FPA) is the global optimization technique, which avoids trapping in local optimum solution. In this paper, a novel hybrid data clustering approach using Flower Pollination Algorithm and K-Means (FPAKM) is proposed. The proposed algorithm results are compared with K-Means and FPA on eight datasets. From the experimental results, FPAKM is better than FPA and K-Means. version:1
arxiv-1505-03229 | APAC: Augmented PAttern Classification with Neural Networks | http://arxiv.org/abs/1505.03229 | id:1505.03229 author:Ikuro Sato, Hiroki Nishimura, Kensuke Yokoi category:cs.CV  published:2015-05-13 summary:Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classification problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classifiers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classification, which is a way of classification using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classifiers. Even our multilayer perceptron model beats some of the convolutional models with recently invented stochastic regularization techniques on the CIFAR-10 dataset. version:1
arxiv-1505-03227 | PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence | http://arxiv.org/abs/1505.03227 | id:1505.03227 author:Keze Wang, Liang Lin, Jiangbo Lu, Chenglong Li, Keyang Shi category:cs.CV 68U10  published:2015-05-13 summary:Driven by recent vision and graphics applications such as image segmentation and object recognition, computing pixel-accurate saliency values to uniformly highlight foreground objects becomes increasingly important. In this paper, we propose a unified framework called PISA, which stands for Pixelwise Image Saliency Aggregating various bottom-up cues and priors. It generates spatially coherent yet detail-preserving, pixel-accurate and fine-grained saliency, and overcomes the limitations of previous methods which use homogeneous superpixel-based and color only treatment. PISA aggregates multiple saliency cues in a global context such as complementary color and structure contrast measures with their spatial priors in the image domain. The saliency confidence is further jointly modeled with a neighborhood consistence constraint into an energy minimization formulation, in which each pixel will be evaluated with multiple hypothetical saliency levels. Instead of using global discrete optimization methods, we employ the cost-volume filtering technique to solve our formulation, assigning the saliency levels smoothly while preserving the edge-aware structure details. In addition, a faster version of PISA is developed using a gradient-driven image sub-sampling strategy to greatly improve the runtime efficiency while keeping comparable detection accuracy. Extensive experiments on a number of public datasets suggest that PISA convincingly outperforms other state-of-the-art approaches. In addition, with this work we also create a new dataset containing $800$ commodity images for evaluating saliency detection. The dataset and source code of PISA can be downloaded at http://vision.sysu.edu.cn/project/PISA/ version:1
arxiv-1412-1841 | Exemplar Dynamics and Sound Merger in Language | http://arxiv.org/abs/1412.1841 | id:1412.1841 author:P. F. Tupper category:cs.CL math.DS nlin.AO 91F20  70F99  published:2014-12-02 summary:We develop a model of phonological contrast in natural language. Specifically, the model describes the maintenance of contrast between different words in a language, and the elimination of such contrast when sounds in the words merge. An example of such a contrast is that provided by the two vowel sounds 'i' and 'e', which distinguish pairs of words such as 'pin' and 'pen' in most dialects of English. We model language users' knowledge of the pronunciation of a word as consisting of collections of labeled exemplars stored in memory. Each exemplar is a detailed memory of a particular utterance of the word in question. In our model an exemplar is represented by one or two phonetic variables along with a weight indicating how strong the memory of the utterance is. Starting from an exemplar-level model we derive integro-differential equations for the evolution of exemplar density fields in phonetic space. Using these latter equations we investigate under what conditions two sounds merge, thus eliminating the contrast. Our main conclusion is that for the preservation of phonological contrast, it is necessary that anomalous utterances of a given word are discarded, and not merely stored in memory as an exemplar of another word. version:2
arxiv-1505-02212 | Equitability, interval estimation, and statistical power | http://arxiv.org/abs/1505.02212 | id:1505.02212 author:Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M. Mitzenmacher category:math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH  published:2015-05-09 summary:For analysis of a high-dimensional dataset, a common approach is to test a null hypothesis of statistical independence on all variable pairs using a non-parametric measure of dependence. However, because this approach attempts to identify any non-trivial relationship no matter how weak, it often identifies too many relationships to be useful. What is needed is a way of identifying a smaller set of relationships that merit detailed further analysis. Here we formally present and characterize equitability, a property of measures of dependence that aims to overcome this challenge. Notionally, an equitable statistic is a statistic that, given some measure of noise, assigns similar scores to equally noisy relationships of different types [Reshef et al. 2011]. We begin by formalizing this idea via a new object called the interpretable interval, which functions as an interval estimate of the amount of noise in a relationship of unknown type. We define an equitable statistic as one with small interpretable intervals. We then draw on the equivalence of interval estimation and hypothesis testing to show that under moderate assumptions an equitable statistic is one that yields well powered tests for distinguishing not only between trivial and non-trivial relationships of all kinds but also between non-trivial relationships of different strengths. This means that equitability allows us to specify a threshold relationship strength $x_0$ and to search for relationships of all kinds with strength greater than $x_0$. Thus, equitability can be thought of as a strengthening of power against independence that enables fruitful analysis of data sets with a small number of strong, interesting relationships and a large number of weaker ones. We conclude with a demonstration of how our two equivalent characterizations of equitability can be used to evaluate the equitability of a statistic in practice. version:2
arxiv-1408-4908 | Theoretical Foundations of Equitability and the Maximal Information Coefficient | http://arxiv.org/abs/1408.4908 | id:1408.4908 author:Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael Mitzenmacher category:stat.ME cs.IT math.IT math.ST q-bio.QM stat.ML stat.TH  published:2014-08-21 summary:The maximal information coefficient (MIC) is a tool for finding the strongest pairwise relationships in a data set with many variables (Reshef et al., 2011). MIC is useful because it gives similar scores to equally noisy relationships of different types. This property, called {\em equitability}, is important for analyzing high-dimensional data sets. Here we formalize the theory behind both equitability and MIC in the language of estimation theory. This formalization has a number of advantages. First, it allows us to show that equitability is a generalization of power against statistical independence. Second, it allows us to compute and discuss the population value of MIC, which we call MIC_*. In doing so we generalize and strengthen the mathematical results proven in Reshef et al. (2011) and clarify the relationship between MIC and mutual information. Introducing MIC_* also enables us to reason about the properties of MIC more abstractly: for instance, we show that MIC_* is continuous and that there is a sense in which it is a canonical "smoothing" of mutual information. We also prove an alternate, equivalent characterization of MIC_* that we use to state new estimators of it as well as an algorithm for explicitly computing it when the joint probability density function of a pair of random variables is known. Our hope is that this paper provides a richer theoretical foundation for MIC and equitability going forward. This paper will be accompanied by a forthcoming companion paper that performs extensive empirical analysis and comparison to other methods and discusses the practical aspects of both equitability and the use of MIC and its related statistics. version:3
arxiv-1505-02214 | An Empirical Study of Leading Measures of Dependence | http://arxiv.org/abs/1505.02214 | id:1505.02214 author:David N. Reshef, Yakir A. Reshef, Pardis C. Sabeti, Michael M. Mitzenmacher category:stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML  published:2015-05-09 summary:In exploratory data analysis, we are often interested in identifying promising pairwise associations for further analysis while filtering out weaker, less interesting ones. This can be accomplished by computing a measure of dependence on all variable pairs and examining the highest-scoring pairs, provided the measure of dependence used assigns similar scores to equally noisy relationships of different types. This property, called equitability, is formalized in Reshef et al. [2015b]. In addition to equitability, measures of dependence can also be assessed by the power of their corresponding independence tests as well as their runtime. Here we present extensive empirical evaluation of the equitability, power against independence, and runtime of several leading measures of dependence. These include two statistics introduced in Reshef et al. [2015a]: MICe, which has equitability as its primary goal, and TICe, which has power against independence as its goal. Regarding equitability, our analysis finds that MICe is the most equitable method on functional relationships in most of the settings we considered, although mutual information estimation proves the most equitable at large sample sizes in some specific settings. Regarding power against independence, we find that TICe, along with Heller and Gorfine's S^DDP, is the state of the art on the relationships we tested. Our analyses also show a trade-off between power against independence and equitability consistent with the theory in Reshef et al. [2015b]. In terms of runtime, MICe and TICe are significantly faster than many other measures of dependence tested, and computing either one makes computing the other trivial. This suggests that a fast and useful strategy for achieving a combination of power against independence and equitability may be to filter relationships by TICe and then to examine the MICe of only the significant ones. version:2
arxiv-1505-02213 | Measuring dependence powerfully and equitably | http://arxiv.org/abs/1505.02213 | id:1505.02213 author:Yakir A. Reshef, David N. Reshef, Hilary K. Finucane, Pardis C. Sabeti, Michael M. Mitzenmacher category:stat.ME cs.IT cs.LG math.IT q-bio.QM stat.ML  published:2015-05-09 summary:For high-dimensional datasets, it is common to evaluate a measure of dependence on every variable pair and retain the highest-scoring pairs for follow-up. If the statistic used systematically assigns higher scores to some relationship types over others, important relationships may be overlooked. This difficulty is avoided if the statistic is equitable [Reshef et al. 2015a], i.e., if, for some measure of noise, it assigns similar scores to equally noisy relationships regardless of relationship type. In this paper, we introduce and characterize a population measure of dependence called MIC*. We show three ways that MIC* can be viewed: as the population value of MIC, a highly equitable statistic from [Reshef et al. 2011], as a canonical "smoothing" of mutual information, and as the supremum of an infinite sequence defined in terms of optimal one-dimensional partitions of the marginals of the joint distribution. Based on this theory, we introduce an efficient algorithm for computing MIC* from the density of a pair of random variables, and we define a new consistent estimator MICe for MIC* that is efficiently computable. (In contrast, there is no known polynomial-time algorithm for computing MIC.) We show through simulations that MICe has better bias-variance properties than MIC, and that it has high equitability with respect to R^2 on a set of functional relationships. While MICe is designed for equitability rather than independence testing, we introduce a related statistic, TICe, that is a trivial side-product of the computation of MICe. We prove the consistency of independence testing based on TICe and show in simulations that this approach achieves excellent power. This paper is accompanied by a companion paper [Reshef et al. 2015b] focused on in-depth empirical evaluation of several leading measures of dependence that finds that the performance of MICe and TICe is state-of-the-art. version:2
arxiv-1406-5667 | Correlation Clustering with Noisy Partial Information | http://arxiv.org/abs/1406.5667 | id:1406.5667 author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG  published:2014-06-22 summary:In this paper, we propose and study a semi-random model for the Correlation Clustering problem on arbitrary graphs G. We give two approximation algorithms for Correlation Clustering instances from this model. The first algorithm finds a solution of value $(1+ \delta) optcost + O_{\delta}(n\log^3 n)$ with high probability, where $optcost$ is the value of the optimal solution (for every $\delta > 0$). The second algorithm finds the ground truth clustering with an arbitrarily small classification error $\eta$ (under some additional assumptions on the instance). version:2
arxiv-1412-1455 | Event Retrieval Using Motion Barcodes | http://arxiv.org/abs/1412.1455 | id:1412.1455 author:Gil Ben-Artzi, Michael Werman, Shmuel Peleg category:cs.CV  published:2014-12-03 summary:We introduce a simple and effective method for retrieval of videos showing a specific event, even when the videos of that event were captured from significantly different viewpoints. Appearance-based methods fail in such cases, as appearances change with large changes of viewpoints. Our method is based on a pixel-based feature, "motion barcode", which records the existence/non-existence of motion as a function of time. While appearance, motion magnitude, and motion direction can vary greatly between disparate viewpoints, the existence of motion is viewpoint invariant. Based on the motion barcode, a similarity measure is developed for videos of the same event taken from very different viewpoints. This measure is robust to occlusions common under different viewpoints, and can be computed efficiently. Event retrieval is demonstrated using challenging videos from stationary and hand held cameras. version:3
arxiv-1505-03105 | Sentiment Analysis For Modern Standard Arabic And Colloquial | http://arxiv.org/abs/1505.03105 | id:1505.03105 author:Hossam S. Ibrahim, Sherif M. Abdou, Mervat Gheith category:cs.CL  published:2015-05-12 summary:The rise of social media such as blogs and social networks has fueled interest in sentiment analysis. With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations, therefore many are now looking to the field of sentiment analysis. In this paper, we present a feature-based sentence level approach for Arabic sentiment analysis. Our approach is using Arabic idioms/saying phrases lexicon as a key importance for improving the detection of the sentiment polarity in Arabic sentences as well as a number of novels and rich set of linguistically motivated features contextual Intensifiers, contextual Shifter and negation handling), syntactic features for conflicting phrases which enhance the sentiment classification accuracy. Furthermore, we introduce an automatic expandable wide coverage polarity lexicon of Arabic sentiment words. The lexicon is built with gold-standard sentiment words as a seed which is manually collected and annotated and it expands and detects the sentiment orientation automatically of new sentiment words using synset aggregation technique and free online Arabic lexicons and thesauruses. Our data focus on modern standard Arabic (MSA) and Egyptian dialectal Arabic tweets and microblogs (hotel reservation, product reviews, etc.). The experimental results using our resources and techniques with SVM classifier indicate high performance levels, with accuracies of over 95%. version:1
arxiv-1505-03093 | A new Level-set based Protocol for Accurate Bone Segmentation from CT Imaging | http://arxiv.org/abs/1505.03093 | id:1505.03093 author:Manuel Pinheiro, J. L. Alves category:physics.med-ph cs.CV  published:2015-05-12 summary:In this work it is proposed a medical image segmentation pipeline for accurate bone segmentation from CT imaging. It is a two-step methodology, with a pre-segmentation step and a segmentation refinement step. First, the user performs a rough segmenting of the desired region of interest. Next, a fully automatic refinement step is applied to the pre-segmented data. The automatic segmentation refinement is composed by several sub-stpng, namely image deconvolution, image cropping and interpolation. The user-defined pre-segmentation is then refined over the deconvolved, cropped, and up-sampled version of the image. The algorithm is applied in the segmentation of CT images of a composite femur bone, reconstructed with different reconstruction protocols. Segmentation outcomes are validated against a gold standard model obtained with coordinate measuring machine Nikon Metris LK V20 with a digital line scanner LC60-D that guarantees an accuracy of 28 $\mu m$. High sub-pixel accuracy models were obtained for all tested Datasets. The algorithm is able to produce high quality segmentation of the composite femur regardless of the surface meshing strategy used. version:1
arxiv-1505-03085 | Indonesian Social Media Sentiment Analysis With Sarcasm Detection | http://arxiv.org/abs/1505.03085 | id:1505.03085 author:Edwin Lunando, Ayu Purwarianti category:cs.CL  published:2015-05-12 summary:Sarcasm is considered one of the most difficult problem in sentiment analysis. In our ob-servation on Indonesian social media, for cer-tain topics, people tend to criticize something using sarcasm. Here, we proposed two additional features to detect sarcasm after a common sentiment analysis is conducted. The features are the negativity information and the number of interjection words. We also employed translated SentiWordNet in the sentiment classification. All the classifications were conducted with machine learning algorithms. The experimental results showed that the additional features are quite effective in the sarcasm detection. version:1
arxiv-1505-03084 | A Survey of Arabic Dialogues Understanding for Spontaneous Dialogues and Instant Message | http://arxiv.org/abs/1505.03084 | id:1505.03084 author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL  published:2015-05-12 summary:Building dialogues systems interaction has recently gained considerable attention, but most of the resources and systems built so far are tailored to English and other Indo-European languages. The need for designing systems for other languages is increasing such as Arabic language. For this reasons, there are more interest for Arabic dialogue acts classification task because it a key player in Arabic language understanding to building this systems. This paper surveys different techniques for dialogue acts classification for Arabic. We describe the main existing techniques for utterances segmentations and classification, annotation schemas, and test corpora for Arabic dialogues understanding that have introduced in the literature version:1
arxiv-1505-03081 | Turn Segmentation into Utterances for Arabic Spontaneous Dialogues and Instance Messages | http://arxiv.org/abs/1505.03081 | id:1505.03081 author:AbdelRahim A. Elmadany, Sherif M. Abdou, Mervat Gheith category:cs.CL  published:2015-05-12 summary:Text segmentation task is an essential processing task for many of Natural Language Processing (NLP) such as text summarization, text translation, dialogue language understanding, among others. Turns segmentation considered the key player in dialogue understanding task for building automatic Human-Computer systems. In this paper, we introduce a novel approach to turn segmentation into utterances for Egyptian spontaneous dialogues and Instance Messages (IM) using Machine Learning (ML) approach as a part of automatic understanding Egyptian spontaneous dialogues and IM task. Due to the lack of Egyptian dialect dialogue corpus the system evaluated by our corpus includes 3001 turns, which are collected, segmented, and annotated manually from Egyptian call-centers. The system achieves F1 scores of 90.74% and accuracy of 95.98%. version:1
arxiv-1505-03036 | Removing systematic errors for exoplanet search via latent causes | http://arxiv.org/abs/1505.03036 | id:1505.03036 author:Bernhard Schölkopf, David W. Hogg, Dun Wang, Daniel Foreman-Mackey, Dominik Janzing, Carl-Johann Simon-Gabriel, Jonas Peters category:stat.ML astro-ph.EP astro-ph.IM cs.LG G.3; I.2.6; J.2  published:2015-05-12 summary:We describe a method for removing the effect of confounders in order to reconstruct a latent quantity of interest. The method, referred to as half-sibling regression, is inspired by recent work in causal inference using additive noise models. We provide a theoretical justification and illustrate the potential of the method in a challenging astronomy application. version:1
arxiv-1504-06603 | WxBS: Wide Baseline Stereo Generalizations | http://arxiv.org/abs/1504.06603 | id:1504.06603 author:Dmytro Mishkin, Jiri Matas, Michal Perdoch, Karel Lenc category:cs.CV  published:2015-04-24 summary:We have presented a new problem -- the wide multiple baseline stereo (WxBS) -- which considers matching of images that simultaneously differ in more than one image acquisition factor such as viewpoint, illumination, sensor type or where object appearance changes significantly, e.g. over time. A new dataset with the ground truth for evaluation of matching algorithms has been introduced and will be made public. We have extensively tested a large set of popular and recent detectors and descriptors and show than the combination of RootSIFT and HalfRootSIFT as descriptors with MSER and Hessian-Affine detectors works best for many different nuisance factors. We show that simple adaptive thresholding improves Hessian-Affine, DoG, MSER (and possibly other) detectors and allows to use them on infrared and low contrast images. A novel matching algorithm for addressing the WxBS problem has been introduced. We have shown experimentally that the WxBS-M matcher dominantes the state-of-the-art methods both on both the new and existing datasets. version:2
arxiv-1311-2854 | Spectral Clustering via the Power Method -- Provably | http://arxiv.org/abs/1311.2854 | id:1311.2854 author:Christos Boutsidis, Alex Gittens, Prabhanjan Kambadur category:cs.LG cs.NA  published:2013-11-12 summary:Spectral clustering is one of the most important algorithms in data mining and machine intelligence; however, its computational complexity limits its application to truly large scale data analysis. The computational bottleneck in spectral clustering is computing a few of the top eigenvectors of the (normalized) Laplacian matrix corresponding to the graph representing the data to be clustered. One way to speed up the computation of these eigenvectors is to use the "power method" from the numerical linear algebra literature. Although the power method has been empirically used to speed up spectral clustering, the theory behind this approach, to the best of our knowledge, remains unexplored. This paper provides the \emph{first} such rigorous theoretical justification, arguing that a small number of power iterations suffices to obtain near-optimal partitionings using the approximate eigenvectors. Specifically, we prove that solving the $k$-means clustering problem on the approximate eigenvectors obtained via the power method gives an additive-error approximation to solving the $k$-means problem on the optimal eigenvectors. version:3
arxiv-1505-02982 | Automatic Script Identification in the Wild | http://arxiv.org/abs/1505.02982 | id:1505.02982 author:Baoguang Shi, Cong Yao, Chengquan Zhang, Xiaowei Guo, Feiyue Huang, Xiang Bai category:cs.CV  published:2015-05-12 summary:With the rapid increase of transnational communication and cooperation, people frequently encounter multilingual scenarios in various situations. In this paper, we are concerned with a relatively new problem: script identification at word or line levels in natural scenes. A large-scale dataset with a great quantity of natural images and 10 types of widely used languages is constructed and released. In allusion to the challenges in script identification in real-world scenarios, a deep learning based algorithm is proposed. The experiments on the proposed dataset demonstrate that our algorithm achieves superior performance, compared with conventional image classification methods, such as the original CNN architecture and LLC. version:1
arxiv-1505-02973 | Comparing methods for Twitter Sentiment Analysis | http://arxiv.org/abs/1505.02973 | id:1505.02973 author:Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis Anagnostopoulos, Theodora Varvarigou category:cs.CL cs.IR cs.SI  published:2015-05-12 summary:This work extends the set of works which deal with the popular problem of sentiment analysis in Twitter. It investigates the most popular document ("tweet") representation methods which feed sentiment evaluation mechanisms. In particular, we study the bag-of-words, n-grams and n-gram graphs approaches and for each of them we evaluate the performance of a lexicon-based and 7 learning-based classification algorithms (namely SVM, Na\"ive Bayesian Networks, Logistic Regression, Multilayer Perceptrons, Best-First Trees, Functional Trees and C4.5) as well as their combinations, using a set of 4451 manually annotated tweets. The results demonstrate the superiority of learning-based methods and in particular of n-gram graphs approaches for predicting the sentiment of tweets. They also show that the combinatory approach has impressive effects on n-grams, raising the confidence up to 83.15% on the 5-Grams, using majority vote and a balanced dataset (equal number of positive, negative and neutral tweets for training). In the n-gram graph cases the improvement was small to none, reaching 94.52% on the 4-gram graphs, using Orthodromic distance and a threshold of 0.001. version:1
arxiv-1412-6857 | Contour Detection Using Cost-Sensitive Convolutional Neural Networks | http://arxiv.org/abs/1412.6857 | id:1412.6857 author:Jyh-Jing Hwang, Tyng-Luh Liu category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500. version:5
arxiv-1412-6071 | Fractional Max-Pooling | http://arxiv.org/abs/1412.6071 | id:1412.6071 author:Benjamin Graham category:cs.CV  published:2014-12-18 summary:Convolutional networks almost always incorporate some form of spatial pooling, and very often it is alpha times alpha max-pooling with alpha=2. Max-pooling act on the hidden layers of the network, reducing their size by an integer multiplicative factor alpha. The amazing by-product of discarding 75% of your data is that you build into the network a degree of invariance with respect to translations and elastic distortions. However, if you simply alternate convolutional layers with max-pooling layers, performance is limited due to the rapid reduction in spatial size, and the disjoint nature of the pooling regions. We have formulated a fractional version of max-pooling where alpha is allowed to take non-integer values. Our version of max-pooling is stochastic as there are lots of different ways of constructing suitable pooling regions. We find that our form of fractional max-pooling reduces overfitting on a variety of datasets: for instance, we improve on the state-of-the art for CIFAR-100 without even using dropout. version:4
arxiv-1505-02870 | Incorporating Type II Error Probabilities from Independence Tests into Score-Based Learning of Bayesian Network Structure | http://arxiv.org/abs/1505.02870 | id:1505.02870 author:Eliot Brenner, David Sontag category:cs.LG stat.ML  published:2015-05-12 summary:We give a new consistent scoring function for structure learning of Bayesian networks. In contrast to traditional approaches to score-based structure learning, such as BDeu or MDL, the complexity penalty that we propose is data-dependent and is given by the probability that a conditional independence test correctly shows that an edge cannot exist. What really distinguishes this new scoring function from earlier work is that it has the property of becoming computationally easier to maximize as the amount of data increases. We prove a polynomial sample complexity result, showing that maximizing this score is guaranteed to correctly learn a structure with no false edges and a distribution close to the generating distribution, whenever there exists a Bayesian network which is a perfect map for the data generating distribution. Although the new score can be used with any search algorithm, in our related UAI 2013 paper [BS13], we have given empirical results showing that it is particularly effective when used together with a linear programming relaxation approach to Bayesian network structure learning. The present paper contains all details of the proofs of the finite-sample complexity results in [BS13] as well as detailed explanation of the computation of the certain error probabilities called beta-values, whose precomputation and tabulation is necessary for the implementation of the algorithm in [BS13]. version:1
arxiv-1505-02867 | The Boundary Forest Algorithm for Online Supervised and Unsupervised Learning | http://arxiv.org/abs/1505.02867 | id:1505.02867 author:Charles Mathy, Nate Derbinsky, José Bento, Jonathan Rosenthal, Jonathan Yedidia category:cs.LG cs.DS cs.IR stat.ML  published:2015-05-12 summary:We describe a new instance-based learning algorithm called the Boundary Forest (BF) algorithm, that can be used for supervised and unsupervised learning. The algorithm builds a forest of trees whose nodes store previously seen examples. It can be shown data points one at a time and updates itself incrementally, hence it is naturally online. Few instance-based algorithms have this property while being simultaneously fast, which the BF is. This is crucial for applications where one needs to respond to input data in real time. The number of children of each node is not set beforehand but obtained from the training procedure, which makes the algorithm very flexible with regards to what data manifolds it can learn. We test its generalization performance and speed on a range of benchmark datasets and detail in which settings it outperforms the state of the art. Empirically we find that training time scales as O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N the amount of data, version:1
arxiv-1412-2129 | An iterative step-function estimator for graphons | http://arxiv.org/abs/1412.2129 | id:1412.2129 author:Diana Cai, Nathanael Ackerman, Cameron Freer category:math.ST stat.CO stat.ML stat.TH  published:2014-12-05 summary:Exchangeable graphs arise via a sampling procedure from measurable functions known as graphons. A natural estimation problem is how well we can recover a graphon given a single graph sampled from it. One general framework for estimating a graphon uses step-functions obtained by partitioning the nodes of the graph according to some clustering algorithm. We propose an iterative step-function estimator (ISFE) that, given an initial partition, iteratively clusters nodes based on their edge densities with respect to the previous iteration's partition. We analyze ISFE and demonstrate its performance in comparison with other graphon estimation techniques. version:2
arxiv-1505-02827 | On Markov chain Monte Carlo methods for tall data | http://arxiv.org/abs/1505.02827 | id:1505.02827 author:Rémi Bardenet, Arnaud Doucet, Chris Holmes category:stat.ME stat.CO stat.ML  published:2015-05-11 summary:Markov chain Monte Carlo methods are often deemed too computationally intensive to be of any practical use for big data applications, and in particular for inference on datasets containing a large number $n$ of individual data points, also known as tall datasets. In scenarios where data are assumed independent, various approaches to scale up the Metropolis-Hastings algorithm in a Bayesian inference context have been recently proposed in machine learning and computational statistics. These approaches can be grouped into two categories: divide-and-conquer approaches and, subsampling-based algorithms. The aims of this article are as follows. First, we present a comprehensive review of the existing literature, commenting on the underlying assumptions and theoretical guarantees of each method. Second, by leveraging our understanding of these limitations, we propose an original subsampling-based approach which samples from a distribution provably close to the posterior distribution of interest, yet can require less than $O(n)$ data point likelihood evaluations at each iteration for certain statistical models in favourable scenarios. Finally, we have only been able so far to propose subsampling-based methods which display good performance in scenarios where the Bernstein-von Mises approximation of the target posterior distribution is excellent. It remains an open challenge to develop such methods in scenarios where the Bernstein-von Mises approximation is poor. version:1
arxiv-1505-02729 | Sample complexity of learning Mahalanobis distance metrics | http://arxiv.org/abs/1505.02729 | id:1505.02729 author:Nakul Verma, Kristin Branson category:cs.LG cs.AI stat.ML  published:2015-05-11 summary:Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset's intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise. version:1
arxiv-1505-02581 | Improving neural networks with bunches of neurons modeled by Kumaraswamy units: Preliminary study | http://arxiv.org/abs/1505.02581 | id:1505.02581 author:Jakub Mikolaj Tomczak category:cs.LG cs.NE  published:2015-05-11 summary:Deep neural networks have recently achieved state-of-the-art results in many machine learning problems, e.g., speech recognition or object recognition. Hitherto, work on rectified linear units (ReLU) provides empirical and theoretical evidence on performance increase of neural networks comparing to typically used sigmoid activation function. In this paper, we investigate a new manner of improving neural networks by introducing a bunch of copies of the same neuron modeled by the generalized Kumaraswamy distribution. As a result, we propose novel non-linear activation function which we refer to as Kumaraswamy unit which is closely related to ReLU. In the experimental study with MNIST image corpora we evaluate the Kumaraswamy unit applied to single-layer (shallow) neural network and report a significant drop in test classification error and test cross-entropy in comparison to sigmoid unit, ReLU and Noisy ReLU. version:1
arxiv-1505-02505 | A Two-Layer Local Constrained Sparse Coding Method for Fine-Grained Visual Categorization | http://arxiv.org/abs/1505.02505 | id:1505.02505 author:Guo Lihua, Guo Chenggan category:cs.CV 68T45 I.4.10  published:2015-05-11 summary:Fine-grained categories are more difficulty distinguished than generic categories due to the similarity of inter-class and the diversity of intra-class. Therefore, the fine-grained visual categorization (FGVC) is considered as one of challenge problems in computer vision recently. A new feature learning framework, which is based on a two-layer local constrained sparse coding architecture, is proposed in this paper. The two-layer architecture is introduced for learning intermediate-level features, and the local constrained term is applied to guarantee the local smooth of coding coefficients. For extracting more discriminative information, local orientation histograms are the input of sparse coding instead of raw pixels. Moreover, a quick dictionary updating process is derived to further improve the training speed. Two experimental results show that our method achieves 85.29% accuracy on the Oxford 102 flowers dataset and 67.8% accuracy on the CUB-200-2011 bird dataset, and the performance of our framework is highly competitive with existing literatures. version:1
arxiv-1505-02496 | Training Deeper Convolutional Networks with Deep Supervision | http://arxiv.org/abs/1505.02496 | id:1505.02496 author:Liwei Wang, Chen-Yu Lee, Zhuowen Tu, Svetlana Lazebnik category:cs.CV  published:2015-05-11 summary:One of the most promising ways of improving the performance of deep convolutional neural networks is by increasing the number of convolutional layers. However, adding layers makes training more difficult and computationally expensive. In order to train deeper networks, we propose to add auxiliary supervision branches after certain intermediate layers during training. We formulate a simple rule of thumb to determine where these branches should be added. The resulting deeply supervised structure makes the training much easier and also produces better classification results on ImageNet and the recently released, larger MIT Places dataset version:1
arxiv-1505-02495 | An Online Learning Algorithm for Neuromorphic Hardware Implementation | http://arxiv.org/abs/1505.02495 | id:1505.02495 author:Chetan Singh Thakur, Runchun Wang, Saeed Afshar, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE  published:2015-05-11 summary:We propose a sign-based online learning (SOL) algorithm for a neuromorphic hardware framework called Trainable Analogue Block (TAB). The TAB framework utilises the principles of neural population coding, implying that it encodes the input stimulus using a large pool of nonlinear neurons. The SOL algorithm is a simple weight update rule that employs the sign of the hidden layer activation and the sign of the output error, which is the difference between the target output and the predicted output. The SOL algorithm is easily implementable in hardware, and can be used in any artificial neural network framework that learns weights by minimising a convex cost function. We show that the TAB framework can be trained for various regression tasks using the SOL algorithm. version:1
arxiv-1410-4281 | Constructing Long Short-Term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition | http://arxiv.org/abs/1410.4281 | id:1410.4281 author:Xiangang Li, Xihong Wu category:cs.CL cs.NE  published:2014-10-16 summary:Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further performance improvement, in this research, deep extensions on LSTM are investigated considering that deep hierarchical model has turned out to be more efficient than a shallow one. Motivated by previous research on constructing deep recurrent neural networks (RNNs), alternative deep LSTM architectures are proposed and empirically evaluated on a large vocabulary conversational telephone speech recognition task. Meanwhile, regarding to multi-GPU devices, the training process for LSTM networks is introduced and discussed. Experimental results demonstrate that the deep LSTM networks benefit from the depth and yield the state-of-the-art performance on this task. version:2
arxiv-1503-00269 | Contrastive Pessimistic Likelihood Estimation for Semi-Supervised Classification | http://arxiv.org/abs/1503.00269 | id:1503.00269 author:Marco Loog category:stat.ML cs.LG stat.ME I.2.6; I.5.1  published:2015-03-01 summary:Improvement guarantees for semi-supervised classifiers can currently only be given under restrictive conditions on the data. We propose a general way to perform semi-supervised parameter estimation for likelihood-based classifiers for which, on the full training set, the estimates are never worse than the supervised solution in terms of the log-likelihood. We argue, moreover, that we may expect these solutions to really improve upon the supervised classifier in particular cases. In a worked-out example for LDA, we take it one step further and essentially prove that its semi-supervised version is strictly better than its supervised counterpart. The two new concepts that form the core of our estimation principle are contrast and pessimism. The former refers to the fact that our objective function takes the supervised estimates into account, enabling the semi-supervised solution to explicitly control the potential improvements over this estimate. The latter refers to the fact that our estimates are conservative and therefore resilient to whatever form the true labeling of the unlabeled data takes on. Experiments demonstrate the improvements in terms of both the log-likelihood and the classification error rate on independent test sets. version:2
arxiv-1505-02434 | Spike and Slab Gaussian Process Latent Variable Models | http://arxiv.org/abs/1505.02434 | id:1505.02434 author:Zhenwen Dai, James Hensman, Neil Lawrence category:stat.ML cs.LG  published:2015-05-10 summary:The Gaussian process latent variable model (GP-LVM) is a popular approach to non-linear probabilistic dimensionality reduction. One design choice for the model is the number of latent variables. We present a spike and slab prior for the GP-LVM and propose an efficient variational inference procedure that gives a lower bound of the log marginal likelihood. The new model provides a more principled approach for selecting latent dimensions than the standard way of thresholding the length-scale parameters. The effectiveness of our approach is demonstrated through experiments on real and simulated data. Further, we extend multi-view Gaussian processes that rely on sharing latent dimensions (known as manifold relevance determination) with spike and slab priors. This allows a more principled approach for selecting a subset of the latent space for each view of data. The extended model outperforms the previous state-of-the-art when applied to a cross-modal multimedia retrieval task. version:1
arxiv-1505-02425 | Fast Rhetorical Structure Theory Discourse Parsing | http://arxiv.org/abs/1505.02425 | id:1505.02425 author:Michael Heilman, Kenji Sagae category:cs.CL  published:2015-05-10 summary:In recent years, There has been a variety of research on discourse parsing, particularly RST discourse parsing. Most of the recent work on RST parsing has focused on implementing new types of features or learning algorithms in order to improve accuracy, with relatively little focus on efficiency, robustness, or practical use. Also, most implementations are not widely available. Here, we describe an RST segmentation and parsing system that adapts models and feature sets from various previous work, as described below. Its accuracy is near state-of-the-art, and it was developed to be fast, robust, and practical. For example, it can process short documents such as news articles or essays in less than a second. version:1
arxiv-1505-02377 | Bounded-Distortion Metric Learning | http://arxiv.org/abs/1505.02377 | id:1505.02377 author:Renjie Liao, Jianping Shi, Ziyang Ma, Jun Zhu, Jiaya Jia category:cs.LG  published:2015-05-10 summary:Metric learning aims to embed one metric space into another to benefit tasks like classification and clustering. Although a greatly distorted metric space has a high degree of freedom to fit training data, it is prone to overfitting and numerical inaccuracy. This paper presents {\it bounded-distortion metric learning} (BDML), a new metric learning framework which amounts to finding an optimal Mahalanobis metric space with a bounded-distortion constraint. An efficient solver based on the multiplicative weights update method is proposed. Moreover, we generalize BDML to pseudo-metric learning and devise the semidefinite relaxation and a randomized algorithm to approximately solve it. We further provide theoretical analysis to show that distortion is a key ingredient for stability and generalization ability of our BDML algorithm. Extensive experiments on several benchmark datasets yield promising results. version:1
arxiv-1406-0067 | Optimization via Low-rank Approximation for Community Detection in Networks | http://arxiv.org/abs/1406.0067 | id:1406.0067 author:Can M. Le, Elizaveta Levina, Roman Vershynin category:stat.ML cs.SI math.ST physics.soc-ph stat.TH 62E10  62G05  published:2014-05-31 summary:Community detection is one of the fundamental problems of network analysis, for which a number of methods have been proposed. Most model-based or criteria-based methods have to solve an optimization problem over a discrete set of labels to find communities, which is computationally infeasible. Some fast spectral algorithms have been proposed for specific methods or models, but only on a case-by-case basis. Here we propose a general approach for maximizing a function of a network adjacency matrix over discrete labels by projecting the set of labels onto a subspace approximating the leading eigenvectors of the expected adjacency matrix. This projection onto a low-dimensional space makes the feasible set of labels much smaller and the optimization problem much easier. We prove a general result about this method and show how to apply it to several previously proposed community detection criteria, establishing its consistency for label estimation in each case and demonstrating the fundamental connection between spectral properties of the network and various model-based approaches to community detection. Simulations and applications to real-world data are included to demonstrate our method performs well for multiple problems over a wide range of parameters. version:2
arxiv-1505-02343 | Bayesian Sparse Tucker Models for Dimension Reduction and Tensor Completion | http://arxiv.org/abs/1505.02343 | id:1505.02343 author:Qibin Zhao, Liqing Zhang, Andrzej Cichocki category:cs.LG cs.NA stat.ML  published:2015-05-10 summary:Tucker decomposition is the cornerstone of modern machine learning on tensorial data analysis, which have attracted considerable attention for multiway feature extraction, compressive sensing, and tensor completion. The most challenging problem is related to determination of model complexity (i.e., multilinear rank), especially when noise and missing data are present. In addition, existing methods cannot take into account uncertainty information of latent factors, resulting in low generalization performance. To address these issues, we present a class of probabilistic generative Tucker models for tensor decomposition and completion with structural sparsity over multilinear latent space. To exploit structural sparse modeling, we introduce two group sparsity inducing priors by hierarchial representation of Laplace and Student-t distributions, which facilitates fully posterior inference. For model learning, we derived variational Bayesian inferences over all model (hyper)parameters, and developed efficient and scalable algorithms based on multilinear operations. Our methods can automatically adapt model complexity and infer an optimal multilinear rank by the principle of maximum lower bound of model evidence. Experimental results and comparisons on synthetic, chemometrics and neuroimaging data demonstrate remarkable performance of our models for recovering ground-truth of multilinear rank and missing entries. version:1
arxiv-1505-03481 | Relations Between Adjacency and Modularity Graph Partitioning | http://arxiv.org/abs/1505.03481 | id:1505.03481 author:Hansi Jiang, Carl Meyer category:stat.ML  published:2015-05-09 summary:In this paper the exact linear relation between the leading eigenvector of the unnormalized modularity matrix and the eigenvectors of the adjacency matrix is developed. Based on this analysis a method to approximate the leading eigenvector of the modularity matrix is given, and the relative error of the approximation is derived. A complete proof of the equivalence between normalized modularity clustering and normalized adjacency clustering is also given. A new metric is defined to describe the agreement of two clustering methods, and some applications and experiments are given to illustrate and corroborate the points that are made in the theoretical development. version:1
arxiv-1410-6834 | Scalable Nonparametric Bayesian Inference on Point Processes with Gaussian Processes | http://arxiv.org/abs/1410.6834 | id:1410.6834 author:Yves-Laurent Kom Samo, Stephen Roberts category:stat.ML  published:2014-10-24 summary:In this paper we propose the first non-parametric Bayesian model using Gaussian Processes to make inference on Poisson Point Processes without resorting to gridding the domain or to introducing latent thinning points. Unlike competing models that scale cubically and have a squared memory requirement in the number of data points, our model has a linear complexity and memory requirement. We propose an MCMC sampler and show that our model is faster, more accurate and generates less correlated samples than competing models on both synthetic and real-life data. Finally, we show that our model easily handles data sizes not considered thus far by alternate approaches. version:2
arxiv-1505-02288 | Should we really use post-hoc tests based on mean-ranks? | http://arxiv.org/abs/1505.02288 | id:1505.02288 author:Alessio Benavoli, Giorgio Corani, Francesca Mangili category:cs.LG math.ST physics.data-an q-bio.QM stat.ML stat.TH  published:2015-05-09 summary:The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms A and B depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between A and B could be declared significant if the pool comprises algorithms C, D, E and not significant if the pool comprises algorithms F, G, H. To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test. version:1
arxiv-1505-02269 | Subset Feature Learning for Fine-Grained Category Classification | http://arxiv.org/abs/1505.02269 | id:1505.02269 author:Zongyuan Ge, Christopher Mccool, Conrad Sanderson, Peter Corke category:cs.CV  published:2015-05-09 summary:Fine-grained categorisation has been a challenging problem due to small inter-class variation, large intra-class variation and low number of training images. We propose a learning system which first clusters visually similar classes and then learns deep convolutional neural network features specific to each subset. Experiments on the popular fine-grained Caltech-UCSD bird dataset show that the proposed method outperforms recent fine-grained categorisation methods under the most difficult setting: no bounding boxes are presented at test time. It achieves a mean accuracy of 77.5%, compared to the previous best performance of 73.2%. We also show that progressive transfer learning allows us to first learn domain-generic features (for bird classification) which can then be adapted to specific set of bird classes, yielding improvements in accuracy. version:1
arxiv-1505-02251 | Probabilistic Cascading for Large Scale Hierarchical Classification | http://arxiv.org/abs/1505.02251 | id:1505.02251 author:Aris Kosmopoulos, Georgios Paliouras, Ion Androutsopoulos category:cs.LG cs.CL cs.IR  published:2015-05-09 summary:Hierarchies are frequently used for the organization of objects. Given a hierarchy of classes, two main approaches are used, to automatically classify new instances: flat classification and cascade classification. Flat classification ignores the hierarchy, while cascade classification greedily traverses the hierarchy from the root to the predicted leaf. In this paper we propose a new approach, which extends cascade classification to predict the right leaf by estimating the probability of each root-to-leaf path. We provide experimental results which indicate that, using the same classification algorithm, one can achieve better results with our approach, compared to the traditional flat and cascade classifications. version:1
arxiv-1505-02250 | Newton Sketch: A Linear-time Optimization Algorithm with Linear-Quadratic Convergence | http://arxiv.org/abs/1505.02250 | id:1505.02250 author:Mert Pilanci, Martin J. Wainwright category:math.OC cs.DS cs.LG stat.ML  published:2015-05-09 summary:We propose a randomized second-order method for optimization known as the Newton Sketch: it is based on performing an approximate Newton step using a randomly projected or sub-sampled Hessian. For self-concordant functions, we prove that the algorithm has super-linear convergence with exponentially high probability, with convergence and complexity guarantees that are independent of condition numbers and related problem-dependent quantities. Given a suitable initialization, similar guarantees also hold for strongly convex and smooth objectives without self-concordance. When implemented using randomized projections based on a sub-sampled Hadamard basis, the algorithm typically has substantially lower complexity than Newton's method. We also describe extensions of our methods to programs involving convex constraints that are equipped with self-concordant barriers. We discuss and illustrate applications to linear programs, quadratic programs with convex constraints, logistic regression and other generalized linear models, as well as semidefinite programs. version:1
arxiv-1505-02247 | Performance Evaluation of Vision-Based Algorithms for MAVs | http://arxiv.org/abs/1505.02247 | id:1505.02247 author:T. Holzmann, R. Prettenthaler, J. Pestana, D. Muschick, G. Graber, C. Mostegel, F. Fraundorfer, H. Bischof category:cs.CV  published:2015-05-09 summary:An important focus of current research in the field of Micro Aerial Vehicles (MAVs) is to increase the safety of their operation in general unstructured environments. Especially indoors, where GPS cannot be used for localization, reliable algorithms for localization and mapping of the environment are necessary in order to keep an MAV airborne safely. In this paper, we compare vision-based real-time capable methods for localization and mapping and point out their strengths and weaknesses. Additionally, we describe algorithms for state estimation, control and navigation, which use the localization and mapping results of our vision-based algorithms as input. version:1
arxiv-1312-2177 | Machine Learning Techniques for Intrusion Detection | http://arxiv.org/abs/1312.2177 | id:1312.2177 author:Mahdi Zamani, Mahnush Movahedi category:cs.CR cs.LG cs.NI C.2.0; K.6.5  published:2013-12-08 summary:An Intrusion Detection System (IDS) is a software that monitors a single or a network of computers for malicious activities (attacks) that are aimed at stealing or censoring information or corrupting network protocols. Most techniques used in today's IDS are not able to deal with the dynamic and complex nature of cyber attacks on computer networks. Hence, efficient adaptive methods like various techniques of machine learning can result in higher detection rates, lower false alarm rates and reasonable computation and communication costs. In this paper, we study several such schemes and compare their performance. We divide the schemes into methods based on classical artificial intelligence (AI) and methods based on computational intelligence (CI). We explain how various characteristics of CI techniques can be used to build efficient IDS. version:2
arxiv-1505-02120 | Bilevel approaches for learning of variational imaging models | http://arxiv.org/abs/1505.02120 | id:1505.02120 author:Luca Calatroni, Cao Chung, Juan Carlos De Los Reyes, Carola-Bibiane Schönlieb, Tuomo Valkonen category:math.OC cs.CV  published:2015-05-08 summary:We review some recent learning approaches in variational imaging, based on bilevel optimisation, and emphasize the importance of their treatment in function space. The paper covers both analytical and numerical techniques. Analytically, we include results on the existence and structure of minimisers, as well as optimality conditions for their characterisation. Based on this information, Newton type methods are studied for the solution of the problems at hand, combining them with sampling techniques in case of large databases. The computational verification of the developed techniques is extensively documented, covering instances with different type of regularisers, several noise models, spatially dependent weights and large image databases. version:1
arxiv-1505-02000 | Deep Learning for Medical Image Segmentation | http://arxiv.org/abs/1505.02000 | id:1505.02000 author:Matthew Lai category:cs.LG cs.AI cs.CV  published:2015-05-08 summary:This report provides an overview of the current state of the art deep learning architectures and optimisation techniques, and uses the ADNI hippocampus MRI dataset as an example to compare the effectiveness and efficiency of different convolutional architectures on the task of patch-based 3-dimensional hippocampal segmentation, which is important in the diagnosis of Alzheimer's Disease. We found that a slightly unconventional "stacked 2D" approach provides much better classification performance than simple 2D patches without requiring significantly more computational power. We also examined the popular "tri-planar" approach used in some recently published studies, and found that it provides much better results than the 2D approaches, but also with a moderate increase in computational power requirement. Finally, we evaluated a full 3D convolutional architecture, and found that it provides marginally better results than the tri-planar approach, but at the cost of a very significant increase in computational power requirement. version:1
arxiv-1505-01980 | Evolving Boolean Networks with RNA Editing | http://arxiv.org/abs/1505.01980 | id:1505.01980 author:Larry Bull category:cs.NE q-bio.MN q-bio.PE  published:2015-05-08 summary:The editing of transcribed RNA by other molecules such that the form of the final product differs from that specified in the corresponding DNA sequence is ubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory network model to explore aspects of RNA editing. In particular, it is shown how dynamically altering expressed sequences via a guide RNA-inspired mechanism can be selected for by simulated evolution under various single and multicellular scenarios. version:1
arxiv-1505-01953 | The structure of optimal parameters for image restoration problems | http://arxiv.org/abs/1505.01953 | id:1505.01953 author:Juan Carlos De Los Reyes, Carola-Bibiane Schönlieb, Tuomo Valkonen category:math.OC cs.CV  published:2015-05-08 summary:We study the qualitative properties of optimal regularisation parameters in variational models for image restoration. The parameters are solutions of bilevel optimisation problems with the image restoration problem as constraint. A general type of regulariser is considered, which encompasses total variation (TV), total generalized variation (TGV) and infimal-convolution total variation (ICTV). We prove that under certain conditions on the given data optimal parameters derived by bilevel optimisation problems exist. A crucial point in the existence proof turns out to be the boundedness of the optimal parameters away from $0$ which we prove in this paper. The analysis is done on the original -- in image restoration typically non-smooth variational problem -- as well as on a smoothed approximation set in Hilbert space which is the one considered in numerical computations. For the smoothed bilevel problem we also prove that it $\Gamma$ converges to the original problem as the smoothing vanishes. All analysis is done in function spaces rather than on the discretised learning problem. version:1
arxiv-1505-01589 | Shadow Optimization from Structured Deep Edge Detection | http://arxiv.org/abs/1505.01589 | id:1505.01589 author:Li Shen, Teck Wee Chua, Karianto Leman category:cs.CV  published:2015-05-07 summary:Local structures of shadow boundaries as well as complex interactions of image regions remain largely unexploited by previous shadow detection approaches. In this paper, we present a novel learning-based framework for shadow region recovery from a single image. We exploit the local structures of shadow edges by using a structured CNN learning framework. We show that using the structured label information in the classification can improve the local consistency of the results and avoid spurious labelling. We further propose and formulate a shadow/bright measure to model the complex interactions among image regions. The shadow and bright measures of each patch are computed from the shadow edges detected in the image. Using the global interaction constraints on patches, we formulate a least-square optimization problem for shadow recovery that can be solved efficiently. Our shadow recovery method achieves state-of-the-art results on the major shadow benchmark databases collected under various conditions. version:2
arxiv-1505-01936 | Noise in Structured-Light Stereo Depth Cameras: Modeling and its Applications | http://arxiv.org/abs/1505.01936 | id:1505.01936 author:Avishek Chatterjee, Venu Madhav Govindu category:cs.CV  published:2015-05-08 summary:Depth maps obtained from commercially available structured-light stereo based depth cameras, such as the Kinect, are easy to use but are affected by significant amounts of noise. This paper is devoted to a study of the intrinsic noise characteristics of such depth maps, i.e. the standard deviation of noise in estimated depth varies quadratically with the distance of the object from the depth camera. We validate this theoretical model against empirical observations and demonstrate the utility of this noise model in three popular applications: depth map denoising, volumetric scan merging for 3D modeling, and identification of 3D planes in depth maps. version:1
arxiv-1410-7220 | Exact and Heuristic Algorithms for Semi-Nonnegative Matrix Factorization | http://arxiv.org/abs/1410.7220 | id:1410.7220 author:Nicolas Gillis, Abhishek Kumar category:math.NA cs.LG cs.NA math.OC stat.ML  published:2014-10-27 summary:Given a matrix $M$ (not necessarily nonnegative) and a factorization rank $r$, semi-nonnegative matrix factorization (semi-NMF) looks for a matrix $U$ with $r$ columns and a nonnegative matrix $V$ with $r$ rows such that $UV$ is the best possible approximation of $M$ according to some metric. In this paper, we study the properties of semi-NMF from which we develop exact and heuristic algorithms. Our contribution is threefold. First, we prove that the error of a semi-NMF of rank $r$ has to be smaller than the best unconstrained approximation of rank $r-1$. This leads us to a new initialization procedure based on the singular value decomposition (SVD) with a guarantee on the quality of the approximation. Second, we propose an exact algorithm (that is, an algorithm that finds an optimal solution), also based on the SVD, for a certain class of matrices (including nonnegative irreducible matrices) from which we derive an initialization for matrices not belonging to that class. Numerical experiments illustrate that this second approach performs extremely well, and allows us to compute optimal semi-NMF decompositions in many situations. Finally, we analyze the computational complexity of semi-NMF proving its NP-hardness, already in the rank-one case (that is, for $r = 1$), and we show that semi-NMF is sometimes ill-posed (that is, an optimal solution does not exist). version:3
arxiv-1412-6504 | Learning to Segment Moving Objects in Videos | http://arxiv.org/abs/1412.6504 | id:1412.6504 author:Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, Jitendra Malik category:cs.CV  published:2014-12-19 summary:We segment moving objects in videos by ranking spatio-temporal segment proposals according to "moving objectness": how likely they are to contain a moving object. In each video frame, we compute segment proposals using multiple figure-ground segmentations on per frame motion boundaries. We rank them with a Moving Objectness Detector trained on image and motion fields to detect moving objects and discard over/under segmentations or background parts of the scene. We extend the top ranked segments into spatio-temporal tubes using random walkers on motion affinities of dense point trajectories. Our final tube ranking consistently outperforms previous segmentation methods in the two largest video segmentation benchmarks currently available, for any number of proposals. Further, our per frame moving object proposals increase the detection rate up to 7\% over previous state-of-the-art static proposal methods. version:2
arxiv-1406-4444 | PRISM: Person Re-Identification via Structured Matching | http://arxiv.org/abs/1406.4444 | id:1406.4444 author:Ziming Zhang, Venkatesh Saligrama category:cs.CV cs.LG stat.ML  published:2014-06-13 summary:Person re-identification (re-id), an emerging problem in visual surveillance, deals with maintaining entities of individuals whilst they traverse various locations surveilled by a camera network. From a visual perspective re-id is challenging due to significant changes in visual appearance of individuals in cameras with different pose, illumination and calibration. Globally the challenge arises from the need to maintain structurally consistent matches among all the individual entities across different camera views. We propose PRISM, a structured matching method to jointly account for these challenges. We view the global problem as a weighted graph matching problem and estimate edge weights by learning to predict them based on the co-occurrences of visual patterns in the training examples. These co-occurrence based scores in turn account for appearance changes by inferring likely and unlikely visual co-occurrences appearing in training instances. We implement PRISM on single shot and multi-shot scenarios. PRISM uniformly outperforms state-of-the-art in terms of matching rate while being computationally efficient. version:4
arxiv-1503-04144 | Exploiting Image-trained CNN Architectures for Unconstrained Video Classification | http://arxiv.org/abs/1503.04144 | id:1503.04144 author:Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava, Ruslan Salakhutdinov category:cs.CV  published:2015-03-13 summary:We conduct an in-depth exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED'14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN- and motion-based features can further increase the mean average precision (mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF-101 dataset. version:3
arxiv-1505-01887 | Optimal Neuron Selection: NK Echo State Networks for Reinforcement Learning | http://arxiv.org/abs/1505.01887 | id:1505.01887 author:Darrell Whitley, Renato Tinós, Francisco Chicano category:cs.NE I.2.8  published:2015-05-07 summary:This paper introduces the NK Echo State Network. The problem of learning in the NK Echo State Network is reduced to the problem of optimizing a special form of a Spin Glass Problem known as an NK Landscape. No weight adjustment is used; all learning is accomplished by spinning up (turning on) or spinning down (turning off) neurons in order to find a combination of neurons that work together to achieve the desired computation. For special types of NK Landscapes, an exact global solution can be obtained in polynomial time using dynamic programming. The NK Echo State Network is applied to a reinforcement learning problem requiring a recurrent network: balancing two poles on a cart given no velocity information. Empirical results shows that the NK Echo State Network learns very rapidly and yields very good generalization. version:1
arxiv-1503-05615 | Learning to Search for Dependencies | http://arxiv.org/abs/1503.05615 | id:1503.05615 author:Kai-Wei Chang, He He, Hal Daumé III, John Langford category:cs.CL cs.LG  published:2015-03-18 summary:We demonstrate that a dependency parser can be built using a credit assignment compiler which removes the burden of worrying about low-level machine learning details from the parser implementation. The result is a simple parser which robustly applies to many languages that provides similar statistical and computational performance with best-to-date transition-based parsing approaches, while avoiding various downsides including randomization, extra feature requirements, and custom learning algorithms. version:2
arxiv-1505-01866 | DART: Dropouts meet Multiple Additive Regression Trees | http://arxiv.org/abs/1505.01866 | id:1505.01866 author:K. V. Rashmi, Ran Gilad-Bachrach category:cs.LG stat.ML  published:2015-05-07 summary:Multiple Additive Regression Trees (MART), an ensemble model of boosted regression trees, is known to deliver high prediction accuracy for diverse tasks, and it is widely used in practice. However, it suffers an issue which we call over-specialization, wherein trees added at later iterations tend to impact the prediction of only a few instances, and make negligible contribution towards the remaining instances. This negatively affects the performance of the model on unseen data, and also makes the model over-sensitive to the contributions of the few, initially added tress. We show that the commonly used tool to address this issue, that of shrinkage, alleviates the problem only to a certain extent and the fundamental issue of over-specialization still remains. In this work, we explore a different approach to address the problem that of employing dropouts, a tool that has been recently proposed in the context of learning deep neural networks. We propose a novel way of employing dropouts in MART, resulting in the DART algorithm. We evaluate DART on ranking, regression and classification tasks, using large scale, publicly available datasets, and show that DART outperforms MART in each of the tasks, with a significant margin. We also show that DART overcomes the issue of over-specialization to a considerable extent. version:1
arxiv-1505-01802 | Optimal Decision-Theoretic Classification Using Non-Decomposable Performance Metrics | http://arxiv.org/abs/1505.01802 | id:1505.01802 author:Nagarajan Natarajan, Oluwasanmi Koyejo, Pradeep Ravikumar, Inderjit S. Dhillon category:cs.LG stat.ML  published:2015-05-07 summary:We provide a general theoretical analysis of expected out-of-sample utility, also referred to as decision-theoretic classification, for non-decomposable binary classification metrics such as F-measure and Jaccard coefficient. Our key result is that the expected out-of-sample utility for many performance metrics is provably optimized by a classifier which is equivalent to a signed thresholding of the conditional probability of the positive class. Our analysis bridges a gap in the literature on binary classification, revealed in light of recent results for non-decomposable metrics in population utility maximization style classification. Our results identify checkable properties of a performance metric which are sufficient to guarantee a probability ranking principle. We propose consistent estimators for optimal expected out-of-sample classification. As a consequence of the probability ranking principle, computational requirements can be reduced from exponential to cubic complexity in the general case, and further reduced to quadratic complexity in special cases. We provide empirical results on simulated and benchmark datasets evaluating the performance of the proposed algorithms for decision-theoretic classification and comparing them to baseline and state-of-the-art methods in population utility maximization for non-decomposable metrics. version:1
arxiv-1505-01757 | Contextual Analysis for Middle Eastern Languages with Hidden Markov Models | http://arxiv.org/abs/1505.01757 | id:1505.01757 author:Kazem Taghva category:cs.CL cs.AI  published:2015-05-07 summary:Displaying a document in Middle Eastern languages requires contextual analysis due to different presentational forms for each character of the alphabet. The words of the document will be formed by the joining of the correct positional glyphs representing corresponding presentational forms of the characters. A set of rules defines the joining of the glyphs. As usual, these rules vary from language to language and are subject to interpretation by the software developers. In this paper, we propose a machine learning approach for contextual analysis based on the first order Hidden Markov Model. We will design and build a model for the Farsi language to exhibit this technology. The Farsi model achieves 94 \% accuracy with the training based on a short list of 89 Farsi vocabularies consisting of 2780 Farsi characters. The experiment can be easily extended to many languages including Arabic, Urdu, and Sindhi. Furthermore, the advantage of this approach is that the same software can be used to perform contextual analysis without coding complex rules for each specific language. Of particular interest is that the languages with fewer speakers can have greater representation on the web, since they are typically ignored by software developers due to lack of financial incentives. version:1
arxiv-1505-01740 | Fast Spectral Unmixing based on Dykstra's Alternating Projection | http://arxiv.org/abs/1505.01740 | id:1505.01740 author:Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret category:cs.CV  published:2015-05-07 summary:This paper presents a fast spectral unmixing algorithm based on Dykstra's alternating projection. The proposed algorithm formulates the fully constrained least squares optimization problem associated with the spectral unmixing task as an unconstrained regression problem followed by a projection onto the intersection of several closed convex sets. This projection is achieved by iteratively projecting onto each of the convex sets individually, following Dyktra's scheme. The sequence thus obtained is guaranteed to converge to the sought projection. Thanks to the preliminary matrix decomposition and variable substitution, the projection is implemented intrinsically in a subspace, whose dimension is very often much lower than the number of bands. A benefit of this strategy is that the order of the computational complexity for each projection is decreased from quadratic to linear time. Numerical experiments considering diverse spectral unmixing scenarios provide evidence that the proposed algorithm competes with the state-of-the-art, namely when the number of endmembers is relatively small, a circumstance often observed in real hyperspectral applications. version:1
arxiv-1408-6299 | An inexact Newton-Krylov algorithm for constrained diffeomorphic image registration | http://arxiv.org/abs/1408.6299 | id:1408.6299 author:Andreas Mang, George Biros category:math.NA cs.CV cs.NA math.OC  published:2014-08-27 summary:We propose numerical algorithms for solving large deformation diffeomorphic image registration problems. We formulate the nonrigid image registration problem as a problem of optimal control. This leads to an infinite-dimensional partial differential equation (PDE) constrained optimization problem. The PDE constraint consists, in its simplest form, of a hyperbolic transport equation for the evolution of the image intensity. The control variable is the velocity field. Tikhonov regularization on the control ensures well-posedness. We consider standard smoothness regularization based on $H^1$- or $H^2$-seminorms. We augment this regularization scheme with a constraint on the divergence of the velocity field rendering the deformation incompressible and thus ensuring that the determinant of the deformation gradient is equal to one, up to the numerical error. We use a Fourier pseudospectral discretization in space and a Chebyshev pseudospectral discretization in time. We use a preconditioned, globalized, matrix-free, inexact Newton-Krylov method for numerical optimization. A parameter continuation is designed to estimate an optimal regularization parameter. Regularity is ensured by controlling the geometric properties of the deformation field. Overall, we arrive at a black-box solver. We study spectral properties of the Hessian, grid convergence, numerical accuracy, computational efficiency, and deformation regularity of our scheme. We compare the designed Newton-Krylov methods with a globalized preconditioned gradient descent. We study the influence of a varying number of unknowns in time. The reported results demonstrate excellent numerical accuracy, guaranteed local deformation regularity, and computational efficiency with an optional control on local mass conservation. The Newton-Krylov methods clearly outperform the Picard method if high accuracy of the inversion is required. version:3
arxiv-1501-00960 | Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution | http://arxiv.org/abs/1501.00960 | id:1501.00960 author:Eitan Adam Pechenick, Christopher M. Danforth, Peter Sheridan Dodds category:physics.soc-ph cond-mat.stat-mech cs.CL stat.AP  published:2015-01-05 summary:It is tempting to treat frequency trends from the Google Books data sets as indicators of the "true" popularity of various words and phrases. Doing so allows us to draw quantitatively strong conclusions about the evolution of cultural perception of a given topic, such as time or gender. However, the Google Books corpus suffers from a number of limitations which make it an obscure mask of cultural popularity. A primary issue is that the corpus is in effect a library, containing one of each book. A single, prolific author is thereby able to noticeably insert new phrases into the Google Books lexicon, whether the author is widely read or not. With this understood, the Google Books corpus remains an important data set to be considered more lexicon-like than text-like. Here, we show that a distinct problematic feature arises from the inclusion of scientific texts, which have become an increasingly substantive portion of the corpus throughout the 1900s. The result is a surge of phrases typical to academic articles but less common in general, such as references to time in the form of citations. We highlight these dynamics by examining and comparing major contributions to the statistical divergence of English data sets between decades in the period 1800--2000. We find that only the English Fiction data set from the second version of the corpus is not heavily affected by professional texts, in clear contrast to the first version of the fiction data set and both unfiltered English data sets. Our findings emphasize the need to fully characterize the dynamics of the Google Books corpus before using these data sets to draw broad conclusions about cultural and linguistic evolution. version:2
arxiv-1505-01631 | Data Fusion of Objects Using Techniques Such as Laser Scanning, Structured Light and Photogrammetry for Cultural Heritage Applications | http://arxiv.org/abs/1505.01631 | id:1505.01631 author:Citlalli Gamez Serna, Ruven Pillay, Alain Tremeau category:cs.CV  published:2015-05-07 summary:In this paper we present a semi-automatic 2D-3D local registration pipeline capable of coloring 3D models obtained from 3D scanners by using uncalibrated images. The proposed pipeline exploits the Structure from Motion (SfM) technique in order to reconstruct a sparse representation of the 3D object and obtain the camera parameters from image feature matches. We then coarsely register the reconstructed 3D model to the scanned one through the Scale Iterative Closest Point (SICP) algorithm. SICP provides the global scale, rotation and translation parameters, using minimal manual user intervention. In the final processing stage, a local registration refinement algorithm optimizes the color projection of the aligned photos on the 3D object removing the blurring/ghosting artefacts introduced due to small inaccuracies during the registration. The proposed pipeline is capable of handling real world cases with a range of characteristics from objects with low level geometric features to complex ones. version:1
arxiv-1505-01627 | Bayesian Optimization for Synthetic Gene Design | http://arxiv.org/abs/1505.01627 | id:1505.01627 author:Javier González, Joseph Longworth, David C. James, Neil D. Lawrence category:stat.ML  published:2015-05-07 summary:We address the problem of synthetic gene design using Bayesian optimization. The main issue when designing a gene is that the design space is defined in terms of long strings of characters of different lengths, which renders the optimization intractable. We propose a three-step approach to deal with this issue. First, we use a Gaussian process model to emulate the behavior of the cell. As inputs of the model, we use a set of biologically meaningful gene features, which allows us to define optimal gene designs rules. Based on the model outputs we define a multi-task acquisition function to optimize simultaneously severals aspects of interest. Finally, we define an evaluation function, which allow us to rank sets of candidate gene sequences that are coherent with the optimal design strategy. We illustrate the performance of this approach in a real gene design experiment with mammalian cells. version:1
arxiv-1505-01625 | Context-Aware Mobility Management in HetNets: A Reinforcement Learning Approach | http://arxiv.org/abs/1505.01625 | id:1505.01625 author:Meryem Simsek, Mehdi Bennis, Ismail Güvenc category:cs.NI cs.LG  published:2015-05-07 summary:The use of small cell deployments in heterogeneous network (HetNet) environments is expected to be a key feature of 4G networks and beyond, and essential for providing higher user throughput and cell-edge coverage. However, due to different coverage sizes of macro and pico base stations (BSs), such a paradigm shift introduces additional requirements and challenges in dense networks. Among these challenges is the handover performance of user equipment (UEs), which will be impacted especially when high velocity UEs traverse picocells. In this paper, we propose a coordination-based and context-aware mobility management (MM) procedure for small cell networks using tools from reinforcement learning. Here, macro and pico BSs jointly learn their long-term traffic loads and optimal cell range expansion, and schedule their UEs based on their velocities and historical rates (exchanged among tiers). The proposed approach is shown to not only outperform the classical MM in terms of UE throughput, but also to enable better fairness. In average, a gain of up to 80\% is achieved for UE throughput, while the handover failure probability is reduced up to a factor of three by the proposed learning based MM approaches. version:1
arxiv-1505-01621 | Blind Compressive Sensing Framework for Collaborative Filtering | http://arxiv.org/abs/1505.01621 | id:1505.01621 author:Anupriya Gogna, Angshul Majumdar category:cs.IR cs.LG  published:2015-05-07 summary:Existing works based on latent factor models have focused on representing the rating matrix as a product of user and item latent factor matrices, both being dense. Latent (factor) vectors define the degree to which a trait is possessed by an item or the affinity of user towards that trait. A dense user matrix is a reasonable assumption as each user will like/dislike a trait to certain extent. However, any item will possess only a few of the attributes and never all. Hence, the item matrix should ideally have a sparse structure rather than a dense one as formulated in earlier works. Therefore we propose to factor the ratings matrix into a dense user matrix and a sparse item matrix which leads us to the Blind Compressed Sensing (BCS) framework. We derive an efficient algorithm for solving the BCS problem based on Majorization Minimization (MM) technique. Our proposed approach is able to achieve significantly higher accuracy and shorter run times as compared to existing approaches. version:1
arxiv-1505-01419 | Fast Differentially Private Matrix Factorization | http://arxiv.org/abs/1505.01419 | id:1505.01419 author:Ziqi Liu, Yu-Xiang Wang, Alexander J. Smola category:cs.LG cs.AI  published:2015-05-06 summary:Differentially private collaborative filtering is a challenging task, both in terms of accuracy and speed. We present a simple algorithm that is provably differentially private, while offering good performance, using a novel connection of differential privacy to Bayesian posterior sampling via Stochastic Gradient Langevin Dynamics. Due to its simplicity the algorithm lends itself to efficient implementation. By careful systems design and by exploiting the power law behavior of the data to maximize CPU cache bandwidth we are able to generate 1024 dimensional models at a rate of 8.5 million recommendations per second on a single PC. version:2
arxiv-1505-01599 | Filter characteristics in image decomposition with singular spectrum analysis | http://arxiv.org/abs/1505.01599 | id:1505.01599 author:Kenji Kume, Naoko Nose-Togawa category:cs.CV cs.NA  published:2015-05-07 summary:Singular spectrum analysis is developed as a nonparametric spectral decomposition of a time series. It can be easily extended to the decomposition of multidimensional lattice-like data through the filtering interpretation. In this viewpoint, the singular spectrum analysis can be understood as the adaptive and optimal generation of the filters and their two-step point-symmetric operation to the original data. In this paper, we point out that, when applied to the multidimensional data, the adaptively generated filters exhibit symmetry properties resulting from the bisymmetric nature of the lag-covariance matrices. The eigenvectors of the lag-covariance matrix are either symmetric or antisymmetric, and for the 2D image data, these lead to the differential-type filters with even- or odd-order derivatives. The dominant filter is a smoothing filter, reflecting the dominance of low-frequency components of the photo images. The others are the edge-enhancement or the noise filters corresponding to the band-pass or the high-pass filters. The implication of the decomposition to the image denoising is briefly discussed. version:1
arxiv-1505-01576 | Learning and Optimization with Submodular Functions | http://arxiv.org/abs/1505.01576 | id:1505.01576 author:Bharath Sankaran, Marjan Ghazvininejad, Xinran He, David Kale, Liron Cohen category:cs.LG  published:2015-05-07 summary:In many naturally occurring optimization problems one needs to ensure that the definition of the optimization problem lends itself to solutions that are tractable to compute. In cases where exact solutions cannot be computed tractably, it is beneficial to have strong guarantees on the tractable approximate solutions. In order operate under these criterion most optimization problems are cast under the umbrella of convexity or submodularity. In this report we will study design and optimization over a common class of functions called submodular functions. Set functions, and specifically submodular set functions, characterize a wide variety of naturally occurring optimization problems, and the property of submodularity of set functions has deep theoretical consequences with wide ranging applications. Informally, the property of submodularity of set functions concerns the intuitive "principle of diminishing returns. This property states that adding an element to a smaller set has more value than adding it to a larger set. Common examples of submodular monotone functions are entropies, concave functions of cardinality, and matroid rank functions; non-monotone examples include graph cuts, network flows, and mutual information. In this paper we will review the formal definition of submodularity; the optimization of submodular functions, both maximization and minimization; and finally discuss some applications in relation to learning and reasoning using submodular functions. version:1
arxiv-1505-01560 | Adaptive Nonparametric Image Parsing | http://arxiv.org/abs/1505.01560 | id:1505.01560 author:Tam V. Nguyen, Canyi Lu, Jose Sepulveda, Shuicheng Yan category:cs.CV  published:2015-05-07 summary:In this paper, we present an adaptive nonparametric solution to the image parsing task, namely annotating each image pixel with its corresponding category label. For a given test image, first, a locality-aware retrieval set is extracted from the training data based on super-pixel matching similarities, which are augmented with feature extraction for better differentiation of local super-pixels. Then, the category of each super-pixel is initialized by the majority vote of the $k$-nearest-neighbor super-pixels in the retrieval set. Instead of fixing $k$ as in traditional non-parametric approaches, here we propose a novel adaptive nonparametric approach which determines the sample-specific k for each test image. In particular, $k$ is adaptively set to be the number of the fewest nearest super-pixels which the images in the retrieval set can use to get the best category prediction. Finally, the initial super-pixel labels are further refined by contextual smoothing. Extensive experiments on challenging datasets demonstrate the superiority of the new solution over other state-of-the-art nonparametric solutions. version:1
arxiv-1504-08142 | Semi-Orthogonal Multilinear PCA with Relaxed Start | http://arxiv.org/abs/1504.08142 | id:1504.08142 author:Qiquan Shi, Haiping Lu category:stat.ML cs.CV cs.LG I.2.6  published:2015-04-30 summary:Principal component analysis (PCA) is an unsupervised method for learning low-dimensional features with orthogonal projections. Multilinear PCA methods extend PCA to deal with multidimensional data (tensors) directly via tensor-to-tensor projection or tensor-to-vector projection (TVP). However, under the TVP setting, it is difficult to develop an effective multilinear PCA method with the orthogonality constraint. This paper tackles this problem by proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA learns low-dimensional features directly from tensors via TVP by imposing the orthogonality constraint in only one mode. This formulation results in more captured variance and more learned features than full orthogonality. For better generalization, we further introduce a relaxed start (RS) strategy to get SO-MPCA-RS by fixing the starting projection vectors, which increases the bias and reduces the variance of the learning model. Experiments on both face (2D) and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing algorithms on the whole, and the relaxed start strategy is also effective for other TVP-based PCA methods. version:2
arxiv-1505-01539 | Graphical Potential Games | http://arxiv.org/abs/1505.01539 | id:1505.01539 author:Luis E. Ortiz category:cs.GT cs.AI stat.ML  published:2015-05-06 summary:Potential games, originally introduced in the early 1990's by Lloyd Shapley, the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a very important class of models in game theory. They have special properties such as the existence of Nash equilibria in pure strategies. This note introduces graphical versions of potential games. Special cases of graphical potential games have already found applicability in many areas of science and engineering beyond economics, including artificial intelligence, computer vision, and machine learning. They have been effectively applied to the study and solution of important real-world problems such as routing and congestion in networks, distributed resource allocation (e.g., public goods), and relaxation-labeling for image segmentation. Implicit use of graphical potential games goes back at least 40 years. Several classes of games considered standard in the literature, including coordination games, local interaction games, lattice games, congestion games, and party-affiliation games, are instances of graphical potential games. This note provides several characterizations of graphical potential games by leveraging well-known results from the literature on probabilistic graphical models. A major contribution of the work presented here that particularly distinguishes it from previous work is establishing that the convergence of certain type of game-playing rules implies that the agents/players must be embedded in some graphical potential game. version:1
arxiv-1411-6069 | Category-Specific Object Reconstruction from a Single Image | http://arxiv.org/abs/1411.6069 | id:1411.6069 author:Abhishek Kar, Shubham Tulsiani, João Carreira, Jitendra Malik category:cs.CV  published:2014-11-22 summary:Object reconstruction from a single image -- in the wild -- is a problem where we can make progress and get meaningful results today. This is the main message of this paper, which introduces an automated pipeline with pixels as inputs and 3D surfaces of various rigid categories as outputs in images of realistic scenes. At the core of our approach are deformable 3D models that can be learned from 2D annotations available in existing object detection datasets, that can be driven by noisy automatic object segmentations and which we complement with a bottom-up module for recovering high-frequency shape details. We perform a comprehensive quantitative analysis and ablation study of our approach using the recently introduced PASCAL 3D+ dataset and show very encouraging automatic reconstructions on PASCAL VOC. version:2
arxiv-1504-06378 | Depth-based hand pose estimation: methods, data, and challenges | http://arxiv.org/abs/1504.06378 | id:1504.06378 author:James Steven Supancic III, Gregory Rogez, Yi Yang, Jamie Shotton, Deva Ramanan category:cs.CV  published:2015-04-24 summary:Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress. version:2
arxiv-1505-01474 | Retaining Experience and Growing Solutions | http://arxiv.org/abs/1505.01474 | id:1505.01474 author:Robyn Ffrancon category:cs.NE  published:2015-05-06 summary:Generally, when genetic programming (GP) is used for function synthesis any valuable experience gained by the system is lost from one problem to the next, even when the problems are closely related. With the aim of developing a system which retains beneficial experience from problem to problem, this paper introduces the novel Node-by-Node Growth Solver (NNGS) algorithm which features a component, called the controller, which can be adapted and improved for use across a set of related problems. NNGS grows a single solution tree from root to leaves. Using semantic backpropagation and acting locally on each node in turn, the algorithm employs the controller to assign subsequent child nodes until a fully formed solution is generated. The aim of this paper is to pave a path towards the use of a neural network as the controller component and also, separately, towards the use of meta-GP as a mechanism for improving the controller component. A proof-of-concept controller is discussed which demonstrates the success and potential of the NNGS algorithm. In this case, the controller constitutes a set of hand written rules which can be used to deterministically and greedily solve standard Boolean function synthesis benchmarks. Even before employing machine learning to improve the controller, the algorithm vastly outperforms other well known recent algorithms on run times, maintains comparable solution sizes, and has a 100% success rate on all Boolean function synthesis benchmarks tested so far. version:1
arxiv-1504-06786 | Deviation Based Pooling Strategies For Full Reference Image Quality Assessment | http://arxiv.org/abs/1504.06786 | id:1504.06786 author:Hossein Ziaei Nafchi, Rachid Hedjam, Atena Shahkolaei, Mohamed Cheriet category:cs.MM cs.CV  published:2015-04-26 summary:The state-of-the-art pooling strategies for perceptual image quality assessment (IQA) are based on the mean and the weighted mean. They are robust pooling strategies which usually provide a moderate to high performance for different IQAs. Recently, standard deviation (SD) pooling was also proposed. Although, this deviation pooling provides a very high performance for a few IQAs, its performance is lower than mean poolings for many other IQAs. In this paper, we propose to use the mean absolute deviation (MAD) and show that it is a more robust and accurate pooling strategy for a wider range of IQAs. In fact, MAD pooling has the advantages of both mean pooling and SD pooling. The joint computation and use of the MAD and SD pooling strategies is also considered in this paper. Experimental results provide useful information on the choice of the proper deviation pooling strategy for different IQA models. version:2
arxiv-1412-6505 | Pooled Motion Features for First-Person Videos | http://arxiv.org/abs/1412.6505 | id:1412.6505 author:M. S. Ryoo, Brandon Rothrock, Larry Matthies category:cs.CV  published:2014-12-19 summary:In this paper, we present a new feature representation for first-person videos. In first-person video understanding (e.g., activity recognition), it is very important to capture both entire scene dynamics (i.e., egomotion) and salient local motion observed in videos. We describe a representation framework based on time series pooling, which is designed to abstract short-term/long-term changes in feature descriptor elements. The idea is to keep track of how descriptor values are changing over time and summarize them to represent motion in the activity video. The framework is general, handling any types of per-frame feature descriptors including conventional motion descriptors like histogram of optical flows (HOF) as well as appearance descriptors from more recent convolutional neural networks (CNN). We experimentally confirm that our approach clearly outperforms previous feature representations including bag-of-visual-words and improved Fisher vector (IFV) when using identical underlying feature descriptors. We also confirm that our feature representation has superior performance to existing state-of-the-art features like local spatio-temporal features and Improved Trajectory Features (originally developed for 3rd-person videos) when handling first-person videos. Multiple first-person activity datasets were tested under various settings to confirm these findings. version:2
arxiv-1505-01462 | Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence | http://arxiv.org/abs/1505.01462 | id:1505.01462 author:Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin J. Wainwright category:cs.LG cs.IT math.IT stat.ML  published:2015-05-06 summary:Data in the form of pairwise comparisons arises in many domains, including preference elicitation, sporting competitions, and peer grading among others. We consider parametric ordinal models for such pairwise comparison data involving a latent vector $w^* \in \mathbb{R}^d$ that represents the "qualities" of the $d$ items being compared; this class of models includes the two most widely used parametric models--the Bradley-Terry-Luce (BTL) and the Thurstone models. Working within a standard minimax framework, we provide tight upper and lower bounds on the optimal error in estimating the quality score vector $w^*$ under this class of models. The bounds depend on the topology of the comparison graph induced by the subset of pairs being compared via its Laplacian spectrum. Thus, in settings where the subset of pairs may be chosen, our results provide principled guidelines for making this choice. Finally, we compare these error rates to those under cardinal measurement models and show that the error rates in the ordinal and cardinal settings have identical scalings apart from constant pre-factors. version:1
arxiv-1504-08308 | Efficient Image-Space Extraction and Representation of 3D Surface Topography | http://arxiv.org/abs/1504.08308 | id:1504.08308 author:Matthias Zeppelzauer, Markus Seidl category:cs.CV  published:2015-04-30 summary:Surface topography refers to the geometric micro-structure of a surface and defines its tactile characteristics (typically in the sub-millimeter range). High-resolution 3D scanning techniques developed recently enable the 3D reconstruction of surfaces including their surface topography. In his paper, we present an efficient image-space technique for the extraction of surface topography from high-resolution 3D reconstructions. Additionally, we filter noise and enhance topographic attributes to obtain an improved representation for subsequent topography classification. Comprehensive experiments show that the our representation captures well topographic attributes and significantly improves classification performance compared to alternative 2D and 3D representations. version:3
arxiv-1505-01393 | Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of Methods and Tools | http://arxiv.org/abs/1505.01393 | id:1505.01393 author:Iana Atanassova, Marc Bertin, Philipp Mayr category:cs.DL cs.CL  published:2015-05-06 summary:The Open Access movement in scientific publishing and search engines like Google Scholar have made scientific articles more broadly accessible. During the last decade, the availability of scientific papers in full text has become more and more widespread thanks to the growing number of publications on online platforms such as ArXiv and CiteSeer. The efforts to provide articles in machine-readable formats and the rise of Open Access publishing have resulted in a number of standardized formats for scientific papers (such as NLM-JATS, TEI, DocBook). Our aim is to stimulate research at the intersection of Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scale text analytics and sense mining of scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing. version:1
arxiv-1505-01371 | Re-scale boosting for regression and classification | http://arxiv.org/abs/1505.01371 | id:1505.01371 author:Shaobo Lin, Yao Wang, Lin Xu category:cs.LG stat.ML F.2.2  published:2015-05-06 summary:Boosting is a learning scheme that combines weak prediction rules to produce a strong composite estimator, with the underlying intuition that one can obtain accurate prediction rules by combining "rough" ones. Although boosting is proved to be consistent and overfitting-resistant, its numerical convergence rate is relatively slow. The aim of this paper is to develop a new boosting strategy, called the re-scale boosting (RBoosting), to accelerate the numerical convergence rate and, consequently, improve the learning performance of boosting. Our studies show that RBoosting possesses the almost optimal numerical convergence rate in the sense that, up to a logarithmic factor, it can reach the minimax nonlinear approximation rate. We then use RBoosting to tackle both the classification and regression problems, and deduce a tight generalization error estimate. The theoretical and experimental results show that RBoosting outperforms boosting in terms of generalization. version:1
arxiv-1406-4216 | Person Re-identification by Local Maximal Occurrence Representation and Metric Learning | http://arxiv.org/abs/1406.4216 | id:1406.4216 author:Shengcai Liao, Yang Hu, Xiangyu Zhu, Stan Z. Li category:cs.CV  published:2014-06-17 summary:Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2%, 4.88%, 28.91%, and 31.55% on the four databases, respectively. version:2
arxiv-1505-01130 | Visual Summary of Egocentric Photostreams by Representative Keyframes | http://arxiv.org/abs/1505.01130 | id:1505.01130 author:Marc Bolaños, Ricard Mestre, Estefanía Talavera, Xavier Giró-i-Nieto, Petia Radeva category:cs.CV cs.IR  published:2015-05-05 summary:Building a visual summary from an egocentric photostream captured by a lifelogging wearable camera is of high interest for different applications (e.g. memory reinforcement). In this paper, we propose a new summarization method based on keyframes selection that uses visual features extracted by means of a convolutional neural network. Our method applies an unsupervised clustering for dividing the photostreams into events, and finally extracts the most relevant keyframe for each event. We assess the results by applying a blind-taste test on a group of 20 people who assessed the quality of the summaries. version:2
arxiv-1505-01350 | Classification of Occluded Objects using Fast Recurrent Processing | http://arxiv.org/abs/1505.01350 | id:1505.01350 author:Ozgur Yilmaz category:cs.CV  published:2015-05-06 summary:Recurrent neural networks are powerful tools for handling incomplete data problems in computer vision, thanks to their significant generative capabilities. However, the computational demand for these algorithms is too high to work in real time, without specialized hardware or software solutions. In this paper, we propose a framework for augmenting recurrent processing capabilities into a feedforward network without sacrificing much from computational efficiency. We assume a mixture model and generate samples of the last hidden layer according to the class decisions of the output layer, modify the hidden layer activity using the samples, and propagate to lower layers. For visual occlusion problem, the iterative procedure emulates feedforward-feedback loop, filling-in the missing hidden layer activity with meaningful representations. The proposed algorithm is tested on a widely used dataset, and shown to achieve 2$\times$ improvement in classification accuracy for occluded objects. When compared to Restricted Boltzmann Machines, our algorithm shows superior performance for occluded object classification. version:1
arxiv-1503-04643 | Template-based Monocular 3D Shape Recovery using Laplacian Meshes | http://arxiv.org/abs/1503.04643 | id:1503.04643 author:Dat Tien Ngo, Jonas Ostlund, Pascal Fua category:cs.CV  published:2015-03-16 summary:We show that by extending the Laplacian formalism, which was first introduced in the Graphics community to regularize 3D meshes, we can turn the monocular 3D shape reconstruction of a deformable surface given correspondences with a reference image into a much better-posed problem. This allows us to quickly and reliably eliminate outliers by simply solving a linear least squares problem. This yields an initial 3D shape estimate, which is not necessarily accurate, but whose 2D projections are. The initial shape is then refined by a constrained optimization problem to output the final surface reconstruction. Our approach allows us to reduce the dimensionality of the surface reconstruction problem without sacrificing accuracy, thus allowing for real-time implementations. version:2
arxiv-1505-01300 | Cats & Co: Categorical Time Series Coclustering | http://arxiv.org/abs/1505.01300 | id:1505.01300 author:Dominique Gay, Romain Guigourès, Marc Boullé, Fabrice Clérot category:cs.DB stat.ML H.2.8  published:2015-05-06 summary:We suggest a novel method of clustering and exploratory analysis of temporal event sequences data (also known as categorical time series) based on three-dimensional data grid models. A data set of temporal event sequences can be represented as a data set of three-dimensional points, each point is defined by three variables: a sequence identifier, a time value and an event value. Instantiating data grid models to the 3D-points turns the problem into 3D-coclustering. The sequences are partitioned into clusters, the time variable is discretized into intervals and the events are partitioned into clusters. The cross-product of the univariate partitions forms a multivariate partition of the representation space, i.e., a grid of cells and it also represents a nonparametric estimator of the joint distribution of the sequences, time and events dimensions. Thus, the sequences are grouped together because they have similar joint distribution of time and events, i.e., similar distribution of events along the time dimension. The best data grid is computed using a parameter-free Bayesian model selection approach. We also suggest several criteria for exploiting the resulting grid through agglomerative hierarchies, for interpreting the clusters of sequences and characterizing their components through insightful visualizations. Extensive experiments on both synthetic and real-world data sets demonstrate that data grid models are efficient, effective and discover meaningful underlying patterns of categorical time series data. version:1
arxiv-1505-01257 | A Deeper Look at Dataset Bias | http://arxiv.org/abs/1505.01257 | id:1505.01257 author:Tatiana Tommasi, Novi Patricia, Barbara Caputo, Tinne Tuytelaars category:cs.CV  published:2015-05-06 summary:The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this paper we propose to verify the potential of the DeCAF features when facing the dataset bias problem. We conduct a series of analyses looking at how existing datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled. version:1
arxiv-1408-3304 | On Pairwise Costs for Network Flow Multi-Object Tracking | http://arxiv.org/abs/1408.3304 | id:1408.3304 author:Visesh Chari, Simon Lacoste-Julien, Ivan Laptev, Josef Sivic category:cs.CV math.OC  published:2014-08-14 summary:Multi-object tracking has been recently approached with the min-cost network flow optimization techniques. Such methods simultaneously resolve multiple object tracks in a video and enable modeling of dependencies among tracks. Min-cost network flow methods also fit well within the "tracking-by-detection" paradigm where object trajectories are obtained by connecting per-frame outputs of an object detector. Object detectors, however, often fail due to occlusions and clutter in the video. To cope with such situations, we propose to add pairwise costs to the min-cost network flow framework. While integer solutions to such a problem become NP-hard, we design a convex relaxation solution with an efficient rounding heuristic which empirically gives certificates of small suboptimality. We evaluate two particular types of pairwise costs and demonstrate improvements over recent tracking methods in real-world video sequences. version:2
arxiv-1505-01221 | The Configurable SAT Solver Challenge (CSSC) | http://arxiv.org/abs/1505.01221 | id:1505.01221 author:Frank Hutter, Marius Lindauer, Adrian Balint, Sam Bayless, Holger Hoos, Kevin Leyton-Brown category:cs.AI cs.LG  published:2015-05-05 summary:It is well known that different solution strategies work well for different types of instances of hard combinatorial problems. As a consequence, most solvers for the propositional satisfiability problem (SAT) expose parameters that allow them to be customized to a particular family of instances. In the international SAT competition series, these parameters are ignored: solvers are run using a single default parameter setting (supplied by the authors) for all benchmark instances in a given track. While this competition format rewards solvers with robust default settings, it does not reflect the situation faced by a practitioner who only cares about performance on one particular application and can invest some time into tuning solver parameters for this application. The new Configurable SAT Solver Competition (CSSC) compares solvers in this latter setting, scoring each solver by the performance it achieved after a fully automated configuration step. This article describes the CSSC in more detail, and reports the results obtained in its two instantiations so far, CSSC 2013 and 2014. version:1
arxiv-1505-01214 | Learning Style Similarity for Searching Infographics | http://arxiv.org/abs/1505.01214 | id:1505.01214 author:Babak Saleh, Mira Dontcheva, Aaron Hertzmann, Zhicheng Liu category:cs.GR cs.CV cs.HC cs.IR cs.MM  published:2015-05-05 summary:Infographics are complex graphic designs integrating text, images, charts and sketches. Despite the increasing popularity of infographics and the rapid growth of online design portfolios, little research investigates how we can take advantage of these design resources. In this paper we present a method for measuring the style similarity between infographics. Based on human perception data collected from crowdsourced experiments, we use computer vision and machine learning algorithms to learn a style similarity metric for infographic designs. We evaluate different visual features and learning algorithms and find that a combination of color histograms and Histograms-of-Gradients (HoG) features is most effective in characterizing the style of infographics. We demonstrate our similarity metric on a preliminary image retrieval test. version:1
arxiv-1505-01206 | Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci | http://arxiv.org/abs/1505.01206 | id:1505.01206 author:Changshuai Wei, Daniel J. Schaid, Qing Lu category:q-bio.QM stat.CO stat.ML  published:2015-05-05 summary:Common complex diseases are likely influenced by the interplay of hundreds, or even thousands, of genetic variants. Converging evidence shows that genetic variants with low marginal effects (LME) play an important role in disease development. Despite their potential significance, discovering LME genetic variants and assessing their joint association on high dimensional data (e.g., genome wide association studies) remain a great challenge. To facilitate joint association analysis among a large ensemble of LME genetic variants, we proposed a computationally efficient and powerful approach, which we call Trees Assembling Mann whitney (TAMW). Through simulation studies and an empirical data application, we found that TAMW outperformed multifactor dimensionality reduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW) when the underlying complex disease involves multiple LME loci and their interactions. For instance, in a simulation with 20 interacting LME loci, TAMW attained a higher power (power=0.931) than both MDR (power=0.599) and LRMW (power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci, TAMW also identified a stronger joint association with CD than those detected by MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct a genome wide analysis. The analysis of 459K single nucleotide polymorphisms was completed in 40 hours using parallel computing, and revealed a joint association predisposing to CD (p-value=2.763e-19). Further analysis of the newly discovered association suggested that 13 genes, such as ATG16L1 and LACC1, may play an important role in CD pathophysiological and etiological processes. version:1
arxiv-1501-05677 | Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs | http://arxiv.org/abs/1501.05677 | id:1501.05677 author:David Tolpin, Jan Willem van de Meent, Brooks Paige, Frank Wood category:cs.AI stat.ML  published:2015-01-22 summary:We introduce an adaptive output-sensitive Metropolis-Hastings algorithm for probabilistic models expressed as programs, Adaptive Lightweight Metropolis-Hastings (AdLMH). The algorithm extends Lightweight Metropolis-Hastings (LMH) by adjusting the probabilities of proposing random variables for modification to improve convergence of the program output. We show that AdLMH converges to the correct equilibrium distribution and compare convergence of AdLMH to that of LMH on several test problems to highlight different aspects of the adaptation scheme. We observe consistent improvement in convergence on the test problems. version:2
arxiv-1412-2604 | Actions and Attributes from Wholes and Parts | http://arxiv.org/abs/1412.2604 | id:1412.2604 author:Georgia Gkioxari, Ross Girshick, Jitendra Malik category:cs.CV  published:2014-12-08 summary:We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system. version:2
arxiv-1310-3567 | An Extreme Learning Machine Approach to Predicting Near Chaotic HCCI Combustion Phasing in Real-Time | http://arxiv.org/abs/1310.3567 | id:1310.3567 author:Adam Vaughan, Stanislav V. Bohac category:cs.LG cs.CE  published:2013-10-14 summary:Fuel efficient Homogeneous Charge Compression Ignition (HCCI) engine combustion timing predictions must contend with non-linear chemistry, non-linear physics, period doubling bifurcation(s), turbulent mixing, model parameters that can drift day-to-day, and air-fuel mixture state information that cannot typically be resolved on a cycle-to-cycle basis, especially during transients. In previous work, an abstract cycle-to-cycle mapping function coupled with $\epsilon$-Support Vector Regression was shown to predict experimentally observed cycle-to-cycle combustion timing over a wide range of engine conditions, despite some of the aforementioned difficulties. The main limitation of the previous approach was that a partially acausual randomly sampled training dataset was used to train proof of concept offline predictions. The objective of this paper is to address this limitation by proposing a new online adaptive Extreme Learning Machine (ELM) extension named Weighted Ring-ELM. This extension enables fully causal combustion timing predictions at randomly chosen engine set points, and is shown to achieve results that are as good as or better than the previous offline method. The broader objective of this approach is to enable a new class of real-time model predictive control strategies for high variability HCCI and, ultimately, to bring HCCI's low engine-out NOx and reduced CO2 emissions to production engines. version:3
arxiv-1505-01173 | Deep Learning for Object Saliency Detection and Image Segmentation | http://arxiv.org/abs/1505.01173 | id:1505.01173 author:Hengyue Pan, Bo Wang, Hui Jiang category:cs.CV  published:2015-05-05 summary:In this paper, we propose several novel deep learning methods for object saliency detection based on the powerful convolutional neural networks. In our approach, we use a gradient descent method to iteratively modify an input image based on the pixel-wise gradients to reduce a cost function measuring the class-specific objectness of the image. The pixel-wise gradients can be efficiently computed using the back-propagation algorithm. The discrepancy between the modified image and the original one may be used as a saliency map for the image. Moreover, we have further proposed several new training methods to learn saliency-specific convolutional nets for object saliency detection, in order to leverage the available pixel-wise segmentation information. Our methods are extremely computationally efficient (processing 20-40 images per second in one GPU). In this work, we use the computed saliency maps for image segmentation. Experimental results on two benchmark tasks, namely Microsoft COCO and Pascal VOC 2012, have shown that our proposed methods can generate high-quality salience maps, clearly outperforming many existing methods. In particular, our approaches excel in handling many difficult images, which contain complex background, highly-variable salient objects, multiple objects, and/or very small salient objects. version:1
arxiv-1505-01164 | Achieving a Hyperlocal Housing Price Index: Overcoming Data Sparsity by Bayesian Dynamical Modeling of Multiple Data Streams | http://arxiv.org/abs/1505.01164 | id:1505.01164 author:You Ren, Emily B. Fox, Andrew Bruce category:stat.AP stat.ME stat.ML  published:2015-05-05 summary:Understanding how housing values evolve over time is important to policy makers, consumers and real estate professionals. Existing methods for constructing housing indices are computed at a coarse spatial granularity, such as metropolitan regions, which can mask or distort price dynamics apparent in local markets, such as neighborhoods and census tracts. A challenge in moving to estimates at, for example, the census tract level is the sparsity of spatiotemporally localized house sales observations. Our work aims at addressing this challenge by leveraging observations from multiple census tracts discovered to have correlated valuation dynamics. Our proposed Bayesian nonparametric approach builds on the framework of latent factor models to enable a flexible, data-driven method for inferring the clustering of correlated census tracts. We explore methods for scalability and parallelizability of computations, yielding a housing valuation index at the level of census tract rather than zip code, and on a monthly basis rather than quarterly. Our analysis is provided on a large Seattle metropolitan housing dataset. version:1
arxiv-1410-0342 | Generalized Low Rank Models | http://arxiv.org/abs/1410.0342 | id:1410.0342 author:Madeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd category:stat.ML cs.LG math.OC  published:2014-10-01 summary:Principal components analysis (PCA) is a well-known technique for approximating a tabular data set by a low rank matrix. Here, we extend the idea of PCA to handle arbitrary data sets consisting of numerical, Boolean, categorical, ordinal, and other data types. This framework encompasses many well known techniques in data analysis, such as nonnegative matrix factorization, matrix completion, sparse and robust PCA, $k$-means, $k$-SVD, and maximum margin matrix factorization. The method handles heterogeneous data sets, and leads to coherent schemes for compressing, denoising, and imputing missing entries across all data types simultaneously. It also admits a number of interesting interpretations of the low rank factors, which allow clustering of examples or of features. We propose several parallel algorithms for fitting generalized low rank models, and describe implementations and numerical results. version:4
arxiv-1505-00641 | fastFM: A Library for Factorization Machines | http://arxiv.org/abs/1505.00641 | id:1505.00641 author:Immanuel Bayer category:cs.LG cs.IR  published:2015-05-04 summary:Factorization Machines (FM) are only used in a narrow range of applications and are not part of the standard toolbox of machine learning models. This is a pity, because even though FMs are recognized as being very successful for recommender system type applications they are a general model to deal with sparse and high dimensional features. Our Factorization Machine implementation provides easy access to many solvers and supports regression, classification and ranking tasks. Such an implementation simplifies the use of FM's for a wide field of applications. This implementation has the potential to improve our understanding of the FM model and drive new development. version:2
arxiv-1301-3516 | Learnable Pooling Regions for Image Classification | http://arxiv.org/abs/1301.3516 | id:1301.3516 author:Mateusz Malinowski, Mario Fritz category:cs.CV cs.LG  published:2013-01-15 summary:Biologically inspired, from the early HMAX model to Spatial Pyramid Matching, pooling has played an important role in visual recognition pipelines. Spatial pooling, by grouping of local codes, equips these methods with a certain degree of robustness to translation and deformation yet preserving important spatial information. Despite the predominance of this approach in current recognition systems, we have seen little progress to fully adapt the pooling strategy to the task at hand. This paper proposes a model for learning task dependent pooling scheme -- including previously proposed hand-crafted pooling schemes as a particular instantiation. In our work, we investigate the role of different regularization terms showing that the smooth regularization term is crucial to achieve strong performance using the presented architecture. Finally, we propose an efficient and parallel method to train the model. Our experiments show improved performance over hand-crafted pooling schemes on the CIFAR-10 and CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on the latter. version:3
arxiv-1410-8027 | Towards a Visual Turing Challenge | http://arxiv.org/abs/1410.8027 | id:1410.8027 author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.GL cs.LG  published:2014-10-29 summary:As language and visual understanding by machines progresses rapidly, we are observing an increasing interest in holistic architectures that tightly interlink both modalities in a joint learning and inference process. This trend has allowed the community to progress towards more challenging and open tasks and refueled the hope at achieving the old AI dream of building machines that could pass a turing test in open domains. In order to steadily make progress towards this goal, we realize that quantifying performance becomes increasingly difficult. Therefore we ask how we can precisely define such challenges and how we can evaluate different algorithms on this open tasks? In this paper, we summarize and discuss such challenges as well as try to give answers where appropriate options are available in the literature. We exemplify some of the solutions on a recently presented dataset of question-answering task based on real-world indoor images that establishes a visual turing challenge. Finally, we argue despite the success of unique ground-truth annotation, we likely have to step away from carefully curated dataset and rather rely on 'social consensus' as the main driving force to create suitable benchmarks. Providing coverage in this inherently ambiguous output space is an emerging challenge that we face in order to make quantifiable progress in this area. version:3
arxiv-1411-5190 | A Pooling Approach to Modelling Spatial Relations for Image Retrieval and Annotation | http://arxiv.org/abs/1411.5190 | id:1411.5190 author:Mateusz Malinowski, Mario Fritz category:cs.CV  published:2014-11-19 summary:Over the last two decades we have witnessed strong progress on modeling visual object classes, scenes and attributes that have significantly contributed to automated image understanding. On the other hand, surprisingly little progress has been made on incorporating a spatial representation and reasoning in the inference process. In this work, we propose a pooling interpretation of spatial relations and show how it improves image retrieval and annotations tasks involving spatial language. Due to the complexity of the spatial language, we argue for a learning-based approach that acquires a representation of spatial relations by learning parameters of the pooling operator. We show improvements on previous work on two datasets and two different tasks as well as provide additional insights on a new dataset with an explicit focus on spatial relations. version:2
arxiv-1503-00339 | Variation of word frequencies in Russian literary texts | http://arxiv.org/abs/1503.00339 | id:1503.00339 author:Vladislav Kargin category:cs.CL physics.soc-ph stat.AP  published:2015-03-01 summary:We study the variation of word frequencies in Russian literary texts. Our findings indicate that the standard deviation of a word's frequency across texts depends on its average frequency according to a power law with exponent $0.62,$ showing that the rarer words have a relatively larger degree of frequency volatility (i.e., "burstiness"). Several latent factors models have been estimated to investigate the structure of the word frequency distribution. The dependence of a word's frequency volatility on its average frequency can be explained by the asymmetry in the distribution of latent factors. version:2
arxiv-1410-0210 | A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input | http://arxiv.org/abs/1410.0210 | id:1410.0210 author:Mateusz Malinowski, Mario Fritz category:cs.AI cs.CL cs.CV cs.LG  published:2014-10-01 summary:We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test. version:4
arxiv-1505-01085 | In Defense of the Direct Perception of Affordances | http://arxiv.org/abs/1505.01085 | id:1505.01085 author:David F. Fouhey, Xiaolong Wang, Abhinav Gupta category:cs.CV  published:2015-05-05 summary:The field of functional recognition or affordance estimation from images has seen a revival in recent years. As originally proposed by Gibson, the affordances of a scene were directly perceived from the ambient light: in other words, functional properties like sittable were estimated directly from incoming pixels. Recent work, however, has taken a mediated approach in which affordances are derived by first estimating semantics or geometry and then reasoning about the affordances. In a tribute to Gibson, this paper explores his theory of affordances as originally proposed. We propose two approaches for direct perception of affordances and show that they obtain good results and can out-perform mediated approaches. We hope this paper can rekindle discussion around direct perception and its implications in the long term. version:1
arxiv-1505-01072 | Mining Measured Information from Text | http://arxiv.org/abs/1505.01072 | id:1505.01072 author:Arun S. Maiya, Dale Visser, Andrew Wan category:cs.CL cs.IR I.2.7; H.3.3  published:2015-05-05 summary:We present an approach to extract measured information from text (e.g., a 1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Such extractions are critically important across a wide range of domains - especially those involving search and exploration of scientific and technical documents. We first propose a rule-based entity extractor to mine measured quantities (i.e., a numeric value paired with a measurement unit), which supports a vast and comprehensive set of both common and obscure measurement units. Our method is highly robust and can correctly recover valid measured quantities even when significant errors are introduced through the process of converting document formats like PDF to plain text. Next, we describe an approach to extracting the properties being measured (e.g., the property "pixel pitch" in the phrase "a pixel pitch as high as 352 {\mu}m"). Finally, we present MQSearch: the realization of a search engine with full support for measured information. version:1
arxiv-1505-00996 | Fast Guided Filter | http://arxiv.org/abs/1505.00996 | id:1505.00996 author:Kaiming He, Jian Sun category:cs.CV  published:2015-05-05 summary:The guided filter is a technique for edge-aware image filtering. Because of its nice visual quality, fast speed, and ease of implementation, the guided filter has witnessed various applications in real products, such as image editing apps in phones and stereo reconstruction, and has been included in official MATLAB and OpenCV. In this note, we remind that the guided filter can be simply sped up from O(N) time to O(N/s^2) time for a subsampling ratio s. In a variety of applications, this leads to a speedup of >10x with almost no visible degradation. We hope this acceleration will improve performance of current applications and further popularize this filter. Code is released. version:1
arxiv-1501-04053 | Stochastic Local Interaction (SLI) Model: Interfacing Machine Learning and Geostatistics | http://arxiv.org/abs/1501.04053 | id:1501.04053 author:Dionissios T. Hristopulos category:cs.LG stat.ML  published:2015-01-16 summary:Machine learning and geostatistics are powerful mathematical frameworks for modeling spatial data. Both approaches, however, suffer from poor scaling of the required computational resources for large data applications. We present the Stochastic Local Interaction (SLI) model, which employs a local representation to improve computational efficiency. SLI combines geostatistics and machine learning with ideas from statistical physics and computational geometry. It is based on a joint probability density function defined by an energy functional which involves local interactions implemented by means of kernel functions with adaptive local kernel bandwidths. SLI is expressed in terms of an explicit, typically sparse, precision (inverse covariance) matrix. This representation leads to a semi-analytical expression for interpolation (prediction), which is valid in any number of dimensions and avoids the computationally costly covariance matrix inversion. version:2
arxiv-1505-00991 | Support Vector Machines for Current Status Data | http://arxiv.org/abs/1505.00991 | id:1505.00991 author:Yael Travis-Lumer, Yair Goldberg category:math.ST stat.ML stat.TH  published:2015-05-05 summary:Current status data is a data format where the time to event is restricted to knowledge of whether or not the failure time exceeds a random monitoring time. We develop a support vector machine learning method for current status data that estimates the failure time expectation as a function of the covariates. In order to obtain the support vector machine decision function, we minimize a regularized version of the empirical risk with respect to a data-dependent loss. We show that the decision function has a closed form. Using finite sample bounds and novel oracle inequalities, we prove that the obtained decision function converges to the true conditional expectation for a large family of probability measures and study the associated learning rates. Finally we present a simulation study that compares the performance of the proposed approach to current state of the art. version:1
arxiv-1504-07846 | Incorporating Road Networks into Territory Design | http://arxiv.org/abs/1504.07846 | id:1504.07846 author:Nitin Ahuja, Matthias Bender, Peter Sanders, Christian Schulz, Andreas Wagner category:math.OC cs.DS cs.NE  published:2015-04-29 summary:Given a set of basic areas, the territory design problem asks to create a predefined number of territories, each containing at least one basic area, such that an objective function is optimized. Desired properties of territories often include a reasonable balance, compact form, contiguity and small average journey times which are usually encoded in the objective function or formulated as constraints. We address the territory design problem by developing graph theoretic models that also consider the underlying road network. The derived graph models enable us to tackle the territory design problem by modifying graph partitioning algorithms and mixed integer programming formulations so that the objective of the planning problem is taken into account. We test and compare the algorithms on several real world instances. version:2
arxiv-1505-00936 | Autoencoding Time Series for Visualisation | http://arxiv.org/abs/1505.00936 | id:1505.00936 author:Nikolaos Gianniotis, Dennis Kügler, Peter Tino, Kai Polsterer, Ranjeev Misra category:astro-ph.IM cs.NE  published:2015-05-05 summary:We present an algorithm for the visualisation of time series. To that end we employ echo state networks to convert time series into a suitable vector representation which is capable of capturing the latent dynamics of the time series. Subsequently, the obtained vector representations are put through an autoencoder and the visualisation is constructed using the activations of the bottleneck. The crux of the work lies with defining an objective function that quantifies the reconstruction error of these representations in a principled manner. We demonstrate the method on synthetic and real data. version:1
arxiv-1505-00908 | Reinforced Decision Trees | http://arxiv.org/abs/1505.00908 | id:1505.00908 author:Aurélia Léon, Ludovic Denoyer category:cs.LG  published:2015-05-05 summary:In order to speed-up classification models when facing a large number of categories, one usual approach consists in organizing the categories in a particular structure, this structure being then used as a way to speed-up the prediction computation. This is for example the case when using error-correcting codes or even hierarchies of categories. But in the majority of approaches, this structure is chosen \textit{by hand}, or during a preliminary step, and not integrated in the learning process. We propose a new model called Reinforced Decision Tree which simultaneously learns how to organize categories in a tree structure and how to classify any input based on this structure. This approach keeps the advantages of existing techniques (low inference complexity) but allows one to build efficient classifiers in one learning step. The learning algorithm is inspired by reinforcement learning and policy-gradient techniques which allows us to integrate the two steps (building the tree, and learning the classifier) in one single algorithm. version:1
arxiv-1412-8556 | Domain-Size Pooling in Local Descriptors: DSP-SIFT | http://arxiv.org/abs/1412.8556 | id:1412.8556 author:Jingming Dong, Stefano Soatto category:cs.CV  published:2014-12-30 summary:We introduce a simple modification of local image descriptors, such as SIFT, based on pooling gradient orientations across different domain sizes, in addition to spatial locations. The resulting descriptor, which we call DSP-SIFT, outperforms other methods in wide-baseline matching benchmarks, including those based on convolutional neural networks, despite having the same dimension of SIFT and requiring no training. version:3
arxiv-1505-00869 | On the Feasibility of Distributed Kernel Regression for Big Data | http://arxiv.org/abs/1505.00869 | id:1505.00869 author:Chen Xu, Yongquan Zhang, Runze Li category:stat.ML  published:2015-05-05 summary:In modern scientific research, massive datasets with huge numbers of observations are frequently encountered. To facilitate the computational process, a divide-and-conquer scheme is often used for the analysis of big data. In such a strategy, a full dataset is first split into several manageable segments; the final output is then averaged from the individual outputs of the segments. Despite its popularity in practice, it remains largely unknown that whether such a distributive strategy provides valid theoretical inferences to the original data. In this paper, we address this fundamental issue for the distributed kernel regression (DKR), where the algorithmic feasibility is measured by the generalization performance of the resulting estimator. To justify DKR, a uniform convergence rate is needed for bounding the generalization error over the individual outputs, which brings new and challenging issues in the big data setup. Under mild conditions, we show that, with a proper number of segments, DKR leads to an estimator that is generalization consistent to the unknown regression function. The obtained results justify the method of DKR and shed light on the feasibility of using other distributed algorithms for processing big data. The promising preference of the method is supported by both simulation and real data examples. version:1
arxiv-1505-00866 | Adaptive diffusion constrained total variation scheme with application to `cartoon + texture + edge' image decomposition | http://arxiv.org/abs/1505.00866 | id:1505.00866 author:Juan C. Moreno, V. B. Surya Prasath, D. Vorotnikov, H. Proenca, K. Palaniappan category:cs.CV 68U10  published:2015-05-05 summary:We consider an image decomposition model involving a variational (minimization) problem and an evolutionary partial differential equation (PDE). We utilize a linear inhomogenuous diffusion constrained and weighted total variation (TV) scheme for image adaptive decomposition. An adaptive weight along with TV regularization splits a given image into three components representing the geometrical (cartoon), textural (small scale - microtextures), and edges (big scale - macrotextures). We study the wellposedness of the coupled variational-PDE scheme along with an efficient numerical scheme based on Chambolle's dual minimization method. We provide extensive experimental results in cartoon-texture-edges decomposition, and denoising as well compare with other related variational, coupled anisotropic diffusion PDE based methods. version:1
arxiv-1505-00863 | A Feature-based Classification Technique for Answering Multi-choice World History Questions | http://arxiv.org/abs/1505.00863 | id:1505.00863 author:Shuangyong Song, Yao Meng, Zhongguang Zheng, Jun Sun category:cs.IR cs.AI cs.CL 68T50 H.3.4  published:2015-05-05 summary:Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11. In this paper, we describe our system for solving real-world university entrance exam questions, which are related to world history. Wikipedia is used as the main external resource for our system. Since problems with choosing right/wrong sentence from multiple sentence choices account for about two-thirds of the total, we individually design a classification based model for solving this type of questions. For other types of questions, we also design some simple methods. version:1
arxiv-1505-00855 | Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature | http://arxiv.org/abs/1505.00855 | id:1505.00855 author:Babak Saleh, Ahmed Elgammal category:cs.CV cs.IR cs.LG cs.MM  published:2015-05-05 summary:In the past few years, the number of fine-art collections that are digitized and publicly available has been growing rapidly. With the availability of such large collections of digitized artworks comes the need to develop multimedia systems to archive and retrieve this pool of data. Measuring the visual similarity between artistic items is an essential step for such multimedia systems, which can benefit more high-level multimedia tasks. In order to model this similarity between paintings, we should extract the appropriate visual features for paintings and find out the best approach to learn the similarity metric based on these features. We investigate a comprehensive list of visual features and metric learning approaches to learn an optimized similarity measure between paintings. We develop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting's style, genre, and artist, as well as providing similarity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks. version:1
arxiv-1504-01441 | Locally Non-rigid Registration for Mobile HDR Photography | http://arxiv.org/abs/1504.01441 | id:1504.01441 author:Orazio Gallo, Alejandro Troccoli, Jun Hu, Kari Pulli, Jan Kautz category:cs.CV  published:2015-04-07 summary:Image registration for stack-based HDR photography is challenging. If not properly accounted for, camera motion and scene changes result in artifacts in the composite image. Unfortunately, existing methods to address this problem are either accurate, but too slow for mobile devices, or fast, but prone to failing. We propose a method that fills this void: our approach is extremely fast---under 700ms on a commercial tablet for a pair of 5MP images---and prevents the artifacts that arise from insufficient registration quality. version:3
arxiv-1505-00835 | A novel plasticity rule can explain the development of sensorimotor intelligence | http://arxiv.org/abs/1505.00835 | id:1505.00835 author:Ralf Der, Georg Martius category:cs.RO cs.LG q-bio.NC I.2.9; I.2.6  published:2015-05-04 summary:Grounding autonomous behavior in the nervous system is a fundamental challenge for neuroscience. In particular, the self-organized behavioral development provides more questions than answers. Are there special functional units for curiosity, motivation, and creativity? This paper argues that these features can be grounded in synaptic plasticity itself, without requiring any higher level constructs. We propose differential extrinsic plasticity (DEP) as a new synaptic rule for self-learning systems and apply it to a number of complex robotic systems as a test case. Without specifying any purpose or goal, seemingly purposeful and adaptive behavior is developed, displaying a certain level of sensorimotor intelligence. These surprising results require no system specific modifications of the DEP rule but arise rather from the underlying mechanism of spontaneous symmetry breaking due to the tight brain-body-environment coupling. The new synaptic rule is biologically plausible and it would be an interesting target for a neurobiolocal investigation. We also argue that this neuronal mechanism may have been a catalyst in natural evolution. version:1
arxiv-1405-3539 | Pattern Recognition in Narrative: Tracking Emotional Expression in Context | http://arxiv.org/abs/1405.3539 | id:1405.3539 author:Fionn Murtagh, Adam Ganz category:cs.AI cs.CL 62H25  62H30  62.07  published:2014-05-14 summary:Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel {\em Madame Bovary}. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives. version:3
arxiv-1505-00824 | Self-Expressive Decompositions for Matrix Approximation and Clustering | http://arxiv.org/abs/1505.00824 | id:1505.00824 author:Eva L. Dyer, Tom A. Goldstein, Raajen Patel, Konrad P. Kording, Richard G. Baraniuk category:cs.IT cs.CV cs.LG math.IT stat.ML  published:2015-05-04 summary:Data-aware methods for dimensionality reduction and matrix decomposition aim to find low-dimensional structure in a collection of data. Classical approaches discover such structure by learning a basis that can efficiently express the collection. Recently, "self expression", the idea of using a small subset of data vectors to represent the full collection, has been developed as an alternative to learning. Here, we introduce a scalable method for computing sparse SElf-Expressive Decompositions (SEED). SEED is a greedy method that constructs a basis by sequentially selecting incoherent vectors from the dataset. After forming a basis from a subset of vectors in the dataset, SEED then computes a sparse representation of the dataset with respect to this basis. We develop sufficient conditions under which SEED exactly represents low rank matrices and vectors sampled from a unions of independent subspaces. We show how SEED can be used in applications ranging from matrix approximation and denoising to clustering, and apply it to numerous real-world datasets. Our results demonstrate that SEED is an attractive low-complexity alternative to other sparse matrix factorization approaches such as sparse PCA and self-expressive methods for clustering. version:1
arxiv-1505-00737 | A Gaussian Scale Space Approach For Exudates Detection, Classification And Severity Prediction | http://arxiv.org/abs/1505.00737 | id:1505.00737 author:Mrinal Haloi, Samarendra Dandapat, Rohit Sinha category:cs.CV 68T45  published:2015-05-04 summary:In the context of Computer Aided Diagnosis system for diabetic retinopathy, we present a novel method for detection of exudates and their classification for disease severity prediction. The method is based on Gaussian scale space based interest map and mathematical morphology. It makes use of support vector machine for classification and location information of the optic disc and the macula region for severity prediction. It can efficiently handle luminance variation and it is suitable for varied sized exudates. The method has been probed in publicly available DIARETDB1V2 and e-ophthaEX databases. For exudate detection the proposed method achieved a sensitivity of 96.54% and prediction of 98.35% in DIARETDB1V2 database. version:1
arxiv-1501-06170 | Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals | http://arxiv.org/abs/1501.06170 | id:1501.06170 author:Minsu Cho, Suha Kwak, Cordelia Schmid, Jean Ponce category:cs.CV  published:2015-01-25 summary:This paper addresses unsupervised discovery and localization of dominant objects from a noisy image collection with multiple object classes. The setting of this problem is fully unsupervised, without even image-level annotations or any assumption of a single dominant class. This is far more general than typical colocalization, cosegmentation, or weakly-supervised localization tasks. We tackle the discovery and localization problem using a part-based region matching approach: We use off-the-shelf region proposals to form a set of candidate bounding boxes for objects and object parts. These regions are efficiently matched across images using a probabilistic Hough transform that evaluates the confidence for each candidate correspondence considering both appearance and spatial consistency. Dominant objects are discovered and localized by comparing the scores of candidate regions and selecting those that stand out over other regions containing them. Extensive experimental evaluations on standard benchmarks demonstrate that the proposed approach significantly outperforms the current state of the art in colocalization, and achieves robust object discovery in challenging mixed-class datasets. version:3
arxiv-1410-6131 | Penalized versus constrained generalized eigenvalue problems | http://arxiv.org/abs/1410.6131 | id:1410.6131 author:Irina Gaynanova, James Booth, Martin T. Wells category:stat.CO stat.ML  published:2014-10-22 summary:We investigate the difference between using an $\ell_1$ penalty versus an $\ell_1$ constraint in generalized eigenvalue problems, such as principal component analysis and discriminant analysis. Our main finding is that an $\ell_1$ penalty may fail to provide very sparse solutions; a severe disadvantage for variable selection that can be remedied by using an $\ell_1$ constraint. Our claims are supported both by empirical evidence and theoretical analysis. Finally, we illustrate the advantages of an $\ell_1$ constraint in the context of discriminant analysis and principal component analysis. version:3
arxiv-1505-00670 | Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation | http://arxiv.org/abs/1505.00670 | id:1505.00670 author:Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, Ronald M. Summers category:cs.CV cs.LG  published:2015-05-04 summary:Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of representative ~216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on document- and sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. version:1
arxiv-1504-08168 | Model Selection and Overfitting in Genetic Programming: Empirical Study [Extended Version] | http://arxiv.org/abs/1504.08168 | id:1504.08168 author:Jan Žegklitz, Petr Pošík category:cs.NE cs.LG  published:2015-04-30 summary:Genetic Programming has been very successful in solving a large area of problems but its use as a machine learning algorithm has been limited so far. One of the reasons is the problem of overfitting which cannot be solved or suppresed as easily as in more traditional approaches. Another problem, closely related to overfitting, is the selection of the final model from the population. In this article we present our research that addresses both problems: overfitting and model selection. We compare several ways of dealing with ovefitting, based on Random Sampling Technique (RST) and on using a validation set, all with an emphasis on model selection. We subject each approach to a thorough testing on artificial and real--world datasets and compare them with the standard approach, which uses the full training data, as a baseline. version:2
arxiv-1505-00581 | Activity recognition from videos with parallel hypergraph matching on GPUs | http://arxiv.org/abs/1505.00581 | id:1505.00581 author:Eric Lombardi, Christian Wolf, Oya Celiktutan, Bülent Sankur category:cs.CV  published:2015-05-04 summary:In this paper, we propose a method for activity recognition from videos based on sparse local features and hypergraph matching. We benefit from special properties of the temporal domain in the data to derive a sequential and fast graph matching algorithm for GPUs. Traditionally, graphs and hypergraphs are frequently used to recognize complex and often non-rigid patterns in computer vision, either through graph matching or point-set matching with graphs. Most formulations resort to the minimization of a difficult discrete energy function mixing geometric or structural terms with data attached terms involving appearance features. Traditional methods solve this minimization problem approximately, for instance with spectral techniques. In this work, instead of solving the problem approximatively, the exact solution for the optimal assignment is calculated in parallel on GPUs. The graphical structure is simplified and regularized, which allows to derive an efficient recursive minimization algorithm. The algorithm distributes subproblems over the calculation units of a GPU, which solves them in parallel, allowing the system to run faster than real-time on medium-end GPUs. version:1
arxiv-1505-00571 | Higher Order Maximum Persistency and Comparison Theorems | http://arxiv.org/abs/1505.00571 | id:1505.00571 author:Alexander Shekhovtsov category:cs.CV cs.DM math.CO  published:2015-05-04 summary:We address combinatorial problems that can be formulated as minimization of a partially separable function of discrete variables (energy minimization in graphical models, weighted constraint satisfaction, pseudo-Boolean optimization, 0-1 polynomial programming). For polyhedral relaxations of such problems it is generally not true that variables integer in the relaxed solution will retain the same values in the optimal discrete solution. Those which do are called persistent. Such persistent variables define a part of a globally optimal solution. Once identified, they can be excluded from the problem, reducing its size. To any polyhedral relaxation we associate a sufficient condition proving persistency of a subset of variables. We set up a specially constructed linear program which determines the set of persistent variables maximal with respect to the relaxation. The condition improves as the relaxation is tightened and possesses all its invariances. The proposed framework explains a variety of existing methods originating from different areas of research and based on different principles. A theoretical comparison is established that relates these methods to the standard linear relaxation and proves that the proposed technique identifies same or larger set of persistent variables. version:1
arxiv-1504-06852 | FlowNet: Learning Optical Flow with Convolutional Networks | http://arxiv.org/abs/1504.06852 | id:1504.06852 author:Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbaş, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, Thomas Brox category:cs.CV cs.LG I.2.6; I.4.8  published:2015-04-26 summary:Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps. version:2
arxiv-1505-00553 | On Regret-Optimal Learning in Decentralized Multi-player Multi-armed Bandits | http://arxiv.org/abs/1505.00553 | id:1505.00553 author:Naumaan Nayyar, Dileep Kalathil, Rahul Jain category:stat.ML cs.LG  published:2015-05-04 summary:We consider the problem of learning in single-player and multiplayer multiarmed bandit models. Bandit problems are classes of online learning problems that capture exploration versus exploitation tradeoffs. In a multiarmed bandit model, players can pick among many arms, and each play of an arm generates an i.i.d. reward from an unknown distribution. The objective is to design a policy that maximizes the expected reward over a time horizon for a single player setting and the sum of expected rewards for the multiplayer setting. In the multiplayer setting, arms may give different rewards to different players. There is no separate channel for coordination among the players. Any attempt at communication is costly and adds to regret. We propose two decentralizable policies, $\tt E^3$ ($\tt E$-$\tt cubed$) and $\tt E^3$-$\tt TS$, that can be used in both single player and multiplayer settings. These policies are shown to yield expected regret that grows at most as O($\log^{1+\epsilon} T$). It is well known that $\log T$ is the lower bound on the rate of growth of regret even in a centralized case. The proposed algorithms improve on prior work where regret grew at O($\log^2 T$). More fundamentally, these policies address the question of additional cost incurred in decentralized online learning, suggesting that there is at most an $\epsilon$-factor cost in terms of order of regret. This solves a problem of relevance in many domains and had been open for a while. version:1
arxiv-1505-00529 | Learning Document Image Binarization from Data | http://arxiv.org/abs/1505.00529 | id:1505.00529 author:Yue Wu, Stephen Rawls, Wael AbdAlmageed, Premkumar Natarajan category:cs.CV  published:2015-05-04 summary:In this paper we present a fully trainable binarization solution for degraded document images. Unlike previous attempts that often used simple features with a series of pre- and post-processing, our solution encodes all heuristics about whether or not a pixel is foreground text into a high-dimensional feature vector and learns a more complicated decision function. In particular, we prepare features of three types: 1) existing features for binarization such as intensity [1], contrast [2], [3], and Laplacian [4], [5]; 2) reformulated features from existing binarization decision functions such those in [6] and [7]; and 3) our newly developed features, namely the Logarithm Intensity Percentile (LIP) and the Relative Darkness Index (RDI). Our initial experimental results show that using only selected samples (about 1.5% of all available training data), we can achieve a binarization performance comparable to those fine-tuned (typically by hand), state-of-the-art methods. Additionally, the trained document binarization classifier shows good generalization capabilities on out-of-domain data. version:1
arxiv-1505-00526 | An Explicit Sampling Dependent Spectral Error Bound for Column Subset Selection | http://arxiv.org/abs/1505.00526 | id:1505.00526 author:Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu category:math.NA stat.ML  published:2015-05-04 summary:In this paper, we consider the problem of column subset selection. We present a novel analysis of the spectral norm reconstruction for a simple randomized algorithm and establish a new bound that depends explicitly on the sampling probabilities. The sampling dependent error bound (i) allows us to better understand the tradeoff in the reconstruction error due to sampling probabilities, (ii) exhibits more insights than existing error bounds that exploit specific probability distributions, and (iii) implies better sampling distributions. In particular, we show that a sampling distribution with probabilities proportional to the square root of the statistical leverage scores is always better than uniform sampling and is better than leverage-based sampling when the statistical leverage scores are very nonuniform. And by solving a constrained optimization problem related to the error bound with an efficient bisection search we are able to achieve better performance than using either the leverage-based distribution or that proportional to the square root of the statistical leverage scores. Numerical simulations demonstrate the benefits of the new sampling distributions for low-rank matrix approximation and least square approximation compared to state-of-the art algorithms. version:1
arxiv-1505-00523 | Modeling Representation of Videos for Anomaly Detection using Deep Learning: A Review | http://arxiv.org/abs/1505.00523 | id:1505.00523 author:Yong Shean Chong, Yong Haur Tay category:cs.CV  published:2015-05-04 summary:This review article surveys the current progresses made toward video-based anomaly detection. We address the most fundamental aspect for video anomaly detection, that is, video feature representation. Much research works have been done in finding the right representation to perform anomaly detection in video streams accurately with an acceptable false alarm rate. However, this is very challenging due to large variations in environment and human movement, and high space-time complexity due to huge dimensionality of video data. The weakly supervised nature of deep learning algorithms can help in learning representations from the video data itself instead of manually designing the right feature for specific scenes. In this paper, we would like to review the existing methods of modeling video representations using deep learning techniques for the task of anomaly detection and action recognition. version:1
arxiv-1411-2861 | Computational Baby Learning | http://arxiv.org/abs/1411.2861 | id:1411.2861 author:Xiaodan Liang, Si Liu, Yunchao Wei, Luoqi Liu, Liang Lin, Shuicheng Yan category:cs.CV  published:2014-11-11 summary:Intuitive observations show that a baby may inherently possess the capability of recognizing a new visual concept (e.g., chair, dog) by learning from only very few positive instances taught by parent(s) or others, and this recognition capability can be gradually further improved by exploring and/or interacting with the real instances in the physical world. Inspired by these observations, we propose a computational model for slightly-supervised object detection, based on prior knowledge modelling, exemplar learning and learning with video contexts. The prior knowledge is modeled with a pre-trained Convolutional Neural Network (CNN). When very few instances of a new concept are given, an initial concept detector is built by exemplar learning over the deep features from the pre-trained CNN. Simulating the baby's interaction with physical world, the well-designed tracking solution is then used to discover more diverse instances from the massive online unlabeled videos. Once a positive instance is detected/identified with high score in each video, more variable instances possibly from different view-angles and/or different distances are tracked and accumulated. Then the concept detector can be fine-tuned based on these new instances. This process can be repeated again and again till we obtain a very mature concept detector. Extensive experiments on Pascal VOC-07/10/12 object detection datasets well demonstrate the effectiveness of our framework. It can beat the state-of-the-art full-training based performances by learning from very few samples for each object category, along with about 20,000 unlabeled videos. version:3
arxiv-1206-3713 | Learning the Structure and Parameters of Large-Population Graphical Games from Behavioral Data | http://arxiv.org/abs/1206.3713 | id:1206.3713 author:Jean Honorio, Luis Ortiz category:cs.LG cs.GT stat.ML  published:2012-06-16 summary:We consider learning, from strictly behavioral data, the structure and parameters of linear influence games (LIGs), a class of parametric graphical games introduced by Irfan and Ortiz (2014). LIGs facilitate causal strategic inference (CSI): Making inferences from causal interventions on stable behavior in strategic settings. Applications include the identification of the most influential individuals in large (social) networks. Such tasks can also support policy-making analysis. Motivated by the computational work on LIGs, we cast the learning problem as maximum-likelihood estimation (MLE) of a generative model defined by pure-strategy Nash equilibria (PSNE). Our simple formulation uncovers the fundamental interplay between goodness-of-fit and model complexity: good models capture equilibrium behavior within the data while controlling the true number of equilibria, including those unobserved. We provide a generalization bound establishing the sample complexity for MLE in our framework. We propose several algorithms including convex loss minimization (CLM) and sigmoidal approximations. We prove that the number of exact PSNE in LIGs is small, with high probability; thus, CLM is sound. We illustrate our approach on synthetic data and real-world U.S. congressional voting records. We briefly discuss our learning framework's generality and potential applicability to general graphical games. version:4
arxiv-1412-3397 | Sequential Labeling with online Deep Learning | http://arxiv.org/abs/1412.3397 | id:1412.3397 author:Gang Chen, Ran Xu, Sargur Srihari category:cs.LG 68T10 I.2.6  published:2014-12-10 summary:Deep learning has attracted great attention recently and yielded the state of the art performance in dimension reduction and classification problems. However, it cannot effectively handle the structured output prediction, e.g. sequential labeling. In this paper, we propose a deep learning structure, which can learn discriminative features for sequential labeling problems. More specifically, we add the inter-relationship between labels in our deep learning structure, in order to incorporate the context information from the sequential data. Thus, our model is more powerful than linear Conditional Random Fields (CRFs) because the objective function learns latent non-linear features so that target labeling can be better predicted. We pretrain the deep structure with stacked restricted Boltzmann machines (RBMs) for feature learning and optimize our objective function with online learning algorithm, a mixture of perceptron training and stochastic gradient descent. We test our model on different challenge tasks, and show that our model outperforms significantly over the completive baselines. version:3
arxiv-1504-01044 | Concept Drift Detection for Streaming Data | http://arxiv.org/abs/1504.01044 | id:1504.01044 author:Heng Wang, Zubin Abraham category:stat.ML cs.LG  published:2015-04-04 summary:Common statistical prediction models often require and assume stationarity in the data. However, in many practical applications, changes in the relationship of the response and predictor variables are regularly observed over time, resulting in the deterioration of the predictive performance of these models. This paper presents Linear Four Rates (LFR), a framework for detecting these concept drifts and subsequently identifying the data points that belong to the new concept (for relearning the model). Unlike conventional concept drift detection approaches, LFR can be applied to both batch and stream data; is not limited by the distribution properties of the response variable (e.g., datasets with imbalanced labels); is independent of the underlying statistical-model; and uses user-specified parameters that are intuitively comprehensible. The performance of LFR is compared to benchmark approaches using both simulated and commonly used public datasets that span the gamut of concept drift types. The results show LFR significantly outperforms benchmark approaches in terms of recall, accuracy and delay in detection of concept drifts across datasets. version:2
arxiv-1502-04416 | Random Subspace Learning Approach to High-Dimensional Outliers Detection | http://arxiv.org/abs/1502.04416 | id:1502.04416 author:Bohan Liu, Ernest Fokoue category:stat.ML 62H25  62H30  published:2015-02-16 summary:We introduce and develop a novel approach to outlier detection based on adaptation of random subspace learning. Our proposed method handles both high-dimension low-sample size and traditional low-dimensional high-sample size datasets. Essentially, we avoid the computational bottleneck of techniques like minimum covariance determinant (MCD) by computing the needed determinants and associated measures in much lower dimensional subspaces. Both theoretical and computational development of our approach reveal that it is computationally more efficient than the regularized methods in high-dimensional low-sample size, and often competes favorably with existing methods as far as the percentage of correct outlier detection is concerned. version:5
arxiv-1505-00482 | Risk Bounds For Mode Clustering | http://arxiv.org/abs/1505.00482 | id:1505.00482 author:Martin Azizyan, Yen-Chi Chen, Aarti Singh, Larry Wasserman category:math.ST cs.LG stat.ML stat.TH  published:2015-05-03 summary:Density mode clustering is a nonparametric clustering method. The clusters are the basins of attraction of the modes of a density estimator. We study the risk of mode-based clustering. We show that the clustering risk over the cluster cores --- the regions where the density is high --- is very small even in high dimensions. And under a low noise condition, the overall cluster risk is small even beyond the cores, in high dimensions. version:1
arxiv-1505-00477 | Kernel Spectral Clustering and applications | http://arxiv.org/abs/1505.00477 | id:1505.00477 author:Rocco Langone, Raghvendra Mall, Carlos Alzate, Johan A. K. Suykens category:cs.LG stat.ML  published:2015-05-03 summary:In this chapter we review the main literature related to kernel spectral clustering (KSC), an approach to clustering cast within a kernel-based optimization setting. KSC represents a least-squares support vector machine based formulation of spectral clustering described by a weighted kernel PCA objective. Just as in the classifier case, the binary clustering model is expressed by a hyperplane in a high dimensional space induced by a kernel. In addition, the multi-way clustering can be obtained by combining a set of binary decision functions via an Error Correcting Output Codes (ECOC) encoding scheme. Because of its model-based nature, the KSC method encompasses three main steps: training, validation, testing. In the validation stage model selection is performed to obtain tuning parameters, like the number of clusters present in the data. This is a major advantage compared to classical spectral clustering where the determination of the clustering parameters is unclear and relies on heuristics. Once a KSC model is trained on a small subset of the entire data, it is able to generalize well to unseen test points. Beyond the basic formulation, sparse KSC algorithms based on the Incomplete Cholesky Decomposition (ICD) and $L_0$, $L_1, L_0 + L_1$, Group Lasso regularization are reviewed. In that respect, we show how it is possible to handle large scale data. Also, two possible ways to perform hierarchical clustering and a soft clustering method are presented. Finally, real-world applications such as image segmentation, power load time-series clustering, document clustering and big data learning are considered. version:1
arxiv-1409-7842 | When Darwin meets Lorenz: Evolving new chaotic attractors through genetic programming | http://arxiv.org/abs/1409.7842 | id:1409.7842 author:Indranil Pan, Saptarshi Das category:nlin.CD cs.NE math.DS  published:2014-09-27 summary:In this paper, we propose a novel methodology for automatically finding new chaotic attractors through a computational intelligence technique known as multi-gene genetic programming (MGGP). We apply this technique to the case of the Lorenz attractor and evolve several new chaotic attractors based on the basic Lorenz template. The MGGP algorithm automatically finds new nonlinear expressions for the different state variables starting from the original Lorenz system. The Lyapunov exponents of each of the attractors are calculated numerically based on the time series of the state variables using time delay embedding techniques. The MGGP algorithm tries to search the functional space of the attractors by aiming to maximise the largest Lyapunov exponent (LLE) of the evolved attractors. To demonstrate the potential of the proposed methodology, we report over one hundred new chaotic attractor structures along with their parameters, which are evolved from just the Lorenz system alone. version:3
arxiv-1505-00444 | Some Theoretical Properties of a Network of Discretely Firing Neurons | http://arxiv.org/abs/1505.00444 | id:1505.00444 author:Stephen Luttrell category:cs.NE I.2.6; I.5.1  published:2015-05-03 summary:The problem of optimising a network of discretely firing neurons is addressed. An objective function is introduced which measures the average number of bits that are needed for the network to encode its state. When this is minimised, it is shown that this leads to a number of results, such as topographic mappings, piecewise linear dependence on the input of the probability of a neuron firing, and factorial encoder networks. version:1
arxiv-1505-00432 | Object Class Detection and Classification using Multi Scale Gradient and Corner Point based Shape Descriptors | http://arxiv.org/abs/1505.00432 | id:1505.00432 author:Basura Fernando, Sezer Karaoglu, Sajib Kumar Saha category:cs.CV  published:2015-05-03 summary:This paper presents a novel multi scale gradient and a corner point based shape descriptors. The novel multi scale gradient based shape descriptor is combined with generic Fourier descriptors to extract contour and region based shape information. Shape information based object class detection and classification technique with a random forest classifier has been optimized. Proposed integrated descriptor in this paper is robust to rotation, scale, translation, affine deformations, noisy contours and noisy shapes. The new corner point based interpolated shape descriptor has been exploited for fast object detection and classification with higher accuracy. version:1
arxiv-1505-00424 | Electron Neutrino Classification in Liquid Argon Time Projection Chamber Detector | http://arxiv.org/abs/1505.00424 | id:1505.00424 author:Piotr Płoński, Dorota Stefan, Robert Sulej, Krzysztof Zaremba category:cs.CV physics.ins-det  published:2015-05-03 summary:Neutrinos are one of the least known elementary particles. The detection of neutrinos is an extremely difficult task since they are affected only by weak sub-atomic force or gravity. Therefore large detectors are constructed to reveal neutrino's properties. Among them the Liquid Argon Time Projection Chamber (LAr-TPC) detectors provide excellent imaging and particle identification ability for studying neutrinos. The computerized methods for automatic reconstruction and identification of particles are needed to fully exploit the potential of the LAr-TPC technique. Herein, the novel method for electron neutrino classification is presented. The method constructs a feature descriptor from images of observed event. It characterizes the signal distribution propagated from vertex of interest, where the particle interacts with the detector medium. The classifier is learned with a constructed feature descriptor to decide whether the images represent the electron neutrino or cascade produced by photons. The proposed approach assumes that the position of primary interaction vertex is known. The method's performance in dependency to the noise in a primary vertex position and deposited energy of particles is studied. version:1
arxiv-1504-03871 | Bio-inspired Unsupervised Learning of Visual Features Leads to Robust Invariant Object Recognition | http://arxiv.org/abs/1504.03871 | id:1504.03871 author:Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Timothée Masquelier category:cs.CV q-bio.NC  published:2015-04-15 summary:Retinal image of surrounding objects varies tremendously due to the changes in position, size, pose, illumination condition, background context, occlusion, noise, and nonrigid deformations. But despite these huge variations, our visual system is able to invariantly recognize any object in just a fraction of a second. To date, various computational models have been proposed to mimic the hierarchical processing of the ventral visual pathway, with limited success. Here, we show that combining a biologically inspired network architecture with a biologically inspired learning rule significantly improves the models' performance when facing challenging object recognition problems. Our model is an asynchronous feedforward spiking neural network. When the network is presented with natural images, the neurons in the entry layers detect edges, and the most activated ones fire first, while neurons in higher layers are equipped with spike timing-dependent plasticity. These neurons progressively become selective to intermediate complexity visual features appropriate for object categorization, as demonstrated using the 3D Object dataset provided by Savarese et al. at CVGLab, Stanford University. The model reached 96% categorization accuracy, which corresponds to two to three times fewer errors than the previous state-of-the-art, demonstrating that it is able to accurately recognize different instances of multiple object classes in various appearance conditions (different views, scales, tilts, and backgrounds). Several statistical analysis techniques are used to show that our model extracts class specific and highly informative features. version:2
arxiv-1505-00423 | Optimal Time-Series Motifs | http://arxiv.org/abs/1505.00423 | id:1505.00423 author:Josif Grabocka, Nicolas Schilling, Lars Schmidt-Thieme category:cs.AI cs.LG  published:2015-05-03 summary:Motifs are the most repetitive/frequent patterns of a time-series. The discovery of motifs is crucial for practitioners in order to understand and interpret the phenomena occurring in sequential data. Currently, motifs are searched among series sub-sequences, aiming at selecting the most frequently occurring ones. Search-based methods, which try out series sub-sequence as motif candidates, are currently believed to be the best methods in finding the most frequent patterns. However, this paper proposes an entirely new perspective in finding motifs. We demonstrate that searching is non-optimal since the domain of motifs is restricted, and instead we propose a principled optimization approach able to find optimal motifs. We treat the occurrence frequency as a function and time-series motifs as its parameters, therefore we \textit{learn} the optimal motifs that maximize the frequency function. In contrast to searching, our method is able to discover the most repetitive patterns (hence optimal), even in cases where they do not explicitly occur as sub-sequences. Experiments on several real-life time-series datasets show that the motifs found by our method are highly more frequent than the ones found through searching, for exactly the same distance threshold. version:1
arxiv-1412-6618 | Permutohedral Lattice CNNs | http://arxiv.org/abs/1412.6618 | id:1412.6618 author:Martin Kiefel, Varun Jampani, Peter V. Gehler category:cs.CV cs.LG cs.NE  published:2014-12-20 summary:This paper presents a convolutional layer that is able to process sparse input features. As an example, for image recognition problems this allows an efficient filtering of signals that do not lie on a dense grid (like pixel position), but of more general features (such as color values). The presented algorithm makes use of the permutohedral lattice data structure. The permutohedral lattice was introduced to efficiently implement a bilateral filter, a commonly used image processing operation. Its use allows for a generalization of the convolution type found in current (spatial) convolutional network architectures. version:3
arxiv-1505-00412 | On a fast bilateral filtering formulation using functional rearrangements | http://arxiv.org/abs/1505.00412 | id:1505.00412 author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10  published:2015-05-03 summary:We introduce an exact reformulation of a broad class of neighborhood filters, among which the bilateral filters, in terms of two functional rearrangements: the decreasing and the relative rearrangements. Independently of the image spatial dimension (one-dimensional signal, image, volume of images, etc.), we reformulate these filters as integral operators defined in a one-dimensional space corresponding to the level sets measures. We prove the equivalence between the usual pixel-based version and the rearranged version of the filter. When restricted to the discrete setting, our reformulation of bilateral filters extends previous results for the so-called fast bilateral filtering. We, in addition, prove that the solution of the discrete setting, understood as constant-wise interpolators, converges to the solution of the continuous setting. Finally, we numerically illustrate computational aspects concerning quality approximation and execution time provided by the rearranged formulation. version:1
arxiv-1211-1323 | Sample Size Planning for Classification Models | http://arxiv.org/abs/1211.1323 | id:1211.1323 author:Claudia Beleites, Ute Neugebauer, Thomas Bocklitz, Christoph Krafft, Jürgen Popp category:stat.AP stat.ME stat.ML 92E99  97K80  62K99 G.3  published:2012-11-06 summary:In biospectroscopy, suitably annotated and statistically independent samples (e. g. patients, batches, etc.) for classifier training and testing are scarce and costly. Learning curves show the model performance as function of the training sample size and can help to determine the sample size needed to train good classifiers. However, building a good model is actually not enough: the performance must also be proven. We discuss learning curves for typical small sample size situations with 5 - 25 independent samples per class. Although the classification models achieve acceptable performance, the learning curve can be completely masked by the random testing uncertainty due to the equally limited test sample size. In consequence, we determine test sample sizes necessary to achieve reasonable precision in the validation and find that 75 - 100 samples will usually be needed to test a good but not perfect classifier. Such a data set will then allow refined sample size planning on the basis of the achieved performance. We also demonstrate how to calculate necessary sample sizes in order to show the superiority of one classifier over another: this often requires hundreds of statistically independent test samples or is even theoretically impossible. We demonstrate our findings with a data set of ca. 2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and three tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive simulation that allows precise determination of the actual performance of the models in question. version:3
arxiv-1411-6718 | LABR: A Large Scale Arabic Sentiment Analysis Benchmark | http://arxiv.org/abs/1411.6718 | id:1411.6718 author:Mahmoud Nabil, Mohamed Aly, Amir Atiya category:cs.CL cs.LG  published:2014-11-25 summary:We introduce LABR, the largest sentiment analysis dataset to-date for the Arabic language. It consists of over 63,000 book reviews, each rated on a scale of 1 to 5 stars. We investigate the properties of the dataset, and present its statistics. We explore using the dataset for two tasks: (1) sentiment polarity classification; and (2) ratings classification. Moreover, we provide standard splits of the dataset into training, validation and testing, for both polarity and ratings classification, in both balanced and unbalanced settings. We extend our previous work by performing a comprehensive analysis on the dataset. In particular, we perform an extended survey of the different classifiers typically used for the sentiment polarity classification problem. We also construct a sentiment lexicon from the dataset that contains both single and compound sentiment words and we explore its effectiveness. We make the dataset and experimental details publicly available. version:2
arxiv-1505-00401 | Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to LIFT, ROC & BIRD | http://arxiv.org/abs/1505.00401 | id:1505.00401 author:David M. W. Powers category:cs.LG cs.AI cs.IR stat.ME stat.ML  published:2015-05-03 summary:Evaluation often aims to reduce the correctness or error characteristics of a system down to a single number, but that always involves trade-offs. Another way of dealing with this is to quote two numbers, such as Recall and Precision, or Sensitivity and Specificity. But it can also be useful to see more than this, and a graphical approach can explore sensitivity to cost, prevalence, bias, noise, parameters and hyper-parameters. Moreover, most techniques are implicitly based on two balanced classes, and our ability to visualize graphically is intrinsically two dimensional, but we often want to visualize in a multiclass context. We review the dichotomous approaches relating to Precision, Recall, and ROC as well as the related LIFT chart, exploring how they handle unbalanced and multiclass data, and deriving new probabilistic and information theoretic variants of LIFT that help deal with the issues associated with the handling of multiple and unbalanced classes. version:1
arxiv-1505-00398 | Structured Block Basis Factorization for Scalable Kernel Matrix Evaluation | http://arxiv.org/abs/1505.00398 | id:1505.00398 author:Ruoxi Wang, Yingzhou Li, Michael W. Mahoney, Eric Darve category:stat.ML cs.LG cs.NA  published:2015-05-03 summary:Kernel matrices are popular in machine learning and scientific computing, but they are limited by their quadratic complexity in both construction and storage. It is well-known that as one varies the kernel parameter, e.g., the width parameter in radial basis function kernels, the kernel matrix changes from a smooth low-rank kernel to a diagonally-dominant and then fully-diagonal kernel. Low-rank approximation methods have been widely-studied, mostly in the first case, to reduce the memory storage and the cost of computing matrix-vector products. Here, we use ideas from scientific computing to propose an extension of these methods to situations where the matrix is not well-approximated by a low-rank matrix. In particular, we construct an efficient block low-rank approximation method---which we call the Block Basis Factorization---and we show that it has $\mathcal{O}(n)$ complexity in both time and memory. Our method works for a wide range of kernel parameters, extending the domain of applicability of low-rank approximation methods, and our empirical results demonstrate the stability (small standard deviation in error) and superiority over current state-of-art kernel approximation algorithms. version:1
arxiv-1505-00389 | Detail-preserving and Content-aware Variational Multi-view Stereo Reconstruction | http://arxiv.org/abs/1505.00389 | id:1505.00389 author:Zhaoxin Li, Kuanquan Wang, Wangmeng Zuo, Deyu Meng, Lei Zhang category:cs.CV  published:2015-05-03 summary:Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-view images is a fundamental yet active research area in computer vision. Despite the steady progress in multi-view stereo reconstruction, most existing methods are still limited in recovering fine-scale details and sharp features while suppressing noises, and may fail in reconstructing regions with few textures. To address these limitations, this paper presents a Detail-preserving and Content-aware Variational (DCV) multi-view stereo method, which reconstructs the 3D surface by alternating between reprojection error minimization and mesh denoising. In reprojection error minimization, we propose a novel inter-image similarity measure, which is effective to preserve fine-scale details of the reconstructed surface and builds a connection between guided image filtering and image registration. In mesh denoising, we propose a content-aware $\ell_{p}$-minimization algorithm by adaptively estimating the $p$ value and regularization parameters based on the current input. It is much more promising in suppressing noise while preserving sharp features than conventional isotropic mesh smoothing. Experimental results on benchmark datasets demonstrate that our DCV method is capable of recovering more surface details, and obtains cleaner and more accurate reconstructions than state-of-the-art methods. In particular, our method achieves the best results among all published methods on the Middlebury dino ring and dino sparse ring datasets in terms of both completeness and accuracy. version:1
