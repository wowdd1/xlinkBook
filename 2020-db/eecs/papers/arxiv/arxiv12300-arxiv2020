arxiv-1510-00772 | Machine Learning for Machine Data from a CATI Network | http://arxiv.org/abs/1510.00772 | id:1510.00772 author:Sou-Cheng T. Choi category:cs.LG  published:2015-10-03 summary:This is a machine learning application paper involving big data. We present high-accuracy prediction methods of rare events in semi-structured machine log files, which are produced at high velocity and high volume by NORC's computer-assisted telephone interviewing (CATI) network for conducting surveys. We judiciously apply natural language processing (NLP) techniques and data-mining strategies to train effective learning and prediction models for classifying uncommon error messages in the log---without access to source code, updated documentation or dictionaries. In particular, our simple but effective approach of features preallocation for learning from imbalanced data coupled with naive Bayes classifiers can be conceivably generalized to supervised or semi-supervised learning and prediction methods for other critical events such as cyberattack detection. version:1
arxiv-1510-00771 | Design and Analysis of a Single-Camera Omnistereo Sensor for Quadrotor Micro Aerial Vehicles (MAVs) | http://arxiv.org/abs/1510.00771 | id:1510.00771 author:Carlos Jaramillo category:cs.CV cs.RO cs.SY  published:2015-10-03 summary:We describe the design and 3D sensing performance of an omnidirectional stereo-vision system (omnistereo) as applied to Micro Aerial Vehicles (MAVs). The proposed omnistereo model employs a monocular camera that is co-axially aligned with a pair of hyperboloidal mirrors (folded catadioptric configuration). We show that this arrangement is practical for performing stereo-vision when mounted on top of propeller-based MAVs characterized by low payloads. The theoretical single viewpoint (SVP) constraint helps us derive analytical solutions for the sensor's projective geometry and generate SVP-compliant panoramic images to compute 3D information from stereo correspondences (in a truly synchronous fashion). We perform an extensive analysis on various system characteristics such as its size, catadioptric spatial resolution, field-of-view. In addition, we pose a probabilistic model for uncertainty estimation of the depth from triangulation for skew back-projection rays. We expect to motivate the reproducibility of our solution since it can be adapted (optimally) to other catadioptric-based omnistereo vision applications. version:1
arxiv-1510-00760 | P-trac Procedure: The Dispersion and Neutralization of Contrasts in Lexicon | http://arxiv.org/abs/1510.00760 | id:1510.00760 author:Afshin Rahimi, Bahram Vazirnezhad, Moharram Eslami category:cs.CL  published:2015-10-03 summary:Cognitive acoustic cues have an important role in shaping the phonological structure of language as a means to optimal communication. In this paper we introduced P-trac procedure in order to track dispersion of contrasts in different contexts in lexicon. The results of applying P-trac procedure to the case of dispersion of contrasts in pre- consonantal contexts and in consonantal positions of CVCC sequences in Persian provide Evidence in favor of phonetic basis of dispersion argued by Licensing by Cue hypothesis and the Dispersion Theory of Contrast. The P- trac procedure is proved to be very effective in revealing the dispersion of contrasts in lexicon especially when comparing the dispersion of contrasts in different contexts. version:1
arxiv-1510-00759 | It is not all downhill from here: Syllable Contact Law in Persian | http://arxiv.org/abs/1510.00759 | id:1510.00759 author:Afshin Rahimi, Moharram Eslami, Bahram Vazirnezhad category:cs.CL  published:2015-10-03 summary:Syllable contact pairs crosslinguistically tend to have a falling sonority slope a constraint which is called the Syllable Contact Law SCL In this study the phonotactics of syllable contacts in 4202 CVCCVC words of Persian lexicon is investigated The consonants of Persian were divided into five sonority categories and the frequency of all possible sonority slopes is computed both in lexicon type frequency and in corpus token frequency Since an unmarked phonological structure has been shown to diachronically become more frequent we expect to see the same pattern for syllable contact pairs with falling sonority slope The correlation of sonority categories of the two consonants in a syllable contact pair is measured using Pointwise Mutual Information version:1
arxiv-1510-00756 | Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width | http://arxiv.org/abs/1510.00756 | id:1510.00756 author:Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré category:cs.LG  published:2015-10-02 summary:Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width---regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers. version:1
arxiv-1506-06438 | Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms | http://arxiv.org/abs/1506.06438 | id:1506.06438 author:Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher Ré category:cs.LG math.OC stat.ML  published:2015-06-22 summary:Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we use our new analysis in three ways: (1) we derive convergence rates for the convex case (Hogwild!) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called Buckwild!, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware. version:2
arxiv-1510-00745 | WHOI-Plankton- A Large Scale Fine Grained Visual Recognition Benchmark Dataset for Plankton Classification | http://arxiv.org/abs/1510.00745 | id:1510.00745 author:Eric C. Orenstein, Oscar Beijbom, Emily E. Peacock, Heidi M. Sosik category:cs.CV  published:2015-10-02 summary:Planktonic organisms are of fundamental importance to marine ecosystems: they form the basis of the food web, provide the link between the atmosphere and the deep ocean, and influence global-scale biogeochemical cycles. Scientists are increasingly using imaging-based technologies to study these creatures in their natural habit. Images from such systems provide an unique opportunity to model and understand plankton ecosystems, but the collected datasets can be enormous. The Imaging FlowCytobot (IFCB) at Woods Hole Oceanographic Institution, for example, is an \emph{in situ} system that has been continuously imaging plankton since 2006. To date, it has generated more than 700 million samples. Manual classification of such a vast image collection is impractical due to the size of the data set. In addition, the annotation task is challenging due to the large space of relevant classes, intra-class variability, and inter-class similarity. Methods for automated classification exist, but the accuracy is often below that of human experts. Here we introduce WHOI-Plankton: a large scale, fine-grained visual recognition dataset for plankton classification, which comprises over 3.4 million expert-labeled images across 70 classes. The labeled image set is complied from over 8 years of near continuous data collection with the IFCB at the Martha's Vineyard Coastal Observatory (MVCO). We discuss relevant metrics for evaluation of classification performance and provide results for a traditional method based on hand-engineered features and two methods based on convolutional neural networks. version:1
arxiv-1510-00726 | A Primer on Neural Network Models for Natural Language Processing | http://arxiv.org/abs/1510.00726 | id:1510.00726 author:Yoav Goldberg category:cs.CL  published:2015-10-02 summary:Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation. version:1
arxiv-1510-00701 | Minimax Lower Bounds for Noisy Matrix Completion Under Sparse Factor Models | http://arxiv.org/abs/1510.00701 | id:1510.00701 author:Abhinav V. Sambasivan, Jarvis D. Haupt category:cs.IT math.IT stat.ML  published:2015-10-02 summary:This paper examines fundamental error characteristics for a general class of matrix completion problems, where matrix of interest is a product of two a priori unknown matrices, one of which is sparse, and the observations are noisy. Our main contributions come in the form of minimax lower bounds for the expected per-element squared error for these problems under several noise/corruption models; specifically, we analyze scenarios where the corruptions are characterized by additive Gaussian noise or additive heavier-tailed (Laplace) noise, Poisson-distributed observations, and highly-quantized (e.g., one-bit) observations. Our results establish that the error bounds derived in (Soni et al., 2014) for complexity-regularized maximum likelihood estimators achieve, up to multiplicative constant and logarithmic factors, the minimax error rates in each of these noise scenarios, provided the sparse factor exhibits linear sparsity. version:1
arxiv-1508-01585 | Applying Deep Learning to Answer Selection: A Study and An Open Task | http://arxiv.org/abs/1508.01585 | id:1508.01585 author:Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, Bowen Zhou category:cs.CL cs.LG  published:2015-08-07 summary:We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied to different languages or domains. Various architectures are presented and compared. We create and release a QA corpus and setup a new QA task in the insurance domain. Experimental results demonstrate superior performance compared to the baseline methods and various technologies give further improvements. For this highly challenging task, the top-1 accuracy can reach up to 65.3% on a test set, which indicates a great potential for practical use. version:2
arxiv-1509-01208 | Fast Clustering and Topic Modeling Based on Rank-2 Nonnegative Matrix Factorization | http://arxiv.org/abs/1509.01208 | id:1509.01208 author:Da Kuang, Barry Drake, Haesun Park category:cs.LG cs.IR cs.NA F.2.1; H.3.3  published:2015-09-03 summary:The importance of unsupervised clustering and topic modeling is well recognized with ever-increasing volumes of text data. In this paper, we propose a fast method for hierarchical clustering and topic modeling called HierNMF2. Our method is based on fast Rank-2 nonnegative matrix factorization (NMF) that performs binary clustering and an efficient node splitting rule. Further utilizing the final leaf nodes generated in HierNMF2 and the idea of nonnegative least squares fitting, we propose a new clustering/topic modeling method called FlatNMF2 that recovers a flat clustering/topic modeling result in a very simple yet significantly more effective way than any other existing methods. We implement highly optimized open source software in C++ for both HierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topic modeling of document data sets. Substantial experimental tests are presented that illustrate significant improvements both in computational time as well as quality of solutions. We compare our methods to other clustering methods including K-means, standard NMF, and CLUTO, and also topic modeling methods including latent Dirichlet allocation (LDA) and recently proposed algorithms for NMF with separability constraints. Overall, we present efficient tools for analyzing large-scale data sets, and techniques that can be generalized to many other data analytics problem domains. version:3
arxiv-1510-00633 | Distributed Multitask Learning | http://arxiv.org/abs/1510.00633 | id:1510.00633 author:Jialei Wang, Mladen Kolar, Nathan Srebro category:stat.ML cs.LG  published:2015-10-02 summary:We consider the problem of distributed multi-task learning, where each machine learns a separate, but related, task. Specifically, each machine learns a linear predictor in high-dimensional space,where all tasks share the same small support. We present a communication-efficient estimator based on the debiased lasso and show that it is comparable with the optimal centralized method. version:1
arxiv-1410-7328 | Exact Expression For Information Distance | http://arxiv.org/abs/1410.7328 | id:1410.7328 author:P. M. B. Vitanyi category:cs.IT cs.CC cs.CV cs.DM math.IT  published:2014-10-27 summary:Information distance can be defined not only between two strings but also in a finite multiset of strings of cardinality greater than two. We give an elementary proof for expressing the information distance. It is exact since for each cardinality of the multiset the lower bound for some multiset equals the upper bound for all multisets up to a constant additive term. We discuss overlap. version:9
arxiv-1510-00627 | Multi-armed Bandits with Application to 5G Small Cells | http://arxiv.org/abs/1510.00627 | id:1510.00627 author:Setareh Maghsudi, Ekram Hossain category:cs.LG cs.DC cs.NI  published:2015-10-02 summary:Due to the pervasive demand for mobile services, next generation wireless networks are expected to be able to deliver high date rates while wireless resources become more and more scarce. This requires the next generation wireless networks to move towards new networking paradigms that are able to efficiently support resource-demanding applications such as personalized mobile services. Examples of such paradigms foreseen for the emerging fifth generation (5G) cellular networks include very densely deployed small cells and device-to-device communications. For 5G networks, it will be imperative to search for spectrum and energy-efficient solutions to the resource allocation problems that i) are amenable to distributed implementation, ii) are capable of dealing with uncertainty and lack of information, and iii) can cope with users' selfishness. The core objective of this article is to investigate and to establish the potential of multi-armed bandit (MAB) framework to address this challenge. In particular, we provide a brief tutorial on bandit problems, including different variations and solution approaches. Furthermore, we discuss recent applications as well as future research directions. In addition, we provide a detailed example of using an MAB model for energy-efficient small cell planning in 5G networks. version:1
arxiv-1409-5786 | Fast Low-rank Representation based Spatial Pyramid Matching for Image Classification | http://arxiv.org/abs/1409.5786 | id:1409.5786 author:Xi Peng, Rui Yan, Bo Zhao, Huajin Tang, Zhang Yi category:cs.CV  published:2014-09-22 summary:Spatial Pyramid Matching (SPM) and its variants have achieved a lot of success in image classification. The main difference among them is their encoding schemes. For example, ScSPM incorporates Sparse Code (SC) instead of Vector Quantization (VQ) into the framework of SPM. Although the methods achieve a higher recognition rate than the traditional SPM, they consume more time to encode the local descriptors extracted from the image. In this paper, we propose using Low Rank Representation (LRR) to encode the descriptors under the framework of SPM. Different from SC, LRR considers the group effect among data points instead of sparsity. Benefiting from this property, the proposed method (i.e., LrrSPM) can offer a better performance. To further improve the generalizability and robustness, we reformulate the rank-minimization problem as a truncated projection problem. Extensive experimental studies show that LrrSPM is more efficient than its counterparts (e.g., ScSPM) while achieving competitive recognition rates on nine image data sets. version:2
arxiv-1507-04886 | Distinguishing short and long $Fermi$ gamma-ray bursts | http://arxiv.org/abs/1507.04886 | id:1507.04886 author:Mariusz Tarnopolski category:astro-ph.HE astro-ph.CO stat.ML  published:2015-07-17 summary:Two classes of gamma-ray bursts (GRBs), short and long, have been determined without any doubts, and are usually ascribed to different progenitors, yet these classes overlap for a variety of descriptive parameters. A subsample of 46 long and 22 short $Fermi$ GRBs with estimated Hurst Exponents (HEs), complemented by minimum variability time-scales (MVTS) and durations ($T_{90}$) is used to perform a supervised Machine Learning (ML) and Monte Carlo (MC) simulation using a Support Vector Machine (SVM) algorithm. It is found that while $T_{90}$ itself performs very well in distinguishing short and long GRBs, the overall success ratio is higher when the training set is complemented by MVTS and HE. These results may allow to introduce a new (non-linear) parameter that might provide less ambiguous classification of GRBs. version:4
arxiv-1510-00562 | Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks | http://arxiv.org/abs/1510.00562 | id:1510.00562 author:Lin Sun, Kui Jia, Dit-Yan Yeung, Bertram E. Shi category:cs.CV  published:2015-10-02 summary:Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos. version:1
arxiv-1502-01908 | Marginalizing Gaussian Process Hyperparameters using Sequential Monte Carlo | http://arxiv.org/abs/1502.01908 | id:1502.01908 author:Andreas Svensson, Johan Dahlin, Thomas B. Schön category:stat.ML stat.CO  published:2015-02-06 summary:Gaussian process regression is a popular method for non-parametric probabilistic modeling of functions. The Gaussian process prior is characterized by so-called hyperparameters, which often have a large influence on the posterior model and can be difficult to tune. This work provides a method for numerical marginalization of the hyperparameters, relying on the rigorous framework of sequential Monte Carlo. Our method is well suited for online problems, and we demonstrate its ability to handle real-world problems with several dimensions and compare it to other marginalization methods. We also conclude that our proposed method is a competitive alternative to the commonly used point estimates maximizing the likelihood, both in terms of computational load and its ability to handle multimodal posteriors. version:2
arxiv-1505-00663 | See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG | http://arxiv.org/abs/1505.00663 | id:1505.00663 author:Wei-Chen Chiu, Mario Fritz category:cs.CV  published:2015-05-04 summary:The Histogram of Oriented Gradient (HOG) descriptor has led to many advances in computer vision over the last decade and is still part of many state of the art approaches. We realize that the associated feature computation is piecewise differentiable and therefore many pipelines which build on HOG can be made differentiable. This lends to advanced introspection as well as opportunities for end-to-end optimization. We present our implementation of $\nabla$HOG based on the auto-differentiation toolbox Chumpy and show applications to pre-image visualization and pose estimation which extends the existing differentiable renderer OpenDR pipeline. Both applications improve on the respective state-of-the-art HOG approaches. version:4
arxiv-1510-00542 | Local Higher-Order Statistics (LHS) describing images with statistics of local non-binarized pixel patterns | http://arxiv.org/abs/1510.00542 | id:1510.00542 author:Gaurav Sharma, Frederic Jurie category:cs.CV  published:2015-10-02 summary:We propose a new image representation for texture categorization and facial analysis, relying on the use of higher-order local differential statistics as features. It has been recently shown that small local pixel pattern distributions can be highly discriminative while being extremely efficient to compute, which is in contrast to the models based on the global structure of images. Motivated by such works, we propose to use higher-order statistics of local non-binarized pixel patterns for the image description. The proposed model does not require either (i) user specified quantization of the space (of pixel patterns) or (ii) any heuristics for discarding low occupancy volumes of the space. We propose to use a data driven soft quantization of the space, with parametric mixture models, combined with higher-order statistics, based on Fisher scores. We demonstrate that this leads to a more expressive representation which, when combined with discriminatively learned classifiers and metrics, achieves state-of-the-art performance on challenging texture and facial analysis datasets, in low complexity setup. Further, it is complementary to higher complexity features and when combined with them improves performance. version:1
arxiv-1408-2327 | On the Consistency of Ordinal Regression Methods | http://arxiv.org/abs/1408.2327 | id:1408.2327 author:Fabian Pedregosa, Francis Bach, Alexandre Gramfort category:cs.LG  published:2014-08-11 summary:Many of the ordinal regression models that have been proposed in the literature can be seen as methods that minimize a convex surrogate of the zero-one, absolute, or squared loss functions. A key property that allows to study the statistical implications of such approximations is that of Fisher consistency. In this paper we will characterize the Fisher consistency of a rich family of surrogate loss functions used in the context of ordinal regression, including support vector ordinal regression, ORBoosting and least absolute deviation. We will see that, for a family of surrogate loss functions that subsumes support vector ordinal regression and ORBoosting, consistency can be fully characterized by the derivative of a real-valued function at zero, as happens for convex margin-based surrogates in binary classification. We also derive excess risk bounds for a surrogate of the absolute error that generalize existing risk bounds for binary classification. Finally, our analysis suggests a novel surrogate of the squared error loss. To prove the empirical performance of such surrogate, we benchmarked it in terms of cross-validation error on 9 different datasets, where it outperforms competing approaches on 7 out of 9 datasets. version:7
arxiv-1510-00479 | Effective Object Tracking in Unstructured Crowd Scenes | http://arxiv.org/abs/1510.00479 | id:1510.00479 author:Ishan Jindal, Shanmuganathan Raman category:cs.CV  published:2015-10-02 summary:In this paper, we are presenting a rotation variant Oriented Texture Curve (OTC) descriptor based mean shift algorithm for tracking an object in an unstructured crowd scene. The proposed algorithm works by first obtaining the OTC features for a manually selected object target, then a visual vocabulary is created by using all the OTC features of the target. The target histogram is obtained using codebook encoding method which is then used in mean shift framework to perform similarity search. Results are obtained on different videos of challenging scenes and the comparison of the proposed approach with several state-of-the-art approaches are provided. The analysis shows the advantages and limitations of the proposed approach for tracking an object in unstructured crowd scenes. version:1
arxiv-1510-00477 | Learning a Discriminative Model for the Perception of Realism in Composite Images | http://arxiv.org/abs/1510.00477 | id:1510.00477 author:Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, Alexei A. Efros category:cs.CV  published:2015-10-02 summary:What makes an image appear realistic? In this work, we are answering this question from a data-driven perspective by learning the perception of visual realism directly from large amounts of data. In particular, we train a Convolutional Neural Network (CNN) model that distinguishes natural photographs from automatically generated composite images. The model learns to predict visual realism of a scene in terms of color, lighting and texture compatibility, without any human annotations pertaining to it. Our model outperforms previous works that rely on hand-crafted heuristics, for the task of classifying realistic vs. unrealistic photos. Furthermore, we apply our learned model to compute optimal parameters of a compositing method, to maximize the visual realism score predicted by our CNN model. We demonstrate its advantage against existing methods via a human perception study. version:1
arxiv-1504-06692 | Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images | http://arxiv.org/abs/1504.06692 | id:1504.06692 author:Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, Zhiheng Huang, Alan Yuille category:cs.CV cs.CL cs.LG  published:2015-04-25 summary:In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on m-RNN with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is http://www.stat.ucla.edu/~junhua.mao/projects/child_learning.html version:2
arxiv-1508-02449 | Towards Machine Wald | http://arxiv.org/abs/1508.02449 | id:1508.02449 author:Houman Owhadi, Clint Scovel category:math.ST cs.LG stat.TH 62C99  68Q32  published:2015-08-10 summary:The past century has seen a steady increase in the need of estimating and predicting complex systems and making (possibly critical) decisions with limited information. Although computers have made possible the numerical evaluation of sophisticated statistical models, these models are still designed \emph{by humans} because there is currently no known recipe or algorithm for dividing the design of a statistical model into a sequence of arithmetic operations. Indeed enabling computers to \emph{think} as \emph{humans} have the ability to do when faced with uncertainty is challenging in several major ways: (1) Finding optimal statistical models remains to be formulated as a well posed problem when information on the system of interest is incomplete and comes in the form of a complex combination of sample data, partial knowledge of constitutive relations and a limited description of the distribution of input random variables. (2) The space of admissible scenarios along with the space of relevant information, assumptions, and/or beliefs, tend to be infinite dimensional, whereas calculus on a computer is necessarily discrete and finite. With this purpose, this paper explores the foundations of a rigorous framework for the scientific computation of optimal statistical estimators/models and reviews their connections with Decision Theory, Machine Learning, Bayesian Inference, Stochastic Optimization, Robust Optimization, Optimal Uncertainty Quantification and Information Based Complexity. version:2
arxiv-1510-00436 | Response to Liu, Xu, and Liang (2015) and Ferrer-i-Cancho and Gómez-Rodríguez (2015) on Dependency Length Minimization | http://arxiv.org/abs/1510.00436 | id:1510.00436 author:Richard Futrell, Kyle Mahowald, Edward Gibson category:cs.CL  published:2015-10-01 summary:We address recent criticisms (Liu et al., 2015; Ferrer-i-Cancho and G\'omez-Rodr\'iguez, 2015) of our work on empirical evidence of dependency length minimization across languages (Futrell et al., 2015). First, we acknowledge error in failing to acknowledge Liu (2008)'s previous work on corpora of 20 languages with similar aims. A correction will appear in PNAS. Nevertheless, we argue that our work provides novel, strong evidence for dependency length minimization as a universal quantitative property of languages, beyond this previous work, because it provides baselines which focus on word order preferences. Second, we argue that our choices of baselines were appropriate because they control for alternative theories. version:1
arxiv-1510-00419 | An Asynchronous Implementation of the Limited Memory CMA-ES | http://arxiv.org/abs/1510.00419 | id:1510.00419 author:Viktor Arkhipov, Maxim Buzdalov, Anatoly Shalyto category:cs.NE 90C56 G.1.6; I.2.8  published:2015-10-01 summary:We present our asynchronous implementation of the LM-CMA-ES algorithm, which is a modern evolution strategy for solving complex large-scale continuous optimization problems. Our implementation brings the best results when the number of cores is relatively high and the computational complexity of the fitness function is also high. The experiments with benchmark functions show that it is able to overcome its origin on the Sphere function, reaches certain thresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly performs much better than the original version on the Rastrigin function. version:1
arxiv-1502-04148 | A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA | http://arxiv.org/abs/1502.04148 | id:1502.04148 author:James Voss, Mikhail Belkin, Luis Rademacher category:cs.LG stat.ML  published:2015-02-13 summary:Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite "inner product") space. The use of this indefinite "inner product" resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing. Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise. version:2
arxiv-1510-00244 | RDF Knowledge Graph Visualization From a Knowledge Extraction System | http://arxiv.org/abs/1510.00244 | id:1510.00244 author:Fadhela Kerdjoudj, Olivier Curé category:cs.HC cs.CL  published:2015-10-01 summary:In this paper, we present a system to visualize RDF knowledge graphs. These graphs are obtained from a knowledge extraction system designed by GEOLSemantics. This extraction is performed using natural language processing and trigger detection. The user can visualize subgraphs by selecting some ontology features like concepts or individuals. The system is also multilingual, with the use of the annotated ontology in English, French, Arabic and Chinese. version:1
arxiv-1510-00240 | Determination of the Internet Anonymity Influence on the Level of Aggression and Usage of Obscene Lexis | http://arxiv.org/abs/1510.00240 | id:1510.00240 author:Rodmonga Potapova, Denis Gordeev category:cs.CL  published:2015-10-01 summary:This article deals with the analysis of the semantic content of the anonymous Russian-speaking forum 2ch.hk, different verbal means of expressing of the emotional state of aggression are revealed for this site, and aggression is classified by its directions. The lexis of different Russian-and English- speaking anonymous forums (2ch.hk and iichan.hk, 4chan.org) and public community "MDK" of the Russian-speaking social network VK is analyzed and compared with the Open Corpus of the Russian language (Opencorpora.org and Brown corpus). The analysis shows that anonymity has no influence on the amount of invective items usage. The effectiveness of moderation was shown for anonymous forums. It was established that Russian obscene lexis was used to express the emotional state of aggression only in 60.4% of cases for 2ch.hk. These preliminary results show that the Russian obscene lexis on the Internet does not have direct dependence on the emotional state of aggression. version:1
arxiv-1510-00203 | Data Association for an Adaptive Multi-target Particle Filter Tracking System | http://arxiv.org/abs/1510.00203 | id:1510.00203 author:R. Alampay, K. Teknomo category:cs.CV I.5.4  published:2015-10-01 summary:This paper presents a novel approach to improve the accuracy of tracking multiple objects in a static scene using a particle filter system by introducing a data association step, a state queue for the collection of tracked objects and adaptive parameters to the system. The data association step makes use of the object detection phase and appearance model to determine if the approximated targets given by the particle filter step match the given set of detected objects. The remaining detected objects are used as information to instantiate new objects for tracking. State queues are also used for each tracked object to deal with occlusion events and occlusion recovery. Finally we present how the parameters adjust to occlusion events. The adaptive property of the system is also used for possible occlusion recovery. Results of the system are then compared to a ground truth data set for performance evaluation. Our system produced accurate results and was able to handle partially occluded objects as well as proper occlusion recovery from tracking multiple objects version:1
arxiv-1504-06260 | First Steps Towards a Runtime Comparison of Natural and Artificial Evolution | http://arxiv.org/abs/1504.06260 | id:1504.06260 author:Tiago Paixão, Jorge Pérez Heredia, Dirk Sudholt, Barbora Trubenová category:cs.NE F.2.2  published:2015-04-23 summary:Evolutionary algorithms (EAs) form a popular optimisation paradigm inspired by natural evolution. In recent years the field of evolutionary computation has developed a rigorous analytical theory to analyse their runtime on many illustrative problems. Here we apply this theory to a simple model of natural evolution. In the Strong Selection Weak Mutation (SSWM) evolutionary regime the time between occurrence of new mutations is much longer than the time it takes for a new beneficial mutation to take over the population. In this situation, the population only contains copies of one genotype and evolution can be modelled as a (1+1)-type process where the probability of accepting a new genotype (improvements or worsenings) depends on the change in fitness. We present an initial runtime analysis of SSWM, quantifying its performance for various parameters and investigating differences to the (1+1)EA. We show that SSWM can have a moderate advantage over the (1+1)EA at crossing fitness valleys and study an example where SSWM outperforms the (1+1)EA by taking advantage of information on the fitness gradient. version:2
arxiv-1505-01121 | Ask Your Neurons: A Neural-based Approach to Answering Questions about Images | http://arxiv.org/abs/1505.01121 | id:1505.01121 author:Mateusz Malinowski, Marcus Rohrbach, Mario Fritz category:cs.CV cs.AI cs.CL  published:2015-05-05 summary:We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus. version:3
arxiv-1503-08677 | Label-Embedding for Image Classification | http://arxiv.org/abs/1503.08677 | id:1503.08677 author:Zeynep Akata, Florent Perronnin, Zaid Harchaoui, Cordelia Schmid category:cs.CV  published:2015-03-30 summary:Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as e.g. class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples. version:2
arxiv-1510-00165 | Using consumer behavior data to reduce energy consumption in smart homes | http://arxiv.org/abs/1510.00165 | id:1510.00165 author:Daniel Schweizer, Michael Zehnder, Holger Wache, Hans-Friedrich Witschel, Danilo Zanatta, Miguel Rodriguez category:cs.CY stat.ML  published:2015-10-01 summary:This paper discusses how usage patterns and preferences of inhabitants can be learned efficiently to allow smart homes to autonomously achieve energy savings. We propose a frequent sequential pattern mining algorithm suitable for real-life smart home event data. The performance of the proposed algorithm is compared to existing algorithms regarding completeness/correctness of the results, run times as well as memory consumption and elaborates on the shortcomings of the different solutions. We also present a recommender system based on the developed algorithm that provides recommendations to the users to reduce their energy consumption. The recommender system was deployed to a set of test homes. The test participants rated the impact of the recommendations on their comfort. We used this feedback to adjust the system parameters and make it more accurate during a second test phase. version:1
arxiv-1507-08566 | Diffusion Adaptation Over Clustered Multitask Networks Based on the Affine Projection Algorithm | http://arxiv.org/abs/1507.08566 | id:1507.08566 author:Vinay Chakravarthi Gogineni, Mrityunjoy Chakraborty category:cs.DC cs.SY math.ST stat.ML stat.TH  published:2015-07-29 summary:Distributed adaptive networks achieve better estimation performance by exploiting temporal and as well spatial diversity while consuming few resources. Recent works have studied the single task distributed estimation problem, in which the nodes estimate a single optimum parameter vector collaboratively. However, there are many important applications where the multiple vectors have to estimated simultaneously, in a collaborative manner. This paper presents multi-task diffusion strategies based on the Affine Projection Algorithm (APA), usage of APA makes the algorithm robust against the correlated input. The performance analysis of the proposed multi-task diffusion APA algorithm is studied in mean and mean square sense. And also a modified multi-task diffusion strategy is proposed that improves the performance in terms of convergence rate and steady state EMSE as well. Simulations are conducted to verify the analytical results. version:4
arxiv-1510-00132 | Disk storage management for LHCb based on Data Popularity estimator | http://arxiv.org/abs/1510.00132 | id:1510.00132 author:Mikhail Hushchyn, Philippe Charpentier, Andrey Ustyuzhanin category:cs.DC cs.LG physics.data-an  published:2015-10-01 summary:This paper presents an algorithm providing recommendations for optimizing the LHCb data storage. The LHCb data storage system is a hybrid system. All datasets are kept as archives on magnetic tapes. The most popular datasets are kept on disks. The algorithm takes the dataset usage history and metadata (size, type, configuration etc.) to generate a recommendation report. This article presents how we use machine learning algorithms to predict future data popularity. Using these predictions it is possible to estimate which datasets should be removed from disk. We use regression algorithms and time series analysis to find the optimal number of replicas for datasets that are kept on disk. Based on the data popularity and the number of replicas optimization, the algorithm minimizes a loss function to find the optimal data distribution. The loss function represents all requirements for data distribution in the data storage system. We demonstrate how our algorithm helps to save disk space and to reduce waiting times for jobs using this data. version:1
arxiv-1509-09060 | Multi-objective Differential Evolution with Helper Functions for Constrained Optimization | http://arxiv.org/abs/1509.09060 | id:1509.09060 author:Tao Xu, Jun He category:cs.NE  published:2015-09-30 summary:Solving constrained optimization problems by multi-objective evolutionary algorithms has scored tremendous achievements in the last decade. Standard multi-objective schemes usually aim at minimizing the objective function and also the degree of constraint violation simultaneously. This paper proposes a new multi-objective method for solving constrained optimization problems. The new method keeps two standard objectives: the original objective function and the sum of degrees of constraint violation. But besides them, four more objectives are added. One is based on the feasible rule. The other three come from the penalty functions. This paper conducts an initial experimental study on thirteen benchmark functions. A simplified version of CMODE is applied to solving multi-objective optimization problems. Our initial experimental results confirm our expectation that adding more helper functions could be useful. The performance of SMODE with more helper functions (four or six) is better than that with only two helper functions. version:2
arxiv-1509-08147 | Amodal Completion and Size Constancy in Natural Scenes | http://arxiv.org/abs/1509.08147 | id:1509.08147 author:Abhishek Kar, Shubham Tulsiani, João Carreira, Jitendra Malik category:cs.CV  published:2015-09-27 summary:We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completion to infer veridical sizes in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scaling ambiguities and we demonstrate qualitative results on challenging real-world scenes. version:2
arxiv-1502-04149 | Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation | http://arxiv.org/abs/1502.04149 | id:1502.04149 author:Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis category:cs.SD cs.AI cs.LG cs.MM  published:2015-02-13 summary:Monaural source separation is important for many real world applications. It is challenging because, with only a single channel of information available, without any constraints, an infinite number of solutions are possible. In this paper, we explore joint optimization of masking functions and deep recurrent neural networks for monaural source separation tasks, including monaural speech separation, monaural singing voice separation, and speech denoising. The joint optimization of the deep recurrent neural networks with an extra masking layer enforces a reconstruction constraint. Moreover, we explore a discriminative criterion for training neural networks to further enhance the separation performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT datasets for speech separation, singing voice separation, and speech denoising tasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to NMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and 4.32--5.42 dB GSIR gain compared to existing models in the singing voice separation task, and outperform NMF and DNN baselines in the speech denoising task. version:4
arxiv-1510-00095 | Supporting Regularized Logistic Regression Privately and Efficiently | http://arxiv.org/abs/1510.00095 | id:1510.00095 author:Wenfa Li, Hongzhe Liu, Peng Yang, Wei Xie category:cs.LG cs.CR q-bio.GN  published:2015-10-01 summary:As one of the most popular statistical and machine learning models, logistic regression with regularization has found wide adoption in biomedicine, social sciences, information technology, and so on. These domains often involve data of human subjects that are contingent upon strict privacy regulations. Increasing concerns over data privacy make it more and more difficult to coordinate and conduct large-scale collaborative studies, which typically rely on cross-institution data sharing and joint analysis. Our work here focuses on safeguarding regularized logistic regression, a widely-used machine learning model in various disciplines while at the same time has not been investigated from a data security and privacy perspective. We consider a common use scenario of multi-institution collaborative studies, such as in the form of research consortia or networks as widely seen in genetics, epidemiology, social sciences, etc. To make our privacy-enhancing solution practical, we demonstrate a non-conventional and computationally efficient method leveraging distributing computing and strong cryptography to provide comprehensive protection over individual-level and summary data. Extensive empirical evaluation on several studies validated the privacy guarantees, efficiency and scalability of our proposal. We also discuss the practical implications of our solution for large-scale studies and applications from various disciplines, including genetic and biomedical studies, smart grid, network analysis, etc. version:1
arxiv-1510-00087 | Clamping Improves TRW and Mean Field Approximations | http://arxiv.org/abs/1510.00087 | id:1510.00087 author:Adrian Weller, Justin Domke category:cs.LG cs.AI stat.ML  published:2015-10-01 summary:We examine the effect of clamping variables for approximate inference in undirected graphical models with pairwise relationships and discrete variables. For any number of variable labels, we demonstrate that clamping and summing approximate sub-partition functions can lead only to a decrease in the partition function estimate for TRW, and an increase for the naive mean field method, in each case guaranteeing an improvement in the approximation and bound. We next focus on binary variables, add the Bethe approximation to consideration and examine ways to choose good variables to clamp, introducing new methods. We show the importance of identifying highly frustrated cycles, and of checking the singleton entropy of a variable. We explore the value of our methods by empirical analysis and draw lessons to guide practitioners. version:1
arxiv-1502-08029 | Describing Videos by Exploiting Temporal Structure | http://arxiv.org/abs/1502.08029 | id:1502.08029 author:Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville category:stat.ML cs.AI cs.CL cs.CV cs.LG  published:2015-02-27 summary:Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions. version:5
arxiv-1505-00122 | Hierarchy of Scales in Language Dynamics | http://arxiv.org/abs/1505.00122 | id:1505.00122 author:Richard A. Blythe category:physics.soc-ph cs.CL  published:2015-05-01 summary:Methods and insights from statistical physics are finding an increasing variety of applications where one seeks to understand the emergent properties of a complex interacting system. One such area concerns the dynamics of language at a variety of levels of description, from the behaviour of individual agents learning simple artificial languages from each other, up to changes in the structure of languages shared by large groups of speakers over historical timescales. In this Colloquium, we survey a hierarchy of scales at which language and linguistic behaviour can be described, along with the main progress in understanding that has been made at each of them---much of which has come from the statistical physics community. We argue that future developments may arise by linking the different levels of the hierarchy together in a more coherent fashion, in particular where this allows more effective use of rich empirical data sets. version:2
arxiv-1509-09294 | General Dynamic Scene Reconstruction from Multiple View Video | http://arxiv.org/abs/1509.09294 | id:1509.09294 author:Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton category:cs.CV  published:2015-09-30 summary:This paper introduces a general approach to dynamic scene reconstruction from multiple moving cameras without prior knowledge or limiting constraints on the scene structure, appearance, or illumination. Existing techniques for dynamic scene reconstruction from multiple wide-baseline camera views primarily focus on accurate reconstruction in controlled environments, where the cameras are fixed and calibrated and background is known. These approaches are not robust for general dynamic scenes captured with sparse moving cameras. Previous approaches for outdoor dynamic scene reconstruction assume prior knowledge of the static background appearance and structure. The primary contributions of this paper are twofold: an automatic method for initial coarse dynamic scene segmentation and reconstruction without prior knowledge of background appearance or structure; and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes from multiple wide-baseline static or moving cameras. Evaluation is performed on a variety of indoor and outdoor scenes with cluttered backgrounds and multiple dynamic non-rigid objects such as people. Comparison with state-of-the-art approaches demonstrates improved accuracy in both multiple view segmentation and dense reconstruction. The proposed approach also eliminates the requirement for prior knowledge of scene structure and appearance. version:1
arxiv-1311-5022 | Extended Formulations for Online Linear Bandit Optimization | http://arxiv.org/abs/1311.5022 | id:1311.5022 author:Shaona Ghosh, Adam Prugel-Bennett category:cs.LG cs.DS  published:2013-11-20 summary:On-line linear optimization on combinatorial action sets (d-dimensional actions) with bandit feedback, is known to have complexity in the order of the dimension of the problem. The exponential weighted strategy achieves the best known regret bound that is of the order of $d^{2}\sqrt{n}$ (where $d$ is the dimension of the problem, $n$ is the time horizon). However, such strategies are provably suboptimal or computationally inefficient. The complexity is attributed to the combinatorial structure of the action set and the dearth of efficient exploration strategies of the set. Mirror descent with entropic regularization function comes close to solving this problem by enforcing a meticulous projection of weights with an inherent boundary condition. Entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension. Here, we argue otherwise and recover the original intuition of exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called `extended formulation'. Such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic entropic analysis. version:3
arxiv-1509-09243 | A spatial compositional model (SCM) for linear unmixing and endmember uncertainty estimation | http://arxiv.org/abs/1509.09243 | id:1509.09243 author:Yuan Zhou, Anand Rangarajan, Paul Gader category:cs.CV  published:2015-09-30 summary:The normal compositional model (NCM) has been extensively used in hyperspectral unmixing. However, most of the previous research has focused on estimation of endmembers and/or their variability. Also, little work has employed spatial information in NCM. In this paper, we show that NCM can be used for calculating the uncertainty of the estimated endmembers with spatial priors incorporated for better unmixing. This results in a spatial compositional model (SCM) which features (i) spatial priors that force neighboring abundances to be similar based on their pixel similarity and (ii) a posterior that is obtained from a likelihood model which does not assume pixel independence. The resulting algorithm turns out to be easy to implement and efficient to run. We compared SCM with current state-of-the-art algorithms on synthetic and real images. The results show that SCM can in the main provide more accurate endmembers and abundances. Moreover, the estimated uncertainty can serve as a prediction of endmember error under certain conditions. version:1
arxiv-1509-09235 | Generative Adversarial Networks in Estimation of Distribution Algorithms for Combinatorial Optimization | http://arxiv.org/abs/1509.09235 | id:1509.09235 author:Malte Probst category:cs.NE  published:2015-09-30 summary:Estimation of Distribution Algorithms (EDAs) require flexible probability models that can be efficiently learned and sampled. Generative Adversarial Networks (GAN) are generative neural networks which can be trained to implicitly model the probability distribution of given data, and it is possible to sample this distribution. We integrate a GAN into an EDA and evaluate the performance of this system when solving combinatorial optimization problems with a single objective. We use several standard benchmark problems and compare the results to state-of-the-art multivariate EDAs. GAN-EDA doe not yield competitive results - the GAN lacks the ability to quickly learn a good approximation of the probability distribution. A key reason seems to be the large amount of noise present in the first EDA generations. version:1
arxiv-1509-09199 | Fault Tolerance in Distributed Neural Computing | http://arxiv.org/abs/1509.09199 | id:1509.09199 author:Anton Kulakov, Mark Zwolinski, Jeff Reeve category:cs.NE cs.DC  published:2015-09-30 summary:With the increasing complexity of computing systems, complete hardware reliability can no longer be guaranteed. We need, however, to ensure overall system reliability. One of the most important features of artificial neural networks is their intrinsic fault-tolerance. The aim of this work is to investigate whether such networks have features that can be applied to wider computational systems. This paper presents an analysis, in both the learning and operational phases, of a distributed feed-forward neural network with decentralised event-driven time management, which is insensitive to intermittent faults caused by unreliable communication or faulty hardware components. The learning rules used in the model are local in space and time, which allows efficient scalable distributed implementation. We investigate the overhead caused by injected faults and analyse the sensitivity to limited failures in the computational hardware in different areas of the network. version:1
arxiv-1509-09187 | Deep Haar Scattering Networks | http://arxiv.org/abs/1509.09187 | id:1509.09187 author:Xiuyuan Cheng, Xu Chen, Stephane Mallat category:cs.LG  published:2015-09-30 summary:An orthogonal Haar scattering transform is a deep network, computed with a hierarchy of additions, subtractions and absolute values, over pairs of coefficients. It provides a simple mathematical model for unsupervised deep network learning. It implements non-linear contractions, which are optimized for classification, with an unsupervised pair matching algorithm, of polynomial complexity. A structured Haar scattering over graph data computes permutation invariant representations of groups of connected points in the graph. If the graph connectivity is unknown, unsupervised Haar pair learning can provide a consistent estimation of connected dyadic groups of points. Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown. version:1
arxiv-1510-03776 | Towards Trainable Media: Using Waves for Neural Network-Style Training | http://arxiv.org/abs/1510.03776 | id:1510.03776 author:Michiel Hermans, Thomas Van Vaerenbergh category:cs.NE physics.optics  published:2015-09-30 summary:In this paper we study the concept of using the interaction between waves and a trainable medium in order to construct a matrix-vector multiplier. In particular we study such a device in the context of the backpropagation algorithm, which is commonly used for training neural networks. Here, the weights of the connections between neurons are trained by multiplying a `forward' signal with a backwards propagating `error' signal. We show that this concept can be extended to trainable media, where the gradient for the local wave number is given by multiplying signal waves and error waves. We provide a numerical example of such a system with waves traveling freely in a trainable medium, and we discuss a potential way to build such a device in an integrated photonics chip. version:1
arxiv-1509-09130 | Learning From Missing Data Using Selection Bias in Movie Recommendation | http://arxiv.org/abs/1509.09130 | id:1509.09130 author:Claire Vernade, Olivier Cappé category:stat.ML cs.IR cs.LG cs.SI  published:2015-09-30 summary:Recommending items to users is a challenging task due to the large amount of missing information. In many cases, the data solely consist of ratings or tags voluntarily contributed by each user on a very limited subset of the available items, so that most of the data of potential interest is actually missing. Current approaches to recommendation usually assume that the unobserved data is missing at random. In this contribution, we provide statistical evidence that existing movie recommendation datasets reveal a significant positive association between the rating of items and the propensity to select these items. We propose a computationally efficient variational approach that makes it possible to exploit this selection bias so as to improve the estimation of ratings from small populations of users. Results obtained with this approach applied to neighborhood-based collaborative filtering illustrate its potential for improving the reliability of the recommendation. version:1
arxiv-1509-09121 | The "handedness" of language: Directional symmetry breaking of sign usage in words | http://arxiv.org/abs/1509.09121 | id:1509.09121 author:Md Izhar Ashraf, Sitabhra Sinha category:cs.CL  published:2015-09-30 summary:Using large written corpora for many different scripts, we show that the occurrence probability distributions of signs at the left and right ends of words have a distinct heterogeneous nature. Characterizing this asymmetry using quantitative inequality measures, we show that the beginning of a word is less restrictive in sign usage than the end. The asymmetry is also seen in undeciphered inscriptions and we use this to infer the direction of writing which agrees with archaeological evidence. Unlike traditional investigations of phonotactic constraints which focus on language-specific patterns, our study reveals a property valid across languages and writing systems. As both language and writing are unique aspects of our species, this universal signature may reflect an innate feature of the human cognitive phenomenon. version:1
arxiv-1509-09114 | Online Object Tracking with Proposal Selection | http://arxiv.org/abs/1509.09114 | id:1509.09114 author:Yang Hua, Karteek Alahari, Cordelia Schmid category:cs.CV  published:2015-09-30 summary:Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object, and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i.e., detection score and edgeness score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance. version:1
arxiv-1510-00001 | Polish to English Statistical Machine Translation | http://arxiv.org/abs/1510.00001 | id:1510.00001 author:Krzysztof Wołk category:cs.CL stat.ML  published:2015-09-30 summary:This research explores the effects of various training settings on a Polish to English Statistical Machine Translation system for spoken language. Various elements of the TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for development, tuning and testing of the translation system. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of the data preparations on the translation results. version:1
arxiv-1509-09097 | Polish - English Speech Statistical Machine Translation Systems for the IWSLT 2013 | http://arxiv.org/abs/1509.09097 | id:1509.09097 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML  published:2015-09-30 summary:This research explores the effects of various training settings from Polish to English Statistical Machine Translation system for spoken language. Various elements of the TED parallel text corpora for the IWSLT 2013 evaluation campaign were used as the basis for training of language models, and for development, tuning and testing of the translation system. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of data preparations on translation results. Our experiments included systems, which use stems and morphological information on Polish words. We also conducted a deep analysis of provided Polish data as preparatory work for the automatic data correction and cleaning phase. version:1
arxiv-1509-09093 | A Sentence Meaning Based Alignment Method for Parallel Text Corpora Preparation | http://arxiv.org/abs/1509.09093 | id:1509.09093 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL cs.IR  published:2015-09-30 summary:Text alignment is crucial to the accuracy of Machine Translation (MT) systems, some NLP tools or any other text processing tasks requiring bilingual data. This research proposes a language independent sentence alignment approach based on Polish (not position-sensitive language) to English experiments. This alignment approach was developed on the TED Talks corpus, but can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence recognition. Some of them value synonyms and semantic text structure analysis as a part of additional information. Minimization of data loss was ensured. The solution is compared to other sentence alignment implementations. Also an improvement in MT system score with text processed with described tool is shown. version:1
arxiv-1509-09090 | Real-Time Statistical Speech Translation | http://arxiv.org/abs/1509.09090 | id:1509.09090 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML  published:2015-09-30 summary:This research investigates the Statistical Machine Translation approaches to translate speech in real time automatically. Such systems can be used in a pipeline with speech recognition and synthesis software in order to produce a real-time voice communication system between foreigners. We obtained three main data sets from spoken proceedings that represent three different types of human speech. TED, Europarl, and OPUS parallel text corpora were used as the basis for training of language models, for developmental tuning and testing of the translation system. We also conducted experiments involving part of speech tagging, compound splitting, linear language model interpolation, TrueCasing and morphosyntactic analysis. We evaluated the effects of variety of data preparations on the translation results using the BLEU, NIST, METEOR and TER metrics and tried to give answer which metric is most suitable for PL-EN language pair. version:1
arxiv-1509-09089 | Moving Object Detection in Video Using Saliency Map and Subspace Learning | http://arxiv.org/abs/1509.09089 | id:1509.09089 author:Yanwei Pang, Li Ye, Xuelong Li, Jing Pan category:cs.CV  published:2015-09-30 summary:Moving object detection is a key to intelligent video analysis. On the one hand, what moves is not only interesting objects but also noise and cluttered background. On the other hand, moving objects without rich texture are prone not to be detected. So there are undesirable false alarms and missed alarms in many algorithms of moving object detection. To reduce the false alarms and missed alarms, in this paper, we propose to incorporate a saliency map into an incremental subspace analysis framework where the saliency map makes estimated background has less chance than foreground (i.e., moving objects) to contain salient objects. The proposed objective function systematically takes account into the properties of sparsity, low-rank, connectivity, and saliency. An alternative minimization algorithm is proposed to seek the optimal solutions. Experimental results on the Perception Test Images Sequences demonstrate that the proposed method is effective in reducing false alarms and missed alarms. version:1
arxiv-1509-09088 | Enhanced Bilingual Evaluation Understudy | http://arxiv.org/abs/1509.09088 | id:1509.09088 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL stat.ML  published:2015-09-30 summary:Our research extends the Bilingual Evaluation Understudy (BLEU) evaluation technique for statistical machine translation to make it more adjustable and robust. We intend to adapt it to resemble human evaluation more. We perform experiments to evaluate the performance of our technique against the primary existing evaluation methods. We describe and show the improvements it makes over existing methods as well as correlation to them. When human translators translate a text, they often use synonyms, different word orders or style, and other similar variations. We propose an SMT evaluation technique that enhances the BLEU metric to consider variations such as those. version:1
arxiv-1509-09030 | Distributed Weighted Parameter Averaging for SVM Training on Big Data | http://arxiv.org/abs/1509.09030 | id:1509.09030 author:Ayan Das, Sourangshu Bhattacharya category:cs.LG  published:2015-09-30 summary:Two popular approaches for distributed training of SVMs on big data are parameter averaging and ADMM. Parameter averaging is efficient but suffers from loss of accuracy with increase in number of partitions, while ADMM in the feature space is accurate but suffers from slow convergence. In this paper, we report a hybrid approach called weighted parameter averaging (WPA), which optimizes the regularized hinge loss with respect to weights on parameters. The problem is shown to be same as solving SVM in a projected space. We also demonstrate an $O(\frac{1}{N})$ stability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space. version:1
arxiv-1509-09011 | Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring | http://arxiv.org/abs/1509.09011 | id:1509.09011 author:Junpei Komiyama, Junya Honda, Hiroshi Nakagawa category:stat.ML cs.LG  published:2015-09-30 summary:Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound. version:1
arxiv-1509-08990 | Learning without Recall: A Case for Log-Linear Learning | http://arxiv.org/abs/1509.08990 | id:1509.08990 author:Mohammad Amin Rahimian, Ali Jadbabaie category:cs.SI cs.LG cs.SY math.OC stat.ML  published:2015-09-30 summary:We analyze a model of learning and belief formation in networks in which agents follow Bayes rule yet they do not recall their history of past observations and cannot reason about how other agents' beliefs are formed. They do so by making rational inferences about their observations which include a sequence of independent and identically distributed private signals as well as the beliefs of their neighboring agents at each time. Fully rational agents would successively apply Bayes rule to the entire history of observations. This leads to forebodingly complex inferences due to lack of knowledge about the global network structure that causes those observations. To address these complexities, we consider a Learning without Recall model, which in addition to providing a tractable framework for analyzing the behavior of rational agents in social networks, can also provide a behavioral foundation for the variety of non-Bayesian update rules in the literature. We present the implications of various choices for time-varying priors of such agents and how this choice affects learning and its rate. version:1
arxiv-1509-08973 | Symbol Emergence in Robotics: A Survey | http://arxiv.org/abs/1509.08973 | id:1509.08973 author:Tadahiro Taniguchi, Takayuki Nagai, Tomoaki Nakamura, Naoto Iwahashi, Tetsuya Ogata, Hideki Asoh category:cs.AI cs.CL cs.CV cs.RO  published:2015-09-29 summary:Humans can learn the use of language through physical interaction with their environment and semiotic communication with other people. It is very important to obtain a computational understanding of how humans can form a symbol system and obtain semiotic skills through their autonomous mental development. Recently, many studies have been conducted on the construction of robotic systems and machine-learning methods that can learn the use of language through embodied multimodal interaction with their environment and other systems. Understanding human social interactions and developing a robot that can smoothly communicate with human users in the long term, requires an understanding of the dynamics of symbol systems and is crucially important. The embodied cognition and social interaction of participants gradually change a symbol system in a constructive manner. In this paper, we introduce a field of research called symbol emergence in robotics (SER). SER is a constructive approach towards an emergent symbol system. The emergent symbol system is socially self-organized through both semiotic communications and physical interactions with autonomous cognitive developmental agents, i.e., humans and developmental robots. Specifically, we describe some state-of-art research topics concerning SER, e.g., multimodal categorization, word discovery, and a double articulation analysis, that enable a robot to obtain words and their embodied meanings from raw sensory--motor information, including visual information, haptic information, auditory information, and acoustic speech signals, in a totally unsupervised manner. Finally, we suggest future directions of research in SER. version:1
arxiv-1509-08972 | VLSI Implementation of Deep Neural Network Using Integral Stochastic Computing | http://arxiv.org/abs/1509.08972 | id:1509.08972 author:Arash Ardakani, François Leduc-Primeau, Naoya Onizawa, Takahiro Hanyu, Warren J. Gross category:cs.NE cs.AR  published:2015-09-29 summary:The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention: many applications in fact require high-speed operations that suit a hardware implementation. However, numerous elements and complex interconnections are usually required, leading to a large area occupation and copious power consumption. Stochastic computing has shown promising results for low-power area-efficient hardware implementations, even though existing stochastic algorithms require long streams that cause long latencies. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral stochastic computing. The proposed architecture uses integer stochastic streams and a modified Finite State Machine-based tanh function to perform computations and even reduce the latency compared to conventional stochastic computation. The proposed architecture has been implemented on a Virtex7 FPGA, resulting in 44.96% and 62.36% average reductions in area and latency compared to the best reported architecture in literature. We also synthesize the circuits in a 65 nm CMOS technology and show that they can tolerate a fault rate of up to 20% on some computations when timing violations are allowed to occur, resulting in power savings. The fault-tolerance property of the proposed architectures make them suitable for inherently unreliable advanced process technologies such as memristor technology. version:1
arxiv-1509-08969 | Light Field Reconstruction Using Shearlet Transform | http://arxiv.org/abs/1509.08969 | id:1509.08969 author:Suren Vagharshakyan, Robert Bregovic, Atanas Gotchev category:cs.CV  published:2015-09-29 summary:In this article we develop an image based rendering technique based on light field reconstruction from a limited set of perspective views acquired by cameras. Our approach utilizes sparse representation of epipolar-plane images in a directionally sensitive transform domain, obtained by an adapted discrete shearlet transform. The used iterative thresholding algorithm provides high-quality reconstruction results for relatively big disparities between neighboring views. The generated densely sampled light field of a given 3D scene is thus suitable for all applications which requires light field reconstruction. The proposed algorithm is compared favorably against state of the art depth image based rendering techniques. version:1
arxiv-1509-08909 | Polish -English Statistical Machine Translation of Medical Texts | http://arxiv.org/abs/1509.08909 | id:1509.08909 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL cs.IR stat.ML  published:2015-09-29 summary:This new research explores the effects of various training methods on a Polish to English Statistical Machine Translation system for medical texts. Various elements of the EMEA parallel text corpora from the OPUS project were used as the basis for training of phrase tables and language models and for development, tuning and testing of the translation system. The BLEU, NIST, METEOR, RIBES and TER metrics have been used to evaluate the effects of various system and data preparations on translation results. Our experiments included systems that used POS tagging, factored phrase models, hierarchical models, syntactic taggers, and many different alignment methods. We also conducted a deep analysis of Polish data as preparatory work for automatic data correction such as true casing and punctuation normalization phase. version:1
arxiv-1509-08902 | Scalable Nonlinear Embeddings for Semantic Category-based Image Retrieval | http://arxiv.org/abs/1509.08902 | id:1509.08902 author:Gaurav Sharma, Bernt Schiele category:cs.CV  published:2015-09-29 summary:We propose a novel algorithm for the task of supervised discriminative distance learning by nonlinearly embedding vectors into a low dimensional Euclidean space. We work in the challenging setting where supervision is with constraints on similar and dissimilar pairs while training. The proposed method is derived by an approximate kernelization of a linear Mahalanobis-like distance metric learning algorithm and can also be seen as a kernel neural network. The number of model parameters and test time evaluation complexity of the proposed method are O(dD) where D is the dimensionality of the input features and d is the dimension of the projection space - this is in contrast to the usual kernelization methods as, unlike them, the complexity does not scale linearly with the number of training examples. We propose a stochastic gradient based learning algorithm which makes the method scalable (w.r.t. the number of training examples), while being nonlinear. We train the method with up to half a million training pairs of 4096 dimensional CNN features. We give empirical comparisons with relevant baselines on seven challenging datasets for the task of low dimensional semantic category based image retrieval. version:1
arxiv-1504-07889 | Bilinear CNN Models for Fine-grained Visual Recognition | http://arxiv.org/abs/1504.07889 | id:1504.07889 author:Tsung-Yu Lin, Aruni RoyChowdhury, Subhransu Maji category:cs.CV  published:2015-04-29 summary:We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and O2P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bcnn. version:3
arxiv-1509-08888 | A Semi-Supervised Method for Predicting Cancer Survival Using Incomplete Clinical Data | http://arxiv.org/abs/1509.08888 | id:1509.08888 author:Hamid Reza Hassanzadeh, John H. Phan, May D. Wang category:cs.LG  published:2015-09-29 summary:Prediction of survival for cancer patients is an open area of research. However, many of these studies focus on datasets with a large number of patients. We present a novel method that is specifically designed to address the challenge of data scarcity, which is often the case for cancer datasets. Our method is able to use unlabeled data to improve classification by adopting a semi-supervised training approach to learn an ensemble classifier. The results of applying our method to three cancer datasets show the promise of semi-supervised learning for prediction of cancer survival. version:1
arxiv-1509-08881 | Building Subject-aligned Comparable Corpora and Mining it for Truly Parallel Sentence Pairs | http://arxiv.org/abs/1509.08881 | id:1509.08881 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL cs.IR stat.ML  published:2015-09-29 summary:Parallel sentences are a relatively scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation. This research explores our methodology for mining such data from previously obtained comparable corpora. The task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora, but parallel sentences are a much more useful resource. Here we propose a web crawling method for building subject-aligned comparable corpora from Wikipedia articles. We also introduce a method for extracting truly parallel sentences that are filtered out from noisy or just comparable sentence pairs. We describe our implementation of a specialized tool for this task as well as training and adaption of a machine translation system that supplies our filter with additional information about the similarity of comparable sentence pairs. version:1
arxiv-1509-08874 | Polish - English Speech Statistical Machine Translation Systems for the IWSLT 2014 | http://arxiv.org/abs/1509.08874 | id:1509.08874 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL  published:2015-09-29 summary:This research explores effects of various training settings between Polish and English Statistical Machine Translation systems for spoken language. Various elements of the TED parallel text corpora for the IWSLT 2014 evaluation campaign were used as the basis for training of language models, and for development, tuning and testing of the translation system as well as Wikipedia based comparable corpora prepared by us. The BLEU, NIST, METEOR and TER metrics were used to evaluate the effects of data preparations on translation results. Our experiments included systems, which use lemma and morphological information on Polish words. We also conducted a deep analysis of provided Polish data as preparatory work for the automatic data correction and cleaning phase. version:1
arxiv-1509-00913 | On-the-Fly Learning in a Perpetual Learning Machine | http://arxiv.org/abs/1509.00913 | id:1509.00913 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-09-03 summary:Despite the promise of brain-inspired machine learning, deep neural networks (DNN) have frustratingly failed to bridge the deceptively large gap between learning and memory. Here, we introduce a Perpetual Learning Machine; a new type of DNN that is capable of brain-like dynamic 'on the fly' learning because it exists in a self-supervised state of Perpetual Stochastic Gradient Descent. Thus, we provide the means to unify learning and memory within a machine learning framework. We also explore the elegant duality of abstraction and synthesis: the Yin and Yang of deep learning. version:3
arxiv-1509-08842 | Automatically Segmenting Oral History Transcripts | http://arxiv.org/abs/1509.08842 | id:1509.08842 author:Ryan Shaw category:cs.CL  published:2015-09-29 summary:Dividing oral histories into topically coherent segments can make them more accessible online. People regularly make judgments about where coherent segments can be extracted from oral histories. But making these judgments can be taxing, so automated assistance is potentially attractive to speed the task of extracting segments from open-ended interviews. When different people are asked to extract coherent segments from the same oral histories, they often do not agree about precisely where such segments begin and end. This low agreement makes the evaluation of algorithmic segmenters challenging, but there is reason to believe that for segmenting oral history transcripts, some approaches are more promising than others. The BayesSeg algorithm performs slightly better than TextTiling, while TextTiling does not perform significantly better than a uniform segmentation. BayesSeg might be used to suggest boundaries to someone segmenting oral histories, but this segmentation task needs to be better defined. version:1
arxiv-1412-6279 | Non-parametric PSF estimation from celestial transit solar images using blind deconvolution | http://arxiv.org/abs/1412.6279 | id:1412.6279 author:Adriana Gonzalez, Véronique Delouille, Laurent Jacques category:cs.CV astro-ph.SR  published:2014-12-19 summary:Context: Characterization of instrumental effects in astronomical imaging is important in order to extract accurate physical information from the observations. The measured image in a real optical instrument is usually represented by the convolution of an ideal image with a Point Spread Function (PSF). Additionally, the image acquisition process is also contaminated by other sources of noise (read-out, photon-counting). The problem of estimating both the PSF and a denoised image is called blind deconvolution and is ill-posed. Aims: We propose a blind deconvolution scheme that relies on image regularization. Contrarily to most methods presented in the literature, our method does not assume a parametric model of the PSF and can thus be applied to any telescope. Methods: Our scheme uses a wavelet analysis prior model on the image and weak assumptions on the PSF. We use observations from a celestial transit, where the occulting body can be assumed to be a black disk. These constraints allow us to retain meaningful solutions for the filter and the image, eliminating trivial, translated and interchanged solutions. Under an additive Gaussian noise assumption, they also enforce noise canceling and avoid reconstruction artifacts by promoting the whiteness of the residual between the blurred observations and the cleaned data. Results: Our method is applied to synthetic and experimental data. The PSF is estimated for the SECCHI/EUVI instrument using the 2007 Lunar transit, and for SDO/AIA using the 2012 Venus transit. Results show that the proposed non-parametric blind deconvolution method is able to estimate the core of the PSF with a similar quality to parametric methods proposed in the literature. We also show that, if these parametric estimations are incorporated in the acquisition model, the resulting PSF outperforms both the parametric and non-parametric methods. version:3
arxiv-1509-08830 | How to Formulate and Solve Statistical Recognition and Learning Problems | http://arxiv.org/abs/1509.08830 | id:1509.08830 author:Michail Schlesinger, Evgeniy Vodolazskiy category:cs.LG  published:2015-09-29 summary:We formulate problems of statistical recognition and learning in a common framework of complex hypothesis testing. Based on arguments from multi-criteria optimization, we identify strategies that are improper for solving these problems and derive a common form of the remaining strategies. We show that some widely used approaches to recognition and learning are improper in this sense. We then propose a generalized formulation of the recognition and learning problem which embraces the whole range of sizes of the learning sample, including the zero size. Learning becomes a special case of recognition without learning. We define the concept of closest to optimal strategy, being a solution to the formulated problem, and describe a technique for finding such a strategy. On several illustrative cases, the strategy is shown to be superior to the widely used learning methods based on maximal likelihood estimation. version:1
arxiv-1509-08101 | Representation Benefits of Deep Feedforward Networks | http://arxiv.org/abs/1509.08101 | id:1509.08101 author:Matus Telgarsky category:cs.LG cs.NE  published:2015-09-27 summary:This note provides a family of classification problems, indexed by a positive integer $k$, where all shallow networks with fewer than exponentially (in $k$) many nodes exhibit error at least $1/6$, whereas a deep network with 2 nodes in each of $2k$ layers achieves zero error, as does a recurrent network with 3 distinct nodes iterated $k$ times. The proof is elementary, and the networks are standard feedforward networks with ReLU (Rectified Linear Unit) nonlinearities. version:2
arxiv-1509-08731 | Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning | http://arxiv.org/abs/1509.08731 | id:1509.08731 author:Shakir Mohamed, Danilo Jimenez Rezende category:stat.ML cs.AI cs.LG  published:2015-09-29 summary:The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions. version:1
arxiv-1509-08715 | Retinex filtering of foggy images: generation of a bulk set with selection and ranking | http://arxiv.org/abs/1509.08715 | id:1509.08715 author:Roberto Marazzato, Amelia Carolina Sparavigna category:cs.CV  published:2015-09-29 summary:In this paper we are proposing the use of GIMP Retinex, a filter of the GNU Image Manipulation Program, for enhancing foggy images. This filter involves adjusting four different parameters to find the output image which has to be preferred according to some specific purposes. Aiming to obtain a processing, which is able of choosing automatically the best image from a given set, we are proposing a method for the generation a bulk set of GIMP Retinex filtered images and a preliminary approach for selecting and ranking them. version:1
arxiv-1507-02049 | DCTNet : A Simple Learning-free Approach for Face Recognition | http://arxiv.org/abs/1507.02049 | id:1507.02049 author:Cong Jie Ng, Andrew Beng Jin Teoh category:cs.CV  published:2015-07-08 summary:PCANet was proposed as a lightweight deep learning network that mainly leverages Principal Component Analysis (PCA) to learn multistage filter banks followed by binarization and block-wise histograming. PCANet was shown worked surprisingly well in various image classification tasks. However, PCANet is data-dependence hence inflexible. In this paper, we proposed a data-independence network, dubbed DCTNet for face recognition in which we adopt Discrete Cosine Transform (DCT) as filter banks in place of PCA. This is motivated by the fact that 2D DCT basis is indeed a good approximation for high ranked eigenvectors of PCA. Both 2D DCT and PCA resemble a kind of modulated sine-wave patterns, which can be perceived as a bandpass filter bank. DCTNet is free from learning as 2D DCT bases can be computed in advance. Besides that, we also proposed an effective method to regulate the block-wise histogram feature vector of DCTNet for robustness. It is shown to provide surprising performance boost when the probe image is considerably different in appearance from the gallery image. We evaluate the performance of DCTNet extensively on a number of benchmark face databases and being able to achieve on par with or often better accuracy performance than PCANet. version:3
arxiv-1302-2489 | Adaptive-treed bandits | http://arxiv.org/abs/1302.2489 | id:1302.2489 author:Adam D. Bull category:math.ST stat.ML stat.TH  published:2013-02-11 summary:We describe a novel algorithm for noisy global optimisation and continuum-armed bandits, with good convergence properties over any continuous reward function having finitely many polynomial maxima. Over such functions, our algorithm achieves square-root regret in bandits, and inverse-square-root error in optimisation, without prior information. Our algorithm works by reducing these problems to tree-armed bandits, and we also provide new results in this setting. We show it is possible to adaptively combine multiple trees so as to minimise the regret, and also give near-matching lower bounds on the regret in terms of the zooming dimension. version:4
arxiv-1509-08660 | Censoring Diffusion for Harvesting WSNs | http://arxiv.org/abs/1509.08660 | id:1509.08660 author:Jesus Fernandez-Bes, Rocío Arroyo-Valles, Jerónimo Arenas-García, Jesús Cid-Sueiro category:cs.SY cs.MA math.OC stat.ML  published:2015-09-29 summary:In this paper, we analyze energy-harvesting adaptive diffusion networks for a distributed estimation problem. In order to wisely manage the available energy resources, we propose a scheme where a censoring algorithm is jointly applied over the diffusion strategy. An energy-aware variation of a diffusion algorithm is used, and a new way of measuring the relevance of the estimates in diffusion networks is proposed in order to apply a subsequent censoring mechanism. Simulation results show the potential benefit of integrating censoring schemes in energy-constrained diffusion networks. version:1
arxiv-1509-08647 | Long-Range Trajectories from Global and Local Motion Representations | http://arxiv.org/abs/1509.08647 | id:1509.08647 author:Eduardo M. Pereira, Jaime S. Cardoso, Ricardo Morla category:cs.CV  published:2015-09-29 summary:Motion is a fundamental cue for scene analysis and human activity understan- ding in videos. It can be encoded in trajectories for tracking objects and for action recognition, or in form of flow to address behaviour analysis in crowded scenes. Each approach can only be applied on limited scenarios. We propose a motion-based system that represents the spatial and temporal features of the flow in terms of long-range trajectories. The novelty resides on the system formulation, its generic approach to handle scene variability and motion variations, motion integration from local and global representations, and the resulting long-range trajectories that overcome trajectory-based approach problems. We report the results and conclusions that state its pertinence on different scenarios, comparing and correlating the extracted trajectories of individual pedestrians, manually annotated. We also propose an evaluation framework and stress the diverse system characteristics that can be used for human activity tasks, namely on motion segmentation. version:1
arxiv-1509-08644 | Neural-based machine translation for medical text domain. Based on European Medicines Agency leaflet texts | http://arxiv.org/abs/1509.08644 | id:1509.08644 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL cs.CY cs.NE stat.ML  published:2015-09-29 summary:The quality of machine translation is rapidly evolving. Today one can find several machine translation systems on the web that provide reasonable translations, although the systems are not perfect. In some specific domains, the quality may decrease. A recently proposed approach to this domain is neural machine translation. It aims at building a jointly-tuned single neural network that maximizes translation performance, a very different approach from traditional statistical machine translation. Recently proposed neural machine translation models often belong to the encoder-decoder family in which a source sentence is encoded into a fixed length vector that is, in turn, decoded to generate a translation. The present research examines the effects of different training methods on a Polish-English Machine Translation system used for medical data. The European Medicines Agency parallel text corpus was used as the basis for training of neural and statistical network-based translation systems. The main machine translation evaluation metrics have also been used in analysis of the systems. A comparison and implementation of a real-time medical translator is the main focus of our experiments. version:1
arxiv-1509-08639 | Tuned and GPU-accelerated parallel data mining from comparable corpora | http://arxiv.org/abs/1509.08639 | id:1509.08639 author:Krzysztof Wołk, Krzysztof Marasek category:cs.CL cs.AI cs.DS  published:2015-09-29 summary:The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such has a very limited availability especially for some languages and very narrow text domains. Is this research we present our improvements to Yalign mining methodology by reimplementing the comparison algorithm, introducing a tuning scripts and by improving performance using GPU computing acceleration. The experiments are conducted on various text domains and bi-data is extracted from the Wikipedia dumps. version:1
arxiv-1509-08634 | Learning dynamic Boltzmann machines with spike-timing dependent plasticity | http://arxiv.org/abs/1509.08634 | id:1509.08634 author:Takayuki Osogami, Makoto Otsuka category:cs.NE cs.AI cs.LG stat.ML  published:2015-09-29 summary:We propose a particularly structured Boltzmann machine, which we refer to as a dynamic Boltzmann machine (DyBM), as a stochastic model of a multi-dimensional time-series. The DyBM can have infinitely many layers of units but allows exact and efficient inference and learning when its parameters have a proposed structure. This proposed structure is motivated by postulates and observations, from biological neural networks, that the synaptic weight is strengthened or weakened, depending on the timing of spikes (i.e., spike-timing dependent plasticity or STDP). We show that the learning rule of updating the parameters of the DyBM in the direction of maximizing the likelihood of given time-series can be interpreted as STDP with long term potentiation and long term depression. The learning rule has a guarantee of convergence and can be performed in a distributed matter (i.e., local in space) with limited memory (i.e., local in time). version:1
arxiv-1509-08627 | Semantics, Representations and Grammars for Deep Learning | http://arxiv.org/abs/1509.08627 | id:1509.08627 author:David Balduzzi category:cs.LG cs.NE stat.ML  published:2015-09-29 summary:Deep learning is currently the subject of intensive study. However, fundamental concepts such as representations are not formally defined -- researchers "know them when they see them" -- and there is no common language for describing and analyzing algorithms. This essay proposes an abstract framework that identifies the essential features of current practice and may provide a foundation for future developments. The backbone of almost all deep learning algorithms is backpropagation, which is simply a gradient computation distributed over a neural network. The main ingredients of the framework are thus, unsurprisingly: (i) game theory, to formalize distributed optimization; and (ii) communication protocols, to track the flow of zeroth and first-order information. The framework allows natural definitions of semantics (as the meaning encoded in functions), representations (as functions whose semantics is chosen to optimized a criterion) and grammars (as communication protocols equipped with first-order convergence guarantees). Much of the essay is spent discussing examples taken from the literature. The ultimate aim is to develop a graphical language for describing the structure of deep learning algorithms that backgrounds the details of the optimization procedure and foregrounds how the components interact. Inspiration is taken from probabilistic graphical models and factor graphs, which capture the essential structural features of multivariate distributions. version:1
arxiv-1509-08582 | Tractable Fully Bayesian Inference via Convex Optimization and Optimal Transport Theory | http://arxiv.org/abs/1509.08582 | id:1509.08582 author:Sanggyun Kim, Diego Mesa, Rui Ma, Todd P. Coleman category:stat.ML  published:2015-09-29 summary:We consider the problem of transforming samples from one continuous source distribution into samples from another target distribution. We demonstrate with optimal transport theory that when the source distribution can be easily sampled from and the target distribution is log-concave, this can be tractably solved with convex optimization. We show that a special case of this, when the source is the prior and the target is the posterior, is Bayesian inference. Here, we can tractably calculate the normalization constant and draw posterior i.i.d. samples. Remarkably, our Bayesian tractability criterion is simply log concavity of the prior and likelihood: the same criterion for tractable calculation of the maximum a posteriori point estimate. With simulated data, we demonstrate how we can attain the Bayes risk in simulations. With physiologic data, we demonstrate improvements over point estimation in intensive care unit outcome prediction and electroencephalography-based sleep staging. version:1
arxiv-1508-00271 | Recurrent Network Models for Human Dynamics | http://arxiv.org/abs/1508.00271 | id:1508.00271 author:Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik category:cs.CV  published:2015-08-02 summary:We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoid drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units. version:2
arxiv-1506-01418 | Parallel Stochastic Gradient Markov Chain Monte Carlo for Matrix Factorisation Models | http://arxiv.org/abs/1506.01418 | id:1506.01418 author:Umut Şimşekli, Hazal Koptagel, Hakan Güldaş, A. Taylan Cemgil, Figen Öztoprak, Ş. İlker Birbil category:stat.ML  published:2015-06-03 summary:For large matrix factorisation problems, we develop a distributed Markov Chain Monte Carlo (MCMC) method based on stochastic gradient Langevin dynamics (SGLD) that we call Parallel SGLD (PSGLD). PSGLD has very favourable scaling properties with increasing data size and is comparable in terms of computational requirements to optimisation methods based on stochastic gradient descent. PSGLD achieves high performance by exploiting the conditional independence structure of the MF models to sub-sample data in a systematic manner as to allow parallelisation and distributed computation. We provide a convergence proof of the algorithm and verify its superior performance on various architectures such as Graphics Processing Units, shared memory multi-core systems and multi-computer clusters. version:2
arxiv-1509-08455 | Efficient Empowerment | http://arxiv.org/abs/1509.08455 | id:1509.08455 author:Maximilian Karl, Justin Bayer, Patrick van der Smagt category:stat.ML cs.LG  published:2015-09-28 summary:Empowerment quantifies the influence an agent has on its environment. This is formally achieved by the maximum of the expected KL-divergence between the distribution of the successor state conditioned on a specific action and a distribution where the actions are marginalised out. This is a natural candidate for an intrinsic reward signal in the context of reinforcement learning: the agent will place itself in a situation where its action have maximum stability and maximum influence on the future. The limiting factor so far has been the computational complexity of the method: the only way of calculation has so far been a brute force algorithm, reducing the applicability of the method to environments with a small set discrete states. In this work, we propose to use an efficient approximation for marginalising out the actions in the case of continuous environments. This allows fast evaluation of empowerment, paving the way towards challenging environments such as real world robotics. The method is presented on a pendulum swing up problem. version:1
arxiv-1509-08439 | Hyper-Fisher Vectors for Action Recognition | http://arxiv.org/abs/1509.08439 | id:1509.08439 author:Sanath Narayan, Kalpathi R. Ramakrishnan category:cs.CV  published:2015-09-28 summary:In this paper, a novel encoding scheme combining Fisher vector and bag-of-words encodings has been proposed for recognizing action in videos. The proposed Hyper-Fisher vector encoding is sum of local Fisher vectors which are computed based on the traditional Bag-of-Words (BoW) encoding. Thus, the proposed encoding is simple and yet an effective representation over the traditional Fisher Vector encoding. By extensive evaluation on challenging action recognition datasets, viz., Youtube, Olympic Sports, UCF50 and HMDB51, we show that the proposed Hyper-Fisher Vector encoding improves the recognition performance by around 2-3% compared to the improved Fisher Vector encoding. We also perform experiments to show that the performance of the Hyper-Fisher Vector is robust to the dictionary size of the BoW encoding. version:1
arxiv-1505-00066 | Pose Induction for Novel Object Categories | http://arxiv.org/abs/1505.00066 | id:1505.00066 author:Shubham Tulsiani, João Carreira, Jitendra Malik category:cs.CV  published:2015-05-01 summary:We address the task of predicting pose for objects of unannotated object categories from a small seed set of annotated object classes. We present a generalized classifier that can reliably induce pose given a single instance of a novel category. In case of availability of a large collection of novel instances, our approach then jointly reasons over all instances to improve the initial estimates. We empirically validate the various components of our algorithm and quantitatively show that our method produces reliable pose estimates. We also show qualitative results on a diverse set of classes and further demonstrate the applicability of our system for learning shape models of novel object classes. version:2
arxiv-1503-00759 | A Review of Relational Machine Learning for Knowledge Graphs | http://arxiv.org/abs/1503.00759 | id:1503.00759 author:Maximilian Nickel, Kevin Murphy, Volker Tresp, Evgeniy Gabrilovich category:stat.ML cs.LG  published:2015-03-02 summary:Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination. version:3
arxiv-1509-07479 | Learning Concept Embeddings with Combined Human-Machine Expertise | http://arxiv.org/abs/1509.07479 | id:1509.07479 author:Michael J. Wilber, Iljung S. Kwak, David Kriegman, Serge Belongie category:cs.CV  published:2015-09-24 summary:This paper presents our work on "SNaCK," a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods. version:2
arxiv-1509-08387 | Quantile Search: A Distance-Penalized Active Learning Algorithm for Spatial Sampling | http://arxiv.org/abs/1509.08387 | id:1509.08387 author:John Lipor, Laura Balzano, Branko Kerkez, Don Scavia category:stat.ML cs.LG 62L05 G.3; H.3.3  published:2015-09-28 summary:Adaptive sampling theory has shown that, with proper assumptions on the signal class, algorithms exist to reconstruct a signal in $\mathbb{R}^{d}$ with an optimal number of samples. We generalize this problem to when the cost of sampling is not only the number of samples but also the distance traveled between samples. This is motivated by our work studying regions of low oxygen concentration in the Great Lakes. We show that for one-dimensional threshold classifiers, a tradeoff between number of samples and distance traveled can be achieved using a generalization of binary search, which we refer to as quantile search. We derive the expected total sampling time for noiseless measurements and the expected number of samples for an extension to the noisy case. We illustrate our results in simulations relevant to our sampling application. version:1
arxiv-1509-08383 | Efficient Discriminative Nonorthogonal Binary Subspace with its Application to Visual Tracking | http://arxiv.org/abs/1509.08383 | id:1509.08383 author:Ang Li, Feng Tang, Yanwen Guo, Hai Tao category:cs.CV  published:2015-09-28 summary:One of the crucial problems in visual tracking is how the object is represented. Conventional appearance-based trackers are using increasingly more complex features in order to be robust. However, complex representations typically not only require more computation for feature extraction, but also make the state inference complicated. We show that with a careful feature selection scheme, extremely simple yet discriminative features can be used for robust object tracking. The central component of the proposed method is a succinct and discriminative representation of the object using discriminative non-orthogonal binary subspace (DNBS) which is spanned by Haar-like features. The DNBS representation inherits the merits of the original NBS in that it efficiently describes the object. It also incorporates the discriminative information to distinguish foreground from background. However, the problem of finding the DNBS bases from an over-complete dictionary is NP-hard. We propose a greedy algorithm called discriminative optimized orthogonal matching pursuit (D-OOMP) to solve this problem. An iterative formulation named iterative D-OOMP is further developed to drastically reduce the redundant computation between iterations and a hierarchical selection strategy is integrated for reducing the search space of features. The proposed DNBS representation is applied to object tracking through SSD-based template matching. We validate the effectiveness of our method through extensive experiments on challenging videos with comparisons against several state-of-the-art trackers and demonstrate its capability to track objects in clutter and moving background. version:1
arxiv-1509-08360 | Compressive spectral embedding: sidestepping the SVD | http://arxiv.org/abs/1509.08360 | id:1509.08360 author:Dinesh Ramasamy, Upamanyu Madhow category:stat.ML cs.LG  published:2015-09-28 summary:Spectral embedding based on the Singular Value Decomposition (SVD) is a widely used "preprocessing" step in many learning tasks, typically leading to dimensionality reduction by projecting onto a number of dominant singular vectors and rescaling the coordinate axes (by a predefined function of the singular value). However, the number of such vectors required to capture problem structure grows with problem size, and even partial SVD computation becomes a bottleneck. In this paper, we propose a low-complexity it compressive spectral embedding algorithm, which employs random projections and finite order polynomial expansions to compute approximations to SVD-based embedding. For an m times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)), and the embedding dimension is O(log(m+n)), both of which are independent of the number of singular vectors whose effect we wish to capture. To the best of our knowledge, this is the first work to circumvent this dependence on the number of singular vectors for general SVD-based embeddings. The key to sidestepping the SVD is the observation that, for downstream inference tasks such as clustering and classification, we are only interested in using the resulting embedding to evaluate pairwise similarity metrics derived from the euclidean norm, rather than capturing the effect of the underlying matrix on arbitrary vectors as a partial SVD tries to do. Our numerical results on network datasets demonstrate the efficacy of the proposed method, and motivate further exploration of its application to large-scale inference tasks. version:1
arxiv-1510-04104 | A Preliminary Study on the Learning Informativeness of Data Subsets | http://arxiv.org/abs/1510.04104 | id:1510.04104 author:Simon Kaltenbacher, Nicholas H. Kirk, Dongheui Lee category:cs.CL cs.RO  published:2015-09-28 summary:Estimating the internal state of a robotic system is complex: this is performed from multiple heterogeneous sensor inputs and knowledge sources. Discretization of such inputs is done to capture saliences, represented as symbolic information, which often presents structure and recurrence. As these sequences are used to reason over complex scenarios, a more compact representation would aid exactness of technical cognitive reasoning capabilities, which are today constrained by computational complexity issues and fallback to representational heuristics or human intervention. Such problems need to be addressed to ensure timely and meaningful human-robot interaction. Our work is towards understanding the variability of learning informativeness when training on subsets of a given input dataset. This is in view of reducing the training size while retaining the majority of the symbolic learning potential. We prove the concept on human-written texts, and conjecture this work will reduce training data size of sequential instructions, while preserving semantic relations, when gathering information from large remote sources. version:1
arxiv-1509-06114 | Vision System and Depth Processing for DRC-HUBO+ | http://arxiv.org/abs/1509.06114 | id:1509.06114 author:Inwook Shim, Seunghak Shin, Yunsu Bok, Kyungdon Joo, Dong-Geol Choi, Joon-Young Lee, Jaesik Park, Jun-Ho Oh, In So Kweon category:cs.CV cs.RO  published:2015-09-21 summary:This paper presents a vision system and a depth processing algorithm for DRC-HUBO+, the winner of the DRC finals 2015. Our system is designed to reliably capture 3D information of a scene and objects robust to challenging environment conditions. We also propose a depth-map upsampling method that produces an outliers-free depth map by explicitly handling depth outliers. Our system is suitable for an interactive robot with real-world that requires accurate object detection and pose estimation. We evaluate our depth processing algorithm over state-of-the-art algorithms on several synthetic and real-world datasets. version:2
arxiv-1509-08329 | Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA for the Design of Training Graphs | http://arxiv.org/abs/1509.08329 | id:1509.08329 author:Alberto N. Escalante-B., Laurenz Wiskott category:cs.AI cs.CV stat.ML  published:2015-09-28 summary:Slow feature analysis (SFA) is an unsupervised learning algorithm that extracts slowly varying features from a time series. Graph-based SFA (GSFA) is a supervised extension that can solve regression problems if followed by a post-processing regression algorithm. A training graph specifies arbitrary connections between the training samples. The connections in current graphs, however, only depend on the rank of the involved labels. Exploiting the exact label values makes further improvements in estimation accuracy possible. In this article, we propose the exact label learning (ELL) method to create a graph that codes the desired label explicitly, so that GSFA is able to extract a normalized version of it directly. The ELL method is used for three tasks: (1) We estimate gender from artificial images of human faces (regression) and show the advantage of coding additional labels, particularly skin color. (2) We analyze two existing graphs for regression. (3) We extract compact discriminative features to classify traffic sign images. When the number of output features is limited, a higher classification rate is obtained compared to a graph equivalent to nonlinear Fisher discriminant analysis. The method is versatile, directly supports multiple labels, and provides higher accuracy compared to current graphs for the problems considered. version:1
arxiv-1503-05430 | What Properties are Desirable from an Electron Microscopy Segmentation Algorithm | http://arxiv.org/abs/1503.05430 | id:1503.05430 author:Toufiq Parag category:cs.CV  published:2015-03-18 summary:The prospect of neural reconstruction from Electron Microscopy (EM) images has been elucidated by the automatic segmentation algorithms. Although segmentation algorithms eliminate the necessity of tracing the neurons by hand, significant manual effort is still essential for correcting the mistakes they make. A considerable amount of human labor is also required for annotating groundtruth volumes for training the classifiers of a segmentation framework. It is critically important to diminish the dependence on human interaction in the overall reconstruction system. This study proposes a novel classifier training algorithm for EM segmentation aimed to reduce the amount of manual effort demanded by the groundtruth annotation and error refinement tasks. Instead of using an exhaustive pixel level groundtruth, an active learning algorithm is proposed for sparse labeling of pixel and boundaries of superpixels. Because over-segmentation errors are in general more tolerable and easier to correct than the under-segmentation errors, our algorithm is designed to prioritize minimization of false-merges over false-split mistakes. Our experiments on both 2D and 3D data suggest that the proposed method yields segmentation outputs that are more amenable to neural reconstruction than those of existing methods. version:3
arxiv-1507-06594 | Neural NILM: Deep Neural Networks Applied to Energy Disaggregation | http://arxiv.org/abs/1507.06594 | id:1507.06594 author:Jack Kelly, William Knottenbelt category:cs.NE I.2.6; I.5.2  published:2015-07-23 summary:Energy disaggregation estimates appliance-by-appliance electricity consumption from a single meter that measures the whole home's electricity demand. Recently, deep neural networks have driven remarkable improvements in classification performance in neighbouring machine learning fields such as image classification and automatic speech recognition. In this paper, we adapt three deep neural network architectures to energy disaggregation: 1) a form of recurrent neural network called `long short-term memory' (LSTM); 2) denoising autoencoders; and 3) a network which regresses the start time, end time and average power demand of each appliance activation. We use seven metrics to test the performance of these algorithms on real aggregate power data from five appliances. Tests are performed against a house not seen during training and against houses seen during training. We find that all three neural nets achieve better F1 scores (averaged over all five appliances) than either combinatorial optimisation or factorial hidden Markov models and that our neural net algorithms generalise well to an unseen house. version:3
arxiv-1509-08239 | Analysis of Intelligent Classifiers and Enhancing the Detection Accuracy for Intrusion Detection System | http://arxiv.org/abs/1509.08239 | id:1509.08239 author:Mohanad Albayati, Biju Issac category:cs.CR cs.LG  published:2015-09-28 summary:In this paper we discuss and analyze some of the intelligent classifiers which allows for automatic detection and classification of networks attacks for any intrusion detection system. We will proceed initially with their analysis using the WEKA software to work with the classifiers on a well-known IDS (Intrusion Detection Systems) dataset like NSL-KDD dataset. The NSL-KDD dataset of network attacks was created in a military network by MIT Lincoln Labs. Then we will discuss and experiment some of the hybrid AI (Artificial Intelligence) classifiers that can be used for IDS, and finally we developed a Java software with three most efficient classifiers and compared it with other options. The outputs would show the detection accuracy and efficiency of the single and combined classifiers used. version:1
arxiv-1505-01345 | A Comprehensive Study On The Applications Of Machine Learning For Diagnosis Of Cancer | http://arxiv.org/abs/1505.01345 | id:1505.01345 author:Mohnish Chakravarti, Tanay Kothari category:cs.LG  published:2015-05-06 summary:Collectively, lung cancer, breast cancer and melanoma was diagnosed in over 535,340 people out of which, 209,400 deaths were reported [13]. It is estimated that over 600,000 people will be diagnosed with these forms of cancer in 2015. Most of the deaths from lung cancer, breast cancer and melanoma result due to late detection. All of these cancers, if detected early, are 100% curable. In this study, we develop and evaluate algorithms to diagnose Breast cancer, Melanoma, and Lung cancer. In the first part of the study, we employed a normalised Gradient Descent and an Artificial Neural Network to diagnose breast cancer with an overall accuracy of 91% and 95% respectively. In the second part of the study, an artificial neural network coupled with image processing and analysis algorithms was employed to achieve an overall accuracy of 93% A naive mobile based application that allowed people to take diagnostic tests on their phones was developed. Finally, a Support Vector Machine algorithm incorporating image processing and image analysis algorithms was developed to diagnose lung cancer with an accuracy of 94%. All of the aforementioned systems had very low false positive and false negative rates. We are developing an online network that incorporates all of these systems and allows people to collaborate globally. version:4
arxiv-1509-08197 | Fast Non-local Stereo Matching based on Hierarchical Disparity Prediction | http://arxiv.org/abs/1509.08197 | id:1509.08197 author:Xuan Luo, Xuejiao Bai, Shuo Li, Hongtao Lu, Sei-ichiro Kamata category:cs.CV  published:2015-09-28 summary:Stereo matching is the key step in estimating depth from two or more images. Recently, some tree-based non-local stereo matching methods have been proposed, which achieved state-of-the-art performance. The algorithms employed some tree structures to aggregate cost and thus improved the performance and reduced the coputation load of the stereo matching. However, the computational complexity of these tree-based algorithms is still high because they search over the entire disparity range. In addition, the extreme greediness of the minimum spanning tree (MST) causes the poor performance in large areas with similar colors but varying disparities. In this paper, we propose an efficient stereo matching method using a hierarchical disparity prediction (HDP) framework to dramatically reduce the disparity search range so as to speed up the tree-based non-local stereo methods. Our disparity prediction scheme works on a graph pyramid derived from an image whose disparity to be estimated. We utilize the disparity of a upper graph to predict a small disparity range for the lower graph. Some independent disparity trees (DT) are generated to form a disparity prediction forest (HDPF) over which the cost aggregation is made. When combined with the state-of-the-art tree-based methods, our scheme not only dramatically speeds up the original methods but also improves their performance by alleviating the second drawback of the tree-based methods. This is partially because our DTs overcome the extreme greediness of the MST. Extensive experimental results on some benchmark datasets demonstrate the effectiveness and efficiency of our framework. For example, the segment-tree based stereo matching becomes about 25.57 times faster and 2.2% more accurate over the Middlebury 2006 full-size dataset. version:1
arxiv-1509-08182 | Robust video object tracking using particle filter with likelihood based feature fusion and adaptive template updating | http://arxiv.org/abs/1509.08182 | id:1509.08182 author:Yi Dai, Bin Liu category:cs.CV 68T45 I.4.8; I.5.4  published:2015-09-28 summary:A robust algorithm solution is proposed for tracking an object in complex video scenes. In this solution, the bootstrap particle filter (PF) is initialized by an object detector, which models the time-evolving background of the video signal by an adaptive Gaussian mixture. The motion of the object is expressed by a Markov model, which defines the state transition prior. The color and texture features are used to represent the object, and a marginal likelihood based feature fusion approach is proposed. A corresponding object template model updating procedure is developed to account for possible scale changes of the object in the tracking process. Experimental results show that our algorithm beats several existing alternatives in tackling challenging scenarios in video tracking tasks. version:1
arxiv-1509-05490 | TransA: An Adaptive Approach for Knowledge Graph Embedding | http://arxiv.org/abs/1509.05490 | id:1509.05490 author:Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu category:cs.CL  published:2015-09-18 summary:Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose \textbf{TransA}, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines. version:2
arxiv-1505-00880 | Multi-view Convolutional Neural Networks for 3D Shape Recognition | http://arxiv.org/abs/1505.00880 | id:1505.00880 author:Hang Su, Subhransu Maji, Evangelos Kalogerakis, Erik Learned-Miller category:cs.CV cs.GR  published:2015-05-05 summary:A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives. version:3
arxiv-1509-08112 | Feature Selection for classification of hyperspectral data by minimizing a tight bound on the VC dimension | http://arxiv.org/abs/1509.08112 | id:1509.08112 author:Phool Preet, Sanjit Singh Batra, Jayadeva category:cs.LG 68T05  68T10  68Q32 I.5.1  I.5.2  I.4  published:2015-09-27 summary:Hyperspectral data consists of large number of features which require sophisticated analysis to be extracted. A popular approach to reduce computational cost, facilitate information representation and accelerate knowledge discovery is to eliminate bands that do not improve the classification and analysis methods being applied. In particular, algorithms that perform band elimination should be designed to take advantage of the specifics of the classification method being used. This paper employs a recently proposed filter-feature-selection algorithm based on minimizing a tight bound on the VC dimension. We have successfully applied this algorithm to determine a reasonable subset of bands without any user-defined stopping criteria on widely used hyperspectral images and demonstrate that this method outperforms state-of-the-art methods in terms of both sparsity of feature set as well as accuracy of classification.\end{abstract} version:1
arxiv-1504-08083 | Fast R-CNN | http://arxiv.org/abs/1504.08083 | id:1504.08083 author:Ross Girshick category:cs.CV  published:2015-04-30 summary:This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn. version:2
arxiv-1509-01698 | HAMSI: Distributed Incremental Optimization Algorithm Using Quadratic Approximations for Partially Separable Problems | http://arxiv.org/abs/1509.01698 | id:1509.01698 author:Umut Şimşekli, Hazal Koptagel, Figen Öztoprak, Ş. İlker Birbil, Ali Taylan Cemgil category:stat.ML cs.LG  published:2015-09-05 summary:We propose HAMSI, a provably convergent incremental algorithm for solving large-scale partially separable optimization problems that frequently emerge in machine learning and inferential statistics. The algorithm is based on a local quadratic approximation and hence allows incorporating a second order curvature information to speed-up the convergence. Furthermore, HAMSI needs almost no tuning, and it is scalable as well as easily parallelizable. In large-scale simulation studies with the MovieLens datasets, we illustrate that the method is superior to a state-of-the-art distributed stochastic gradient descent method in terms of convergence behavior. This performance gain comes at the expense of using memory that scales only linearly with the total size of the optimization variables. We conclude that HAMSI may be considered as a viable alternative in many scenarios, where first order methods based on variants of stochastic gradient descent are applicable. version:2
arxiv-1506-01929 | Learning to track for spatio-temporal action localization | http://arxiv.org/abs/1506.01929 | id:1506.01929 author:Philippe Weinzaepfel, Zaid Harchaoui, Cordelia Schmid category:cs.CV  published:2015-06-05 summary:We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP. version:2
arxiv-1509-08083 | Non-asymptotic Analysis of $\ell_1$-norm Support Vector Machines | http://arxiv.org/abs/1509.08083 | id:1509.08083 author:Anton Kolleck, Jan Vybíral category:cs.IT cs.LG math.FA math.IT math.ST stat.TH  published:2015-09-27 summary:Support Vector Machines (SVM) with $\ell_1$ penalty became a standard tool in analysis of highdimensional classification problems with sparsity constraints in many applications including bioinformatics and signal processing. Although SVM have been studied intensively in the literature, this paper has to our knowledge first non-asymptotic results on the performance of $\ell_1$-SVM in identification of sparse classifiers. We show that a $d$-dimensional $s$-sparse classification vector can be (with high probability) well approximated from only $O(s\log(d))$ Gaussian trials. The methods used in the proof include concentration of measure and probability in Banach spaces. version:1
arxiv-1509-08075 | Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing | http://arxiv.org/abs/1509.08075 | id:1509.08075 author:Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi category:cs.CV  published:2015-09-27 summary:We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset. version:1
arxiv-1501-07359 | Learning And-Or Models to Represent Context and Occlusion for Car Detection and Viewpoint Estimation | http://arxiv.org/abs/1501.07359 | id:1501.07359 author:Tianfu Wu, Bo Li, Song-Chun Zhu category:cs.CV  published:2015-01-29 summary:This paper presents a method for learning And-Or models to represent context and occlusion for car detection and viewpoint estimation. The learned And-Or model represents car-to-car context and occlusion configurations at three levels: (i) spatially-aligned cars, (ii) single car under different occlusion configurations, and (iii) a small number of parts. The And-Or model embeds a grammar for representing large structural and appearance variations in a reconfigurable hierarchy. The learning process consists of two stages in a weakly supervised way (i.e., only bounding boxes of single cars are annotated). Firstly, the structure of the And-Or model is learned with three components: (a) mining multi-car contextual patterns based on layouts of annotated single car bounding boxes, (b) mining occlusion configurations between single cars, and (c) learning different combinations of part visibility based on car 3D CAD simulation. The And-Or model is organized in a directed and acyclic graph which can be inferred by Dynamic Programming. Secondly, the model parameters (for appearance, deformation and bias) are jointly trained using Weak-Label Structural SVM. In experiments, we test our model on four car detection datasets --- the KITTI dataset \cite{Geiger12}, the PASCAL VOC2007 car dataset~\cite{pascal}, and two self-collected car datasets, namely the Street-Parking car dataset and the Parking-Lot car dataset, and three datasets for car viewpoint estimation --- the PASCAL VOC2006 car dataset~\cite{pascal}, the 3D car dataset~\cite{savarese}, and the PASCAL3D+ car dataset~\cite{xiang_wacv14}. Compared with state-of-the-art variants of deformable part-based models and other methods, our model achieves significant improvement consistently on the four detection datasets, and comparable performance on car viewpoint estimation. version:2
arxiv-1509-08062 | End-to-End Text-Dependent Speaker Verification | http://arxiv.org/abs/1509.08062 | id:1509.08062 author:Georg Heigold, Ignacio Moreno, Samy Bengio, Noam Shazeer category:cs.LG cs.SD  published:2015-09-27 summary:In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal "Ok Google" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications like ours that require highly accurate, easy-to-maintain systems with a small footprint. version:1
arxiv-1509-07450 | A 128 channel Extreme Learning Machine based Neural Decoder for Brain Machine Interfaces | http://arxiv.org/abs/1509.07450 | id:1509.07450 author:Yi Chen, Enyi Yao, Arindam Basu category:cs.LG cs.HC  published:2015-09-22 summary:Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning co-processor in 0.35um CMOS for motor intention decoding in brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3% for movement type. The same co-processor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5% with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by ~2X. version:2
arxiv-1509-08038 | Deep Trans-layer Unsupervised Networks for Representation Learning | http://arxiv.org/abs/1509.08038 | id:1509.08038 author:Wentao Zhu, Jun Miao, Laiyun Qing, Xilin Chen category:cs.NE cs.CV cs.LG  published:2015-09-27 summary:Learning features from massive unlabelled data is a vast prevalent topic for high-level tasks in many machine learning applications. The recent great improvements on benchmark data sets achieved by increasingly complex unsupervised learning methods and deep learning models with lots of parameters usually requires many tedious tricks and much expertise to tune. However, filters learned by these complex architectures are quite similar to standard hand-crafted features visually. In this paper, unsupervised learning methods, such as PCA or auto-encoder, are employed as the building block to learn filter banks at each layer. The lower layer responses are transferred to the last layer (trans-layer) to form a more complete representation retaining more information. In addition, some beneficial methods such as local contrast normalization and whitening are added to the proposed deep trans-layer networks to further boost performance. The trans-layer representations are followed by block histograms with binary encoder schema to learn translation and rotation invariant representations, which are utilized to do high-level tasks such as recognition and classification. Compared to traditional deep learning methods, the implemented feature learning method has much less parameters and is validated in several typical experiments, such as digit recognition on MNIST and MNIST variations, object recognition on Caltech 101 dataset and face verification on LFW dataset. The deep trans-layer unsupervised learning achieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per class and 75.98% accuracy on 30 samples per class on Caltech 101 dataset, 87.10% on LFW dataset. version:1
arxiv-1505-02146 | DeepBox: Learning Objectness with Convolutional Networks | http://arxiv.org/abs/1505.02146 | id:1505.02146 author:Weicheng Kuo, Bharath Hariharan, Jitendra Malik category:cs.CV  published:2015-05-08 summary:Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that objectness is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image. version:2
arxiv-1011-3189 | Warping Peirce Quincuncial Panoramas | http://arxiv.org/abs/1011.3189 | id:1011.3189 author:Chamberlain Fong, Brian K. Vogel category:cs.CV cs.GR  published:2010-11-14 summary:The Peirce quincuncial projection is a mapping of the surface of a sphere to the interior of a square. It is a conformal map except for four points on the equator. These points of non-conformality cause significant artifacts in photographic applications. In this paper, we propose an algorithm and user-interface to mitigate these artifacts. Moreover, in order to facilitate an interactive user-interface, we present a fast algorithm for calculating the Peirce quincuncial projection of spherical imagery. We then promote the Peirce quincuncial projection as a viable alternative to the more popular stereographic projection in some scenarios. version:5
arxiv-1509-07982 | Targeted Fused Ridge Estimation of Inverse Covariance Matrices from Multiple High-Dimensional Data Classes | http://arxiv.org/abs/1509.07982 | id:1509.07982 author:Anders Ellern Bilgrau, Carel F. W. Peeters, Poul Svante Eriksen, Martin Bøgsted, Wessel N. van Wieringen category:stat.ME q-bio.MN stat.ML  published:2015-09-26 summary:We consider the problem of jointly estimating multiple precision matrices from (aggregated) high-dimensional data consisting of distinct classes. An $\ell_2$-penalized maximum-likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other $\ell_2$-penalized estimators as special cases. In addition, the approach allows for the specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. It has many applications in (multi)factorial study designs. We focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic Gaussian graphical modeling. Situations are considered in which the classes are defined by data sets and/or (subtypes of) diseases. The performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. Its practical usability is illustrated by the differential network modeling of 11 large-scale diffuse large B-cell lymphoma gene expression data sets. The estimator and its related procedures are incorporated into the R-package rags2ridges. version:1
arxiv-1509-07975 | Modeling Curiosity in a Mobile Robot for Long-Term Autonomous Exploration and Monitoring | http://arxiv.org/abs/1509.07975 | id:1509.07975 author:Yogesh Girdhar, Gregory Dudek category:cs.RO cs.CV cs.LG  published:2015-09-26 summary:This paper presents a novel approach to modeling curiosity in a mobile robot, which is useful for monitoring and adaptive data collection tasks, especially in the context of long term autonomous missions where pre-programmed missions are likely to have limited utility. We use a realtime topic modeling technique to build a semantic perception model of the environment, using which, we plan a path through the locations in the world with high semantic information content. The life-long learning behavior of the proposed perception model makes it suitable for long-term exploration missions. We validate the approach using simulated exploration experiments using aerial and underwater data, and demonstrate an implementation on the Aqua underwater robot in a variety of scenarios. We find that the proposed exploration paths that are biased towards locations with high topic perplexity, produce better terrain models with high discriminative power. Moreover, we show that the proposed algorithm implemented on Aqua robot is able to do tasks such as coral reef inspection, diver following, and sea floor exploration, without any prior training or preparation. version:1
arxiv-1503-07340 | A Bayesian Approach to Sparse plus Low rank Network Identification | http://arxiv.org/abs/1503.07340 | id:1503.07340 author:Mattia Zorzi, Alessandro Chiuso category:math.OC stat.ML  published:2015-03-25 summary:We consider the problem of modeling multivariate time series with parsimonious dynamical models which can be represented as sparse dynamic Bayesian networks with few latent nodes. This structure translates into a sparse plus low rank model. In this paper, we propose a Gaussian regression approach to identify such a model. version:2
arxiv-1506-07704 | AttentionNet: Aggregating Weak Directions for Accurate Object Detection | http://arxiv.org/abs/1506.07704 | id:1506.07704 author:Donggeun Yoo, Sunggyun Park, Joon-Young Lee, Anthony S. Paek, In So Kweon category:cs.CV cs.LG  published:2015-06-25 summary:We present a novel detection method using a deep convolutional neural network (CNN), named AttentionNet. We cast an object detection problem as an iterative classification problem, which is the most suitable form of a CNN. AttentionNet provides quantized weak directions pointing a target object and the ensemble of iterative predictions from AttentionNet converges to an accurate object boundary box. Since AttentionNet is a unified network for object detection, it detects objects without any separated models from the object proposal to the post bounding-box regression. We evaluate AttentionNet by a human detection task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 with an 8-layered architecture only. version:2
arxiv-1509-07946 | A Revisit of Infinite Population Models for Evolutionary Algorithms on Continuous Optimization Problems | http://arxiv.org/abs/1509.07946 | id:1509.07946 author:Bo Song, Victor O. K. Li category:cs.NE math.OC  published:2015-09-26 summary:Infinite population models are important tools for studying population dynamics of evolutionary algorithms. They describe how the distributions of populations change between consecutive generations. In general, infinite population models are derived from Markov chains by exploiting symmetries between individuals in the population and analyzing the limit as the population size goes to infinity. In this paper, we study the theoretical foundations of infinite population models of evolutionary algorithms on continuous optimization problems. First, we show that the convergence proofs in a widely cited study were in fact problematic and incomplete. We further show that the modeling assumption of exchangeability of individuals cannot yield the transition equation. Then, in order to analyze infinite population models, we build an analytical framework based on convergence in distribution of random elements which take values in the metric space of infinite sequences. The framework is concise and mathematically rigorous. It also provides an infrastructure for studying the convergence of the stacking of operators and of iterating the algorithm which previous studies failed to address. Finally, we use the framework to prove the convergence of infinite population models for the mutation operator and the $k$-ary recombination operator. We show that these operators can provide accurate predictions for real population dynamics as the population size goes to infinity, provided that the initial population is identically and independently distributed. version:1
arxiv-1505-00256 | DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving | http://arxiv.org/abs/1505.00256 | id:1505.00256 author:Chenyi Chen, Ari Seff, Alain Kornhauser, Jianxiong Xiao category:cs.CV  published:2015-05-01 summary:Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website. version:3
arxiv-1412-7415 | A prototype Malayalam to Sign Language Automatic Translator | http://arxiv.org/abs/1412.7415 | id:1412.7415 author:Jestin Joy, Kannan Balakrishnan category:cs.CL  published:2014-12-23 summary:Sign language, which is a medium of communication for deaf people, uses manual communication and body language to convey meaning, as opposed to using sound. This paper presents a prototype Malayalam text to sign language translation system. The proposed system takes Malayalam text as input and generates corresponding Sign Language. Output animation is rendered using a computer generated model. This system will help to disseminate information to the deaf people in public utility places like railways, banks, hospitals etc. This will also act as an educational tool in learning Sign Language. version:2
arxiv-1509-07943 | Super-Resolution Off the Grid | http://arxiv.org/abs/1509.07943 | id:1509.07943 author:Qingqing Huang, Sham M. Kakade category:cs.LG  published:2015-09-26 summary:Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly. Suppose we have k point sources in d dimensions, where the points are separated by at least \Delta from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees: - The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/\Delta) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as {\Omega}( d/\Delta). - The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation \Delta. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities. Our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hyper-grid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition). version:1
arxiv-1509-07927 | Algorithms for Linear Bandits on Polyhedral Sets | http://arxiv.org/abs/1509.07927 | id:1509.07927 author:Manjesh K. Hanawal, Amir Leshem, Venkatesh Saligrama category:cs.LG  published:2015-09-26 summary:We study stochastic linear optimization problem with bandit feedback. The set of arms take values in an $N$-dimensional space and belong to a bounded polyhedron described by finitely many linear inequalities. We provide a lower bound for the expected regret that scales as $\Omega(N\log T)$. We then provide a nearly optimal algorithm and show that its expected regret scales as $O(N\log^{1+\epsilon}(T))$ for an arbitrary small $\epsilon >0$. The algorithm alternates between exploration and exploitation intervals sequentially where deterministic set of arms are played in the exploration intervals and greedily selected arm is played in the exploitation intervals. We also develop an algorithm that achieves the optimal regret when sub-Gaussianity parameter of the noise term is known. Our key insight is that for a polyhedron the optimal arm is robust to small perturbations in the reward function. Consequently, a greedily selected arm is guaranteed to be optimal when the estimation error falls below some suitable threshold. Our solution resolves a question posed by Rusmevichientong and Tsitsiklis (2011) that left open the possibility of efficient algorithms with asymptotic logarithmic regret bounds. We also show that the regret upper bounds hold with probability $1$. Our numerical investigations show that while theoretical results are asymptotic the performance of our algorithms compares favorably to state-of-the-art algorithms in finite time as well. version:1
arxiv-1509-04767 | Zero-Shot Learning via Semantic Similarity Embedding | http://arxiv.org/abs/1509.04767 | id:1509.04767 author:Ziming Zhang, Venkatesh Saligrama category:cs.CV stat.ML  published:2015-09-15 summary:In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (\eg attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition. version:2
arxiv-1509-03502 | Person Recognition in Personal Photo Collections | http://arxiv.org/abs/1509.03502 | id:1509.03502 author:Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele category:cs.CV  published:2015-09-11 summary:Recognising persons in everyday photos presents major challenges (occluded faces, different clothing, locations, etc.) for machine vision. We propose a convnet based person recognition system on which we provide an in-depth analysis of informativeness of different body cues, impact of training data, and the common failure modes of the system. In addition, we discuss the limitations of existing benchmarks and propose more challenging ones. Our method is simple and is built on open source and open data, yet it improves the state of the art results on a large dataset of social media photos (PIPA). version:2
arxiv-1509-07845 | Selecting Relevant Web Trained Concepts for Automated Event Retrieval | http://arxiv.org/abs/1509.07845 | id:1509.07845 author:Bharat Singh, Xintong Han, Zhe Wu, Vlad I. Morariu, Larry S. Davis category:cs.CV cs.CL cs.IR  published:2015-09-25 summary:Complex event retrieval is a challenging research problem, especially when no training videos are available. An alternative to collecting training videos is to train a large semantic concept bank a priori. Given a text description of an event, event retrieval is performed by selecting concepts linguistically related to the event description and fusing the concept responses on unseen videos. However, defining an exhaustive concept lexicon and pre-training it requires vast computational resources. Therefore, recent approaches automate concept discovery and training by leveraging large amounts of weakly annotated web data. Compact visually salient concepts are automatically obtained by the use of concept pairs or, more generally, n-grams. However, not all visually salient n-grams are necessarily useful for an event query--some combinations of concepts may be visually compact but irrelevant--and this drastically affects performance. We propose an event retrieval algorithm that constructs pairs of automatically discovered concepts and then prunes those concepts that are unlikely to be helpful for retrieval. Pruning depends both on the query and on the specific video instance being evaluated. Our approach also addresses calibration and domain adaptation issues that arise when applying concept detectors to unseen videos. We demonstrate large improvements over other vision based systems on the TRECVID MED 13 dataset. version:1
arxiv-1506-02629 | Generalization in Adaptive Data Analysis and Holdout Reuse | http://arxiv.org/abs/1506.02629 | id:1506.02629 author:Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, Aaron Roth category:cs.LG cs.DS  published:2015-06-08 summary:Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in (Dwork et al., 2014), where we focused on the problem of estimating expectations of adaptively chosen functions. In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment. We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in (Dwork et al., 2014) is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. version:2
arxiv-1509-07831 | Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories | http://arxiv.org/abs/1509.07831 | id:1509.07831 author:Jaeyong Sung, Ian Lenz, Ashutosh Saxena category:cs.RO cs.AI cs.CV cs.LG  published:2015-09-25 summary:A robot operating in a real-world environment needs to perform reasoning with a variety of sensing modalities. However, manually designing features that allow a learning algorithm to relate these different modalities can be extremely challenging. In this work, we consider the task of manipulating novel objects and appliances. To this end, we learn to embed point-cloud, natural language, and manipulation trajectory data into a shared embedding space using a deep neural network. In order to learn semantically meaningful spaces throughout our network, we introduce a method for pre-training its lower layers for multimodal feature embedding and a method for fine-tuning this embedding space using a loss-based margin. We test our model on the Robobarista dataset [22], where we achieve significant improvements in both accuracy and inference time over the previous state of the art. version:1
arxiv-1509-07823 | Computational Intelligence Challenges and Applications on Large-Scale Astronomical Time Series Databases | http://arxiv.org/abs/1509.07823 | id:1509.07823 author:Pablo Huijse, Pablo A. Estevez, Pavlos Protopapas, Jose C. Principe, Pablo Zegers category:astro-ph.IM cs.LG  published:2015-09-25 summary:Time-domain astronomy (TDA) is facing a paradigm shift caused by the exponential growth of the sample size, data complexity and data generation rates of new astronomical sky surveys. For example, the Large Synoptic Survey Telescope (LSST), which will begin operations in northern Chile in 2022, will generate a nearly 150 Petabyte imaging dataset of the southern hemisphere sky. The LSST will stream data at rates of 2 Terabytes per hour, effectively capturing an unprecedented movie of the sky. The LSST is expected not only to improve our understanding of time-varying astrophysical objects, but also to reveal a plethora of yet unknown faint and fast-varying phenomena. To cope with a change of paradigm to data-driven astronomy, the fields of astroinformatics and astrostatistics have been created recently. The new data-oriented paradigms for astronomy combine statistics, data mining, knowledge discovery, machine learning and computational intelligence, in order to provide the automated and robust methods needed for the rapid detection and classification of known astrophysical objects as well as the unsupervised characterization of novel phenomena. In this article we present an overview of machine learning and computational intelligence applications to TDA. Future big data challenges and new lines of research in TDA, focusing on the LSST, are identified and discussed from the viewpoint of computational intelligence/machine learning. Interdisciplinary collaboration will be required to cope with the challenges posed by the deluge of astronomical data coming from the LSST. version:1
arxiv-1506-00511 | Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions | http://arxiv.org/abs/1506.00511 | id:1506.00511 author:Jimmy Ba, Kevin Swersky, Sanja Fidler, Ruslan Salakhutdinov category:cs.LG cs.CV cs.NE  published:2015-06-01 summary:One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods. version:2
arxiv-1408-3115 | On Data Preconditioning for Regularized Loss Minimization | http://arxiv.org/abs/1408.3115 | id:1408.3115 author:Tianbao Yang, Rong Jin, Shenghuo Zhu, Qihang Lin category:cs.NA cs.LG stat.ML  published:2014-08-13 summary:In this work, we study data preconditioning, a well-known and long-existing technique, for boosting the convergence of first-order methods for regularized loss minimization. It is well understood that the condition number of the problem, i.e., the ratio of the Lipschitz constant to the strong convexity modulus, has a harsh effect on the convergence of the first-order optimization methods. Therefore, minimizing a small regularized loss for achieving good generalization performance, yielding an ill conditioned problem, becomes the bottleneck for big data problems. We provide a theory on data preconditioning for regularized loss minimization. In particular, our analysis exhibits an appropriate data preconditioner and characterizes the conditions on the loss function and on the data under which data preconditioning can reduce the condition number and therefore boost the convergence for minimizing the regularized loss. To make the data preconditioning practically useful, we endeavor to employ and analyze a random sampling approach to efficiently compute the preconditioned data. The preliminary experiments validate our theory. version:4
arxiv-1509-07755 | A Mathematical Theory for Clustering in Metric Spaces | http://arxiv.org/abs/1509.07755 | id:1509.07755 author:Cheng-Shang Chang, Wanjiun Liao, Yu-Sheng Chen, Li-Heng Liou category:cs.LG  published:2015-09-25 summary:Clustering is one of the most fundamental problems in data analysis and it has been studied extensively in the literature. Though many clustering algorithms have been proposed, clustering theories that justify the use of these clustering algorithms are still unsatisfactory. In particular, one of the fundamental challenges is to address the following question: What is a cluster in a set of data points? In this paper, we make an attempt to address such a question by considering a set of data points associated with a distance measure (metric). We first propose a new cohesion measure in terms of the distance measure. Using the cohesion measure, we define a cluster as a set of points that are cohesive to themselves. For such a definition, we show there are various equivalent statements that have intuitive explanations. We then consider the second question: How do we find clusters and good partitions of clusters under such a definition? For such a question, we propose a hierarchical agglomerative algorithm and a partitional algorithm. Unlike standard hierarchical agglomerative algorithms, our hierarchical agglomerative algorithm has a specific stopping criterion and it stops with a partition of clusters. Our partitional algorithm, called the K-sets algorithm in the paper, appears to be a new iterative algorithm. Unlike the Lloyd iteration that needs two-step minimization, our K-sets algorithm only takes one-step minimization. One of the most interesting findings of our paper is the duality result between a distance measure and a cohesion measure. Such a duality result leads to a dual K-sets algorithm for clustering a set of data points with a cohesion measure. The dual K-sets algorithm converges in the same way as a sequential version of the classical kernel K-means algorithm. The key difference is that a cohesion measure does not need to be positive semi-definite. version:1
arxiv-1509-07751 | Efficient Computation of the Quasi Likelihood function for Discretely Observed Diffusion Processes | http://arxiv.org/abs/1509.07751 | id:1509.07751 author:Lars Josef Höök, Erik Lindström category:stat.CO q-fin.ST stat.ML  published:2015-09-25 summary:We introduce a simple method for nearly simultaneous computation of all moments needed for quasi maximum likelihood estimation of parameters in discretely observed stochastic differential equations commonly seen in finance. The method proposed in this papers is not restricted to any particular dynamics of the differential equation and is virtually insensitive to the sampling interval. The key contribution of the paper is that computational complexity is sublinear in the number of observations as we compute all moments through a single operation. Furthermore, that operation can be done offline. The simulations show that the method is unbiased for all practical purposes for any sampling design, including random sampling, and that the computational cost is comparable (actually faster for moderate and large data sets) to the simple, often severely biased, Euler-Maruyama approximation. version:1
arxiv-1509-07728 | Online Stochastic Linear Optimization under One-bit Feedback | http://arxiv.org/abs/1509.07728 | id:1509.07728 author:Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou category:cs.LG  published:2015-09-25 summary:In this paper, we study a special bandit setting of online stochastic linear optimization, where only one-bit of information is revealed to the learner at each round. This problem has found many applications including online advertisement and online recommendation. We assume the binary feedback is a random variable generated from the logit model, and aim to minimize the regret defined by the unknown linear function. Although the existing method for generalized linear bandit can be applied to our problem, the high computational cost makes it impractical for real-world problems. To address this challenge, we develop an efficient online learning algorithm by exploiting particular structures of the observation model. Specifically, we adopt online Newton step to estimate the unknown parameter and derive a tight confidence region based on the exponential concavity of the logistic loss. Our analysis shows that the proposed algorithm achieves a regret bound of $O(d\sqrt{T})$, which matches the optimal result of stochastic linear bandits. version:1
arxiv-1507-02750 | Utility-based Dueling Bandits as a Partial Monitoring Game | http://arxiv.org/abs/1507.02750 | id:1507.02750 author:Pratik Gajane, Tanguy Urvoy category:cs.LG  published:2015-07-10 summary:Partial monitoring is a generic framework for sequential decision-making with incomplete feedback. It encompasses a wide class of problems such as dueling bandits, learning with expect advice, dynamic pricing, dark pools, and label efficient prediction. We study the utility-based dueling bandit problem as an instance of partial monitoring problem and prove that it fits the time-regret partial monitoring hierarchy as an easy - i.e. Theta (sqrt{T})- instance. We survey some partial monitoring algorithms and see how they could be used to solve dueling bandits efficiently. Keywords: Online learning, Dueling Bandits, Partial Monitoring, Partial Feedback, Multiarmed Bandits version:2
arxiv-1509-08302 | A hybrid COA$ε$-constraint method for solving multi-objective problems | http://arxiv.org/abs/1509.08302 | id:1509.08302 author:Mahdi parvizi, Elham Shadkam, Niloofar Jahani category:cs.NE  published:2015-09-25 summary:In this paper, a hybrid method for solving multi-objective problem has been provided. The proposed method is combining the {\epsilon}-Constraint and the Cuckoo algorithm. First the multi objective problem transfers into a single-objective problem using $\epsilon$-Constraint, then the Cuckoo optimization algorithm will optimize the problem in each task. At last the optimized Pareto frontier will be drawn. The advantage of this method is the high accuracy and the dispersion of its Pareto frontier. In order to testing the efficiency of the suggested method, a lot of test problems have been solved using this method. Comparing the results of this method with the results of other similar methods shows that the Cuckoo algorithm is more suitable for solving the multi-objective problems. version:1
arxiv-1503-03429 | Dense image registration and deformable surface reconstruction in presence of occlusions and minimal texture | http://arxiv.org/abs/1503.03429 | id:1503.03429 author:Dat Tien Ngo, Sanghuyk Park, Anne Jorstad, Alberto Crivellaro, Chang Yoo, Pascal Fua category:cs.CV  published:2015-03-11 summary:Deformable surface tracking from monocular images is well-known to be under-constrained. Occlusions often make the task even more challenging, and can result in failure if the surface is not sufficiently textured. In this work, we explicitly address the problem of 3D reconstruction of poorly textured, occluded surfaces, proposing a framework based on a template-matching approach that scales dense robust features by a relevancy score. Our approach is extensively compared to current methods employing both local feature matching and dense template alignment. We test on standard datasets as well as on a new dataset (that will be made publicly available) of a sparsely textured, occluded surface. Our framework achieves state-of-the-art results for both well and poorly textured, occluded surfaces. version:3
arxiv-1509-07627 | Feature Evaluation of Deep Convolutional Neural Networks for Object Recognition and Detection | http://arxiv.org/abs/1509.07627 | id:1509.07627 author:Hirokatsu Kataoka, Kenji Iwata, Yutaka Satoh category:cs.CV cs.AI cs.MM  published:2015-09-25 summary:In this paper, we evaluate convolutional neural network (CNN) features using the AlexNet architecture and very deep convolutional network (VGGNet) architecture. To date, most CNN researchers have employed the last layers before output, which were extracted from the fully connected feature layers. However, since it is unlikely that feature representation effectiveness is dependent on the problem, this study evaluates additional convolutional layers that are adjacent to fully connected layers, in addition to executing simple tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and transformation, using tools such as principal component analysis. In our experiments, we carried out detection and classification tasks using the Caltech 101 and Daimler Pedestrian Benchmark Datasets. version:1
arxiv-1509-07618 | Self-localization Using Visual Experience Across Domains | http://arxiv.org/abs/1509.07618 | id:1509.07618 author:Taisho Tsukamoto, Kanji Tanaka category:cs.CV  published:2015-09-25 summary:In this study, we aim to solve the single-view robot self-localization problem by using visual experience across domains. Although the bag-of-words method constitutes a popular approach to single-view localization, it fails badly when it's visual vocabulary is learned and tested in different domains. Further, we are interested in using a cross-domain setting, in which the visual vocabulary is learned in different seasons and routes from the input query/database scenes. Our strategy is to mine a cross-domain visual experience, a library of raw visual images collected in different domains, to discover the relevant visual patterns that effectively explain the input scene, and use them for scene retrieval. In particular, we show that the appearance and the pose of the mined visual patterns of a query scene can be efficiently and discriminatively matched against those of the database scenes by employing image-to-class distance and spatial pyramid matching. Experimental results obtained using a novel cross-domain dataset show that our system achieves promising results despite our visual vocabulary being learned and tested in different domains. version:1
arxiv-1509-07615 | Discriminative Map Retrieval Using View-Dependent Map Descriptor | http://arxiv.org/abs/1509.07615 | id:1509.07615 author:Enfu Liu, Kanji Tanaka category:cs.CV  published:2015-09-25 summary:Map retrieval, the problem of similarity search over a large collection of 2D pointset maps previously built by mobile robots, is crucial for autonomous navigation in indoor and outdoor environments. Bag-of-words (BoW) methods constitute a popular approach to map retrieval; however, these methods have extremely limited descriptive ability because they ignore the spatial layout information of the local features. The main contribution of this paper is an extension of the bag-of-words map retrieval method to enable the use of spatial information from local features. Our strategy is to explicitly model a unique viewpoint of an input local map; the pose of the local feature is defined with respect to this unique viewpoint, and can be viewed as an additional invariant feature for discriminative map retrieval. Specifically, we wish to determine a unique viewpoint that is invariant to moving objects, clutter, occlusions, and actual viewpoints. Hence, we perform scene parsing to analyze the scene structure, and consider the "center" of the scene structure to be the unique viewpoint. Our scene parsing is based on a Manhattan world grammar that imposes a quasi-Manhattan world constraint to enable the robust detection of a scene structure that is invariant to clutter and moving objects. Experimental results using the publicly available radish dataset validate the efficacy of the proposed approach. version:1
arxiv-1509-07612 | Sentiment Uncertainty and Spam in Twitter Streams and Its Implications for General Purpose Realtime Sentiment Analysis | http://arxiv.org/abs/1509.07612 | id:1509.07612 author:Nils Haldenwang, Oliver Vornberger category:cs.CL  published:2015-09-25 summary:State of the art benchmarks for Twitter Sentiment Analysis do not consider the fact that for more than half of the tweets from the public stream a distinct sentiment cannot be chosen. This paper provides a new perspective on Twitter Sentiment Analysis by highlighting the necessity of explicitly incorporating uncertainty. Moreover, a dataset of high quality to evaluate solutions for this new problem is introduced and made publicly available. version:1
arxiv-1509-07611 | Incremental Loop Closure Verification by Guided Sampling | http://arxiv.org/abs/1509.07611 | id:1509.07611 author:Kanji Tanaka category:cs.CV  published:2015-09-25 summary:Loop closure detection, the task of identifying locations revisited by a robot in a sequence of odometry and perceptual observations, is typically formulated as a combination of two subtasks: (1) bag-of-words image retrieval and (2) post-verification using RANSAC geometric verification. The main contribution of this study is the proposal of a novel post-verification framework that achieves good precision recall trade-off in loop closure detection. This study is motivated by the fact that not all loop closure hypotheses are equally plausible (e.g., owing to mutual consistency between loop closure constraints) and that if we have evidence that one hypothesis is more plausible than the others, then it should be verified more frequently. We demonstrate that the problem of loop closure detection can be viewed as an instance of a multi-model hypothesize-and-verify framework and build guided sampling strategies on the framework where loop closures proposed using image retrieval are verified in a planned order (rather than in a conventional uniform order) to operate in a constant time. Experimental results using a stereo SLAM system confirm that the proposed strategy, the use of loop closure constraints and robot trajectory hypotheses as a guide, achieves promising results despite the fact that there exists a significant number of false positive constraints and hypotheses. version:1
arxiv-1504-05929 | A Hierarchical Distance-dependent Bayesian Model for Event Coreference Resolution | http://arxiv.org/abs/1504.05929 | id:1504.05929 author:Bishan Yang, Claire Cardie, Peter Frazier category:cs.CL stat.ML  published:2015-04-22 summary:We present a novel hierarchical distance-dependent Bayesian model for event coreference resolution. While existing generative models for event coreference resolution are completely unsupervised, our model allows for the incorporation of pairwise distances between event mentions -- information that is widely used in supervised coreference models to guide the generative clustering processing for better event clustering both within and across documents. We model the distances between event mentions using a feature-rich learnable distance function and encode them as Bayesian priors for nonparametric clustering. Experiments on the ECB+ corpus show that our model outperforms state-of-the-art methods for both within- and cross-document event coreference resolution. version:2
arxiv-1509-07553 | Linear-time Learning on Distributions with Approximate Kernel Embeddings | http://arxiv.org/abs/1509.07553 | id:1509.07553 author:Dougal J. Sutherland, Junier B. Oliva, Barnabás Póczos, Jeff Schneider category:stat.ML cs.LG  published:2015-09-24 summary:Many interesting machine learning problems are best posed by considering instances that are distributions, or sample sets drawn from distributions. Previous work devoted to machine learning tasks with distributional inputs has done so through pairwise kernel evaluations between pdfs (or sample sets). While such an approach is fine for smaller datasets, the computation of an $N \times N$ Gram matrix is prohibitive in large datasets. Recent scalable estimators that work over pdfs have done so only with kernels that use Euclidean metrics, like the $L_2$ distance. However, there are a myriad of other useful metrics available, such as total variation, Hellinger distance, and the Jensen-Shannon divergence. This work develops the first random features for pdfs whose dot product approximates kernels using these non-Euclidean metrics, allowing estimators using such kernels to scale to large datasets by working in a primal space, without computing large Gram matrices. We provide an analysis of the approximation error in using our proposed random features and show empirically the quality of our approximation both in estimating a Gram matrix and in solving learning tasks in real-world and synthetic data. version:1
arxiv-1509-07543 | On Optimizing Human-Machine Task Assignments | http://arxiv.org/abs/1509.07543 | id:1509.07543 author:Andreas Veit, Michael Wilber, Rajan Vaish, Serge Belongie, James Davis, Vishal Anand, Anshu Aviral, Prithvijit Chakrabarty, Yash Chandak, Sidharth Chaturvedi, Chinmaya Devaraj, Ankit Dhall, Utkarsh Dwivedi, Sanket Gupte, Sharath N. Sridhar, Karthik Paga, Anuj Pahuja, Aditya Raisinghani, Ayush Sharma, Shweta Sharma, Darpana Sinha, Nisarg Thakkar, K. Bala Vignesh, Utkarsh Verma, Kanniganti Abhishek, Amod Agrawal, Arya Aishwarya, Aurgho Bhattacharjee, Sarveshwaran Dhanasekar, Venkata Karthik Gullapalli, Shuchita Gupta, Chandana G, Kinjal Jain, Simran Kapur, Meghana Kasula, Shashi Kumar, Parth Kundaliya, Utkarsh Mathur, Alankrit Mishra, Aayush Mudgal, Aditya Nadimpalli, Munakala Sree Nihit, Akanksha Periwal, Ayush Sagar, Ayush Shah, Vikas Sharma, Yashovardhan Sharma, Faizal Siddiqui, Virender Singh, Abhinav S., Anurag. D. Yadav category:cs.HC cs.CV  published:2015-09-24 summary:When crowdsourcing systems are used in combination with machine inference systems in the real world, they benefit the most when the machine system is deeply integrated with the crowd workers. However, if researchers wish to integrate the crowd with "off-the-shelf" machine classifiers, this deep integration is not always possible. This work explores two strategies to increase accuracy and decrease cost under this setting. First, we show that reordering tasks presented to the human can create a significant accuracy improvement. Further, we show that greedily choosing parameters to maximize machine accuracy is sub-optimal, and joint optimization of the combined system improves performance. version:1
arxiv-1509-07513 | Description of the Odin Event Extraction Framework and Rule Language | http://arxiv.org/abs/1509.07513 | id:1509.07513 author:Marco A. Valenzuela-Escárcega, Gus Hahn-Powell, Mihai Surdeanu category:cs.CL  published:2015-09-24 summary:This document describes the Odin framework, which is a domain-independent platform for developing rule-based event extraction models. Odin aims to be powerful (the rule language allows the modeling of complex syntactic structures) and robust (to recover from syntactic parsing errors, syntactic patterns can be freely mixed with surface, token-based patterns), while remaining simple (some domain grammars can be up and running in minutes), and fast (Odin processes over 100 sentences/second in a real-world domain with over 200 rules). Here we include a thorough definition of the Odin rule language, together with a description of the Odin API in the Scala language, which allows one to apply these rules to arbitrary texts. version:1
arxiv-1509-07481 | Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks | http://arxiv.org/abs/1509.07481 | id:1509.07481 author:Zhiguang Wang, Tim Oates category:cs.LG  published:2015-09-24 summary:We propose an off-line approach to explicitly encode temporal patterns spatially as different types of images, namely, Gramian Angular Fields and Markov Transition Fields. This enables the use of techniques from computer vision for feature learning and classification. We used Tiled Convolutional Neural Networks to learn high-level features from individual GAF, MTF, and GAF-MTF images on 12 benchmark time series datasets and two real spatial-temporal trajectory datasets. The classification results of our approach are competitive with state-of-the-art approaches on both types of data. An analysis of the features and weights learned by the CNNs explains why the approach works. version:1
arxiv-1509-07473 | Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences | http://arxiv.org/abs/1509.07473 | id:1509.07473 author:Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, Serge Belongie category:cs.CV  published:2015-09-24 summary:With the rapid proliferation of smart mobile devices, users now take millions of photos every day. These include large numbers of clothing and accessory images. We would like to answer questions like `What outfit goes well with this pair of shoes?' To answer these types of questions, one has to go beyond learning visual similarity and learn a visual notion of compatibility across categories. In this paper, we propose a novel learning framework to help answer these types of questions. The main idea of this framework is to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, we use a Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs of items that are either compatible or incompatible. We model compatibility based on co-occurrence in large-scale user behavior data; in particular co-purchase data from Amazon.com. To learn cross-category fit, we introduce a strategic method to sample training data, where pairs of items are heterogeneous dyads, i.e., the two elements of a pair belong to different high-level categories. While this approach is applicable to a wide variety of settings, we focus on the representative problem of learning compatible clothing style. Our results indicate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that go well together. version:1
arxiv-1509-07469 | Channel Vector Subspace Estimation from Low-Dimensional Projections | http://arxiv.org/abs/1509.07469 | id:1509.07469 author:Saeid Haghighatshoar, Giuseppe Caire category:cs.IT math.IT stat.ML  published:2015-09-24 summary:In this paper, we propose efficient algorithms for estimating the signal subspace of mobile users in a wireless communication environment with a multi-antenna base-station with $M$ antennas. We assume that, for reducing the RF front-end complexity and overall A/D conversion rate, the JSDM transmitter/receiver is split into the product of a baseband linear projection (digital) and an RF reconfigurable beamforming network (analog) with only $m \ll M$ RF chains. This implies that only $m$ analog observations can be obtained for subspace estimation, and the standard sample covariance estimator is not available. We develop efficient algorithms that estimate the dominant signal subspace from sampling only $m=O(2 \sqrt{M})$ specific array elements according to a coprime scheme, and for a given $p \leq M$, return a $p$-dimensional beamformer that has a performance comparable with the best $p$-dim beamformer designed by knowing the exact covariance matrix of the received signal. We asses the performance of our proposed estimators both analytically and empirically via numerical simulations, and compare it with that of the other state-of-the-art methods proposed in the literature, which are also reviewed and put in the context of estimating the subspace of the signal. version:1
arxiv-1504-07339 | Convolutional Channel Features | http://arxiv.org/abs/1504.07339 | id:1504.07339 author:Bin Yang, Junjie Yan, Zhen Lei, Stan Z. Li category:cs.CV  published:2015-04-28 summary:Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation. version:3
arxiv-1509-07577 | A Review of Feature Selection Methods Based on Mutual Information | http://arxiv.org/abs/1509.07577 | id:1509.07577 author:Jorge R. Vergara, Pablo A. Estévez category:cs.LG stat.ML  published:2015-09-24 summary:In this work we present a review of the state of the art of information theoretic feature selection methods. The concepts of feature relevance, redundance and complementarity (synergy) are clearly defined, as well as Markov blanket. The problem of optimal feature selection is defined. A unifying theoretical framework is described, which can retrofit successful heuristic criteria, indicating the approximations made by each method. A number of open problems in the field are presented. version:1
arxiv-1509-07422 | Adaptive Sequential Optimization with Applications to Machine Learning | http://arxiv.org/abs/1509.07422 | id:1509.07422 author:Craig Wilson, Venugopal V. Veeravalli category:cs.LG cs.DS  published:2015-09-24 summary:A framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such as stochastic gradient descent (SGD). The optimization problems change slowly in the sense that the minimizers change at either a fixed or bounded rate. A method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples needed from the distributions underlying each problem in order to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. Experiments with synthetic and real data are used to confirm that this approach performs well. version:1
arxiv-1508-02884 | Towards Real-time Customer Experience Prediction for Telecommunication Operators | http://arxiv.org/abs/1508.02884 | id:1508.02884 author:Ernesto Diaz-Aviles, Fabio Pinelli, Karol Lynch, Zubair Nabi, Yiannis Gkoufas, Eric Bouillet, Francesco Calabrese, Eoin Coughlan, Peter Holland, Jason Salzwedel category:cs.CY cs.IR stat.ML I.2.6; K.4.0; H.3.3  published:2015-08-12 summary:Telecommunications operators (telcos) traditional sources of income, voice and SMS, are shrinking due to customers using over-the-top (OTT) applications such as WhatsApp or Viber. In this challenging environment it is critical for telcos to maintain or grow their market share, by providing users with as good an experience as possible on their network. But the task of extracting customer insights from the vast amounts of data collected by telcos is growing in complexity and scale everey day. How can we measure and predict the quality of a user's experience on a telco network in real-time? That is the problem that we address in this paper. We present an approach to capture, in (near) real-time, the mobile customer experience in order to assess which conditions lead the user to place a call to a telco's customer care center. To this end, we follow a supervised learning approach for prediction and train our 'Restricted Random Forest' model using, as a proxy for bad experience, the observed customer transactions in the telco data feed before the user places a call to a customer care center. We evaluate our approach using a rich dataset provided by a major African telecommunication's company and a novel big data architecture for both the training and scoring of predictive models. Our empirical study shows our solution to be effective at predicting user experience by inferring if a customer will place a call based on his current context. These promising results open new possibilities for improved customer service, which will help telcos to reduce churn rates and improve customer experience, both factors that directly impact their revenue growth. version:2
arxiv-1407-4211 | A marginal sampler for $σ$-Stable Poisson-Kingman mixture models | http://arxiv.org/abs/1407.4211 | id:1407.4211 author:María Lomelí, Stefano Favaro, Yee Whye Teh category:stat.CO stat.ML  published:2014-07-16 summary:We investigate the class of $\sigma$-stable Poisson-Kingman random probability measures (RPMs) in the context of Bayesian nonparametric mixture modeling. This is a large class of discrete RPMs which encompasses most of the the popular discrete RPMs used in Bayesian nonparametrics, such as the Dirichlet process, Pitman-Yor process, the normalized inverse Gaussian process and the normalized generalized Gamma process. We show how certain sampling properties and marginal characterizations of $\sigma$-stable Poisson-Kingman RPMs can be usefully exploited for devising a Markov chain Monte Carlo (MCMC) algorithm for making inference in Bayesian nonparametric mixture modeling. Specifically, we introduce a novel and efficient MCMC sampling scheme in an augmented space that has a fixed number of auxiliary variables per iteration. We apply our sampling scheme for a density estimation and clustering tasks with unidimensional and multidimensional datasets, and we compare it against competing sampling schemes. version:3
arxiv-1509-07035 | Designing Behaviour in Bio-inspired Robots Using Associative Topologies of Spiking-Neural-Networks | http://arxiv.org/abs/1509.07035 | id:1509.07035 author:Cristian Jimenez-Romero, David Sousa-Rodrigues, Jeffrey H. Johnson category:cs.RO cs.AI cs.NE  published:2015-09-23 summary:This study explores the design and control of the behaviour of agents and robots using simple circuits of spiking neurons and Spike Timing Dependent Plasticity (STDP) as a mechanism of associative and unsupervised learning. Based on a "reward and punishment" classical conditioning, it is demonstrated that these robots learnt to identify and avoid obstacles as well as to identify and look for rewarding stimuli. Using the simulation and programming environment NetLogo, a software engine for the Integrate and Fire model was developed, which allowed us to monitor in discrete time steps the dynamics of each single neuron, synapse and spike in the proposed neural networks. These spiking neural networks (SNN) served as simple brains for the experimental robots. The Lego Mindstorms robot kit was used for the embodiment of the simulated agents. In this paper the topological building blocks are presented as well as the neural parameters required to reproduce the experiments. This paper summarizes the resulting behaviour as well as the observed dynamics of the neural circuits. The Internet-link to the NetLogo code is included in the annex. version:2
arxiv-1509-02634 | Semantic Image Segmentation via Deep Parsing Network | http://arxiv.org/abs/1509.02634 | id:1509.02634 author:Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2015-09-09 summary:This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy. version:2
arxiv-1411-7766 | Deep Learning Face Attributes in the Wild | http://arxiv.org/abs/1411.7766 | id:1411.7766 author:Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-11-28 summary:Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts. version:3
arxiv-1509-07344 | Opinion mining from twitter data using evolutionary multinomial mixture models | http://arxiv.org/abs/1509.07344 | id:1509.07344 author:Md. Abul Hasnat, Julien Velcin, Stéphane Bonnevay, Julien Jacques category:cs.IR stat.ML  published:2015-09-24 summary:Image of an entity can be defined as a structured and dynamic representation which can be extracted from the opinions of a group of users or population. Automatic extraction of such an image has certain importance in political science and sociology related studies, e.g., when an extended inquiry from large-scale data is required. We study the images of two politically significant entities of France. These images are constructed by analyzing the opinions collected from a well known social media called Twitter. Our goal is to build a system which can be used to automatically extract the image of entities over time. In this paper, we propose a novel evolutionary clustering method based on the parametric link among Multinomial mixture models. First we propose the formulation of a generalized model that establishes parametric links among the Multinomial distributions. Afterward, we follow a model-based clustering approach to explore different parametric sub-models and select the best model. For the experiments, first we use synthetic temporal data. Next, we apply the method to analyze the annotated social media data. Results show that the proposed method is better than the state-of-the-art based on the common evaluation metrics. Additionally, our method can provide interpretation about the temporal evolution of the clusters. version:1
arxiv-1504-07550 | Facial landmark detection using structured output deep neural networks | http://arxiv.org/abs/1504.07550 | id:1504.07550 author:Soufiane Belharbi, Clement Chatelain, Romain Herault, Sebastien Adam category:cs.LG stat.ML  published:2015-04-28 summary:Facial landmark detection is an important step for many perception tasks such as face recognition and facial analysis. Regression-based methods have shown a large success. In particular, deep neural networks (DNN) has demonstrated a strong capability to model the high non-linearity between the face image and the face shape. In this paper, we tackle this task as a structured output problem, where we exploit the strong dependencies that lie between the outputs. Beside learning a regression mapping function from the input to the output, we learn, in an unsupervised way, the inter-dependencies between the outputs. For this, we propose a generic regression framework for structured output problems. Our framework allows a successful incorporation of learning the output structure into DNN using the pre-training trick. We apply our method on a facial landmark detection task, where the output is strongly structured. We evaluate our DNN, named Input/Output Deep Architecture (IODA), on two public challenging datasets: LFPW and HELEN. We show that IODA outperforms traditional deep architectures. version:3
arxiv-1505-01918 | An Asymptotically Optimal Policy for Uniform Bandits of Unknown Support | http://arxiv.org/abs/1505.01918 | id:1505.01918 author:Wesley Cowan, Michael N. Katehakis category:stat.ML cs.LG  published:2015-05-08 summary:Consider the problem of a controller sampling sequentially from a finite number of $N \geq 2$ populations, specified by random variables $X^i_k$, $ i = 1,\ldots , N,$ and $k = 1, 2, \ldots$; where $X^i_k$ denotes the outcome from population $i$ the $k^{th}$ time it is sampled. It is assumed that for each fixed $i$, $\{ X^i_k \}_{k \geq 1}$ is a sequence of i.i.d. uniform random variables over some interval $[a_i, b_i]$, with the support (i.e., $a_i, b_i$) unknown to the controller. The objective is to have a policy $\pi$ for deciding, based on available data, from which of the $N$ populations to sample from at any time $n=1,2,\ldots$ so as to maximize the expected sum of outcomes of $n$ samples or equivalently to minimize the regret due to lack on information of the parameters $\{ a_i \}$ and $\{ b_i \}$. In this paper, we present a simple inflated sample mean (ISM) type policy that is asymptotically optimal in the sense of its regret achieving the asymptotic lower bound of Burnetas and Katehakis (1996). Additionally, finite horizon regret bounds are given. version:3
arxiv-1506-08959 | A Large-Scale Car Dataset for Fine-Grained Categorization and Verification | http://arxiv.org/abs/1506.08959 | id:1506.08959 author:Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV cs.AI  published:2015-06-30 summary:Updated on 24/09/2015: This update provides preliminary experiment results for fine-grained classification on the surveillance data of CompCars. The train/test splits are provided in the updated dataset. See details in Section 6. version:2
arxiv-1509-07234 | Sparsity-based Correction of Exponential Artifacts | http://arxiv.org/abs/1509.07234 | id:1509.07234 author:Yin Ding, Ivan W. Selesnick category:cs.LG  published:2015-09-24 summary:This paper describes an exponential transient excision algorithm (ETEA). In biomedical time series analysis, e.g., in vivo neural recording and electrocorticography (ECoG), some measurement artifacts take the form of piecewise exponential transients. The proposed method is formulated as an unconstrained convex optimization problem, regularized by smoothed l1-norm penalty function, which can be solved by majorization-minimization (MM) method. With a slight modification of the regularizer, ETEA can also suppress more irregular piecewise smooth artifacts, especially, ocular artifacts (OA) in electroencephalog- raphy (EEG) data. Examples of synthetic signal, EEG data, and ECoG data are presented to illustrate the proposed algorithms. version:1
arxiv-1509-07225 | Automatic Concept Discovery from Parallel Text and Visual Corpora | http://arxiv.org/abs/1509.07225 | id:1509.07225 author:Chen Sun, Chuang Gan, Ram Nevatia category:cs.CV  published:2015-09-24 summary:Humans connect language and vision to perceive the world. How to build a similar connection for computers? One possible way is via visual concepts, which are text terms that relate to visually discriminative entities. We propose an automatic visual concept discovery algorithm using parallel text and visual corpora; it filters text terms based on the visual discriminative power of the associated images, and groups them into concepts using visual and semantic similarities. We illustrate the applications of the discovered concepts using bidirectional image and sentence retrieval task and image tagging task, and show that the discovered concepts not only outperform several large sets of manually selected concepts significantly, but also achieves the state-of-the-art performance in the retrieval task. version:1
arxiv-1509-07211 | Noise-Robust ASR for the third 'CHiME' Challenge Exploiting Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent Neural Network | http://arxiv.org/abs/1509.07211 | id:1509.07211 author:Zaihu Pang, Fengyun Zhu category:cs.SD cs.CL  published:2015-09-24 summary:In this paper, the Lingban entry to the third 'CHiME' speech separation and recognition challenge is presented. A time-frequency masking based speech enhancement front-end is proposed to suppress the environmental noise utilizing multi-channel coherence and spatial cues. The state-of-the-art speech recognition techniques, namely recurrent neural network based acoustic and language modeling, state space minimum Bayes risk based discriminative acoustic modeling, and i-vector based acoustic condition modeling, are carefully integrated into the speech recognition back-end. To further improve the system performance by fully exploiting the advantages of different technologies, the final recognition results are obtained by lattice combination and rescoring. Evaluations carried out on the official dataset prove the effectiveness of the proposed systems. Comparing with the best baseline result, the proposed system obtains consistent improvements with over 57% relative word error rate reduction on the real-data test set. version:1
arxiv-1509-07179 | IllinoisSL: A JAVA Library for Structured Prediction | http://arxiv.org/abs/1509.07179 | id:1509.07179 author:Kai-Wei Chang, Shyam Upadhyay, Ming-Wei Chang, Vivek Srikumar, Dan Roth category:cs.LG cs.CL stat.ML  published:2015-09-23 summary:IllinoisSL is a Java library for learning structured prediction models. It supports structured Support Vector Machines and structured Perceptron. The library consists of a core learning module and several applications, which can be executed from command-lines. Documentation is provided to guide users. In Comparison to other structured learning libraries, IllinoisSL is efficient, general, and easy to use. version:1
arxiv-1505-01749 | Object detection via a multi-region & semantic segmentation-aware CNN model | http://arxiv.org/abs/1505.01749 | id:1505.01749 author:Spyros Gidaris, Nikos Komodakis category:cs.CV cs.LG cs.NE  published:2015-05-07 summary:We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin. version:3
arxiv-1411-4423 | A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis | http://arxiv.org/abs/1411.4423 | id:1411.4423 author:Sotirios P. Chatzis category:cs.CV  published:2014-11-17 summary:Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free; thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art. version:5
arxiv-1509-07093 | A review of learning vector quantization classifiers | http://arxiv.org/abs/1509.07093 | id:1509.07093 author:David Nova, Pablo A. Estevez category:cs.LG astro-ph.IM cs.NE stat.ML  published:2015-09-23 summary:In this work we present a review of the state of the art of Learning Vector Quantization (LVQ) classifiers. A taxonomy is proposed which integrates the most relevant LVQ approaches to date. The main concepts associated with modern LVQ approaches are defined. A comparison is made among eleven LVQ classifiers using one real-world and two artificial datasets. version:1
arxiv-1509-07087 | Deep Temporal Sigmoid Belief Networks for Sequence Modeling | http://arxiv.org/abs/1509.07087 | id:1509.07087 author:Zhe Gan, Chunyuan Li, Ricardo Henao, David Carlson, Lawrence Carin category:stat.ML cs.LG  published:2015-09-23 summary:Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences. version:1
arxiv-1307-3598 | Fractionally-Supervised Classification | http://arxiv.org/abs/1307.3598 | id:1307.3598 author:Irene Vrbik, Paul D. McNicholas category:stat.ME stat.AP stat.CO stat.ML  published:2013-07-13 summary:Traditionally, there are three species of classification: unsupervised, supervised, and semi-supervised. Supervised and semi-supervised classification differ by whether or not weight is given to unlabelled observations in the classification procedure. In unsupervised classification, or clustering, all observations are unlabeled and hence full weight is given to unlabelled observations. When some observations are unlabelled, it can be very difficult to \textit{a~priori} choose the optimal level of supervision, and the consequences of a sub-optimal choice can be non-trivial. A flexible fractionally-supervised approach to classification is introduced, where any level of supervision --- ranging from unsupervised to supervised --- can be attained. Our approach uses a weighted likelihood, wherein weights control the relative role that labelled and unlabelled data have in building a classifier. A comparison between our approach and the traditional species is presented using simulated and real data. Gaussian mixture models are used as a vehicle to illustrate our fractionally-supervised classification approach; however, it is broadly applicable and variations on the postulated model can be easily made. version:5
arxiv-1509-07079 | Well Tops Guided Prediction of Reservoir Properties using Modular Neural Network Concept A Case Study from Western Onshore, India | http://arxiv.org/abs/1509.07079 | id:1509.07079 author:Soumi Chaki, Akhilesh K Verma, Aurobinda Routray, William K Mohanty, Mamata Jenamani category:cs.NE cs.CE  published:2015-09-23 summary:This paper proposes a complete framework consisting pre-processing, modeling, and post-processing stages to carry out well tops guided prediction of a reservoir property (sand fraction) from three seismic attributes (seismic impedance, instantaneous amplitude, and instantaneous frequency) using the concept of modular artificial neural network (MANN). The data set used in this study comprising three seismic attributes and well log data from eight wells, is acquired from a western onshore hydrocarbon field of India. Firstly, the acquired data set is integrated and normalized. Then, well log analysis and segmentation of the total depth range into three different units (zones) separated by well tops are carried out. Secondly, three different networks are trained corresponding to three different zones using combined data set of seven wells and then trained networks are validated using the remaining test well. The target property of the test well is predicted using three different tuned networks corresponding to three zones; and then the estimated values obtained from three different networks are concatenated to represent the predicted log along the complete depth range of the testing well. The application of multiple simpler networks instead of a single one improves the prediction accuracy in terms of performance metrics such as correlation coefficient, root mean square error, absolute error mean and program execution time. version:1
arxiv-1509-07075 | 3D Scan Registration using Curvelet Features in Planetary Environments | http://arxiv.org/abs/1509.07075 | id:1509.07075 author:Siddhant Ahuja, Peter Iles, Steven L. Waslander category:cs.CV cs.RO  published:2015-09-23 summary:Topographic mapping in planetary environments relies on accurate 3D scan registration methods. However, most global registration algorithms relying on features such as FPFH and Harris-3D show poor alignment accuracy in these settings due to the poor structure of the Mars-like terrain and variable resolution, occluded, sparse range data that is hard to register without some a-priori knowledge of the environment. In this paper, we propose an alternative approach to 3D scan registration using the curvelet transform that performs multi-resolution geometric analysis to obtain a set of coefficients indexed by scale (coarsest to finest), angle and spatial position. Features are detected in the curvelet domain to take advantage of the directional selectivity of the transform. A descriptor is computed for each feature by calculating the 3D spatial histogram of the image gradients, and nearest neighbor based matching is used to calculate the feature correspondences. Correspondence rejection using Random Sample Consensus identifies inliers, and a locally optimal Singular Value Decomposition-based estimation of the rigid-body transformation aligns the laser scans given the re-projected correspondences in the metric space. Experimental results on a publicly available data-set of planetary analogue indoor facility, as well as simulated and real-world scans from Neptec Design Group's IVIGMS 3D laser rangefinder at the outdoor CSA Mars yard demonstrates improved performance over existing methods in the challenging sparse Mars-like terrain. version:1
arxiv-1509-07065 | A Novel Pre-processing Scheme to Improve the Prediction of Sand Fraction from Seismic Attributes using Neural Networks | http://arxiv.org/abs/1509.07065 | id:1509.07065 author:Soumi Chaki, Aurobinda Routray, William K. Mohanty category:cs.CE cs.LG  published:2015-09-23 summary:This paper presents a novel pre-processing scheme to improve the prediction of sand fraction from multiple seismic attributes such as seismic impedance, amplitude and frequency using machine learning and information filtering. The available well logs along with the 3-D seismic data have been used to benchmark the proposed pre-processing stage using a methodology which primarily consists of three steps: pre-processing, training and post-processing. An Artificial Neural Network (ANN) with conjugate-gradient learning algorithm has been used to model the sand fraction. The available sand fraction data from the high resolution well logs has far more information content than the low resolution seismic attributes. Therefore, regularization schemes based on Fourier Transform (FT), Wavelet Decomposition (WD) and Empirical Mode Decomposition (EMD) have been proposed to shape the high resolution sand fraction data for effective machine learning. The input data sets have been segregated into training, testing and validation sets. The test results are primarily used to check different network structures and activation function performances. Once the network passes the testing phase with an acceptable performance in terms of the selected evaluators, the validation phase follows. In the validation stage, the prediction model is tested against unseen data. The network yielding satisfactory performance in the validation stage is used to predict lithological properties from seismic attributes throughout a given volume. Finally, a post-processing scheme using 3-D spatial filtering is implemented for smoothing the sand fraction in the volume. Prediction of lithological properties using this framework is helpful for Reservoir Characterization. version:1
arxiv-1506-03099 | Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks | http://arxiv.org/abs/1506.03099 | id:1506.03099 author:Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer category:cs.LG cs.CL cs.CV  published:2015-06-09 summary:Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the MSCOCO image captioning challenge, 2015. version:3
arxiv-1509-06957 | Fast k-NN search | http://arxiv.org/abs/1509.06957 | id:1509.06957 author:Ville Hyvönen, Teemu Pitkänen, Sotiris Tasoulis, Liang Wang, Teemu Roos, Jukka Corander category:stat.ML cs.DS cs.LG  published:2015-09-23 summary:Random projection trees have proven to be effective for approximate nearest neighbor searches in high dimensional spaces where conventional methods are not applicable due to excessive usage of memory and computational time. We show that building multiple trees on the same data can improve the performance even further, without significantly increasing the total computational cost of queries when executed in a modern parallel computing environment. Our experiments identify suitable parameter values to achieve accurate searches with extremely fast query times, while also retaining a feasible complexity for index construction. version:1
arxiv-1509-06939 | Enabling Depth-driven Visual Attention on the iCub Humanoid Robot: Instructions for Use and New Perspectives | http://arxiv.org/abs/1509.06939 | id:1509.06939 author:Giulia Pasquale, Tanis Mar, Carlo Ciliberto, Lorenzo Rosasco, Lorenzo Natale category:cs.RO cs.CV  published:2015-09-23 summary:The importance of depth perception in the interactions that humans have within their nearby space is a well established fact. Consequently, it is also well known that the possibility of exploiting good stereo information would ease and, in many cases, enable, a large variety of attentional and interactive behaviors on humanoid robotic platforms. However, the difficulty of computing real-time and robust binocular disparity maps from moving stereo cameras often prevents from relying on this kind of cue to visually guide robots' attention and actions in real-world scenarios. The contribution of this paper is two-fold: first, we show that the Efficient Large-scale Stereo Matching algorithm (ELAS) by A. Geiger et al. 2010 for computation of the disparity map is well suited to be used on a humanoid robotic platform as the iCub robot; second, we show how, provided with a fast and reliable stereo system, implementing relatively challenging visual behaviors in natural settings can require much less effort. As a case of study we consider the common situation where the robot is asked to focus the attention on one object close in the scene, showing how a simple but effective disparity-based segmentation solves the problem in this case. Indeed this example paves the way to a variety of other similar applications. version:1
arxiv-1509-06937 | Fully automatic multi-language translation with a catalogue of phrases - successful employment for the Swiss avalanche bulletin | http://arxiv.org/abs/1509.06937 | id:1509.06937 author:Kurt Winkler, Tobias Kuhn category:cs.CL  published:2015-09-23 summary:The Swiss avalanche bulletin is produced twice a day in four languages. Due to the lack of time available for manual translation, a fully automated translation system is employed, based on a catalogue of predefined phrases and predetermined rules of how these phrases can be combined to produce sentences. Because this catalogue of phrases is limited to a small sublanguage, the system is able to automatically translate such sentences from German into the target languages French, Italian and English without subsequent proofreading or correction. Having been operational for two winter seasons, we assess here the quality of the produced texts based on two different surveys where participants rated texts from real avalanche bulletins from both origins, the catalogue of phrases versus manually written and translated texts. With a mean recognition rate of 55%, users can hardly distinguish between thetwo types of texts, and give very similar ratings with respect to their language quality. Overall, the output from the catalogue system can be considered virtually equivalent to a text written by avalanche forecasters and then manually translated by professional translators. Furthermore, forecasters declared that all relevant situations were captured by the system with sufficient accuracy. Forecaster's working load did not change with the introduction of the catalogue: the extra time to find matching sentences is compensated by the fact that they no longer need to double-check manually translated texts. The reduction of daily translation costs is expected to offset the initial development costs within a few years. version:1
arxiv-1509-06928 | Automatic Dialect Detection in Arabic Broadcast Speech | http://arxiv.org/abs/1509.06928 | id:1509.06928 author:Ahmed Ali, Peter Bell, Steve Renals category:cs.CL  published:2015-09-23 summary:We investigate different approaches for dialect identification in Arabic broadcast speech, using phonetic, lexical features obtained from a speech recognition system, and acoustic features using the i-vector framework. We studied both generative and discriminate classifiers, and we combined these features using a multi-class Support Vector Machine (SVM). We validated our results on an Arabic/English language identification task, with an accuracy of 100%. We used these features in a binary classifier to discriminate between Modern Standard Arabic (MSA) and Dialectal Arabic, with an accuracy of 100%. We further report results using the proposed method to discriminate between the five most widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North African, and MSA, with an accuracy of 52%. We discuss dialect identification errors in the context of dialect code-switching between Dialectal Arabic and MSA, and compare the error pattern between manually labeled data, and the output from our classifier. We also release the train and test data as standard corpus for dialect identification. version:1
arxiv-1509-06925 | Robust Object Tracking with a Hierarchical Ensemble Framework | http://arxiv.org/abs/1509.06925 | id:1509.06925 author:Mengmeng Wang, Yong Liu category:cs.CV  published:2015-09-23 summary:Autonomous robots enjoy a wide popularity nowadays and have been applied in many applications, such as home security, entertainment, delivery, navigation and guidance. It is vital to robots to track objects accurately in these applications, so it is necessary to focus on tracking algorithms to improve the robustness and accuracy. In this paper, we propose a robust object tracking algorithm based on a hierarchical ensemble framework which can incorporate information including individual pixel features, local patches and holistic target models. The framework combines multiple ensemble models simultaneously instead of using a single ensemble model individually. A discriminative model which accounts for the matching degree of local patches is adopted via a bottom ensemble layer, and a generative model which exploits holistic templates is used to search for the object through the middle ensemble layer as well as an adaptive Kalman filter. We test the proposed tracker on challenging benchmark image sequences. Both qualitative and quantitative evaluations demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms, especially when the appearance changes dramatically and the occlusions occur. version:1
arxiv-1509-06920 | Predicting Climate Variability over the Indian Region Using Data Mining Strategies | http://arxiv.org/abs/1509.06920 | id:1509.06920 author:Naresh Kumar Mallenahalli category:stat.ML physics.ao-ph  published:2015-09-23 summary:In this paper an approach based on expectation maximization (EM) clustering to find the climate regions and a support vector machine to build a predictive model for each of these regions is proposed. To minimize the biases in the estimations a ten cross fold validation is adopted both for obtaining clusters and building the predictive models. The EM clustering could identify all the zones as per the Koppen classification over Indian region. The proposed strategy when employed for predicting temperature has resulted in an RMSE of $1.19$ in the Montane climate region and $0.89$ in the Humid Sub Tropical region as compared to $2.9$ and $0.95$ respectively predicted using k-means and linear regression method. version:1
arxiv-1506-03607 | P-CNN: Pose-based CNN Features for Action Recognition | http://arxiv.org/abs/1506.03607 | id:1506.03607 author:Guilhem Chéron, Ivan Laptev, Cordelia Schmid category:cs.CV  published:2015-06-11 summary:This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of a representation derived from human pose. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art. version:2
arxiv-1509-06893 | Efficient reconstruction of transmission probabilities in a spreading process from partial observations | http://arxiv.org/abs/1509.06893 | id:1509.06893 author:Andrey Y. Lokhov, Theodor Misiakiewicz category:physics.soc-ph cond-mat.stat-mech cs.LG cs.SI stat.ML  published:2015-09-23 summary:An important problem of reconstruction of diffusion network and transmission probabilities from the data has attracted a considerable attention in the past several years. A number of recent papers introduced efficient algorithms for the estimation of spreading parameters, based on the maximization of the likelihood of observed cascades, assuming that the full information for all the nodes in the network is available. In this work, we focus on a more realistic and restricted scenario, in which only a partial information on the cascades is available: either the set of activation times for a limited number of nodes, or the states of nodes for a subset of observation times. To tackle this problem, we first introduce a framework based on the maximization of the likelihood of the incomplete diffusion trace. However, we argue that the computation of this incomplete likelihood is a computationally hard problem, and show that a fast and robust reconstruction of transmission probabilities in sparse networks can be achieved with a new algorithm based on recently introduced dynamic message-passing equations for the spreading processes. The suggested approach can be easily generalized to a large class of discrete and continuous dynamic models, as well as to the cases of dynamically-changing networks and noisy information. version:1
arxiv-1509-06853 | New Fuzzy LBP Features for Face Recognition | http://arxiv.org/abs/1509.06853 | id:1509.06853 author:Abdullah Gubbi, Mohammed Fazle Azeem, Zahid Ansari category:cs.CV  published:2015-09-23 summary:There are many Local texture features each very in way they implement and each of the Algorithm trying improve the performance. An attempt is made in this paper to represent a theoretically very simple and computationally effective approach for face recognition. In our implementation the face image is divided into 3x3 sub-regions from which the features are extracted using the Local Binary Pattern (LBP) over a window, fuzzy membership function and at the central pixel. The LBP features possess the texture discriminative property and their computational cost is very low. By utilising the information from LBP, membership function, and central pixel, the limitations of traditional LBP is eliminated. The bench mark database like ORL and Sheffield Databases are used for the evaluation of proposed features with SVM classifier. For the proposed approach K-fold and ROC curves are obtained and results are compared. version:1
arxiv-1509-06849 | Minimum Weight Perfect Matching via Blossom Belief Propagation | http://arxiv.org/abs/1509.06849 | id:1509.06849 author:Sungsoo Ahn, Sejun Park, Michael Chertkov, Jinwoo Shin category:cs.DS cs.AI stat.ML  published:2015-09-23 summary:Max-product Belief Propagation (BP) is a popular message-passing algorithm for computing a Maximum-A-Posteriori (MAP) assignment over a distribution represented by a Graphical Model (GM). It has been shown that BP can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption: the respective Linear Programming (LP) relaxation is tight, i.e., no integrality gap is present. However, when LP shows an integrality gap, no model has been known which can be solved systematically via sequential applications of BP. In this paper, we develop the first such algorithm, coined Blossom-BP, for solving the minimum weight matching problem over arbitrary graphs. Each step of the sequential algorithm requires applying BP over a modified graph constructed by contractions and expansions of blossoms, i.e., odd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs, where n is the number of vertices in the original graph. In essence, the Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom algorithm by jumping at once over many sub-steps with a single BP. Moreover, our result provides an interpretation of the Edmonds' algorithm as a sequence of LPs. version:1
arxiv-1507-01273 | Learning Deep Neural Network Policies with Continuous Memory States | http://arxiv.org/abs/1507.01273 | id:1507.01273 author:Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, Pieter Abbeel category:cs.LG cs.RO  published:2015-07-05 summary:Policy learning for partially observed control tasks requires policies that can remember salient information from past observations. In this paper, we present a method for learning policies with internal memory for high-dimensional, continuous systems, such as robotic manipulators. Our approach consists of augmenting the state and action space of the system with continuous-valued memory states that the policy can read from and write to. Learning general-purpose policies with this type of memory representation directly is difficult, because the policy must automatically figure out the most salient information to memorize at each time step. We show that, by decomposing this policy search problem into a trajectory optimization phase and a supervised learning phase through a method called guided policy search, we can acquire policies with effective memorization and recall strategies. Intuitively, the trajectory optimization phase chooses the values of the memory states that will make it easier for the policy to produce the right action in future states, while the supervised learning phase encourages the policy to use memorization actions to produce those memory states. We evaluate our method on tasks involving continuous control in manipulation and navigation settings, and show that our method can learn complex policies that successfully complete a range of tasks that require memory. version:2
arxiv-1509-06842 | A Feature-Based Comparison of Evolutionary Computing Techniques for Constrained Continuous Optimisation | http://arxiv.org/abs/1509.06842 | id:1509.06842 author:Shayan Poursoltan, Frank Neumann category:cs.AI cs.NE  published:2015-09-23 summary:Evolutionary algorithms have been frequently applied to constrained continuous optimisation problems. We carry out feature based comparisons of different types of evolutionary algorithms such as evolution strategies, differential evolution and particle swarm optimisation for constrained continuous optimisation. In our study, we examine how sets of constraints influence the difficulty of obtaining close to optimal solutions. Using a multi-objective approach, we evolve constrained continuous problems having a set of linear and/or quadratic constraints where the different evolutionary approaches show a significant difference in performance. Afterwards, we discuss the features of the constraints that exhibit a difference in performance of the different evolutionary approaches under consideration. version:1
arxiv-1407-4446 | Probabilistic Group Testing under Sum Observations: A Parallelizable 2-Approximation for Entropy Loss | http://arxiv.org/abs/1407.4446 | id:1407.4446 author:Weidong Han, Purnima Rajan, Peter I. Frazier, Bruno M. Jedynak category:cs.IT cs.LG math.IT math.OC math.ST stat.ML stat.TH  published:2014-07-16 summary:We consider the problem of group testing with sum observations and noiseless answers, in which we aim to locate multiple objects by querying the number of objects in each of a sequence of chosen sets. We study a probabilistic setting with entropy loss, in which we assume a joint Bayesian prior density on the locations of the objects and seek to choose the sets queried to minimize the expected entropy of the Bayesian posterior distribution after a fixed number of questions. We present a new non-adaptive policy, called the dyadic policy, show it is optimal among non-adaptive policies, and is within a factor of two of optimal among adaptive policies. This policy is quick to compute, its nonadaptive nature makes it easy to parallelize, and our bounds show it performs well even when compared with adaptive policies. We also study an adaptive greedy policy, which maximizes the one-step expected reduction in entropy, and show that it performs at least as well as the dyadic policy, offering greater query efficiency but reduced parallelism. Numerical experiments demonstrate that both procedures outperform a divide-and-conquer benchmark policy from the literature, called sequential bifurcation, and show how these procedures may be applied in a stylized computer vision problem. version:3
arxiv-1509-06831 | Density Estimation via Discrepancy | http://arxiv.org/abs/1509.06831 | id:1509.06831 author:Kun Yang, Hao Su, Wing Hung Wang category:stat.ML  published:2015-09-23 summary:Given i.i.d samples from some unknown continuous density on hyper-rectangle $[0, 1]^d$, we attempt to learn a piecewise constant function that approximates this underlying density non-parametrically. Our density estimate is defined on a binary split of $[0, 1]^d$ and built up sequentially according to discrepancy criteria; the key ingredient is to control the discrepancy adaptively in each sub-rectangle to achieve overall bound. We prove that the estimate, even though simple as it appears, preserves most of the estimation power. By exploiting its structure, it can be directly applied to some important pattern recognition tasks such as mode seeking and density landscape exploration. We demonstrate its applicability through simulations and examples. version:1
arxiv-1509-06825 | Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours | http://arxiv.org/abs/1509.06825 | id:1509.06825 author:Lerrel Pinto, Abhinav Gupta category:cs.LG cs.CV cs.RO  published:2015-09-23 summary:Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping. version:1
arxiv-1506-07552 | Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms | http://arxiv.org/abs/1506.07552 | id:1506.07552 author:Yuchen Zhang, Michael I. Jordan category:cs.LG  published:2015-06-24 summary:Stochastic algorithms are efficient approaches to solving machine learning and optimization problems. In this paper, we propose a general framework called Splash for parallelizing stochastic algorithms on multi-node distributed systems. Splash consists of a programming interface and an execution engine. Using the programming interface, the user develops sequential stochastic algorithms without concerning any detail about distributed computing. The algorithm is then automatically parallelized by a communication-efficient execution engine. We provide theoretical justifications on the optimal rate of convergence for parallelizing stochastic gradient descent. Splash is built on top of Apache Spark. The real-data experiments on logistic regression, collaborative filtering and topic modeling verify that Splash yields order-of-magnitude speedup over single-thread stochastic algorithms and over state-of-the-art implementations on Spark. version:2
arxiv-1412-7024 | Training deep neural networks with low precision multiplications | http://arxiv.org/abs/1412.7024 | id:1412.7024 author:Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David category:cs.LG cs.CV cs.NE  published:2014-12-22 summary:Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications. version:5
arxiv-1509-06812 | Learning Wake-Sleep Recurrent Attention Models | http://arxiv.org/abs/1509.06812 | id:1509.06812 author:Jimmy Ba, Roger Grosse, Ruslan Salakhutdinov, Brendan Frey category:cs.LG  published:2015-09-22 summary:Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation. version:1
arxiv-1509-06807 | Bandit Label Inference for Weakly Supervised Learning | http://arxiv.org/abs/1509.06807 | id:1509.06807 author:Ke Li, Jitendra Malik category:cs.LG stat.ML  published:2015-09-22 summary:The scarcity of data annotated at the desired level of granularity is a recurring issue in many applications. Significant amounts of effort have been devoted to developing weakly supervised methods tailored to each individual setting, which are often carefully designed to take advantage of the particular properties of weak supervision regimes, form of available data and prior knowledge of the task at hand. Unfortunately, it is difficult to adapt these methods to new tasks and/or forms of data, which often require different weak supervision regimes or models. We present a general-purpose method that can solve any weakly supervised learning problem irrespective of the weak supervision regime or the model. The proposed method turns any off-the-shelf strongly supervised classifier into a weakly supervised classifier and allows the user to specify any arbitrary weakly supervision regime via a loss function. We apply the method to several different weak supervision regimes and demonstrate competitive results compared to methods specifically engineered for those settings. version:1
arxiv-1509-05982 | Denoising without access to clean data using a partitioned autoencoder | http://arxiv.org/abs/1509.05982 | id:1509.05982 author:Dan Stowell, Richard E. Turner category:cs.NE cs.LG  published:2015-09-20 summary:Training a denoising autoencoder neural network requires access to truly clean data, a requirement which is often impractical. To remedy this, we introduce a method to train an autoencoder using only noisy data, having examples with and without the signal class of interest. The autoencoder learns a partitioned representation of signal and noise, learning to reconstruct each separately. We illustrate the method by denoising birdsong audio (available abundantly in uncontrolled noisy datasets) using a convolutional autoencoder. version:2
arxiv-1509-03600 | Hardness of Online Sleeping Combinatorial Optimization Problems | http://arxiv.org/abs/1509.03600 | id:1509.03600 author:Satyen Kale, Chansoo Lee, Dávid Pál category:cs.LG cs.DS  published:2015-09-11 summary:We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems, using per-action regret as the performance measure, are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of Online Shortest Paths, Online Minimum Spanning Tree, Online $k$-Subsets, Online $k$-Truncated Permutations, Online Minimum Cut, and Online Bipartite Matching. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 (Koolen et al., 2015). We also give an efficient reduction of the task of minimizing per-action regret to the task of minimizing ranking regret, a different performance measure. Thus, existing efficient algorithms for minimizing ranking regret under various restrictions of the adversary can be used to efficiently minimize per-action regret as well. version:2
arxiv-1506-08891 | Detecting Table Region in PDF Documents Using Distant Supervision | http://arxiv.org/abs/1506.08891 | id:1506.08891 author:Miao Fan, Doo Soon Kim category:cs.CV cs.IR  published:2015-06-29 summary:Superior to state-of-the-art approaches which compete in table recognition with 67 annotated government reports in PDF format released by {\it ICDAR 2013 Table Competition}, this paper contributes a novel paradigm leveraging large-scale unlabeled PDF documents to open-domain table detection. We integrate the paradigm into our latest developed system ({\it PdfExtra}) to detect the region of tables by means of 9,466 academic articles from the entire repository of {\it ACL Anthology}, where almost all papers are archived by PDF format without annotation for tables. The paradigm first designs heuristics to automatically construct weakly labeled data. It then feeds diverse evidences, such as layouts of documents and linguistic features, which are extracted by {\it Apache PDFBox} and processed by {\it Stanford NLP} toolkit, into different canonical classifiers. We finally use these classifiers, i.e. {\it Naive Bayes}, {\it Logistic Regression} and {\it Support Vector Machine}, to collaboratively vote on the region of tables. Experimental results show that {\it PdfExtra} achieves a great leap forward, compared with the state-of-the-art approach. Moreover, we discuss the factors of different features, learning models and even domains of documents that may impact the performance. Extensive evaluations demonstrate that our paradigm is compatible enough to leverage various features and learning models for open-domain table region detection within PDF files. version:6
arxiv-1509-06690 | Invariants of objects and their images under surjective maps | http://arxiv.org/abs/1509.06690 | id:1509.06690 author:Irina A. Kogan, Peter J. Olver category:math.DG cs.CV I.2.10  published:2015-09-22 summary:We examine the relationships between the differential invariants of objects and of their images under a surjective map. We analyze both the case when the underlying transformation group is projectable and hence induces an action on the image, and the case when only a proper subgroup of the entire group acts projectably. In the former case, we establish a constructible isomorphism between the algebra of differential invariants of the images and the algebra of fiber-wise constant (gauge) differential invariants of the objects. In the latter case, we describe residual effects of the full transformation group on the image invariants. Our motivation comes from the problem of reconstruction of an object from multiple-view images, with central and parallel projections of curves from three-dimensional space to the two-dimensional plane serving as our main examples. version:1
arxiv-1509-06673 | Classification error in multiclass discrimination from Markov data | http://arxiv.org/abs/1509.06673 | id:1509.06673 author:Sören Christensen, Albrecht Irle, Lars Willert category:stat.ML math.ST stat.TH  published:2015-09-22 summary:As a model for an on-line classification setting we consider a stochastic process $(X_{-n},Y_{-n})_{n}$, the present time-point being denoted by 0, with observables $ \ldots,X_{-n},X_{-n+1},\ldots, X_{-1}, X_0$ from which the pattern $Y_0$ is to be inferred. So in this classification setting, in addition to the present observation $X_0$ a number $l$ of preceding observations may be used for classification, thus taking a possible dependence structure into account as it occurs e.g. in an ongoing classification of handwritten characters. We treat the question how the performance of classifiers is improved by using such additional information. For our analysis, a hidden Markov model is used. Letting $R_l$ denote the minimal risk of misclassification using $l$ preceding observations we show that the difference $\sup_k R_l - R_{l+k} $ decreases exponentially fast as $l$ increases. This suggests that a small $l$ might already lead to a noticeable improvement. To follow this point we look at the use of past observations for kernel classification rules. Our practical findings in simulated hidden Markov models and in the classification of handwritten characters indicate that using $l=1$, i.e. just the last preceding observation in addition to $X_0$, can lead to a substantial reduction of the risk of misclassification. So, in the presence of stochastic dependencies, we advocate to use $ X_{-1},X_0$ for finding the pattern $Y_0$ instead of only $X_0$ as one would in the independent situation. version:1
arxiv-1506-00054 | Learning quantitative sequence-function relationships from massively parallel experiments | http://arxiv.org/abs/1506.00054 | id:1506.00054 author:Gurinder S. Atwal, Justin B. Kinney category:q-bio.QM math.ST physics.bio-ph physics.data-an stat.ML stat.TH  published:2015-05-30 summary:A fundamental aspect of biological information processing is the ubiquity of sequence-function relationships -- functions that map the sequence of DNA, RNA, or protein to a biochemically relevant activity. Most sequence-function relationships in biology are quantitative, but only recently have experimental techniques for effectively measuring these relationships been developed. The advent of such "massively parallel" experiments presents an exciting opportunity for the concepts and methods of statistical physics to inform the study of biological systems. After reviewing these recent experimental advances, we focus on the problem of how to infer parametric models of sequence-function relationships from the data produced by these experiments. Specifically, we retrace and extend recent theoretical work showing that inference based on mutual information, not the standard likelihood-based approach, is often necessary for accurately learning the parameters of these models. Closely connected with this result is the emergence of "diffeomorphic modes" -- directions in parameter space that are far less constrained by data than likelihood-based inference would suggest. Analogous to Goldstone modes in physics, diffeomorphic modes arise from an arbitrarily broken symmetry of the inference problem. An analytically tractable model of a massively parallel experiment is then described, providing an explicit demonstration of these fundamental aspects of statistical inference. This paper concludes with an outlook on the theoretical and computational challenges currently facing studies of quantitative sequence-function relationships. version:2
arxiv-1505-06606 | Robust Optimization for Deep Regression | http://arxiv.org/abs/1505.06606 | id:1505.06606 author:Vasileios Belagiannis, Christian Rupprecht, Gustavo Carneiro, Nassir Navab category:cs.CV  published:2015-05-25 summary:Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets. version:2
arxiv-1509-06594 | A Compositional Explanation of the Pet Fish Phenomenon | http://arxiv.org/abs/1509.06594 | id:1509.06594 author:Bob Coecke, Martha Lewis category:cs.AI cs.CL math.CT  published:2015-09-22 summary:The `pet fish' phenomenon is often cited as a paradigm example of the `non-compositionality' of human concept use. We show here how this phenomenon is naturally accommodated within a compositional distributional model of meaning. This model describes the meaning of a composite concept by accounting for interaction between its constituents via their grammatical roles. We give two illustrative examples to show how the qualitative phenomena are exhibited. We go on to apply the model to experimental data, and finally discuss extensions of the formalism. version:1
arxiv-1509-06589 | Graph Kernels exploiting Weisfeiler-Lehman Graph Isomorphism Test Extensions | http://arxiv.org/abs/1509.06589 | id:1509.06589 author:Giovanni Da San Martino, Nicolò Navarin, Alessandro Sperduti category:cs.LG cs.AI  published:2015-09-22 summary:In this paper we present a novel graph kernel framework inspired the by the Weisfeiler-Lehman (WL) isomorphism tests. Any WL test comprises a relabelling phase of the nodes based on test-specific information extracted from the graph, for example the set of neighbours of a node. We defined a novel relabelling and derived two kernels of the framework from it. The novel kernels are very fast to compute and achieve state-of-the-art results on five real-world datasets. version:1
arxiv-1509-06585 | A Review of Features for the Discrimination of Twitter Users: Application to the Prediction of Offline Influence | http://arxiv.org/abs/1509.06585 | id:1509.06585 author:Jean-Valère Cossu, Vincent Labatut, Nicolas Dugué category:cs.CL cs.SI  published:2015-09-22 summary:Many works related to Twitter aim at characterizing its users in some way: role on the service (spammers, bots, organizations, etc.), nature of the user (socio-professional category, age, etc.), topics of interest, and others. However, for a given user classification problem, it is very difficult to select a set of appropriate features, because the many features described in the literature are very heterogeneous, with name overlaps and collisions, and numerous very close variants. In this article, we review a wide range of such features. In order to present a clear state-of-the-art description, we unify their names, definitions and relationships, and we propose a new, neutral, typology. We then illustrate the interest of our review by applying a selection of these features to the offline influence detection problem. This task consists in identifying users which are influential in real-life, based on their Twitter account and related data. We show that most features deemed efficient to predict online influence, such as the numbers of retweets and followers, are not relevant to this problem. However, We propose several content-based approaches to label Twitter users as Influencers or not. We also rank them according to a predicted influence level. Our proposals are evaluated over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art methods. version:1
arxiv-1509-06576 | Homotopy relations for digital images | http://arxiv.org/abs/1509.06576 | id:1509.06576 author:Laurence Boxer, P. Christopher Staecker category:math.GN cs.CV 55P10  55Q05 I.4.m  published:2015-09-22 summary:We introduce three generalizations of homotopy equivalence in digital images, to allow us to express whether a finite and an infinite digital image are similar with respect to homotopy. We show that these three generalizations are not equivalent to ordinary homotopy equivalence, and give several examples. We show that, like homotopy equivalence, our three generalizations imply isomorphism of fundamental groups, and are version:1
arxiv-1509-06557 | Local Multi-Grouped Binary Descriptor with Ring-based Pooling Configuration and Optimization | http://arxiv.org/abs/1509.06557 | id:1509.06557 author:Yongqiang Gao, Weilin Huang, Yu Qiao category:cs.CV  published:2015-09-22 summary:Local binary descriptors are attracting increasingly attention due to their great advantages in computational speed, which are able to achieve real-time performance in numerous image/vision applications. Various methods have been proposed to learn data-dependent binary descriptors. However, most existing binary descriptors aim overly at computational simplicity at the expense of significant information loss which causes ambiguity in similarity measure using Hamming distance. In this paper, by considering multiple features might share complementary information, we present a novel local binary descriptor, referred as Ring-based Multi-Grouped Descriptor (RMGD), to successfully bridge the performance gap between current binary and floated-point descriptors. Our contributions are two-fold. Firstly, we introduce a new pooling configuration based on spatial ring-region sampling, allowing for involving binary tests on the full set of pairwise regions with different shapes, scales and distances. This leads to a more meaningful description than existing methods which normally apply a limited set of pooling configurations. Then, an extended Adaboost is proposed for efficient bit selection by emphasizing high variance and low correlation, achieving a highly compact representation. Secondly, the RMGD is computed from multiple image properties where binary strings are extracted. We cast multi-grouped features integration as rankSVM or sparse SVM learning problem, so that different features can compensate strongly for each other, which is the key to discriminativeness and robustness. The performance of RMGD was evaluated on a number of publicly available benchmarks, where the RMGD outperforms the state-of-the-art binary descriptors significantly. version:1
arxiv-1509-06535 | Deep Boltzmann Machines in Estimation of Distribution Algorithms for Combinatorial Optimization | http://arxiv.org/abs/1509.06535 | id:1509.06535 author:Malte Probst, Franz Rothlauf category:cs.NE  published:2015-09-22 summary:Estimation of Distribution Algorithms (EDAs) require flexible probability models that can be efficiently learned and sampled. Deep Boltzmann Machines (DBMs) are generative neural networks with these desired properties. We integrate a DBM into an EDA and evaluate the performance of this system in solving combinatorial optimization problems with a single objective. We compare the results to the Bayesian Optimization Algorithm. The performance of DBM-EDA was superior to BOA for difficult additively decomposable functions, i.e., concatenated deceptive traps of higher order. For most other benchmark problems, DBM-EDA cannot clearly outperform BOA, or other neural network-based EDAs. In particular, it often yields optimal solutions for a subset of the runs (with fewer evaluations than BOA), but is unable to provide reliable convergence to the global optimum competitively. At the same time, the model building process is computationally more expensive than that of other EDAs using probabilistic models from the neural network family, such as DAE-EDA. version:1
arxiv-1601-03483 | A survey on feature weighting based K-Means algorithms | http://arxiv.org/abs/1601.03483 | id:1601.03483 author:Renato Cordeiro de Amorim category:cs.LG  published:2015-09-22 summary:In a real-world data set there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process. With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research. This paper elaborates on the concept of feature weighting and addresses these issues by critically analysing some of the most popular, or innovative, feature weighting mechanisms based in K-Means. version:1
arxiv-1509-06492 | Modifying iterated Laplace approximations | http://arxiv.org/abs/1509.06492 | id:1509.06492 author:Tiep Mai, Simon Wilson category:stat.ME stat.ML  published:2015-09-22 summary:In this paper, several modifications are introduced to the functional approximation method iterLap to reduce the approximation error, including stopping rule adjustment, proposal of new residual function, starting point selection for numerical optimisation, scaling of Hessian matrix. Illustrative examples are also provided to show the trade-off between running time and accuracy of the original and modified methods. version:1
arxiv-1401-3632 | Bayesian Conditional Density Filtering | http://arxiv.org/abs/1401.3632 | id:1401.3632 author:Shaan Qamar, Rajarshi Guhaniyogi, David B. Dunson category:stat.ML cs.LG stat.CO  published:2014-01-15 summary:We propose a Conditional Density Filtering (C-DF) algorithm for efficient online Bayesian inference. C-DF adapts MCMC sampling to the online setting, sampling from approximations to conditional posterior distributions obtained by propagating surrogate conditional sufficient statistics (a function of data and parameter estimates) as new data arrive. These quantities eliminate the need to store or process the entire dataset simultaneously and offer a number of desirable features. Often, these include a reduction in memory requirements and runtime and improved mixing, along with state-of-the-art parameter inference and prediction. These improvements are demonstrated through several illustrative examples including an application to high dimensional compressed regression. Finally, we show that C-DF samples converge to the target posterior distribution asymptotically as sampling proceeds and more data arrives. version:3
arxiv-1509-06470 | Understand Scene Categories by Objects: A Semantic Regularized Scene Classifier Using Convolutional Neural Networks | http://arxiv.org/abs/1509.06470 | id:1509.06470 author:Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu category:cs.CV  published:2015-09-22 summary:Scene classification is a fundamental perception task for environmental understanding in today's robotics. In this paper, we have attempted to exploit the use of popular machine learning technique of deep learning to enhance scene understanding, particularly in robotics applications. As scene images have larger diversity than the iconic object images, it is more challenging for deep learning methods to automatically learn features from scene images with less samples. Inspired by human scene understanding based on object knowledge, we address the problem of scene classification by encouraging deep neural networks to incorporate object-level information. This is implemented with a regularization of semantic segmentation. With only 5 thousand training images, as opposed to 2.5 million images, we show the proposed deep architecture achieves superior scene classification results to the state-of-the-art on a publicly available SUN RGB-D dataset. In addition, performance of semantic segmentation, the regularizer, also reaches a new record with refinement derived from predicted scene labels. Finally, we apply our SUN RGB-D dataset trained model to a mobile robot captured images to classify scenes in our university demonstrating the generalization ability of the proposed algorithm. version:1
arxiv-1509-06459 | Stochastic gradient descent methods for estimation with large data sets | http://arxiv.org/abs/1509.06459 | id:1509.06459 author:Dustin Tran, Panos Toulis, Edoardo M. Airoldi category:stat.CO stat.ME stat.ML  published:2015-09-22 summary:We develop methods for parameter estimation in settings with large-scale data sets, where traditional methods are no longer tenable. Our methods rely on stochastic approximations, which are computationally efficient as they maintain one iterate as a parameter estimate, and successively update that iterate based on a single data point. When the update is based on a noisy gradient, the stochastic approximation is known as standard stochastic gradient descent, which has been fundamental in modern applications with large data sets. Additionally, our methods are numerically stable because they employ implicit updates of the iterates. Intuitively, an implicit update is a shrinked version of a standard one, where the shrinkage factor depends on the observed Fisher information at the corresponding data point. This shrinkage prevents numerical divergence of the iterates, which can be caused either by excess noise or outliers. Our sgd package in R offers the most extensive and robust implementation of stochastic gradient descent methods. We demonstrate that sgd dominates alternative software in runtime for several estimation problems with massive data sets. Our applications include the wide class of generalized linear models as well as M-estimation for robust regression. version:1
arxiv-1509-06458 | Harmonic Extension | http://arxiv.org/abs/1509.06458 | id:1509.06458 author:Zuoqiang Shi, Jian Sun, Minghao Tian category:cs.LG math.NA  published:2015-09-22 summary:In this paper, we consider the harmonic extension problem, which is widely used in many applications of machine learning. We find that the transitional method of graph Laplacian fails to produce a good approximation of the classical harmonic function. To tackle this problem, we propose a new method called the point integral method (PIM). We consider the harmonic extension problem from the point of view of solving PDEs on manifolds. The basic idea of the PIM method is to approximate the harmonicity using an integral equation, which is easy to be discretized from points. Based on the integral equation, we explain the reason why the transitional graph Laplacian may fail to approximate the harmonicity in the classical sense and propose a different approach which we call the volume constraint method (VCM). Theoretically, both the PIM and the VCM computes a harmonic function with convergence guarantees, and practically, they are both simple, which amount to solve a linear system. One important application of the harmonic extension in machine learning is semi-supervised learning. We run a popular semi-supervised learning algorithm by Zhu et al. over a couple of well-known datasets and compare the performance of the aforementioned approaches. Our experiments show the PIM performs the best. version:1
arxiv-1509-06457 | Identifying collusion groups using spectral clustering | http://arxiv.org/abs/1509.06457 | id:1509.06457 author:Suneel Sarswat, Kandathil Mathew Abraham, Subir Kumar Ghosh category:q-fin.TR cs.CE stat.ML  published:2015-09-22 summary:In an illiquid stock, traders can collude and place orders on a predetermined price and quantity at a fixed schedule. This is usually done to manipulate the price of the stock or to create artificial liquidity in the stock, which may mislead genuine investors. Here, the problem is to identify such group of colluding traders. We modeled the problem instance as a graph, where each trader corresponds to a vertex of the graph and trade corresponds to edges of the graph. Further, we assign weights on edges depending on total volume, total number of trades, maximum change in the price and commonality between two vertices. Spectral clustering algorithms are used on the constructed graph to identify colluding group(s). We have compared our results with simulated data to show the effectiveness of spectral clustering to detecting colluding groups. Moreover, we also have used parameters of real data to test the effectiveness of our algorithm. version:1
arxiv-1509-06451 | From Facial Parts Responses to Face Detection: A Deep Learning Approach | http://arxiv.org/abs/1509.06451 | id:1509.06451 author:Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2015-09-22 summary:In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed. version:1
arxiv-1509-06449 | Efficient Neighborhood Selection for Gaussian Graphical Models | http://arxiv.org/abs/1509.06449 | id:1509.06449 author:Yingxiang Yang, Jalal Etesami, Negar Kiyavash category:stat.ML cs.IT cs.LG math.IT  published:2015-09-22 summary:This paper addresses the problem of neighborhood selection for Gaussian graphical models. We present two heuristic algorithms: a forward-backward greedy algorithm for general Gaussian graphical models based on mutual information test, and a threshold-based algorithm for walk summable Gaussian graphical models. Both algorithms are shown to be structurally consistent, and efficient. Numerical results show that both algorithms work very well. version:1
arxiv-1506-08259 | Twitter User Geolocation Using a Unified Text and Network Prediction Model | http://arxiv.org/abs/1506.08259 | id:1506.08259 author:Afshin Rahimi, Trevor Cohn, Timothy Baldwin category:cs.CL cs.SI  published:2015-06-27 summary:We propose a label propagation approach to geolocation prediction based on Modified Adsorption, with two enhancements:(1) the removal of "celebrity" nodes to increase location homophily and boost tractability, and (2) he incorporation of text-based geolocation priors for test users. Experiments over three Twitter benchmark datasets achieve state-of-the-art results, and demonstrate the effectiveness of the enhancements. version:3
arxiv-1508-07647 | Love Thy Neighbors: Image Annotation by Exploiting Image Metadata | http://arxiv.org/abs/1508.07647 | id:1508.07647 author:Justin Johnson, Lamberto Ballan, Fei-Fei Li category:cs.CV  published:2015-08-30 summary:Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata. We build on this intuition to improve multilabel image annotation. Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically, in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata. version:2
arxiv-1504-02141 | Detecting falls with X-Factor HMMs when the training data for falls is not available | http://arxiv.org/abs/1504.02141 | id:1504.02141 author:Shehroz S. Khan, Michelle E. Karg, Dana Kulic, Jesse Hoey category:cs.LG cs.AI  published:2015-04-08 summary:Identification of falls while performing normal activities of daily living (ADL) is important to ensure personal safety and well-being. However, falling is a short term activity that occurs infrequently. This poses a challenge to traditional classification algorithms, because there may be very little training data for falls (or none at all). This paper proposes an approach for the identification of falls using a wearable device in the absence of training data for falls but with plentiful data for normal ADL. We propose three `X-Factor' Hidden Markov Model (XHMMs) approaches. The XHMMs model unseen falls using "inflated" output covariances (observation models). To estimate the inflated covariances, we propose a novel cross validation method to remove "outliers" from the normal ADL that serve as proxies for the unseen falls and allow learning the XHMMs using only normal activities. We tested the proposed XHMM approaches on two activity recognition datasets and show high detection rates for falls in the absence of fall-specific training data. We show that the traditional method of choosing a threshold based on maximum of negative of log-likelihood to identify unseen falls is ill-posed for this problem. We also show that supervised classification methods perform poorly when very limited fall data are available during the training phase. version:3
arxiv-1506-03379 | The Online Coupon-Collector Problem and Its Application to Lifelong Reinforcement Learning | http://arxiv.org/abs/1506.03379 | id:1506.03379 author:Emma Brunskill, Lihong Li category:cs.LG cs.AI  published:2015-06-10 summary:Transferring knowledge across a sequence of related tasks is an important challenge in reinforcement learning (RL). Despite much encouraging empirical evidence, there has been little theoretical analysis. In this paper, we study a class of lifelong RL problems: the agent solves a sequence of tasks modeled as finite Markov decision processes (MDPs), each of which is from a finite set of MDPs with the same state/action sets and different transition/reward functions. Motivated by the need for cross-task exploration in lifelong learning, we formulate a novel online coupon-collector problem and give an optimal algorithm. This allows us to develop a new lifelong RL algorithm, whose overall sample complexity in a sequence of tasks is much smaller than single-task learning, even if the sequence of tasks is generated by an adversary. Benefits of the algorithm are demonstrated in simulated problems, including a recently introduced human-robot interaction problem. version:2
arxiv-1509-06420 | A Multi-Agent System Approach to Load-Balancing and Resource Allocation for Distributed Computing | http://arxiv.org/abs/1509.06420 | id:1509.06420 author:Soumya Banerjee, Joshua Hecker category:cs.NE cs.DC  published:2015-09-21 summary:In this research we use a decentralized computing approach to allocate and schedule tasks on a massively distributed grid. Using emergent properties of multi-agent systems, the algorithm dynamically creates and dissociates clusters to serve the changing resource demands of a global task queue. The algorithm is compared to a standard First-in First-out (FIFO) scheduling algorithm. Experiments done on a simulator show that the distributed resource allocation protocol (dRAP) algorithm outperforms the FIFO scheduling algorithm on time to empty queue, average waiting time and CPU utilization. Such a decentralized computing approach holds promise for massively distributed processing scenarios like SETI@home and Google MapReduce. version:1
arxiv-1506-07643 | Conservativeness of untied auto-encoders | http://arxiv.org/abs/1506.07643 | id:1506.07643 author:Daniel Jiwoong Im, Mohamed Ishmael Diwan Belghazi, Roland Memisevic category:cs.LG  published:2015-06-25 summary:We discuss necessary and sufficient conditions for an auto-encoder to define a conservative vector field, in which case it is associated with an energy function akin to the unnormalized log-probability of the data. We show that the conditions for conservativeness are more general than for encoder and decoder weights to be the same ("tied weights"), and that they also depend on the form of the hidden unit activation function, but that contractive training criteria, such as denoising, will enforce these conditions locally. Based on these observations, we show how we can use auto-encoders to extract the conservative component of a vector field. version:3
arxiv-1509-06839 | Estimating Random Delays in Modbus Network Using Experiments and General Linear Regression Neural Networks with Genetic Algorithm Smoothing | http://arxiv.org/abs/1509.06839 | id:1509.06839 author:B. Sreram, F. Bounapane, B. Subathra, Seshadhri Srinivasan category:cs.SY cs.NE  published:2015-09-21 summary:Time-varying delays adversely affect the performance of networked control sys-tems (NCS) and in the worst-case can destabilize the entire system. Therefore, modelling network delays is important for designing NCS. However, modelling time-varying delays is challenging because of their dependence on multiple pa-rameters such as length, contention, connected devices, protocol employed, and channel loading. Further, these multiple parameters are inherently random and de-lays vary in a non-linear fashion with respect to time. This makes estimating ran-dom delays challenging. This investigation presents a methodology to model de-lays in NCS using experiments and general regression neural network (GRNN) due to their ability to capture non-linear relationship. To compute the optimal smoothing parameter that computes the best estimates, genetic algorithm is used. The objective of the genetic algorithm is to compute the optimal smoothing pa-rameter that minimizes the mean absolute percentage error (MAPE). Our results illustrate that the resulting GRNN is able to predict the delays with less than 3% error. The proposed delay model gives a framework to design compensation schemes for NCS subjected to time-varying delays. version:1
arxiv-1509-05463 | Learning from Synthetic Data Using a Stacked Multichannel Autoencoder | http://arxiv.org/abs/1509.05463 | id:1509.05463 author:Xi Zhang, Yanwei Fu, Shanshan Jiang, Leonid Sigal, Gady Agam category:cs.CV cs.AI  published:2015-09-17 summary:Learning from synthetic data has many important and practical applications. An example of application is photo-sketch recognition. Using synthetic data is challenging due to the differences in feature distributions between synthetic and real data, a phenomenon we term synthetic gap. In this paper, we investigate and formalize a general framework-Stacked Multichannel Autoencoder (SMCAE) that enables bridging the synthetic gap and learning from synthetic data more efficiently. In particular, we show that our SMCAE can not only transform and use synthetic data on the challenging face-sketch recognition task, but that it can also help simulate real images, which can be used for training classifiers for recognition. Preliminary experiments validate the effectiveness of the framework. version:2
arxiv-1504-06201 | High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and its Applications to High-Level Vision | http://arxiv.org/abs/1504.06201 | id:1504.06201 author:Gedas Bertasius, Jianbo Shi, Lorenzo Torresani category:cs.CV  published:2015-04-23 summary:Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a "High-for-Low" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run. Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of high-level vision tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a "Low-for-High" scheme, where low-level boundaries aid high-level vision tasks. Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art high-level vision methods on three distinct tasks. version:3
arxiv-1509-06321 | Evaluating the visualization of what a Deep Neural Network has learned | http://arxiv.org/abs/1509.06321 | id:1509.06321 author:Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller category:cs.CV  published:2015-09-21 summary:Deep Neural Networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multi-layer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the ''importance'' of individual pixels wrt the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main result is that the recently proposed Layer-wise Relevance Propagation (LRP) algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of neural network performance. version:1
arxiv-1509-06290 | A Bayesian Compressed Sensing Kalman Filter for Direction of Arrival Estimation | http://arxiv.org/abs/1509.06290 | id:1509.06290 author:Matthew Hawes, Lyudmila Mihaylova, Francois Septier, Simon Godsill category:stat.ML cs.IT math.IT  published:2015-09-21 summary:In this paper, we look to address the problem of estimating the dynamic direction of arrival (DOA) of a narrowband signal impinging on a sensor array from the far field. The initial estimate is made using a Bayesian compressive sensing (BCS) framework and then tracked using a Bayesian compressed sensing Kalman filter (BCSKF). The BCS framework splits the angular region into N potential DOAs and enforces a belief that only a few of the DOAs will have a non-zero valued signal present. A BCSKF can then be used to track the change in the DOA using the same framework. There can be an issue when the DOA approaches the endfire of the array. In this angular region current methods can struggle to accurately estimate and track changes in the DOAs. To tackle this problem, we propose changing the traditional sparse belief associated with BCS to a belief that the estimated signals will match the predicted signals given a known DOA change. This is done by modelling the difference between the expected sparse received signals and the estimated sparse received signals as a Gaussian distribution. Example test scenarios are provided and comparisons made with the traditional BCS based estimation method. They show that an improvement in estimation accuracy is possible without a significant increase in computational complexity. version:1
arxiv-1409-7618 | Multiple Object Tracking: A Literature Review | http://arxiv.org/abs/1409.7618 | id:1409.7618 author:Wenhan Luo, Junliang Xing, Xiaoqin Zhang, Xiaowei Zhao, Tae-Kyun Kim category:cs.CV I.4.8  published:2014-09-26 summary:Multiple Object Tracking is an important computer vision task which has gained increasing attention due to its academic and commercial potential. Although different approaches have been proposed to tackle it, there still exist many issues unsolved. In order to help readers understand this topic, we contribute a systematic and comprehensive review. In the review, we inspect recent advances in various aspects and propose some interesting directions for future research. To our best knowledge, there has not been any review about this topic in the community. We endeavor to provide a thorough review on the development of this problem in the last decades. The main contributions are fourfold: 1) Key aspects in a multiple object tracking system, including how to formulate MOT generally, how to categorize MOT algorithms, what needs to be considered when developing a MOT system and how to evaluate a MOT system, are discussed from the perspective of understanding a topic. We believe this could not only provide researchers, especially new comers to the topic of MOT, a general understanding of the state of the arts, but also help them to comprehend the essential components of a MOT system and the inter-component connection. 2) Instead of enumerating individual works, we discuss existing work according to the various aspects involved in a MOT system. In each aspect, methods are divided into different groups and each group is discussed in details for the principles, advances and drawbacks. 3) We examine experiments of existing publications and give tables which list results on the popular data sets to provide convenient comparison. We also provide some interesting discoveries by analyzing these tables. 4) We offer some potential directions and respective discussions about MOT, which are still open issues and need more research efforts. This would be helpful to identify interesting problems. version:3
arxiv-1508-00021 | Artificial Neural Networks Applied to Taxi Destination Prediction | http://arxiv.org/abs/1508.00021 | id:1508.00021 author:Alexandre de Brébisson, Étienne Simon, Alex Auvolat, Pascal Vincent, Yoshua Bengio category:cs.LG cs.NE  published:2015-07-31 summary:We describe our first-place solution to the ECML/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence. version:2
arxiv-1509-06243 | LEWIS: Latent Embeddings for Word Images and their Semantics | http://arxiv.org/abs/1509.06243 | id:1509.06243 author:Albert Gordo, Jon Almazan, Naila Murray, Florent Perronnin category:cs.CV  published:2015-09-21 summary:The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images. Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the semantics into consideration. In this paper, we ask the following question: \emph{can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point?} For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant. This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded. This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and concepts mined from a lexical database (WordNet). Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy version:1
arxiv-1503-01954 | Denoising Autoencoders for fast Combinatorial Black Box Optimization | http://arxiv.org/abs/1503.01954 | id:1503.01954 author:Malte Probst category:cs.NE I.2.6; I.2.8  published:2015-03-06 summary:Estimation of Distribution Algorithms (EDAs) require flexible probability models that can be efficiently learned and sampled. Autoencoders (AE) are generative stochastic networks with these desired properties. We integrate a special type of AE, the Denoising Autoencoder (DAE), into an EDA and evaluate the performance of DAE-EDA on several combinatorial optimization problems with a single objective. We asses the number of fitness evaluations as well as the required CPU times. We compare the results to the performance to the Bayesian Optimization Algorithm (BOA) and RBM-EDA, another EDA which is based on a generative neural network which has proven competitive with BOA. For the considered problem instances, DAE-EDA is considerably faster than BOA and RBM-EDA, sometimes by orders of magnitude. The number of fitness evaluations is higher than for BOA, but competitive with RBM-EDA. These results show that DAEs can be useful tools for problems with low but non-negligible fitness evaluation costs. version:2
arxiv-1501-00438 | (Non-) asymptotic properties of Stochastic Gradient Langevin Dynamics | http://arxiv.org/abs/1501.00438 | id:1501.00438 author:Sebastian J. Vollmer, Konstantinos C. Zygalakis, and Yee Whye Teh category:stat.ME math.ST stat.ML stat.TH 60J05  65C05  published:2015-01-02 summary:Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data sets is computationally infeasible. The recently proposed stochastic gradient Langevin dynamics (SGLD) method circumvents this problem in three ways: it generates proposed moves using only a subset of the data, it skips the Metropolis-Hastings accept-reject step, and it uses sequences of decreasing step sizes. In \cite{TehThierryVollmerSGLD2014}, we provided the mathematical foundations for the decreasing step size SGLD, including consistency and a central limit theorem. However, in practice the SGLD is run for a relatively small number of iterations, and its step size is not decreased to zero. The present article investigates the behaviour of the SGLD with fixed step size. In particular we characterise the asymptotic bias explicitly, along with its dependence on the step size and the variance of the stochastic gradient. On that basis a modified SGLD which removes the asymptotic bias due to the variance of the stochastic gradients up to first order in the step size is derived. Moreover, we are able to obtain bounds on the finite-time bias, variance and mean squared error (MSE). The theory is illustrated with a Gaussian toy model for which the bias and the MSE for the estimation of moments can be obtained explicitly. For this toy model we study the gain of the SGLD over the standard Euler method in the limit of large data sets. version:2
arxiv-1503-09022 | Multi-label Classification using Labels as Hidden Nodes | http://arxiv.org/abs/1503.09022 | id:1503.09022 author:Jesse Read, Jaakko Hollmén category:stat.ML cs.LG  published:2015-03-31 summary:Competitive methods for multi-label classification typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods have an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability. version:2
arxiv-1509-06163 | The Utility of Clustering in Prediction Tasks | http://arxiv.org/abs/1509.06163 | id:1509.06163 author:Shubhendu Trivedi, Zachary A. Pardos, Neil T. Heffernan category:cs.LG  published:2015-09-21 summary:We explore the utility of clustering in reducing error in various prediction tasks. Previous work has hinted at the improvement in prediction accuracy attributed to clustering algorithms if used to pre-process the data. In this work we more deeply investigate the direct utility of using clustering to improve prediction accuracy and provide explanations for why this may be so. We look at a number of datasets, run k-means at different scales and for each scale we train predictors. This produces k sets of predictions. These predictions are then combined by a na\"ive ensemble. We observed that this use of a predictor in conjunction with clustering improved the prediction accuracy in most datasets. We believe this indicates the predictive utility of exploiting structure in the data and the data compression handed over by clustering. We also found that using this method improves upon the prediction of even a Random Forests predictor which suggests this method is providing a novel, and useful source of variance in the prediction process. version:1
arxiv-1509-06161 | Cascaded Regressor based 3D Face Reconstruction from a Single Arbitrary View Image | http://arxiv.org/abs/1509.06161 | id:1509.06161 author:Feng Liu, Dan Zeng, Jing Li, Qijun Zhao category:cs.CV  published:2015-09-21 summary:State-of-the-art methods reconstruct three-dimensional (3D) face shapes from a single image by fitting 3D face models to input images or by directly learning mapping functions between two-dimensional (2D) images and 3D faces. However, they are often difficult to use in real-world applications due to expensive online optimization or to the requirement of frontal face images. This paper approaches the 3D face reconstruction problem as a regression problem rather than a model fitting problem. Given an input face image along with some pre-defined facial landmarks on it, a series of shape adjustments to the initial 3D face shape are computed through cascaded regressors based on the deviations between the input landmarks and the landmarks obtained from the reconstructed 3D faces. The cascaded regressors are offline learned from a set of 3D faces and their corresponding 2D face images in various views. By treating the landmarks that are invisible in large view angles as missing data, the proposed method can handle arbitrary view face images in a unified way with the same regressors. Experiments on the BFM and Bosphorus databases demonstrate that the proposed method can reconstruct 3D faces from arbitrary view images more efficiently and more accurately than existing methods. version:1
arxiv-1509-06103 | Noise Robust IOA/CAS Speech Separation and Recognition System For The Third 'CHIME' Challenge | http://arxiv.org/abs/1509.06103 | id:1509.06103 author:Xiaofei Wang, Chao Wu, Pengyuan Zhang, Ziteng Wang, Yong Liu, Xu Li, Qiang Fu, Yonghong Yan category:cs.SD cs.CL  published:2015-09-21 summary:This paper presents the contribution to the third 'CHiME' speech separation and recognition challenge including both front-end signal processing and back-end speech recognition. In the front-end, Multi-channel Wiener filter (MWF) is designed to achieve background noise reduction. Different from traditional MWF, optimized parameter for the tradeoff between noise reduction and target signal distortion is built according to the desired noise reduction level. In the back-end, several techniques are taken advantage to improve the noisy Automatic Speech Recognition (ASR) performance including Deep Neural Network (DNN), Convolutional Neural Network (CNN) and Long short-term memory (LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary language model finite state transducer, and ROVER scheme. Experimental results show the proposed system combining front-end and back-end is effective to improve the ASR performance. version:1
arxiv-1509-06095 | Multilayer bootstrap network for unsupervised speaker recognition | http://arxiv.org/abs/1509.06095 | id:1509.06095 author:Xiao-Lei Zhang category:cs.LG cs.SD  published:2015-09-21 summary:We apply multilayer bootstrap network (MBN), a recent proposed unsupervised learning method, to unsupervised speaker recognition. The proposed method first extracts supervectors from an unsupervised universal background model, then reduces the dimension of the high-dimensional supervectors by multilayer bootstrap network, and finally conducts unsupervised speaker recognition by clustering the low-dimensional data. The comparison results with 2 unsupervised and 1 supervised speaker recognition techniques demonstrate the effectiveness and robustness of the proposed method. version:1
arxiv-1509-06088 | Significance Analysis of High-Dimensional, Low-Sample Size Partially Labeled Data | http://arxiv.org/abs/1509.06088 | id:1509.06088 author:Qiyi Lu, Xingye Qiao category:stat.ML cs.LG stat.ME  published:2015-09-21 summary:Classification and clustering are both important topics in statistical learning. A natural question herein is whether predefined classes are really different from one another, or whether clusters are really there. Specifically, we may be interested in knowing whether the two classes defined by some class labels (when they are provided), or the two clusters tagged by a clustering algorithm (where class labels are not provided), are from the same underlying distribution. Although both are challenging questions for the high-dimensional, low-sample size data, there has been some recent development for both. However, when it is costly to manually place labels on observations, it is often that only a small portion of the class labels is available. In this article, we propose a significance analysis approach for such type of data, namely partially labeled data. Our method makes use of the whole data and tries to test the class difference as if all the labels were observed. Compared to a testing method that ignores the label information, our method provides a greater power, meanwhile, maintaining the size, illustrated by a comprehensive simulation study. Theoretical properties of the proposed method are studied with emphasis on the high-dimensional, low-sample size setting. Our simulated examples help to understand when and how the information extracted from the labeled data can be effective. A real data example further illustrates the usefulness of the proposed method. version:1
arxiv-1509-06066 | On Large-Scale Retrieval: Binary or n-ary Coding? | http://arxiv.org/abs/1509.06066 | id:1509.06066 author:Mahyar Najibi, Mohammad Rastegari, Larry S. Davis category:cs.CV  published:2015-09-20 summary:The growing amount of data available in modern-day datasets makes the need to efficiently search and retrieve information. To make large-scale search feasible, Distance Estimation and Subset Indexing are the main approaches. Although binary coding has been popular for implementing both techniques, n-ary coding (known as Product Quantization) is also very effective for Distance Estimation. However, their relative performance has not been studied for Subset Indexing. We investigate whether binary or n-ary coding works better under different retrieval strategies. This leads to the design of a new n-ary coding method, "Linear Subspace Quantization (LSQ)" which, unlike other n-ary encoders, can be used as a similarity-preserving embedding. Experiments on image retrieval show that when Distance Estimation is used, n-ary LSQ outperforms other methods. However, when Subset Indexing is applied, interestingly, binary codings are more effective and binary LSQ achieves the best accuracy. version:1
arxiv-1509-06061 | A Statistical Theory of Deep Learning via Proximal Splitting | http://arxiv.org/abs/1509.06061 | id:1509.06061 author:Nicholas G. Polson, Brandon T. Willard, Massoud Heidari category:stat.ML  published:2015-09-20 summary:In this paper we develop a statistical theory and an implementation of deep learning models. We show that an elegant variable splitting scheme for the alternating direction method of multipliers optimises a deep learning objective. We allow for non-smooth non-convex regularisation penalties to induce sparsity in parameter weights. We provide a link between traditional shallow layer statistical models such as principal component and sliced inverse regression and deep layer models. We also define the degrees of freedom of a deep learning predictor and a predictive MSE criteria to perform model selection for comparing architecture designs. We focus on deep multiclass logistic learning although our methods apply more generally. Our results suggest an interesting and previously under-exploited relationship between deep learning and proximal splitting techniques. To illustrate our methodology, we provide a multi-class logit classification analysis of Fisher's Iris data where we illustrate the convergence of our algorithm. Finally, we conclude with directions for future research. version:1
arxiv-1509-06057 | Impact of noise on a dynamical system: prediction and uncertainties from a swarm-optimized neural network | http://arxiv.org/abs/1509.06057 | id:1509.06057 author:C. H. López-Caraballo, J. A. Lazzús, I. Salfate, P. Rojas, M. Rivera, L. Palma-Chilla category:physics.comp-ph cs.NE  published:2015-09-20 summary:In this study, an artificial neural network (ANN) based on particle swarm optimization (PSO) was developed for the time series prediction. The hybrid ANN+PSO algorithm was applied on Mackey--Glass chaotic time series in the short-term $x(t+6)$. The performance prediction was evaluated and compared with another studies available in the literature. Also, we presented properties of the dynamical system via the study of chaotic behaviour obtained from the predicted time series. Next, the hybrid ANN+PSO algorithm was complemented with a Gaussian stochastic procedure (called {\it stochastic} hybrid ANN+PSO) in order to obtain a new estimator of the predictions, which also allowed us to compute uncertainties of predictions for noisy Mackey--Glass chaotic time series. Thus, we studied the impact of noise for several cases with a white noise level ($\sigma_{N}$) from 0.01 to 0.1. version:1
arxiv-1509-06053 | Early text classification: a Naive solution | http://arxiv.org/abs/1509.06053 | id:1509.06053 author:Hugo Jair Escalante, Manuel Montes-y-Gómez, Luis Villaseñor-Pineda, Marcelo Luis Errecalde category:cs.CL  published:2015-09-20 summary:Text classification is a widely studied problem, and it can be considered solved for some domains and under certain circumstances. There are scenarios, however, that have received little or no attention at all, despite its relevance and applicability. One of such scenarios is early text classification, where one needs to know the category of a document by using partial information only. A document is processed as a sequence of terms, and the goal is to devise a method that can make predictions as fast as possible. The importance of this variant of the text classification problem is evident in domains like sexual predator detection, where one wants to identify an offender as early as possible. This paper analyzes the suitability of the standard naive Bayes classifier for approaching this problem. Specifically, we assess its performance when classifying documents after seeing an increasingly number of terms. A simple modification to the standard naive Bayes implementation allows us to make predictions with partial information. To the best of our knowledge naive Bayes has not been used for this purpose before. Throughout an extensive experimental evaluation we show the effectiveness of the classifier for early text classification. What is more, we show that this simple solution is very competitive when compared with state of the art methodologies that are more elaborated. We foresee our work will pave the way for the development of more effective early text classification techniques based in the naive Bayes formulation. version:1
arxiv-1509-06041 | Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep Networks | http://arxiv.org/abs/1509.06041 | id:1509.06041 author:Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang category:cs.CV cs.IR cs.LG  published:2015-09-20 summary:Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using images and videos to express their opinions and share their experiences. Sentiment analysis of such large scale visual content can help better extract user sentiments toward events or topics, such as those in image tweets, so that prediction of sentiment from visual content is complementary to textual sentiment analysis. Motivated by the needs in leveraging large scale yet noisy training data to solve the extremely challenging problem of image sentiment analysis, we employ Convolutional Neural Networks (CNN). We first design a suitable CNN architecture for image sentiment analysis. We obtain half a million training samples by using a baseline sentiment algorithm to label Flickr images. To make use of such noisy machine labeled data, we employ a progressive strategy to fine-tune the deep network. Furthermore, we improve the performance on Twitter images by inducing domain transfer with a small number of manually labeled Twitter images. We have conducted extensive experiments on manually labeled Twitter images. The results show that the proposed CNN can achieve better performance in image sentiment analysis than competing algorithms. version:1
arxiv-1509-06035 | Image Retrieval Based on LBP Pyramidal Multiresolution using Reversible Watermarking | http://arxiv.org/abs/1509.06035 | id:1509.06035 author:H. Ouahi, K. Afdel, M. Machkour category:cs.CV  published:2015-09-20 summary:In the medical field, images are increasingly used to facilitate diagnosis of diseases. These images are stored in multimedia databases accompanied by doctor s prescriptions and other information related to patients.Search for medical images has become for clinical applications an essential tool to bring effective aid in diagnosis. Content Based Image Retrieval (CBIR) is one of the possible solutions to effectively manage these databases. Our contribution is to define a relevant descriptor to retrieve images based on multiresolution analysis of texture using Local Binary Pattern LBP. This descriptor once calculated and information s relating to the patient; will be placed in the image using the technique of reversible watermarking. Thereby, the image, descriptor of its contents, the BFILE locator and patientrelated information become a single entity, so even the administrator cannot have access to the patient private data. version:1
arxiv-1509-06033 | Deep Convolutional Features for Image Based Retrieval and Scene Categorization | http://arxiv.org/abs/1509.06033 | id:1509.06033 author:Arsalan Mousavian, Jana Kosecka category:cs.CV  published:2015-09-20 summary:Several recent approaches showed how the representations learned by Convolutional Neural Networks can be repurposed for novel tasks. Most commonly it has been shown that the activation features of the last fully connected layers (fc7 or fc6) of the network, followed by a linear classifier outperform the state-of-the-art on several recognition challenge datasets. Instead of recognition, this paper focuses on the image retrieval problem and proposes a examines alternative pooling strategies derived for CNN features. The presented scheme uses the features maps from an earlier layer 5 of the CNN architecture, which has been shown to preserve coarse spatial information and is semantically meaningful. We examine several pooling strategies and demonstrate superior performance on the image retrieval task (INRIA Holidays) at the fraction of the computational cost, while using a relatively small memory requirements. In addition to retrieval, we see similar efficiency gains on the SUN397 scene categorization dataset, demonstrating wide applicability of this simple strategy. We also introduce and evaluate a novel GeoPlaces5K dataset from different geographical locations in the world for image retrieval that stresses more dramatic changes in appearance and viewpoint. version:1
arxiv-1509-06016 | Image Set Querying Based Localization | http://arxiv.org/abs/1509.06016 | id:1509.06016 author:Lei Deng, Siyuan Huang, Yueqi Duan, Baohua Chen, Jie Zhou category:cs.CV  published:2015-09-20 summary:Conventional single image based localization methods usually fail to localize a querying image when there exist large variations between the querying image and the pre-built scene. To address this, we propose an image-set querying based localization approach. When the localization by a single image fails to work, the system will ask the user to capture more auxiliary images. First, a local 3D model is established for the querying image set. Then, the pose of the querying image set is estimated by solving a nonlinear optimization problem, which aims to match the local 3D model against the pre-built scene. Experiments have shown the effectiveness and feasibility of the proposed approach. version:1
arxiv-1508-04025 | Effective Approaches to Attention-based Neural Machine Translation | http://arxiv.org/abs/1508.04025 | id:1508.04025 author:Minh-Thang Luong, Hieu Pham, Christopher D. Manning category:cs.CL  published:2015-08-17 summary:An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. version:5
arxiv-1509-05962 | Telugu OCR Framework using Deep Learning | http://arxiv.org/abs/1509.05962 | id:1509.05962 author:Rakesh Achanta, Trevor Hastie category:stat.ML cs.AI cs.CV cs.LG cs.NE  published:2015-09-20 summary:In this paper, we address the task of Optical Character Recognition(OCR) for the Telugu script. We present an end-to-end framework that segments the text image, classifies the characters and extracts lines using a language model. The segmentation is based on mathematical morphology. The classification module, which is the most challenging task of the three, is a deep convolutional neural network. The language is modelled as a third degree markov chain at the glyph level. Telugu script is a complex abugida and the language is agglutinative, making the problem hard. In this paper we apply the latest advances in neural networks to achieve acceptable error rates. version:1
arxiv-1409-4614 | Lexical Normalisation of Twitter Data | http://arxiv.org/abs/1409.4614 | id:1409.4614 author:Bilal Ahmed category:cs.CL  published:2014-09-16 summary:Twitter with over 500 million users globally, generates over 100,000 tweets per minute . The 140 character limit per tweet, perhaps unintentionally, encourages users to use shorthand notations and to strip spellings to their bare minimum "syllables" or elisions e.g. "srsly". The analysis of twitter messages which typically contain misspellings, elisions, and grammatical errors, poses a challenge to established Natural Language Processing (NLP) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in English language. In order to make sense of Twitter messages it is necessary to first transform them into a canonical form, consistent with the dictionary or grammar. This process, performed at the level of individual tokens ("words"), is called lexical normalisation. This paper investigates various techniques for lexical normalisation of Twitter data and presents the findings as the techniques are applied to process raw data from Twitter. version:4
arxiv-1412-6601 | Using Neural Networks for Click Prediction of Sponsored Search | http://arxiv.org/abs/1412.6601 | id:1412.6601 author:Afroze Ibrahim Baqapuri, Ilya Trofimov category:cs.LG cs.NE  published:2014-12-20 summary:Sponsored search is a multi-billion dollar industry and makes up a major source of revenue for search engines (SE). click-through-rate (CTR) estimation plays a crucial role for ads selection, and greatly affects the SE revenue, advertiser traffic and user experience. We propose a novel architecture for solving CTR prediction problem by combining artificial neural networks (ANN) with decision trees. First we compare ANN with respect to other popular machine learning models being used for this task. Then we go on to combine ANN with MatrixNet (proprietary implementation of boosted trees) and evaluate the performance of the system as a whole. The results show that our approach provides significant improvement over existing models. version:3
arxiv-1509-05897 | Face Photo Sketch Synthesis via Larger Patch and Multiresolution Spline | http://arxiv.org/abs/1509.05897 | id:1509.05897 author:Xu Yang category:cs.CV  published:2015-09-19 summary:Face photo sketch synthesis has got some researchers' attention in recent years because of its potential applications in digital entertainment and law enforcement. Some patches based methods have been proposed to solve this problem. These methods usually focus more on how to get a sketch patch for a given photo patch than how to blend these generated patches. However, without appropriately blending method, some jagged parts and mottled points will appear in the entire face sketch. In order to get a smoother sketch, we propose a new method to reduce such jagged parts and mottled points. In our system, we resort to an existed method, which is Markov Random Fields (MRF), to train a crude face sketch firstly. Then this crude sketch face sketch will be divided into some larger patches again and retrained by Non-Negative Matrix Factorization (NMF). At last, we use Multiresolution Spline and a blend trick named full-coverage trick to blend these retrained patches. The experiment results show that compared with some previous method, we can get a smoother face sketch. version:1
arxiv-1601-03481 | A Fuzzy MLP Approach for Non-linear Pattern Classification | http://arxiv.org/abs/1601.03481 | id:1601.03481 author:Tirtharaj Dash, H. S. Behera category:cs.NE  published:2015-09-19 summary:In case of decision making problems, classification of pattern is a complex and crucial task. Pattern classification using multilayer perceptron (MLP) trained with back propagation learning becomes much complex with increase in number of layers, number of nodes and number of epochs and ultimate increases computational time [31]. In this paper, an attempt has been made to use fuzzy MLP and its learning algorithm for pattern classification. The time and space complexities of the algorithm have been analyzed. A training performance comparison has been carried out between MLP and the proposed fuzzy-MLP model by considering six cases. Results are noted against different learning rates ranging from 0 to 1. A new performance evaluation factor 'convergence gain' has been introduced. It is observed that the number of epochs drastically reduced and performance increased compared to MLP. The average and minimum gain has been found to be 93% and 75% respectively. The best gain is found to be 95% and is obtained by setting the learning rate to 0.55. version:1
arxiv-1506-04214 | Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting | http://arxiv.org/abs/1506.04214 | id:1506.04214 author:Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun Woo category:cs.CV  published:2015-06-13 summary:The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting. version:2
arxiv-1504-02789 | Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models | http://arxiv.org/abs/1504.02789 | id:1504.02789 author:Ashesh Jain, Hema S. Koppula, Bharad Raghavan, Shane Soh, Ashutosh Saxena category:cs.CV  published:2015-04-10 summary:Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the driving context from both inside and outside of the car. We propose an Autoregressive Input-Output HMM to model the contextual information alongwith the maneuvers. We evaluate our approach on a diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate maneuvers 3.5 seconds before they occur with over 80\% F1-score in real-time. version:2
arxiv-1509-05844 | Similar Handwritten Chinese Character Discrimination by Weakly Supervised Learning | http://arxiv.org/abs/1509.05844 | id:1509.05844 author:Zhibo Yang, Huanle Xu, Keda Fu, Yong Xia category:cs.CV  published:2015-09-19 summary:Traditional approaches for handwritten Chinese character recognition suffer in classifying similar characters. In this paper, we propose to discriminate similar handwritten Chinese characters by using weakly supervised learning. Our approach learns a discriminative SVM for each similar pair which simultaneously localizes the discriminative region of similar character and makes the classification. For the first time, similar handwritten Chinese character recognition (SHCCR) is formulated as an optimization problem extended from SVM. We also propose a novel feature descriptor, Gradient Context, and apply bag-of-words model to represent regions with different scales. In our method, we do not need to select a sized-fixed sub-window to differentiate similar characters. The unconstrained property makes our method well adapted to high variance in the size and position of discriminative regions in similar handwritten Chinese characters. We evaluate our proposed approach over the CASIA Chinese character data set and the results show that our method outperforms the state of the art. version:1
arxiv-1509-04874 | DenseBox: Unifying Landmark Localization with End to End Object Detection | http://arxiv.org/abs/1509.04874 | id:1509.04874 author:Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu category:cs.CV  published:2015-09-16 summary:How can a single fully convolutional neural network (FCN) perform on object detection? We introduce DenseBox, a unified end-to-end FCN framework that directly predicts bounding boxes and object class confidences through all locations and scales of an image. Our contribution is two-fold. First, we show that a single FCN, if designed and optimized carefully, can detect multiple different objects extremely accurately and efficiently. Second, we show that when incorporating with landmark localization during multi-task learning, DenseBox further improves object detection accuray. We present experimental results on public benchmark datasets including MALF face detection and KITTI car detection, that indicate our DenseBox is the state-of-the-art system for detecting challenging objects such as faces and cars. version:3
arxiv-1504-03071 | Robobarista: Object Part based Transfer of Manipulation Trajectories from Crowd-sourcing in 3D Pointclouds | http://arxiv.org/abs/1504.03071 | id:1504.03071 author:Jaeyong Sung, Seok Hyun Jin, Ashutosh Saxena category:cs.RO cs.AI cs.LG  published:2015-04-13 summary:There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and design a deep learning model that can handle large noise in the manipulation demonstrations and learns features from three different modalities: point-clouds, language and trajectory. In order to collect a large number of manipulation demonstrations for different objects, we developed a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot can even manipulate objects it has never seen before. version:2
arxiv-1509-05808 | Word, graph and manifold embedding from Markov processes | http://arxiv.org/abs/1509.05808 | id:1509.05808 author:Tatsunori B. Hashimoto, David Alvarez-Melis, Tommi S. Jaakkola category:cs.CL cs.LG stat.ML  published:2015-09-18 summary:Continuous vector representations of words and objects appear to carry surprisingly rich semantic content. In this paper, we advance both the conceptual and theoretical understanding of word embeddings in three ways. First, we ground embeddings in semantic spaces studied in cognitive-psychometric literature and introduce new evaluation tasks. Second, in contrast to prior work, we take metric recovery as the key object of study, unify existing algorithms as consistent metric recovery methods based on co-occurrence counts from simple Markov random walks, and propose a new recovery algorithm. Third, we generalize metric recovery to graphs and manifolds, relating co-occurence counts on random walks in graphs and random processes on manifolds to the underlying metric to be recovered, thereby reconciling manifold estimation and embedding algorithms. We compare embedding algorithms across a range of tasks, from nonlinear dimensionality reduction to three semantic language tasks, including analogies, sequence completion, and classification. version:1
arxiv-1509-05765 | "Oddball SGD": Novelty Driven Stochastic Gradient Descent for Training Deep Neural Networks | http://arxiv.org/abs/1509.05765 | id:1509.05765 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-09-18 summary:Stochastic Gradient Descent (SGD) is arguably the most popular of the machine learning methods applied to training deep neural networks (DNN) today. It has recently been demonstrated that SGD can be statistically biased so that certain elements of the training set are learned more rapidly than others. In this article, we place SGD into a feedback loop whereby the probability of selection is proportional to error magnitude. This provides a novelty-driven oddball SGD process that learns more rapidly than traditional SGD by prioritising those elements of the training set with the largest novelty (error). In our DNN example, oddball SGD trains some 50x faster than regular SGD. version:1
arxiv-1509-05753 | Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses | http://arxiv.org/abs/1509.05753 | id:1509.05753 author:Carlo Baldassi, Alessandro Ingrosso, Carlo Lucibello, Luca Saglietti, Riccardo Zecchina category:cond-mat.dis-nn q-bio.NC stat.ML  published:2015-09-18 summary:We show that discrete synaptic weights can be efficiently used for learning in large scale neural systems, and lead to unanticipated computational performance. We focus on the representative case of learning random patterns with binary synapses in single layer networks. The standard statistical analysis shows that this problem is exponentially dominated by isolated solutions that are extremely hard to find algorithmically. Here, we introduce a novel method that allows us to find analytical evidence for the existence of subdominant and extremely dense regions of solutions. Numerical experiments confirm these findings. We also show that the dense regions are surprisingly accessible by simple learning protocols, and that these synaptic configurations are robust to perturbations and generalize better than typical solutions. These outcomes extend to synapses with multiple states and to deeper neural architectures. The large deviation measure also suggests how to design novel algorithmic schemes for optimization based on local entropy maximization. version:1
arxiv-1509-05742 | Evaluation of Protein-protein Interaction Predictors with Noisy Partially Labeled Data Sets | http://arxiv.org/abs/1509.05742 | id:1509.05742 author:Haohan Wang, Madhavi K. Ganapathiraju category:cs.AI stat.ML  published:2015-09-18 summary:Protein-protein interaction (PPI) prediction is an important problem in machine learning and computational biology. However, there is no data set for training or evaluation purposes, where all the instances are accurately labeled. Instead, what is available are instances of positive class (with possibly noisy labels) and no instances of negative class. The non-availability of negative class data is typically handled with the observation that randomly chosen protein-pairs have a nearly 100% chance of being negative class, as only 1 in 1,500 protein pairs expected is expected to be an interacting pair. In this paper, we focused on the problem that non-availability of accurately labeled testing data sets in the domain of protein-protein interaction (PPI) prediction may lead to biased evaluation results. We first showed that not acknowledging the inherent skew in the interactome (i.e. rare occurrence of positive instances) leads to an over-estimated accuracy of the predictor. Then we show that, with the belief that positive interactions are a rare category, sampling random pairs of proteins excluding known interacting proteins set as the negative testing data set could lead to an under-estimated evaluation result. We formalized those two problems to validate the above claim, and based on the formalization, we proposed a balancing method to cancel out the over-estimation with under-estimation. Finally, our experiments validated the theoretical aspects and showed that this balancing evaluation could evaluate the exact performance without availability of golden standard data sets. version:1
arxiv-1509-05736 | Building a Pilot Software Quality-in-Use Benchmark Dataset | http://arxiv.org/abs/1509.05736 | id:1509.05736 author:Issa Atoum, Chih How Bong, Narayanan Kulathuramaiyer category:cs.SE cs.CL  published:2015-09-18 summary:Prepared domain specific datasets plays an important role to supervised learning approaches. In this article a new sentence dataset for software quality-in-use is proposed. Three experts were chosen to annotate the data using a proposed annotation scheme. Then the data were reconciled in a (no match eliminate) process to reduce bias. The Kappa, k statistics revealed an acceptable level of agreement; moderate to substantial agreement between the experts. The built data can be used to evaluate software quality-in-use models in sentiment analysis models. Moreover, the annotation scheme can be used to extend the current dataset. version:1
arxiv-1509-04752 | Bayesian inference for spatio-temporal spike and slab priors | http://arxiv.org/abs/1509.04752 | id:1509.04752 author:Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen category:stat.ML stat.CO stat.ME  published:2015-09-15 summary:In this work we address the problem of solving a series of underdetermined linear inverse problems subject to a sparsity constraint. We generalize the spike and slab prior distribution to encode a priori correlation of the support of the solution in both space and time by imposing a transformed Gaussian process on the spike and slab probabilities. An expectation propagation (EP) algorithm for posterior inference under the proposed model is derived. For large scale problems, the standard EP algorithm can be prohibitively slow. We therefore introduce three different approximation schemes to reduce the computational complexity. Finally, we demonstrate the proposed model using numerical experiments based on both synthetic and real data sets. version:2
arxiv-1507-00410 | Convolutional Color Constancy | http://arxiv.org/abs/1507.00410 | id:1507.00410 author:Jonathan T. Barron category:cs.CV  published:2015-07-02 summary:Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%. version:2
arxiv-1509-05722 | Energy saving in smart homes based on consumer behaviour: A case study | http://arxiv.org/abs/1509.05722 | id:1509.05722 author:Michael Zehnder, Holger Wache, Hans-Friedrich Witschel, Danilo Zanatta, Miguel Rodriguez category:stat.ML cs.AI cs.MA cs.SY  published:2015-09-18 summary:This paper presents a case study of a recommender system that can be used to save energy in smart homes without lowering the comfort of the inhabitants. We present an algorithm that uses consumer behavior data only and uses machine learning to suggest actions for inhabitants to reduce the energy consumption of their homes. The system mines for frequent and periodic patterns in the event data provided by the Digitalstrom home automation system. These patterns are converted into association rules, prioritized and compared with the current behavior of the inhabitants. If the system detects an opportunities to save energy without decreasing the comfort level it sends a recommendation to the residents. version:1
arxiv-1509-05646 | Computational evolution of decision-making strategies | http://arxiv.org/abs/1509.05646 | id:1509.05646 author:Peter Kvam, Joseph Cesario, Jory Schossau, Heather Eisthen, Arend Hintze category:cs.NE q-bio.NC  published:2015-09-18 summary:Most research on adaptive decision-making takes a strategy-first approach, proposing a method of solving a problem and then examining whether it can be implemented in the brain and in what environments it succeeds. We present a method for studying strategy development based on computational evolution that takes the opposite approach, allowing strategies to develop in response to the decision-making environment via Darwinian evolution. We apply this approach to a dynamic decision-making problem where artificial agents make decisions about the source of incoming information. In doing so, we show that the complexity of the brains and strategies of evolved agents are a function of the environment in which they develop. More difficult environments lead to larger brains and more information use, resulting in strategies resembling a sequential sampling approach. Less difficult environments drive evolution toward smaller brains and less information use, resulting in simpler heuristic-like strategies. version:1
arxiv-1507-08467 | A Model for Foraging Ants, Controlled by Spiking Neural Networks and Double Pheromones | http://arxiv.org/abs/1507.08467 | id:1507.08467 author:Cristian Jimenez-Romero, David Sousa-Rodrigues, Jeffrey H. Johnson, Vitorino Ramos category:cs.NE cs.AI  published:2015-07-30 summary:A model of an Ant System where ants are controlled by a spiking neural circuit and a second order pheromone mechanism in a foraging task is presented. A neural circuit is trained for individual ants and subsequently the ants are exposed to a virtual environment where a swarm of ants performed a resource foraging task. The model comprises an associative and unsupervised learning strategy for the neural circuit of the ant. The neural circuit adapts to the environment by means of classical conditioning. The initially unknown environment includes different types of stimuli representing food and obstacles which, when they come in direct contact with the ant, elicit a reflex response in the motor neural system of the ant: moving towards or away from the source of the stimulus. The ants are released on a landscape with multiple food sources where one ant alone would have difficulty harvesting the landscape to maximum efficiency. The introduction of a double pheromone mechanism yields better results than traditional ant colony optimization strategies. Traditional ant systems include mainly a positive reinforcement pheromone. This approach uses a second pheromone that acts as a marker for forbidden paths (negative feedback). This blockade is not permanent and is controlled by the evaporation rate of the pheromones. The combined action of both pheromones acts as a collective stigmergic memory of the swarm, which reduces the search space of the problem. This paper explores how the adaptation and learning abilities observed in biologically inspired cognitive architectures is synergistically enhanced by swarm optimization strategies. The model portraits two forms of artificial intelligent behaviour: at the individual level the spiking neural network is the main controller and at the collective level the pheromone distribution is a map towards the solution emerged by the colony. version:3
arxiv-1509-05634 | Linearized Kernel Dictionary Learning | http://arxiv.org/abs/1509.05634 | id:1509.05634 author:Alona Golts, Michael Elad category:cs.CV  published:2015-09-18 summary:In this paper we present a new approach of incorporating kernels into dictionary learning. The kernel K-SVD algorithm (KKSVD), which has been introduced recently, shows an improvement in classification performance, with relation to its linear counterpart K-SVD. However, this algorithm requires the storage and handling of a very large kernel matrix, which leads to high computational cost, while also limiting its use to setups with small number of training examples. We address these problems by combining two ideas: first we approximate the kernel matrix using a cleverly sampled subset of its columns using the Nystr\"{o}m method; secondly, as we wish to avoid using this matrix altogether, we decompose it by SVD to form new "virtual samples," on which any linear dictionary learning can be employed. Our method, termed "Linearized Kernel Dictionary Learning" (LKDL) can be seamlessly applied as a pre-processing stage on top of any efficient off-the-shelf dictionary learning scheme, effectively "kernelizing" it. We demonstrate the effectiveness of our method on several tasks of both supervised and unsupervised classification and show the efficiency of the proposed scheme, its easy integration and performance boosting properties. version:1
arxiv-1509-06279 | Sports highlights generation based on acoustic events detection: A rugby case study | http://arxiv.org/abs/1509.06279 | id:1509.06279 author:Anant Baijal, Jaeyoun Cho, Woojung Lee, Byeong-Seob Ko category:cs.SD cs.AI cs.LG  published:2015-09-18 summary:We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient. version:1
arxiv-1507-00955 | Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and Their Combination | http://arxiv.org/abs/1507.00955 | id:1507.00955 author:Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste category:cs.CL cs.IR cs.LG stat.ME stat.ML  published:2015-07-03 summary:This paper covers the two approaches for sentiment analysis: i) lexicon based method; ii) machine learning method. We describe several techniques to implement these approaches and discuss how they can be adopted for sentiment classification of Twitter messages. We present a comparative study of different lexicon combinations and show that enhancing sentiment lexicons with emoticons, abbreviations and social-media slang expressions increases the accuracy of lexicon-based classification for Twitter. We discuss the importance of feature generation and feature selection processes for machine learning sentiment classification. To quantify the performance of the main sentiment analysis methods over Twitter we run these algorithms on a benchmark Twitter dataset from the SemEval-2013 competition, task 2-B. The results show that machine learning method based on SVM and Naive Bayes classifiers outperforms the lexicon method. We present a new ensemble method that uses a lexicon based sentiment score as input feature for the machine learning approach. The combined method proved to produce more precise classifications. We also show that employing a cost-sensitive classifier for highly unbalanced datasets yields an improvement of sentiment classification performance up to 7%. version:3
arxiv-1509-05592 | Color-Stripe Structured Light Robust to Surface Color and Discontinuity | http://arxiv.org/abs/1509.05592 | id:1509.05592 author:Kwang Hee Lee, Changsoo Je, Sang Wook Lee category:cs.CV cs.GR physics.optics I.2.10; I.4.8  published:2015-09-18 summary:Multiple color stripes have been employed for structured light-based rapid range imaging to increase the number of uniquely identifiable stripes. The use of multiple color stripes poses two problems: (1) object surface color may disturb the stripe color and (2) the number of adjacent stripes required for identifying a stripe may not be maintained near surface discontinuities such as occluding boundaries. In this paper, we present methods to alleviate those problems. Log-gradient filters are employed to reduce the influence of object colors, and color stripes in two and three directions are used to increase the chance of identifying correct stripes near surface discontinuities. Experimental results demonstrate the effectiveness of our methods. version:1
arxiv-1502-00956 | ORB-SLAM: a Versatile and Accurate Monocular SLAM System | http://arxiv.org/abs/1502.00956 | id:1502.00956 author:Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos category:cs.RO cs.CV  published:2015-02-03 summary:This paper presents ORB-SLAM, a feature-based monocular SLAM system that operates in real time, in small and large, indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public. version:2
arxiv-1509-05536 | Efficient Clustering on Riemannian Manifolds: A Kernelised Random Projection Approach | http://arxiv.org/abs/1509.05536 | id:1509.05536 author:Kun Zhao, Azadeh Alavi, Arnold Wiliem, Brian C. Lovell category:cs.CV  published:2015-09-18 summary:Reformulating computer vision problems over Riemannian manifolds has demonstrated superior performance in various computer vision applications. This is because visual data often forms a special structure lying on a lower dimensional space embedded in a higher dimensional space. However, since these manifolds belong to non-Euclidean topological spaces, exploiting their structures is computationally expensive, especially when one considers the clustering analysis of massive amounts of data. To this end, we propose an efficient framework to address the clustering problem on Riemannian manifolds. This framework implements random projections for manifold points via kernel space, which can preserve the geometric structure of the original space, but is computationally efficient. Here, we introduce three methods that follow our framework. We then validate our framework on several computer vision applications by comparing against popular clustering methods on Riemannian manifolds. Experimental results demonstrate that our framework maintains the performance of the clustering whilst massively reducing computational complexity by over two orders of magnitude in some cases. version:1
arxiv-1506-03478 | Generative Image Modeling Using Spatial LSTMs | http://arxiv.org/abs/1506.03478 | id:1506.03478 author:Lucas Theis, Matthias Bethge category:stat.ML cs.CV cs.LG  published:2015-06-10 summary:Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting. version:2
arxiv-1509-05520 | An Experimental Survey on Correlation Filter-based Tracking | http://arxiv.org/abs/1509.05520 | id:1509.05520 author:Zhe Chen, Zhibin Hong, Dacheng Tao category:cs.CV  published:2015-09-18 summary:Over these years, Correlation Filter-based Trackers (CFTs) have aroused increasing interests in the field of visual object tracking, and have achieved extremely compelling results in different competitions and benchmarks. In this paper, our goal is to review the developments of CFTs with extensive experimental results. 11 trackers are surveyed in our work, based on which a general framework is summarized. Furthermore, we investigate different training schemes for correlation filters, and also discuss various effective improvements that have been made recently. Comprehensive experiments have been conducted to evaluate the effectiveness and efficiency of the surveyed CFTs, and comparisons have been made with other competing trackers. The experimental results have shown that state-of-art performance, in terms of robustness, speed and accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF. We find that further improvements for correlation filter-based tracking can be made on estimating scales, applying part-based tracking strategy and cooperating with long-term tracking methods. version:1
arxiv-1509-05517 | A Light Sliding-Window Part-of-Speech Tagger for the Apertium Free/Open-Source Machine Translation Platform | http://arxiv.org/abs/1509.05517 | id:1509.05517 author:Gang Chen, Mikel L. Forcada category:cs.CL  published:2015-09-18 summary:This paper describes a free/open-source implementation of the light sliding-window (LSW) part-of-speech tagger for the Apertium free/open-source machine translation platform. Firstly, the mechanism and training process of the tagger are reviewed, and a new method for incorporating linguistic rules is proposed. Secondly, experiments are conducted to compare the performances of the tagger under different window settings, with or without Apertium-style "forbid" rules, with or without Constraint Grammar, and also with respect to the traditional HMM tagger in Apertium. version:1
arxiv-1509-05186 | Accelerated Distance Computation with Encoding Tree for High Dimensional Data | http://arxiv.org/abs/1509.05186 | id:1509.05186 author:Shicong Liu, Junru Shao, Hongtao Lu category:cs.CV  published:2015-09-17 summary:We propose a novel distance to calculate distance between high dimensional vector pairs, utilizing vector quantization generated encodings. Vector quantization based methods are successful in handling large scale high dimensional data. These methods compress vectors into short encodings, and allow efficient distance computation between an uncompressed vector and compressed dataset without decompressing explicitly. However for large datasets, these distance computing methods perform excessive computations. We avoid excessive computations by storing the encodings on an Encoding Tree(E-Tree), interestingly the memory consumption is also lowered. We also propose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree and E-Forest is compatible with various existing quantization-based methods. We show by experiments our methods speed-up distance computing for high dimensional data drastically, and various existing algorithms can benefit from our methods. version:2
arxiv-1410-7383 | A Ternary Non-Commutative Latent Factor Model for Scalable Three-Way Real Tensor Completion | http://arxiv.org/abs/1410.7383 | id:1410.7383 author:Guy Baruch category:stat.ML  published:2014-10-26 summary:Motivated by large-scale Collaborative-Filtering applications, we present a Non-Commuting Latent Factor (NCLF) tensor-completion approach for modeling three-way arrays, which is diagonal like the standard PARAFAC, but wherein different terms distinguish different kinds of three-way relations of co-clusters, as determined by permutations of latent factors. The first key component of the algebraic representation is the usage of two non-commutative real trilinear operations as the building blocks of the approximation. These operations are the standard three dimensional triple-product and a trilinear product on a two-dimensional real vector space, which is a representation of the real Clifford Algebra Cl(1,1) (a certain Majorana spinor). Both operations are purely ternary in that they cannot be decomposed into two group-operations on the relevant spaces. The second key component of the method is combining these operations using permutation-symmetry preserving linear combinations. We apply the model to the MovieLens and Fannie Mae datasets, and find that it outperforms the PARAFAC model. We propose some future directions, such as unsupervised-learning. version:6
arxiv-1509-05473 | Algorithmic statistics, prediction and machine learning | http://arxiv.org/abs/1509.05473 | id:1509.05473 author:Alexey Milovanov category:cs.LG cs.IT math.IT  published:2015-09-17 summary:Algorithmic statistics considers the following problem: given a binary string $x$ (e.g., some experimental data), find a "good" explanation of this data. It uses algorithmic information theory to define formally what is a good explanation. In this paper we extend this framework in two directions. First, the explanations are not only interesting in themselves but also used for prediction: we want to know what kind of data we may reasonably expect in similar situations (repeating the same experiment). We show that some kind of hierarchy can be constructed both in terms of algorithmic statistics and using the notion of a priori probability, and these two approaches turn out to be equivalent. Second, a more realistic approach that goes back to machine learning theory, assumes that we have not a single data string $x$ but some set of "positive examples" $x_1,\ldots,x_l$ that all belong to some unknown set $A$, a property that we want to learn. We want this set $A$ to contain all positive examples and to be as small and simple as possible. We show how algorithmic statistic can be extended to cover this situation. version:1
arxiv-1509-05472 | Learning to Hash for Indexing Big Data - A Survey | http://arxiv.org/abs/1509.05472 | id:1509.05472 author:Jun Wang, Wei Liu, Sanjiv Kumar, Shih-Fu Chang category:cs.LG  published:2015-09-17 summary:The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types, including unsupervised, semi-supervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area. version:1
arxiv-1509-05438 | Sparse Fisher's Linear Discriminant Analysis for Partially Labeled Data | http://arxiv.org/abs/1509.05438 | id:1509.05438 author:Qiyi Lu, Xingye Qiao category:stat.ML stat.ME  published:2015-09-17 summary:Classification is an important tool with many useful applications. Among the many classification methods, Fisher's Linear Discriminant Analysis (LDA) is a traditional model-based approach which makes use of the covariance information. However, in the high-dimensional, low-sample size setting, LDA cannot be directly deployed because the sample covariance is not invertible. While there are modern methods designed to deal with high-dimensional data, they may not fully use the covariance information as LDA does. Hence in some situations, it is still desirable to use a model-based method such as LDA for classification. This article exploits the potential of LDA in more complicated data settings. In many real applications, it is costly to manually place labels on observations; hence it is often that only a small portion of labeled data is available while a large number of observations are left without a label. It is a great challenge to obtain good classification performance through the labeled data alone, especially when the dimension is greater than the size of the labeled data. In order to overcome this issue, we propose a semi-supervised sparse LDA classifier to take advantage of the seemingly useless unlabeled data. They provide additional information which helps to boost the classification performance in some situations. A direct estimation method is used to reconstruct LDA and achieve the sparsity; meanwhile we employ the difference-convex algorithm to handle the non-convex loss function associated with the unlabeled data. Theoretical properties of the proposed classifier are studied. Our simulated examples help to understand when and how the information extracted from the unlabeled data can be useful. A real data example further illustrates the usefulness of the proposed method. version:1
arxiv-1509-05371 | DeXpression: Deep Convolutional Neural Network for Expression Recognition | http://arxiv.org/abs/1509.05371 | id:1509.05371 author:Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel, Marcus Liwicki category:cs.CV cs.LG  published:2015-09-17 summary:We propose a convolutional neural network (CNN) architecture for facial expression recognition. The proposed architecture is independent of any hand-crafted feature extraction and performs better than the earlier proposed convolutional neural network based approaches. We visualize the automatically extracted features which have been learned by the network in order to provide a better understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP) and MMI Facial Expression Databse are used for the quantitative evaluation. On the CKP set the current state of the art approach, using CNNs, achieves an accuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion recognition is 93.33%. The proposed architecture achieves 99.6% for CKP and 98.63% for MMI, therefore performing better than the state of the art using CNNs. Automatic facial expression recognition has a broad spectrum of applications such as human-computer interaction and safety systems. This is due to the fact that non-verbal cues are important forms of communication and play a pivotal role in interpersonal communication. The performance of the proposed architecture endorses the efficacy and reliable usage of the proposed work for real world applications. version:1
arxiv-1509-05366 | Facial Descriptors for Human Interaction Recognition In Still Images | http://arxiv.org/abs/1509.05366 | id:1509.05366 author:Gokhan Tanisik, Cemil Zalluhoglu, Nazli Ikizler-Cinbis category:cs.CV  published:2015-09-17 summary:This paper presents a novel approach in a rarely studied area of computer vision: Human interaction recognition in still images. We explore whether the facial regions and their spatial configurations contribute to the recognition of interactions. In this respect, our method involves extraction of several visual features from the facial regions, as well as incorporation of scene characteristics and deep features to the recognition. Extracted multiple features are utilized within a discriminative learning framework for recognizing interactions between people. Our designed facial descriptors are based on the observation that relative positions, size and locations of the faces are likely to be important for characterizing human interactions. Since there is no available dataset in this relatively new domain, a comprehensive new dataset which includes several images of human interactions is collected. Our experimental results show that faces and scene characteristics contain important information to recognize interactions between people. version:1
arxiv-1509-05329 | Recurrent Spatial Transformer Networks | http://arxiv.org/abs/1509.05329 | id:1509.05329 author:Søren Kaae Sønderby, Casper Kaae Sønderby, Lars Maaløe, Ole Winther category:cs.CV  published:2015-09-17 summary:We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest. version:1
arxiv-1509-05301 | Humans Are Easily Fooled by Digital Images | http://arxiv.org/abs/1509.05301 | id:1509.05301 author:Victor Schetinger, Manuel M. Oliveira, Roberto da Silva, Tiago J. Carvalho category:cs.GR cs.CV cs.HC  published:2015-09-17 summary:Digital images are ubiquitous in our modern lives, with uses ranging from social media to news, and even scientific papers. For this reason, it is crucial evaluate how accurate people are when performing the task of identify doctored images. In this paper, we performed an extensive user study evaluating subjects capacity to detect fake images. After observing an image, users have been asked if it had been altered or not. If the user answered the image has been altered, he had to provide evidence in the form of a click on the image. We collected 17,208 individual answers from 383 users, using 177 images selected from public forensic databases. Different from other previously studies, our method propose different ways to avoid lucky guess when evaluating users answers. Our results indicate that people show inaccurate skills at differentiating between altered and non-altered images, with an accuracy of 58%, and only identifying the modified images 46.5% of the time. We also track user features such as age, answering time, confidence, providing deep analysis of how such variables influence on the users' performance. version:1
arxiv-1509-05285 | Decadal climate predictions using sequential learning algorithms | http://arxiv.org/abs/1509.05285 | id:1509.05285 author:Ehud Strobach, Golan Bel category:physics.ao-ph physics.data-an stat.ML 62C99  published:2015-09-17 summary:Ensembles of climate models are commonly used to improve climate predictions and assess the uncertainties associated with them. Weighting the models according to their performances holds the promise of further improving their predictions. Here, we use an ensemble of decadal climate predictions to demonstrate the ability of sequential learning algorithms (SLAs) to reduce the forecast errors and reduce the uncertainties. Three different SLAs are considered, and their performances are compared with those of an equally weighted ensemble, a linear regression and the climatology. Predictions of four different variables--the surface temperature, the zonal and meridional wind, and pressure--are considered. The spatial distributions of the performances are presented, and the statistical significance of the improvements achieved by the SLAs is tested. Based on the performances of the SLAs, we propose one to be highly suitable for the improvement of decadal climate predictions. version:1
arxiv-1509-05281 | Network analysis of named entity interactions in written texts | http://arxiv.org/abs/1509.05281 | id:1509.05281 author:Diego R. Amancio category:cs.CL physics.data-an physics.soc-ph  published:2015-09-17 summary:The use of methods borrowed from statistics and physics has allowed for the discovery of unprecedent patterns of human behavior and cognition by establishing links between models features and language structure. While current models have been useful to identify patterns via analysis of syntactical and semantical networks, only a few works have probed the relevance of investigating the structure arising from the relationship between relevant entities such as characters, locations and organizations. In this study, we introduce a model that links entities appearing in the same context in order to capture the complexity of entities organization through a networked representation. Computational simulations in books revealed that the proposed model displays interesting topological features, such as short typical shortest path length, high values of clustering coefficient and modular organization. The effectiveness of the our model was verified in a practical pattern recognition task in real networks. When compared with the traditional word adjacency networks, our model displayed optimized results in identifying unknown references in texts. Because the proposed model plays a complementary role in characterizing unstructured documents via topological analysis of named entities, we believe that it could be useful to improve the characterization written texts when combined with other traditional approaches based on statistical and deeper paradigms. version:1
arxiv-1509-05267 | Deep Multi-task Learning for Railway Track Inspection | http://arxiv.org/abs/1509.05267 | id:1509.05267 author:Xavier Gibert, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2015-09-17 summary:Railroad tracks need to be periodically inspected and monitored to ensure safe transportation. Automated track inspection using computer vision and pattern recognition methods have recently shown the potential to improve safety by allowing for more frequent inspections while reducing human errors. Achieving full automation is still very challenging due to the number of different possible failure modes as well as the broad range of image variations that can potentially trigger false alarms. Also, the number of defective components is very small, so not many training examples are available for the machine to learn a robust anomaly detector. In this paper, we show that detection performance can be improved by combining multiple detectors within a multi-task learning framework. We show that this approach results in better accuracy in detecting defects on railway ties and fasteners. version:1
arxiv-1509-05257 | (Blue) Taxi Destination and Trip Time Prediction from Partial Trajectories | http://arxiv.org/abs/1509.05257 | id:1509.05257 author:Hoang Thanh Lam, Ernesto Diaz-Aviles, Alessandra Pascale, Yiannis Gkoufas, Bei Chen category:stat.ML cs.AI cs.CY cs.LG I.2.6; I.5.2  published:2015-09-17 summary:Real-time estimation of destination and travel time for taxis is of great importance for existing electronic dispatch systems. We present an approach based on trip matching and ensemble learning, in which we leverage the patterns observed in a dataset of roughly 1.7 million taxi journeys to predict the corresponding final destination and travel time for ongoing taxi trips, as a solution for the ECML/PKDD Discovery Challenge 2015 competition. The results of our empirical evaluation show that our approach is effective and very robust, which led our team -- BlueTaxi -- to the 3rd and 7th position of the final rankings for the trip time and destination prediction tasks, respectively. Given the fact that the final rankings were computed using a very small test set (with only 320 trips) we believe that our approach is one of the most robust solutions for the challenge based on the consistency of our good results across the test sets. version:1
arxiv-1509-05209 | Extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints | http://arxiv.org/abs/1509.05209 | id:1509.05209 author:Antonio Trenta, Anthony Hunter, Sebastian Riedel category:cs.CL cs.AI  published:2015-09-17 summary:Systematic use of the published results of randomized clinical trials is increasingly important in evidence-based medicine. In order to collate and analyze the results from potentially numerous trials, evidence tables are used to represent trials concerning a set of interventions of interest. An evidence table has columns for the patient group, for each of the interventions being compared, for the criterion for the comparison (e.g. proportion who survived after 5 years from treatment), and for each of the results. Currently, it is a labour-intensive activity to read each published paper and extract the information for each field in an evidence table. There have been some NLP studies investigating how some of the features from papers can be extracted, or at least the relevant sentences identified. However, there is a lack of an NLP system for the systematic extraction of each item of information required for an evidence table. We address this need by a combination of a maximum entropy classifier, and integer linear programming. We use the later to handle constraints on what is an acceptable classification of the features to be extracted. With experimental results, we demonstrate substantial advantages in using global constraints (such as the features describing the patient group, and the interventions, must occur before the features describing the results of the comparison). version:1
arxiv-1509-05195 | Improved Residual Vector Quantization for High-dimensional Approximate Nearest Neighbor Search | http://arxiv.org/abs/1509.05195 | id:1509.05195 author:Shicong Liu, Hongtao Lu, Junru Shao category:cs.CV  published:2015-09-17 summary:Quantization methods have been introduced to perform large scale approximate nearest search tasks. Residual Vector Quantization (RVQ) is one of the effective quantization methods. RVQ uses a multi-stage codebook learning scheme to lower the quantization error stage by stage. However, there are two major limitations for RVQ when applied to on high-dimensional approximate nearest neighbor search: 1. The performance gain diminishes quickly with added stages. 2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an improved residual vector quantization (IRVQ) method, our IRVQ learns codebook with a hybrid method of subspace clustering and warm-started k-means on each stage to prevent performance gain from dropping, and uses a multi-path encoding scheme to encode a vector with lower distortion. Experimental results on the benchmark datasets show that our method gives substantially improves RVQ and delivers better performance compared to the state-of-the-art. version:1
arxiv-1509-05194 | HCLAE: High Capacity Locally Aggregating Encodings for Approximate Nearest Neighbor Search | http://arxiv.org/abs/1509.05194 | id:1509.05194 author:Shicong Liu, Junru Shao, Hongtao Lu category:cs.CV  published:2015-09-17 summary:Vector quantization-based approaches are successful to solve Approximate Nearest Neighbor (ANN) problems which are critical to many applications. The idea is to generate effective encodings to allow fast distance approximation. We propose quantization-based methods should partition the data space finely and exhibit locality of the dataset to allow efficient non-exhaustive search. In this paper, we introduce the concept of High Capacity Locality Aggregating Encodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn HCLAE by a simulated annealing procedure. The quantization error is lower than other state-of-the-art. The algorithms of DA can be easily extended to an online learning scheme, allowing effective handle of large scale data. Further, we propose Aggregating-Tree (A-Tree), a non-exhaustive search method using HCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up on ANN-Search tasks, compared to the state-of-the-art. version:1
arxiv-1509-05173 | Taming the ReLU with Parallel Dither in a Deep Neural Network | http://arxiv.org/abs/1509.05173 | id:1509.05173 author:Andrew J. R. Simpson category:cs.LG 68Txx  published:2015-09-17 summary:Rectified Linear Units (ReLU) seem to have displaced traditional 'smooth' nonlinearities as activation-function-du-jour in many - but not all - deep neural network (DNN) applications. However, nobody seems to know why. In this article, we argue that ReLU are useful because they are ideal demodulators - this helps them perform fast abstract learning. However, this fast learning comes at the expense of serious nonlinear distortion products - decoy features. We show that Parallel Dither acts to suppress the decoy features, preventing overfitting and leaving the true features cleanly demodulated for rapid, reliable learning. version:1
arxiv-1509-05086 | Fast Sequence Component Analysis for Attack Detection in Synchrophasor Networks | http://arxiv.org/abs/1509.05086 | id:1509.05086 author:Jordan Landford, Rich Meier, Richard Barella, Xinghui Zhao, Eduardo Cotilla-Sanchez, Robert B. Bass, Scott Wallace category:cs.LG cs.CR  published:2015-09-17 summary:Modern power systems have begun integrating synchrophasor technologies into part of daily operations. Given the amount of solutions offered and the maturity rate of application development it is not a matter of "if" but a matter of "when" in regards to these technologies becoming ubiquitous in control centers around the world. While the benefits are numerous, the functionality of operator-level applications can easily be nullified by injection of deceptive data signals disguised as genuine measurements. Such deceptive action is a common precursor to nefarious, often malicious activity. A correlation coefficient characterization and machine learning methodology are proposed to detect and identify injection of spoofed data signals. The proposed method utilizes statistical relationships intrinsic to power system parameters, which are quantified and presented. Several spoofing schemes have been developed to qualitatively and quantitatively demonstrate detection capabilities. version:1
