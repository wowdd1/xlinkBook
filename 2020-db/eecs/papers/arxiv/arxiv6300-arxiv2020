arxiv-1407-0577 | Systematic Derivation of Behaviour Characterisations in Evolutionary Robotics | http://arxiv.org/abs/1407.0577 | id:1407.0577 author:Jorge Gomes, Pedro Mariano, Anders Lyhne Christensen category:cs.NE cs.MA cs.RO  published:2014-07-02 summary:Evolutionary techniques driven by behavioural diversity, such as novelty search, have shown significant potential in evolutionary robotics. These techniques rely on priorly specified behaviour characterisations to estimate the similarity between individuals. Characterisations are typically defined in an ad hoc manner based on the experimenter's intuition and knowledge about the task. Alternatively, generic characterisations based on the sensor-effector values of the agents are used. In this paper, we propose a novel approach that allows for systematic derivation of behaviour characterisations for evolutionary robotics, based on a formal description of the agents and their environment. Systematically derived behaviour characterisations (SDBCs) go beyond generic characterisations in that they can contain task-specific features related to the internal state of the agents, environmental features, and relations between them. We evaluate SDBCs with novelty search in three simulated collective robotics tasks. Our results show that SDBCs yield a performance comparable to the task-specific characterisations, in terms of both solution quality and behaviour space exploration. version:1
arxiv-1407-0576 | Novelty Search in Competitive Coevolution | http://arxiv.org/abs/1407.0576 | id:1407.0576 author:Jorge Gomes, Pedro Mariano, Anders Lyhne Christensen category:cs.NE cs.MA  published:2014-07-02 summary:One of the main motivations for the use of competitive coevolution systems is their ability to capitalise on arms races between competing species to evolve increasingly sophisticated solutions. Such arms races can, however, be hard to sustain, and it has been shown that the competing species often converge prematurely to certain classes of behaviours. In this paper, we investigate if and how novelty search, an evolutionary technique driven by behavioural novelty, can overcome convergence in coevolution. We propose three methods for applying novelty search to coevolutionary systems with two species: (i) score both populations according to behavioural novelty; (ii) score one population according to novelty, and the other according to fitness; and (iii) score both populations with a combination of novelty and fitness. We evaluate the methods in a predator-prey pursuit task. Our results show that novelty-based approaches can evolve a significantly more diverse set of solutions, when compared to traditional fitness-based coevolution. version:1
arxiv-1310-5034 | A Theoretical and Experimental Comparison of the EM and SEM Algorithm | http://arxiv.org/abs/1310.5034 | id:1310.5034 author:Johannes Blömer, Kathrin Bujna, Daniel Kuntze category:cs.LG stat.ML  published:2013-10-18 summary:In this paper we provide a new analysis of the SEM algorithm. Unlike previous work, we focus on the analysis of a single run of the algorithm. First, we discuss the algorithm for general mixture distributions. Second, we consider Gaussian mixture models and show that with high probability the update equations of the EM algorithm and its stochastic variant are almost the same, given that the input set is sufficiently large. Our experiments confirm that this still holds for a large number of successive update steps. In particular, for Gaussian mixture models, we show that the stochastic variant runs nearly twice as fast. version:2
arxiv-1407-0286 | DC approximation approaches for sparse optimization | http://arxiv.org/abs/1407.0286 | id:1407.0286 author:Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, Xuan Thanh Vo category:cs.NA cs.LG stat.ML 90C26  90C90  published:2014-07-01 summary:Sparse optimization refers to an optimization problem involving the zero-norm in objective or constraints. In this paper, nonconvex approximation approaches for sparse optimization have been studied with a unifying point of view in DC (Difference of Convex functions) programming framework. Considering a common DC approximation of the zero-norm including all standard sparse inducing penalty functions, we studied the consistency between global minimums (resp. local minimums) of approximate and original problems. We showed that, in several cases, some global minimizers (resp. local minimizers) of the approximate problem are also those of the original problem. Using exact penalty techniques in DC programming, we proved stronger results for some particular approximations, namely, the approximate problem, with suitable parameters, is equivalent to the original problem. The efficiency of several sparse inducing penalty functions have been fully analyzed. Four DCA (DC Algorithm) schemes were developed that cover all standard algorithms in nonconvex sparse approximation approaches as special versions. They can be viewed as, an $\ell _{1}$-perturbed algorithm / reweighted-$\ell _{1}$ algorithm / reweighted-$\ell _{1}$ algorithm. We offer a unifying nonconvex approximation approach, with solid theoretical tools as well as efficient algorithms based on DC programming and DCA, to tackle the zero-norm and sparse optimization. As an application, we implemented our methods for the feature selection in SVM (Support Vector Machine) problem and performed empirical comparative numerical experiments on the proposed algorithms with various approximation functions. version:2
arxiv-1403-1124 | Estimating complex causal effects from incomplete observational data | http://arxiv.org/abs/1403.1124 | id:1403.1124 author:Juha Karvanen category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2014-03-05 summary:Despite the major advances taken in causal modeling, causality is still an unfamiliar topic for many statisticians. In this paper, it is demonstrated from the beginning to the end how causal effects can be estimated from observational data assuming that the causal structure is known. To make the problem more challenging, the causal effects are highly nonlinear and the data are missing at random. The tools used in the estimation include causal models with design, causal calculus, multiple imputation and generalized additive models. The main message is that a trained statistician can estimate causal effects by judiciously combining existing tools. version:2
arxiv-1407-0449 | Classification-based Approximate Policy Iteration: Experiments and Extended Discussions | http://arxiv.org/abs/1407.0449 | id:1407.0449 author:Amir-massoud Farahmand, Doina Precup, André M. S. Barreto, Mohammad Ghavamzadeh category:cs.LG cs.SY math.OC stat.ML I.2.6; I.2.8  published:2014-07-02 summary:Tackling large approximate dynamic programming or reinforcement learning problems requires methods that can exploit regularities, or intrinsic structure, of the problem in hand. Most current methods are geared towards exploiting the regularities of either the value function or the policy. We introduce a general classification-based approximate policy iteration (CAPI) framework, which encompasses a large class of algorithms that can exploit regularities of both the value function and the policy space, depending on what is advantageous. This framework has two main components: a generic value function estimator and a classifier that learns a policy based on the estimated value function. We establish theoretical guarantees for the sample complexity of CAPI-style algorithms, which allow the policy evaluation step to be performed by a wide variety of algorithms (including temporal-difference-style methods), and can handle nonparametric representations of policies. Our bounds on the estimation error of the performance loss are tighter than existing results. We also illustrate this approach empirically on several problems, including a large HIV control task. version:1
arxiv-1407-0067 | Rates of Convergence for Nearest Neighbor Classification | http://arxiv.org/abs/1407.0067 | id:1407.0067 author:Kamalika Chaudhuri, Sanjoy Dasgupta category:cs.LG math.ST stat.ML stat.TH  published:2014-06-30 summary:Nearest neighbor methods are a popular class of nonparametric estimators with several desirable properties, such as adaptivity to different distance scales in different regions of space. Prior work on convergence rates for nearest neighbor classification has not fully reflected these subtle properties. We analyze the behavior of these estimators in metric spaces and provide finite-sample, distribution-dependent rates of convergence under minimal assumptions. As a by-product, we are able to establish the universal consistency of nearest neighbor in a broader range of data spaces than was previously known. We illustrate our upper and lower bounds by introducing smoothness classes that are customized for nearest neighbor classification. version:2
arxiv-1407-0424 | A Dynamic Simulation-Optimization Model for Adaptive Management of Urban Water Distribution System Contamination Threats | http://arxiv.org/abs/1407.0424 | id:1407.0424 author:Amin Rasekh, Kelly Brumbelow category:cs.OH cs.NE  published:2014-07-01 summary:Urban water distribution systems hold a critical and strategic position in preserving public health and industrial growth. Despite the ubiquity of these urban systems, aging infrastructure, and increased risk of terrorism, decision support models for a timely and adaptive contamination emergency response still remain at an undeveloped stage. Emergency response is characterized as a progressive, interactive, and adaptive process that involves parallel activities of processing streaming information and executing response actions. This study develops a dynamic decision support model that adaptively simulates the time-varying emergency environment and tracks changing best health protection response measures at every stage of an emergency in real-time. Feedback mechanisms between the contaminated network, emergency managers, and consumers are incorporated in a dynamic simulation model to capture time-varying characteristics of an emergency environment. An evolutionary-computation-based dynamic optimization model is developed to adaptively identify time-dependant optimal health protection measures during an emergency. This dynamic simulation-optimization model treats perceived contaminant source attributes as time-varying parameters to account for perceived contamination source updates as more data stream in over time. Performance of the developed dynamic decision support model is analyzed and demonstrated using a mid-size virtual city that resembles the dynamics and complexity of real-world urban systems. This adaptive emergency response optimization model is intended to be a major component of an all-inclusive cyberinfrastructure for efficient contamination threat management, which is currently under development. version:1
arxiv-1209-5429 | copulaedas: An R Package for Estimation of Distribution Algorithms Based on Copulas | http://arxiv.org/abs/1209.5429 | id:1209.5429 author:Yasser Gonzalez-Fernandez, Marta Soto category:cs.NE cs.MS  published:2012-09-24 summary:The use of copula-based models in EDAs (estimation of distribution algorithms) is currently an active area of research. In this context, the copulaedas package for R provides a platform where EDAs based on copulas can be implemented and studied. The package offers complete implementations of various EDAs based on copulas and vines, a group of well-known optimization problems, and utility functions to study the performance of the algorithms. Newly developed EDAs can be easily integrated into the package by extending an S4 class with generic functions for their main components. This paper presents copulaedas by providing an overview of EDAs based on copulas, a description of the implementation of the package, and an illustration of its use through examples. The examples include running the EDAs defined in the package, implementing new algorithms, and performing an empirical study to compare the behavior of different algorithms on benchmark functions and a real-world problem. version:4
arxiv-1407-0342 | A New Path to Construct Parametric Orientation Field: Sparse FOMFE Model and Compressed Sparse FOMFE Model | http://arxiv.org/abs/1407.0342 | id:1407.0342 author:Jinwei Xu, Jiankun Hu, Xiuping Jia category:cs.CV cs.CR  published:2014-07-01 summary:Orientation field, representing the fingerprint ridge structure direction, plays a crucial role in fingerprint-related image processing tasks. Orientation field is able to be constructed by either non-parametric or parametric methods. In this paper, the advantages and disadvantages regarding to the existing non-parametric and parametric approaches are briefly summarized. With the further investigation for constructing the orientation field by parametric technique, two new models - sparse FOMFE model and compressed sparse FOMFE model are introduced, based on the rapidly developing signal sparse representation and compressed sensing theories. The experiments on high-quality fingerprint image dataset (plain and rolled print) and poor-quality fingerprint image dataset (latent print) demonstrate their feasibilities to construct the orientation field in a sparse or even compressed sparse mode. The comparisons among the state-of-art orientation field modeling approaches show that the proposed two models have the potential availability in big data-oriented fingerprint indexing tasks. version:1
arxiv-1407-0265 | Supervised learning in Spiking Neural Networks with Limited Precision: SNN/LP | http://arxiv.org/abs/1407.0265 | id:1407.0265 author:Evangelos Stromatias, John Marsland category:cs.NE  published:2014-07-01 summary:A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural Networks. This novel algorithm uses limited precision for both synaptic weights and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for the supervised training. The results are comparable or better than previously published work. The results are applicable to the realization of large scale hardware neural networks. One of the trained networks is implemented in programmable hardware. version:1
arxiv-1407-0221 | Imaging with Kantorovich-Rubinstein discrepancy | http://arxiv.org/abs/1407.0221 | id:1407.0221 author:Jan Lellmann, Dirk A. Lorenz, Carola Schönlieb, Tuomo Valkonen category:cs.CV math.NA  published:2014-07-01 summary:We propose the use of the Kantorovich-Rubinstein norm from optimal transport in imaging problems. In particular, we discuss a variational regularisation model endowed with a Kantorovich-Rubinstein discrepancy term and total variation regularization in the context of image denoising and cartoon-texture decomposition. We point out connections of this approach to several other recently proposed methods such as total generalized variation and norms capturing oscillating patterns. We also show that the respective optimization problem can be turned into a convex-concave saddle point problem with simple constraints and hence, can be solved by standard tools. Numerical examples exhibit interesting features and favourable performance for denoising and cartoon-texture decomposition. version:1
arxiv-1407-0179 | Mind the Nuisance: Gaussian Process Classification using Privileged Noise | http://arxiv.org/abs/1407.0179 | id:1407.0179 author:Daniel Hernández-Lobato, Viktoriia Sharmanska, Kristian Kersting, Christoph H. Lampert, Novi Quadrianto category:stat.ML cs.LG  published:2014-07-01 summary:The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian Process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC sigmoid likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information. version:1
arxiv-1407-0167 | Mathematical Language Processing Project | http://arxiv.org/abs/1407.0167 | id:1407.0167 author:Robert Pagael, Moritz Schubotz category:cs.DL cs.CL cs.IR  published:2014-07-01 summary:In natural language, words and phrases themselves imply the semantics. In contrast, the meaning of identifiers in mathematical formulae is undefined. Thus scientists must study the context to decode the meaning. The Mathematical Language Processing (MLP) project aims to support that process. In this paper, we compare two approaches to discover identifier-definition tuples. At first we use a simple pattern matching approach. Second, we present the MLP approach that uses part-of-speech tag based distances as well as sentence positions to calculate identifier-definition probabilities. The evaluation of our prototypical system, applied on the Wikipedia text corpus, shows that our approach augments the user experience substantially. While hovering the identifiers in the formula, tool-tips with the most probable definitions occur. Tests with random samples show that the displayed definitions provide a good match with the actual meaning of the identifiers. version:1
arxiv-1401-2818 | Multilinear Wavelets: A Statistical Shape Space for Human Faces | http://arxiv.org/abs/1401.2818 | id:1401.2818 author:Alan Brunton, Timo Bolkart, Stefanie Wuhrer category:cs.CV cs.GR  published:2014-01-13 summary:We present a statistical model for $3$D human faces in varying expression, which decomposes the surface of the face using a wavelet transform, and learns many localized, decorrelated multilinear models on the resulting coefficients. Using this model we are able to reconstruct faces from noisy and occluded $3$D face scans, and facial motion sequences. Accurate reconstruction of face shape is important for applications such as tele-presence and gaming. The localized and multi-scale nature of our model allows for recovery of fine-scale detail while retaining robustness to severe noise and occlusion, and is computationally efficient and scalable. We validate these properties experimentally on challenging data in the form of static scans and motion sequences. We show that in comparison to a global multilinear model, our model better preserves fine detail and is computationally faster, while in comparison to a localized PCA model, our model better handles variation in expression, is faster, and allows us to fix identity parameters for a given subject. version:2
arxiv-1307-0998 | A Unified Framework of Elementary Geometric Transformation Representation | http://arxiv.org/abs/1307.0998 | id:1307.0998 author:F. Lu, Z. Chen category:cs.CV  published:2013-07-03 summary:As an extension of projective homology, stereohomology is proposed via an extension of Desargues theorem and the extended Desargues configuration. Geometric transformations such as reflection, translation, central symmetry, central projection, parallel projection, shearing, central dilation, scaling, and so on are all included in stereohomology and represented as Householder-Chen elementary matrices. Hence all these geometric transformations are called elementary. This makes it possible to represent these elementary geometric transformations in homogeneous square matrices independent of a particular choice of coordinate system. version:3
arxiv-1405-3352 | Newton-Type Iterative Solver for Multiple View $L2$ Triangulation | http://arxiv.org/abs/1405.3352 | id:1405.3352 author:F. Lu, Z. Chen category:cs.CV cs.GR  published:2014-05-14 summary:In this note, we show that the L2 optimal solutions to most real multiple view L2 triangulation problems can be efficiently obtained by two-stage Newton-like iterative methods, while the difficulty of such problems mainly lies in how to verify the L2 optimality. Such a working two-stage bundle adjustment approach features: first, the algorithm is initialized by symmedian point triangulation, a multiple-view generalization of the mid-point method; second, a symbolic-numeric method is employed to compute derivatives accurately; third, globalizing strategy such as line search or trust region is smoothly applied to the underlying iteration which assures algorithm robustness in general cases. Numerical comparison with tfml method shows that the local minimizers obtained by the two-stage iterative bundle adjustment approach proposed here are also the L2 optimal solutions to all the calibrated data sets available online by the Oxford visual geometry group. Extensive numerical experiments indicate the bundle adjustment approach solves more than 99% the real triangulation problems optimally. An IEEE 754 double precision C++ implementation shows that it takes only about 0.205 second tocompute allthe 4983 points in the Oxford dinosaur data setvia Gauss-Newton iteration hybrid with a line search strategy on a computer with a 3.4GHz Intel i7 CPU. version:2
arxiv-1407-0061 | Block matching algorithm for motion estimation based on Artificial Bee Colony (ABC) | http://arxiv.org/abs/1407.0061 | id:1407.0061 author:Erik Cuevas, Daniel Zaldivar, Marco Perez, Humberto Sossa, Valentin Osuna category:cs.NE  published:2014-06-30 summary:Block matching (BM) motion estimation plays a very important role in video coding. In a BM approach, image frames in a video sequence are divided into blocks. For each block in the current frame, the best matching block is identified inside a region of the previous frame, aiming to minimize the sum of absolute differences (SAD). Unfortunately, the SAD evaluation is computationally expensive and represents the most consuming operation in the BM process. Therefore, BM motion estimation can be approached as an optimization problem, where the goal is to find the best matching block within a search space. The simplest available BM method is the full search algorithm (FSA) which finds the most accurate motion vector through an exhaustive computation of SAD values for all elements of the search window. Recently, several fast BM algorithms have been proposed to reduce the number of SAD operations by calculating only a fixed subset of search locations at the price of poor accuracy. In this paper, a new algorithm based on Artificial Bee Colony (ABC) optimization is proposed to reduce the number of search locations in the BM process. In our algorithm, the computation of search locations is drastically reduced by considering a fitness calculation strategy which indicates when it is feasible to calculate or only estimate new search locations. Since the proposed algorithm does not consider any fixed search pattern or any other movement assumption as most of other BM approaches do, a high probability for finding the true minimum (accurate motion vector) is expected. Conducted simulations show that the proposed method achieves the best balance over other fast BM algorithms, in terms of both estimation accuracy and computational cost. version:1
arxiv-1407-0044 | Infinite Structured Hidden Semi-Markov Models | http://arxiv.org/abs/1407.0044 | id:1407.0044 author:Jonathan H. Huggins, Frank Wood category:stat.ME stat.AP stat.ML  published:2014-06-30 summary:This paper reviews recent advances in Bayesian nonparametric techniques for constructing and performing inference in infinite hidden Markov models. We focus on variants of Bayesian nonparametric hidden Markov models that enhance a posteriori state-persistence in particular. This paper also introduces a new Bayesian nonparametric framework for generating left-to-right and other structured, explicit-duration infinite hidden Markov models that we call the infinite structured hidden semi-Markov model. version:1
arxiv-1406-7811 | An optimization algorithm for multimodal functions inspired by collective animal behavior | http://arxiv.org/abs/1406.7811 | id:1406.7811 author:Erik Cuevas, Mauricio Gonzalez category:cs.NE  published:2014-06-30 summary:Interest in multimodal function optimization is expanding rapidly since real world optimization problems often demand locating multiple optima within a search space. This article presents a new multimodal optimization algorithm named as the Collective Animal Behavior (CAB). Animal groups, such as schools of fish, flocks of birds, swarms of locusts and herds of wildebeest, exhibit a variety of behaviors including swarming about a food source, milling around a central location or migrating over large distances in aligned groups. These collective behaviors are often advantageous to groups, allowing them to increase their harvesting efficiency to follow better migration routes, to improve their aerodynamic and to avoid predation. In the proposed algorithm, searcher agents are a group of animals which interact to each other based on the biological laws of collective motion. Experimental results demonstrate that the proposed algorithm is capable of finding global and local optima of benchmark multimodal optimization problems with a higher efficiency in comparison to other methods reported in the literature. version:1
arxiv-1406-7799 | Subjective and Objective Quality Assessment of Image: A Survey | http://arxiv.org/abs/1406.7799 | id:1406.7799 author:Pedram Mohammadi, Abbas Ebrahimi-Moghadam, Shahram Shirani category:cs.MM cs.CV  published:2014-06-30 summary:With the increasing demand for image-based applications, the efficient and reliable evaluation of image quality has increased in importance. Measuring the image quality is of fundamental importance for numerous image processing applications, where the goal of image quality assessment (IQA) methods is to automatically evaluate the quality of images in agreement with human quality judgments. Numerous IQA methods have been proposed over the past years to fulfill this goal. In this paper, a survey of the quality assessment methods for conventional image signals, as well as the newly emerged ones, which includes the high dynamic range (HDR) and 3-D images, is presented. A comprehensive explanation of the subjective and objective IQA and their classification is provided. Six widely used subjective quality datasets, and performance measures are reviewed. Emphasis is given to the full-reference image quality assessment (FR-IQA) methods, and 9 often-used quality measures (including mean squared error (MSE), structural similarity index (SSIM), multi-scale structural similarity index (MS-SSIM), visual information fidelity (VIF), most apparent distortion (MAD), feature similarity measure (FSIM), feature similarity measure for color images (FSIMC), dynamic range independent measure (DRIM), and tone-mapped images quality index (TMQI)) are carefully described, and their performance and computation time on four subjective quality datasets are evaluated. Furthermore, a brief introduction to 3-D IQA is provided and the issues related to this area of research are reviewed. version:1
arxiv-1406-7758 | Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process Hyper-Parameters | http://arxiv.org/abs/1406.7758 | id:1406.7758 author:Ziyu Wang, Nando de Freitas category:stat.ML cs.LG  published:2014-06-30 summary:Bayesian optimisation has gained great popularity as a tool for optimising the parameters of machine learning algorithms and models. Somewhat ironically, setting up the hyper-parameters of Bayesian optimisation methods is notoriously hard. While reasonable practical solutions have been advanced, they can often fail to find the best optima. Surprisingly, there is little theoretical analysis of this crucial problem in the literature. To address this, we derive a cumulative regret bound for Bayesian optimisation with Gaussian processes and unknown kernel hyper-parameters in the stochastic setting. The bound, which applies to the expected improvement acquisition function and sub-Gaussian observation noise, provides us with guidelines on how to design hyper-parameter estimation methods. A simple simulation demonstrates the importance of following these guidelines. version:1
arxiv-1407-0014 | Dispersion and Line Formation in Artificial Swarm Intelligence | http://arxiv.org/abs/1407.0014 | id:1407.0014 author:Donghwa Jeong, Kiju Lee category:cs.NE  published:2014-06-30 summary:One of the major motifs in collective or swarm intelligence is that, even though individuals follow simple rules, the resulting global behavior can be complex and intelligent. In artificial swarm systems, such as swarm robots, the goal is to use systems that are as simple and cheap as possible, deploy many of them, and coordinate them to conduct complex tasks that each individual cannot accomplish. Shape formation in artificial intelligence systems is usually required for specific task-oriented performance, including 1) forming sensing grids, 2) exploring and mapping in space, underwater, or hazardous environments, and 3) forming a barricade for surveillance or protecting an area or a person. This paper presents a dynamic model of an artificial swarm system based on a virtual spring damper model and algorithms for dispersion without a leader and line formation with an interim leader using only the distance estimation among the neighbors. version:1
arxiv-1407-0013 | Relevance Singular Vector Machine for low-rank matrix sensing | http://arxiv.org/abs/1407.0013 | id:1407.0013 author:Martin Sundin, Saikat Chatterjee, Magnus Jansson, Cristian R. Rojas category:cs.NA cs.LG math.ST stat.TH  published:2014-06-30 summary:In this paper we develop a new Bayesian inference method for low rank matrix reconstruction. We call the new method the Relevance Singular Vector Machine (RSVM) where appropriate priors are defined on the singular vectors of the underlying matrix to promote low rank. To accelerate computations, a numerically efficient approximation is developed. The proposed algorithms are applied to matrix completion and matrix reconstruction problems and their performance is studied numerically. version:1
arxiv-1401-8092 | Cross-calibration of Time-of-flight and Colour Cameras | http://arxiv.org/abs/1401.8092 | id:1401.8092 author:Miles Hansard, Georgios Evangelidis, Quentin Pelorson, Radu Horaud category:cs.CV cs.RO  published:2014-01-31 summary:Time-of-flight cameras provide depth information, which is complementary to the photometric appearance of the scene in ordinary images. It is desirable to merge the depth and colour information, in order to obtain a coherent scene representation. However, the individual cameras will have different viewpoints, resolutions and fields of view, which means that they must be mutually calibrated. This paper presents a geometric framework for this multi-view and multi-modal calibration problem. It is shown that three-dimensional projective transformations can be used to align depth and parallax-based representations of the scene, with or without Euclidean reconstruction. A new evaluation procedure is also developed; this allows the reprojection error to be decomposed into calibration and sensor-dependent components. The complete approach is demonstrated on a network of three time-of-flight and six colour cameras. The applications of such a system, to a range of automatic scene-interpretation problems, are discussed. version:2
arxiv-1210-3456 | Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data | http://arxiv.org/abs/1210.3456 | id:1210.3456 author:Mingjun Zhong, Rong Liu, Bo Liu category:stat.AP cs.LG q-bio.GN q-bio.MN stat.ML  published:2012-10-12 summary:MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play important regulatory roles in post-transcriptional gene regulation by inhibiting the translation of the mRNA into proteins or otherwise cleaving the target mRNA. Inferring miRNA targets provides useful information for understanding the roles of miRNA in biological processes that are potentially involved in complex diseases. Statistical methodologies for point estimation, such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm, have been proposed to identify the interactions of miRNA and mRNA based on sequence and expression data. In this paper, we propose using the Bayesian LASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the interactions between miRNA and mRNA using expression data. The proposed Bayesian methods explore the posterior distributions for those parameters required to model the miRNA-mRNA interactions. These approaches can be used to observe the inferred effects of the miRNAs on the targets by plotting the posterior distributions of those parameters. For comparison purposes, the Least Squares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO (nLASSO), and the proposed Bayesian approaches were applied to four public datasets. We concluded that nLASSO and nBLASSO perform best in terms of sensitivity and specificity. Compared to the point estimate algorithms, which only provide single estimates for those parameters, the Bayesian methods are more meaningful and provide credible intervals, which take into account the uncertainty of the inferred interactions of the miRNA and mRNA. Furthermore, Bayesian methods naturally provide statistical significance to select convincing inferred interactions, while point estimate algorithms require a manually chosen threshold, which is less meaningful, to choose the possible interactions. version:2
arxiv-1404-7055 | Data Requirement for Phylogenetic Inference from Multiple Loci: A New Distance Method | http://arxiv.org/abs/1404.7055 | id:1404.7055 author:Gautam Dasarathy, Robert Nowak, Sebastien Roch category:q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH  published:2014-04-28 summary:We consider the problem of estimating the evolutionary history of a set of species (phylogeny or species tree) from several genes. It is known that the evolutionary history of individual genes (gene trees) might be topologically distinct from each other and from the underlying species tree, possibly confounding phylogenetic analysis. A further complication in practice is that one has to estimate gene trees from molecular sequences of finite length. We provide the first full data-requirement analysis of a species tree reconstruction method that takes into account estimation errors at the gene level. Under that criterion, we also devise a novel reconstruction algorithm that provably improves over all previous methods in a regime of interest. version:2
arxiv-1406-7638 | Direct Density-Derivative Estimation and Its Application in KL-Divergence Approximation | http://arxiv.org/abs/1406.7638 | id:1406.7638 author:Hiroaki Sasaki, Yung-Kyun Noh, Masashi Sugiyama category:stat.ML  published:2014-06-30 summary:Estimation of density derivatives is a versatile tool in statistical data analysis. A naive approach is to first estimate the density and then compute its derivative. However, such a two-step approach does not work well because a good density estimator does not necessarily mean a good density-derivative estimator. In this paper, we give a direct method to approximate the density derivative without estimating the density itself. Our proposed estimator allows analytic and computationally efficient approximation of multi-dimensional high-order density derivatives, with the ability that all hyper-parameters can be chosen objectively by cross-validation. We further show that the proposed density-derivative estimator is useful in improving the accuracy of non-parametric KL-divergence estimation via metric learning. The practical superiority of the proposed method is experimentally demonstrated in change detection and feature selection. version:1
arxiv-1406-3922 | Personalized Medical Treatments Using Novel Reinforcement Learning Algorithms | http://arxiv.org/abs/1406.3922 | id:1406.3922 author:Yousuf M. Soliman category:cs.LG stat.ML  published:2014-06-16 summary:In both the fields of computer science and medicine there is very strong interest in developing personalized treatment policies for patients who have variable responses to treatments. In particular, I aim to find an optimal personalized treatment policy which is a non-deterministic function of the patient specific covariate data that maximizes the expected survival time or clinical outcome. I developed an algorithmic framework to solve multistage decision problem with a varying number of stages that are subject to censoring in which the "rewards" are expected survival times. In specific, I developed a novel Q-learning algorithm that dynamically adjusts for these parameters. Furthermore, I found finite upper bounds on the generalized error of the treatment paths constructed by this algorithm. I have also shown that when the optimal Q-function is an element of the approximation space, the anticipated survival times for the treatment regime constructed by the algorithm will converge to the optimal treatment path. I demonstrated the performance of the proposed algorithmic framework via simulation studies and through the analysis of chronic depression data and a hypothetical clinical trial. The censored Q-learning algorithm I developed is more effective than the state of the art clinical decision support systems and is able to operate in environments when many covariate parameters may be unobtainable or censored. version:2
arxiv-1407-0008 | Navigating Robot Swarms Using Collective Intelligence Learned from Golden Shiner Fish | http://arxiv.org/abs/1407.0008 | id:1407.0008 author:Grace Gao category:cs.NE  published:2014-06-30 summary:Navigating networked robot swarms often requires knowing where to go, sensing the environment, and path-planning based on the destination and barriers in the environment. Such a process is computationally intensive. Moreover, as the network scales up, the computational load increases quadratically, or even exponentially. Unlike these man-made systems, most biological systems scale linearly in complexity. Furthermore, the scale of a biological swarm can even enable collective intelligence. One example comes from observations of golden shiner fish. Golden shiners naturally prefer darkness and school together. Each individual golden shiner does not know where the darkness is. Neither does it sense the light gradients in the environment. However, by moving together as a school, they always end up in the shady area. We apply such collective intelligence learned from golden shiner fish to navigating robot swarms. Each individual robot's dynamic is based on the gold shiners' movement strategy---a random walk with its speed modulated by the light intensity and its direction affected by its neighbors. The theoretical analysis and simulation results show that our method 1) promises to navigate a robot swarm with little situational knowledge, 2) simplifies control and decision-making for each individual robot, 3) requires minimal or even no information exchange within the swarm, and 4) is highly distributed, adaptive, and robust. version:1
arxiv-1407-0007 | Information Transfer in Swarms with Leaders | http://arxiv.org/abs/1407.0007 | id:1407.0007 author:Yu Sun, Louis F. Rossi, Chien-Chung Shen, Jennifer Miller, X. Rosalind Wang, Joseph T. Lizier, Mikhail Prokopenko, Upul Senanayake category:cs.NE  published:2014-06-30 summary:Swarm dynamics is the study of collections of agents that interact with one another without central control. In natural systems, insects, birds, fish and other large mammals function in larger units to increase the overall fitness of the individuals. Their behavior is coordinated through local interactions to enhance mate selection, predator detection, migratory route identification and so forth [Andersson and Wallander 2003; Buhl et al. 2006; Nagy et al. 2010; Partridge 1982; Sumpter et al. 2008]. In artificial systems, swarms of autonomous agents can augment human activities such as search and rescue, and environmental monitoring by covering large areas with multiple nodes [Alami et al. 2007; Caruso et al. 2008; Ogren et al. 2004; Paley et al. 2007; Sibley et al. 2002]. In this paper, we explore the interplay between swarm dynamics, covert leadership and theoretical information transfer. A leader is a member of the swarm that acts upon information in addition to what is provided by local interactions. Depending upon the leadership model, leaders can use their external information either all the time or in response to local conditions [Couzin et al. 2005; Sun et al. 2013]. A covert leader is a leader that is treated no differently than others in the swarm, so leaders and followers participate equally in whatever interaction model is used [Rossi et al. 2007]. In this study, we use theoretical information transfer as a means of analyzing swarm interactions to explore whether or not it is possible to distinguish between followers and leaders based on interactions within the swarm. We find that covert leaders can be distinguished from followers in a swarm because they receive less transfer entropy than followers. version:1
arxiv-1406-7558 | Human Communication Systems Evolve by Cultural Selection | http://arxiv.org/abs/1406.7558 | id:1406.7558 author:Nicolas Fay, Monica Tamariz, T Mark Ellison, Dale Barr category:cs.SI cs.CL physics.soc-ph  published:2014-06-29 summary:Human communication systems, such as language, evolve culturally; their components undergo reproduction and variation. However, a role for selection in cultural evolutionary dynamics is less clear. Often neutral evolution (also known as 'drift') models, are used to explain the evolution of human communication systems, and cultural evolution more generally. Under this account, cultural change is unbiased: for instance, vocabulary, baby names and pottery designs have been found to spread through random copying. While drift is the null hypothesis for models of cultural evolution it does not always adequately explain empirical results. Alternative models include cultural selection, which assumes variant adoption is biased. Theoretical models of human communication argue that during conversation interlocutors are biased to adopt the same labels and other aspects of linguistic representation (including prosody and syntax). This basic alignment mechanism has been extended by computer simulation to account for the emergence of linguistic conventions. When agents are biased to match the linguistic behavior of their interlocutor, a single variant can propagate across an entire population of interacting computer agents. This behavior-matching account operates at the level of the individual. We call it the Conformity-biased model. Under a different selection account, called content-biased selection, functional selection or replicator selection, variant adoption depends upon the intrinsic value of the particular variant (e.g., ease of learning or use). This second alternative account operates at the level of the cultural variant. Following Boyd and Richerson we call it the Content-biased model. The present paper tests the drift model and the two biased selection models' ability to explain the spread of communicative signal variants in an experimental micro-society. version:1
arxiv-1405-1004 | Model Consistency of Partly Smooth Regularizers | http://arxiv.org/abs/1405.1004 | id:1405.1004 author:Samuel Vaiter, Gabriel Peyré, Jalal M. Fadili category:math.OC cs.IT math.IT stat.ML  published:2014-05-05 summary:This paper studies least-square regression penalized with partly smooth convex regularizers. This class of functions is very large and versatile allowing to promote solutions conforming to some notion of low-complexity. Indeed, they force solutions of variational problems to belong to a low-dimensional manifold (the so-called model) which is stable under small perturbations of the function. This property is crucial to make the underlying low-complexity model robust to small noise. We show that a generalized "irrepresentable condition" implies stable model selection under small noise perturbations in the observations and the design matrix, when the regularization parameter is tuned proportionally to the noise level. This condition is shown to be almost a necessary condition. We then show that this condition implies model consistency of the regularized estimator. That is, with a probability tending to one as the number of measurements increases, the regularized estimator belongs to the correct low-dimensional model manifold. This work unifies and generalizes several previous ones, where model consistency is known to hold for sparse, group sparse, total variation and low-rank regularizations. version:3
arxiv-1406-7539 | Exploring Task Mappings on Heterogeneous MPSoCs using a Bias-Elitist Genetic Algorithm | http://arxiv.org/abs/1406.7539 | id:1406.7539 author:Wei Quan, Andy D. Pimentel category:cs.PF cs.NE C.4  published:2014-06-29 summary:Exploration of task mappings plays a crucial role in achieving high performance in heterogeneous multi-processor system-on-chip (MPSoC) platforms. The problem of optimally mapping a set of tasks onto a set of given heterogeneous processors for maximal throughput has been known, in general, to be NP-complete. The problem is further exacerbated when multiple applications (i.e., bigger task sets) and the communication between tasks are also considered. Previous research has shown that Genetic Algorithms (GA) typically are a good choice to solve this problem when the solution space is relatively small. However, when the size of the problem space increases, classic genetic algorithms still suffer from the problem of long evolution times. To address this problem, this paper proposes a novel bias-elitist genetic algorithm that is guided by domain-specific heuristics to speed up the evolution process. Experimental results reveal that our proposed algorithm is able to handle large scale task mapping problems and produces high-quality mapping solutions in only a short time period. version:1
arxiv-1406-7536 | Estimating the distribution of Galaxy Morphologies on a continuous space | http://arxiv.org/abs/1406.7536 | id:1406.7536 author:Giuseppe Vinci, Peter Freeman, Jeffrey Newman, Larry Wasserman, Christopher Genovese category:astro-ph.GA astro-ph.CO stat.AP stat.CO stat.ML 85-08  published:2014-06-29 summary:The incredible variety of galaxy shapes cannot be summarized by human defined discrete classes of shapes without causing a possibly large loss of information. Dictionary learning and sparse coding allow us to reduce the high dimensional space of shapes into a manageable low dimensional continuous vector space. Statistical inference can be done in the reduced space via probability distribution estimation and manifold estimation. version:1
arxiv-1406-7525 | Fusion Based Holistic Road Scene Understanding | http://arxiv.org/abs/1406.7525 | id:1406.7525 author:Wenqi Huang, Xiaojin Gong category:cs.CV  published:2014-06-29 summary:This paper addresses the problem of holistic road scene understanding based on the integration of visual and range data. To achieve the grand goal, we propose an approach that jointly tackles object-level image segmentation and semantic region labeling within a conditional random field (CRF) framework. Specifically, we first generate semantic object hypotheses by clustering 3D points, learning their prior appearance models, and using a deep learning method for reasoning their semantic categories. The learned priors, together with spatial and geometric contexts, are incorporated in CRF. With this formulation, visual and range data are fused thoroughly, and moreover, the coupled segmentation and semantic labeling problem can be inferred via Graph Cuts. Our approach is validated on the challenging KITTI dataset that contains diverse complicated road scenarios. Both quantitative and qualitative evaluations demonstrate its effectiveness. version:1
arxiv-1406-7483 | Jabalin: a Comprehensive Computational Model of Modern Standard Arabic Verbal Morphology Based on Traditional Arabic Prosody | http://arxiv.org/abs/1406.7483 | id:1406.7483 author:Alicia Gonzalez Martinez, Susana Lopez Hervas, Doaa Samy, Carlos G. Arques, Antonio Moreno Sandoval category:cs.CL  published:2014-06-29 summary:The computational handling of Modern Standard Arabic is a challenge in the field of natural language processing due to its highly rich morphology. However, several authors have pointed out that the Arabic morphological system is in fact extremely regular. The existing Arabic morphological analyzers have exploited this regularity to variable extent, yet we believe there is still some scope for improvement. Taking inspiration in traditional Arabic prosody, we have designed and implemented a compact and simple morphological system which in our opinion takes further advantage of the regularities encountered in the Arabic morphological system. The output of the system is a large-scale lexicon of inflected forms that has subsequently been used to create an Online Interface for a morphological analyzer of Arabic verbs. The Jabalin Online Interface is available at http://elvira.lllf.uam.es/jabalin/, hosted at the LLI-UAM lab. The generation system is also available under a GNU GPL 3 license. version:1
arxiv-1302-6768 | Missing Entries Matrix Approximation and Completion | http://arxiv.org/abs/1302.6768 | id:1302.6768 author:Gil Shabat, Yaniv Shmueli, Amir Averbuch category:math.NA cs.LG stat.ML  published:2013-02-27 summary:We describe several algorithms for matrix completion and matrix approximation when only some of its entries are known. The approximation constraint can be any whose approximated solution is known for the full matrix. For low rank approximations, similar algorithms appears recently in the literature under different names. In this work, we introduce new theorems for matrix approximation and show that these algorithms can be extended to handle different constraints such as nuclear norm, spectral norm, orthogonality constraints and more that are different than low rank approximations. As the algorithms can be viewed from an optimization point of view, we discuss their convergence to global solution for the convex case. We also discuss the optimal step size and show that it is fixed in each iteration. In addition, the derived matrix completion flow is robust and does not require any parameters. This matrix completion flow is applicable to different spectral minimizations and can be applied to physics, mathematics and electrical engineering problems such as data reconstruction of images and data coming from PDEs such as Helmholtz equation used for electromagnetic waves. version:2
arxiv-1406-5577 | Graphical structure of conditional independencies in determinantal point processes | http://arxiv.org/abs/1406.5577 | id:1406.5577 author:Tvrtko Tadić category:math.PR math.ST stat.ML stat.TH  published:2014-06-21 summary:Determinantal point process have recently been used as models in machine learning and this has raised questions regarding the characterizations of conditional independence. In this paper we investigate characterizations of conditional independence. We describe some conditional independencies through the conditions on the kernel of a determinantal point process, and show many can be obtained using the graph induced by a kernel of the $L$-ensemble. version:2
arxiv-1109-0887 | Learning Nonlinear Functions Using Regularized Greedy Forest | http://arxiv.org/abs/1109.0887 | id:1109.0887 author:Rie Johnson, Tong Zhang category:stat.ML  published:2011-09-05 summary:We consider the problem of learning a forest of nonlinear decision rules with general loss functions. The standard methods employ boosted decision trees such as Adaboost for exponential loss and Friedman's gradient boosting for general loss. In contrast to these traditional boosting algorithms that treat a tree learner as a black box, the method we propose directly learns decision forests via fully-corrective regularized greedy search using the underlying forest structure. Our method achieves higher accuracy and smaller models than gradient boosting (and Adaboost with exponential loss) on many datasets. version:7
arxiv-1406-7445 | Contrastive Feature Induction for Efficient Structure Learning of Conditional Random Fields | http://arxiv.org/abs/1406.7445 | id:1406.7445 author:Ni Lao, Jun Zhu category:cs.LG  published:2014-06-28 summary:Structure learning of Conditional Random Fields (CRFs) can be cast into an L1-regularized optimization problem. To avoid optimizing over a fully linked model, gain-based or gradient-based feature selection methods start from an empty model and incrementally add top ranked features to it. However, for high-dimensional problems like statistical relational learning, training time of these incremental methods can be dominated by the cost of evaluating the gain or gradient of a large collection of candidate features. In this study we propose a fast feature evaluation algorithm called Contrastive Feature Induction (CFI), which only evaluates a subset of features that involve both variables with high signals (deviation from mean) and variables with high errors (residue). We prove that the gradient of candidate features can be represented solely as a function of signals and errors, and that CFI is an efficient approximation of gradient-based evaluation methods. Experiments on synthetic and real data sets show competitive learning speed and accuracy of CFI on pairwise CRFs, compared to state-of-the-art structure learning methods such as full optimization over all features, and Grafting. version:1
arxiv-1406-7444 | Learning to Deblur | http://arxiv.org/abs/1406.7444 | id:1406.7444 author:Christian J. Schuler, Michael Hirsch, Stefan Harmeling, Bernhard Schölkopf category:cs.CV cs.LG  published:2014-06-28 summary:We describe a learning-based approach to blind image deconvolution. It uses a deep layered architecture, parts of which are borrowed from recent work on neural network learning, and parts of which incorporate computations that are specific to image deconvolution. The system is trained end-to-end on a set of artificially generated training examples, enabling competitive performance in blind deconvolution, both with respect to quality and runtime. version:1
arxiv-1406-7429 | Comparison of SVM Optimization Techniques in the Primal | http://arxiv.org/abs/1406.7429 | id:1406.7429 author:Jonathan Katzman, Diane Duros category:cs.LG  published:2014-06-28 summary:This paper examines the efficacy of different optimization techniques in a primal formulation of a support vector machine (SVM). Three main techniques are compared. The dataset used to compare all three techniques was the Sentiment Analysis on Movie Reviews dataset, from kaggle.com. version:1
arxiv-1405-3518 | Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme for Sentiment Analysis and Text Classification | http://arxiv.org/abs/1405.3518 | id:1405.3518 author:Yoon Kim, Owen Zhang category:cs.CL cs.IR  published:2014-05-14 summary:We provide a simple but novel supervised weighting scheme for adjusting term frequency in tf-idf for sentiment analysis and text classification. We compare our method to baseline weighting schemes and find that it outperforms them on multiple benchmarks. The method is robust and works well on both snippets and longer documents. version:2
arxiv-1406-7399 | Intelligent Emergency Message Broadcasting in VANET Using PSO | http://arxiv.org/abs/1406.7399 | id:1406.7399 author:Ghassan Samara, Tareq Alhmiedat category:cs.NI cs.NE  published:2014-06-28 summary:The new type of Mobile Ad hoc Network which is called Vehicular Ad hoc Networks (VANET) created a fertile environment for research. In this research, a protocol Particle Swarm Optimization Contention Based Broadcast (PCBB) is proposed, for fast andeffective dissemination of emergency messages within a geographical area to distribute the emergency message and achieve the safety system, this research will help the VANET system to achieve its safety goals in intelligent and efficient way. version:1
arxiv-1406-7362 | Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning | http://arxiv.org/abs/1406.7362 | id:1406.7362 author:Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG cs.NE  published:2014-06-28 summary:Many state-of-the-art results obtained with deep networks are achieved with the largest models that could be trained, and if more computation power was available, we might be able to exploit much larger datasets in order to improve generalization ability. Whereas in learning algorithms such as decision trees the ratio of capacity (e.g., the number of parameters) to computation is very favorable (up to exponentially more parameters than computation), the ratio is essentially 1 for deep neural networks. Conditional computation has been proposed as a way to increase the capacity of a deep neural network without increasing the amount of computation required, by activating some parameters and computation "on-demand", on a per-example basis. In this note, we propose a novel parametrization of weight matrices in neural networks which has the potential to increase up to exponentially the ratio of the number of parameters to computation. The proposed approach is based on turning on some parameters (weight matrices) when specific bit patterns of hidden unit activations are obtained. In order to better control for the overfitting that might result, we propose a parametrization that is tree-structured, where each node of the tree corresponds to a prefix of a sequence of sign bits, or gating units, associated with hidden units. version:1
arxiv-1401-1549 | Optimal Demand Response Using Device Based Reinforcement Learning | http://arxiv.org/abs/1401.1549 | id:1401.1549 author:Zheng Wen, Daniel O'Neill, Hamid Reza Maei category:cs.LG cs.AI cs.SY  published:2014-01-08 summary:Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS's rescheduling problem as a reinforcement learning (RL) problem, and argue that this RL problem can be approximately solved by decomposing it over device clusters. Compared with existing formulations, our new formulation (1) does not require explicitly modeling the user's dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example. version:2
arxiv-1406-7330 | Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization | http://arxiv.org/abs/1406.7330 | id:1406.7330 author:Felix Ming Fai Wong, Zhenming Liu, Mung Chiang category:cs.LG q-fin.ST  published:2014-06-27 summary:We revisit the problem of predicting directional movements of stock prices based on news articles: here our algorithm uses daily articles from The Wall Street Journal to predict the closing stock prices on the same day. We propose a unified latent space model to characterize the "co-movements" between stock prices and news articles. Unlike many existing approaches, our new model is able to simultaneously leverage the correlations: (a) among stock prices, (b) among news articles, and (c) between stock prices and news articles. Thus, our model is able to make daily predictions on more than 500 stocks (most of which are not even mentioned in any news article) while having low complexity. We carry out extensive backtesting on trading strategies based on our algorithm. The result shows that our model has substantially better accuracy rate (55.7%) compared to many widely used algorithms. The return (56%) and Sharpe ratio due to a trading strategy based on our model are also much higher than baseline indices. version:1
arxiv-1406-7314 | On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels | http://arxiv.org/abs/1406.7314 | id:1406.7314 author:Imen Trabelsi, Dorra Ben Ayed category:cs.CL cs.LG  published:2014-06-27 summary:The speech feature extraction has been a key focus in robust speech recognition research; it significantly affects the recognition performance. In this paper, we first study a set of different features extraction methods such as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP) with several features normalization techniques like rasta filtering and cepstral mean subtraction (CMS). Based on this, a comparative evaluation of these features is performed on the task of text independent speaker identification using a combination between gaussian mixture models (GMM) and linear and non-linear kernels based on support vector machine (SVM). version:1
arxiv-1306-5825 | Fourier PCA and Robust Tensor Decomposition | http://arxiv.org/abs/1306.5825 | id:1306.5825 author:Navin Goyal, Santosh Vempala, Ying Xiao category:cs.LG cs.DS stat.ML  published:2013-06-25 summary:Fourier PCA is Principal Component Analysis of a matrix obtained from higher order derivatives of the logarithm of the Fourier transform of a distribution.We make this method algorithmic by developing a tensor decomposition method for a pair of tensors sharing the same vectors in rank-$1$ decompositions. Our main application is the first provably polynomial-time algorithm for underdetermined ICA, i.e., learning an $n \times m$ matrix $A$ from observations $y=Ax$ where $x$ is drawn from an unknown product distribution with arbitrary non-Gaussian components. The number of component distributions $m$ can be arbitrarily higher than the dimension $n$ and the columns of $A$ only need to satisfy a natural and efficiently verifiable nondegeneracy condition. As a second application, we give an alternative algorithm for learning mixtures of spherical Gaussians with linearly independent means. These results also hold in the presence of Gaussian noise. version:5
arxiv-1407-0380 | A Multi Level Data Fusion Approach for Speaker Identification on Telephone Speech | http://arxiv.org/abs/1407.0380 | id:1407.0380 author:Imen Trabelsi, Dorra Ben Ayed category:cs.SD cs.LG  published:2014-06-27 summary:Several speaker identification systems are giving good performance with clean speech but are affected by the degradations introduced by noisy audio conditions. To deal with this problem, we investigate the use of complementary information at different levels for computing a combined match score for the unknown speaker. In this work, we observe the effect of two supervised machine learning approaches including support vectors machines (SVM) and na\"ive bayes (NB). We define two feature vector sets based on mel frequency cepstral coefficients (MFCC) and relative spectral perceptual linear predictive coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture Model (GMM). Several ways of combining these information sources give significant improvements in a text-independent speaker identification task using a very large telephone degraded NTIMIT database. version:1
arxiv-1311-2191 | Neighborhood filters and the decreasing rearrangement | http://arxiv.org/abs/1311.2191 | id:1311.2191 author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10  published:2013-11-09 summary:Nonlocal filters are simple and powerful techniques for image denoising. In this paper, we give new insights into the analysis of one kind of them, the Neighborhood filter, by using a classical although not very common transformation: the decreasing rearrangement of a function (the image). Independently of the dimension of the image, we reformulate the Neighborhood filter and its iterative variants as an integral operator defined in a one-dimensional space. The simplicity of this formulation allows to perform a detailed analysis of its properties. Among others, we prove that the filter behaves asymptotically as a shock filter combined with a border diffusive term, responsible for the staircaising effect and the loss of contrast. version:2
arxiv-1403-1252 | Inducing Language Networks from Continuous Space Word Representations | http://arxiv.org/abs/1403.1252 | id:1403.1252 author:Bryan Perozzi, Rami Al-Rfou, Vivek Kulkarni, Steven Skiena category:cs.LG cs.CL cs.SI  published:2014-03-06 summary:Recent advancements in unsupervised feature learning have developed powerful latent representations of words. However, it is still not clear what makes one representation better than another and how we can learn the ideal representation. Understanding the structure of latent spaces attained is key to any future advancement in unsupervised learning. In this work, we introduce a new view of continuous space word representations as language networks. We explore two techniques to create language networks from learned features by inducing them for two popular word representation methods and examining the properties of their resulting networks. We find that the induced networks differ from other methods of creating language networks, and that they contain meaningful community structure. version:2
arxiv-1307-1662 | Polyglot: Distributed Word Representations for Multilingual NLP | http://arxiv.org/abs/1307.1662 | id:1307.1662 author:Rami Al-Rfou, Bryan Perozzi, Steven Skiena category:cs.CL cs.LG  published:2013-07-05 summary:Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications. version:2
arxiv-1403-6652 | DeepWalk: Online Learning of Social Representations | http://arxiv.org/abs/1403.6652 | id:1403.6652 author:Bryan Perozzi, Rami Al-Rfou, Steven Skiena category:cs.SI cs.LG H.2.8; I.2.6; I.5.1  published:2014-03-26 summary:We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide $F_1$ scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection. version:2
arxiv-1402-4078 | Performance Limits of Dictionary Learning for Sparse Coding | http://arxiv.org/abs/1402.4078 | id:1402.4078 author:Alexander Jung, Yonina C. Eldar, Norbert Görtz category:stat.ML  published:2014-02-17 summary:We consider the problem of dictionary learning under the assumption that the observed signals can be represented as sparse linear combinations of the columns of a single large dictionary matrix. In particular, we analyze the minimax risk of the dictionary learning problem which governs the mean squared error (MSE) performance of any learning scheme, regardless of its computational complexity. By following an established information-theoretic method based on Fanos inequality, we derive a lower bound on the minimax risk for a given dictionary learning problem. This lower bound yields a characterization of the sample-complexity, i.e., a lower bound on the required number of observations such that consistent dictionary learning schemes exist. Our bounds may be compared with the performance of a given learning scheme, allowing to characterize how far the method is from optimal performance. version:2
arxiv-1406-7179 | Optimal Population Codes for Control and Estimation | http://arxiv.org/abs/1406.7179 | id:1406.7179 author:Alex Susemihl, Ron Meir, Manfred Opper category:stat.ML cs.IT math.IT q-bio.NC  published:2014-06-27 summary:Agents acting in the natural world aim at selecting appropriate actions based on noisy and partial sensory observations. Many behaviors leading to decision mak- ing and action selection in a closed loop setting are naturally phrased within a control theoretic framework. Within the framework of optimal Control Theory, one is usually given a cost function which is minimized by selecting a control law based on the observations. While in standard control settings the sensors are assumed fixed, biological systems often gain from the extra flexibility of optimiz- ing the sensors themselves. However, this sensory adaptation is geared towards control rather than perception, as is often assumed. In this work we show that sen- sory adaptation for control differs from sensory adaptation for perception, even for simple control setups. This implies, consistently with recent experimental results, that when studying sensory adaptation, it is essential to account for the task being performed. version:1
arxiv-1406-6946 | An improved computer vision method for detecting white blood cells | http://arxiv.org/abs/1406.6946 | id:1406.6946 author:Erik Cuevas, Margarita Diaz, Miguel Manzanares, Daniel Zaldivar, Marco Perez category:cs.CV  published:2014-06-26 summary:The automatic detection of White Blood Cells (WBC) still remains as an unsolved issue in medical imaging. The analysis of WBC images has engaged researchers from fields of medicine and computer vision alike. Since WBC can be approximated by an ellipsoid form, an ellipse detector algorithm may be successfully applied in order to recognize them. This paper presents an algorithm for the automatic detection of WBC embedded into complicated and cluttered smear images that considers the complete process as a multi-ellipse detection problem. The approach, based on the Differential Evolution (DE) algorithm, transforms the detection task into an optimization problem where individuals emulate candidate ellipses. An objective function evaluates if such candidate ellipses are really present in the edge image of the smear. Guided by the values of such function, the set of encoded candidate ellipses (individuals) are evolved using the DE algorithm so that they can fit into the WBC enclosed within the edge-only map of the image. Experimental results from white blood cell images with a varying range of complexity are included to validate the efficiency of the proposed technique in terms of accuracy and robustness. version:2
arxiv-1406-7128 | On a new formulation of nonlocal image filters involving the relative rearrangement | http://arxiv.org/abs/1406.7128 | id:1406.7128 author:Gonzalo Galiano, Julián Velasco category:cs.CV 68U10  published:2014-06-27 summary:Nonlocal filters are simple and powerful techniques for image denoising. In this paper we study the reformulation of a broad class of nonlocal filters in terms of two functional rearrangements: the decreasing and the relative rearrangements. Independently of the dimension of the image, we reformulate these filters as integral operators defined in a one-dimensional space corresponding to the level sets measures. We prove the equivalency between the original and the rearranged versions of the filters and propose a discretization in terms of constant-wise interpolators, which we prove to be convergent to the solution of the continuous setting. For some particular cases, this new formulation allows us to perform a detailed analysis of the filtering properties. Among others, we prove that the filtered image is a contrast change of the original image, and that the filtering procedure behaves asymptotically as a shock filter combined with a border diffusive term, responsible for the staircaising effect and the loss of contrast. version:1
arxiv-1406-7120 | Template Matching based Object Detection Using HOG Feature Pyramid | http://arxiv.org/abs/1406.7120 | id:1406.7120 author:Anish Acharya category:cs.CV  published:2014-06-27 summary:This article provides a step by step development of designing a Object Detection scheme using the HOG based Feature Pyramid aligned with the concept of Template Matching. version:1
arxiv-1406-7112 | 3D planar patch extraction from stereo using probabilistic region growing | http://arxiv.org/abs/1406.7112 | id:1406.7112 author:Vasileios Zografos category:cs.CV  published:2014-06-27 summary:This article presents a novel 3D planar patch extraction method using a probabilistic region growing algorithm. Our method works by simultaneously initiating multiple planar patches from seed points, the latter determined by an intensity-based 2D segmentation algorithm in the stereo-pair images. The patches are grown incrementally and in parallel as 3D scene points are considered for membership, using a probabilistic distance likelihood measure. In addition, we have incorporated prior information based on the noise model in the 2D images and the scene configuration but also include the intensity information resulting from the initial segmentation. This method works well across many different data-sets, involving real and synthetic examples of both regularly and non-regularly sampled data, and is fast enough that may be used for robot navigation tasks of path detection and obstacle avoidance. version:1
arxiv-1406-7075 | Adaptive texture energy measure method | http://arxiv.org/abs/1406.7075 | id:1406.7075 author:Omer Faruk Ertugrul category:cs.CV  published:2014-06-27 summary:Recent developments in image quality, data storage, and computational capacity have heightened the need for texture analysis in image process. To date various methods have been developed and introduced for assessing textures in images. One of the most popular texture analysis methods is the Texture Energy Measure (TEM) and it has been used for detecting edges, levels, waves, spots and ripples by employing predefined TEM masks to images. Despite several success- ful studies, TEM has a number of serious weaknesses in use. The major drawback is; the masks are predefined therefore they cannot be adapted to image. A new method, Adaptive Texture Energy Measure Method (aTEM), was offered to over- come this disadvantage of TEM by using adaptive masks by adjusting the contrast, sharpening and orientation angle of the mask. To assess the applicability of aTEM, it is compared with TEM. The accuracy of the classification of butterfly, flower seed and Brodatz datasets are 0.08, 0.3292 and 0.3343, respectively by TEM and 0.0053, 0.2417 and 0.3153, respectively by aTEM. The results of this study indicate that aTEM is a successful method for texture analysis. version:1
arxiv-1406-7062 | Adaptive Mesh Representation and Restoration of Biomedical Images | http://arxiv.org/abs/1406.7062 | id:1406.7062 author:Ke Liu, Ming Xu, Zeyun Yu category:cs.CV  published:2014-06-27 summary:The triangulation of images has become an active research area in recent years for its compressive representation and ease of image processing and visualization. However, little work has been done on how to faithfully recover image intensities from a triangulated mesh of an image, a process also known as image restoration or decoding from meshes. The existing methods such as linear interpolation, least-square interpolation, or interpolation based on radial basis functions (RBFs) work to some extent, but often yield blurred features (edges, corners, etc.). The main reason for this problem is due to the isotropically-defined Euclidean distance that is taken into consideration in these methods, without considering the anisotropicity of feature intensities in an image. Moreover, most existing methods use intensities defined at mesh nodes whose intensities are often ambiguously defined on or near image edges (or feature boundaries). In the current paper, a new method of restoring an image from its triangulation representation is proposed, by utilizing anisotropic radial basis functions (ARBFs). This method considers not only the geometrical (Euclidean) distances but also the local feature orientations (anisotropic intensities). Additionally, this method is based on the intensities of mesh faces instead of mesh nodes and thus provides a more robust restoration. The two strategies together guarantee excellent feature-preserving restoration of an image with arbitrary super-resolutions from its triangulation representation, as demonstrated by various experiments provided in the paper. version:1
arxiv-1408-0016 | Architecture of a Web-based Predictive Editor for Controlled Natural Language Processing | http://arxiv.org/abs/1408.0016 | id:1408.0016 author:Stephen Guy, Rolf Schwitter category:cs.CL cs.AI  published:2014-06-27 summary:In this paper, we describe the architecture of a web-based predictive text editor being developed for the controlled natural language PENG$^{ASP)$. This controlled language can be used to write non-monotonic specifications that have the same expressive power as Answer Set Programs. In order to support the writing process of these specifications, the predictive text editor communicates asynchronously with the controlled natural language processor that generates lookahead categories and additional auxiliary information for the author of a specification text. The text editor can display multiple sets of lookahead categories simultaneously for different possible sentence completions, anaphoric expressions, and supports the addition of new content words to the lexicon. version:1
arxiv-1304-7981 | Generalized Canonical Correlation Analysis for Classification | http://arxiv.org/abs/1304.7981 | id:1304.7981 author:Cencheng Shen, Ming Sun, Minh Tang, Carey E. Priebe category:stat.ML  published:2013-04-30 summary:For multiple multivariate data sets, we derive conditions under which Generalized Canonical Correlation Analysis (GCCA) improves classification performance of the projected datasets, compared to standard Canonical Correlation Analysis (CCA) using only two data sets. We illustrate our theoretical results with simulations and a real data experiment. version:5
arxiv-1406-6947 | Deep Learning Multi-View Representation for Face Recognition | http://arxiv.org/abs/1406.6947 | id:1406.6947 author:Zhenyao Zhu, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-06-26 summary:Various factors, such as identities, views (poses), and illuminations, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of human brain. Intriguingly, even without accessing 3D data, human not only can recognize face identity, but can also imagine face images of a person under different viewpoints given a single 2D image, making face perception in the brain robust to view changes. In this sense, human brain has learned and encoded 3D face models from 2D images. To take into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and infer a full spectrum of multi-view images in the meanwhile, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data. version:1
arxiv-1406-6897 | Edge Label Inference in Generalized Stochastic Block Models: from Spectral Theory to Impossibility Results | http://arxiv.org/abs/1406.6897 | id:1406.6897 author:Jiaming Xu, Laurent Massoulié, Marc Lelarge category:math.ST stat.ML stat.TH  published:2014-06-26 summary:The classical setting of community detection consists of networks exhibiting a clustered structure. To more accurately model real systems we consider a class of networks (i) whose edges may carry labels and (ii) which may lack a clustered structure. Specifically we assume that nodes possess latent attributes drawn from a general compact space and edges between two nodes are randomly generated and labeled according to some unknown distribution as a function of their latent attributes. Our goal is then to infer the edge label distributions from a partially observed network. We propose a computationally efficient spectral algorithm and show it allows for asymptotically correct inference when the average node degree could be as low as logarithmic in the total number of nodes. Conversely, if the average node degree is below a specific constant threshold, we show that no algorithm can achieve better inference than guessing without using the observations. As a byproduct of our analysis, we show that our model provides a general procedure to construct random graph models with a spectrum asymptotic to a pre-specified eigenvalue distribution such as a power-law distribution. version:1
arxiv-1312-4967 | Estimation of Human Body Shape and Posture Under Clothing | http://arxiv.org/abs/1312.4967 | id:1312.4967 author:Stefanie Wuhrer, Leonid Pishchulin, Alan Brunton, Chang Shu, Jochen Lang category:cs.CV cs.GR  published:2013-12-17 summary:Estimating the body shape and posture of a dressed human subject in motion represented as a sequence of (possibly incomplete) 3D meshes is important for virtual change rooms and security. To solve this problem, statistical shape spaces encoding human body shape and posture variations are commonly used to constrain the search space for the shape estimate. In this work, we propose a novel method that uses a posture-invariant shape space to model body shape variation combined with a skeleton-based deformation to model posture variation. Our method can estimate the body shape and posture of both static scans and motion sequences of dressed human body scans. In case of motion sequences, our method takes advantage of motion cues to solve for a single body shape estimate along with a sequence of posture estimates. We apply our approach to both static scans and motion sequences and demonstrate that using our method, higher fitting accuracy is achieved than when using a variant of the popular SCAPE model as statistical model. version:2
arxiv-1406-6854 | A Fully Automated Latent Fingerprint Matcher with Embedded Self-learning Segmentation Module | http://arxiv.org/abs/1406.6854 | id:1406.6854 author:Jinwei Xu, Jiankun Hu, Xiuping Jia category:cs.CR cs.CV  published:2014-06-26 summary:Latent fingerprint has the practical value to identify the suspects who have unintentionally left a trace of fingerprint in the crime scenes. However, designing a fully automated latent fingerprint matcher is a very challenging task as it needs to address many challenging issues including the separation of overlapping structured patterns over the partial and poor quality latent fingerprint image, and finding a match against a large background database that would have different resolutions. Currently there is no fully automated latent fingerprint matcher available to the public and most literature reports have utilized a specialized latent fingerprint matcher COTS3 which is not accessible to the public. This will make it infeasible to assess and compare the relevant research work which is vital for this research community. In this study, we target to develop a fully automated latent matcher for adaptive detection of the region of interest and robust matching of latent prints. Unlike the manually conducted matching procedure, the proposed latent matcher can run like a sealed black box without any manual intervention. This matcher consists of the following two modules: (i) the dictionary learning-based region of interest (ROI) segmentation scheme; and (ii) the genetic algorithm-based minutiae set matching unit. Experimental results on NIST SD27 latent fingerprint database demonstrates that the proposed matcher outperforms the currently public state-of-art latent fingerprint matcher. version:1
arxiv-1406-6844 | FrameNet Resource Grammar Library for GF | http://arxiv.org/abs/1406.6844 | id:1406.6844 author:Normunds Gruzitis, Peteris Paikens, Guntis Barzdins category:cs.CL  published:2014-06-26 summary:In this paper we present an ongoing research investigating the possibility and potential of integrating frame semantics, particularly FrameNet, in the Grammatical Framework (GF) application grammar development. An important component of GF is its Resource Grammar Library (RGL) that encapsulates the low-level linguistic knowledge about morphology and syntax of currently more than 20 languages facilitating rapid development of multilingual applications. In the ideal case, porting a GF application grammar to a new language would only require introducing the domain lexicon - translation equivalents that are interlinked via common abstract terms. While it is possible for a highly restricted CNL, developing and porting a less restricted CNL requires above average linguistic knowledge about the particular language, and above average GF experience. Specifying a lexicon is mostly straightforward in the case of nouns (incl. multi-word units), however, verbs are the most complex category (in terms of both inflectional paradigms and argument structure), and adding them to a GF application grammar is not a straightforward task. In this paper we are focusing on verbs, investigating the possibility of creating a multilingual FrameNet-based GF library. We propose an extension to the current RGL, allowing GF application developers to define clauses on the semantic level, thus leaving the language-specific syntactic mapping to this extension. We demonstrate our approach by reengineering the MOLTO Phrasebook application grammar. version:1
arxiv-1406-6832 | Overlapping Community Detection Optimization and Nash Equilibrium | http://arxiv.org/abs/1406.6832 | id:1406.6832 author:Michel Crampes, Michel Plantié category:cs.SI physics.soc-ph stat.ML  published:2014-06-26 summary:Community detection using both graphs and social networks is the focus of many algorithms. Recent methods aimed at optimizing the so-called modularity function proceed by maximizing relations within communities while minimizing inter-community relations. However, given the NP-completeness of the problem, these algorithms are heuristics that do not guarantee an optimum. In this paper, we introduce a new algorithm along with a function that takes an approximate solution and modifies it in order to reach an optimum. This reassignment function is considered a 'potential function' and becomes a necessary condition to asserting that the computed optimum is indeed a Nash Equilibrium. We also use this function to simultaneously show partitioning and overlapping communities, two detection and visualization modes of great value in revealing interesting features of a social network. Our approach is successfully illustrated through several experiments on either real unipartite, multipartite or directed graphs of medium and large-sized datasets. version:1
arxiv-1406-6812 | Online learning in MDPs with side information | http://arxiv.org/abs/1406.6812 | id:1406.6812 author:Yasin Abbasi-Yadkori, Gergely Neu category:cs.LG stat.ML  published:2014-06-26 summary:We study online learning of finite Markov decision process (MDP) problems when a side information vector is available. The problem is motivated by applications such as clinical trials, recommendation systems, etc. Such applications have an episodic structure, where each episode corresponds to a patient/customer. Our objective is to compete with the optimal dynamic policy that can take side information into account. We propose a computationally efficient algorithm and show that its regret is at most $O(\sqrt{T})$, where $T$ is the number of rounds. To best of our knowledge, this is the first regret bound for this setting. version:1
arxiv-1406-6720 | Mass-Univariate Hypothesis Testing on MEEG Data using Cross-Validation | http://arxiv.org/abs/1406.6720 | id:1406.6720 author:Seyed Mostafa Kia category:stat.ML cs.LG math.ST stat.TH  published:2014-06-25 summary:Recent advances in statistical theory, together with advances in the computational power of computers, provide alternative methods to do mass-univariate hypothesis testing in which a large number of univariate tests, can be properly used to compare MEEG data at a large number of time-frequency points and scalp locations. One of the major problematic aspects of this kind of mass-univariate analysis is due to high number of accomplished hypothesis tests. Hence procedures that remove or alleviate the increased probability of false discoveries are crucial for this type of analysis. Here, I propose a new method for mass-univariate analysis of MEEG data based on cross-validation scheme. In this method, I suggest a hierarchical classification procedure under k-fold cross-validation to detect which sensors at which time-bin and which frequency-bin contributes in discriminating between two different stimuli or tasks. To achieve this goal, a new feature extraction method based on the discrete cosine transform (DCT) employed to get maximum advantage of all three data dimensions. Employing cross-validation and hierarchy architecture alongside the DCT feature space makes this method more reliable and at the same time enough sensitive to detect the narrow effects in brain activities. version:1
arxiv-1406-1509 | Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in the Othello League | http://arxiv.org/abs/1406.1509 | id:1406.1509 author:Wojciech Jaśkowski category:cs.NE cs.AI cs.LG 68T05  published:2014-06-05 summary:N-tuple networks have been successfully used as position evaluation functions for board games such as Othello or Connect Four. The effectiveness of such networks depends on their architecture, which is determined by the placement of constituent n-tuples, sequences of board locations, providing input to the network. The most popular method of placing n-tuples consists in randomly generating a small number of long, snake-shaped board location sequences. In comparison, we show that learning n-tuple networks is significantly more effective if they involve a large number of systematically placed, short, straight n-tuples. Moreover, we demonstrate that in order to obtain the best performance and the steepest learning curve for Othello it is enough to use n-tuples of size just 2, yielding a network consisting of only 288 weights. The best such network evolved in this study has been evaluated in the online Othello League, obtaining the performance of nearly 96% --- more than any other player to date. version:3
arxiv-1406-6670 | Learning the ergodic decomposition | http://arxiv.org/abs/1406.6670 | id:1406.6670 author:Nabil Al-Najjar, Eran Shmaya category:math.ST math.PR stat.ML stat.TH  published:2014-06-25 summary:A Bayesian agent learns about the structure of a stationary process from ob- serving past outcomes. We prove that his predictions about the near future become ap- proximately those he would have made if he knew the long run empirical frequencies of the process. version:1
arxiv-1406-6651 | Causality Networks | http://arxiv.org/abs/1406.6651 | id:1406.6651 author:Ishanu Chattopadhyay category:cs.LG cs.IT math.IT q-fin.ST stat.ML 68Q32  published:2014-06-25 summary:While correlation measures are used to discern statistical relationships between observed variables in almost all branches of data-driven scientific inquiry, what we are really interested in is the existence of causal dependence. Designing an efficient causality test, that may be carried out in the absence of restrictive pre-suppositions on the underlying dynamical structure of the data at hand, is non-trivial. Nevertheless, ability to computationally infer statistical prima facie evidence of causal dependence may yield a far more discriminative tool for data analysis compared to the calculation of simple correlations. In the present work, we present a new non-parametric test of Granger causality for quantized or symbolic data streams generated by ergodic stationary sources. In contrast to state-of-art binary tests, our approach makes precise and computes the degree of causal dependence between data streams, without making any restrictive assumptions, linearity or otherwise. Additionally, without any a priori imposition of specific dynamical structure, we infer explicit generative models of causal cross-dependence, which may be then used for prediction. These explicit models are represented as generalized probabilistic automata, referred to crossed automata, and are shown to be sufficient to capture a fairly general class of causal dependence. The proposed algorithms are computationally efficient in the PAC sense; $i.e.$, we find good models of cross-dependence with high probability, with polynomial run-times and sample complexities. The theoretical results are applied to weekly search-frequency data from Google Trends API for a chosen set of socially "charged" keywords. The causality network inferred from this dataset reveals, quite expectedly, the causal importance of certain keywords. It is also illustrated that correlation analysis fails to gather such insight. version:1
arxiv-1406-6633 | Active Learning and Best-Response Dynamics | http://arxiv.org/abs/1406.6633 | id:1406.6633 author:Maria-Florina Balcan, Chris Berlind, Avrim Blum, Emma Cohen, Kaushik Patnaik, Le Song category:cs.LG cs.GT  published:2014-06-25 summary:We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising. version:1
arxiv-1406-6618 | When is it Better to Compare than to Score? | http://arxiv.org/abs/1406.6618 | id:1406.6618 author:Nihar B. Shah, Sivaraman Balakrishnan, Joseph Bradley, Abhay Parekh, Kannan Ramchandran, Martin Wainwright category:stat.ML cs.LG  published:2014-06-25 summary:When eliciting judgements from humans for an unknown quantity, one often has the choice of making direct-scoring (cardinal) or comparative (ordinal) measurements. In this paper we study the relative merits of either choice, providing empirical and theoretical guidelines for the selection of a measurement scheme. We provide empirical evidence based on experiments on Amazon Mechanical Turk that in a variety of tasks, (pairwise-comparative) ordinal measurements have lower per sample noise and are typically faster to elicit than cardinal ones. Ordinal measurements however typically provide less information. We then consider the popular Thurstone and Bradley-Terry-Luce (BTL) models for ordinal measurements and characterize the minimax error rates for estimating the unknown quantity. We compare these minimax error rates to those under cardinal measurement models and quantify for what noise levels ordinal measurements are better. Finally, we revisit the data collected from our experiments and show that fitting these models confirms this prediction: for tasks where the noise in ordinal measurements is sufficiently low, the ordinal approach results in smaller errors in the estimation. version:1
arxiv-1310-0322 | Optical Flow on Evolving Surfaces with Space and Time Regularisation | http://arxiv.org/abs/1310.0322 | id:1310.0322 author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV  published:2013-10-01 summary:We extend the concept of optical flow with spatiotemporal regularisation to a dynamic non-Euclidean setting. Optical flow is traditionally computed from a sequence of flat images. The purpose of this paper is to introduce variational motion estimation for images that are defined on an evolving surface. Volumetric microscopy images depicting a live zebrafish embryo serve as both biological motivation and test data. version:2
arxiv-1406-6595 | 3DUNDERWORLD-SLS: An Open-Source Structured-Light Scanning System for Rapid Geometry Acquisition | http://arxiv.org/abs/1406.6595 | id:1406.6595 author:Kyriakos Herakleous, Charalambos Poullis category:cs.CV  published:2014-06-25 summary:Recently, there has been an increase in the demand of virtual 3D objects representing real-life objects. A plethora of methods and systems have already been proposed for the acquisition of the geometry of real-life objects ranging from those which employ active sensor technology, passive sensor technology or a combination of various techniques. In this paper we present the development of a 3D scanning system which is based on the principle of structured-light, without having particular requirements for specialized equipment. We discuss the intrinsic details and inherent difficulties of structured-light scanning techniques and present our solutions. Finally, we introduce our open-source scanning system "3DUNDERWORLD-SLS" which implements the proposed techniques. We have performed extensive testing with a wide range of models and report the results. Furthermore, we present a comprehensive evaluation of the system and a comparison with a high-end commercial 3D scanner. version:1
arxiv-1406-6568 | Support vector machine classification of dimensionally reduced structural MRI images for dementia | http://arxiv.org/abs/1406.6568 | id:1406.6568 author:V. A. Miller, S. Erlien, J. Piersol category:cs.CV cs.LG physics.med-ph  published:2014-06-25 summary:We classify very-mild to moderate dementia in patients (CDR ranging from 0 to 2) using a support vector machine classifier acting on dimensionally reduced feature set derived from MRI brain scans of the 416 subjects available in the OASIS-Brains dataset. We use image segmentation and principal component analysis to reduce the dimensionality of the data. Our resulting feature set contains 11 features for each subject. Performance of the classifiers is evaluated using 10-fold cross-validation. Using linear and (gaussian) kernels, we obtain a training classification accuracy of 86.4% (90.1%), test accuracy of 85.0% (85.7%), test precision of 68.7% (68.5%), test recall of 68.0% (74.0%), and test Matthews correlation coefficient of 0.594 (0.616). version:1
arxiv-1403-4467 | A hybrid formalism to parse Sign Languages | http://arxiv.org/abs/1403.4467 | id:1403.4467 author:Rémi Dubot, Christophe Collet category:cs.CL  published:2014-03-18 summary:Sign Language (SL) linguistic is dependent on the expensive task of annotating. Some automation is already available for low-level information (eg. body part tracking) and the lexical level has shown significant progresses. The syntactic level lacks annotated corpora as well as complete and consistent models. This article presents a solution for the automatic annotation of SL syntactic elements. It exposes a formalism able to represent both constituency-based and dependency-based models. The first enable the representation the structures one may want to annotate, the second aims at fulfilling the holes of the first. A parser is presented and used to conduct two experiments on the solution. One experiment is on a real corpus, the other is on a synthetic corpus. version:2
arxiv-1406-6560 | Multi Circle Detection on Images Using Artificial Bee Colony (ABC) Optimization | http://arxiv.org/abs/1406.6560 | id:1406.6560 author:Erik Cuevas, Felipe Sencion-Echauri, Daniel Zaldivar, Marco Perez Cisneros category:cs.CV cs.NE  published:2014-06-25 summary:Hough transform (HT) has been the most common method for circle detection, exhibiting robustness, but adversely demanding considerable computational effort and large memory requirements. Alternative approaches include heuristic methods that employ iterative optimization procedures for detecting multiple circles. Since only one circle can be marked at each optimization cycle, multiple executions must be enforced in order to achieve multi detection. This paper presents an algorithm for automatic detection of multiple circular shapes that considers the overall process as a multi-modal optimization problem. The approach is based on the artificial bee colony (ABC) algorithm, a swarm optimization algorithm inspired by the intelligent foraging behavior of honey bees. Unlike the original ABC algorithm, the proposed approach presents the addition of a memory for discarded solutions. Such memory allows holding important information regarding other local optima which might have emerged during the optimization process. The detector uses a combination of three non-collinear edge points as parameters to determine circle candidates. A matching function (nectar- amount) determines if such circle candidates (bee-food-sources) are actually present in the image. Guided by the values of such matching functions, the set of encoded candidate circles are evolved through the ABC algorithm so that the best candidate (global optimum) can be fitted into an actual circle within the edge only image. Then, an analysis of the incorporated memory is executed in order to identify potential local optima, i.e., other circles. version:1
arxiv-1406-6538 | A Bimodal Co-Sparse Analysis Model for Image Processing | http://arxiv.org/abs/1406.6538 | id:1406.6538 author:Martin Kiechle, Tim Habigt, Simon Hawe, Martin Kleinsteuber category:cs.CV  published:2014-06-25 summary:The success of many computer vision tasks lies in the ability to exploit the interdependency between different image modalities such as intensity and depth. Fusing corresponding information can be achieved on several levels, and one promising approach is the integration at a low level. Moreover, sparse signal models have successfully been used in many vision applications. Within this area of research, the so called co-sparse analysis model has attracted considerably less attention than its well-known counterpart, the sparse synthesis model, although it has been proven to be very useful in various image processing applications. In this paper, we propose a co-sparse analysis model that is able to capture the interdependency of two image modalities. It is based on the assumption that a pair of analysis operators exists, so that the co-supports of the corresponding bimodal image structures are correlated. We propose an algorithm that is able to learn such a coupled pair of operators from registered and noise-free training data. Furthermore, we explain how this model can be applied to solve linear inverse problems in image processing and how it can be used for image registration tasks. This paper extends the work of some of the authors by two major contributions. Firstly, a modification of the learning process is proposed that a priori guarantees unit norm and zero-mean of the rows of the operator. This accounts for the intuition that contrast in image modalities carries the most information. Secondly, the model is used in a novel bimodal image registration algorithm which estimates the transformation parameters of unregistered images of different modalities. version:1
arxiv-1406-6507 | Weakly-supervised Discovery of Visual Pattern Configurations | http://arxiv.org/abs/1406.6507 | id:1406.6507 author:Hyun Oh Song, Yong Jae Lee, Stefanie Jegelka, Trevor Darrell category:cs.CV cs.LG  published:2014-06-25 summary:The increasing prominence of weakly labeled data nurtures a growing demand for object detection methods that can cope with minimal supervision. We propose an approach that automatically identifies discriminative configurations of visual patterns that are characteristic of a given object class. We formulate the problem as a constrained submodular optimization problem and demonstrate the benefits of the discovered configurations in remedying mislocalizations and finding informative positive and negative training examples. Together, these lead to state-of-the-art weakly-supervised detection results on the challenging PASCAL VOC dataset. version:1
arxiv-1406-6453 | A Quantitative Neural Coding Model of Sensory Memory | http://arxiv.org/abs/1406.6453 | id:1406.6453 author:Peilei Liu, Ting Wang category:cs.NE q-bio.NC  published:2014-06-25 summary:The coding mechanism of sensory memory on the neuron scale is one of the most important questions in neuroscience. We have put forward a quantitative neural network model, which is self organized, self similar, and self adaptive, just like an ecosystem following Darwin theory. According to this model, neural coding is a mult to one mapping from objects to neurons. And the whole cerebrum is a real-time statistical Turing Machine, with powerful representing and learning ability. This model can reconcile some important disputations, such as: temporal coding versus rate based coding, grandmother cell versus population coding, and decay theory versus interference theory. And it has also provided explanations for some key questions such as memory consolidation, episodic memory, consciousness, and sentiment. Philosophical significance is indicated at last. version:1
arxiv-1406-6398 | Incremental Clustering: The Case for Extra Clusters | http://arxiv.org/abs/1406.6398 | id:1406.6398 author:Margareta Ackerman, Sanjoy Dasgupta category:cs.LG  published:2014-06-24 summary:The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters. version:1
arxiv-1406-6390 | Image patch analysis and clustering of sunspots: a dimensionality reduction approach | http://arxiv.org/abs/1406.6390 | id:1406.6390 author:Kevin R. Moon, Jimmy J. Li, Veronique Delouille, Fraser Watson, Alfred O. Hero III category:cs.CV astro-ph.SR  published:2014-06-24 summary:Sunspots, as seen in white light or continuum images, are associated with regions of high magnetic activity on the Sun, visible on magnetogram images. Their complexity is correlated with explosive solar activity and so classifying these active regions is useful for predicting future solar activity. Current classification of sunspot groups is visually based and suffers from bias. Supervised learning methods can reduce human bias but fail to optimally capitalize on the information present in sunspot images. This paper uses two image modalities (continuum and magnetogram) to characterize the spatial and modal interactions of sunspot and magnetic active region images and presents a new approach to cluster the images. Specifically, in the framework of image patch analysis, we estimate the number of intrinsic parameters required to describe the spatial and modal dependencies, the correlation between the two modalities and the corresponding spatial patterns, and examine the phenomena at different scales within the images. To do this, we use linear and nonlinear intrinsic dimension estimators, canonical correlation analysis, and multiresolution analysis of intrinsic dimension. version:1
arxiv-1406-6336 | A multilevel thresholding algorithm using Electromagnetism Optimization | http://arxiv.org/abs/1406.6336 | id:1406.6336 author:Diego Oliva, Erik Cuevas, Gonzalo Pajares, Daniel Zaldivar, Valentin Osuna category:cs.CV  published:2014-06-24 summary:Segmentation is one of the most important tasks in image processing. It consist in classify the pixels into two or more groups depending on their intensity levels and a threshold value. The quality of the segmentation depends on the method applied to select the threshold. The use of the classical implementations for multilevel thresholding is computationally expensive since they exhaustively search the best values to optimize the objective function. Under such conditions, the use of optimization evolutionary approaches has been extended. The Electromagnetism Like algorithm (EMO) is an evolutionary method which mimics the attraction repulsion mechanism among charges to evolve the members of a population. Different to other algorithms, EMO exhibits interesting search capabilities whereas maintains a low computational overhead. In this paper, a multilevel thresholding (MT) algorithm based on the EMO is introduced. The approach combines the good search capabilities of EMO algorithm with objective functions proposed by the popular MT methods of Otsu and Kapur. The algorithm takes random samples from a feasible search space inside the image histogram. Such samples build each particle in the EMO context whereas its quality is evaluated considering the objective that is function employed by the Otsu or Kapur method. Guided by these objective values the set of candidate solutions are evolved through the EMO operators until an optimal solution is found. The approach generates a multilevel segmentation algorithm which can effectively identify the threshold values of a digital image in a reduced number of iterations. Experimental results show performance evidence of the implementation of EMO for digital image segmentation. version:1
arxiv-1406-6323 | Dense Correspondences Across Scenes and Scales | http://arxiv.org/abs/1406.6323 | id:1406.6323 author:Moria Tau, Tal Hassner category:cs.CV  published:2014-06-24 summary:We seek a practical method for establishing dense correspondences between two images with similar content, but possibly different 3D scenes. One of the challenges in designing such a system is the local scale differences of objects appearing in the two images. Previous methods often considered only small subsets of image pixels; matching only pixels for which stable scales may be reliably estimated. More recently, others have considered dense correspondences, but with substantial costs associated with generating, storing and matching scale invariant descriptors. Our work here is motivated by the observation that pixels in the image have contexts -- the pixels around them -- which may be exploited in order to estimate local scales reliably and repeatably. Specifically, we make the following contributions. (i) We show that scales estimated in sparse interest points may be propagated to neighboring pixels where this information cannot be reliably determined. Doing so allows scale invariant descriptors to be extracted anywhere in the image, not just in detected interest points. (ii) We present three different means for propagating this information: using only the scales at detected interest points, using the underlying image information to guide the propagation of this information across each image, separately, and using both images simultaneously. Finally, (iii), we provide extensive results, both qualitative and quantitative, demonstrating that accurate dense correspondences can be obtained even between very different images, with little computational costs beyond those required by existing methods. version:1
arxiv-1406-6291 | Studying Collective Human Decision Making and Creativity with Evolutionary Computation | http://arxiv.org/abs/1406.6291 | id:1406.6291 author:Hiroki Sayama, Shelley D. Dionne category:cs.NE cs.MA nlin.AO  published:2014-06-24 summary:We report a summary of our interdisciplinary research project "Evolutionary Perspective on Collective Decision Making" that was conducted through close collaboration between computational, organizational and social scientists at Binghamton University. We redefined collective human decision making and creativity as evolution of ecologies of ideas, where populations of ideas evolve via continual applications of evolutionary operators such as reproduction, recombination, mutation, selection, and migration of ideas, each conducted by participating humans. Based on this evolutionary perspective, we generated hypotheses about collective human decision making using agent-based computer simulations. The hypotheses were then tested through several experiments with real human subjects. Throughout this project, we utilized evolutionary computation (EC) in non-traditional ways---(1) as a theoretical framework for reinterpreting the dynamics of idea generation and selection, (2) as a computational simulation model of collective human decision making processes, and (3) as a research tool for collecting high-resolution experimental data of actual collaborative design and decision making from human subjects. We believe our work demonstrates untapped potential of EC for interdisciplinary research involving human and social dynamics. version:1
arxiv-1406-6273 | Image Completion for View Synthesis Using Markov Random Fields and Efficient Belief Propagation | http://arxiv.org/abs/1406.6273 | id:1406.6273 author:Julian Habigt, Klaus Diepold category:cs.CV  published:2014-06-24 summary:View synthesis is a process for generating novel views from a scene which has been recorded with a 3-D camera setup. It has important applications in 3-D post-production and 2-D to 3-D conversion. However, a central problem in the generation of novel views lies in the handling of disocclusions. Background content, which was occluded in the original view, may become unveiled in the synthesized view. This leads to missing information in the generated view which has to be filled in a visually plausible manner. We present an inpainting algorithm for disocclusion filling in synthesized views based on Markov random fields and efficient belief propagation. We compare the result to two state-of-the-art algorithms and demonstrate a significant improvement in image quality. version:1
arxiv-1307-3949 | On Soft Power Diagrams | http://arxiv.org/abs/1307.3949 | id:1307.3949 author:Steffen Borgwardt category:cs.LG math.OC stat.ML 90C90  90C46  68Q32  published:2013-07-15 summary:Many applications in data analysis begin with a set of points in a Euclidean space that is partitioned into clusters. Common tasks then are to devise a classifier deciding which of the clusters a new point is associated to, finding outliers with respect to the clusters, or identifying the type of clustering used for the partition. One of the common kinds of clusterings are (balanced) least-squares assignments with respect to a given set of sites. For these, there is a 'separating power diagram' for which each cluster lies in its own cell. In the present paper, we aim for efficient algorithms for outlier detection and the computation of thresholds that measure how similar a clustering is to a least-squares assignment for fixed sites. For this purpose, we devise a new model for the computation of a 'soft power diagram', which allows a soft separation of the clusters with 'point counting properties'; e.g. we are able to prescribe how many points we want to classify as outliers. As our results hold for a more general non-convex model of free sites, we describe it and our proofs in this more general way. Its locally optimal solutions satisfy the aforementioned point counting properties. For our target applications that use fixed sites, our algorithms are efficiently solvable to global optimality by linear programming. version:2
arxiv-1406-6247 | Recurrent Models of Visual Attention | http://arxiv.org/abs/1406.6247 | id:1406.6247 author:Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu category:cs.LG cs.CV stat.ML  published:2014-06-24 summary:Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so. version:1
arxiv-1406-6201 | Saccadic Eye Movements and the Generalized Pareto Distribution | http://arxiv.org/abs/1406.6201 | id:1406.6201 author:Reiner Lenz category:cs.CV I.5.4  published:2014-06-24 summary:We describe a statistical analysis of the eye tracker measurements in a database with 15 observers viewing 1003 images under free-viewing conditions. In contrast to the common approach of investigating the properties of the fixation points we analyze the properties of the transition phases between fixations. We introduce hyperbolic geometry as a tool to measure the step length between consecutive eye positions. We show that the step lengths, measured in hyperbolic and euclidean geometry, follow a generalized Pareto distribution. The results based on the hyperbolic distance are more robust than those based on euclidean geometry. We show how the structure of the space of generalized Pareto distributions can be used to characterize and identify individual observers. version:1
arxiv-1406-6200 | Combining predictions from linear models when training and test inputs differ | http://arxiv.org/abs/1406.6200 | id:1406.6200 author:Thijs van Ommen category:stat.ME cs.LG stat.ML  published:2014-06-24 summary:Methods for combining predictions from different models in a supervised learning setting must somehow estimate/predict the quality of a model's predictions at unknown future inputs. Many of these methods (often implicitly) make the assumption that the test inputs are identical to the training inputs, which is seldom reasonable. By failing to take into account that prediction will generally be harder for test inputs that did not occur in the training set, this leads to the selection of too complex models. Based on a novel, unbiased expression for KL divergence, we propose XAIC and its special case FAIC as versions of AIC intended for prediction that use different degrees of knowledge of the test inputs. Both methods substantially differ from and may outperform all the known versions of AIC even when the training and test inputs are iid, and are especially useful for deterministic inputs and under covariate shift. Our experiments on linear models suggest that if the test and training inputs differ substantially, then XAIC and FAIC predictively outperform AIC, BIC and several other methods including Bayesian model averaging. version:1
arxiv-1406-6176 | Composite Likelihood Estimation for Restricted Boltzmann machines | http://arxiv.org/abs/1406.6176 | id:1406.6176 author:Muneki Yasuda, Shun Kataoka, Yuji Waizumi, Kazuyuki Tanaka category:cs.LG  published:2014-06-24 summary:Learning the parameters of graphical models using the maximum likelihood estimation is generally hard which requires an approximation. Maximum composite likelihood estimations are statistical approximations of the maximum likelihood estimation which are higher-order generalizations of the maximum pseudo-likelihood estimation. In this paper, we propose a composite likelihood method and investigate its property. Furthermore, we apply our composite likelihood method to restricted Boltzmann machines. version:1
arxiv-1406-7002 | A Concise Information-Theoretic Derivation of the Baum-Welch algorithm | http://arxiv.org/abs/1406.7002 | id:1406.7002 author:Alireza Nejati, Charles Unsworth category:cs.IT cs.LG math.IT  published:2014-06-24 summary:We derive the Baum-Welch algorithm for hidden Markov models (HMMs) through an information-theoretical approach using cross-entropy instead of the Lagrange multiplier approach which is universal in machine learning literature. The proposed approach provides a more concise derivation of the Baum-Welch method and naturally generalizes to multiple observations. version:1
arxiv-1406-6147 | Incorporating Near-Infrared Information into Semantic Image Segmentation | http://arxiv.org/abs/1406.6147 | id:1406.6147 author:Neda Salamati, Diane Larlus, Gabriela Csurka, Sabine Süsstrunk category:cs.CV  published:2014-06-24 summary:Recent progress in computational photography has shown that we can acquire near-infrared (NIR) information in addition to the normal visible (RGB) band, with only slight modifications to standard digital cameras. Due to the proximity of the NIR band to visible radiation, NIR images share many properties with visible images. However, as a result of the material dependent reflection in the NIR part of the spectrum, such images reveal different characteristics of the scene. We investigate how to effectively exploit these differences to improve performance on the semantic image segmentation task. Based on a state-of-the-art segmentation framework and a novel manually segmented image database (both indoor and outdoor scenes) that contain 4-channel images (RGB+NIR), we study how to best incorporate the specific characteristics of the NIR response. We show that adding NIR leads to improved performance for classes that correspond to a specific type of material in both outdoor and indoor scenes. We also discuss the results with respect to the physical properties of the NIR response. version:1
arxiv-1306-5707 | Synthesizing Manipulation Sequences for Under-Specified Tasks using Unrolled Markov Random Fields | http://arxiv.org/abs/1306.5707 | id:1306.5707 author:Jaeyong Sung, Bart Selman, Ashutosh Saxena category:cs.RO cs.AI cs.LG  published:2013-06-24 summary:Many tasks in human environments require performing a sequence of navigation and manipulation steps involving objects. In unstructured human environments, the location and configuration of the objects involved often change in unpredictable ways. This requires a high-level planning strategy that is robust and flexible in an uncertain environment. We propose a novel dynamic planning strategy, which can be trained from a set of example sequences. High level tasks are expressed as a sequence of primitive actions or controllers (with appropriate parameters). Our score function, based on Markov Random Field (MRF), captures the relations between environment, controllers, and their arguments. By expressing the environment using sets of attributes, the approach generalizes well to unseen scenarios. We train the parameters of our MRF using a maximum margin learning method. We provide a detailed empirical validation of our overall framework demonstrating successful plan strategies for a variety of tasks. version:2
arxiv-1406-6130 | Generalized Mixability via Entropic Duality | http://arxiv.org/abs/1406.6130 | id:1406.6130 author:Mark D. Reid, Rafael M. Frongillo, Robert C. Williamson, Nishant Mehta category:cs.LG  published:2014-06-24 summary:Mixability is a property of a loss which characterizes when fast convergence is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the exp and log operations present in the usual theory are not as special as one might have thought. In doing this we introduce a more general notion of $\Phi$-mixability where $\Phi$ is a general entropy (\ie, any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical aggregating algorithm, is guaranteed a constant regret when used with $\Phi$-mixable losses. We characterize precisely which $\Phi$ have $\Phi$-mixable losses and put forward a number of conjectures about the optimality and relationships between different choices of entropy. version:1
arxiv-1406-6114 | Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform | http://arxiv.org/abs/1406.6114 | id:1406.6114 author:Sakthithasan Sripirakas, Russel Pears category:cs.LG  published:2014-06-24 summary:In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form. version:1
arxiv-1406-6101 | Improved Frame Level Features and SVM Supervectors Approach for the Recogniton of Emotional States from Speech: Application to categorical and dimensional states | http://arxiv.org/abs/1406.6101 | id:1406.6101 author:Imen Trabelsi, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG  published:2014-06-23 summary:The purpose of speech emotion recognition system is to classify speakers utterances into different emotional states such as disgust, boredom, sadness, neutral and happiness. Speech features that are commonly used in speech emotion recognition rely on global utterance level prosodic features. In our work, we evaluate the impact of frame level feature extraction. The speech samples are from Berlin emotional database and the features extracted from these utterances are energy, different variant of mel frequency cepstrum coefficients, velocity and acceleration features. version:1
arxiv-1406-6084 | From Black-Scholes to Online Learning: Dynamic Hedging under Adversarial Environments | http://arxiv.org/abs/1406.6084 | id:1406.6084 author:Henry Lam, Zhenming Liu category:cs.DS cs.LG q-fin.PR F.2; I.2.6  published:2014-06-23 summary:We consider a non-stochastic online learning approach to price financial options by modeling the market dynamic as a repeated game between the nature (adversary) and the investor. We demonstrate that such framework yields analogous structure as the Black-Scholes model, the widely popular option pricing model in stochastic finance, for both European and American options with convex payoffs. In the case of non-convex options, we construct approximate pricing algorithms, and demonstrate that their efficiency can be analyzed through the introduction of an artificial probability measure, in parallel to the so-called risk-neutral measure in the finance literature, even though our framework is completely adversarial. Continuous-time convergence results and extensions to incorporate price jumps are also presented. version:1
arxiv-1112-4863 | A Novel M-Estimator for Robust PCA | http://arxiv.org/abs/1112.4863 | id:1112.4863 author:Teng Zhang, Gilad Lerman category:stat.ML math.OC  published:2011-12-20 summary:We study the basic problem of robust subspace recovery. That is, we assume a data set that some of its points are sampled around a fixed subspace and the rest of them are spread in the whole ambient space, and we aim to recover the fixed underlying subspace. We first estimate "robust inverse sample covariance" by solving a convex minimization procedure; we then recover the subspace by the bottom eigenvectors of this matrix (their number correspond to the number of eigenvalues close to 0). We guarantee exact subspace recovery under some conditions on the underlying data. Furthermore, we propose a fast iterative algorithm, which linearly converges to the matrix minimizing the convex problem. We also quantify the effect of noise and regularization and discuss many other practical and theoretical issues for improving the subspace recovery in various settings. When replacing the sum of terms in the convex energy function (that we minimize) with the sum of squares of terms, we obtain that the new minimizer is a scaled version of the inverse sample covariance (when exists). We thus interpret our minimizer and its subspace (spanned by its bottom eigenvectors) as robust versions of the empirical inverse covariance and the PCA subspace respectively. We compare our method with many other algorithms for robust PCA on synthetic and real data sets and demonstrate state-of-the-art speed and accuracy. version:4
arxiv-1406-6020 | Stationary Mixing Bandits | http://arxiv.org/abs/1406.6020 | id:1406.6020 author:Julien Audiffren, Liva Ralaivola category:cs.LG  published:2014-06-23 summary:We study the bandit problem where arms are associated with stationary phi-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of recovering some independence by ignoring the value of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off. To do so, we provide a UCB strategy together with a general regret analysis for the case where the size of the independence blocks (the ignored rewards) is fixed and we go a step beyond by providing an algorithm that is able to compute the size of the independence blocks from the data. Finally, we give an analysis of our bandit problem in the restless case, i.e., in the situation where the time counters for all mixing processes simultaneously evolve. version:1
arxiv-1406-5979 | Reinforcement and Imitation Learning via Interactive No-Regret Learning | http://arxiv.org/abs/1406.5979 | id:1406.5979 author:Stephane Ross, J. Andrew Bagnell category:cs.LG stat.ML  published:2014-06-23 summary:Recent work has demonstrated that problems-- particularly imitation learning and structured prediction-- where a learner's predictions influence the input-distribution it is tested on can be naturally addressed by an interactive approach and analyzed using no-regret online learning. These approaches to imitation learning, however, neither require nor benefit from information about the cost of actions. We extend existing results in two directions: first, we develop an interactive imitation learning approach that leverages cost information; second, we extend the technique to address reinforcement learning. The results provide theoretical support to the commonly observed successes of online approximate policy iteration. Our approach suggests a broad new family of algorithms and provides a unifying view of existing techniques for imitation and reinforcement learning. version:1
arxiv-1406-5947 | Committees of deep feedforward networks trained with few data | http://arxiv.org/abs/1406.5947 | id:1406.5947 author:Bogdan Miclut, Thomas Kaester, Thomas Martinetz, Erhardt Barth category:cs.CV cs.NE  published:2014-06-23 summary:Deep convolutional neural networks are known to give good results on image classification tasks. In this paper we present a method to improve the classification result by combining multiple such networks in a committee. We adopt the STL-10 dataset which has very few training examples and show that our method can achieve results that are better than the state of the art. The networks are trained layer-wise and no backpropagation is used. We also explore the effects of dataset augmentation by mirroring, rotation, and scaling. version:1
arxiv-1406-5910 | Multi-utility Learning: Structured-output Learning with Multiple Annotation-specific Loss Functions | http://arxiv.org/abs/1406.5910 | id:1406.5910 author:Roman Shapovalov, Dmitry Vetrov, Anton Osokin, Pushmeet Kohli category:cs.CV cs.LG  published:2014-06-23 summary:Structured-output learning is a challenging problem; particularly so because of the difficulty in obtaining large datasets of fully labelled instances for training. In this paper we try to overcome this difficulty by presenting a multi-utility learning framework for structured prediction that can learn from training instances with different forms of supervision. We propose a unified technique for inferring the loss functions most suitable for quantifying the consistency of solutions with the given weak annotation. We demonstrate the effectiveness of our framework on the challenging semantic image segmentation problem for which a wide variety of annotations can be used. For instance, the popular training datasets for semantic segmentation are composed of images with hard-to-generate full pixel labellings, as well as images with easy-to-obtain weak annotations, such as bounding boxes around objects, or image-level labels that specify which object categories are present in an image. Experimental evaluation shows that the use of annotation-specific loss functions dramatically improves segmentation accuracy compared to the baseline system where only one type of weak annotation is used. version:1
arxiv-1406-5824 | VideoSET: Video Summary Evaluation through Text | http://arxiv.org/abs/1406.5824 | id:1406.5824 author:Serena Yeung, Alireza Fathi, Li Fei-Fei category:cs.CV cs.CL cs.IR  published:2014-06-23 summary:In this paper we present VideoSET, a method for Video Summary Evaluation through Text that can evaluate how well a video summary is able to retain the semantic information contained in its original video. We observe that semantics is most easily expressed in words, and develop a text-based approach for the evaluation. Given a video summary, a text representation of the video summary is first generated, and an NLP-based metric is then used to measure its semantic distance to ground-truth text summaries written by humans. We show that our technique has higher agreement with human judgment than pixel-based distance metrics. We also release text annotations and ground-truth text summaries for a number of publicly available video datasets, for use by the computer vision community. version:1
arxiv-1406-5807 | A Unified Quantitative Model of Vision and Audition | http://arxiv.org/abs/1406.5807 | id:1406.5807 author:Peilei Liu, Ting Wang category:cs.CV q-bio.NC q-bio.QM I.5.4; I.5.2  published:2014-06-23 summary:We have put forwards a unified quantitative framework of vision and audition, based on existing data and theories. According to this model, the retina is a feedforward network self-adaptive to inputs in a specific period. After fully grown, cells become specialized detectors based on statistics of stimulus history. This model has provided explanations for perception mechanisms of colour, shape, depth and motion. Moreover, based on this ground we have put forwards a bold conjecture that single ear can detect sound direction. This is complementary to existing theories and has provided better explanations for sound localization. version:1
arxiv-1406-6314 | Further heuristics for $k$-means: The merge-and-split heuristic and the $(k,l)$-means | http://arxiv.org/abs/1406.6314 | id:1406.6314 author:Frank Nielsen, Richard Nock category:cs.LG cs.CV cs.IR stat.ML  published:2014-06-23 summary:Finding the optimal $k$-means clustering is NP-hard in general and many heuristics have been designed for minimizing monotonically the $k$-means objective. We first show how to extend Lloyd's batched relocation heuristic and Hartigan's single-point relocation heuristic to take into account empty-cluster and single-point cluster events, respectively. Those events tend to increasingly occur when $k$ or $d$ increases, or when performing several restarts. First, we show that those special events are a blessing because they allow to partially re-seed some cluster centers while further minimizing the $k$-means objective function. Second, we describe a novel heuristic, merge-and-split $k$-means, that consists in merging two clusters and splitting this merged cluster again with two new centers provided it improves the $k$-means objective. This novel heuristic can improve Hartigan's $k$-means when it has converged to a local minimum. We show empirically that this merge-and-split $k$-means improves over the Hartigan's heuristic which is the {\em de facto} method of choice. Finally, we propose the $(k,l)$-means objective that generalizes the $k$-means objective by associating the data points to their $l$ closest cluster centers, and show how to either directly convert or iteratively relax the $(k,l)$-means into a $k$-means in order to reach better local minima. version:1
arxiv-1406-5765 | Environmental Sensing by Wearable Device for Indoor Activity and Location Estimation | http://arxiv.org/abs/1406.5765 | id:1406.5765 author:Ming Jin, Han Zou, Kevin Weekly, Ruoxi Jia, Alexandre M. Bayen, Costas J. Spanos category:cs.HC stat.ML  published:2014-06-22 summary:We present results from a set of experiments in this pilot study to investigate the causal influence of user activity on various environmental parameters monitored by occupant carried multi-purpose sensors. Hypotheses with respect to each type of measurements are verified, including temperature, humidity, and light level collected during eight typical activities: sitting in lab / cubicle, indoor walking / running, resting after physical activity, climbing stairs, taking elevators, and outdoor walking. Our main contribution is the development of features for activity and location recognition based on environmental measurements, which exploit location- and activity-specific characteristics and capture the trends resulted from the underlying physiological process. The features are statistically shown to have good separability and are also information-rich. Fusing environmental sensing together with acceleration is shown to achieve classification accuracy as high as 99.13%. For building applications, this study motivates a sensor fusion paradigm for learning individualized activity, location, and environmental preferences for energy management and user comfort. version:1
arxiv-1407-3695 | Recovery of Images with Missing Pixels using a Gradient Compressive Sensing Algorithm | http://arxiv.org/abs/1407.3695 | id:1407.3695 author:Isidora Stanković category:cs.CV  published:2014-06-22 summary:This paper investigates the possibility of reconstruction of images considering that they are sparse in the DCT transformation domain. Two approaches are considered. One when the image is pre-processed in the DCT domain, using 8x8 blocks. The image is made sparse by setting the smallest DCT coefficients to zero. In the other case the original image is considered without pre-processing, assuming the sparsity as intrinsic property of the analyzed image. A gradient based algorithm is used to recover a large number of missing pixels in the image. The case of a salt-and-paper noise affecting a large number of pixels is easily reduced to the case of missing pixels and considered within the same framework. The reconstruction of images affected with salt-and-paper impulsive is compared with the images filtered using a median filter. The same algorithm can be used considering transformation of the whole image. Reconstructions of black and white and colour images are considered. version:1
arxiv-1406-5752 | Divide-and-Conquer Learning by Anchoring a Conical Hull | http://arxiv.org/abs/1406.5752 | id:1406.5752 author:Tianyi Zhou, Jeff Bilmes, Carlos Guestrin category:stat.ML cs.LG  published:2014-06-22 summary:We reduce a broad class of machine learning problems, usually addressed by EM or sampling, to the problem of finding the $k$ extremal rays spanning the conical hull of a data point set. These $k$ "anchors" lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the $k$ anchors, we propose a novel divide-and-conquer learning scheme "DCA" that distributes the problem to $\mathcal O(k\log k)$ same-type sub-problems on different low-D random hyperplanes, each can be solved by any solver. For the 2D sub-problem, we present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine for other methods to check whether a point is covered in a conical hull, which improves algorithm design in multiple dimensions and brings significant speedup to learning. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on rich datasets. version:1
arxiv-1406-5736 | Convex Optimization Learning of Faithful Euclidean Distance Representations in Nonlinear Dimensionality Reduction | http://arxiv.org/abs/1406.5736 | id:1406.5736 author:Chao Ding, Hou-Duo Qi category:stat.ML cs.LG math.OC  published:2014-06-22 summary:Classical multidimensional scaling only works well when the noisy distances observed in a high dimensional space can be faithfully represented by Euclidean distances in a low dimensional space. Advanced models such as Maximum Variance Unfolding (MVU) and Minimum Volume Embedding (MVE) use Semi-Definite Programming (SDP) to reconstruct such faithful representations. While those SDP models are capable of producing high quality configuration numerically, they suffer two major drawbacks. One is that there exist no theoretically guaranteed bounds on the quality of the configuration. The other is that they are slow in computation when the data points are beyond moderate size. In this paper, we propose a convex optimization model of Euclidean distance matrices. We establish a non-asymptotic error bound for the random graph model with sub-Gaussian noise, and prove that our model produces a matrix estimator of high accuracy when the order of the uniform sample size is roughly the degree of freedom of a low-rank matrix up to a logarithmic factor. Our results partially explain why MVU and MVE often work well. Moreover, we develop a fast inexact accelerated proximal gradient method. Numerical experiments show that the model can produce configurations of high quality on large data points that the SDP approach would struggle to cope with. version:1
arxiv-1406-5710 | Natural Color Image Enhancement based on Modified Multiscale Retinex Algorithm and Performance Evaluation usingWavelet Energy | http://arxiv.org/abs/1406.5710 | id:1406.5710 author:M. C Hanumantharaju, M. Ravishankar, D. R Rameshbabu category:cs.CV 68U10 I.4.3  published:2014-06-22 summary:This paper presents a new color image enhancement technique based on modified MultiScale Retinex(MSR) algorithm and visual quality of the enhanced images are evaluated using a new metric, namely, wavelet energy. The color image enhancement is achieved by down sampling the value component of HSV color space converted image into three scales (normal, medium and fine) following the contrast stretching operation. These down sampled value components are enhanced using the MSR algorithm. The value component is reconstructed by averaging each pixels of the lower scale image with that of the upper scale image subsequent to up sampling the lower scale image. This process replaces dark pixel by the average pixels of both the lower scale and upper scale, while retaining the bright pixels. The quality of the reconstructed images in the proposed method is found to be good and far better then the other researchers method. The performance of the proposed scheme is evaluated using new wavelet domain based assessment criterion, referred as wavelet energy. This scheme computes the energy of both original and enhanced image in wavelet domain. The number of edge details as well as wavelet energy is less in a poor quality image compared with naturally enhanced image. Experimental results presented confirms that the proposed wavelet energy based color image quality assessment technique efficiently characterizes both the local and global details of enhanced image. version:1
arxiv-1406-5691 | A CNL for Contract-Oriented Diagrams | http://arxiv.org/abs/1406.5691 | id:1406.5691 author:John J. Camilleri, Gabriele Paganelli, Gerardo Schneider category:cs.CL cs.FL  published:2014-06-22 summary:We present a first step towards a framework for defining and manipulating normative documents or contracts described as Contract-Oriented (C-O) Diagrams. These diagrams provide a visual representation for such texts, giving the possibility to express a signatory's obligations, permissions and prohibitions, with or without timing constraints, as well as the penalties resulting from the non-fulfilment of a contract. This work presents a CNL for verbalising C-O Diagrams, a web-based tool allowing editing in this CNL, and another for visualising and manipulating the diagrams interactively. We then show how these proof-of-concept tools can be used by applying them to a small example. version:1
arxiv-1406-5679 | Deep Fragment Embeddings for Bidirectional Image Sentence Mapping | http://arxiv.org/abs/1406.5679 | id:1406.5679 author:Andrej Karpathy, Armand Joulin, Li Fei-Fei category:cs.CV cs.CL cs.LG  published:2014-06-22 summary:We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit. version:1
arxiv-1406-5665 | Constant Factor Approximation for Balanced Cut in the PIE model | http://arxiv.org/abs/1406.5665 | id:1406.5665 author:Konstantin Makarychev, Yury Makarychev, Aravindan Vijayaraghavan category:cs.DS cs.LG  published:2014-06-22 summary:We propose and study a new semi-random semi-adversarial model for Balanced Cut, a planted model with permutation-invariant random edges (PIE). Our model is much more general than planted models considered previously. Consider a set of vertices V partitioned into two clusters $L$ and $R$ of equal size. Let $G$ be an arbitrary graph on $V$ with no edges between $L$ and $R$. Let $E_{random}$ be a set of edges sampled from an arbitrary permutation-invariant distribution (a distribution that is invariant under permutation of vertices in $L$ and in $R$). Then we say that $G + E_{random}$ is a graph with permutation-invariant random edges. We present an approximation algorithm for the Balanced Cut problem that finds a balanced cut of cost $O( E_{random} ) + n \text{polylog}(n)$ in this model. In the regime when $ E_{random} = \Omega(n \text{polylog}(n))$, this is a constant factor approximation with respect to the cost of the planted cut. version:1
arxiv-1406-5653 | Interactively Test Driving an Object Detector: Estimating Performance on Unlabeled Data | http://arxiv.org/abs/1406.5653 | id:1406.5653 author:Rushil Anirudh, Pavan Turaga category:cs.CV  published:2014-06-21 summary:In this paper, we study the problem of `test-driving' a detector, i.e. allowing a human user to get a quick sense of how well the detector generalizes to their specific requirement. To this end, we present the first system that estimates detector performance interactively without extensive ground truthing using a human in the loop. We approach this as a problem of estimating proportions and show that it is possible to make accurate inferences on the proportion of classes or groups within a large data collection by observing only $5-10\%$ of samples from the data. In estimating the false detections (for precision), the samples are chosen carefully such that the overall characteristics of the data collection are preserved. Next, inspired by its use in estimating disease propagation we apply pooled testing approaches to estimate missed detections (for recall) from the dataset. The estimates thus obtained are close to the ones obtained using ground truth, thus reducing the need for extensive labeling which is expensive and time consuming. version:1
arxiv-1401-5054 | Análisis e implementación de algoritmos evolutivos para la optimización de simulaciones en ingeniería civil. (draft) | http://arxiv.org/abs/1401.5054 | id:1401.5054 author:José Alberto García Gutiérrez, Alejandro Mateo Hernández Díaz category:cs.NE cs.AI  published:2014-01-20 summary:This paper studies the applicability of evolutionary algorithms, particularly, the evolution strategies family in order to estimate a degradation parameter in the shear design of reinforced concrete members. This problem represents a great computational task and is highly relevant in the framework of the structural engineering that for the first time is solved using genetic algorithms. You are viewing a draft, the authors appreciate corrections, comments and suggestions to this work. version:3
arxiv-1406-5638 | Minimax-optimal Inference from Partial Rankings | http://arxiv.org/abs/1406.5638 | id:1406.5638 author:Bruce Hajek, Sewoong Oh, Jiaming Xu category:stat.ML math.ST stat.TH  published:2014-06-21 summary:This paper studies the problem of inferring a global preference based on the partial rankings provided by many users over different subsets of items according to the Plackett-Luce model. A question of particular interest is how to optimally assign items to users for ranking and how many item assignments are needed to achieve a target estimation error. For a given assignment of items to users, we first derive an oracle lower bound of the estimation error that holds even for the more general Thurstone models. Then we show that the Cram\'er-Rao lower bound and our upper bounds inversely depend on the spectral gap of the Laplacian of an appropriately defined comparison graph. When the system is allowed to choose the item assignment, we propose a random assignment scheme. Our oracle lower bound and upper bounds imply that it is minimax-optimal up to a logarithmic factor among all assignment schemes and the lower bound can be achieved by the maximum likelihood estimator as well as popular rank-breaking schemes that decompose partial rankings into pairwise comparisons. The numerical experiments corroborate our theoretical findings. version:1
arxiv-1406-5633 | Thermodynamic-RAM Technology Stack | http://arxiv.org/abs/1406.5633 | id:1406.5633 author:M. Alexander Nugent, Timothy W. Molter category:cs.NE  published:2014-06-21 summary:We introduce a technology stack or specification describing the multiple levels of abstraction and specialization needed to implement a neuromorphic processor based on the theory of AHaH Computing. This specific implementation is called Thermodynamic-RAM (kT-RAM). Bringing us closer to brain-like neural computation, kT-RAM will provide a general-purpose adaptive hardware resource to existing computing platforms enabling fast and low-power machine learning capabilities that are currently hampered by the separation of memory and processing. The motivation for defining the technology stack is two-fold. First, explaining kT-RAM is much easier if it is broken down into smaller, more manageable pieces. Secondly, groups interested in realizing kT-RAM can choose a level to contribute to that matches their interest and expertise. The levels of the Thermodynamic-RAM technology stack include the memristor, Knowm-Synapse, AHaH Node, kT-RAM, kT-RAM instruction set, sparse spike encoding, kT-RAM emulator, and SENSE Server. version:1
arxiv-1211-0025 | Venn-Abers predictors | http://arxiv.org/abs/1211.0025 | id:1211.0025 author:Vladimir Vovk, Ivan Petej category:cs.LG stat.ML 68T05  68T10  published:2012-10-31 summary:This paper continues study, both theoretical and empirical, of the method of Venn prediction, concentrating on binary prediction problems. Venn predictors produce probability-type predictions for the labels of test objects which are guaranteed to be well calibrated under the standard assumption that the observations are generated independently from the same distribution. We give a simple formalization and proof of this property. We also introduce Venn-Abers predictors, a new class of Venn predictors based on the idea of isotonic regression, and report promising empirical results both for Venn-Abers predictors and for their more computationally efficient simplified version. version:2
arxiv-1406-5600 | From conformal to probabilistic prediction | http://arxiv.org/abs/1406.5600 | id:1406.5600 author:Vladimir Vovk, Ivan Petej, Valentina Fedorova category:cs.LG 68T10  published:2014-06-21 summary:This paper proposes a new method of probabilistic prediction, which is based on conformal prediction. The method is applied to the standard USPS data set and gives encouraging results. version:1
arxiv-1406-5598 | A survey on phrase structure learning methods for text classification | http://arxiv.org/abs/1406.5598 | id:1406.5598 author:Reshma Prasad, Mary Priya Sebastian category:cs.CL  published:2014-06-21 summary:Text classification is a task of automatic classification of text into one of the predefined categories. The problem of text classification has been widely studied in different communities like natural language processing, data mining and information retrieval. Text classification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc. The performance of text classification improves notably when phrase patterns are used. The use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement of text classification task. Phrase structure extraction is the first step to continue with the phrase pattern identification. In this survey, detailed study of phrase structure learning methods have been carried out. This will enable future work in several NLP tasks, which uses syntactic information from phrase structure like grammar checkers, question answering, information extraction, machine translation, text classification. The paper also provides different levels of classification and detailed comparison of the phrase structure learning methods. version:1
arxiv-1406-5143 | The Sample Complexity of Learning Linear Predictors with the Squared Loss | http://arxiv.org/abs/1406.5143 | id:1406.5143 author:Ohad Shamir category:cs.LG stat.ML  published:2014-06-19 summary:In this short note, we provide tight sample complexity bounds for learning linear predictors with respect to the squared loss. Our focus is on an agnostic setting, where no assumptions are made on the data distribution. This contrasts with standard results in the literature, which either make distributional assumptions, refer to specific parameter settings, or use other performance measures. version:2
arxiv-1406-5565 | An Open Source Pattern Recognition Toolbox for MATLAB | http://arxiv.org/abs/1406.5565 | id:1406.5565 author:Kenneth D. Morton Jr., Peter Torrione, Leslie Collins, Sam Keene category:stat.ML cs.CV cs.LG cs.MS  published:2014-06-21 summary:Pattern recognition and machine learning are becoming integral parts of algorithms in a wide range of applications. Different algorithms and approaches for machine learning include different tradeoffs between performance and computation, so during algorithm development it is often necessary to explore a variety of different approaches to a given task. A toolbox with a unified framework across multiple pattern recognition techniques enables algorithm developers the ability to rapidly evaluate different choices prior to deployment. MATLAB is a widely used environment for algorithm development and prototyping, and although several MATLAB toolboxes for pattern recognition are currently available these are either incomplete, expensive, or restrictively licensed. In this work we describe a MATLAB toolbox for pattern recognition and machine learning known as the PRT (Pattern Recognition Toolbox), licensed under the permissive MIT license. The PRT includes many popular techniques for data preprocessing, supervised learning, clustering, regression and feature selection, as well as a methodology for combining these components using a simple, uniform syntax. The resulting algorithms can be evaluated using cross-validation and a variety of scoring metrics to ensure robust performance when the algorithm is deployed. This paper presents an overview of the PRT as well as an example of usage on Fisher's Iris dataset. version:1
arxiv-1405-0782 | Optimality guarantees for distributed statistical estimation | http://arxiv.org/abs/1405.0782 | id:1405.0782 author:John C. Duchi, Michael I. Jordan, Martin J. Wainwright, Yuchen Zhang category:cs.IT cs.LG math.IT math.ST stat.TH  published:2014-05-05 summary:Large data sets often require performing distributed statistical estimation, with a full data set split across multiple machines and limited communication between machines. To study such scenarios, we define and study some refinements of the classical minimax risk that apply to distributed settings, comparing to the performance of estimators with access to the entire data. Lower bounds on these quantities provide a precise characterization of the minimum amount of communication required to achieve the centralized minimax risk. We study two classes of distributed protocols: one in which machines send messages independently over channels without feedback, and a second allowing for interactive communication, in which a central server broadcasts the messages from a given machine to all other machines. We establish lower bounds for a variety of problems, including location estimation in several families and parameter estimation in different types of regression models. Our results include a novel class of quantitative data-processing inequalities used to characterize the effects of limited communication. version:2
arxiv-1408-5093 | Caffe: Convolutional Architecture for Fast Feature Embedding | http://arxiv.org/abs/1408.5093 | id:1408.5093 author:Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell category:cs.CV cs.LG cs.NE  published:2014-06-20 summary:Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia. version:1
arxiv-1406-5472 | Inferring the Why in Images | http://arxiv.org/abs/1406.5472 | id:1406.5472 author:Hamed Pirsiavash, Carl Vondrick, Antonio Torralba category:cs.CV  published:2014-06-20 summary:Humans have the remarkable capability to infer the motivations of other people's actions, likely due to cognitive skills known in psychophysics as the theory of mind. In this paper, we strive to build a computational model that predicts the motivation behind the actions of people from images. To our knowledge, this challenging problem has not yet been extensively explored in computer vision. We present a novel learning based framework that uses high-level visual recognition to infer why people are performing an actions in images. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their own experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models to mine knowledge stored in massive amounts of text. While we are still far away from automatically inferring motivation, our results suggest that transferring knowledge from language into vision can help machines understand why a person might be performing an action in an image. version:1
arxiv-1311-1756 | An Inexact Proximal Path-Following Algorithm for Constrained Convex Minimization | http://arxiv.org/abs/1311.1756 | id:1311.1756 author:Quoc Tran Dinh, Anastasios Kyrillidis, Volkan Cevher category:math.OC stat.ML  published:2013-11-07 summary:Many scientific and engineering applications feature nonsmooth convex minimization problems over convex sets. In this paper, we address an important instance of this broad class where we assume that the nonsmooth objective is equipped with a tractable proximity operator and that the convex constraint set affords a self-concordant barrier. We provide a new joint treatment of proximal and self-concordant barrier concepts and illustrate that such problems can be efficiently solved, without the need of lifting the problem dimensions, as in disciplined convex optimization approach. We propose an inexact path-following algorithmic framework and theoretically characterize the worst-case analytical complexity of this framework when the proximal subproblems are solved inexactly. To show the merits of our framework, we apply its instances to both synthetic and real-world applications, where it shows advantages over standard interior point methods. As a by-product, we describe how our framework can obtain points on the Pareto frontier of regularized problems with self-concordant objectives in a tuning free fashion. version:2
arxiv-1405-4141 | Classification using log Gaussian Cox processes | http://arxiv.org/abs/1405.4141 | id:1405.4141 author:Alexander G. de. G Matthews, Zoubin Ghahramani category:stat.ML stat.CO stat.ME  published:2014-05-16 summary:McCullagh and Yang (2006) suggest a family of classification algorithms based on Cox processes. We further investigate the log Gaussian variant which has a number of appealing properties. Conditioned on the covariates, the distribution over labels is given by a type of conditional Markov random field. In the supervised case, computation of the predictive probability of a single test point scales linearly with the number of training points and the multiclass generalization is straightforward. We show new links between the supervised method and classical nonparametric methods. We give a detailed analysis of the pairwise graph representable Markov random field, which we use to extend the model to semi-supervised learning problems, and propose an inference method based on graph min-cuts. We give the first experimental analysis on supervised and semi-supervised datasets and show good empirical performance. version:2
arxiv-1401-0514 | Structured Generative Models of Natural Source Code | http://arxiv.org/abs/1401.0514 | id:1401.0514 author:Chris J. Maddison, Daniel Tarlow category:cs.PL cs.LG stat.ML  published:2014-01-02 summary:We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have three key properties: First, they incorporate both sequential and hierarchical structure. Second, we learn a distributed representation of source code elements. Finally, they integrate closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope. Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the models, measured by the probability of generating test programs. version:2
arxiv-1406-5295 | Rows vs Columns for Linear Systems of Equations - Randomized Kaczmarz or Coordinate Descent? | http://arxiv.org/abs/1406.5295 | id:1406.5295 author:Aaditya Ramdas category:math.OC cs.LG cs.NA math.NA stat.ML  published:2014-06-20 summary:This paper is about randomized iterative algorithms for solving a linear system of equations $X \beta = y$ in different settings. Recent interest in the topic was reignited when Strohmer and Vershynin (2009) proved the linear convergence rate of a Randomized Kaczmarz (RK) algorithm that works on the rows of $X$ (data points). Following that, Leventhal and Lewis (2010) proved the linear convergence of a Randomized Coordinate Descent (RCD) algorithm that works on the columns of $X$ (features). The aim of this paper is to simplify our understanding of these two algorithms, establish the direct relationships between them (though RK is often compared to Stochastic Gradient Descent), and examine the algorithmic commonalities or tradeoffs involved with working on rows or columns. We also discuss Kernel Ridge Regression and present a Kaczmarz-style algorithm that works on data points and having the advantage of solving the problem without ever storing or forming the Gram matrix, one of the recognized problems encountered when scaling kernelized methods. version:1
arxiv-1406-5286 | Enhancing Pure-Pixel Identification Performance via Preconditioning | http://arxiv.org/abs/1406.5286 | id:1406.5286 author:Nicolas Gillis, Wing-Kin Ma category:stat.ML cs.LG math.NA math.OC  published:2014-06-20 summary:In this paper, we analyze different preconditionings designed to enhance robustness of pure-pixel search algorithms, which are used for blind hyperspectral unmixing and which are equivalent to near-separable nonnegative matrix factorization algorithms. Our analysis focuses on the successive projection algorithm (SPA), a simple, efficient and provably robust algorithm in the pure-pixel algorithm class. Recently, a provably robust preconditioning was proposed by Gillis and Vavasis (arXiv:1310.2273) which requires the resolution of a semidefinite program (SDP) to find a data points-enclosing minimum volume ellipsoid. Since solving the SDP in high precisions can be time consuming, we generalize the robustness analysis to approximate solutions of the SDP, that is, solutions whose objective function values are some multiplicative factors away from the optimal value. It is shown that a high accuracy solution is not crucial for robustness, which paves the way for faster preconditionings (e.g., based on first-order optimization methods). This first contribution also allows us to provide a robustness analysis for two other preconditionings. The first one is pre-whitening, which can be interpreted as an optimal solution of the same SDP with additional constraints. We analyze robustness of pre-whitening which allows us to characterize situations in which it performs competitively with the SDP-based preconditioning. The second one is based on SPA itself and can be interpreted as an optimal solution of a relaxation of the SDP. It is extremely fast while competing with the SDP-based preconditioning on several synthetic data sets. version:1
arxiv-1406-4966 | Inner Product Similarity Search using Compositional Codes | http://arxiv.org/abs/1406.4966 | id:1406.4966 author:Chao Du, Jingdong Wang category:cs.CV cs.LG stat.ML  published:2014-06-19 summary:This paper addresses the nearest neighbor search problem under inner product similarity and introduces a compact code-based approach. The idea is to approximate a vector using the composition of several elements selected from a source dictionary and to represent this vector by a short code composed of the indices of the selected elements. The inner product between a query vector and a database vector is efficiently estimated from the query vector and the short code of the database vector. We show the superior performance of the proposed group $M$-selection algorithm that selects $M$ elements from $M$ source dictionaries for vector approximation in terms of search accuracy and efficiency for compact codes of the same length via theoretical and empirical analysis. Experimental results on large-scale datasets ($1M$ and $1B$ SIFT features, $1M$ linear models and Netflix) demonstrate the superiority of the proposed approach. version:2
arxiv-1406-4200 | Lifted Tree-Reweighted Variational Inference | http://arxiv.org/abs/1406.4200 | id:1406.4200 author:Hung Hai Bui, Tuyen N. Huynh, David Sontag category:cs.AI stat.ML  published:2014-06-17 summary:We analyze variational inference for highly symmetric graphical models such as those arising from first-order probabilistic models. We first show that for these graphical models, the tree-reweighted variational objective lends itself to a compact lifted formulation which can be solved much more efficiently than the standard TRW formulation for the ground graphical model. Compared to earlier work on lifted belief propagation, our formulation leads to a convex optimization problem for lifted marginal inference and provides an upper bound on the partition function. We provide two approaches for improving the lifted TRW upper bound. The first is a method for efficiently computing maximum spanning trees in highly symmetric graphs, which can be used to optimize the TRW edge appearance probabilities. The second is a method for tightening the relaxation of the marginal polytope using lifted cycle inequalities and novel exchangeable cluster consistency constraints. version:2
arxiv-1406-5212 | R-CNNs for Pose Estimation and Action Detection | http://arxiv.org/abs/1406.5212 | id:1406.5212 author:Georgia Gkioxari, Bharath Hariharan, Ross Girshick, Jitendra Malik category:cs.CV  published:2014-06-19 summary:We present convolutional neural networks for the tasks of keypoint (pose) prediction and action classification of people in unconstrained images. Our approach involves training an R-CNN detector with loss functions depending on the task being tackled. We evaluate our method on the challenging PASCAL VOC dataset and compare it to previous leading approaches. Our method gives state-of-the-art results for keypoint and action prediction. Additionally, we introduce a new dataset for action detection, the task of simultaneously localizing people and classifying their actions, and present results using our approach. version:1
arxiv-1406-5161 | Fast Support Vector Machines Using Parallel Adaptive Shrinking on Distributed Systems | http://arxiv.org/abs/1406.5161 | id:1406.5161 author:Jeyanthi Narasimhan, Abhinav Vishnu, Lawrence Holder, Adolfy Hoisie category:cs.DC cs.LG  published:2014-06-19 summary:Support Vector Machines (SVM), a popular machine learning technique, has been applied to a wide range of domains such as science, finance, and social networks for supervised learning. Whether it is identifying high-risk patients by health-care professionals, or potential high-school students to enroll in college by school districts, SVMs can play a major role for social good. This paper undertakes the challenge of designing a scalable parallel SVM training algorithm for large scale systems, which includes commodity multi-core machines, tightly connected supercomputers and cloud computing systems. Intuitive techniques for improving the time-space complexity including adaptive elimination of samples for faster convergence and sparse format representation are proposed. Under sample elimination, several heuristics for {\em earliest possible} to {\em lazy} elimination of non-contributing samples are proposed. In several cases, where an early sample elimination might result in a false positive, low overhead mechanisms for reconstruction of key data structures are proposed. The algorithm and heuristics are implemented and evaluated on various publicly available datasets. Empirical evaluation shows up to 26x speed improvement on some datasets against the sequential baseline, when evaluated on multiple compute nodes, and an improvement in execution time up to 30-60\% is readily observed on a number of other datasets against our parallel baseline. version:1
arxiv-1406-5095 | MRF-based Background Initialisation for Improved Foreground Detection in Cluttered Surveillance Videos | http://arxiv.org/abs/1406.5095 | id:1406.5095 author:Vikas Reddy, Conrad Sanderson, Andres Sanin, Brian C. Lovell category:cs.CV I.5.4; I.4.5; I.4.6  published:2014-06-19 summary:Robust foreground object segmentation via background modelling is a difficult problem in cluttered environments, where obtaining a clear view of the background to model is almost impossible. In this paper, we propose a method capable of robustly estimating the background and detecting regions of interest in such environments. In particular, we propose to extend the background initialisation component of a recent patch-based foreground detection algorithm with an elaborate technique based on Markov Random Fields, where the optimal labelling solution is computed using iterated conditional modes. Rather than relying purely on local temporal statistics, the proposed technique takes into account the spatial continuity of the entire background. Experiments with several tracking algorithms on the CAVIAR dataset indicate that the proposed method leads to considerable improvements in object tracking accuracy, when compared to methods based on Gaussian mixture models and feature histograms. version:1
arxiv-1406-5074 | Robust Outlier Detection Technique in Data Mining: A Univariate Approach | http://arxiv.org/abs/1406.5074 | id:1406.5074 author:Singh Vijendra, Pathak Shivani category:cs.CV  published:2014-06-19 summary:Outliers are the points which are different from or inconsistent with the rest of the data. They can be novel, new, abnormal, unusual or noisy information. Outliers are sometimes more interesting than the majority of the data. The main challenges of outlier detection with the increasing complexity, size and variety of datasets, are how to catch similar outliers as a group, and how to evaluate the outliers. This paper describes an approach which uses Univariate outlier detection as a pre-processing step to detect the outlier and then applies K-means algorithm hence to analyse the effects of the outliers on the cluster analysis of dataset. version:1
arxiv-1406-5036 | Inferring causal structure: a quantum advantage | http://arxiv.org/abs/1406.5036 | id:1406.5036 author:Katja Ried, Megan Agnew, Lydia Vermeyden, Dominik Janzing, Robert W. Spekkens, Kevin J. Resch category:quant-ph cs.LG gr-qc stat.ML  published:2014-06-19 summary:The problem of using observed correlations to infer causal relations is relevant to a wide variety of scientific disciplines. Yet given correlations between just two classical variables, it is impossible to determine whether they arose from a causal influence of one on the other or a common cause influencing both, unless one can implement a randomized intervention. We here consider the problem of causal inference for quantum variables. We introduce causal tomography, which unifies and generalizes conventional quantum tomography schemes to provide a complete solution to the causal inference problem using a quantum analogue of a randomized trial. We furthermore show that, in contrast to the classical case, observed quantum correlations alone can sometimes provide a solution. We implement a quantum-optical experiment that allows us to control the causal relation between two optical modes, and two measurement schemes -- one with and one without randomization -- that extract this relation from the observed correlations. Our results show that entanglement and coherence, known to be central to quantum information processing, also provide a quantum advantage for causal inference. version:1
arxiv-1406-5035 | Why are images smooth? | http://arxiv.org/abs/1406.5035 | id:1406.5035 author:Uriel Feige category:cs.CV  published:2014-06-19 summary:It is a well observed phenomenon that natural images are smooth, in the sense that nearby pixels tend to have similar values. We describe a mathematical model of images that makes no assumptions on the nature of the environment that images depict. It only assumes that images can be taken at different scales (zoom levels). We provide quantitative bounds on the smoothness of a typical image in our model, as a function of the number of available scales. These bounds can serve as a baseline against which to compare the observed smoothness of natural images. version:1
arxiv-1406-3175 | Fast and Robust Least Squares Estimation in Corrupted Linear Models | http://arxiv.org/abs/1406.3175 | id:1406.3175 author:Brian McWilliams, Gabriel Krummenacher, Mario Lucic, Joachim M. Buhmann category:stat.ML  published:2014-06-12 summary:Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares. version:2
arxiv-1404-4606 | How Many Topics? Stability Analysis for Topic Models | http://arxiv.org/abs/1404.4606 | id:1404.4606 author:Derek Greene, Derek O'Callaghan, Pádraig Cunningham category:cs.LG cs.CL cs.IR  published:2014-04-16 summary:Topic modeling refers to the task of discovering the underlying thematic structure in a text corpus, where the output is commonly presented as a report of the top terms appearing in each topic. Despite the diversity of topic modeling algorithms that have been proposed, a common challenge in successfully applying these techniques is the selection of an appropriate number of topics for a given corpus. Choosing too few topics will produce results that are overly broad, while choosing too many will result in the "over-clustering" of a corpus into many small, highly-similar topics. In this paper, we propose a term-centric stability analysis strategy to address this issue, the idea being that a model with an appropriate number of topics will be more robust to perturbations in the data. Using a topic modeling approach based on matrix factorization, evaluations performed on a range of corpora show that this strategy can successfully guide the model selection process. version:3
arxiv-1404-7796 | Majority Vote of Diverse Classifiers for Late Fusion | http://arxiv.org/abs/1404.7796 | id:1404.7796 author:Emilie Morvant, Amaury Habrard, Stéphane Ayache category:stat.ML cs.LG cs.MM  published:2014-04-30 summary:In the past few years, a lot of attention has been devoted to multimedia indexing by fusing multimodal informations. Two kinds of fusion schemes are generally considered: The early fusion and the late fusion. We focus on late classifier fusion, where one combines the scores of each modality at the decision level. To tackle this problem, we investigate a recent and elegant well-founded quadratic program named MinCq coming from the machine learning PAC-Bayesian theory. MinCq looks for the weighted combination, over a set of real-valued functions seen as voters, leading to the lowest misclassification rate, while maximizing the voters' diversity. We propose an extension of MinCq tailored to multimedia indexing. Our method is based on an order-preserving pairwise loss adapted to ranking that allows us to improve Mean Averaged Precision measure while taking into account the diversity of the voters that we want to fuse. We provide evidence that this method is naturally adapted to late fusion procedures and confirm the good behavior of our approach on the challenging PASCAL VOC'07 benchmark. version:2
arxiv-1406-4877 | On the Application of Generic Summarization Algorithms to Music | http://arxiv.org/abs/1406.4877 | id:1406.4877 author:Francisco Raposo, Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.LG cs.SD H.5.5  published:2014-06-18 summary:Several generic summarization algorithms were developed in the past and successfully applied in fields such as text and speech summarization. In this paper, we review and apply these algorithms to music. To evaluate this summarization's performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier's performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets. We show that Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA) all improve classification performance in both datasets used for testing. version:1
arxiv-1306-2557 | Fast LSTD using stochastic approximation: Finite time analysis and application to traffic control | http://arxiv.org/abs/1306.2557 | id:1306.2557 author:L. A. Prashanth, Nathaniel Korda, Rémi Munos category:cs.LG stat.ML  published:2013-06-11 summary:We propose a stochastic approximation based method with randomisation of samples for policy evaluation using the least squares temporal difference (LSTD) algorithm. Our method results in an $O(d)$ improvement in complexity in comparison to regular LSTD, where $d$ is the dimension of the data. We provide convergence rate results for our proposed method, both in high probability and in expectation. Moreover, we also establish that using our scheme in place of LSTD does not impact the rate of convergence of the approximate value function to the true value function. This result coupled with the low complexity of our method makes it attractive for implementation in big data settings, where $d$ is large. Further, we also analyse a similar low-complexity alternative for least squares regression and provide finite-time bounds there. We demonstrate the practicality of our method for LSTD empirically by combining it with the LSPI algorithm in a traffic signal control application. version:4
arxiv-1406-4784 | Improved Densification of One Permutation Hashing | http://arxiv.org/abs/1406.4784 | id:1406.4784 author:Anshumali Shrivastava, Ping Li category:stat.ME cs.DS cs.IR cs.LG  published:2014-06-18 summary:The existing work on densification of one permutation hashing reduces the query processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing (LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$, where $d$ is the number of nonzeros of the data vector, $K$ is the number of hashes in each hash table, and $L$ is the number of hash tables. While that is a substantial improvement, our analysis reveals that the existing densification scheme is sub-optimal. In particular, there is no enough randomness in that procedure, which affects its accuracy on very sparse datasets. In this paper, we provide a new densification procedure which is provably better than the existing scheme. This improvement is more significant for very sparse datasets which are common over the web. The improved technique has the same cost of $O(d + KL)$ for query processing, thereby making it strictly preferable over the existing procedure. Experimental evaluations on public datasets, in the task of hashing based near neighbor search, support our theoretical findings. version:1
arxiv-1406-4781 | Predictive Modelling of Bone Age through Classification and Regression of Bone Shapes | http://arxiv.org/abs/1406.4781 | id:1406.4781 author:Anthony Bagnall, Luke Davis category:cs.LG physics.med-ph  published:2014-06-18 summary:Bone age assessment is a task performed daily in hospitals worldwide. This involves a clinician estimating the age of a patient from a radiograph of the non-dominant hand. Our approach to automated bone age assessment is to modularise the algorithm into the following three stages: segment and verify hand outline; segment and verify bones; use the bone outlines to construct models of age. In this paper we address the final question: given outlines of bones, can we learn how to predict the bone age of the patient? We examine two alternative approaches. Firstly, we attempt to train classifiers on individual bones to predict the bone stage categories commonly used in bone ageing. Secondly, we construct regression models to directly predict patient age. We demonstrate that models built on summary features of the bone outline perform better than those built using the one dimensional representation of the outline, and also do at least as well as other automated systems. We show that models constructed on just three bones are as accurate at predicting age as expert human assessors using the standard technique. We also demonstrate the utility of the model by quantifying the importance of ethnicity and sex on age development. Our conclusion is that the feature based system of separating the image processing from the age modelling is the best approach for automated bone ageing, since it offers flexibility and transparency and produces accurate estimates. version:1
arxiv-1406-4773 | Deep Learning Face Representation by Joint Identification-Verification | http://arxiv.org/abs/1406.4773 | id:1406.4773 author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-06-18 summary:The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15% face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67%. version:1
arxiv-1406-4770 | Mass Classification Method in Mammogram Using Fuzzy K-Nearest Neighbour Equality | http://arxiv.org/abs/1406.4770 | id:1406.4770 author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV  published:2014-06-18 summary:Mass classification of objects is an important area of research and application in a variety of fields. In this paper, we present an efficient computer aided mass classification method in digitized mammograms using Fuzzy K-Nearest Neighbor Equality, which performs benign or malignant classification on region of interest that contains mass. One of the major mammographic characteristics for mass classification is texture. Fuzzy K-Nearest Neighbor Equality exploits this important factor to classify the mass into benign or malignant. The statistical textural features used in characterizing the masses are Haralick and Run length features. The main aim of the method is to increase the effectiveness and efficiency of the classification process in an objective manner to reduce the numbers of false positive of malignancies. In this paper proposes a novel Fuzzy K-Nearest Neighbor Equality algorithm for classifying the marked regions into benign and malignant and 94.46 sensitivity,96.81 specificity and 96.52 accuracy is achieved that is very much promising compare to the radiologists' accuracy. version:1
arxiv-1406-4757 | An Experimental Evaluation of Nearest Neighbour Time Series Classification | http://arxiv.org/abs/1406.4757 | id:1406.4757 author:Anthony Bagnall, Jason Lines category:cs.LG  published:2014-06-18 summary:Data mining research into time series classification (TSC) has focussed on alternative distance measures for nearest neighbour classifiers. It is standard practice to use 1-NN with Euclidean or dynamic time warping (DTW) distance as a straw man for comparison. As part of a wider investigation into elastic distance measures for TSC~\cite{lines14elastic}, we perform a series of experiments to test whether this standard practice is valid. Specifically, we compare 1-NN classifiers with Euclidean and DTW distance to standard classifiers, examine whether the performance of 1-NN Euclidean approaches that of 1-NN DTW as the number of cases increases, assess whether there is any benefit of setting $k$ for $k$-NN through cross validation whether it is worth setting the warping path for DTW through cross validation and finally is it better to use a window or weighting for DTW. Based on experiments on 77 problems, we conclude that 1-NN with Euclidean distance is fairly easy to beat but 1-NN with DTW is not, if window size is set through cross validation. version:1
arxiv-1406-4710 | Typed Hilbert Epsilon Operators and the Semantics of Determiner Phrases (Invited Lecture) | http://arxiv.org/abs/1406.4710 | id:1406.4710 author:Christian Retoré category:cs.CL cs.AI cs.LO math.LO  published:2014-06-18 summary:The semantics of determiner phrases, be they definite de- scriptions, indefinite descriptions or quantified noun phrases, is often as- sumed to be a fully solved question: common nouns are properties, and determiners are generalised quantifiers that apply to two predicates: the property corresponding to the common noun and the one corresponding to the verb phrase. We first present a criticism of this standard view. Firstly, the semantics of determiners does not follow the syntactical structure of the sentence. Secondly the standard interpretation of the indefinite article cannot ac- count for nominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry between the two properties of a generalised quantifier. In the sequel, we propose a treatment of determiners and quantifiers as Hilbert terms in a richly typed system that we initially developed for lexical semantics, using a many sorted logic for semantical representations. We present this semantical framework called the Montagovian generative lexicon and show how these terms better match the syntactical structure and avoid the aforementioned problems of the standard approach. Hilbert terms rather differ from choice functions in that there is one polymorphic operator and not one operator per formula. They also open an intriguing connection between the logic for meaning assembly, the typed lambda calculus handling compositionality and the many-sorted logic for semantical representations. Furthermore epsilon terms naturally introduce type-judgements and confirm the claim that type judgment are a form of presupposition. version:1
arxiv-1406-4296 | Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams | http://arxiv.org/abs/1406.4296 | id:1406.4296 author:Adrien Gaidon, Gloria Zen, Jose A. Rodriguez-Serrano category:cs.CV cs.LG  published:2014-06-17 summary:Learning object detectors requires massive amounts of labeled training samples from the specific data source of interest. This is impractical when dealing with many different sources (e.g., in camera networks), or constantly changing ones such as mobile cameras (e.g., in robotics or driving assistant systems). In this paper, we address the problem of self-learning detectors in an autonomous manner, i.e. (i) detectors continuously updating themselves to efficiently adapt to streaming data sources (contrary to transductive algorithms), (ii) without any labeled data strongly related to the target data stream (contrary to self-paced learning), and (iii) without manual intervention to set and update hyper-parameters. To that end, we propose an unsupervised, on-line, and self-tuning learning algorithm to optimize a multi-task learning convex objective. Our method uses confident but laconic oracles (high-precision but low-recall off-the-shelf generic detectors), and exploits the structure of the problem to jointly learn on-line an ensemble of instance-level trackers, from which we derive an adapted category-level object detector. Our approach is validated on real-world publicly available video object datasets. version:2
arxiv-1406-4690 | The Frobenius anatomy of word meanings II: possessive relative pronouns | http://arxiv.org/abs/1406.4690 | id:1406.4690 author:Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke category:cs.CL math.CT 18Dxx  18Axx I.2.7; F.4.1  published:2014-06-18 summary:Within the categorical compositional distributional model of meaning, we provide semantic interpretations for the subject and object roles of the possessive relative pronoun `whose'. This is done in terms of Frobenius algebras over compact closed categories. These algebras and their diagrammatic language expose how meanings of words in relative clauses interact with each other. We show how our interpretation is related to Montague-style semantics and provide a truth-theoretic interpretation. We also show how vector spaces provide a concrete interpretation and provide preliminary corpus-based experimental evidence. In a prequel to this paper, we used similar methods and dealt with the case of subject and object relative pronouns. version:1
arxiv-1406-4682 | Exact Decoding on Latent Variable Conditional Models is NP-Hard | http://arxiv.org/abs/1406.4682 | id:1406.4682 author:Xu Sun category:cs.AI cs.CC cs.LG  published:2014-06-18 summary:Latent variable conditional models, including the latent conditional random fields as a special case, are popular models for many natural language processing and vision processing tasks. The computational complexity of the exact decoding/inference in latent conditional random fields is unclear. In this paper, we try to clarify the computational complexity of the exact decoding. We analyze the complexity and demonstrate that it is an NP-hard problem even on a sequential labeling setting. Furthermore, we propose the latent-dynamic inference (LDI-Naive) method and its bounded version (LDI-Bounded), which are able to perform exact-inference or almost-exact-inference by using top-$n$ search and dynamic programming. version:1
arxiv-1406-4472 | Notes on hierarchical ensemble methods for DAG-structured taxonomies | http://arxiv.org/abs/1406.4472 | id:1406.4472 author:Giorgio Valentini category:cs.AI cs.LG stat.ML I.2.6  published:2014-06-17 summary:Several real problems ranging from text classification to computational biology are characterized by hierarchical multi-label classification tasks. Most of the methods presented in literature focused on tree-structured taxonomies, but only few on taxonomies structured according to a Directed Acyclic Graph (DAG). In this contribution novel classification ensemble algorithms for DAG-structured taxonomies are introduced. In particular Hierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs are presented and discussed. version:2
arxiv-1406-4445 | RAPID: Rapidly Accelerated Proximal Gradient Algorithms for Convex Minimization | http://arxiv.org/abs/1406.4445 | id:1406.4445 author:Ziming Zhang, Venkatesh Saligrama category:stat.ML cs.LG math.OC  published:2014-06-13 summary:In this paper, we propose a new algorithm to speed-up the convergence of accelerated proximal gradient (APG) methods. In order to minimize a convex function $f(\mathbf{x})$, our algorithm introduces a simple line search step after each proximal gradient step in APG so that a biconvex function $f(\theta\mathbf{x})$ is minimized over scalar variable $\theta>0$ while fixing variable $\mathbf{x}$. We propose two new ways of constructing the auxiliary variables in APG based on the intermediate solutions of the proximal gradient and the line search steps. We prove that at arbitrary iteration step $t (t\geq1)$, our algorithm can achieve a smaller upper-bound for the gap between the current and optimal objective values than those in the traditional APG methods such as FISTA, making it converge faster in practice. In fact, our algorithm can be potentially applied to many important convex optimization problems, such as sparse linear regression and kernel SVMs. Our experimental results clearly demonstrate that our algorithm converges faster than APG in all of the applications above, even comparable to some sophisticated solvers. version:2
arxiv-1406-4631 | A Sober Look at Spectral Learning | http://arxiv.org/abs/1406.4631 | id:1406.4631 author:Han Zhao, Pascal Poupart category:cs.LG  published:2014-06-18 summary:Spectral learning recently generated lots of excitement in machine learning, largely because it is the first known method to produce consistent estimates (under suitable conditions) for several latent variable models. In contrast, maximum likelihood estimates may get trapped in local optima due to the non-convex nature of the likelihood function of latent variable models. In this paper, we do an empirical evaluation of spectral learning (SL) and expectation maximization (EM), which reveals an important gap between the theory and the practice. First, SL often leads to negative probabilities. Second, EM often yields better estimates than spectral learning and it does not seem to get stuck in local optima. We discuss how the rank of the model parameters and the amount of training data can yield negative probabilities. We also question the common belief that maximum likelihood estimators are necessarily inconsistent. version:1
arxiv-1406-4619 | A Generalized Markov-Chain Modelling Approach to $(1,λ)$-ES Linear Optimization: Technical Report | http://arxiv.org/abs/1406.4619 | id:1406.4619 author:Alexandre Chotard, Martin Holena category:cs.NA cs.LG cs.NE  published:2014-06-18 summary:Several recent publications investigated Markov-chain modelling of linear optimization by a $(1,\lambda)$-ES, considering both unconstrained and linearly constrained optimization, and both constant and varying step size. All of them assume normality of the involved random steps, and while this is consistent with a black-box scenario, information on the function to be optimized (e.g. separability) may be exploited by the use of another distribution. The objective of our contribution is to complement previous studies realized with normal steps, and to give sufficient conditions on the distribution of the random steps for the success of a constant step-size $(1,\lambda)$-ES on the simple problem of a linear function with a linear constraint. The decomposition of a multidimensional distribution into its marginals and the copula combining them is applied to the new distributional assumptions, particular attention being paid to distributions with Archimedean copulas. version:1
arxiv-1406-4580 | Primitives for Dynamic Big Model Parallelism | http://arxiv.org/abs/1406.4580 | id:1406.4580 author:Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A. Gibson, Eric P. Xing category:stat.ML cs.DC cs.LG  published:2014-06-18 summary:When training large machine learning models with many variables or parameters, a single machine is often inadequate since the model may be too large to fit in memory, while training can take a long time even with stochastic updates. A natural recourse is to turn to distributed cluster computing, in order to harness additional memory and processors. However, naive, unstructured parallelization of ML algorithms can make inefficient use of distributed memory, while failing to obtain proportional convergence speedups - or can even result in divergence. We develop a framework of primitives for dynamic model-parallelism, STRADS, in order to explore partitioning and update scheduling of model variables in distributed ML algorithms - thus improving their memory efficiency while presenting new opportunities to speed up convergence without compromising inference correctness. We demonstrate the efficacy of model-parallel algorithms implemented in STRADS versus popular implementations for Topic Modeling, Matrix Factorization and Lasso. version:1
arxiv-1406-4519 | DFacTo: Distributed Factorization of Tensors | http://arxiv.org/abs/1406.4519 | id:1406.4519 author:Joon Hee Choi, S. V. N. Vishwanathan category:stat.ML  published:2014-06-17 summary:We present a technique for significantly speeding up Alternating Least Squares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor factorization. By exploiting properties of the Khatri-Rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms. Our algorithm, DFacTo, only requires two sparse matrix-vector products and is easy to parallelize. DFacTo is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform one iteration of the ALS algorithm and 1,143 seconds to perform one iteration of the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries. version:1
arxiv-1406-4484 | Block matching algorithm based on Harmony Search optimization for motion estimation | http://arxiv.org/abs/1406.4484 | id:1406.4484 author:Erik Cuevas category:cs.CV  published:2014-06-17 summary:Motion estimation is one of the major problems in developing video coding applications. Among all motion estimation approaches, Block-matching (BM) algorithms are the most popular methods due to their effectiveness and simplicity for both software and hardware implementations. A BM approach assumes that the movement of pixels within a defined region of the current frame can be modeled as a translation of pixels contained in the previous frame. In this procedure, the motion vector is obtained by minimizing a certain matching metric that is produced for the current frame over a determined search window from the previous frame. Unfortunately, the evaluation of such matching measurement is computationally expensive and represents the most consuming operation in the BM process. Therefore, BM motion estimation can be viewed as an optimization problem whose goal is to find the best-matching block within a search space. The simplest available BM method is the Full Search Algorithm (FSA) which finds the most accurate motion vector through an exhaustive computation of all the elements of the search space. Recently, several fast BM algorithms have been proposed to reduce the search positions by calculating only a fixed subset of motion vectors despite lowering its accuracy. On the other hand, the Harmony Search (HS) algorithm is a population-based optimization method that is inspired by the music improvisation process in which a musician searches for harmony and continues to polish the pitches to obtain a better harmony. In this paper, a new BM algorithm that combines HS with a fitness approximation model is proposed. The approach uses motion vectors belonging to the search window as potential solutions. A fitness function evaluates the matching quality of each motion vector candidate. version:1
arxiv-1406-4469 | Authorship Attribution through Function Word Adjacency Networks | http://arxiv.org/abs/1406.4469 | id:1406.4469 author:Santiago Segarra, Mark Eisen, Alejandro Ribeiro category:cs.CL cs.LG stat.ML  published:2014-06-17 summary:A method for authorship attribution based on function word adjacency networks (WANs) is introduced. Function words are parts of speech that express grammatical relationships between other words but do not carry lexical meaning on their own. In the WANs in this paper, nodes are function words and directed edges stand in for the likelihood of finding the sink word in the ordered vicinity of the source word. WANs of different authors can be interpreted as transition probabilities of a Markov chain and are therefore compared in terms of their relative entropies. Optimal selection of WAN parameters is studied and attribution accuracy is benchmarked across a diverse pool of authors and varying text lengths. This analysis shows that, since function words are independent of content, their use tends to be specific to an author and that the relational data captured by function WANs is a good summary of stylometric fingerprints. Attribution accuracy is observed to exceed the one achieved by methods that rely on word frequencies alone. Further combining WANs with methods that rely on word frequencies alone, results in larger attribution accuracy, indicating that both sources of information encode different aspects of authorial styles. version:1
arxiv-1402-4501 | A Kernel Independence Test for Random Processes | http://arxiv.org/abs/1402.4501 | id:1402.4501 author:Kacper Chwialkowski, Arthur Gretton category:stat.ML 62G10  published:2014-02-18 summary:A new non parametric approach to the problem of testing the independence of two random process is developed. The test statistic is the Hilbert Schmidt Independence Criterion (HSIC), which was used previously in testing independence for i.i.d pairs of variables. The asymptotic behaviour of HSIC is established when computed from samples drawn from random processes. It is shown that earlier bootstrap procedures which worked in the i.i.d. case will fail for random processes, and an alternative consistent estimate of the p-values is proposed. Tests on artificial data and real-world Forex data indicate that the new test procedure discovers dependence which is missed by linear approaches, while the earlier bootstrap procedure returns an elevated number of false positives. The code is available online: https://github.com/kacperChwialkowski/HSIC . version:3
arxiv-1310-4375 | Fast Computation of Wasserstein Barycenters | http://arxiv.org/abs/1310.4375 | id:1310.4375 author:Marco Cuturi, Arnaud Doucet category:stat.ML  published:2013-10-16 summary:We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem. version:3
arxiv-1405-5047 | Single camera pose estimation using Bayesian filtering and Kinect motion priors | http://arxiv.org/abs/1405.5047 | id:1405.5047 author:Michael Burke, Joan Lasenby category:cs.CV cs.HC  published:2014-05-20 summary:Traditional approaches to upper body pose estimation using monocular vision rely on complex body models and a large variety of geometric constraints. We argue that this is not ideal and somewhat inelegant as it results in large processing burdens, and instead attempt to incorporate these constraints through priors obtained directly from training data. A prior distribution covering the probability of a human pose occurring is used to incorporate likely human poses. This distribution is obtained offline, by fitting a Gaussian mixture model to a large dataset of recorded human body poses, tracked using a Kinect sensor. We combine this prior information with a random walk transition model to obtain an upper body model, suitable for use within a recursive Bayesian filtering framework. Our model can be viewed as a mixture of discrete Ornstein-Uhlenbeck processes, in that states behave as random walks, but drift towards a set of typically observed poses. This model is combined with measurements of the human head and hand positions, using recursive Bayesian estimation to incorporate temporal information. Measurements are obtained using face detection and a simple skin colour hand detector, trained using the detected face. The suggested model is designed with analytical tractability in mind and we show that the pose tracking can be Rao-Blackwellised using the mixture Kalman filter, allowing for computational efficiency while still incorporating bio-mechanical properties of the upper body. In addition, the use of the proposed upper body model allows reliable three-dimensional pose estimates to be obtained indirectly for a number of joints that are often difficult to detect using traditional object recognition strategies. Comparisons with Kinect sensor results and the state of the art in 2D pose estimation highlight the efficacy of the proposed approach. version:2
arxiv-1406-4311 | Sparse Estimation with the Swept Approximated Message-Passing Algorithm | http://arxiv.org/abs/1406.4311 | id:1406.4311 author:Andre Manoel, Florent Krzakala, Eric W. Tramel, Lenka Zdeborová category:cs.IT cond-mat.dis-nn math.IT physics.data-an stat.ML  published:2014-06-17 summary:Approximate Message Passing (AMP) has been shown to be a superior method for inference problems, such as the recovery of signals from sets of noisy, lower-dimensionality measurements, both in terms of reconstruction accuracy and in computational efficiency. However, AMP suffers from serious convergence issues in contexts that do not exactly match its assumptions. We propose a new approach to stabilizing AMP in these contexts by applying AMP updates to individual coefficients rather than in parallel. Our results show that this change to the AMP iteration can provide theoretically expected, but hitherto unobtainable, performance for problems on which the standard AMP iteration diverges. Additionally, we find that the computational costs of this swept coefficient update scheme is not unduly burdensome, allowing it to be applied efficiently to signals of large dimensionality. version:1
arxiv-1312-6042 | Learning States Representations in POMDP | http://arxiv.org/abs/1312.6042 | id:1312.6042 author:Gabriella Contardo, Ludovic Denoyer, Thierry Artieres, Patrick Gallinari category:cs.LG  published:2013-12-20 summary:We propose to deal with sequential processes where only partial observations are available by learning a latent representation space on which policies may be accurately learned. version:4
arxiv-1406-4237 | An Evolutionary Approach for Optimal Citing and Sizing of Micro-Grid in Radial Distribution Systems | http://arxiv.org/abs/1406.4237 | id:1406.4237 author:Eswari. J, Dr. S. Jeyadevi category:cs.NE  published:2014-06-17 summary:This Paper presents the methodology of penetration of Micro-Grids (MG) in the radial distribution system (RDS). The aim of this paper is to minimize a total real power loss that descends the performance of the radial distribution system by integrating various renewable resources as Distributed Generation (DG). The combination of different types of renewable energy resources contributes a sustainable MG. These resources are optimally sized and located using evolutionary approach in various penetration levels. The optimal solutions are experimented with IEEE 33 radial distribution system using Particle Swarm Optimization (PSO) technique. The results are quite promising and authenticate its potential to solve problem in radial distribution system effectively. version:1
arxiv-1406-4211 | Mapping the Economic Crisis: Some Preliminary Investigations | http://arxiv.org/abs/1406.4211 | id:1406.4211 author:Pierre Bourreau, Thierry Poibeau category:cs.CL  published:2014-06-17 summary:In this paper we describe our contribution to the PoliInformatics 2014 Challenge on the 2007-2008 financial crisis. We propose a state of the art technique to extract information from texts and provide different representations, giving first a static overview of the domain and then a dynamic representation of its main evolutions. We show that this strategy provides a practical solution to some recent theories in social sciences that are facing a lack of methods and tools to automatically extract information from natural language texts. version:1
arxiv-1406-4205 | Replicating Kernels with a Short Stride Allows Sparse Reconstructions with Fewer Independent Kernels | http://arxiv.org/abs/1406.4205 | id:1406.4205 author:Peter F. Schultz, Dylan M. Paiton, Wei Lu, Garrett T. Kenyon category:q-bio.QM cs.CV  published:2014-06-17 summary:In sparse coding it is common to tile an image into nonoverlapping patches, and then use a dictionary to create a sparse representation of each tile independently. In this situation, the overcompleteness of the dictionary is the number of dictionary elements divided by the patch size. In deconvolutional neural networks (DCNs), dictionaries learned on nonoverlapping tiles are replaced by a family of convolution kernels. Hence adjacent points in the feature maps (V1 layers) have receptive fields in the image that are translations of each other. The translational distance is determined by the dimensions of V1 in comparison to the dimensions of the image space. We refer to this translational distance as the stride. We implement a type of DCN using a modified Locally Competitive Algorithm (LCA) to investigate the relationship between the number of kernels, the stride, the receptive field size, and the quality of reconstruction. We find, for example, that for 16x16-pixel receptive fields, using eight kernels and a stride of 2 leads to sparse reconstructions of comparable quality as using 512 kernels and a stride of 16 (the nonoverlapping case). We also find that for a given stride and number of kernels, the patch size does not significantly affect reconstruction quality. Instead, the learned convolution kernels have a natural support radius independent of the patch size. version:1
arxiv-1406-4203 | Construction of non-convex polynomial loss functions for training a binary classifier with quantum annealing | http://arxiv.org/abs/1406.4203 | id:1406.4203 author:Ryan Babbush, Vasil Denchev, Nan Ding, Sergei Isakov, Hartmut Neven category:cs.LG quant-ph  published:2014-06-17 summary:Quantum annealing is a heuristic quantum algorithm which exploits quantum resources to minimize an objective function embedded as the energy levels of a programmable physical system. To take advantage of a potential quantum advantage, one needs to be able to map the problem of interest to the native hardware with reasonably low overhead. Because experimental considerations constrain our objective function to take the form of a low degree PUBO (polynomial unconstrained binary optimization), we employ non-convex loss functions which are polynomial functions of the margin. We show that these loss functions are robust to label noise and provide a clear advantage over convex methods. These loss functions may also be useful for classical approaches as they compile to regularized risk expressions which can be evaluated in constant time with respect to the number of training examples. version:1
arxiv-1403-5045 | Matroid Bandits: Fast Combinatorial Optimization with Learning | http://arxiv.org/abs/1403.5045 | id:1403.5045 author:Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, Brian Eriksson category:cs.LG cs.AI cs.SY stat.ML  published:2014-03-20 summary:A matroid is a notion of independence in combinatorial optimization which is closely related to computational efficiency. In particular, it is well known that the maximum of a constrained modular function can be found greedily if and only if the constraints are associated with a matroid. In this paper, we bring together the ideas of bandits and matroids, and propose a new class of combinatorial bandits, matroid bandits. The objective in these problems is to learn how to maximize a modular function on a matroid. This function is stochastic and initially unknown. We propose a practical algorithm for solving our problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds, gap-dependent and gap-free, on its regret. Both bounds are sublinear in time and at most linear in all other quantities of interest. The gap-dependent upper bound is tight and we prove a matching lower bound on a partition matroid bandit. Finally, we evaluate our method on three real-world problems and show that it is practical. version:3
arxiv-1406-4057 | Embedded Controlled Languages | http://arxiv.org/abs/1406.4057 | id:1406.4057 author:Aarne Ranta category:cs.CL  published:2014-06-16 summary:Inspired by embedded programming languages, an embedded CNL (controlled natural language) is a proper fragment of an entire natural language (its host language), but it has a parser that recognizes the entire host language. This makes it possible to process out-of-CNL input and give useful feedback to users, instead of just reporting syntax errors. This extended abstract explains the main concepts of embedded CNL implementation in GF (Grammatical Framework), with examples from machine translation and some other ongoing work. version:1
arxiv-1310-5438 | Variational Bayesian inference for linear and logistic regression | http://arxiv.org/abs/1310.5438 | id:1310.5438 author:Jan Drugowitsch category:stat.ML  published:2013-10-21 summary:The article describe the model, derivation, and implementation of variational Bayesian inference for linear and logistic regression, both with and without automatic relevance determination. It has the dual function of acting as a tutorial for the derivation of variational Bayesian inference for simple models, as well as documenting, and providing brief examples for the MATLAB functions that implement this inference. These functions are freely available online. version:2
arxiv-1209-0738 | Sparse coding for multitask and transfer learning | http://arxiv.org/abs/1209.0738 | id:1209.0738 author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:cs.LG stat.ML  published:2012-09-04 summary:We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping. version:3
arxiv-1406-4007 | Impact of Exponent Parameter Value for the Partition Matrix on the Performance of Fuzzy C Means Algorithm | http://arxiv.org/abs/1406.4007 | id:1406.4007 author:Dibya Jyoti Bora, Anil Kumar Gupta category:cs.CV  published:2014-06-16 summary:Soft Clustering plays a very important rule on clustering real world data where a data item contributes to more than one cluster. Fuzzy logic based algorithms are always suitable for performing soft clustering tasks. Fuzzy C Means (FCM) algorithm is a very popular fuzzy logic based algorithm. In case of fuzzy logic based algorithm, the parameter like exponent for the partition matrix that we have to fix for the clustering task plays a very important rule on the performance of the algorithm. In this paper, an experimental analysis is done on FCM algorithm to observe the impact of this parameter on the performance of the algorithm. version:1
arxiv-1311-4029 | Blind Deconvolution with Non-local Sparsity Reweighting | http://arxiv.org/abs/1311.4029 | id:1311.4029 author:Dilip Krishnan, Joan Bruna, Rob Fergus category:cs.CV  published:2013-11-16 summary:Blind deconvolution has made significant progress in the past decade. Most successful algorithms are classified either as Variational or Maximum a-Posteriori ($MAP$). In spite of the superior theoretical justification of variational techniques, carefully constructed $MAP$ algorithms have proven equally effective in practice. In this paper, we show that all successful $MAP$ and variational algorithms share a common framework, relying on the following key principles: sparsity promotion in the gradient domain, $l_2$ regularization for kernel estimation, and the use of convex (often quadratic) cost functions. Our observations lead to a unified understanding of the principles required for successful blind deconvolution. We incorporate these principles into a novel algorithm that improves significantly upon the state of the art. version:2
arxiv-1406-3987 | Towards an Error Correction Memory to Enhance Technical Texts Authoring in LELIE | http://arxiv.org/abs/1406.3987 | id:1406.3987 author:Juyeon Kang, Patrick Saint Dizier category:cs.CL  published:2014-06-16 summary:In this paper, we investigate and experiment the notion of error correction memory applied to error correction in technical texts. The main purpose is to induce relatively generic correction patterns associated with more contextual correction recommendations, based on previously memorized and analyzed corrections. The notion of error correction memory is developed within the framework of the LELIE project and illustrated on the case of fuzzy lexical items, which is a major problem in technical texts. version:1
arxiv-1406-3976 | Handling non-compositionality in multilingual CNLs | http://arxiv.org/abs/1406.3976 | id:1406.3976 author:Ramona Enache, Inari Listenmaa, Prasanth Kolachina category:cs.CL  published:2014-06-16 summary:In this paper, we describe methods for handling multilingual non-compositional constructions in the framework of GF. We specifically look at methods to detect and extract non-compositional phrases from parallel texts and propose methods to handle such constructions in GF grammars. We expect that the methods to handle non-compositional constructions will enrich CNLs by providing more flexibility in the design of controlled languages. We look at two specific use cases of non-compositional constructions: a general-purpose method to detect and extract multilingual multiword expressions and a procedure to identify nominal compounds in German. We evaluate our procedure for multiword expressions by performing a qualitative analysis of the results. For the experiments on nominal compounds, we incorporate the detected compounds in a full SMT pipeline and evaluate the impact of our method in machine translation process. version:1
arxiv-1406-3969 | Translation Of Telugu-Marathi and Vice-Versa using Rule Based Machine Translation | http://arxiv.org/abs/1406.3969 | id:1406.3969 author:Siddhartha Ghosh, Sujata Thamke, Kalyani U. R. S category:cs.CL  published:2014-06-16 summary:In todays digital world automated Machine Translation of one language to another has covered a long way to achieve different kinds of success stories. Whereas Babel Fish supports a good number of foreign languages and only Hindi from Indian languages, the Google Translator takes care of about 10 Indian languages. Though most of the Automated Machine Translation Systems are doing well but handling Indian languages needs a major care while handling the local proverbs/ idioms. Most of the Machine Translation system follows the direct translation approach while translating one Indian language to other. Our research at KMIT R&D Lab found that handling the local proverbs/idioms is not given enough attention by the earlier research work. This paper focuses on two of the majorly spoken Indian languages Marathi and Telugu, and translation between them. Handling proverbs and idioms of both the languages have been given a special care, and the research outcome shows a significant achievement in this direction. version:1
arxiv-1406-3949 | A Fusion of Labeled-Grid Shape Descriptors with Weighted Ranking Algorithm for Shapes Recognition | http://arxiv.org/abs/1406.3949 | id:1406.3949 author:Jamil Ahmad, Zahoor Jan, Zia-ud-Din, Shoaib Muhammad Khan category:cs.CV  published:2014-06-16 summary:Retrieving similar images from a large dataset based on the image content has been a very active research area and is a very challenging task. Studies have shown that retrieving similar images based on their shape is a very effective method. For this purpose a large number of methods exist in literature. The combination of more than one feature has also been investigated for this purpose and has shown promising results. In this paper a fusion based shapes recognition method has been proposed. A set of local boundary based and region based features are derived from the labeled grid based representation of the shape and are combined with a few global shape features to produce a composite shape descriptor. This composite shape descriptor is then used in a weighted ranking algorithm to find similarities among shapes from a large dataset. The experimental analysis has shown that the proposed method is powerful enough to discriminate the geometrically similar shapes from the non-similar ones. version:1
arxiv-1406-3926 | Bayesian Optimal Control of Smoothly Parameterized Systems: The Lazy Posterior Sampling Algorithm | http://arxiv.org/abs/1406.3926 | id:1406.3926 author:Yasin Abbasi-Yadkori, Csaba Szepesvari category:cs.LG stat.ML  published:2014-06-16 summary:We study Bayesian optimal control of a general class of smoothly parameterized Markov decision problems. Since computing the optimal control is computationally expensive, we design an algorithm that trades off performance for computational efficiency. The algorithm is a lazy posterior sampling method that maintains a distribution over the unknown parameter. The algorithm changes its policy only when the variance of the distribution is reduced sufficiently. Importantly, we analyze the algorithm and show the precise nature of the performance vs. computation tradeoff. Finally, we show the effectiveness of the method on a web server control application. version:1
arxiv-1406-3915 | A Bengali HMM Based Speech Synthesis System | http://arxiv.org/abs/1406.3915 | id:1406.3915 author:Sankar Mukherjee, Shyamal Kumar Das Mandal category:cs.SD cs.CL cs.MM  published:2014-06-16 summary:The paper presents the capability of an HMM-based TTS system to produce Bengali speech. In this synthesis method, trajectories of speech parameters are generated from the trained Hidden Markov Models. A final speech waveform is synthesized from those speech parameters. In our experiments, spectral properties were represented by Mel Cepstrum Coefficients. Both the training and synthesis issues are investigated in this paper using annotated Bengali speech database. Experimental evaluation depicts that the developed text-to-speech system is capable of producing adequately natural speech in terms of intelligibility and intonation for Bengali. version:1
arxiv-1406-3906 | Human-Machine CRFs for Identifying Bottlenecks in Holistic Scene Understanding | http://arxiv.org/abs/1406.3906 | id:1406.3906 author:Roozbeh Mottaghi, Sanja Fidler, Alan Yuille, Raquel Urtasun, Devi Parikh category:cs.CV  published:2014-06-16 summary:Recent trends in image understanding have pushed for holistic scene understanding models that jointly reason about various tasks such as object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers. In this work, we are interested in understanding the roles of these different tasks in improved scene understanding, in particular semantic segmentation, object detection and scene recognition. Towards this goal, we "plug-in" human subjects for each of the various components in a state-of-the-art conditional random field model. Comparisons among various hybrid human-machine CRFs give us indications of how much "head room" there is to improve scene understanding by focusing research efforts on various individual tasks. version:1
arxiv-1406-3896 | Freeze-Thaw Bayesian Optimization | http://arxiv.org/abs/1406.3896 | id:1406.3896 author:Kevin Swersky, Jasper Snoek, Ryan Prescott Adams category:stat.ML cs.LG  published:2014-06-16 summary:In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice. version:1
arxiv-1406-3895 | The Laplacian K-modes algorithm for clustering | http://arxiv.org/abs/1406.3895 | id:1406.3895 author:Weiran Wang, Miguel Á. Carreira-Perpiñán category:cs.LG stat.ME stat.ML  published:2014-06-16 summary:In addition to finding meaningful clusters, centroid-based clustering algorithms such as K-means or mean-shift should ideally find centroids that are valid patterns in the input space, representative of data in their cluster. This is challenging with data having a nonconvex or manifold structure, as with images or text. We introduce a new algorithm, Laplacian K-modes, which naturally combines three powerful ideas in clustering: the explicit use of assignment variables (as in K-means); the estimation of cluster centroids which are modes of each cluster's density estimate (as in mean-shift); and the regularizing effect of the graph Laplacian, which encourages similar assignments for nearby points (as in spectral clustering). The optimization algorithm alternates an assignment step, which is a convex quadratic program, and a mean-shift step, which separates for each cluster centroid. The algorithm finds meaningful density estimates for each cluster, even with challenging problems where the clusters have manifold structure, are highly nonconvex or in high dimension. It also provides centroids that are valid patterns, truly representative of their cluster (unlike K-means), and an out-of-sample mapping that predicts soft assignments for a new point. version:1
arxiv-1208-5842 | Tenacious tagging of images via Mellin monomials | http://arxiv.org/abs/1208.5842 | id:1208.5842 author:Kieran G. Larkin, Peter A. Fletcher, Stephen J. Hardy category:cs.CV math.CA  published:2012-08-29 summary:We describe a method for attaching persistent metadata to an image. The method can be interpreted as a template-based blind watermarking scheme, robust to common editing operations, namely: cropping, rotation, scaling, stretching, shearing, compression, printing, scanning, noise, and color removal. Robustness is achieved through the reciprocity of the embedding and detection invariants. The embedded patterns are real onedimensional Mellin monomial patterns distributed over two-dimensions. The embedded patterns are scale invariant and can be directly embedded in an image by simple pixel addition. Detection achieves rotation and general affine invariance by signal projection using implicit Radon transformation. Embedded signals contract to one-dimension in the two-dimensional Fourier polar domain. The real signals are detected by correlation with complex Mellin monomial templates. Using a unique template of 4 chirp patterns we detect the affine signature with exquisite sensitivity and moderate security. The practical implementation achieves efficiencies through fast Fourier transform (FFT) correspondences such as the projection-slice theorem, the FFT correlation relation, and fast resampling via the chirp-z transform. The overall method utilizes orthodox spread spectrum patterns for the payload and performs well in terms of the classic robustness-capacity-visibility performance triangle. Tags are entirely imperceptible with a mean SSIM greater than 0.988 in all cases tested. Watermarked images survive almost all Stirmark attacks. The method is ideal for attaching metadata robustly to both digital and analogue images. version:5
arxiv-1406-3884 | Learning An Invariant Speech Representation | http://arxiv.org/abs/1406.3884 | id:1406.3884 author:Georgios Evangelopoulos, Stephen Voinea, Chiyuan Zhang, Lorenzo Rosasco, Tomaso Poggio category:cs.SD cs.LG  published:2014-06-16 summary:Recognition of speech, and in particular the ability to generalize and learn from small sets of labelled examples like humans do, depends on an appropriate representation of the acoustic input. We formulate the problem of finding robust speech features for supervised learning with small sample complexity as a problem of learning representations of the signal that are maximally invariant to intraclass transformations and deformations. We propose an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluate its validity for voiced speech sound classification. Our version of the theory requires the memory-based, unsupervised storage of acoustic templates -- such as specific phones or words -- together with all the transformations of each that normally occur. A quasi-invariant representation for a speech segment can be obtained by projecting it to each template orbit, i.e., the set of transformed signals, and computing the associated one-dimensional empirical probability distributions. The computations can be performed by modules of filtering and pooling, and extended to hierarchical architectures. In this paper, we apply a single-layer, multicomponent representation for phonemes and demonstrate improved accuracy and decreased sample complexity for vowel classification compared to standard spectral, cepstral and perceptual features. version:1
arxiv-1406-3855 | Human language reveals a universal positivity bias | http://arxiv.org/abs/1406.3855 | id:1406.3855 author:Peter Sheridan Dodds, Eric M. Clark, Suma Desu, Morgan R. Frank, Andrew J. Reagan, Jake Ryland Williams, Lewis Mitchell, Kameron Decker Harris, Isabel M. Kloumann, James P. Bagrow, Karine Megerdoomian, Matthew T. McMahon, Brian F. Tivnan, Christopher M. Danforth category:physics.soc-ph cs.CL cs.SI  published:2014-06-15 summary:Using human evaluation of 100,000 words spread across 24 corpora in 10 languages diverse in origin and culture, we present evidence of a deep imprint of human sociality in language, observing that (1) the words of natural human language possess a universal positivity bias; (2) the estimated emotional content of words is consistent between languages under translation; and (3) this positivity bias is strongly independent of frequency of word usage. Alongside these general regularities, we describe inter-language variations in the emotional spectrum of languages which allow us to rank corpora. We also show how our word evaluations can be used to construct physical-like instruments for both real-time and offline measurement of the emotional content of large-scale texts. version:1
arxiv-1406-3843 | Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models | http://arxiv.org/abs/1406.3843 | id:1406.3843 author:Yichuan Zhang, Charles Sutton category:stat.CO cs.AI cs.LG  published:2014-06-15 summary:Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC. version:1
arxiv-1406-3840 | Optimal Resource Allocation with Semi-Bandit Feedback | http://arxiv.org/abs/1406.3840 | id:1406.3840 author:Tor Lattimore, Koby Crammer, Csaba Szepesvári category:cs.LG  published:2014-06-15 summary:We study a sequential resource allocation problem involving a fixed number of recurring jobs. At each time-step the manager should distribute available resources among the jobs in order to maximise the expected number of completed jobs. Allocating more resources to a given job increases the probability that it completes, but with a cut-off. Specifically, we assume a linear model where the probability increases linearly until it equals one, after which allocating additional resources is wasteful. We assume the difficulty of each job is unknown and present the first algorithm for this problem and prove upper and lower bounds on its regret. Despite its apparent simplicity, the problem has a rich structure: we show that an appropriate optimistic algorithm can improve its learning speed dramatically beyond the results one normally expects for similar problems as the problem becomes resource-laden. version:1
arxiv-1406-3837 | An Incremental Reseeding Strategy for Clustering | http://arxiv.org/abs/1406.3837 | id:1406.3837 author:Xavier Bresson, Huiyi Hu, Thomas Laurent, Arthur Szlam, James von Brecht category:stat.ML cs.LG  published:2014-06-15 summary:In this work we propose a simple and easily parallelizable algorithm for multiway graph partitioning. The algorithm alternates between three basic components: diffusing seed vertices over the graph, thresholding the diffused seeds, and then randomly reseeding the thresholded clusters. We demonstrate experimentally that the proper combination of these ingredients leads to an algorithm that achieves state-of-the-art performance in terms of cluster purity on standard benchmarks datasets. Moreover, the algorithm runs an order of magnitude faster than the other algorithms that achieve comparable results in terms of accuracy. We also describe a coarsen, cluster and refine approach similar to GRACLUS and METIS that removes an additional order of magnitude from the runtime of our algorithm while still maintaining competitive accuracy. version:1
arxiv-1406-3830 | Modelling, Visualising and Summarising Documents with a Single Convolutional Neural Network | http://arxiv.org/abs/1406.3830 | id:1406.3830 author:Misha Denil, Alban Demiraj, Nal Kalchbrenner, Phil Blunsom, Nando de Freitas category:cs.CL cs.LG stat.ML  published:2014-06-15 summary:Capturing the compositional process which maps the meaning of words to that of documents is a central challenge for researchers in Natural Language Processing and Information Retrieval. We introduce a model that is able to represent the meaning of documents by embedding them in a low dimensional vector space, while preserving distinctions of word and sentence order crucial for capturing nuanced semantics. Our model is based on an extended Dynamic Convolution Neural Network, which learns convolution filters at both the sentence and document level, hierarchically learning to capture and compose low level lexical features into high level semantic concepts. We demonstrate the effectiveness of this model on a range of document modelling tasks, achieving strong results with no feature engineering and with a more compact model. Inspired by recent advances in visualising deep convolution networks for computer vision, we present a novel visualisation technique for our document networks which not only provides insight into their learning process, but also can be interpreted to produce a compelling automatic summarisation system for texts. version:1
arxiv-1406-4518 | A Heuristic Method to Generate Better Initial Population for Evolutionary Methods | http://arxiv.org/abs/1406.4518 | id:1406.4518 author:Erfan Khaji, Amin Satlikh Mohammadi category:cs.NE  published:2014-06-15 summary:Initial population plays an important role in heuristic algorithms such as GA as it help to decrease the time those algorithms need to achieve an acceptable result. Furthermore, it may influence the quality of the final answer given by evolutionary algorithms. In this paper, we shall introduce a heuristic method to generate a target based initial population which possess two mentioned characteristics. The efficiency of the proposed method has been shown by presenting the results of our tests on the benchmarks. version:1
arxiv-1406-3816 | Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning | http://arxiv.org/abs/1406.3816 | id:1406.3816 author:Francesco Orabona category:cs.LG stat.ML  published:2014-06-15 summary:Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function, using the range space of the fractional integral operator associated with the kernel. version:1
arxiv-1406-3793 | Neural tuning size is a key factor underlying holistic face processing | http://arxiv.org/abs/1406.3793 | id:1406.3793 author:Cheston Tan, Tomaso Poggio category:cs.AI cs.CV cs.NE q-bio.NC  published:2014-06-15 summary:Faces are a class of visual stimuli with unique significance, for a variety of reasons. They are ubiquitous throughout the course of a person's life, and face recognition is crucial for daily social interaction. Faces are also unlike any other stimulus class in terms of certain physical stimulus characteristics. Furthermore, faces have been empirically found to elicit certain characteristic behavioral phenomena, which are widely held to be evidence of "holistic" processing of faces. However, little is known about the neural mechanisms underlying such holistic face processing. In other words, for the processing of faces by the primate visual system, the input and output characteristics are relatively well known, but the internal neural computations are not. The main aim of this work is to further the fundamental understanding of what causes the visual processing of faces to be different from that of objects. In this computational modeling work, we show that a single factor - "neural tuning size" - is able to account for three key phenomena that are characteristic of face processing, namely the Composite Face Effect (CFE), Face Inversion Effect (FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle provides specific neural tuning properties that correspond to the poorly-understood notion of holistic face processing, and connects these neural properties to psychophysical behavior. Overall, our work provides a unified and parsimonious theoretical account for the disparate empirical data on face-specific processing, deepening the fundamental understanding of face processing. version:1
arxiv-1406-3792 | Interval Forecasting of Electricity Demand: A Novel Bivariate EMD-based Support Vector Regression Modeling Framework | http://arxiv.org/abs/1406.3792 | id:1406.3792 author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.LG stat.AP  published:2014-06-15 summary:Highly accurate interval forecasting of electricity demand is fundamental to the success of reducing the risk when making power system planning and operational decisions by providing a range rather than point estimation. In this study, a novel modeling framework integrating bivariate empirical mode decomposition (BEMD) and support vector regression (SVR), extended from the well-established empirical mode decomposition (EMD) based time series modeling framework in the energy demand forecasting literature, is proposed for interval forecasting of electricity demand. The novelty of this study arises from the employment of BEMD, a new extension of classical empirical model decomposition (EMD) destined to handle bivariate time series treated as complex-valued time series, as decomposition method instead of classical EMD only capable of decomposing one-dimensional single-valued time series. This proposed modeling framework is endowed with BEMD to decompose simultaneously both the lower and upper bounds time series, constructed in forms of complex-valued time series, of electricity demand on a monthly per hour basis, resulting in capturing the potential interrelationship between lower and upper bounds. The proposed modeling framework is justified with monthly interval-valued electricity demand data per hour in Pennsylvania-New Jersey-Maryland Interconnection, indicating it as a promising method for interval-valued electricity demand forecasting. version:1
arxiv-1405-7102 | Detection Bank: An Object Detection Based Video Representation for Multimedia Event Recognition | http://arxiv.org/abs/1405.7102 | id:1405.7102 author:Tim Althoff, Hyun Oh Song, Trevor Darrell category:cs.MM cs.CV  published:2014-05-28 summary:While low-level image features have proven to be effective representations for visual recognition tasks such as object recognition and scene classification, they are inadequate to capture complex semantic meaning required to solve high-level visual tasks such as multimedia event detection and recognition. Recognition or retrieval of events and activities can be improved if specific discriminative objects are detected in a video sequence. In this paper, we propose an image representation, called Detection Bank, based on the detection images from a large number of windowed object detectors where an image is represented by different statistics derived from these detections. This representation is extended to video by aggregating the key frame level image representations through mean and max pooling. We empirically show that it captures complementary information to state-of-the-art representations such as Spatial Pyramid Matching and Object Bank. These descriptors combined with our Detection Bank representation significantly outperforms any of the representations alone on TRECVID MED 2011 data. version:2
arxiv-1406-3726 | Evaluation of Machine Learning Techniques for Green Energy Prediction | http://arxiv.org/abs/1406.3726 | id:1406.3726 author:Ankur Sahai category:cs.LG  published:2014-06-14 summary:We evaluate the following Machine Learning techniques for Green Energy (Wind, Solar) Prediction: Bayesian Inference, Neural Networks, Support Vector Machines, Clustering techniques (PCA). Our objective is to predict green energy using weather forecasts, predict deviations from forecast green energy, find correlation amongst different weather parameters and green energy availability, recover lost or missing energy (/ weather) data. We use historical weather data and weather forecasts for the same. version:1
arxiv-1406-3714 | Mining of product reviews at aspect level | http://arxiv.org/abs/1406.3714 | id:1406.3714 author:Richa Sharma, Shweta Nigam, Rekha Jain category:cs.CL cs.IR  published:2014-06-14 summary:Todays world is a world of Internet, almost all work can be done with the help of it, from simple mobile phone recharge to biggest business deals can be done with the help of this technology. People spent their most of the times on surfing on the Web it becomes a new source of entertainment, education, communication, shopping etc. Users not only use these websites but also give their feedback and suggestions that will be useful for other users. In this way a large amount of reviews of users are collected on the Web that needs to be explored, analyse and organized for better decision making. Opinion Mining or Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the users views or opinions explained in the form of positive, negative or neutral comments and quotes underlying the text. Aspect based opinion mining is one of the level of Opinion mining that determines the aspect of the given reviews and classify the review for each feature. In this paper an aspect based opinion mining system is proposed to classify the reviews as positive, negative and neutral for each feature. Negation is also handled in the proposed system. Experimental results using reviews of products show the effectiveness of the system. version:1
arxiv-1406-3711 | Dimensionality reduction for time series data | http://arxiv.org/abs/1406.3711 | id:1406.3711 author:Diego Vidaurre, Iead Rezek, Samuel L. Harrison, Stephen S. Smith, Mark Woolrich category:stat.ML  published:2014-06-14 summary:Despite the fact that they do not consider the temporal nature of data, classic dimensionality reduction techniques, such as PCA, are widely applied to time series data. In this paper, we introduce a factor decomposition specific for time series that builds upon the Bayesian multivariate autoregressive model and hence evades the assumption that data points are mutually independent. The key is to find a low-rank estimation of the autoregressive matrices. As in the probabilistic version of other factor models, this induces a latent low-dimensional representation of the original data. We discuss some possible generalisations and alternatives, with the most relevant being a technique for simultaneous smoothing and dimensionality reduction. To illustrate the potential applications, we apply the model on a synthetic data set and different types of neuroimaging data (EEG and ECoG). version:1
arxiv-1406-3692 | Analyzing Social and Stylometric Features to Identify Spear phishing Emails | http://arxiv.org/abs/1406.3692 | id:1406.3692 author:Prateek Dewan, Anand Kashyap, Ponnurangam Kumaraguru category:cs.CY cs.LG cs.SI  published:2014-06-14 summary:Spear phishing is a complex targeted attack in which, an attacker harvests information about the victim prior to the attack. This information is then used to create sophisticated, genuine-looking attack vectors, drawing the victim to compromise confidential information. What makes spear phishing different, and more powerful than normal phishing, is this contextual information about the victim. Online social media services can be one such source for gathering vital information about an individual. In this paper, we characterize and examine a true positive dataset of spear phishing, spam, and normal phishing emails from Symantec's enterprise email scanning service. We then present a model to detect spear phishing emails sent to employees of 14 international organizations, by using social features extracted from LinkedIn. Our dataset consists of 4,742 targeted attack emails sent to 2,434 victims, and 9,353 non targeted attack emails sent to 5,912 non victims; and publicly available information from their LinkedIn profiles. We applied various machine learning algorithms to this labeled data, and achieved an overall maximum accuracy of 97.76% in identifying spear phishing emails. We used a combination of social features from LinkedIn profiles, and stylometric features extracted from email subjects, bodies, and attachments. However, we achieved a slightly better accuracy of 98.28% without the social features. Our analysis revealed that social features extracted from LinkedIn do not help in identifying spear phishing emails. To the best of our knowledge, this is one of the first attempts to make use of a combination of stylometric features extracted from emails, and social features extracted from an online social network to detect targeted spear phishing emails. version:1
arxiv-1406-1774 | Small Sample Learning of Superpixel Classifiers for EM Segmentation- Extended Version | http://arxiv.org/abs/1406.1774 | id:1406.1774 author:Toufiq Parag, Stephen Plaza, Louis Scheffer category:cs.CV  published:2014-06-06 summary:Pixel and superpixel classifiers have become essential tools for EM segmentation algorithms. Training these classifiers remains a major bottleneck primarily due to the requirement of completely annotating the dataset which is tedious, error-prone and costly. In this paper, we propose an interactive learning scheme for the superpixel classifier for EM segmentation. Our algorithm is "active semi-supervised" because it requests the labels of a small number of examples from user and applies label propagation technique to generate these queries. Using only a small set ($<20\%$) of all datapoints, the proposed algorithm consistently generates a classifier almost as accurate as that estimated from a complete groundtruth. We provide segmentation results on multiple datasets to show the strength of these classifiers. version:2
arxiv-1406-3587 | Quaternion Gradient and Hessian | http://arxiv.org/abs/1406.3587 | id:1406.3587 author:Dongpo Xu, Danilo P. Mandic category:math.NA cs.LG  published:2014-06-13 summary:The optimization of real scalar functions of quaternion variables, such as the mean square error or array output power, underpins many practical applications. Solutions often require the calculation of the gradient and Hessian, however, real functions of quaternion variables are essentially non-analytic. To address this issue, we propose new definitions of quaternion gradient and Hessian, based on the novel generalized HR (GHR) calculus, thus making possible efficient derivation of optimization algorithms directly in the quaternion field, rather than transforming the problem to the real domain, as is current practice. In addition, unlike the existing quaternion gradients, the GHR calculus allows for the product and chain rule, and for a one-to-one correspondence of the proposed quaternion gradient and Hessian with their real counterparts. Properties of the quaternion gradient and Hessian relevant to numerical applications are elaborated, and the results illuminate the usefulness of the GHR calculus in greatly simplifying the derivation of the quaternion least mean squares, and in quaternion least square and Newton algorithm. The proposed gradient and Hessian are also shown to enable the same generic forms as the corresponding real- and complex-valued algorithms, further illustrating the advantages in algorithm design and evaluation. version:1
arxiv-1406-3496 | EigenEvent: An Algorithm for Event Detection from Complex Data Streams in Syndromic Surveillance | http://arxiv.org/abs/1406.3496 | id:1406.3496 author:Hadi Fanaee-T, João Gama category:cs.AI cs.LG stat.AP  published:2014-06-13 summary:Syndromic surveillance systems continuously monitor multiple pre-diagnostic daily streams of indicators from different regions with the aim of early detection of disease outbreaks. The main objective of these systems is to detect outbreaks hours or days before the clinical and laboratory confirmation. The type of data that is being generated via these systems is usually multivariate and seasonal with spatial and temporal dimensions. The algorithm What's Strange About Recent Events (WSARE) is the state-of-the-art method for such problems. It exhaustively searches for contrast sets in the multivariate data and signals an alarm when find statistically significant rules. This bottom-up approach presents a much lower detection delay comparing the existing top-down approaches. However, WSARE is very sensitive to the small-scale changes and subsequently comes with a relatively high rate of false alarms. We propose a new approach called EigenEvent that is neither fully top-down nor bottom-up. In this method, we instead of top-down or bottom-up search, track changes in data correlation structure via eigenspace techniques. This new methodology enables us to detect both overall changes (via eigenvalue) and dimension-level changes (via eigenvectors). Experimental results on hundred sets of benchmark data reveals that EigenEvent presents a better overall performance comparing state-of-the-art, in particular in terms of the false alarm rate. version:1
arxiv-1406-3474 | Heterogeneous Multi-task Learning for Human Pose Estimation with Deep Convolutional Neural Network | http://arxiv.org/abs/1406.3474 | id:1406.3474 author:Sijin Li, Zhi-Qiang Liu, Antoni B. Chan category:cs.CV cs.LG cs.NE  published:2014-06-13 summary:We propose an heterogeneous multi-task learning framework for human pose estimation from monocular image with deep convolutional neural network. In particular, we simultaneously learn a pose-joint regressor and a sliding-window body-part detector in a deep network architecture. We show that including the body-part detection task helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several data sets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts. version:1
arxiv-1406-3460 | Are Style Guides Controlled Languages? The Case of Koenig & Bauer AG | http://arxiv.org/abs/1406.3460 | id:1406.3460 author:Karolina Suchowolec category:cs.CL  published:2014-06-13 summary:Controlled natural languages for industrial application are often regarded as a response to the challenges of translation and multilingual communication. This paper presents a quite different approach taken by Koenig & Bauer AG, where the main goal was the improvement of the authoring process for technical documentation. Most importantly, this paper explores the notion of a controlled language and demonstrates how style guides can emerge from non-linguistic considerations. Moreover, it shows the transition from loose language recommendations into precise and prescriptive rules and investigates whether such rules can be regarded as a full-fledged controlled language. version:1
arxiv-1204-6452 | Optimality of Graphlet Screening in High Dimensional Variable Selection | http://arxiv.org/abs/1204.6452 | id:1204.6452 author:Jiashun Jin, Cun-Hui Zhang, Qi Zhang category:math.ST stat.ME stat.ML stat.TH  published:2012-04-29 summary:Consider a linear regression model where the design matrix X has n rows and p columns. We assume (a) p is much large than n, (b) the coefficient vector beta is sparse in the sense that only a small fraction of its coordinates is nonzero, and (c) the Gram matrix G = X'X is sparse in the sense that each row has relatively few large coordinates (diagonals of G are normalized to 1). The sparsity in G naturally induces the sparsity of the so-called graph of strong dependence (GOSD). We find an interesting interplay between the signal sparsity and the graph sparsity, which ensures that in a broad context, the set of true signals decompose into many different small-size components of GOSD, where different components are disconnected. We propose Graphlet Screening (GS) as a new approach to variable selection, which is a two-stage Screen and Clean method. The key methodological innovation of GS is to use GOSD to guide both the screening and cleaning. Compared to m-variate brute-forth screening that has a computational cost of p^m, the GS only has a computational cost of p (up to some multi-log(p) factors) in screening. We measure the performance of any variable selection procedure by the minimax Hamming distance. We show that in a very broad class of situations, GS achieves the optimal rate of convergence in terms of the Hamming distance. Somewhat surprisingly, the well-known procedures subset selection and the lasso are rate non-optimal, even in very simple settings and even when their tuning parameters are ideally set. version:2
arxiv-1406-3418 | Fingers' Angle Calculation using Level-Set Method | http://arxiv.org/abs/1406.3418 | id:1406.3418 author:Ankit Chaudhary, J. L. Raheja, K. Das, S. Raheja category:cs.CV  published:2014-06-13 summary:In the current age, use of natural communication in human computer interaction is a known and well installed thought. Hand gesture recognition and gesture based applications has gained a significant amount of popularity amongst people all over the world. It has a number of applications ranging from security to entertainment. These applications generally are real time applications and need fast, accurate communication with machines. On the other end, gesture based communications have few limitations also like bent finger information is not provided in vision based techniques. In this paper, a novel method for fingertip detection and for angle calculation of both hands bent fingers is discussed. Angle calculation has been done before with sensor based gloves/devices. This study has been conducted in the context of natural computing for calculating angles without using any wired equipment, colors, marker or any device. The pre-processing and segmentation of the region of interest is performed in a HSV color space and a binary format respectively. Fingertips are detected using level-set method and angles were calculated using geometrical analysis. This technique requires no training for system to perform the task. version:1
arxiv-1312-1743 | Dual coordinate solvers for large-scale structural SVMs | http://arxiv.org/abs/1312.1743 | id:1312.1743 author:Deva Ramanan category:cs.LG cs.CV  published:2013-12-06 summary:This manuscript describes a method for training linear SVMs (including binary SVMs, SVM regression, and structural SVMs) from large, out-of-core training datasets. Current strategies for large-scale learning fall into one of two camps; batch algorithms which solve the learning problem given a finite datasets, and online algorithms which can process out-of-core datasets. The former typically requires datasets small enough to fit in memory. The latter is often phrased as a stochastic optimization problem; such algorithms enjoy strong theoretical properties but often require manual tuned annealing schedules, and may converge slowly for problems with large output spaces (e.g., structural SVMs). We discuss an algorithm for an "intermediate" regime in which the data is too large to fit in memory, but the active constraints (support vectors) are small enough to remain in memory. In this case, one can design rather efficient learning algorithms that are as stable as batch algorithms, but capable of processing out-of-core datasets. We have developed such a MATLAB-based solver and used it to train a collection of recognition systems for articulated pose estimation, facial analysis, 3D object recognition, and action classification, all with publicly-available code. This writeup describes the solver in detail. version:2
arxiv-1307-5302 | Kernel Adaptive Metropolis-Hastings | http://arxiv.org/abs/1307.5302 | id:1307.5302 author:Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, Arthur Gretton category:stat.ML cs.LG  published:2013-07-19 summary:A Kernel Adaptive Metropolis-Hastings algorithm is introduced, for the purpose of sampling from a target distribution with strongly nonlinear support. The algorithm embeds the trajectory of the Markov chain into a reproducing kernel Hilbert space (RKHS), such that the feature space covariance of the samples informs the choice of proposal. The procedure is computationally efficient and straightforward to implement, since the RKHS moves can be integrated out analytically: our proposal distribution in the original space is a normal distribution whose mean and covariance depend on where the current sample lies in the support of the target distribution, and adapts to its local covariance structure. Furthermore, the procedure requires neither gradients nor any other higher order information about the target, making it particularly attractive for contexts such as Pseudo-Marginal MCMC. Kernel Adaptive Metropolis-Hastings outperforms competing fixed and adaptive samplers on multivariate, highly nonlinear target distributions, arising in both real-world and synthetic examples. Code may be downloaded at https://github.com/karlnapf/kameleon-mcmc. version:3
arxiv-1402-6010 | Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social Media | http://arxiv.org/abs/1402.6010 | id:1402.6010 author:Linhong Zhu, Aram Galstyan, James Cheng, Kristina Lerman category:cs.SI cs.CL cs.IR  published:2014-02-24 summary:The growing popularity of social media (e.g, Twitter) allows users to easily share information with each other and influence others by expressing their own sentiments on various subjects. In this work, we propose an unsupervised \emph{tri-clustering} framework, which analyzes both user-level and tweet-level sentiments through co-clustering of a tripartite graph. A compelling feature of the proposed framework is that the quality of sentiment clustering of tweets, users, and features can be mutually improved by joint clustering. We further investigate the evolution of user-level sentiments and latent feature vectors in an online framework and devise an efficient online algorithm to sequentially update the clustering of tweets, users and features with newly arrived data. The online framework not only provides better quality of both dynamic user-level and tweet-level sentiment analysis, but also improves the computational and storage efficiency. We verified the effectiveness and efficiency of the proposed approaches on the November 2012 California ballot Twitter data. version:3
arxiv-1406-2807 | The Secrets of Salient Object Segmentation | http://arxiv.org/abs/1406.2807 | id:1406.2807 author:Yin Li, Xiaodi Hou, Christof Koch, James M. Rehg, Alan L. Yuille category:cs.CV  published:2014-06-11 summary:In this paper we provide an extensive evaluation of fixation prediction and salient object segmentation algorithms as well as statistics of major datasets. Our analysis identifies serious design flaws of existing salient object benchmarks, called the dataset design bias, by over emphasizing the stereotypical concepts of saliency. The dataset design bias does not only create the discomforting disconnection between fixations and salient object segmentation, but also misleads the algorithm designing. Based on our analysis, we propose a new high quality dataset that offers both fixation and salient object segmentation ground-truth. With fixations and salient object being presented simultaneously, we are able to bridge the gap between fixations and salient objects, and propose a novel method for salient object segmentation. Finally, we report significant benchmark progress on three existing datasets of segmenting salient objects version:2
arxiv-1406-3284 | Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition | http://arxiv.org/abs/1406.3284 | id:1406.3284 author:Charles F. Cadieu, Ha Hong, Daniel L. K. Yamins, Nicolas Pinto, Diego Ardila, Ethan A. Solomon, Najib J. Majaj, James J. DiCarlo category:q-bio.NC cs.NE  published:2014-06-12 summary:The primate visual system achieves remarkable visual object recognition performance even in brief presentations and under changes to object exemplar, geometric transformations, and background variation (a.k.a. core visual object recognition). This remarkable performance is mediated by the representation formed in inferior temporal (IT) cortex. In parallel, recent advances in machine learning have led to ever higher performing models of object recognition using artificial deep neural networks (DNNs). It remains unclear, however, whether the representational performance of DNNs rivals that of the brain. To accurately produce such a comparison, a major difficulty has been a unifying metric that accounts for experimental limitations such as the amount of noise, the number of neural recording sites, and the number trials, and computational limitations such as the complexity of the decoding classifier and the number of classifier training examples. In this work we perform a direct comparison that corrects for these experimental limitations and computational considerations. As part of our methodology, we propose an extension of "kernel analysis" that measures the generalization accuracy as a function of representational complexity. Our evaluations show that, unlike previous bio-inspired models, the latest DNNs rival the representational performance of IT cortex on this visual object recognition task. Furthermore, we show that models that perform well on measures of representational performance also perform well on measures of representational similarity to IT and on measures of predicting individual IT multi-unit responses. Whether these DNNs rely on computational mechanisms similar to the primate visual system is yet to be determined, but, unlike all previous bio-inspired models, that possibility cannot be ruled out merely on representational performance grounds. version:1
arxiv-1406-3282 | A swarm optimization algorithm inspired in the behavior of the social-spider | http://arxiv.org/abs/1406.3282 | id:1406.3282 author:Erik Cuevas, Miguel Cienfuegos, Daniel Zaldivar, Marco Perez category:cs.NE  published:2014-06-12 summary:Swarm intelligence is a research field that models the collective behavior in swarms of insects or animals. Several algorithms arising from such models have been proposed to solve a wide range of complex optimization problems. In this paper, a novel swarm algorithm called the Social Spider Optimization (SSO) is proposed for solving optimization tasks. The SSO algorithm is based on the simulation of cooperative behavior of social-spiders. In the proposed algorithm, individuals emulate a group of spiders which interact to each other based on the biological laws of the cooperative colony. The algorithm considers two different search agents (spiders): males and females. Depending on gender, each individual is conducted by a set of different evolutionary operators which mimic different cooperative behaviors that are typically found in the colony. In order to illustrate the proficiency and robustness of the proposed approach, it is compared to other well-known evolutionary methods. The comparison examines several standard benchmark functions that are commonly considered within the literature of evolutionary algorithms. The outcome shows a high performance of the proposed method for searching a global optimum with several benchmark functions. version:1
arxiv-1403-7471 | Approximate Decentralized Bayesian Inference | http://arxiv.org/abs/1403.7471 | id:1403.7471 author:Trevor Campbell, Jonathan P. How category:cs.LG  published:2014-03-28 summary:This paper presents an approximate method for performing Bayesian inference in models with conditional independence over a decentralized network of learning agents. The method first employs variational inference on each individual learning agent to generate a local approximate posterior, the agents transmit their local posteriors to other agents in the network, and finally each agent combines its set of received local posteriors. The key insight in this work is that, for many Bayesian models, approximate inference schemes destroy symmetry and dependencies in the model that are crucial to the correct application of Bayes' rule when combining the local posteriors. The proposed method addresses this issue by including an additional optimization step in the combination procedure that accounts for these broken dependencies. Experiments on synthetic and real data demonstrate that the decentralized method provides advantages in computational performance and predictive test likelihood over previous batch and distributed methods. version:3
arxiv-1406-3156 | A hybrid neuro--wavelet predictor for QoS control and stability | http://arxiv.org/abs/1406.3156 | id:1406.3156 author:Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cs.DC cs.NI cs.PF cs.SY 68T05  published:2014-06-12 summary:For distributed systems to properly react to peaks of requests, their adaptation activities would benefit from the estimation of the amount of requests. This paper proposes a solution to produce a short-term forecast based on data characterising user behaviour of online services. We use \emph{wavelet analysis}, providing compression and denoising on the observed time series of the amount of past user requests; and a \emph{recurrent neural network} trained with observed data and designed so as to provide well-timed estimations of future requests. The said ensemble has the ability to predict the amount of future user requests with a root mean squared error below 0.06\%. Thanks to prediction, advance resource provision can be performed for the duration of a request peak and for just the right amount of resources, hence avoiding over-provisioning and associated costs. Moreover, reliable provision lets users enjoy a level of availability of services unaffected by load variations. version:1
arxiv-1406-3149 | A Cascade Neural Network Architecture investigating Surface Plasmon Polaritons propagation for thin metals in OpenMP | http://arxiv.org/abs/1406.3149 | id:1406.3149 author:Francesco Bonanno, Giacomo Capizzi, Grazia Lo Sciuto, Christian Napoli, Giuseppe Pappalardo, Emiliano Tramontana category:cs.NE cond-mat.mes-hall cond-mat.mtrl-sci cs.DC cs.LG 68T05  published:2014-06-12 summary:Surface plasmon polaritons (SPPs) confined along metal-dielectric interface have attracted a relevant interest in the area of ultracompact photonic circuits, photovoltaic devices and other applications due to their strong field confinement and enhancement. This paper investigates a novel cascade neural network (NN) architecture to find the dependance of metal thickness on the SPP propagation. Additionally, a novel training procedure for the proposed cascade NN has been developed using an OpenMP-based framework, thus greatly reducing training time. The performed experiments confirm the effectiveness of the proposed NN architecture for the problem at hand. version:1
arxiv-1406-3140 | Expressive Power and Approximation Errors of Restricted Boltzmann Machines | http://arxiv.org/abs/1406.3140 | id:1406.3140 author:Guido Montufar, Johannes Rauh, Nihat Ay category:stat.ML math.PR 82C32  68Q99  published:2014-06-12 summary:We present explicit classes of probability distributions that can be learned by Restricted Boltzmann Machines (RBMs) depending on the number of units that they contain, and which are representative for the expressive power of the model. We use this to show that the maximal Kullback-Leibler divergence to the RBM model with $n$ visible and $m$ hidden units is bounded from above by $n - \left\lfloor \log(m+1) \right\rfloor - \frac{m+1}{2^{\left\lfloor\log(m+1)\right\rfloor}} \approx (n -1) - \log(m+1)$. In this way we can specify the number of hidden units that guarantees a sufficiently rich model containing different classes of distributions and respecting a given error tolerance. version:1
arxiv-1406-3100 | Learning ELM network weights using linear discriminant analysis | http://arxiv.org/abs/1406.3100 | id:1406.3100 author:Philip de Chazal, Jonathan Tapson, André van Schaik category:cs.NE cs.LG stat.ML  published:2014-06-12 summary:We present an alternative to the pseudo-inverse method for determining the hidden to output weight values for Extreme Learning Machines performing classification tasks. The method is based on linear discriminant analysis and provides Bayes optimal single point estimates for the weight values. version:1
arxiv-1406-2903 | A Brief State of the Art for Ontology Authoring | http://arxiv.org/abs/1406.2903 | id:1406.2903 author:Hazem Safwat, Brian Davis category:cs.CL  published:2014-06-11 summary:One of the main challenges for building the Semantic web is Ontology Authoring. Controlled Natural Languages CNLs offer a user friendly means for non-experts to author ontologies. This paper provides a snapshot of the state-of-the-art for the core CNLs for ontology authoring and reviews their respective evaluations. version:2
arxiv-1406-0919 | Gradient Sliding for Composite Optimization | http://arxiv.org/abs/1406.0919 | id:1406.0919 author:Guanghui Lan category:math.OC cs.CC stat.ML  published:2014-06-04 summary:We consider in this paper a class of composite optimization problems whose objective function is given by the summation of a general smooth and nonsmooth component, together with a relatively simple nonsmooth term. We present a new class of first-order methods, namely the gradient sliding algorithms, which can skip the computation of the gradient for the smooth component from time to time. As a consequence, these algorithms require only ${\cal O}(1/\sqrt{\epsilon})$ gradient evaluations for the smooth component in order to find an $\epsilon$-solution for the composite problem, while still maintaining the optimal ${\cal O}(1/\epsilon^2)$ bound on the total number of subgradient evaluations for the nonsmooth component. We then present a stochastic counterpart for these algorithms and establish similar complexity bounds for solving an important class of stochastic composite optimization problems. Moreover, if the smooth component in the composite function is strongly convex, the developed gradient sliding algorithms can significantly reduce the number of graduate and subgradient evaluations for the smooth and nonsmooth component to ${\cal O} (\log (1/\epsilon))$ and ${\cal O}(1/\epsilon)$, respectively. Finally, we generalize these algorithms to the case when the smooth component is replaced by a nonsmooth one possessing a certain bi-linear saddle point structure. version:2
arxiv-1406-2375 | Parsing Semantic Parts of Cars Using Graphical Models and Segment Appearance Consistency | http://arxiv.org/abs/1406.2375 | id:1406.2375 author:Wenhao Lu, Xiaochen Lian, Alan Yuille category:cs.CV  published:2014-06-09 summary:This paper addresses the problem of semantic part parsing (segmentation) of cars, i.e.assigning every pixel within the car to one of the parts (e.g.body, window, lights, license plates and wheels). We formulate this as a landmark identification problem, where a set of landmarks specifies the boundaries of the parts. A novel mixture of graphical models is proposed, which dynamically couples the landmarks to a hierarchy of segments. When modeling pairwise relation between landmarks, this coupling enables our model to exploit the local image contents in addition to spatial deformation, an aspect that most existing graphical models ignore. In particular, our model enforces appearance consistency between segments within the same part. Parsing the car, including finding the optimal coupling between landmarks and segments in the hierarchy, is performed by dynamic programming. We evaluate our method on a subset of PASCAL VOC 2010 car images and on the car subset of 3D Object Category dataset (CAR3D). We show good results and, in particular, quantify the effectiveness of using the segment appearance consistency in terms of accuracy of part localization and segmentation. version:2
arxiv-1406-3070 | Distributed Parameter Estimation in Probabilistic Graphical Models | http://arxiv.org/abs/1406.3070 | id:1406.3070 author:Yariv Dror Mizrahi, Misha Denil, Nando de Freitas category:stat.ML  published:2014-06-11 summary:This paper presents foundational theoretical results on distributed parameter estimation for undirected probabilistic graphical models. It introduces a general condition on composite likelihood decompositions of these models which guarantees the global consistency of distributed estimators, provided the local estimators are consistent. version:1
arxiv-1402-0929 | Input Warping for Bayesian Optimization of Non-stationary Functions | http://arxiv.org/abs/1402.0929 | id:1402.0929 author:Jasper Snoek, Kevin Swersky, Richard S. Zemel, Ryan P. Adams category:stat.ML cs.LG  published:2014-02-05 summary:Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions which can be queried efficiently, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in "log-space," to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably. version:3
arxiv-1406-2969 | Truncated Nuclear Norm Minimization for Image Restoration Based On Iterative Support Detection | http://arxiv.org/abs/1406.2969 | id:1406.2969 author:Yilun Wang, Xinhua Su category:cs.CV cs.LG stat.ML  published:2014-06-11 summary:Recovering a large matrix from limited measurements is a challenging task arising in many real applications, such as image inpainting, compressive sensing and medical imaging, and this kind of problems are mostly formulated as low-rank matrix approximation problems. Due to the rank operator being non-convex and discontinuous, most of the recent theoretical studies use the nuclear norm as a convex relaxation and the low-rank matrix recovery problem is solved through minimization of the nuclear norm regularized problem. However, a major limitation of nuclear norm minimization is that all the singular values are simultaneously minimized and the rank may not be well approximated \cite{hu2012fast}. Correspondingly, in this paper, we propose a new multi-stage algorithm, which makes use of the concept of Truncated Nuclear Norm Regularization (TNNR) proposed in \citep{hu2012fast} and Iterative Support Detection (ISD) proposed in \citep{wang2010sparse} to overcome the above limitation. Besides matrix completion problems considered in \citep{hu2012fast}, the proposed method can be also extended to the general low-rank matrix recovery problems. Extensive experiments well validate the superiority of our new algorithms over other state-of-the-art methods. version:1
arxiv-1406-2952 | Bird Species Categorization Using Pose Normalized Deep Convolutional Nets | http://arxiv.org/abs/1406.2952 | id:1406.2952 author:Steve Branson, Grant Van Horn, Serge Belongie, Pietro Perona category:cs.CV  published:2014-06-11 summary:We propose an architecture for fine-grained visual categorization that approaches expert human performance in the classification of bird species. Our architecture first computes an estimate of the object's pose; this is used to compute local image features which are, in turn, used for classification. The features are computed by applying deep convolutional nets to image patches that are located and normalized by the pose. We perform an empirical study of a number of pose normalization schemes, including an investigation of higher order geometric warping functions. We propose a novel graph-based clustering algorithm for learning a compact pose normalization space. We perform a detailed investigation of state-of-the-art deep convolutional feature implementations and fine-tuning feature learning for fine-grained classification. We observe that a model that integrates lower-level feature layers with pose-normalized extraction routines and higher-level feature layers with unaligned image features works best. Our experiments advance state-of-the-art performance on bird species recognition, with a large improvement of correct classification rates over previous methods (75% vs. 55-65%). version:1
arxiv-1406-2895 | Acoustic Gait-based Person Identification using Hidden Markov Models | http://arxiv.org/abs/1406.2895 | id:1406.2895 author:Jürgen T. Geiger, Maximilian Kneißl, Björn Schuller, Gerhard Rigoll category:cs.HC cs.CV  published:2014-06-11 summary:We present a system for identifying humans by their walking sounds. This problem is also known as acoustic gait recognition. The goal of the system is to analyse sounds emitted by walking persons (mostly the step sounds) and identify those persons. These sounds are characterised by the gait pattern and are influenced by the movements of the arms and legs, but also depend on the type of shoe. We extract cepstral features from the recorded audio signals and use hidden Markov models for dynamic classification. A cyclic model topology is employed to represent individual gait cycles. This topology allows to model and detect individual steps, leading to very promising identification rates. For experimental validation, we use the publicly available TUM GAID database, which is a large gait recognition database containing 3050 recordings of 305 subjects in three variations. In the best setup, an identification rate of 65.5 % is achieved out of 155 subjects. This is a relative improvement of almost 30 % compared to our previous work, which used various audio features and support vector machines. version:1
arxiv-1406-2889 | Explicit Computation of Input Weights in Extreme Learning Machines | http://arxiv.org/abs/1406.2889 | id:1406.2889 author:Jonathan Tapson, Philip de Chazal, André van Schaik category:cs.NE  published:2014-06-11 summary:We present a closed form expression for initializing the input weights in a multi-layer perceptron, which can be used as the first step in synthesis of an Extreme Learning Ma-chine. The expression is based on the standard function for a separating hyperplane as computed in multilayer perceptrons and linear Support Vector Machines; that is, as a linear combination of input data samples. In the absence of supervised training for the input weights, random linear combinations of training data samples are used to project the input data to a higher dimensional hidden layer. The hidden layer weights are solved in the standard ELM fashion by computing the pseudoinverse of the hidden layer outputs and multiplying by the desired output values. All weights for this method can be computed in a single pass, and the resulting networks are more accurate and more consistent on some standard problems than regular ELM networks of the same size. version:1
arxiv-1406-2880 | POS Tagging and its Applications for Mathematics | http://arxiv.org/abs/1406.2880 | id:1406.2880 author:Ulf Schöneberg, Wolfram Sperber category:cs.DL cs.CL cs.IR  published:2014-06-11 summary:Content analysis of scientific publications is a nontrivial task, but a useful and important one for scientific information services. In the Gutenberg era it was a domain of human experts; in the digital age many machine-based methods, e.g., graph analysis tools and machine-learning techniques, have been developed for it. Natural Language Processing (NLP) is a powerful machine-learning approach to semiautomatic speech and language processing, which is also applicable to mathematics. The well established methods of NLP have to be adjusted for the special needs of mathematics, in particular for handling mathematical formulae. We demonstrate a mathematics-aware part of speech tagger and give a short overview about our adaptation of NLP methods for mathematical publications. We show the use of the tools developed for key phrase extraction and classification in the database zbMATH. version:1
arxiv-1406-2864 | Algebraic-Combinatorial Methods for Low-Rank Matrix Completion with Application to Athletic Performance Prediction | http://arxiv.org/abs/1406.2864 | id:1406.2864 author:Duncan A. J. Blythe, Louis Theran, Franz Kiraly category:stat.ML  published:2014-06-11 summary:This paper presents novel algorithms which exploit the intrinsic algebraic and combinatorial structure of the matrix completion task for estimating missing en- tries in the general low rank setting. For positive data, we achieve results out- performing the state of the art nuclear norm, both in accuracy and computational efficiency, in simulations and in the task of predicting athletic performance from partially observed data. version:1
arxiv-1406-2623 | Maximum Likelihood-based Online Adaptation of Hyper-parameters in CMA-ES | http://arxiv.org/abs/1406.2623 | id:1406.2623 author:Ilya Loshchilov, Marc Schoenauer, Michèle Sebag, Nikolaus Hansen category:cs.NE cs.AI  published:2014-06-10 summary:The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is widely accepted as a robust derivative-free continuous optimization algorithm for non-linear and non-convex optimization problems. CMA-ES is well known to be almost parameterless, meaning that only one hyper-parameter, the population size, is proposed to be tuned by the user. In this paper, we propose a principled approach called self-CMA-ES to achieve the online adaptation of CMA-ES hyper-parameters in order to improve its overall performance. Experimental results show that for larger-than-default population size, the default settings of hyper-parameters of CMA-ES are far from being optimal, and that self-CMA-ES allows for dynamically approaching optimal settings. version:2
arxiv-1406-2823 | A Hitchhiker's Guide to Search-Based Software Engineering for Software Product Lines | http://arxiv.org/abs/1406.2823 | id:1406.2823 author:Roberto E. Lopez-Herrejon, Javier Ferrer, Francisco Chicano, Lukas Linsbauer, Alexander Egyed, Enrique Alba category:cs.SE cs.NE  published:2014-06-11 summary:Search Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. The capacity of SBSE techniques to tackle problems involving large search spaces make their application attractive for Software Product Lines (SPLs). In recent years, several publications have appeared that apply SBSE techniques to SPL problems. In this paper, we present the results of a systematic mapping study of such publications. We identified the stages of the SPL life cycle where SBSE techniques have been used, what case studies have been employed and how they have been analysed. This mapping study revealed potential venues for further research as well as common misunderstanding and pitfalls when applying SBSE techniques that we address by providing a guideline for researchers and practitioners interested in exploiting these techniques. version:1
arxiv-1403-4334 | Bregman Divergences for Infinite Dimensional Covariance Matrices | http://arxiv.org/abs/1403.4334 | id:1403.4334 author:Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli category:cs.CV  published:2014-03-18 summary:We introduce an approach to computing and comparing Covariance Descriptors (CovDs) in infinite-dimensional spaces. CovDs have become increasingly popular to address classification problems in computer vision. While CovDs offer some robustness to measurement variations, they also throw away part of the information contained in the original data by only retaining the second-order statistics over the measurements. Here, we propose to overcome this limitation by first mapping the original data to a high-dimensional Hilbert space, and only then compute the CovDs. We show that several Bregman divergences can be computed between the resulting CovDs in Hilbert space via the use of kernels. We then exploit these divergences for classification purposes. Our experiments demonstrate the benefits of our approach on several tasks, such as material and texture recognition, person re-identification, and action recognition from motion capture data. version:3
arxiv-1406-2784 | Provable Tensor Factorization with Missing Data | http://arxiv.org/abs/1406.2784 | id:1406.2784 author:Prateek Jain, Sewoong Oh category:stat.ML  published:2014-06-11 summary:We study the problem of low-rank tensor factorization in the presence of missing data. We ask the following question: how many sampled entries do we need, to efficiently and exactly reconstruct a tensor with a low-rank orthogonal decomposition? We propose a novel alternating minimization based method which iteratively refines estimates of the singular vectors. We show that under certain standard assumptions, our method can recover a three-mode $n\times n\times n$ dimensional rank-$r$ tensor exactly from $O(n^{3/2} r^5 \log^4 n)$ randomly sampled entries. In the process of proving this result, we solve two challenging sub-problems for tensors with missing data. First, in the process of analyzing the initialization step, we prove a generalization of a celebrated result by Szemer\'edie et al. on the spectrum of random graphs. Next, we prove global convergence of alternating minimization with a good initialization. Simulations suggest that the dependence of the sample size on dimensionality $n$ is indeed tight. version:1
arxiv-1406-2732 | Deep Epitomic Convolutional Neural Networks | http://arxiv.org/abs/1406.2732 | id:1406.2732 author:George Papandreou category:cs.CV cs.LG  published:2014-06-10 summary:Deep convolutional neural networks have recently proven extremely competitive in challenging image recognition tasks. This paper proposes the epitomic convolution as a new building block for deep neural networks. An epitomic convolution layer replaces a pair of consecutive convolution and max-pooling layers found in standard deep convolutional neural networks. The main version of the proposed model uses mini-epitomes in place of filters and computes responses invariant to small translations by epitomic search instead of max-pooling over image positions. The topographic version of the proposed model uses large epitomes to learn filter maps organized in translational topographies. We show that error back-propagation can successfully learn multiple epitomic layers in a supervised fashion. The effectiveness of the proposed method is assessed in image classification tasks on standard benchmarks. Our experiments on Imagenet indicate improved recognition performance compared to standard convolutional neural networks of similar architecture. Our models pre-trained on Imagenet perform excellently on Caltech-101. We also obtain competitive image classification results on the small-image MNIST and CIFAR-10 datasets. version:1
arxiv-1406-2721 | Learning Latent Variable Gaussian Graphical Models | http://arxiv.org/abs/1406.2721 | id:1406.2721 author:Zhaoshi Meng, Brian Eriksson, Alfred O. Hero III category:stat.ML cs.LG math.ST stat.TH  published:2014-06-10 summary:Gaussian graphical models (GGM) have been widely used in many high-dimensional applications ranging from biological and financial data to recommender systems. Sparsity in GGM plays a central role both statistically and computationally. Unfortunately, real-world data often does not fit well to sparse graphical models. In this paper, we focus on a family of latent variable Gaussian graphical models (LVGGM), where the model is conditionally sparse given latent variables, but marginally non-sparse. In LVGGM, the inverse covariance matrix has a low-rank plus sparse structure, and can be learned in a regularized maximum likelihood framework. We derive novel parameter estimation error bounds for LVGGM under mild conditions in the high-dimensional setting. These results complement the existing theory on the structural learning, and open up new possibilities of using LVGGM for statistical inference. version:1
arxiv-1406-2710 | A Multiplicative Model for Learning Distributed Text-Based Attribute Representations | http://arxiv.org/abs/1406.2710 | id:1406.2710 author:Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov category:cs.LG cs.CL  published:2014-06-10 summary:In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation. version:1
arxiv-1312-3613 | Augur: a Modeling Language for Data-Parallel Probabilistic Inference | http://arxiv.org/abs/1312.3613 | id:1312.3613 author:Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock, Stephen J. Green, Guy L. Steele Jr category:stat.ML cs.AI cs.DC cs.PL  published:2013-12-12 summary:It is time-consuming and error-prone to implement inference procedures for each new probabilistic model. Probabilistic programming addresses this problem by allowing a user to specify the model and having a compiler automatically generate an inference procedure for it. For this approach to be practical, it is important to generate inference code that has reasonable performance. In this paper, we present a probabilistic programming language and compiler for Bayesian networks designed to make effective use of data-parallel architectures such as GPUs. Our language is fully integrated within the Scala programming language and benefits from tools such as IDE support, type-checking, and code completion. We show that the compiler can generate data-parallel inference code scalable to thousands of GPU cores by making use of the conditional independence relationships in the Bayesian network. version:2
arxiv-1406-2671 | Conceptors: an easy introduction | http://arxiv.org/abs/1406.2671 | id:1406.2671 author:Herbert Jaeger category:cs.NE  published:2014-06-10 summary:Conceptors provide an elementary neuro-computational mechanism which sheds a fresh and unifying light on a diversity of cognitive phenomena. A number of demanding learning and processing tasks can be solved with unprecedented ease, robustness and accuracy. Some of these tasks were impossible to solve before. This entirely informal paper introduces the basic principles of conceptors and highlights some of their usages. version:1
arxiv-1406-2661 | Generative Adversarial Networks | http://arxiv.org/abs/1406.2661 | id:1406.2661 author:Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG  published:2014-06-10 summary:We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. version:1
arxiv-1406-2646 | Learning with Cross-Kernels and Ideal PCA | http://arxiv.org/abs/1406.2646 | id:1406.2646 author:Franz J Király, Martin Kreuzer, Louis Theran category:cs.LG math.AC stat.ML  published:2014-06-10 summary:We describe how cross-kernel matrices, that is, kernel matrices between the data and a custom chosen set of `feature spanning points' can be used for learning. The main potential of cross-kernels lies in the fact that (a) only one side of the matrix scales with the number of data points, and (b) cross-kernels, as opposed to the usual kernel matrices, can be used to certify for the data manifold. Our theoretical framework, which is based on a duality involving the feature space and vanishing ideals, indicates that cross-kernels have the potential to be used for any kind of kernel learning. We present a novel algorithm, Ideal PCA (IPCA), which cross-kernelizes PCA. We demonstrate on real and synthetic data that IPCA allows to (a) obtain PCA-like features faster and (b) to extract novel and empirically validated features certifying for the data manifold. version:1
arxiv-1406-2622 | Equivalence of Learning Algorithms | http://arxiv.org/abs/1406.2622 | id:1406.2622 author:Julien Audiffren, Hachem Kadri category:cs.LG stat.ML  published:2014-06-10 summary:The purpose of this paper is to introduce a concept of equivalence between machine learning algorithms. We define two notions of algorithmic equivalence, namely, weak and strong equivalence. These notions are of paramount importance for identifying when learning prop erties from one learning algorithm can be transferred to another. Using regularized kernel machines as a case study, we illustrate the importance of the introduced equivalence concept by analyzing the relation between kernel ridge regression (KRR) and m-power regularized least squares regression (M-RLSR) algorithms. version:1
arxiv-1406-2602 | Graph Approximation and Clustering on a Budget | http://arxiv.org/abs/1406.2602 | id:1406.2602 author:Ethan Fetaya, Ohad Shamir, Shimon Ullman category:stat.ML cs.AI cs.CV cs.LG  published:2014-06-10 summary:We consider the problem of learning from a similarity matrix (such as spectral clustering and lowd imensional embedding), when computing pairwise similarities are costly, and only a limited number of entries can be observed. We provide a theoretical analysis using standard notions of graph approximation, significantly generalizing previous results (which focused on spectral clustering with two clusters). We also propose a new algorithmic approach based on adaptive sampling, which experimentally matches or improves on previous methods, while being considerably more general and computationally cheaper. version:1
arxiv-1406-2580 | Identification of Orchid Species Using Content-Based Flower Image Retrieval | http://arxiv.org/abs/1406.2580 | id:1406.2580 author:D. H. Apriyanti, A. A. Arymurthy, L. T. Handoko category:cs.CV cs.IR cs.LG  published:2014-06-10 summary:In this paper, we developed the system for recognizing the orchid species by using the images of flower. We used MSRM (Maximal Similarity based on Region Merging) method for segmenting the flower object from the background and extracting the shape feature such as the distance from the edge to the centroid point of the flower, aspect ratio, roundness, moment invariant, fractal dimension and also extract color feature. We used HSV color feature with ignoring the V value. To retrieve the image, we used Support Vector Machine (SVM) method. Orchid is a unique flower. It has a part of flower called lip (labellum) that distinguishes it from other flowers even from other types of orchids. Thus, in this paper, we proposed to do feature extraction not only on flower region but also on lip (labellum) region. The result shows that our proposed method can increase the accuracy value of content based flower image retrieval for orchid species up to $\pm$ 14%. The most dominant feature is Centroid Contour Distance, Moment Invariant and HSV Color. The system accuracy is 85,33% in validation phase and 79,33% in testing phase. version:1
arxiv-1406-2572 | Identifying and attacking the saddle point problem in high-dimensional non-convex optimization | http://arxiv.org/abs/1406.2572 | id:1406.2572 author:Yann Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio category:cs.LG math.OC stat.ML  published:2014-06-10 summary:A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. version:1
arxiv-1406-2545 | A Flexible Fitness Function for Community Detection in Complex Networks | http://arxiv.org/abs/1406.2545 | id:1406.2545 author:Fabricio Olivetti de Franca, Guilherme Palermo Coelho category:cs.NE cs.SI physics.soc-ph  published:2014-06-10 summary:Most community detection algorithms from the literature work as optimization tools that minimize a given \textit{fitness function}, while assuming that each node belongs to a single community. Since there is no hard concept of what a community is, most proposed fitness functions focus on a particular definition. As such, these functions do not always lead to partitions that correspond to those observed in practice. This paper proposes a new flexible fitness function that allows the identification of communities with distinct characteristics. Such flexibility was evaluated through the adoption of an immune-inspired optimization algorithm, named cob-aiNet[C], to identify both disjoint and overlapping communities in a set of benchmark networks. The results have shown that the obtained partitions are much closer to the ground-truth than those obtained by the optimization of the modularity function. version:1
arxiv-1406-2541 | Predictive Entropy Search for Efficient Global Optimization of Black-box Functions | http://arxiv.org/abs/1406.2541 | id:1406.2541 author:José Miguel Hernández-Lobato, Matthew W. Hoffman, Zoubin Ghahramani category:stat.ML cs.LG  published:2014-06-10 summary:We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance. version:1
arxiv-1406-2539 | Maximizing Diversity for Multimodal Optimization | http://arxiv.org/abs/1406.2539 | id:1406.2539 author:Fabricio Olivetti de Franca category:cs.NE  published:2014-06-10 summary:Most multimodal optimization algorithms use the so called \textit{niching methods}~\cite{mahfoud1995niching} in order to promote diversity during optimization, while others, like \textit{Artificial Immune Systems}~\cite{de2010conceptual} try to find multiple solutions as its main objective. One of such algorithms, called \textit{dopt-aiNet}~\cite{de2005artificial}, introduced the Line Distance that measures the distance between two solutions regarding their basis of attraction. In this short abstract I propose the use of the Line Distance measure as the main objective-function in order to locate multiple optima at once in a population. version:1
arxiv-1406-2538 | FrameNet CNL: a Knowledge Representation and Information Extraction Language | http://arxiv.org/abs/1406.2538 | id:1406.2538 author:Guntis Barzdins category:cs.CL cs.AI cs.IR cs.LG  published:2014-06-10 summary:The paper presents a FrameNet-based information extraction and knowledge representation framework, called FrameNet-CNL. The framework is used on natural language documents and represents the extracted knowledge in a tailor-made Frame-ontology from which unambiguous FrameNet-CNL paraphrase text can be generated automatically in multiple languages. This approach brings together the fields of information extraction and CNL, because a source text can be considered belonging to FrameNet-CNL, if information extraction parser produces the correct knowledge representation as a result. We describe a state-of-the-art information extraction parser used by a national news agency and speculate that FrameNet-CNL eventually could shape the natural language subset used for writing the newswire articles. version:1
arxiv-1406-2528 | Denosing Using Wavelets and Projections onto the L1-Ball | http://arxiv.org/abs/1406.2528 | id:1406.2528 author:A. Enis Cetin, Mohammad Tofighi category:math.OC cs.CV  published:2014-06-10 summary:Both wavelet denoising and denosing methods using the concept of sparsity are based on soft-thresholding. In sparsity based denoising methods, it is assumed that the original signal is sparse in some transform domains such as the wavelet domain and the wavelet subsignals of the noisy signal are projected onto L1-balls to reduce noise. In this lecture note, it is shown that the size of the L1-ball or equivalently the soft threshold value can be determined using linear algebra. The key step is an orthogonal projection onto the epigraph set of the L1-norm cost function. version:1
arxiv-1403-8098 | Hyperspectral image superresolution: An edge-preserving convex formulation | http://arxiv.org/abs/1403.8098 | id:1403.8098 author:Miguel Simões, José Bioucas-Dias, Luis B. Almeida, Jocelyn Chanussot category:cs.CV physics.data-an stat.ML  published:2014-03-31 summary:Hyperspectral remote sensing images (HSIs) are characterized by having a low spatial resolution and a high spectral resolution, whereas multispectral images (MSIs) are characterized by low spectral and high spatial resolutions. These complementary characteristics have stimulated active research in the inference of images with high spatial and spectral resolutions from HSI-MSI pairs. In this paper, we formulate this data fusion problem as the minimization of a convex objective function containing two data-fitting terms and an edge-preserving regularizer. The data-fitting terms are quadratic and account for blur, different spatial resolutions, and additive noise; the regularizer, a form of vector Total Variation, promotes aligned discontinuities across the reconstructed hyperspectral bands. The optimization described above is rather hard, owing to its non-diagonalizable linear operators, to the non-quadratic and non-smooth nature of the regularizer, and to the very large size of the image to be inferred. We tackle these difficulties by tailoring the Split Augmented Lagrangian Shrinkage Algorithm (SALSA)---an instance of the Alternating Direction Method of Multipliers (ADMM)---to this optimization problem. By using a convenient variable splitting and by exploiting the fact that HSIs generally "live" in a low-dimensional subspace, we obtain an effective algorithm that yields state-of-the-art results, as illustrated by experiments. version:2
arxiv-1403-5997 | Bayesian calibration for forensic evidence reporting | http://arxiv.org/abs/1403.5997 | id:1403.5997 author:Niko Brümmer, Albert Swart category:stat.ML cs.LG  published:2014-03-24 summary:We introduce a Bayesian solution for the problem in forensic speaker recognition, where there may be very little background material for estimating score calibration parameters. We work within the Bayesian paradigm of evidence reporting and develop a principled probabilistic treatment of the problem, which results in a Bayesian likelihood-ratio as the vehicle for reporting weight of evidence. We show in contrast, that reporting a likelihood-ratio distribution does not solve this problem. Our solution is experimentally exercised on a simulated forensic scenario, using NIST SRE'12 scores, which demonstrates a clear advantage for the proposed method compared to the traditional plugin calibration recipe. version:3
arxiv-1304-2998 | Detecting Directionality in Random Fields Using the Monogenic Signal | http://arxiv.org/abs/1304.2998 | id:1304.2998 author:Sofia Olhede, David Ramírez, Peter J. Schreier category:cs.IT cs.CV math.IT  published:2013-04-10 summary:Detecting and analyzing directional structures in images is important in many applications since one-dimensional patterns often correspond to important features such as object contours or trajectories. Classifying a structure as directional or non-directional requires a measure to quantify the degree of directionality and a threshold, which needs to be chosen based on the statistics of the image. In order to do this, we model the image as a random field. So far, little research has been performed on analyzing directionality in random fields. In this paper, we propose a measure to quantify the degree of directionality based on the random monogenic signal, which enables a unique decomposition of a 2D signal into local amplitude, local orientation, and local phase. We investigate the second-order statistical properties of the monogenic signal for isotropic, anisotropic, and unidirectional random fields. We analyze our measure of directionality for finite-size sample images, and determine a threshold to distinguish between unidirectional and non-unidirectional random fields, which allows the automatic classification of images. version:3
arxiv-1302-6937 | Online Convex Optimization Against Adversaries with Memory and Application to Statistical Arbitrage | http://arxiv.org/abs/1302.6937 | id:1302.6937 author:Oren Anava, Elad Hazan, Shie Mannor category:cs.LG  published:2013-02-27 summary:The framework of online learning with memory naturally captures learning problems with temporal constraints, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement our theoretic results with an application to statistical arbitrage in finance: we devise algorithms for constructing mean-reverting portfolios. version:2
arxiv-1406-2419 | Why do linear SVMs trained on HOG features perform so well? | http://arxiv.org/abs/1406.2419 | id:1406.2419 author:Hilton Bristow, Simon Lucey category:cs.CV cs.LG  published:2014-06-10 summary:Linear Support Vector Machines trained on HOG features are now a de facto standard across many visual perception tasks. Their popularisation can largely be attributed to the step-change in performance they brought to pedestrian detection, and their subsequent successes in deformable parts models. This paper explores the interactions that make the HOG-SVM symbiosis perform so well. By connecting the feature extraction and learning processes rather than treating them as disparate plugins, we show that HOG features can be viewed as doing two things: (i) inducing capacity in, and (ii) adding prior to a linear SVM trained on pixels. From this perspective, preserving second-order statistics and locality of interactions are key to good performance. We demonstrate surprising accuracy on expression recognition and pedestrian detection tasks, by assuming only the importance of preserving such local second-order interactions. version:1
arxiv-1406-1833 | Unsupervised Feature Learning through Divergent Discriminative Feature Accumulation | http://arxiv.org/abs/1406.1833 | id:1406.1833 author:Paul A. Szerlip, Gregory Morse, Justin K. Pugh, Kenneth O. Stanley category:cs.NE cs.LG  published:2014-06-06 summary:Unlike unsupervised approaches such as autoencoders that learn to reconstruct their inputs, this paper introduces an alternative approach to unsupervised feature learning called divergent discriminative feature accumulation (DDFA) that instead continually accumulates features that make novel discriminations among the training set. Thus DDFA features are inherently discriminative from the start even though they are trained without knowledge of the ultimate classification problem. Interestingly, DDFA also continues to add new features indefinitely (so it does not depend on a hidden layer size), is not based on minimizing error, and is inherently divergent instead of convergent, thereby providing a unique direction of research for unsupervised feature learning. In this paper the quality of its learned features is demonstrated on the MNIST dataset, where its performance confirms that indeed DDFA is a viable technique for learning useful features. version:2
arxiv-1406-2407 | Optimization Methods for Convolutional Sparse Coding | http://arxiv.org/abs/1406.2407 | id:1406.2407 author:Hilton Bristow, Simon Lucey category:cs.CV  published:2014-06-10 summary:Sparse and convolutional constraints form a natural prior for many optimization problems that arise from physical processes. Detecting motifs in speech and musical passages, super-resolving images, compressing videos, and reconstructing harmonic motions can all leverage redundancies introduced by convolution. Solving problems involving sparse and convolutional constraints remains a difficult computational problem, however. In this paper we present an overview of convolutional sparse coding in a consistent framework. The objective involves iteratively optimizing a convolutional least-squares term for the basis functions, followed by an L1-regularized least squares term for the sparse coefficients. We discuss a range of optimization methods for solving the convolutional sparse coding objective, and the properties that make each method suitable for different applications. In particular, we concentrate on computational complexity, speed to {\epsilon} convergence, memory usage, and the effect of implied boundary conditions. We present a broad suite of examples covering different signal and application domains to illustrate the general applicability of convolutional sparse coding, and the efficacy of the available optimization methods. version:1
arxiv-1406-2400 | Controlled Natural Language Generation from a Multilingual FrameNet-based Grammar | http://arxiv.org/abs/1406.2400 | id:1406.2400 author:Dana Dannélls, Normunds Grūzītis category:cs.CL  published:2014-06-10 summary:This paper presents a currently bilingual but potentially multilingual FrameNet-based grammar library implemented in Grammatical Framework. The contribution of this paper is two-fold. First, it offers a methodological approach to automatically generate the grammar based on semantico-syntactic valence patterns extracted from FrameNet-annotated corpora. Second, it provides a proof of concept for two use cases illustrating how the acquired multilingual grammar can be exploited in different CNL applications in the domains of arts and tourism. version:1
arxiv-1406-2395 | ExpertBayes: Automatically refining manually built Bayesian networks | http://arxiv.org/abs/1406.2395 | id:1406.2395 author:Ezilda Almeida, Pedro Ferreira, Tiago Vinhoza, Inês Dutra, Jingwei Li, Yirong Wu, Elizabeth Burnside category:cs.AI cs.LG stat.ML  published:2014-06-10 summary:Bayesian network structures are usually built using only the data and starting from an empty network or from a naive Bayes structure. Very often, in some domains, like medicine, a prior structure knowledge is already known. This structure can be automatically or manually refined in search for better performance models. In this work, we take Bayesian networks built by specialists and show that minor perturbations to this original network can yield better classifiers with a very small computational cost, while maintaining most of the intended meaning of the original model. version:1
arxiv-1308-4941 | Automatic Labeling for Entity Extraction in Cyber Security | http://arxiv.org/abs/1308.4941 | id:1308.4941 author:Robert A. Bridges, Corinne L. Jones, Michael D. Iannacone, Kelly M. Testa, John R. Goodall category:cs.IR cs.CL  published:2013-08-22 summary:Timely analysis of cyber-security information necessitates automated information extraction from unstructured text. While state-of-the-art extraction methods produce extremely accurate results, they require ample training data, which is generally unavailable for specialized applications, such as detecting security related entities; moreover, manual annotation of corpora is very costly and often not a viable solution. In response, we develop a very precise method to automatically label text from several data sources by leveraging related, domain-specific, structured data and provide public access to a corpus annotated with cyber-security entities. Next, we implement a Maximum Entropy Model trained with the average perceptron on a portion of our corpus ($\sim$750,000 words) and achieve near perfect precision, recall, and accuracy, with training times under 17 seconds. version:3
arxiv-1406-2298 | Explaining Violation Traces with Finite State Natural Language Generation Models | http://arxiv.org/abs/1406.2298 | id:1406.2298 author:Gordon J. Pace, Michael Rosner category:cs.SE cs.CL  published:2014-06-09 summary:An essential element of any verification technique is that of identifying and communicating to the user, system behaviour which leads to a deviation from the expected behaviour. Such behaviours are typically made available as long traces of system actions which would benefit from a natural language explanation of the trace and especially in the context of business logic level specifications. In this paper we present a natural language generation model which can be used to explain such traces. A key idea is that the explanation language is a CNL that is, formally speaking, regular language susceptible transformations that can be expressed with finite state machinery. At the same time it admits various forms of abstraction and simplification which contribute to the naturalness of explanations that are communicated to the user. version:1
arxiv-1406-2283 | Depth Map Prediction from a Single Image using a Multi-Scale Deep Network | http://arxiv.org/abs/1406.2283 | id:1406.2283 author:David Eigen, Christian Puhrsch, Rob Fergus category:cs.CV  published:2014-06-09 summary:Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation. version:1
arxiv-1406-2282 | Robust Estimation of 3D Human Poses from a Single Image | http://arxiv.org/abs/1406.2282 | id:1406.2282 author:Chunyu Wang, Yizhou Wang, Zhouchen Lin, Alan L. Yuille, Wen Gao category:cs.CV  published:2014-06-09 summary:Human pose estimation is a key step to action recognition. We propose a method of estimating 3D human poses from a single image, which works in conjunction with an existing 2D pose/joint detector. 3D pose estimation is challenging because multiple 3D poses may correspond to the same 2D pose after projection due to the lack of depth information. Moreover, current 2D pose estimators are usually inaccurate which may cause errors in the 3D estimation. We address the challenges in three ways: (i) We represent a 3D pose as a linear combination of a sparse set of bases learned from 3D human skeletons. (ii) We enforce limb length constraints to eliminate anthropomorphically implausible skeletons. (iii) We estimate a 3D pose by minimizing the $L_1$-norm error between the projection of the 3D pose and the corresponding 2D detection. The $L_1$-norm loss term is robust to inaccurate 2D joint estimations. We use the alternating direction method (ADM) to solve the optimization problem efficiently. Our approach outperforms the state-of-the-arts on three benchmark datasets. version:1
arxiv-1406-2240 | Feature Selection For High-Dimensional Clustering | http://arxiv.org/abs/1406.2240 | id:1406.2240 author:Larry Wasserman, Martin Azizyan, Aarti Singh category:math.ST stat.ML stat.TH  published:2014-06-09 summary:We present a nonparametric method for selecting informative features in high-dimensional clustering problems. We start with a screening step that uses a test for multimodality. Then we apply kernel density estimation and mode clustering to the selected features. The output of the method consists of a list of relevant features, and cluster assignments. We provide explicit bounds on the error rate of the resulting clustering. In addition, we provide the first error bounds on mode based clustering. version:1
arxiv-1406-2235 | A Hybrid Latent Variable Neural Network Model for Item Recommendation | http://arxiv.org/abs/1406.2235 | id:1406.2235 author:Michael R. Smith, Tony Martinez, Michael Gashler category:cs.LG cs.IR cs.NE stat.ML  published:2014-06-09 summary:Collaborative filtering is used to recommend items to a user without requiring a knowledge of the item itself and tends to outperform other techniques. However, collaborative filtering suffers from the cold-start problem, which occurs when an item has not yet been rated or a user has not rated any items. Incorporating additional information, such as item or user descriptions, into collaborative filtering can address the cold-start problem. In this paper, we present a neural network model with latent input variables (latent neural network or LNN) as a hybrid collaborative filtering technique that addresses the cold-start problem. LNN outperforms a broad selection of content-based filters (which make recommendations based on item descriptions) and other hybrid approaches while maintaining the accuracy of state-of-the-art collaborative filtering techniques. version:1
arxiv-1404-0736 | Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation | http://arxiv.org/abs/1404.0736 | id:1404.0736 author:Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus category:cs.CV cs.LG  published:2014-04-02 summary:We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the linear structure present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2x, while keeping the accuracy within 1% of the original model. version:2
arxiv-1406-2206 | Efficient Sparse Clustering of High-Dimensional Non-spherical Gaussian Mixtures | http://arxiv.org/abs/1406.2206 | id:1406.2206 author:Martin Azizyan, Aarti Singh, Larry Wasserman category:math.ST stat.ML stat.TH  published:2014-06-09 summary:We consider the problem of clustering data points in high dimensions, i.e. when the number of data points may be much smaller than the number of dimensions. Specifically, we consider a Gaussian mixture model (GMM) with non-spherical Gaussian components, where the clusters are distinguished by only a few relevant dimensions. The method we propose is a combination of a recent approach for learning parameters of a Gaussian mixture model and sparse linear discriminant analysis (LDA). In addition to cluster assignments, the method returns an estimate of the set of features relevant for clustering. Our results indicate that the sample complexity of clustering depends on the sparsity of the relevant feature set, while only scaling logarithmically with the ambient dimension. Additionally, we require much milder assumptions than existing work on clustering in high dimensions. In particular, we do not require spherical clusters nor necessitate mean separation along relevant dimensions. version:1
arxiv-1406-2204 | How Easy is it to Learn a Controlled Natural Language for Building a Knowledge Base? | http://arxiv.org/abs/1406.2204 | id:1406.2204 author:Sandra Williams, Richard Power, Allan Third category:cs.CL  published:2014-06-09 summary:Recent developments in controlled natural language editors for knowledge engineering (KE) have given rise to expectations that they will make KE tasks more accessible and perhaps even enable non-engineers to build knowledge bases. This exploratory research focussed on novices and experts in knowledge engineering during their attempts to learn a controlled natural language (CNL) known as OWL Simplified English and use it to build a small knowledge base. Participants' behaviours during the task were observed through eye-tracking and screen recordings. This was an attempt at a more ambitious user study than in previous research because we used a naturally occurring text as the source of domain knowledge, and left them without guidance on which information to select, or how to encode it. We have identified a number of skills (competencies) required for this difficult task and key problems that authors face. version:1
arxiv-1312-4564 | Adaptive Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/abs/1312.4564 | id:1312.4564 author:Peilin Zhao, Jinwei Yang, Tong Zhang, Ping Li category:stat.ML cs.LG I.2.6; G.1.6  published:2013-12-16 summary:The Alternating Direction Method of Multipliers (ADMM) has been studied for years. The traditional ADMM algorithm needs to compute, at each iteration, an (empirical) expected loss function on all training examples, resulting in a computational complexity proportional to the number of training examples. To reduce the time complexity, stochastic ADMM algorithms were proposed to replace the expected function with a random loss function associated with one uniformly drawn example plus a Bregman divergence. The Bregman divergence, however, is derived from a simple second order proximal function, the half squared norm, which could be a suboptimal choice. In this paper, we present a new family of stochastic ADMM algorithms with optimal second order proximal functions, which produce a new family of adaptive subgradient methods. We theoretically prove that their regret bounds are as good as the bounds which could be achieved by the best proximal function that can be chosen in hindsight. Encouraging empirical results on a variety of real-world datasets confirm the effectiveness and efficiency of the proposed algorithms. version:4
arxiv-1312-5813 | Unsupervised Pretraining Encourages Moderate-Sparseness | http://arxiv.org/abs/1312.5813 | id:1312.5813 author:Jun Li, Wei Luo, Jian Yang, Xiaotong Yuan category:cs.LG cs.NE  published:2013-12-20 summary:It is well known that direct training of deep neural networks will generally lead to poor results. A major progress in recent years is the invention of various pretraining methods to initialize network parameters and it was shown that such methods lead to good prediction performance. However, the reason for the success of pretraining has not been fully understood, although it was argued that regularization and better optimization play certain roles. This paper provides another explanation for the effectiveness of pretraining, where we show pretraining leads to a sparseness of hidden unit activation in the resulting neural networks. The main reason is that the pretraining models can be interpreted as an adaptive sparse coding. Compared to deep neural network with sigmoid function, our experimental results on MNIST and Birdsong further support this sparseness observation. version:2
arxiv-1406-2098 | Learning directed acyclic graphs via bootstrap aggregating | http://arxiv.org/abs/1406.2098 | id:1406.2098 author:Ru Wang, Jie Peng category:stat.ML stat.CO stat.ME  published:2014-06-09 summary:Probabilistic graphical models are graphical representations of probability distributions. Graphical models have applications in many fields including biology, social sciences, linguistic, neuroscience. In this paper, we propose directed acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed procedure is named as DAGBag. Specifically, an ensemble of DAGs is first learned based on bootstrap resamples of the data and then an aggregated DAG is derived by minimizing the overall distance to the entire ensemble. A family of metrics based on the structural hamming distance is defined for the space of DAGs (of a given node set) and is used for aggregation. Under the high-dimensional-low-sample size setting, the graph learned on one data set often has excessive number of false positive edges due to over-fitting of the noise. Aggregation overcomes over-fitting through variance reduction and thus greatly reduces false positives. We also develop an efficient implementation of the hill climbing search algorithm of DAG learning which makes the proposed method computationally competitive for the high-dimensional regime. The DAGBag procedure is implemented in the R package dagbag. version:1
arxiv-1406-2096 | RuleCNL: A Controlled Natural Language for Business Rule Specifications | http://arxiv.org/abs/1406.2096 | id:1406.2096 author:Paul Brillant Feuto Njonko, Sylviane Cardey, Peter Greenfield, Walid El Abed category:cs.SE cs.CL  published:2014-06-09 summary:Business rules represent the primary means by which companies define their business, perform their actions in order to reach their objectives. Thus, they need to be expressed unambiguously to avoid inconsistencies between business stakeholders and formally in order to be machine-processed. A promising solution is the use of a controlled natural language (CNL) which is a good mediator between natural and formal languages. This paper presents RuleCNL, which is a CNL for defining business rules. Its core feature is the alignment of the business rule definition with the business vocabulary which ensures traceability and consistency with the business domain. The RuleCNL tool provides editors that assist end-users in the writing process and automatic mappings into the Semantics of Business Vocabulary and Business Rules (SBVR) standard. SBVR is grounded in first order logic and includes constructs called semantic formulations that structure the meaning of rules. version:1
arxiv-1406-2049 | Image Tag Completion by Low-rank Factorization with Dual Reconstruction Structure Preserved | http://arxiv.org/abs/1406.2049 | id:1406.2049 author:Xue Li, Yu-Jin Zhang, Bin Shen, Bao-Di Liu category:cs.CV cs.IR  published:2014-06-09 summary:A novel tag completion algorithm is proposed in this paper, which is designed with the following features: 1) Low-rank and error s-parsity: the incomplete initial tagging matrix D is decomposed into the complete tagging matrix A and a sparse error matrix E. However, instead of minimizing its nuclear norm, A is further factor-ized into a basis matrix U and a sparse coefficient matrix V, i.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables our algorithm to recover latent structures from noisy initial data and avoid performing too much denoising; 2) Local reconstruction structure consistency: to steer the completion of D, the local linear reconstruction structures in feature space and tag space are obtained and preserved by U and V respectively. Such a scheme could alleviate the negative effect of distances measured by low-level features and incomplete tags. Thus, we can seek a balance between exploiting as much information and not being mislead to suboptimal performance. Experiments conducted on Corel5k dataset and the newly issued Flickr30Concepts dataset demonstrate the effectiveness and efficiency of the proposed method. version:1
arxiv-1406-2031 | Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts | http://arxiv.org/abs/1406.2031 | id:1406.2031 author:Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille category:cs.CV  published:2014-06-08 summary:Detecting objects becomes difficult when we need to deal with large shape deformation, occlusion and low resolution. We propose a novel approach to i) handle large deformations and partial occlusions in animals (as examples of highly deformable objects), ii) describe them in terms of body parts, and iii) detect them when their body parts are hard to detect (e.g., animals depicted at low resolution). We represent the holistic object and body parts separately and use a fully connected model to arrange templates for the holistic object and body parts. Our model automatically decouples the holistic object or body parts from the model when they are hard to detect. This enables us to represent a large number of holistic object and body part combinations to better deal with different "detectability" patterns caused by deformations, occlusion and/or low resolution. We apply our method to the six animal categories in the PASCAL VOC dataset and show that our method significantly improves state-of-the-art (by 4.1% AP) and provides a richer representation for objects. During training we use annotations for body parts (e.g., head, torso, etc), making use of a new dataset of fully annotated object parts for PASCAL VOC 2010, which provides a mask for each part. version:1
arxiv-1406-2022 | Two-dimensional Sentiment Analysis of text | http://arxiv.org/abs/1406.2022 | id:1406.2022 author:Rahul Tejwani category:cs.IR cs.CL  published:2014-06-08 summary:Sentiment Analysis aims to get the underlying viewpoint of the text, which could be anything that holds a subjective opinion, such as an online review, Movie rating, Comments on Blog posts etc. This paper presents a novel approach that classify text in two-dimensional Emotional space, based on the sentiments of the author. The approach uses existing lexical resources to extract feature set, which is trained using Supervised Learning techniques. version:1
arxiv-1406-1943 | Structured Dictionary Learning for Classification | http://arxiv.org/abs/1406.1943 | id:1406.1943 author:Yuanming Suo, Minh Dao, Umamahesh Srinivas, Vishal Monga, Trac D. Tran category:cs.CV  published:2014-06-08 summary:Sparsity driven signal processing has gained tremendous popularity in the last decade. At its core, the assumption is that the signal of interest is sparse with respect to either a fixed transformation or a signal dependent dictionary. To better capture the data characteristics, various dictionary learning methods have been proposed for both reconstruction and classification tasks. For classification particularly, most approaches proposed so far have focused on designing explicit constraints on the sparse code to improve classification accuracy while simply adopting $l_0$-norm or $l_1$-norm for sparsity regularization. Motivated by the success of structured sparsity in the area of Compressed Sensing, we propose a structured dictionary learning framework (StructDL) that incorporates the structure information on both group and task levels in the learning process. Its benefits are two-fold: (i) the label consistency between dictionary atoms and training data are implicitly enforced; and (ii) the classification performance is more robust in the cases of a small dictionary size or limited training data than other techniques. Using the subspace model, we derive the conditions for StructDL to guarantee the performance and show theoretically that StructDL is superior to $l_0$-norm or $l_1$-norm regularized dictionary learning for classification. Extensive experiments have been performed on both synthetic simulations and real world applications, such as face recognition and object classification, to demonstrate the validity of the proposed DL framework. version:1
arxiv-1406-2613 | Simulation based Hardness Evaluation of a Multi-Objective Genetic Algorithm | http://arxiv.org/abs/1406.2613 | id:1406.2613 author:Shahab U. Ansari, Sameen Mansha category:cs.NE  published:2014-06-07 summary:Studies have shown that multi-objective optimization problems are hard problems. Such problems either require longer time to converge to an optimum solution, or may not converge at all. Recently some researchers have claimed that real culprit for increasing the hardness of multi-objective problems are not the number of objectives themselves rather it is the increased size of solution set, incompatibility of solutions, and high probability of finding suboptimal solution due to increased number of local maxima. In this work, we have setup a simple framework for the evaluation of hardness of multi-objective genetic algorithms (MOGA). The algorithm is designed for a pray-predator game where a player is to improve its lifespan, challenging level and usability of the game arena through number of generations. A rigorous set of experiments are performed for quantifying the hardness in terms of evolution for increasing number of objective functions. In genetic algorithm, crossover and mutation with equal probability are applied to create offspring in each generation. First, each objective function is maximized individually by ranking the competing players on the basis of the fitness (cost) function, and then a multi-objective cost function (sum of individual cost functions) is maximized with ranking, and also without ranking where dominated solutions are also allowed to evolve. version:1
arxiv-1402-1869 | On the Number of Linear Regions of Deep Neural Networks | http://arxiv.org/abs/1402.1869 | id:1402.1869 author:Guido Montúfar, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG cs.NE  published:2014-02-08 summary:We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers. version:2
arxiv-1406-1925 | Shape-from-intrinsic operator | http://arxiv.org/abs/1406.1925 | id:1406.1925 author:Davide Boscaini, Davide Eynard, Michael M. Bronstein category:cs.CV  published:2014-06-07 summary:Shape-from-X is an important class of problems in the fields of geometry processing, computer graphics, and vision, attempting to recover the structure of a shape from some observations. In this paper, we formulate the problem of shape-from-operator (SfO), recovering an embedding of a mesh from intrinsic differential operators defined on the mesh. Particularly interesting instances of our SfO problem include synthesis of shape analogies, shape-from-Laplacian reconstruction, and shape exaggeration. Numerically, we approach the SfO problem by splitting it into two optimization sub-problems that are applied in an alternating scheme: metric-from-operator (reconstruction of the discrete metric from the intrinsic operator) and embedding-from-metric (finding a shape embedding that would realize a given metric, a setting of the multidimensional scaling problem). version:1
arxiv-1406-1916 | Compressed Gaussian Process | http://arxiv.org/abs/1406.1916 | id:1406.1916 author:Rajarshi Guhaniyogi, David B. Dunson category:stat.ML  published:2014-06-07 summary:Nonparametric regression for massive numbers of samples (n) and features (p) is an increasingly important problem. In big n settings, a common strategy is to partition the feature space, and then separately apply simple models to each partition set. We propose an alternative approach, which avoids such partitioning and the associated sensitivity to neighborhood choice and distance metrics, by using random compression combined with Gaussian process regression. The proposed approach is particularly motivated by the setting in which the response is conditionally independent of the features given the projection to a low dimensional manifold. Conditionally on the random compression matrix and a smoothness parameter, the posterior distribution for the regression surface and posterior predictive distributions are available analytically. Running the analysis in parallel for many random compression matrices and smoothness parameters, model averaging is used to combine the results. The algorithm can be implemented rapidly even in very big n and p problems, has strong theoretical justification, and is found to yield state of the art predictive performance. version:1
arxiv-1406-1906 | Refinement-Cut: User-Guided Segmentation Algorithm for Translational Science | http://arxiv.org/abs/1406.1906 | id:1406.1906 author:Jan Egger category:cs.CV  published:2014-06-07 summary:In this contribution, a semi-automatic segmentation algorithm for (medical) image analysis is presented. More precise, the approach belongs to the category of interactive contouring algorithms, which provide real-time feedback of the segmentation result. However, even with interactive real-time contouring approaches there are always cases where the user cannot find a satisfying segmentation, e.g. due to homogeneous appearances between the object and the background, or noise inside the object. For these difficult cases the algorithm still needs additional user support. However, this additional user support should be intuitive and rapid integrated into the segmentation process, without breaking the interactive real-time segmentation feedback. I propose a solution where the user can support the algorithm by an easy and fast placement of one or more seed points to guide the algorithm to a satisfying segmentation result also in difficult cases. These additional seed(s) restrict(s) the calculation of the segmentation for the algorithm, but at the same time, still enable to continue with the interactive real-time feedback segmentation. For a practical and genuine application in translational science, the approach has been tested on medical data from the clinical routine in 2D and 3D. version:1
arxiv-1403-3378 | Box Drawings for Learning with Imbalanced Data | http://arxiv.org/abs/1403.3378 | id:1403.3378 author:Siong Thye Goh, Cynthia Rudin category:stat.ML cs.LG  published:2014-03-13 summary:The vast majority of real world classification problems are imbalanced, meaning there are far fewer data from the class of interest (the positive class) than from other classes. We propose two machine learning algorithms to handle highly imbalanced classification problems. The classifiers constructed by both methods are created as unions of parallel axis rectangles around the positive examples, and thus have the benefit of being interpretable. The first algorithm uses mixed integer programming to optimize a weighted balance between positive and negative class accuracies. Regularization is introduced to improve generalization performance. The second method uses an approximation in order to assist with scalability. Specifically, it follows a \textit{characterize then discriminate} approach, where the positive class is characterized first by boxes, and then each box boundary becomes a separate discriminative classifier. This method has the computational advantages that it can be easily parallelized, and considers only the relevant regions of feature space. version:2
arxiv-1406-1870 | Toward verbalizing ontologies in isiZulu | http://arxiv.org/abs/1406.1870 | id:1406.1870 author:C. Maria Keet, Langa Khumalo category:cs.CL I.2.1  published:2014-06-07 summary:IsiZulu is one of the eleven official languages of South Africa and roughly half the population can speak it. It is the first (home) language for over 10 million people in South Africa. Only a few computational resources exist for isiZulu and its related Nguni languages, yet the imperative for tool development exists. We focus on natural language generation, and the grammar options and preferences in particular, which will inform verbalization of knowledge representation languages and could contribute to machine translation. The verbalization pattern specification shows that the grammar rules are elaborate and there are several options of which one may have preference. We devised verbalization patterns for subsumption, basic disjointness, existential and universal quantification, and conjunction. This was evaluated in a survey among linguists and non-linguists. Some differences between linguists and non-linguists can be observed, with the former much more in agreement, and preferences depend on the overall structure of the sentence, such as singular for subsumption and plural in other cases. version:1
arxiv-1402-1864 | An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning | http://arxiv.org/abs/1402.1864 | id:1402.1864 author:Andreas Maurer, Massimiliano Pontil, Bernardino Romera-Paredes category:cs.LG stat.ML  published:2014-02-08 summary:From concentration inequalities for the suprema of Gaussian or Rademacher processes an inequality is derived. It is applied to sharpen existing and to derive novel bounds on the empirical Rademacher complexities of unit balls in various norms appearing in the context of structured sparsity and multitask dictionary learning or matrix factorization. A key role is played by the largest eigenvalue of the data covariance matrix. version:2
arxiv-1406-1831 | Analyzing noise in autoencoders and deep networks | http://arxiv.org/abs/1406.1831 | id:1406.1831 author:Ben Poole, Jascha Sohl-Dickstein, Surya Ganguli category:cs.NE cs.LG  published:2014-06-06 summary:Autoencoders have emerged as a useful framework for unsupervised learning of internal representations, and a wide variety of apparently conceptually disparate regularization techniques have been proposed to generate useful features. Here we extend existing denoising autoencoders to additionally inject noise before the nonlinearity, and at the hidden unit activations. We show that a wide variety of previous methods, including denoising, contractive, and sparse autoencoders, as well as dropout can be interpreted using this framework. This noise injection framework reaps practical benefits by providing a unified strategy to develop new internal representations by designing the nature of the injected noise. We show that noisy autoencoders outperform denoising autoencoders at the very task of denoising, and are competitive with other single-layer techniques on MNIST, and CIFAR-10. We also show that types of noise other than dropout improve performance in a deep network through sparsifying, decorrelating, and spreading information across representations. version:1
arxiv-1406-2639 | A New 2.5D Representation for Lymph Node Detection using Random Sets of Deep Convolutional Neural Network Observations | http://arxiv.org/abs/1406.2639 | id:1406.2639 author:Holger R. Roth, Le Lu, Ari Seff, Kevin M. Cherry, Joanne Hoffman, Shijun Wang, Jiamin Liu, Evrim Turkbey, Ronald M. Summers category:cs.CV cs.LG cs.NE  published:2014-06-06 summary:Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards 100% sensitivity at the cost of high FP levels (40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work. version:1
arxiv-1406-1411 | Advances in Learning Bayesian Networks of Bounded Treewidth | http://arxiv.org/abs/1406.1411 | id:1406.1411 author:Siqi Nie, Denis Deratani Maua, Cassio Polpo de Campos, Qiang Ji category:cs.AI cs.LG stat.ML 68T37  published:2014-06-05 summary:This work presents novel algorithms for learning Bayesian network structures with bounded treewidth. Both exact and approximate methods are developed. The exact method combines mixed-integer linear programming formulations for structure learning and treewidth computation. The approximate method consists in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and subsequently selecting, exactly or approximately, the best structure whose moral graph is a subgraph of that $k$-tree. Some properties of these methods are discussed and proven. The approaches are empirically compared to each other and to a state-of-the-art method for learning bounded treewidth structures on a collection of public data sets with up to 100 variables. The experiments show that our exact algorithm outperforms the state of the art, and that the approximate approach is fairly accurate. version:2
arxiv-1406-1770 | Computational role of eccentricity dependent cortical magnification | http://arxiv.org/abs/1406.1770 | id:1406.1770 author:Tomaso Poggio, Jim Mutch, Leyla Isik category:cs.LG q-bio.NC  published:2014-06-06 summary:We develop a sampling extension of M-theory focused on invariance to scale and translation. Quite surprisingly, the theory predicts an architecture of early vision with increasing receptive field sizes and a high resolution fovea -- in agreement with data about the cortical magnification factor, V1 and the retina. From the slope of the inverse of the magnification factor, M-theory predicts a cortical "fovea" in V1 in the order of $40$ by $40$ basic units at each receptive field size -- corresponding to a foveola of size around $26$ minutes of arc at the highest resolution, $\approx 6$ degrees at the lowest resolution. It also predicts uniform scale invariance over a fixed range of scales independently of eccentricity, while translation invariance should depend linearly on spatial frequency. Bouma's law of crowding follows in the theory as an effect of cortical area-by-cortical area pooling; the Bouma constant is the value expected if the signature responsible for recognition in the crowding experiments originates in V2. From a broader perspective, the emerging picture suggests that visual recognition under natural conditions takes place by composing information from a set of fixations, with each fixation providing recognition from a space-scale image fragment -- that is an image patch represented at a set of increasing sizes and decreasing resolutions. version:1
arxiv-1406-1765 | Linguistic Analysis of Requirements of a Space Project and their Conformity with the Recommendations Proposed by a Controlled Natural Language | http://arxiv.org/abs/1406.1765 | id:1406.1765 author:Anne Condamines, Maxime Warnier category:cs.SE cs.CL  published:2014-06-06 summary:The long term aim of the project carried out by the French National Space Agency (CNES) is to design a writing guide based on the real and regular writing of requirements. As a first step in the project, this paper proposes a lin-guistic analysis of requirements written in French by CNES engineers. The aim is to determine to what extent they conform to two rules laid down in INCOSE, a recent guide for writing requirements. Although CNES engineers are not obliged to follow any Controlled Natural Language in their writing of requirements, we believe that language regularities are likely to emerge from this task, mainly due to the writers' experience. The issue is approached using natural language processing tools to identify sentences that do not comply with INCOSE rules. We further review these sentences to understand why the recommendations cannot (or should not) always be applied when specifying large-scale projects. version:1
arxiv-1406-1691 | Towards a Better Understanding of the Local Attractor in Particle Swarm Optimization: Speed and Solution Quality | http://arxiv.org/abs/1406.1691 | id:1406.1691 author:Vanessa Lange, Manuel Schmitt, Rolf Wanka category:cs.NE I.2.8  published:2014-06-06 summary:Particle Swarm Optimization (PSO) is a popular nature-inspired meta-heuristic for solving continuous optimization problems. Although this technique is widely used, the understanding of the mechanisms that make swarms so successful is still limited. We present the first substantial experimental investigation of the influence of the local attractor on the quality of exploration and exploitation. We compare in detail classical PSO with the social-only variant where local attractors are ignored. To measure the exploration capabilities, we determine how frequently both variants return results in the neighborhood of the global optimum. We measure the quality of exploitation by considering only function values from runs that reached a search point sufficiently close to the global optimum and then comparing in how many digits such values still deviate from the global minimum value. It turns out that the local attractor significantly improves the exploration, but sometimes reduces the quality of the exploitation. As a compromise, we propose and evaluate a hybrid PSO which switches off its local attractors at a certain point in time. The effects mentioned can also be observed by measuring the potential of the swarm. version:1
arxiv-1401-8257 | Online Clustering of Bandits | http://arxiv.org/abs/1401.8257 | id:1401.8257 author:Claudio Gentile, Shuai Li, Giovanni Zappella category:cs.LG stat.ML  published:2014-01-31 summary:We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation ("bandit") strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems. version:3
arxiv-1406-1626 | Ant Colony Optimization for Inferring Key Gene Interactions | http://arxiv.org/abs/1406.1626 | id:1406.1626 author:Khalid Raza, Mahish Kohli category:cs.NE cs.CE  published:2014-06-06 summary:Inferring gene interaction network from gene expression data is an important task in systems biology research. The gene interaction network, especially key interactions, plays an important role in identifying biomarkers for disease that further helps in drug design. Ant colony optimization is an optimization algorithm based on natural evolution and has been used in many optimization problems. In this paper, we applied ant colony optimization algorithm for inferring the key gene interactions from gene expression data. The algorithm has been tested on two different kinds of benchmark datasets and observed that it successfully identify some key gene interactions. version:1
arxiv-1406-1621 | Separable Cosparse Analysis Operator Learning | http://arxiv.org/abs/1406.1621 | id:1406.1621 author:Matthias Seibert, Julian Wörmann, Rémi Gribonval, Martin Kleinsteuber category:cs.LG stat.ML  published:2014-06-06 summary:The ability of having a sparse representation for a certain class of signals has many applications in data analysis, image processing, and other research fields. Among sparse representations, the cosparse analysis model has recently gained increasing interest. Many signals exhibit a multidimensional structure, e.g. images or three-dimensional MRI scans. Most data analysis and learning algorithms use vectorized signals and thereby do not account for this underlying structure. The drawback of not taking the inherent structure into account is a dramatic increase in computational cost. We propose an algorithm for learning a cosparse Analysis Operator that adheres to the preexisting structure of the data, and thus allows for a very efficient implementation. This is achieved by enforcing a separable structure on the learned operator. Our learning algorithm is able to deal with multidimensional data of arbitrary order. We evaluate our method on volumetric data at the example of three-dimensional MRI scans. version:1
arxiv-1406-1580 | Machine learning approach for text and document mining | http://arxiv.org/abs/1406.1580 | id:1406.1580 author:Vishwanath Bijalwan, Pinki Kumari, Jordan Pascual, Vijay Bhaskar Semwal category:cs.IR cs.LG  published:2014-06-06 summary:Text Categorization (TC), also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set. If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task. TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers. In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents. version:1
