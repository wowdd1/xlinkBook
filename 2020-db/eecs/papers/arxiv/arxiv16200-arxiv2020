arxiv-1604-01171 | Restricted Isometry Constants for Gaussian and Rademacher matrices | http://arxiv.org/abs/1604.01171 | id:1604.01171 author:Sandrine Dallaporta, Yohann de Castro category:math.ST cs.IT math.IT math.PR stat.ML stat.TH  published:2016-04-05 summary:Restricted Isometry Constants (RICs) are a pivotal notion in Compressed Sensing (CS) as these constants finely assess how a linear operator is conditioned on the set of sparse vectors and hence how it performs in stable and robust sparse regression (SRSR). While it is an open problem to construct deterministic matrices with apposite RICs, one can prove that such matrices exist using random matrices models. One of the most popular model may be the sub-Gaussian matrices since it encompasses random matrices with Gaussian or Rademacher i.i.d. entries. In this paper, we provide a description of the phase transition on SRSR for those matrices using state-of-the-art (small) deviation estimates on their extreme eigenvalues. In particular, we show new upper bounds on RICs for Gaussian and Rademacher matrices. This allows us to derive a new lower bound on the probability of getting SRSR. One of the benefit of this novel approach is to broaden the scope of phase transition on RICs and SRSR to the quest of universality results in Random Matrix Theory. version:1
arxiv-1511-06442 | Fast Metric Learning For Deep Neural Networks | http://arxiv.org/abs/1511.06442 | id:1511.06442 author:Henry Gouk, Bernhard Pfahringer, Michael Cree category:cs.LG cs.CV stat.ML  published:2015-11-19 summary:Similarity metrics are a core component of many information retrieval and machine learning systems. In this work we propose a method capable of learning a similarity metric from data equipped with a binary relation. By considering only the similarity constraints, and initially ignoring the features, we are able to learn target vectors for each instance using one of several appropriately designed loss functions. A regression model can then be constructed that maps novel feature vectors to the same target vector space, resulting in a feature extractor that computes vectors for which a predefined metric is a meaningful measure of similarity. We present results on both multiclass and multi-label classification datasets that demonstrate considerably faster convergence, as well as higher accuracy on the majority of the intrinsic evaluation tasks and all extrinsic evaluation tasks. version:5
arxiv-1511-05950 | Staleness-aware Async-SGD for Distributed Deep Learning | http://arxiv.org/abs/1511.05950 | id:1511.05950 author:Wei Zhang, Suyog Gupta, Xiangru Lian, Ji Liu category:cs.LG  published:2015-11-18 summary:Deep neural networks have been shown to achieve state-of-the-art performance in several machine learning tasks. Stochastic Gradient Descent (SGD) is the preferred optimization algorithm for training these networks and asynchronous SGD (ASGD) has been widely adopted for accelerating the training of large-scale deep networks in a distributed computing environment. However, in practice it is quite challenging to tune the training hyperparameters (such as learning rate) when using ASGD so as achieve convergence and linear speedup, since the stability of the optimization algorithm is strongly influenced by the asynchronous nature of parameter updates. In this paper, we propose a variant of the ASGD algorithm in which the learning rate is modulated according to the gradient staleness and provide theoretical guarantees for convergence of this algorithm. Experimental verification is performed on commonly-used image classification benchmarks: CIFAR10 and Imagenet to demonstrate the superior effectiveness of the proposed approach, compared to SSGD (Synchronous SGD) and the conventional ASGD algorithm. version:5
arxiv-1604-01146 | Less is more: zero-shot learning from online textual documents with noise suppression | http://arxiv.org/abs/1604.01146 | id:1604.01146 author:Ruizhi Qiao, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2016-04-05 summary:Classifying a visual concept merely from its associated online textual source, such as a Wikipedia article, is an attractive research topic in zero-shot learning because it alleviates the burden of manually collecting semantic attributes. Several recent works have pursued this approach by exploring various ways of connecting the visual and text domains. This paper revisits this idea by stepping further to consider one important factor: the textual representation is usually too noisy for the zero-shot learning application. This consideration motivates us to design a simple-but-effective zero-shot learning method capable of suppressing noise in the text. More specifically, we propose an $l_{2,1}$-norm based objective function which can simultaneously suppress the noisy signal in the text and learn a function to match the text document and visual features. We also develop an optimization algorithm to efficiently solve the resulting problem. By conducting experiments on two large datasets, we demonstrate that the proposed method significantly outperforms the competing methods which rely on online information sources but without explicit noise suppression. We further make an in-depth analysis of the proposed method and provide insight as to what kind of information in documents is useful for zero-shot learning. version:1
arxiv-1602-00370 | Visualizing Large-scale and High-dimensional Data | http://arxiv.org/abs/1602.00370 | id:1602.00370 author:Jian Tang, Jingzhou Liu, Ming Zhang, Qiaozhu Mei category:cs.LG cs.HC  published:2016-02-01 summary:We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets. version:2
arxiv-1511-02821 | Partial Membership Latent Dirichlet Allocation | http://arxiv.org/abs/1511.02821 | id:1511.02821 author:Chao Chen, Alina Zare, J. Tory Cobb category:stat.ML cs.CV  published:2015-11-09 summary:Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting imagery. These models are confined to crisp segmentation. Yet, there are many images in which some regions cannot be assigned a crisp label (e.g., transition regions between a foggy sky and the ground or between sand and water at a beach). In these cases, a visual word is best represented with partial memberships across multiple topics. To address this, we present a partial membership latent Dirichlet allocation (PM-LDA) model and associated parameter estimation algorithms. Experimental results on two natural image datasets and one SONAR image dataset show that PM-LDA can produce both crisp and soft semantic image segmentations; a capability existing methods do not have. version:2
arxiv-1603-07076 | Viewpoint Invariant 3D Human Pose Estimation with Recurrent Error Feedback | http://arxiv.org/abs/1603.07076 | id:1603.07076 author:Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, Li Fei-Fei category:cs.CV  published:2016-03-23 summary:We propose a viewpoint invariant model for 3D human pose estimation from a single depth image. To achieve viewpoint invariance, our deep discriminative model embeds local regions into a learned viewpoint invariant feature space. Formulated as a multi-task learning problem, our model is able to selectively predict partial poses in the presence of noise and occlusion. Our approach leverages a convolutional and recurrent network with a top-down error feedback mechanism to self-correct previous pose estimates in an end-to-end manner. We evaluate our model on a previously published depth dataset and a newly collected human pose dataset containing 100K annotated depth images from extreme viewpoints. Experiments show that our model achieves competitive performance on frontal views while achieving state-of-the-art performance on alternate viewpoints. version:2
arxiv-1604-01109 | Counting Grid Aggregation for Event Retrieval and Recognition | http://arxiv.org/abs/1604.01109 | id:1604.01109 author:Zhanning Gao, Gang Hua, Dongqing Zhang, Jianru Xue, Nanning Zheng category:cs.CV  published:2016-04-05 summary:Event retrieval and recognition in a large corpus of videos necessitates a holistic fixed-size visual representation at the video clip level that is comprehensive, compact, and yet discriminative. It shall comprehensively aggregate information across relevant video frames, while suppress redundant information, leading to a compact representation that can effectively differentiate among different visual events. In search for such a representation, we propose to build a spatially consistent counting grid model to aggregate together deep features extracted from different video frames. The spatial consistency of the counting grid model is achieved by introducing a prior model estimated from a large corpus of video data. The counting grid model produces an intermediate tensor representation for each video, which automatically identifies and removes the feature redundancy across the different frames. The tensor representation is subsequently reduced to a fixed-size vector representation by averaging over the counting grid. When compared to existing methods on both event retrieval and event classification benchmarks, we achieve significantly better accuracy with much more compact representation. version:1
arxiv-1602-04375 | Science Question Answering using Instructional Materials | http://arxiv.org/abs/1602.04375 | id:1602.04375 author:Mrinmaya Sachan, Avinava Dubey, Eric P. Xing category:cs.CL cs.AI cs.IR cs.LG  published:2016-02-13 summary:We provide a solution for elementary science test using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines. version:2
arxiv-1604-01093 | BundleFusion: Real-time Globally Consistent 3D Reconstruction using On-the-fly Surface Re-integration | http://arxiv.org/abs/1604.01093 | id:1604.01093 author:Angela Dai, Matthias Nießner, Michael Zollhöfer, Shahram Izadi, Christian Theobalt category:cs.GR cs.CV  published:2016-04-05 summary:Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed reality and robotic applications. However, scalability brings challenges of drift in pose estimation, introducing significant errors in the accumulated model. Approaches often require hours of offline processing to globally correct model errors. Recent online methods demonstrate compelling results, but suffer from: (1) needing minutes to perform online correction preventing true real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation resulting in many tracking failures; or (3) supporting only unstructured point-based representations, which limit scan quality and applicability. We systematically address these issues with a novel, real-time, end-to-end reconstruction framework. At its core is a robust pose estimation strategy, optimizing per frame for a global set of camera poses by considering the complete history of RGB-D input with an efficient hierarchical approach. We remove the heavy reliance on temporal tracking, and continually localize to the globally optimized frames instead. We contribute a parallelizable optimization framework, which employs correspondences based on sparse features and dense geometric and photometric matching. Our approach estimates globally optimized (i.e., bundle adjusted) poses in real-time, supports robust tracking with recovery from gross tracking failures (i.e., relocalization), and re-estimates the 3D model in real-time to ensure global consistency; all within a single framework. Our approach outperforms state-of-the-art online systems with quality on par to offline methods, but with unprecedented speed and scan completeness. Our framework leads to a comprehensive online scanning solution for large indoor environments, enabling ease of use and high-quality results. version:1
arxiv-1310-5796 | Relative Deviation Learning Bounds and Generalization with Unbounded Loss Functions | http://arxiv.org/abs/1310.5796 | id:1310.5796 author:Corinna Cortes, Spencer Greenberg, Mehryar Mohri category:cs.LG  published:2013-10-22 summary:We present an extensive analysis of relative deviation bounds, including detailed proofs of two-sided inequalities and their implications. We also give detailed proofs of two-sided generalization bounds that hold in the general case of unbounded loss functions, under the assumption that a moment of the loss is bounded. These bounds are useful in the analysis of importance weighting and other learning tasks such as unbounded regression. version:4
arxiv-1604-01088 | Optimal Parameter Settings for the $(1+(λ, λ))$ Genetic Algorithm | http://arxiv.org/abs/1604.01088 | id:1604.01088 author:Benjamin Doerr category:cs.NE cs.DS  published:2016-04-04 summary:The $(1+(\lambda,\lambda))$ genetic algorithm is one of the few algorithms for which a super-constant speed-up through the use of crossover could be proven. So far, this algorithm has been used with parameters based also on intuitive considerations. In this work, we rigorously regard the whole parameter space and show that the asymptotic time complexity proven by Doerr and Doerr (GECCO 2015) for the intuitive choice is best possible among all settings for population size, mutation probability, and crossover bias. version:1
arxiv-1603-06160 | Stochastic Variance Reduction for Nonconvex Optimization | http://arxiv.org/abs/1603.06160 | id:1603.06160 author:Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, Alex Smola category:math.OC cs.LG cs.NE stat.ML  published:2016-03-19 summary:We study nonconvex finite-sum problems and analyze stochastic variance reduced gradient (SVRG) methods for them. SVRG and related methods have recently surged into prominence for convex optimization given their edge over stochastic gradient descent (SGD); but their theoretical analysis almost exclusively assumes convexity. In contrast, we prove non-asymptotic rates of convergence (to stationary points) of SVRG for nonconvex optimization, and show that it is provably faster than SGD and gradient descent. We also analyze a subclass of nonconvex problems on which SVRG attains linear convergence to the global optimum. We extend our analysis to mini-batch variants of SVRG, showing (theoretical) linear speedup due to mini-batching in parallel settings. version:2
arxiv-1604-01075 | A Dynamic Bayesian Network Model for Inventory Level Estimation in Retail Marketing | http://arxiv.org/abs/1604.01075 | id:1604.01075 author:Luis I. Reyes-Castro, Andres G. Abad category:stat.ML  published:2016-04-04 summary:Many retailers today employ inventory management systems based on Re-Order Point Policies, most of which rely on the assumption that all decreases in product inventory levels result from product sales. Unfortunately, it usually happens that small but random quantities of the product get lost, stolen or broken without record as time passes, e.g., as a consequence of shoplifting. This is usual for retailers handling large varieties of inexpensive products, e.g., grocery stores. In turn, over time these discrepancies lead to stock freezing problems, i.e., situations where the system believes the stock is above the re-order point but the actual stock is at zero, and so no replenishments or sales occur. Motivated by these issues, we model the interaction between sales, losses, replenishments and inventory levels as a Dynamic Bayesian Network (DBN), where the inventory levels are unobserved (i.e., hidden) variables we wish to estimate. We present an Expectation-Maximization (EM) algorithm to estimate the parameters of the sale and loss distributions, which relies on solving a one-dimensional dynamic program for the E-step and on solving two separate one-dimensional nonlinear programs for the M-step. version:1
arxiv-1602-04912 | Uniform {\varepsilon}-Stability of Distributed Nonlinear Filtering over DNAs: Gaussian-Finite HMMs | http://arxiv.org/abs/1602.04912 | id:1602.04912 author:Dionysios S. Kalogerias, Athina P. Petropulu category:math.ST math.OC stat.ML stat.TH  published:2016-02-16 summary:In this work, we study stability of distributed filtering of Markov chains with finite state space, partially observed in conditionally Gaussian noise. We consider a nonlinear filtering scheme over a Distributed Network of Agents (DNA), which relies on the distributed evaluation of the likelihood part of the centralized nonlinear filter and is based on a particular specialization of the Alternating Direction Method of Multipliers (ADMM) for fast average consensus. Assuming the same number of consensus steps between any two consecutive noisy measurements for each sensor in the network, we fully characterize a minimal number of such steps, such that the distributed filter remains uniformly stable with a prescribed accuracy level, {\varepsilon} \in (0,1], within a finite operational horizon, T, and across all sensors. Stability is in the sense of the of the \ell_1-norm between the centralized and distributed versions of the posterior at each sensor, and at each time within T. Roughly speaking, our main result shows that uniform {\varepsilon}-stability of the distributed filtering process depends only loglinearly on T and (roughly) the size of the network, and only logarithmically on 1/{\varepsilon}. If this total loglinear bound is fulfilled, any additional consensus iterations will incur a fully quantified further exponential decay in the consensus error. Our bounds are universal, in the sense that they are independent of the particular structure of the Gaussian Hidden Markov Model (HMM) under consideration. version:2
arxiv-1604-00990 | Direct Visual Odometry using Bit-Planes | http://arxiv.org/abs/1604.00990 | id:1604.00990 author:Hatem Alismail, Brett Browning, Simon Lucey category:cs.RO cs.CV  published:2016-04-04 summary:Feature descriptors, such as SIFT and ORB, are well-known for their robustness to illumination changes, which has made them popular for feature-based VSLAM\@. However, in degraded imaging conditions such as low light, low texture, blur and specular reflections, feature extraction is often unreliable. In contrast, direct VSLAM methods which estimate the camera pose by minimizing the photometric error using raw pixel intensities are often more robust to low textured environments and blur. Nonetheless, at the core of direct VSLAM is the reliance on a consistent photometric appearance across images, otherwise known as the brightness constancy assumption. Unfortunately, brightness constancy seldom holds in real world applications. In this work, we overcome brightness constancy by incorporating feature descriptors into a direct visual odometry framework. This combination results in an efficient algorithm that combines the strength of both feature-based algorithms and direct methods. Namely, we achieve robustness to arbitrary photometric variations while operating in low-textured and poorly lit environments. Our approach utilizes an efficient binary descriptor, which we call Bit-Planes, and show how it can be used in the gradient-based optimization required by direct methods. Moreover, we show that the squared Euclidean distance between Bit-Planes is equivalent to the Hamming distance. Hence, the descriptor may be used in least squares optimization without sacrificing its photometric invariance. Finally, we present empirical results that demonstrate the robustness of the approach in poorly lit underground environments. version:1
arxiv-1604-00989 | Clustering Millions of Faces by Identity | http://arxiv.org/abs/1604.00989 | id:1604.00989 author:Charles Otto, Dayong Wang, Anil K. Jain category:cs.CV  published:2016-04-04 summary:In this work, we attempt to address the following problem: Given a large number of unlabeled face images, cluster them into the individual identities present in this data. We consider this a relevant problem in different application scenarios ranging from social media to law enforcement. In large-scale scenarios the number of faces in the collection can be of the order of hundreds of million, while the number of clusters can range from a few thousand to millions--leading to difficulties in terms of both run-time complexity and evaluating clustering and per-cluster quality. An efficient and effective Rank-Order clustering algorithm is developed to achieve the desired scalability, and better clustering accuracy than other well-known algorithms such as k-means and spectral clustering. We cluster up to 123 million face images into over 10 million clusters, and analyze the results in terms of both external cluster quality measures (known face labels) and internal cluster quality measures (unknown face labels) and run-time. Our algorithm achieves an F-measure of 0.87 on a benchmark unconstrained face dataset (LFW, consisting of 13K faces), and 0.27 on the largest dataset considered (13K images in LFW, plus 123M distractor images). Additionally, we present preliminary work on video frame clustering (achieving 0.71 F-measure when clustering all frames in the benchmark YouTube Faces dataset). A per-cluster quality measure is developed which can be used to rank individual clusters and to automatically identify a subset of good quality clusters for manual exploration. version:1
arxiv-1603-09382 | Deep Networks with Stochastic Depth | http://arxiv.org/abs/1603.09382 | id:1603.09382 author:Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Weinberger category:cs.LG cs.CV cs.NE  published:2016-03-30 summary:Very deep convolutional networks with hundreds or more layers have lead to significant reductions in error on competitive benchmarks like the ImageNet or COCO tasks. Although the unmatched expressiveness of the many deep layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes and the training time can be painfully slow even on modern computers. In this paper we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and obtain deep networks. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. The resulting networks are short (in expectation) during training and deep during testing. Training Residual Networks with stochastic depth is compellingly simple to implement, yet effective. We show that this approach successfully addresses the training difficulties of deep networks and complements the recent success of Residual and Highway Networks. It reduces training time substantially and improves the test errors on almost all data sets significantly (CIFAR-10, CIFAR-100, SVHN). Intriguingly, with stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91%) on CIFAR-10. version:2
arxiv-1604-00974 | Writer-independent Feature Learning for Offline Signature Verification using Deep Convolutional Neural Networks | http://arxiv.org/abs/1604.00974 | id:1604.00974 author:Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira category:cs.CV stat.ML  published:2016-04-04 summary:Automatic Offline Handwritten Signature Verification has been researched over the last few decades from several perspectives, using insights from graphology, computer vision, signal processing, among others. In spite of the advancements on the field, building classifiers that can separate between genuine signatures and skilled forgeries (forgeries made targeting a particular signature) is still hard. We propose approaching the problem from a feature learning perspective. Our hypothesis is that, in the absence of a good model of the data generation process, it is better to learn the features from data, instead of using hand-crafted features that have no resemblance to the signature generation process. To this end, we use Deep Convolutional Neural Networks to learn features in a writer-independent format, and use this model to obtain a feature representation on another set of users, where we train writer-dependent classifiers. We tested our method in two datasets: GPDS-960 and Brazilian PUC-PR. Our experimental results show that the features learned in a subset of the users are discriminative for the other users, including across different datasets, reaching close to the state-of-the-art in the GPDS dataset, and improving the state-of-the-art in the Brazilian PUC-PR dataset. version:1
arxiv-1603-03144 | Part-of-Speech Tagging for Historical English | http://arxiv.org/abs/1603.03144 | id:1603.03144 author:Yi Yang, Jacob Eisenstein category:cs.CL cs.DL  published:2016-03-10 summary:As more historical texts are digitized, there is interest in applying natural language processing tools to these archives. However, the performance of these tools is often unsatisfactory, due to language change and genre differences. Spelling normalization heuristics are the dominant solution for dealing with historical texts, but this approach fails to account for changes in usage and vocabulary. In this empirical paper, we assess the capability of domain adaptation techniques to cope with historical texts, focusing on the classic benchmark task of part-of-speech tagging. We evaluate several domain adaptation methods on the task of tagging Early Modern English and Modern British English texts in the Penn Corpora of Historical English. We demonstrate that the Feature Embedding method for unsupervised domain adaptation outperforms word embeddings and Brown clusters, showing the importance of embedding the entire feature space, rather than just individual words. Feature Embeddings also give better performance than spelling normalization, but the combination of the two methods is better still, yielding a 5% raw improvement in tagging accuracy on Early Modern English texts. version:2
arxiv-1602-00877 | Partial Recovery Bounds for the Sparse Stochastic Block Model | http://arxiv.org/abs/1602.00877 | id:1602.00877 author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.SI math.IT stat.ML  published:2016-02-02 summary:In this paper, we study the information-theoretic limits of community detection in the symmetric two-community stochastic block model, with intra-community and inter-community edge probabilities $\frac{a}{n}$ and $\frac{b}{n}$ respectively. We consider the sparse setting, in which $a$ and $b$ do not scale with $n$, and provide upper and lower bounds on the proportion of community labels recovered on average. We provide a numerical example for which the bounds are near-matching for moderate values of $a - b$, and matching in the limit as $a-b$ grows large. version:2
arxiv-1604-00938 | Multi-Field Structural Decomposition for Question Answering | http://arxiv.org/abs/1604.00938 | id:1604.00938 author:Tomasz Jurczyk, Jinho D. Choi category:cs.CL  published:2016-04-04 summary:This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an absolute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers. version:1
arxiv-1604-00933 | Entity Type Recognition using an Ensemble of Distributional Semantic Models to Enhance Query Understanding | http://arxiv.org/abs/1604.00933 | id:1604.00933 author:Walid Shalaby, Khalifeh Al Jadda, Mohammed Korayem, Trey Grainger category:cs.CL cs.IR  published:2016-04-04 summary:We present an ensemble approach for categorizing search query entities in the recruitment domain. Understanding the types of entities expressed in a search query (Company, Skill, Job Title, etc.) enables more intelligent information retrieval based upon those entities compared to a traditional keyword-based search. Because search queries are typically very short, leveraging a traditional bag-of-words model to identify entity types would be inappropriate due to the lack of contextual information. Our approach instead combines clues from different sources of varying complexity in order to collect real-world knowledge about query entities. We employ distributional semantic representations of query entities through two models: 1) contextual vectors generated from encyclopedic corpora like Wikipedia, and 2) high dimensional word embedding vectors generated from millions of job postings using word2vec. Additionally, our approach utilizes both entity linguistic properties obtained from WordNet and ontological properties extracted from DBpedia. We evaluate our approach on a data set created at CareerBuilder; the largest job board in the US. The data set contains entities extracted from millions of job seekers/recruiters search queries, job postings, and resume documents. After constructing the distributional vectors of search entities, we use supervised machine learning to infer search entity types. Empirical results show that our approach outperforms the state-of-the-art word2vec distributional semantics model trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of 97% using the proposed distributional representations ensemble. version:1
arxiv-1604-00923 | Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning | http://arxiv.org/abs/1604.00923 | id:1604.00923 author:Philip S. Thomas, Emma Brunskill category:cs.LG cs.AI  published:2016-04-04 summary:In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates. version:1
arxiv-1604-00906 | Detecting Engagement in Egocentric Video | http://arxiv.org/abs/1604.00906 | id:1604.00906 author:Yu-Chuan Su, Kristen Grauman category:cs.CV  published:2016-04-04 summary:In a wearable camera video, we see what the camera wearer sees. While this makes it easy to know roughly what he chose to look at, it does not immediately reveal when he was engaged with the environment. Specifically, at what moments did his focus linger, as he paused to gather more information about something he saw? Knowing this answer would benefit various applications in video summarization and augmented reality, yet prior work focuses solely on the "what" question (estimating saliency, gaze) without considering the "when" (engagement). We propose a learning-based approach that uses long-term egomotion cues to detect engagement, specifically in browsing scenarios where one frequently takes in new visual information (e.g., shopping, touring). We introduce a large, richly annotated dataset for ego-engagement that is the first of its kind. Our approach outperforms a wide array of existing methods. We show engagement can be detected well independent of both scene appearance and the camera wearer's identity. version:1
arxiv-1604-00895 | HDRFusion: HDR SLAM using a low-cost auto-exposure RGB-D sensor | http://arxiv.org/abs/1604.00895 | id:1604.00895 author:Shuda Li, Ankur Handa, Yang Zhang, Andrew Calway category:cs.CV  published:2016-04-04 summary:We describe a new method for comparing frame appearance in a frame-to-model 3-D mapping and tracking system using an low dynamic range (LDR) RGB-D camera which is robust to brightness changes caused by auto exposure. It is based on a normalised radiance measure which is invariant to exposure changes and not only robustifies the tracking under changing lighting conditions, but also enables the following exposure compensation perform accurately to allow online building of high dynamic range (HDR) maps. The latter facilitates the frame-to-model tracking to minimise drift as well as better capturing light variation within the scene. Results from experiments with synthetic and real data demonstrate that the method provides both improved tracking and maps with far greater dynamic range of luminosity. version:1
arxiv-1603-05157 | Comparing Convolutional Neural Networks to Traditional Models for Slot Filling | http://arxiv.org/abs/1603.05157 | id:1603.05157 author:Heike Adel, Benjamin Roth, Hinrich Schütze category:cs.CL  published:2016-03-16 summary:We address relation classification in the context of slot filling, the task of finding and evaluating fillers like "Steve Jobs" for the slot X in "X founded Apple". We propose a convolutional neural network which splits the input sentence into three parts according to the relation arguments and compare it to state-of-the-art and traditional approaches of relation classification. Finally, we combine different methods and show that the combination is better than individual approaches. We also analyze the effect of genre differences on performance. version:2
arxiv-1604-00861 | Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings | http://arxiv.org/abs/1604.00861 | id:1604.00861 author:Giambattista Parascandolo, Heikki Huttunen, Tuomas Virtanen category:cs.SD cs.LG cs.NE  published:2016-04-04 summary:In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively. version:1
arxiv-1604-01376 | Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms | http://arxiv.org/abs/1604.01376 | id:1604.01376 author:Valentina Zantedeschi, Rémi Emonet, Marc Sebban category:cs.NA cs.LG  published:2016-04-04 summary:Many theoretical results in the machine learning domain stand only for functions that are Lipschitz continuous. Lipschitz continuity is a strong form of continuity that linearly bounds the variations of a function. In this paper, we derive tight Lipschitz constants for two families of metrics: Mahalanobis distances and bounded-space bilinear forms. To our knowledge, this is the first time the Mahalanobis distance is formally proved to be Lipschitz continuous and that such tight Lipschitz constants are derived. version:1
arxiv-1604-00834 | In narrative texts punctuation marks obey the same statistics as words | http://arxiv.org/abs/1604.00834 | id:1604.00834 author:Andrzej Kulig, Jaroslaw Kwapien, Tomasz Stanisz, Stanislaw Drozdz category:cs.CL  published:2016-04-04 summary:From a grammar point of view, the role of punctuation marks in a sentence is formally defined and well understood. In semantic analysis punctuation plays also a crucial role as a method of avoiding ambiguity of the meaning. A different situation can be observed in the statistical analyses of language samples, where the decision on whether the punctuation marks should be considered or should be neglected is seen rather as arbitrary and at present it belongs to a researcher's preference. An objective of this work is to shed some light onto this problem by providing us with an answer to the question whether the punctuation marks may be treated as ordinary words and whether they should be included in any analysis of the word co-occurences. We already know from our previous study \cite{drozdz2016} that full stops that determine the length of sentences are the main carrier of long-range correlations. Now we extend that study and analyze statistical properties of the most common punctuation marks in a few Indo-European languages, investigate their frequencies, and locate them accordingly in the Zipf rank-frequency plots as well as study their role in the word-adjacency networks. We show that, from a statistical viewpoint, the punctuation marks reveal properties that are qualitatively similar to the properties of the most frequent words like articles, conjunctions, pronouns, and prepositions. This refers to both the Zipfian analysis and the network analysis. Our results can be exploited in the computer-based analyses of large text corpora and be incorporated in the related automated systems. As a side result, we propose an efficient method of sampling the language corpora for a word-adjacency network analysis. version:1
arxiv-1604-00825 | Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers | http://arxiv.org/abs/1604.00825 | id:1604.00825 author:Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller, Wojciech Samek category:cs.CV  published:2016-04-04 summary:Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets. version:1
arxiv-1512-07679 | Deep Reinforcement Learning in Large Discrete Action Spaces | http://arxiv.org/abs/1512.07679 | id:1512.07679 author:Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, Ben Coppin category:cs.AI cs.LG cs.NE stat.ML  published:2015-12-24 summary:Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Recommender systems, industrial plants and language models are only some of the many real-world tasks involving large numbers of discrete actions for which current methods are difficult or even often impossible to apply. An ability to generalize over the set of actions as well as sub-linear complexity relative to the size of the set are both necessary to handle such tasks. Current approaches are not able to provide both of these, which motivates the work in this paper. Our proposed approach leverages prior information about the actions to embed them in a continuous space upon which it can generalize. Additionally, approximate nearest-neighbor methods allow for logarithmic-time lookup complexity relative to the number of actions, which is necessary for time-wise tractable training. This combined approach allows reinforcement learning methods to be applied to large-scale learning problems previously intractable with current methods. We demonstrate our algorithm's abilities on a series of tasks having up to one million actions. version:2
arxiv-1603-06430 | Deep Learning in Bioinformatics | http://arxiv.org/abs/1603.06430 | id:1603.06430 author:Seonwoo Min, Byunghan Lee, Sungroh Yoon category:cs.LG q-bio.GN  published:2016-03-21 summary:As we are living in the era of big data, transforming biomedical big data into valuable knowledge has been one of the most important problems in bioinformatics. At the same time, deep learning has advanced rapidly since early 2000s and is recently showing a state-of-the-art performance in various fields. So naturally, applying deep learning in bioinformatics to gain insights from data is under the spotlight of both the academia and the industry. This article reviews some research of deep learning in bioinformatics. To provide a big picture, we categorized the research by both bioinformatics domains (i.e., omics, biomedical imaging, biomedical signal processing) and deep learning architectures (i.e., deep neural network, convolutional neural network, recurrent neural network, modified neural network) as well as present brief descriptions of each work. Additionally, we introduce a few issues of deep learning in bioinformatics such as problems of class imbalance data and suggest future research directions such as multimodal deep learning. We believe that this paper could provide valuable insights and be a starting point for researchers to apply deep learning in their bioinformatics studies. version:4
arxiv-1604-00790 | Image Captioning with Deep Bidirectional LSTMs | http://arxiv.org/abs/1604.00790 | id:1604.00790 author:Cheng Wang, Haojin Yang, Christian Bartz, Christoph Meinel category:cs.CV cs.CL cs.MM  published:2016-04-04 summary:This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our model builds on a deep convolutional neural network (CNN) and two separate LSTM networks. It is capable of learning long term visual-language interactions by making use of history and future context information at high level semantic space. Two novel deep bidirectional variant models, in which we increase the depth of nonlinearity transition in different way, are proposed to learn hierarchical visual-language embeddings. Data augmentation techniques such as multi-crop, multi-scale and vertical mirror are proposed to prevent overfitting in training deep models. We visualize the evolution of bidirectional LSTM internal states over time and qualitatively analyze how our models "translate" image to sentence. Our proposed models are evaluated on caption generation and image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models achieve highly competitive performance to the state-of-the-art results on caption generation even without integrating additional mechanism (e.g. object detection, attention model etc.) and significantly outperform recent methods on retrieval task. version:1
arxiv-1604-00788 | Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models | http://arxiv.org/abs/1604.00788 | id:1604.00788 author:Minh-Thang Luong, Christopher D. Manning category:cs.CL cs.LG  published:2016-04-04 summary:Nearly all previous work in neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents a novel word-character solution to achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult the character components for rare words. Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed. The two-fold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones; at the same time, it never produces unknown words as in the case of word-based models. On the WMT'15 English to Czech translation task, this hybrid approach offers a boost of up to +7.9 BLEU points over models that do not handle unknown words. Our best hybrid system has established a new state-of-the-art result with 19.9 BLEU score. We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words. version:1
arxiv-1604-00783 | Topic Model Based Multi-Label Classification from the Crowd | http://arxiv.org/abs/1604.00783 | id:1604.00783 author:Divya Padmanabhan, Satyanath Bhat, Shirish Shevade, Y. Narahari category:cs.LG  published:2016-04-04 summary:Multi-label classification is a common supervised machine learning problem where each instance is associated with multiple classes. The key challenge in this problem is learning the correlations between the classes. An additional challenge arises when the labels of the training instances are provided by noisy, heterogeneous crowdworkers with unknown qualities. We first assume labels from a perfect source and propose a novel topic model where the present as well as the absent classes generate the latent topics and hence the words. We non-trivially extend our topic model to the scenario where the labels are provided by noisy crowdworkers. Extensive experimentation on real world datasets reveals the superior performance of the proposed model. The proposed model learns the qualities of the annotators as well, even with minimal training data. version:1
arxiv-1603-06463 | Controlling Explanatory Heatmap Resolution and Semantics via Decomposition Depth | http://arxiv.org/abs/1603.06463 | id:1603.06463 author:Sebastian Bach, Alexander Binder, Klaus-Robert Müller, Wojciech Samek category:cs.CV  published:2016-03-21 summary:We present an application of the Layer-wise Relevance Propagation (LRP) algorithm to state of the art deep convolutional neural networks and Fisher Vector classifiers to compare the image perception and prediction strategies of both classifiers with the use of visualized heatmaps. Layer-wise Relevance Propagation (LRP) is a method to compute scores for individual components of an input image, denoting their contribution to the prediction of the classifier for one particular test point. We demonstrate the impact of different choices of decomposition cut-off points during the LRP-process, controlling the resolution and semantics of the heatmap on test images from the PASCAL VOC 2007 test data set. version:3
arxiv-1604-00772 | The CMA Evolution Strategy: A Tutorial | http://arxiv.org/abs/1604.00772 | id:1604.00772 author:Nikolaus Hansen category:cs.LG stat.ML  published:2016-04-04 summary:This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain. version:1
arxiv-1604-00734 | Capturing Semantic Similarity for Entity Linking with Convolutional Neural Networks | http://arxiv.org/abs/1604.00734 | id:1604.00734 author:Matthew Francis-Landau, Greg Durrett, Dan Klein category:cs.CL  published:2016-04-04 summary:A key challenge in entity linking is making effective use of contextual information to disambiguate mentions that might refer to different entities in different contexts. We present a model that uses convolutional neural networks to capture semantic correspondence between a mention's context and a proposed target entity. These convolutional networks operate at multiple granularities to exploit various kinds of topic information, and their rich parameterization gives them the capacity to learn which n-grams characterize different topics. We combine these networks with a sparse linear model to achieve state-of-the-art performance on multiple entity linking datasets, outperforming the prior systems of Durrett and Klein (2014) and Nguyen et al. (2014). version:1
arxiv-1604-00730 | Waterdrop Stereo | http://arxiv.org/abs/1604.00730 | id:1604.00730 author:Shaodi You, Robby T. Tan, Rei Kawakami, Yasuhiro Mukaigawa, Katsushi Ikeuchi category:cs.CV  published:2016-04-04 summary:This paper introduces depth estimation from water drops. The key idea is that a single water drop adhered to window glass is totally transparent and convex, and thus optically acts like a fisheye lens. If we have more than one water drop in a single image, then through each of them we can see the environment with different view points, similar to stereo. To realize this idea, we need to rectify every water drop imagery to make radially distorted planar surfaces look flat. For this rectification, we consider two physical properties of water drops: (1) A static water drop has constant volume, and its geometric convex shape is determined by the balance between the tension force and gravity. This implies that the 3D geometric shape can be obtained by minimizing the overall potential energy, which is the sum of the tension energy and the gravitational potential energy. (2) The imagery inside a water-drop is determined by the water-drop 3D shape and total reflection at the boundary. This total reflection generates a dark band commonly observed in any adherent water drops. Hence, once the 3D shape of water drops are recovered, we can rectify the water drop images through backward raytracing. Subsequently, we can compute depth using stereo. In addition to depth estimation, we can also apply image refocusing. Experiments on real images and a quantitative evaluation show the effectiveness of our proposed method. To our best knowledge, never before have adherent water drops been used to estimate depth. version:1
arxiv-1502-01710 | Text Understanding from Scratch | http://arxiv.org/abs/1502.01710 | id:1502.01710 author:Xiang Zhang, Yann LeCun category:cs.LG cs.CL  published:2015-02-05 summary:This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up to abstract text concepts, using temporal convolutional networks (ConvNets). We apply ConvNets to various large-scale datasets, including ontology classification, sentiment analysis, and text categorization. We show that temporal ConvNets can achieve astonishing performance without the knowledge of words, phrases, sentences and any other syntactic or semantic structures with regards to a human language. Evidence shows that our models can work for both English and Chinese. version:5
arxiv-1509-01626 | Character-level Convolutional Networks for Text Classification | http://arxiv.org/abs/1509.01626 | id:1509.01626 author:Xiang Zhang, Junbo Zhao, Yann LeCun category:cs.LG cs.CL  published:2015-09-04 summary:This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks. version:3
arxiv-1511-05190 | Jet-Images -- Deep Learning Edition | http://arxiv.org/abs/1511.05190 | id:1511.05190 author:Luke de Oliveira, Michael Kagan, Lester Mackey, Benjamin Nachman, Ariel Schwartzman category:hep-ph physics.data-an stat.ML  published:2015-11-16 summary:Building on the notion of a particle physics detector as a camera and the collimated streams of high energy particles, or jets, it measures as an image, we investigate the potential of machine learning techniques based on deep learning architectures to identify highly boosted W bosons. Modern deep learning algorithms trained on jet images can out-perform standard physically-motivated feature driven approaches to jet tagging. We develop techniques for visualizing how these features are learned by the network and what additional information is used to improve performance. This interplay between physically-motivated feature driven tools and supervised learning algorithms is general and can be used to significantly increase the sensitivity to discover new particles and new forces, and gain a deeper understanding of the physics within jets. version:2
arxiv-1512-05726 | Semi-supervised Question Retrieval with Gated Convolutions | http://arxiv.org/abs/1512.05726 | id:1512.05726 author:Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi Jaakkola, Katerina Tymoshenko, Alessandro Moschitti, Lluis Marquez category:cs.CL cs.NE  published:2015-12-17 summary:Question answering forums are rapidly growing in size with no effective automated ability to refer to and reuse answers already available for previous posted questions. In this paper, we develop a methodology for finding semantically related questions. The task is difficult since 1) key pieces of information are often buried in extraneous details in the question body and 2) available annotations on similar questions are scarce and fragmented. We design a recurrent and convolutional model (gated convolution) to effectively map questions to their semantic representations. The models are pre-trained within an encoder-decoder framework (from body to title) on the basis of the entire raw corpus, and fine-tuned discriminatively from limited annotations. Our evaluation demonstrates that our model yields substantial gains over a standard IR baseline and various neural network architectures (including CNNs, LSTMs and GRUs). version:2
arxiv-1511-00060 | Top-down Tree Long Short-Term Memory Networks | http://arxiv.org/abs/1511.00060 | id:1511.00060 author:Xingxing Zhang, Liang Lu, Mirella Lapata category:cs.CL cs.LG  published:2015-10-31 summary:Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance. version:3
arxiv-1511-05960 | ABC-CNN: An Attention Based Convolutional Neural Network for Visual Question Answering | http://arxiv.org/abs/1511.05960 | id:1511.05960 author:Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia category:cs.CV  published:2015-11-18 summary:We propose a novel attention based deep learning architecture for visual question answering task (VQA). Given an image and an image related natural language question, VQA generates the natural language answer for the question. Generating the correct answers requires the model's attention to focus on the regions corresponding to the question, because different questions inquire about the attributes of different image regions. We introduce an attention based configurable convolutional neural network (ABC-CNN) to learn such question-guided attention. ABC-CNN determines an attention map for an image-question pair by convolving the image feature map with configurable convolutional kernels derived from the question's semantics. We evaluate the ABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR, and VQA dataset. ABC-CNN model achieves significant improvements over state-of-the-art methods on these datasets. The question-guided attention generated by ABC-CNN is also shown to reflect the regions that are highly relevant to the questions. version:2
arxiv-1604-00697 | A New Learning Method for Inference Accuracy, Core Occupation, and Performance Co-optimization on TrueNorth Chip | http://arxiv.org/abs/1604.00697 | id:1604.00697 author:Wei Wen, Chunpeng Wu, Yandan Wang, Kent Nixon, Qing Wu, Mark Barnell, Hai Li, Yiran Chen category:cs.NE cs.AI C.1.3; I.2.6; I.5.1  published:2016-04-03 summary:IBM TrueNorth chip uses digital spikes to perform neuromorphic computing and achieves ultrahigh execution parallelism and power efficiency. However, in TrueNorth chip, low quantization resolution of the synaptic weights and spikes significantly limits the inference (e.g., classification) accuracy of the deployed neural network model. Existing workaround, i.e., averaging the results over multiple copies instantiated in spatial and temporal domains, rapidly exhausts the hardware resources and slows down the computation. In this work, we propose a novel learning method on TrueNorth platform that constrains the random variance of each computation copy and reduces the number of needed copies. Compared to the existing learning method, our method can achieve up to 68.8% reduction of the required neuro-synaptic cores or 6.5X speedup, with even slightly improved inference accuracy. version:1
arxiv-1603-00391 | Noisy Activation Functions | http://arxiv.org/abs/1603.00391 | id:1603.00391 author:Caglar Gulcehre, Marcin Moczulski, Misha Denil, Yoshua Bengio category:cs.LG cs.NE stat.ML  published:2016-03-01 summary:Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradient. Large noise will dominate the noise-free gradient and allow stochastic gradient descent toexplore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps training in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results. version:3
arxiv-0812-4044 | The Offset Tree for Learning with Partial Labels | http://arxiv.org/abs/0812.4044 | id:0812.4044 author:Alina Beygelzimer, John Langford category:cs.LG cs.AI  published:2008-12-21 summary:We present an algorithm, called the Offset Tree, for learning to make decisions in situations where the payoff of only one choice is observed, rather than all choices. The algorithm reduces this setting to binary classification, allowing one to reuse of any existing, fully supervised binary classification algorithm in this partial information setting. We show that the Offset Tree is an optimal reduction to binary classification. In particular, it has regret at most $(k-1)$ times the regret of the binary classifier it uses (where $k$ is the number of choices), and no reduction to binary classification can do better. This reduction is also computationally optimal, both at training and test time, requiring just $O(\log_2 k)$ work to train on an example or make a prediction. Experiments with the Offset Tree show that it generally performs better than several alternative approaches. version:3
arxiv-1603-08148 | Pointing the Unknown Words | http://arxiv.org/abs/1603.08148 | id:1603.08148 author:Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, Yoshua Bengio category:cs.CL cs.LG cs.NE  published:2016-03-26 summary:The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems, including both traditional count based and deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models with attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one of the softmax layers predicts the location of a word in the source sentence, and the other softmax layer predicts a word in the shortlist vocabulary. The decision of which softmax layer to use at each timestep is adaptively made by an MLP which is conditioned on the context. We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. Using our proposed model, we observe improvements in two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset. version:2
arxiv-1604-00676 | Multi-Bias Non-linear Activation in Deep Neural Networks | http://arxiv.org/abs/1604.00676 | id:1604.00676 author:Hongyang Li, Wanli Ouyang, Xiaogang Wang category:cs.CV  published:2016-04-03 summary:As a widely used non-linear activation, Rectified Linear Unit (ReLU) separates noise and signal in a feature map by learning a threshold or bias. However, we argue that the classification of noise and signal not only depends on the magnitude of responses, but also the context of how the feature responses would be used to detect more abstract patterns in higher layers. In order to output multiple response maps with magnitude in different ranges for a particular visual pattern, existing networks employing ReLU and its variants have to learn a large number of redundant filters. In this paper, we propose a multi-bias non-linear activation (MBA) layer to explore the information hidden in the magnitudes of responses. It is placed after the convolution layer to decouple the responses to a convolution kernel into multiple maps by multi-thresholding magnitudes, thus generating more patterns in the feature space at a low computational cost. It provides great flexibility of selecting responses to different visual patterns in different magnitude ranges to form rich representations in higher layers. Such a simple and yet effective scheme achieves the state-of-the-art performance on several benchmarks. version:1
arxiv-1508-02452 | Primal-Dual Active-Set Methods for Isotonic Regression and Trend Filtering | http://arxiv.org/abs/1508.02452 | id:1508.02452 author:Zheng Han, Frank E. Curtis category:math.OC cs.LG  published:2015-08-10 summary:Isotonic regression (IR) is a non-parametric calibration method used in supervised learning. For performing large-scale IR, we propose a primal-dual active-set (PDAS) algorithm which, in contrast to the state-of-the-art Pool Adjacent Violators (PAV) algorithm, can be parallized and is easily warm-started thus well-suited in the online settings. We prove that, like the PAV algorithm, our PDAS algorithm for IR is convergent and has a work complexity of O(n), though our numerical experiments suggest that our PDAS algorithm is often faster than PAV. In addition, we propose PDAS variants (with safeguarding to ensure convergence) for solving related trend filtering (TF) problems, providing the results of experiments to illustrate their effectiveness. version:2
arxiv-1604-00653 | A Characterization of the Non-Uniqueness of Nonnegative Matrix Factorizations | http://arxiv.org/abs/1604.00653 | id:1604.00653 author:W. Pan, F. Doshi-Velez category:cs.LG stat.ML  published:2016-04-03 summary:Nonnegative matrix factorization (NMF) is a popular dimension reduction technique that produces interpretable decomposition of the data into parts. However, this decompostion is not generally identifiable (even up to permutation and scaling). While other studies have provide criteria under which NMF is identifiable, we present the first (to our knowledge) characterization of the non-identifiability of NMF. We describe exactly when and how non-uniqueness can occur, which has important implications for algorithms to efficiently discover alternate solutions, if they exist. version:1
arxiv-1604-00647 | Multi-Relational Learning at Scale with ADMM | http://arxiv.org/abs/1604.00647 | id:1604.00647 author:Lucas Drumond, Ernesto Diaz-Aviles, Lars Schmidt-Thieme category:stat.ML cs.AI cs.LG  published:2016-04-03 summary:Learning from multiple-relational data which contains noise, ambiguities, or duplicate entities is essential to a wide range of applications such as statistical inference based on Web Linked Data, recommender systems, computational biology, and natural language processing. These tasks usually require working with very large and complex datasets - e.g., the Web graph - however, current approaches to multi-relational learning are not practical for such scenarios due to their high computational complexity and poor scalability on large data. In this paper, we propose a novel and scalable approach for multi-relational factorization based on consensus optimization. Our model, called ConsMRF, is based on the Alternating Direction Method of Multipliers (ADMM) framework, which enables us to optimize each target relation using a smaller set of parameters than the state-of-the-art competitors in this task. Due to ADMM's nature, ConsMRF can be easily parallelized which makes it suitable for large multi-relational data. Experiments on large Web datasets - derived from DBpedia, Wikipedia and YAGO - show the efficiency and performance improvement of ConsMRF over strong competitors. In addition, ConsMRF near-linear scalability indicates great potential to tackle Web-scale problem sizes. version:1
arxiv-1512-05830 | Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks | http://arxiv.org/abs/1512.05830 | id:1512.05830 author:Li Shen, Zhouchen Lin, Qingming Huang category:cs.CV cs.LG  published:2015-12-18 summary:Learning deeper convolutional neural networks becomes a tendency in recent years. However, many empirical evidences suggest that performance improvement cannot be gained by simply stacking more layers. In this paper, we consider the issue from an information theoretical perspective, and propose a novel method Relay Backpropagation, that encourages the propagation of effective information through the network in training stage. By virtue of the method, we achieved the first place in ILSVRC 2015 Scene Classification Challenge. Extensive experiments on two challenging large scale datasets demonstrate the effectiveness of our method is not restricted to a specific dataset or network architecture. Our models will be available to the research community later. version:2
arxiv-1604-00606 | GAL: A Global-Attributes Assisted Labeling System for Outdoor Scenes | http://arxiv.org/abs/1604.00606 | id:1604.00606 author:Yuzhuo Ren, Chen Chen, Shangwen Li, C. -C. Jay Kuo category:cs.CV  published:2016-04-03 summary:An approach that extracts global attributes from outdoor images to facilitate geometric layout labeling is investigated in this work. The proposed Global-attributes Assisted Labeling (GAL) system exploits both local features and global attributes. First, by following a classical method, we use local features to provide initial labels for all super-pixels. Then, we develop a set of techniques to extract global attributes from 2D outdoor images. They include sky lines, ground lines, vanishing lines, etc. Finally, we propose the GAL system that integrates global attributes in the conditional random field (CRF) framework to improve initial labels so as to offer a more robust labeling result. The performance of the proposed GAL system is demonstrated and benchmarked with several state-of-the-art algorithms against a popular outdoor scene layout dataset. version:1
arxiv-1604-00600 | HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection | http://arxiv.org/abs/1604.00600 | id:1604.00600 author:Tao Kong, Anbang Yao, Yurong Chen, Fuchun Sun category:cs.CV  published:2016-04-03 summary:Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances. State-of-the-art region proposal methods usually need several thousand proposals to get high recall, thus hurting the detection efficiency. Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals, it still struggles in small-size object detection and precise localization (e.g., large IoU thresholds), mainly due to the coarseness of its feature maps. In this paper, we present a deep hierarchical network, namely HyperNet, for handling region proposal generation and object detection jointly. Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space. The Hyper Features well incorporate deep but highly semantic, intermediate but really complementary, and shallow but naturally high-resolution features of the image, thus enabling us to construct HyperNet by sharing them both in generating proposals and detecting objects via an end-to-end joint training strategy. For the deep VGG16 model, our method achieves completely leading recall and state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It runs with a speed of 5 fps (including all steps) on a GPU, thus having the potential for real-time processing. version:1
arxiv-1603-06313 | Convex block-sparse linear regression with expanders -- provably | http://arxiv.org/abs/1603.06313 | id:1603.06313 author:Anastasios Kyrillidis, Bubacarr Bah, Rouzbeh Hasheminezhad, Quoc Tran-Dinh, Luca Baldassarre, Volkan Cevher category:cs.IT math.IT math.OC stat.ML  published:2016-03-21 summary:Sparse matrices are favorable objects in machine learning and optimization. When such matrices are used, in place of dense ones, the overall complexity requirements in optimization can be significantly reduced in practice, both in terms of space and run-time. Prompted by this observation, we study a convex optimization scheme for block-sparse recovery from linear measurements. To obtain linear sketches, we use expander matrices, i.e., sparse matrices containing only few non-zeros per column. Hitherto, to the best of our knowledge, such algorithmic solutions have been only studied from a non-convex perspective. Our aim here is to theoretically characterize the performance of convex approaches under such setting. Our key novelty is the expression of the recovery error in terms of the model-based norm, while assuring that solution lives in the model. To achieve this, we show that sparse model-based matrices satisfy a group version of the null-space property. Our experimental findings on synthetic and real applications support our claims for faster recovery in the convex setting -- as opposed to using dense sensing matrices, while showing a competitive recovery performance. version:2
arxiv-1603-02199 | Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection | http://arxiv.org/abs/1603.02199 | id:1603.02199 author:Sergey Levine, Peter Pastor, Alex Krizhevsky, Deirdre Quillen category:cs.LG cs.AI cs.CV cs.RO  published:2016-03-07 summary:We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. version:3
arxiv-1604-00562 | Reasoning About Pragmatics with Neural Listeners and Speakers | http://arxiv.org/abs/1604.00562 | id:1604.00562 author:Jacob Andreas, Dan Klein category:cs.CL cs.NE  published:2016-04-02 summary:We present a model for pragmatically describing scenes, in which contrastive behavior results from a combination of inference-driven pragmatics and learned semantics. Like previous learned approaches to language generation, our model uses a simple feature-driven architecture (here a pair of neural "listener" and "speaker" models) to ground language in the world. Like inference-driven approaches to pragmatics, our model actively reasons about listener behavior when selecting utterances. For training, our approach requires only ordinary captions, annotated _without_ demonstration of the pragmatic behavior the model ultimately exhibits. In human evaluations on a referring expression game, our approach succeeds 81% of the time, compared to a 64% success rate using existing techniques. version:1
arxiv-1604-00558 | Channel Equalization Using Multilayer Perceptron Networks | http://arxiv.org/abs/1604.00558 | id:1604.00558 author:Saba Baloch, Javed Ali Baloch, Mukhtiar Ali Unar category:cs.NE  published:2016-04-02 summary:In most digital communication systems, bandwidth limited channel along with multipath propagation causes ISI (Inter Symbol Interference) to occur. This phenomenon causes distortion of the given transmitted symbol due to other transmitted symbols. With the help of equalization ISI can be reduced. This paper presents a solution to the ISI problem by performing blind equalization using ANN (Artificial Neural Networks). The simulated network is a multilayer feedforward Perceptron ANN, which has been trained by utilizing the error back-propagation algorithm. The weights of the network are updated in accordance with training of the network. This paper presents a very effective method for blind channel equalization, being more efficient than the pre-existing algorithms. The obtained results show a visible reduction in the noise content. version:1
arxiv-1604-00557 | SAM: Support Vector Machine Based Active Queue Management | http://arxiv.org/abs/1604.00557 | id:1604.00557 author:Muhammad Saleh Shah, Asim Imdad Wagan, Mukhtiar Ali Unar category:cs.NI cs.LG  published:2016-04-02 summary:Recent years have seen an increasing interest in the design of AQM (Active Queue Management) controllers. The purpose of these controllers is to manage the network congestion under varying loads, link delays and bandwidth. In this paper, a new AQM controller is proposed which is trained by using the SVM (Support Vector Machine) with the RBF (Radial Basis Function) kernal. The proposed controller is called the support vector based AQM (SAM) controller. The performance of the proposed controller has been compared with three conventional AQM controllers, namely the Random Early Detection, Blue and Proportional Plus Integral Controller. The preliminary simulation studies show that the performance of the proposed controller is comparable to the conventional controllers. However, the proposed controller is more efficient in controlling the queue size than the conventional controllers. version:1
arxiv-1604-00552 | pH Prediction by Artificial Neural Networks for the Drinking Water of the Distribution System of Hyderabad City | http://arxiv.org/abs/1604.00552 | id:1604.00552 author:Niaz Ahmed Memon, Mukhtiar Ali Unar, Abdul Khalique Ansari category:cs.NE  published:2016-04-02 summary:In this research, feedforward ANN (Artificial Neural Network) model is developed and validated for predicting the pH at 10 different locations of the distribution system of drinking water of Hyderabad city. The developed model is MLP (Multilayer Perceptron) with back propagation algorithm.The data for the training and testing of the model are collected through an experimental analysis on weekly basis in a routine examination for maintaining the quality of drinking water in the city. 17 parameters are taken into consideration including pH. These all parameters are taken as input variables for the model and then pH is predicted for 03 phases;raw water of river Indus,treated water in the treatment plants and then treated water in the distribution system of drinking water. The training and testing results of this model reveal that MLP neural networks are exceedingly extrapolative for predicting the pH of river water, untreated and treated water at all locations of the distribution system of drinking water of Hyderabad city. The optimum input and output weights are generated with minimum MSE (Mean Square Error) < 5%.Experimental, predicted and tested values of pH are plotted and the effectiveness of the model is determined by calculating the coefficient of correlation (R2=0.999) of trained and tested results. version:1
arxiv-1604-00546 | Image Quality Assessment for Performance Evaluation of Focus Measure Operators | http://arxiv.org/abs/1604.00546 | id:1604.00546 author:Farida Memon, Mukhtiar Ali Unar, Sheeraz Memon category:cs.CV  published:2016-04-02 summary:This paper presents the performance evaluation of eight focus measure operators namely Image CURV (Curvature), GRAE (Gradient Energy), HISE (Histogram Entropy), LAPM (Modified Laplacian), LAPV (Variance of Laplacian), LAPD (Diagonal Laplacian), LAP3 (Laplacian in 3D Window) and WAVS (Sum of Wavelet Coefficients). Statistical matrics such as MSE (Mean Squared Error), PNSR (Peak Signal to Noise Ratio), SC (Structural Content), NCC (Normalized Cross Correlation), MD (Maximum Difference) and NAE (Normalized Absolute Error) are used to evaluate stated focus measures in this research. . FR (Full Reference) method of the image quality assessment is utilized in this paper. Results indicate that LAPD method is comparatively better than other seven focus operators at typical imaging conditions. version:1
arxiv-1604-00533 | Voronoi Region-Based Adaptive Unsupervised Color Image Segmentation | http://arxiv.org/abs/1604.00533 | id:1604.00533 author:R. Hettiarachchi, J. F. Peters category:cs.CV  published:2016-04-02 summary:Color image segmentation is a crucial step in many computer vision and pattern recognition applications. This article introduces an adaptive and unsupervised clustering approach based on Voronoi regions, which can be applied to solve the color image segmentation problem. The proposed method performs region splitting and merging within Voronoi regions of the Dirichlet Tessellated image (also called a Voronoi diagram) , which improves the efficiency and the accuracy of the number of clusters and cluster centroids estimation process. Furthermore, the proposed method uses cluster centroid proximity to merge proximal clusters in order to find the final number of clusters and cluster centroids. In contrast to the existing adaptive unsupervised cluster-based image segmentation algorithms, the proposed method uses K-means clustering algorithm in place of the Fuzzy C-means algorithm to find the final segmented image. The proposed method was evaluated on three different unsupervised image segmentation evaluation benchmarks and its results were compared with two other adaptive unsupervised cluster-based image segmentation algorithms. The experimental results reported in this article confirm that the proposed method outperforms the existing algorithms in terms of the quality of image segmentation results. Also, the proposed method results in the lowest average execution time per image compared to the existing methods reported in this article. version:1
arxiv-1512-00103 | Multilingual Language Processing From Bytes | http://arxiv.org/abs/1512.00103 | id:1512.00103 author:Dan Gillick, Cliff Brunk, Oriol Vinyals, Amarnag Subramanya category:cs.CL  published:2015-12-01 summary:We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate directly on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of- the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning "from scratch" in that they do not rely on any elements of the standard pipeline in Natural Language Processing (including tokenization), and thus can run in standalone fashion on raw text. version:2
arxiv-1604-00503 | Discriminative Phrase Embedding for Paraphrase Identification | http://arxiv.org/abs/1604.00503 | id:1604.00503 author:Wenpeng Yin, Hinrich Schütze category:cs.CL  published:2016-04-02 summary:This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification. version:1
arxiv-1604-00502 | Online Updating of Word Representations for Part-of-Speech Tagging | http://arxiv.org/abs/1604.00502 | id:1604.00502 author:Wenpeng Yin, Tobias Schnabel, Hinrich Schütze category:cs.CL  published:2016-04-02 summary:We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA. version:1
arxiv-1604-00494 | A Fully Convolutional Neural Network for Cardiac Segmentation in Short-Axis MRI | http://arxiv.org/abs/1604.00494 | id:1604.00494 author:Phi Vu Tran category:cs.CV  published:2016-04-02 summary:Automated cardiac segmentation from magnetic resonance imaging datasets is an essential step in the timely diagnosis and management of cardiac pathologies. We propose to tackle the problem of automated left and right ventricle segmentation through the application of a deep fully convolutional neural network architecture. Our model is efficiently trained end-to-end in a single learning stage from whole-image inputs and ground truths to make inference at every pixel. To our knowledge, this is the first application of a fully convolutional neural network architecture for pixel-wise labeling in cardiac magnetic resonance imaging. Numerical experiments demonstrate that our model is robust to outperform previous fully automated methods across multiple evaluation measures on a range of cardiac datasets. It is equally noteworthy that our model leverages commodity compute resources such as the graphics processing unit to enable fast, state-of-the-art cardiac segmentation at massive scales. The models and code will be released open-source in the near future. version:1
arxiv-1604-00475 | Robust video object tracking via Bayesian model averaging based feature fusion | http://arxiv.org/abs/1604.00475 | id:1604.00475 author:Yi Dai, Bin Liu category:cs.CV  published:2016-04-02 summary:In this article, we are concerned with tracking an object of interest in video stream. We propose an algorithm that is robust against occlusion, the presence of confusing colors, abrupt changes in the object feature space and changes in object size. We develop the algorithm within a Bayesian modeling framework. The state space model is used for capturing the temporal correlation in the sequence of frame images by modeling the underlying dynamics of the tracking system. The Bayesian model averaging (BMA) strategy is proposed for fusing multi-clue information in the observations. Any number of object features are allowed to be involved in the proposed framework. Every feature represents one source of information to be fused and is associated with an observation model. The state inference is performed by employing the particle filter methods. In comparison with related approaches, the BMA based tracker is shown to have robustness, expressivity, and comprehensibility. version:1
arxiv-1604-00470 | Overlay Text Extraction From TV News Broadcast | http://arxiv.org/abs/1604.00470 | id:1604.00470 author:Raghvendra Kannao, Prithwijit Guha category:cs.CV  published:2016-04-02 summary:The text data present in overlaid bands convey brief descriptions of news events in broadcast videos. The process of text extraction becomes challenging as overlay text is presented in widely varying formats and often with animation effects. We note that existing edge density based methods are well suited for our application on account of their simplicity and speed of operation. However, these methods are sensitive to thresholds and have high false positive rates. In this paper, we present a contrast enhancement based preprocessing stage for overlay text detection and a parameter free edge density based scheme for efficient text band detection. The second contribution of this paper is a novel approach for multiple text region tracking with a formal identification of all possible detection failure cases. The tracking stage enables us to establish the temporal presence of text bands and their linking over time. The third contribution is the adoption of Tesseract OCR for the specific task of overlay text recognition using web news articles. The proposed approach is tested and found superior on news videos acquired from three Indian English television news channels along with benchmark datasets. version:1
arxiv-1511-04891 | Sherlock: Scalable Fact Learning in Images | http://arxiv.org/abs/1511.04891 | id:1511.04891 author:Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed Elgammal category:cs.CV cs.CL cs.LG  published:2015-11-16 summary:We study scalable and uniform understanding of facts in images. Existing visual recognition systems are typically modeled differently for each fact type such as objects, actions, and interactions. We propose a setting where all these facts can be modeled simultaneously with a capacity to understand unbounded number of facts in a structured way. The training data comes as structured facts in images, including (1) objects (e.g., $<$boy$>$), (2) attributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and (4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic language view (e.g., $<$ boy, playing$>$) and a visual view (an image with this fact). We show that learning visual facts in a structured way enables not only a uniform but also generalizable visual understanding. We propose and investigate recent and strong approaches from the multiview learning literature and also introduce two learning representation models as potential baselines. We applied the investigated methods on several datasets that we augmented with structured facts and a large scale dataset of more than 202,000 facts and 814,000 images. Our experiments show the advantage of relating facts by the structure by the proposed models compared to the designed baselines on bidirectional fact retrieval. version:4
arxiv-1604-00462 | Centralized and Decentralized Global Outer-synchronization of Asymmetric Recurrent Time-varying Neural Network by Data-sampling | http://arxiv.org/abs/1604.00462 | id:1604.00462 author:Wenlian Lu, Ren Zheng, Tianping Chen category:cs.NE cs.SY math.CA  published:2016-04-02 summary:In this paper, we discuss the outer-synchronization of the asymmetrically connected recurrent time-varying neural networks. By both centralized and decentralized discretization data sampling principles, we derive several sufficient conditions based on diverse vector norms that guarantee that any two trajectories from different initial values of the identical neural network system converge together. The lower bounds of the common time intervals between data samples in centralized and decentralized principles are proved to be positive, which guarantees exclusion of Zeno behavior. A numerical example is provided to illustrate the efficiency of the theoretical results. version:1
arxiv-1604-00461 | Embedding Lexical Features via Low-Rank Tensors | http://arxiv.org/abs/1604.00461 | id:1604.00461 author:Mo Yu, Mark Dredze, Raman Arora, Matthew Gormley category:cs.CL cs.AI cs.LG  published:2016-04-02 summary:Modern NLP models rely heavily on engineered features, which often combine word and contextual information into complex lexical features. Such combination results in large numbers of features, which can lead to over-fitting. We present a new model that represents complex lexical features---comprised of parts for words, contextual information and labels---in a tensor that captures conjunction information among these parts. We apply low-rank tensor approximations to the corresponding parameter tensors to reduce the parameter space and improve prediction speed. Furthermore, we investigate two methods for handling features that include $n$-grams of mixed lengths. Our model achieves state-of-the-art results on tasks in relation extraction, PP-attachment, and preposition disambiguation. version:1
arxiv-1604-00457 | Stability of Analytic Neural Networks with Event-triggered Synaptic Feedbacks | http://arxiv.org/abs/1604.00457 | id:1604.00457 author:Ren Zheng, Xinlei Yi, Wenlian Lu, Tianping Chen category:cs.NE math.DS nlin.AO  published:2016-04-02 summary:In this paper, we investigate stability of a class of analytic neural networks with the synaptic feedback via event-triggered rules. This model is general and include Hopfield neural network as a special case. These event-trigger rules can efficiently reduces loads of computation and information transmission at synapses of the neurons. The synaptic feedback of each neuron keeps a constant value based on the outputs of the other neurons at its latest triggering time but changes at its next triggering time, which is determined by certain criterion. It is proved that every trajectory of the analytic neural network converges to certain equilibrium under this event-triggered rule for all initial values except a set of zero measure. The main technique of the proof is the Lojasiewicz inequality to prove the finiteness of trajectory length. The realization of this event-triggered rule is verified by the exclusion of Zeno behaviors. Numerical examples are provided to illustrate the efficiency of the theoretical results. version:1
arxiv-1604-00449 | 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction | http://arxiv.org/abs/1604.00449 | id:1604.00449 author:Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, Silvio Savarese category:cs.CV cs.AI  published:2016-04-02 summary:Inspired by the recent success of methods that employ shape priors to achieve robust 3D reconstructions, we propose a novel recurrent neural network architecture that we call the 3D Recurrent Reconstruction Neural Network (3D-R2N2). The network learns a mapping from images of objects to their underlying 3D shapes from a large collection of synthetic data. Our network takes in one or more images of an object instance from arbitrary viewpoints and outputs a reconstruction of the object in the form of a 3D occupancy grid. Unlike most of the previous works, our network does not require any image annotations or object class labels for training or testing. Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail (because of lack of texture and/or wide baseline). version:1
arxiv-1602-07776 | Recurrent Neural Network Grammars | http://arxiv.org/abs/1602.07776 | id:1602.07776 author:Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, Noah A. Smith category:cs.CL cs.NE  published:2016-02-25 summary:We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese. version:2
arxiv-1604-00433 | Cross Quality Distillation | http://arxiv.org/abs/1604.00433 | id:1604.00433 author:Jong-Chyi Su, Subhransu Maji category:cs.CV  published:2016-04-01 summary:We propose a technique for training recognition models when high-quality data is available at training time but not at testing time. Our approach, called Cross Quality Distillation (CQD), first trains a model on the high-quality data and encourages a second model trained on the low-quality data to generalize in the same way as the first. The technique is fairly general and only requires the ability to generate low-quality data from the high-quality data. We apply this to learn models for recognizing low-resolution images using labeled high-resolution images, non-localized objects using labeled localized objects, edge images using labeled color images, etc. Experiments on various fine-grained recognition datasets demonstrate that the technique leads to large improvements in recognition accuracy on the low-quality data. We also establish connections of CQD to other areas of machine learning such as domain adaptation, model compression, and learning using privileged information, and show that the technique is general and can be applied to other settings. Finally, we present further insights into why the technique works through visualizations and establishing its relationship to curriculum learning. version:1
arxiv-1604-00427 | Leaving Some Stones Unturned: Dynamic Feature Prioritization for Activity Detection in Streaming Video | http://arxiv.org/abs/1604.00427 | id:1604.00427 author:Yu-Chuan Su, Kristen Grauman category:cs.CV  published:2016-04-01 summary:Current approaches for activity recognition often ignore constraints on computational resources: 1) they rely on extensive feature computation to obtain rich descriptors on all frames, and 2) they assume batch-mode access to the entire test video at once. We propose a new active approach to activity recognition that prioritizes "what to compute when" in order to make timely predictions. The main idea is to learn a policy that dynamically schedules the sequence of features to compute on selected frames of a given test video. In contrast to traditional static feature selection, our approach continually re-prioritizes computation based on the accumulated history of observations and accounts for the transience of those observations in ongoing video. We develop variants to handle both the batch and streaming settings. On two challenging datasets, our method provides significantly better accuracy than alternative techniques for a wide range of computational budgets. version:1
arxiv-1604-00409 | Structure from Motion on a Sphere | http://arxiv.org/abs/1604.00409 | id:1604.00409 author:Jonathan Ventura category:cs.CV  published:2016-04-01 summary:We describe a special case of structure from motion where the camera rotates on a sphere. The camera's optical axis lies normal to the sphere's surface. In this case, the camera's pose is minimally represented by three rotation parameters. From analysis of the epipolar geometry we derive a novel and efficient solution for the essential matrix relating two images, requiring only three point correspondences in the minimal case. We apply this solver in a structure-from-motion pipeline that aggregates pairwise relations by rotation averaging followed by bundle adjustment with an inverse depth parameterization. Our methods enable scene modeling with an outward-facing camera and object scanning with an inward-facing camera. version:1
arxiv-1604-00400 | Revisiting Summarization Evaluation for Scientific Articles | http://arxiv.org/abs/1604.00400 | id:1604.00400 author:Arman Cohan, Nazli Goharian category:cs.CL  published:2016-04-01 summary:Evaluation of text summarization approaches have been mostly based on metrics that measure similarities of system generated summaries with a set of human written gold-standard summaries. The most widely used metric in summarization evaluation has been the ROUGE family. ROUGE solely relies on lexical overlaps between the terms and phrases in the sentences; therefore, in cases of terminology variations and paraphrasing, ROUGE is not as effective. Scientific article summarization is one such case that is different from general domain summarization (e.g. newswire data). We provide an extensive analysis of ROUGE's effectiveness as an evaluation metric for scientific summarization; we show that, contrary to the common belief, ROUGE is not much reliable in evaluating scientific summaries. We furthermore show how different variants of ROUGE result in very different correlations with the manual Pyramid scores. Finally, we propose an alternative metric for summarization evaluation which is based on the content relevance between a system generated summary and the corresponding human written summaries. We call our metric SERA (Summarization Evaluation by Relevance Analysis). Unlike ROUGE, SERA consistently achieves high correlations with manual scores which shows its effectiveness in evaluation of scientific article summarization. version:1
arxiv-1603-05959 | Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation | http://arxiv.org/abs/1603.05959 | id:1603.05959 author:Konstantinos Kamnitsas, Christian Ledig, Virginia F. J. Newcombe, Joanna P. Simpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben Glocker category:cs.CV cs.AI  published:2016-03-18 summary:We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumors, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available. version:2
arxiv-1604-00385 | Large-Scale Electron Microscopy Image Segmentation in Spark | http://arxiv.org/abs/1604.00385 | id:1604.00385 author:Stephen M. Plaza, Stuart E. Berg category:q-bio.QM cs.CV  published:2016-04-01 summary:The emerging field of connectomics aims to unlock the mysteries of the brain by understanding the connectivity between neurons. To map this connectivity, we acquire thousands of electron microscopy (EM) images with nanometer-scale resolution. After aligning these images, the resulting dataset has the potential to reveal the shapes of neurons and the synaptic connections between them. However, imaging the brain of even a tiny organism like the fruit fly yields terabytes of data. It can take years of manual effort to examine such image volumes and trace their neuronal connections. One solution is to apply image segmentation algorithms to help automate the tracing tasks. In this paper, we propose a novel strategy to apply such segmentation on very large datasets that exceed the capacity of a single machine. Our solution is robust to potential segmentation errors which could otherwise severely compromise the quality of the overall segmentation, for example those due to poor classifier generalizability or anomalies in the image dataset. We implement our algorithms in a Spark application which minimizes disk I/O, and apply them to a few large EM datasets, revealing both their effectiveness and scalability. We hope this work will encourage external contributions to EM segmentation by providing 1) a flexible plugin architecture that deploys easily on different cluster environments and 2) an in-memory representation of segmentation that could be conducive to new advances. version:1
arxiv-1604-00367 | Person Re-identification in Appearance Impaired Scenarios | http://arxiv.org/abs/1604.00367 | id:1604.00367 author:Mengran Gou, Xikang Zhang, Angels Rates-Borras, Sadjad Asghari-Esfeden, Mario Sznaier, Octavia Camps category:cs.CV  published:2016-04-01 summary:Person re-identification is critical in surveillance applications. Current approaches rely on appearance based features extracted from a single or multiple shots of the target and candidate matches. These approaches are at a disadvantage when trying to distinguish between candidates dressed in similar colors or when targets change their clothing. In this paper we propose a dynamics-based feature to overcome this limitation. The main idea is to capture soft biometrics from gait and motion patterns by gathering dense short trajectories (tracklets) which are Fisher vector encoded. To illustrate the merits of the proposed features we introduce three new "appearance-impaired" datasets. Our experiments on the original and the appearance impaired datasets demonstrate the benefits of incorporating dynamics-based information with appearance-based information to re-identification algorithms. version:1
arxiv-1508-01939 | Minimax Optimal Variable Clustering in G-models via Cord | http://arxiv.org/abs/1508.01939 | id:1508.01939 author:Florentina Bunea, Christophe Giraud, Xi Luo category:stat.ME math.ST stat.ML stat.TH  published:2015-08-08 summary:The goal of variable clustering is to partition a random vector ${\bf X} \in R^p$ in sub-groups of similar probabilistic behavior. Popular methods such as hierarchical clustering or $K$-means are algorithmic procedures applied to observations on ${\bf X}$, while no population level target is defined prior to estimation. We take a different view in this paper, where we propose and investigate model based variable clustering. We consider three models, of increasing level of complexity, termed generically $G$-models, with $G$ standing for the partition to be estimated. Motivated by the potential lack of identifiability of the $G$-latent models, which are currently used in problems involving variable clustering, we introduce two new classes of models, the $G$-exchangeable and the $G$-block covariance models. We show that both classes are identifiable, for any distribution of ${\bf X}$. Our focus is on clusters that are invariant with respect to unknown monotone transformations of the data, and that can be estimated in a computationally feasible manner. Both desiderata can be met if the clusters correspond to blocks in the copula correlation matrix of ${\bf X}$, assumed to have a Gaussian copula distribution. This motivates the introduction of a new similarity metric for cluster membership, CORD, and a homonymous method for cluster estimation. Central to our work is the derivation of the minimax value of the CORD cluster separation for exact partition recovery. We obtained the surprising result that this value is of order $\sqrt{{\log (p)}/{n}}$, irrespective of the number of clusters, or of the size of the smallest cluster. Our new procedure, CORD, available on CRAN, achieves this bound, is easy to implement and has computational complexity that is polynomial in $p$. version:2
arxiv-1604-00326 | How to Transfer? Zero-Shot Object Recognition via Hierarchical Transfer of Semantic Attributes | http://arxiv.org/abs/1604.00326 | id:1604.00326 author:Ziad Al-Halah, Rainer Stiefelhagen category:cs.CV  published:2016-04-01 summary:Attribute based knowledge transfer has proven very successful in visual object analysis and learning previously unseen classes. However, the common approach learns and transfers attributes without taking into consideration the embedded structure between the categories in the source set. Such information provides important cues on the intra-attribute variations. We propose to capture these variations in a hierarchical model that expands the knowledge source with additional abstraction levels of attributes. We also provide a novel transfer approach that can choose the appropriate attributes to be shared with an unseen class. We evaluate our approach on three public datasets: aPascal, Animals with Attributes and CUB-200-2011 Birds. The experiments demonstrate the effectiveness of our model with significant improvement over state-of-the-art. version:1
arxiv-1604-00317 | A Semisupervised Approach for Language Identification based on Ladder Networks | http://arxiv.org/abs/1604.00317 | id:1604.00317 author:Ehud Ben-Reuven, Jacob Goldberger category:cs.CL cs.LG cs.NE  published:2016-04-01 summary:In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset. version:1
arxiv-1604-00312 | Automated Alertness and Emotion Detection for Empathic Feedback During E-Learning | http://arxiv.org/abs/1604.00312 | id:1604.00312 author:S L Happy, A. Dasgupta, P. Patnaik, A. Routray category:cs.CV cs.CY cs.HC  published:2016-04-01 summary:In the context of education technology, empathic interaction with the user and feedback by the learning system using multiple inputs such as video, voice and text inputs is an important area of research. In this paper, a nonintrusive, standalone model for intelligent assessment of alertness and emotional state as well as generation of appropriate feedback has been proposed. Using the non-intrusive visual cues, the system classifies emotion and alertness state of the user, and provides appropriate feedback according to the detected cognitive state using facial expressions, ocular parameters, postures, and gestures. Assessment of alertness level using ocular parameters such as PERCLOS and saccadic parameters, emotional state from facial expression analysis, and detection of both relevant cognitive and emotional states from upper body gestures and postures has been proposed. Integration of such a system in e-learning environment is expected to enhance students performance through interaction, feedback, and positive mood induction. version:1
arxiv-1604-00279 | Using Recurrent Neural Networks to Optimize Dynamical Decoupling for Quantum Memory | http://arxiv.org/abs/1604.00279 | id:1604.00279 author:Moritz August, Xiaotong Ni category:quant-ph cs.LG cs.NE  published:2016-04-01 summary:We utilize machine learning models which are based on recurrent neural networks to optimize dynamical decoupling (DD) sequences. DD is a relatively simple technique for suppressing the errors in quantum memory for certain noise models. In numerical simulations, we show that with minimum use of prior knowledge and starting from random sequences, the models are able to improve over time and eventually output DD-sequences with performance better than that of the well known DD-families. Furthermore, our algorithm is easy to implement in experiments to find solutions tailored to the specific hardware, as it treats the figure of merit as a black box. version:1
arxiv-1604-00255 | Network structure, metadata and the prediction of missing nodes | http://arxiv.org/abs/1604.00255 | id:1604.00255 author:Darko Hric, Tiago P. Peixoto, Santo Fortunato category:physics.soc-ph cs.SI stat.ML  published:2016-04-01 summary:The empirical validation of community detection methods is often based on available annotations on the nodes that serve as putative indicators of the large-scale network structure. Most often, the suitability of the annotations as topological descriptors itself is not assessed, and without this it is not possible to ultimately distinguish between actual shortcomings of the community detection algorithms on one hand, and the incompleteness, inaccuracy or structured nature of the data annotations themselves on the other. In this work we present a principled method to access both aspects simultaneously. We construct a joint generative model for the data and metadata, and a non-parametric Bayesian framework to infer its parameters from annotated datasets. We assess the quality of the metadata not according to its direct alignment with the network communities, but rather in its capacity to predict the placement of edges in the network. We also show how this feature can be used to predict the connections to missing nodes when only the metadata is available. By investigating a wide range of datasets, we show that while there are seldom exact agreements between metadata tokens and the inferred data groups, the metadata is often informative of the network structure nevertheless, and can improve the prediction of missing nodes. This shows that the method uncovers meaningful patterns in both the data and metadata, without requiring or expecting a perfect agreement between the two. version:1
arxiv-1507-02293 | COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution | http://arxiv.org/abs/1507.02293 | id:1507.02293 author:Mehrdad Farajtabar, Yichen Wang, Manuel Gomez Rodriguez, Shuang Li, Hongyuan Zha, Le Song category:cs.SI cs.LG physics.soc-ph stat.ML  published:2015-07-08 summary:Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives. version:2
arxiv-1604-00239 | Tensor Representations via Kernel Linearization for Action Recognition from 3D Skeletons | http://arxiv.org/abs/1604.00239 | id:1604.00239 author:Piotr Koniusz, Anoop Cherian, Fatih Porikli category:cs.CV  published:2016-04-01 summary:In this paper, we explore tensor representations that can compactly capture higher-order relationships between skeleton joints for 3D action recognition. We first define RBF kernels on 3D joint sequences, which are then linearized to form kernel descriptors. The higher-order outer-products of these kernel descriptors form our tensor representations. We present two different kernels for action recognition, namely (i) a sequence compatibility kernel that captures the spatio-temporal compatibility of joints in one sequence against those in the other, and (ii) a dynamics compatibility kernel that explicitly models the action dynamics of a sequence. Tensors formed from these kernels are then used to train an SVM. We present experiments on several benchmark datasets and demonstrate state of the art results, substantiating the effectiveness of our representations. version:1
arxiv-1603-05587 | Reliable Prediction Intervals for Local Linear Regression | http://arxiv.org/abs/1603.05587 | id:1603.05587 author:Mohammad Ghasemi Hamed, Masoud Ebadi Kivaj category:stat.ME cs.LG  published:2016-03-17 summary:This paper introduces two methods for estimating reliable prediction intervals for local linear least-squares regressions, named Bounded Oscillation Prediction Intervals (BOPI). It also proposes a new measure for comparing interval prediction models named Equivalent Gaussian Standard Deviation (EGSD). The experimental results compare BOPI to other methods using coverage probability, Mean Interval Size and the introduced EGSD measure. The results were generally in favor of the BOPI on considered benchmark regression datasets. It also, reports simulation studies validating the BOPI method's reliability. version:4
arxiv-1604-00169 | Gaussian process optimization through sampling from the maximum distribution | http://arxiv.org/abs/1604.00169 | id:1604.00169 author:Hildo Bijl, Thomas B. Schön, Jan-Willem van Wingerden, Michel Verhaegen category:stat.ML cs.SY  published:2016-04-01 summary:This paper first presents a novel algorithm approximating the distribution of the maximum (both its position and its value) of a Gaussian process. This algorithm uses particles in a similar way as Sequential Monte Carlo samplers. It is subsequently applied to the problem of Gaussian Process Optimization (GPO). The resulting GPO algorithm does not use an acquisition function, which makes it different from other GPO algorithms. Through various example problems, including a wind turbine load mitigation example, we find that the resulting algorithm on average outperforms existing GPO algorithms. In addition, because no acquisition function has to be optimized, the algorithm can easily and efficiently be applied to problems with high-dimensional input spaces. version:1
arxiv-1604-00151 | Gradient-based learning algorithms with constant-error estimators: stability and convergence | http://arxiv.org/abs/1604.00151 | id:1604.00151 author:Arunselvan Ramaswamy, Shalabh Bhatnagar category:cs.SY stat.ML 93E15  93E35  published:2016-04-01 summary:Implementations of stochastic gradient search algorithms such as back propagation typically rely on finite difference ($FD$) approximation methods. These methods are used to approximate the objective function gradient in steepest descent algorithms as well as the gradient and Hessian inverse in Newton based schemes. The convergence analyses of such schemes critically require that perturbation parameters in the estimators of the gradient/Hessian approach zero. However, in practice, the perturbation parameter is often held fixed to a `small' constant resulting in constant-error estimates. We present in this paper a theoretical framework based on set-valued dynamical systems to analyze the aforementioned. Easily verifiable conditions are presented for stability and convergence when using such $FD$ estimators for the gradient/Hessian. In addition, our framework dispenses with a critical restriction on the step-sizes (learning rate) when using FD estimators. version:1
arxiv-1604-00147 | Learning a Pose Lexicon for Semantic Action Recognition | http://arxiv.org/abs/1604.00147 | id:1604.00147 author:Lijuan Zhou, Wanqing Li, Philip Ogunbona category:cs.CV  published:2016-04-01 summary:This paper presents a novel method for learning a pose lexicon comprising semantic poses defined by textual instructions and their associated visual poses defined by visual features. The proposed method simultaneously takes two input streams, semantic poses and visual pose candidates, and statistically learns a mapping between them to construct the lexicon. With the learned lexicon, action recognition can be cast as the problem of finding the maximum translation probability of a sequence of semantic poses given a stream of visual pose candidates. Experiments evaluating pre-trained and zero-shot action recognition conducted on MSRC-12 gesture and WorkoutSu-10 exercise datasets were used to verify the efficacy of the proposed method. version:1
arxiv-1604-00136 | It's Moving! A Probabilistic Model for Causal Motion Segmentation in Moving Camera Videos | http://arxiv.org/abs/1604.00136 | id:1604.00136 author:Pia Bideau, Erik Learned-Miller category:cs.CV  published:2016-04-01 summary:The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level. version:1
arxiv-1604-00133 | Good Practice in CNN Feature Transfer | http://arxiv.org/abs/1604.00133 | id:1604.00133 author:Liang Zheng, Yali Zhao, Shengjin Wang, Jingdong Wang, Qi Tian category:cs.CV  published:2016-04-01 summary:The objective of this paper is the effective transfer of the Convolutional Neural Network (CNN) feature in image search and classification. Systematically, we study three facts in CNN transfer. 1) We demonstrate the advantage of using images with a properly large size as input to CNN instead of the conventionally resized one. 2) We benchmark the performance of different CNN layers improved by average/max pooling on the feature maps. Our observation suggests that the Conv5 feature yields very competitive accuracy under such pooling step. 3) We find that the simple combination of pooled features extracted across various CNN layers is effective in collecting evidences from both low and high level descriptors. Following these good practices, we are capable of improving the state of the art on a number of benchmarks to a large margin. version:1
arxiv-1511-02301 | The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations | http://arxiv.org/abs/1511.02301 | id:1511.02301 author:Felix Hill, Antoine Bordes, Sumit Chopra, Jason Weston category:cs.CL  published:2015-11-07 summary:We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance. version:4
arxiv-1604-00126 | Nonparametric Spherical Topic Modeling with Word Embeddings | http://arxiv.org/abs/1604.00126 | id:1604.00126 author:Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, Sam Gershman category:cs.CL cs.IR cs.LG stat.ML  published:2016-04-01 summary:Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference. version:1
arxiv-1604-00125 | AttSum: Joint Learning of Focusing and Summarization with Neural Attention | http://arxiv.org/abs/1604.00125 | id:1604.00125 author:Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei category:cs.IR cs.CL  published:2016-04-01 summary:Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. It is also observed that the sentences recognized to focus on the query indeed meet the query need. version:1
arxiv-1603-03153 | Zipf's law emerges asymptotically during phase transitions in communicative systems | http://arxiv.org/abs/1603.03153 | id:1603.03153 author:Bohdan B. Khomtchouk, Claes Wahlestedt category:physics.soc-ph cs.CL  published:2016-03-10 summary:Zipf's law predicts a power-law relationship between word rank and frequency in language communication systems, and is widely reported in texts yet remains enigmatic as to its origins. Computer simulations have shown that language communication systems emerge at an abrupt phase transition in the fidelity of mappings between symbols and objects. Since the phase transition approximates the Heaviside or step function, we show that Zipfian scaling emerges asymptotically at high rank based on the Laplace transform. We thereby demonstrate that Zipf's law gradually emerges from the moment of phase transition in communicative systems. We show that this power-law scaling behavior explains the emergence of natural languages at phase transitions. We find that the emergence of Zipf's law during language communication suggests that the use of rare words in a lexicon is critical for the construction of an effective communicative system at the phase transition. version:2
arxiv-1604-00117 | Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding | http://arxiv.org/abs/1604.00117 | id:1604.00117 author:Aaron Jaech, Larry Heck, Mari Ostendorf category:cs.CL  published:2016-04-01 summary:The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques. version:1
arxiv-1601-04153 | Studying Very Low Resolution Recognition Using Deep Networks | http://arxiv.org/abs/1601.04153 | id:1601.04153 author:Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu, Thomas S. Huang category:cs.CV cs.AI cs.LG  published:2016-01-16 summary:Visual recognition research often assumes a sufficient resolution of the region of interest (ROI). That is usually violated in practice, inspiring us to explore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI in a VLRR problem can be smaller than $16 \times 16$ pixels, and is challenging to be recognized even by human experts. We attempt to solve the VLRR problem using deep learning methods. Taking advantage of techniques primarily in super resolution, domain adaptation and robust regression, we formulate a dedicated deep learning method and demonstrate how these techniques are incorporated step by step. Any extra complexity, when introduced, is fully justified by both analysis and simulation results. The resulting \textit{Robust Partially Coupled Networks} achieves feature enhancement and recognition simultaneously. It allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers. Finally, the effectiveness of the proposed models is evaluated on three different VLRR tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances. version:2
arxiv-1603-09439 | The Open World of Micro-Videos | http://arxiv.org/abs/1603.09439 | id:1603.09439 author:Phuc Xuan Nguyen, Gregory Rogez, Charless Fowlkes, Deva Ramanan category:cs.CV  published:2016-03-31 summary:Micro-videos are six-second videos popular on social media networks with several unique properties. Firstly, because of the authoring process, they contain significantly more diversity and narrative structure than existing collections of video "snippets". Secondly, because they are often captured by hand-held mobile cameras, they contain specialized viewpoints including third-person, egocentric, and self-facing views seldom seen in traditional produced video. Thirdly, due to to their continuous production and publication on social networks, aggregate micro-video content contains interesting open-world dynamics that reflects the temporal evolution of tag topics. These aspects make micro-videos an appealing well of visual data for developing large-scale models for video understanding. We analyze a novel dataset of micro-videos labeled with 58 thousand tags. To analyze this data, we introduce viewpoint-specific and temporally-evolving models for video understanding, defined over state-of-the-art motion and deep visual features. We conclude that our dataset opens up new research opportunities for large-scale video analysis, novel viewpoints, and open-world dynamics. version:2
arxiv-1604-00100 | A Compositional Approach to Language Modeling | http://arxiv.org/abs/1604.00100 | id:1604.00100 author:Kushal Arora, Anand Rangarajan category:cs.CL  published:2016-04-01 summary:Traditional language models treat language as a finite state automaton on a probability space over words. This is a very strong assumption when modeling something inherently complex such as language. In this paper, we challenge this by showing how the linear chain assumption inherent in previous work can be translated into a sequential composition tree. We then propose a new model that marginalizes over all possible composition trees thereby removing any underlying structural assumptions. As the partition function of this new model is intractable, we use a recently proposed sentence level evaluation metric Contrastive Entropy to evaluate our model. Given this new evaluation metric, we report more than 100% improvement across distortion levels over current state of the art recurrent neural network based language models. version:1
arxiv-1604-00092 | Variational reaction-diffusion systems for semantic segmentation | http://arxiv.org/abs/1604.00092 | id:1604.00092 author:Paul Vernaza category:cs.CV cs.LG  published:2016-04-01 summary:A novel global energy model for multi-class semantic image segmentation is proposed that admits very efficient exact inference and derivative calculations for learning. Inference in this model is equivalent to MAP inference in a particular kind of vector-valued Gaussian Markov random field, and ultimately reduces to solving a linear system of linear PDEs known as a reaction-diffusion system. Solving this system can be achieved in time scaling near-linearly in the number of image pixels by reducing it to sequential FFTs, after a linear change of basis. The efficiency and differentiability of the model make it especially well-suited for integration with convolutional neural networks, even allowing it to be used in interior, feature-generating layers and stacked multiple times. Experimental results are shown demonstrating that the model can be employed profitably in conjunction with different convolutional net architectures, and that doing so compares favorably to joint training of a fully-connected CRF with a convolutional net. version:1
arxiv-1604-00077 | Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection | http://arxiv.org/abs/1604.00077 | id:1604.00077 author:Sheng-syun Shen, Hung-yi Lee category:cs.CL  published:2016-03-31 summary:Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence classification tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper. version:1
arxiv-1602-07857 | Modeling cumulative biological phenomena with Suppes-Bayes causal networks | http://arxiv.org/abs/1602.07857 | id:1602.07857 author:Daniele Ramazzotti, Alex Graudenzi, Marco Antoniotti category:cs.AI cs.LG  published:2016-02-25 summary:Several statistical techniques have been recently developed for the inference of cancer progression models from the increasingly available NGS cross-sectional mutational profiles. A particular algorithm, CAPRI, was proven to be the most efficient with respect to sample size and level of noise in the data. The algorithm combines structural constraints based on Suppes' theory of probabilistic causation and maximum likelihood fit with regularization, and defines constrained Bayesian networks, named Suppes-Bayes Causal Networks (SBCNs), which account for the selective advantage relations among genomic events. In general, SBCNs are effective in modeling any phenomenon driven by cumulative dynamics, as long as the modeled events are persistent. Here we discuss on the effectiveness of the SBCN theoretical framework and we investigate the influence of: (i) the priors based on Suppes' theory and (ii) different maximum likelihood regularization parameters on the inference performance estimated on large synthetically generated datasets. version:2
arxiv-1604-00066 | To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction | http://arxiv.org/abs/1604.00066 | id:1604.00066 author:Wenbin Li, Seyedmajid Azimi, Aleš Leonardis, Mario Fritz category:cs.CV cs.AI cs.RO  published:2016-03-31 summary:Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel object and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage. In this paper, we contrast a more traditional approach of taking a model-based route with explicit 3D representations and physical simulation by an end-to-end approach that directly predicts stability and related quantities from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way bypassing the need for an explicit simulation. We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. The evaluation is carried out on synthetic data and compared to human judgments on the same stimuli. version:1
arxiv-1204-0563 | Kernel Methods for the Approximation of Some Key Quantities of Nonlinear Systems | http://arxiv.org/abs/1204.0563 | id:1204.0563 author:Jake Bouvrie, Boumediene Hamzi category:math.OC math.DS stat.ML  published:2012-04-03 summary:We introduce a data-based approach to estimating key quantities which arise in the study of nonlinear control systems and random nonlinear dynamical systems. Our approach hinges on the observation that much of the existing linear theory may be readily extended to nonlinear systems - with a reasonable expectation of success - once the nonlinear system has been mapped into a high or infinite dimensional feature space. In particular, we develop computable, non-parametric estimators approximating controllability and observability energy functions for nonlinear systems, and study the ellipsoids they induce. In all cases the relevant quantities are estimated from simulated or observed data. It is then shown that the controllability energy estimator provides a key means for approximating the invariant measure of an ergodic, stochastically forced nonlinear system. version:2
arxiv-1108-2903 | Kernel Methods for the Approximation of Nonlinear Systems | http://arxiv.org/abs/1108.2903 | id:1108.2903 author:Jake Bouvrie, Boumediene Hamzi category:math.OC cs.SY math.DS stat.ML  published:2011-08-14 summary:We introduce a data-driven order reduction method for nonlinear control systems, drawing on recent progress in machine learning and statistical dimensionality reduction. The method rests on the assumption that the nonlinear system behaves linearly when lifted into a high (or infinite) dimensional feature space where balanced truncation may be carried out implicitly. This leads to a nonlinear reduction map which can be combined with a representation of the system belonging to a reproducing kernel Hilbert space to give a closed, reduced order dynamical system which captures the essential input-output characteristics of the original model. Empirical simulations illustrating the approach are also provided. version:3
arxiv-1604-00036 | Modeling Visual Compatibility through Hierarchical Mid-level Elements | http://arxiv.org/abs/1604.00036 | id:1604.00036 author:Jose Oramas, Tinne Tuytelaars category:cs.CV  published:2016-03-31 summary:In this paper we present a hierarchical method to discover mid-level elements with the objective of modeling visual compatibility between objects. At the base-level, our method identifies patterns of CNN activations with the aim of modeling different variations/styles in which objects of the classes of interest may occur. At the top-level, the proposed method discovers patterns of co-occurring activations of base-level elements that define visual compatibility between pairs of object classes. Experiments on the massive Amazon dataset show the strength of our method at describing object classes and the characteristics that drive the compatibility between them. version:1
arxiv-1603-09739 | Hierarchical Quickest Change Detection via Surrogates | http://arxiv.org/abs/1603.09739 | id:1603.09739 author:Prithwish Chakraborty, Sathappan Muthiah, Ravi Tandon, Naren Ramakrishnan category:cs.LG cs.IT math.IT stat.ML  published:2016-03-31 summary:Change detection (CD) in time series data is a critical problem as it reveal changes in the underlying generative processes driving the time series. Despite having received significant attention, one important unexplored aspect is how to efficiently utilize additional correlated information to improve the detection and the understanding of changepoints. We propose hierarchical quickest change detection (HQCD), a framework that formalizes the process of incorporating additional correlated sources for early changepoint detection. The core ideas behind HQCD are rooted in the theory of quickest detection and HQCD can be regarded as its novel generalization to a hierarchical setting. The sources are classified into targets and surrogates, and HQCD leverages this structure to systematically assimilate observed data to update changepoint statistics across layers. The decision on actual changepoints are provided by minimizing the delay while still maintaining reliability bounds. In addition, HQCD also uncovers interesting relations between changes at targets from changes across surrogates. We validate HQCD for reliability and performance against several state-of-the-art methods for both synthetic dataset (known changepoints) and several real-life examples (unknown changepoints). Our experiments indicate that we gain significant robustness without loss of detection delay through HQCD. Our real-life experiments also showcase the usefulness of the hierarchical setting by connecting the surrogate sources (such as Twitter chatter) to target sources (such as Employment related protests that ultimately lead to major uprisings). version:1
arxiv-1603-09738 | Pessimistic Uplift Modeling | http://arxiv.org/abs/1603.09738 | id:1603.09738 author:Atef Shaar, Talel Abdessalem, Olivier Segard category:cs.LG  published:2016-03-31 summary:Uplift modeling is a machine learning technique that aims to model treatment effects heterogeneity. It has been used in business and health sectors to predict the effect of a specific action on a given individual. Despite its advantages, uplift models show high sensitivity to noise and disturbance, which leads to unreliable results. In this paper we show different approaches to address the problem of uplift modeling, we demonstrate how disturbance in data can affect uplift measurement. We propose a new approach, we call it Pessimistic Uplift Modeling, that minimizes disturbance effects. We compared our approach with the existing uplift methods, on simulated and real data-sets. The experiments show that our approach outperforms the existing approaches, especially in the case of high noise data environment. version:1
arxiv-1602-03206 | Design of false color palettes for grayscale reproduction | http://arxiv.org/abs/1602.03206 | id:1602.03206 author:Filip A. Sala category:cs.GR cs.CV  published:2016-02-06 summary:Design of false color palette is quite easy but some effort has to be done to achieve good dynamic range, contrast and overall appearance of the palette. Such palettes, for instance, are commonly used in scientific papers for presenting the data. However, to lower the cost of the paper most scientists decide to let the data to be printed in grayscale. The same applies to e-book readers based on e-ink where most of them are still grayscale. For majority of false color palettes reproducing them in grayscale results in ambiguous mapping of the colors and may be misleading for the reader. In this article design of false color palettes suitable for grayscale reproduction is described. Due to the monotonic change of luminance of these palettes grayscale representation is very similar to the data directly presented with a grayscale palette. Some suggestions and examples how to design such palettes are provided. version:2
arxiv-1603-09727 | Neural Language Correction with Character-Based Attention | http://arxiv.org/abs/1603.09727 | id:1603.09727 author:Ziang Xie, Anand Avati, Naveen Arivazhagan, Dan Jurafsky, Andrew Y. Ng category:cs.CL cs.AI  published:2016-03-31 summary:Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art $F_{0.5}$-score on the CoNLL 2014 Shared Task. We further demonstrate that training the network on additional data with synthesized errors can improve performance. version:1
arxiv-1603-09725 | Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion | http://arxiv.org/abs/1603.09725 | id:1603.09725 author:Israel D. Gebru, Silèye Ba, Xiaofei Li, Radu Horaud category:cs.CV cs.SD  published:2016-03-31 summary:Speaker diarization consists of assigning speech signals to speakers engaged in dialog. An audio-visual spatiotemporal diarization model is proposed. The model is well suited for challenging scenarios that consist of several participants engaged in multi-party dialog while they move around and turn their heads towards the other participants rather than facing the cameras and the microphones. Multiple-person visual tracking is combined with multiple speech-source localization in order to tackle the person-to-speech association problem. The latter is solved within a novel audio-visual fusion method on the following grounds: binaural spectral features are first extracted from a microphone pair, then a supervised audio-visual alignment technique maps these features onto an image, and finally a semi-supervised clustering method assigns binaural spectral features to visible persons. The main advantage of this method over previous work is that it processes in a principled way speech signals uttered simultaneously by multiple persons. The diarization itself is cast into a latent-variable temporal graphical model that infers speaker identities and speech turns, based on the output of the audio-visual association process available at each time slice, and on the dynamics of the diarization variable itself. The proposed formulation yields an efficient exact inference procedure. A novel dataset, that contains audio-visual training data as well as a number of scenarios involving several participants engaged in formal and informal dialog, is introduced. The proposed method is thoroughly tested and benchmarked with respect to several state-of-the art diarization algorithms. version:1
arxiv-1512-02063 | An Explicit Rate Bound for the Over-Relaxed ADMM | http://arxiv.org/abs/1512.02063 | id:1512.02063 author:Guilherme França, José Bento category:stat.ML math.OC  published:2015-12-07 summary:The framework of Integral Quadratic Constraints of Lessard et al. (2014) reduces the computation of upper bounds on the convergence rate of several optimization algorithms to semi-definite programming (SDP). Followup work by Nishihara et al. (2015) applies this technique to the entire family of over-relaxed Alternating Direction Method of Multipliers (ADMM). Unfortunately, they only provide an explicit error bound for sufficiently large values of some of the parameters of the problem, leaving the computation for the general case as a numerical optimization problem. In this paper we provide an exact analytical solution to this SDP and obtain a general and explicit upper bound on the convergence rate of the entire family of over-relaxed ADMM. Furthermore, we demonstrate that it is not possible to extract from this SDP a general bound better than ours. We end with a few numerical illustrations of our result and a comparison between the convergence rate we obtain for the ADMM with known convergence rates for the Gradient Descent. version:2
arxiv-1511-06909 | BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies | http://arxiv.org/abs/1511.06909 | id:1511.06909 author:Shihao Ji, S. V. N. Vishwanathan, Nadathur Satish, Michael J. Anderson, Pradeep Dubey category:cs.LG cs.CL cs.NE stat.ML  published:2015-11-21 summary:We propose BlackOut, an approximation algorithm to efficiently train massive recurrent neural network language models (RNNLMs) with million word vocabularies. BlackOut is motivated by using a discriminative loss, and we describe a new sampling strategy which significantly reduces computation while improving stability, sample efficiency, and rate of convergence. One way to understand BlackOut is to view it as an extension of the DropOut strategy to the output layer, wherein we use a discriminative training loss and a weighted sampling scheme. We also establish close connections between BlackOut, importance sampling, and noise contrastive estimation (NCE). Our experiments, on the recently released one billion word language modeling benchmark, demonstrate scalability and accuracy of BlackOut; we outperform the state-of-the art, and achieve the lowest perplexity scores on this dataset. Moreover, unlike other established methods which typically require GPUs or CPU clusters, we show that a carefully implemented version of BlackOut requires only 1-10 days on a single machine to train a RNNLM with a million word vocabulary and billions of parameters on one billion words. Although we describe BlackOut in the context of RNNLM training, it can be used to any networks with large softmax output layers. version:7
arxiv-1510-01444 | Stochastic subGradient Methods with Linear Convergence for Polyhedral Convex Optimization | http://arxiv.org/abs/1510.01444 | id:1510.01444 author:Tianbao Yang, Qihang Lin category:cs.LG math.OC  published:2015-10-06 summary:In this paper, we show that simple {Stochastic} subGradient Decent methods with multiple Restarting, named {\bf RSGD}, can achieve a \textit{linear convergence rate} for a class of non-smooth and non-strongly convex optimization problems where the epigraph of the objective function is a polyhedron, to which we refer as {\bf polyhedral convex optimization}. Its applications in machine learning include $\ell_1$ constrained or regularized piecewise linear loss minimization and submodular function minimization. To the best of our knowledge, this is the first result on the linear convergence rate of stochastic subgradient methods for non-smooth and non-strongly convex optimization problems. version:5
arxiv-1603-09638 | Building Better Detection with Privileged Information | http://arxiv.org/abs/1603.09638 | id:1603.09638 author:Z. Berkay Celik, Patrick McDaniel, Rauf Izmailov, Nicolas Papernot, Ananthram Swami category:cs.CR cs.LG stat.ML  published:2016-03-31 summary:Modern detection systems use sensor outputs available in the deployment environment to probabilistically identify attacks. These systems are trained on past or synthetic feature vectors to create a model of anomalous or normal behavior. Thereafter, run-time collected sensor outputs are compared to the model to identify attacks (or the lack of attack). While this approach to detection has been proven to be effective in many environments, it is limited to training on only features that can be reliably collected at test-time. Hence, they fail to leverage the often vast amount of ancillary information available from past forensic analysis and post-mortem data. In short, detection systems don't train (and thus don't learn from) features that are unavailable or too costly to collect at run-time. In this paper, we leverage recent advances in machine learning to integrate privileged information --features available at training time, but not at run-time-- into detection algorithms. We apply three different approaches to model training with privileged information; knowledge transfer, model influence, and distillation, and empirically validate their performance in a range of detection domains. Our evaluation shows that privileged information can increase detector precision and recall: we observe an average of 4.8% decrease in detection error for malware traffic detection over a system with no privileged information, 3.53% for fast-flux domain bot detection, 3.33% for malware classification, 11.2% for facial user authentication. We conclude by exploring the limitations and applications of different privileged information techniques in detection systems. version:1
arxiv-1603-09630 | Differentiable Pooling for Unsupervised Acoustic Model Adaptation | http://arxiv.org/abs/1603.09630 | id:1603.09630 author:Pawel Swietojanski, Steve Renals category:cs.CL cs.LG  published:2016-03-31 summary:We present a deep neural network (DNN) acoustic model including parametrised and differentiable pooling operators. Unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator. In particular, we experiment with two types of pooling parametrisations: learned $L_p$-norm pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker-dependent. We perform investigations using three different large vocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard conversational telephone speech. We demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models, with word error rates reductions ranging from 5--20\% with respect to unadapted systems, which themselves are better than the baseline fully-connected DNN-based acoustic models. We also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other feature- and model-space adaptation methods, as well as providing an analysis of the characteristics of each of the proposed approaches. version:1
arxiv-1603-09620 | Online Optimization with Costly and Noisy Measurements using Random Fourier Expansions | http://arxiv.org/abs/1603.09620 | id:1603.09620 author:Laurens Bliek, Hans R. G. W. Verstraete, Michel Verhaegen, Sander Wahls category:cs.LG math.OC stat.ML  published:2016-03-31 summary:This paper analyzes DONE, an online optimization algorithm that iteratively minimizes an unknown function with costly and noisy measurements. The algorithm maintains a surrogate of the unknown function in the form of a random Fourier expansion (RFE). The surrogate is updated whenever a new measurement is available, and then used to determine the next measurement point. The algorithm is comparable to Bayesian optimization algorithms, but its computational complexity per iteration does not depend on the number of measurements. We derive several theoretical results that provide insight on how the hyperparameters of the algorithm should be chosen. The algorithm is compared to a Bayesian optimization algorithm for a benchmark problem and two optics applications, namely, optical coherence tomography and optical beam-forming network tuning. It is found that the DONE algorithm is significantly faster than Bayesian optimization in all three discussed problems, while keeping a similar or better performance. version:1
arxiv-1603-09599 | Total Variation Applications in Computer Vision | http://arxiv.org/abs/1603.09599 | id:1603.09599 author:Vania V. Estrela, Hermes Aguiar Magalhaes, Osamu Saotome category:cs.CV  published:2016-03-31 summary:The objectives of this chapter are: (i) to introduce a concise overview of regularization; (ii) to define and to explain the role of a particular type of regularization called total variation norm (TV-norm) in computer vision tasks; (iii) to set up a brief discussion on the mathematical background of TV methods; and (iv) to establish a relationship between models and a few existing methods to solve problems cast as TV-norm. For the most part, image-processing algorithms blur the edges of the estimated images, however TV regularization preserves the edges with no prior information on the observed and the original images. The regularization scalar parameter {\lambda} controls the amount of regularization allowed and it is an essential to obtain a high-quality regularized output. A wide-ranging review of several ways to put into practice TV regularization as well as its advantages and limitations are discussed. version:1
arxiv-1601-00248 | Contrastive Entropy: A new evaluation metric for unnormalized language models | http://arxiv.org/abs/1601.00248 | id:1601.00248 author:Kushal Arora, Anand Rangarajan category:cs.CL  published:2016-01-03 summary:Perplexity (per word) is the most widely used metric for evaluating language models. Despite this, there has been no dearth of criticism for this metric. Most of these criticisms center around lack of correlation with extrinsic metrics like word error rate (WER), dependence upon shared vocabulary for model comparison and unsuitability for unnormalized language model evaluation. In this paper, we address the last problem and propose a new discriminative entropy based intrinsic metric that works for both traditional word level models and unnormalized language models like sentence level models. We also propose a discriminatively trained sentence level interpretation of recurrent neural network based language model (RNN) as an example of unnormalized sentence level model. We demonstrate that for word level models, contrastive entropy shows a strong correlation with perplexity. We also observe that when trained at lower distortion levels, sentence level RNN considerably outperforms traditional RNNs on this new metric. version:2
arxiv-1603-09584 | Sparse Representation of Multivariate Extremes with Applications to Anomaly Ranking | http://arxiv.org/abs/1603.09584 | id:1603.09584 author:Nicolas Goix, Anne Sabourin, Stéphan Clémençon category:stat.ML  published:2016-03-31 summary:Extremes play a special role in Anomaly Detection. Beyond inference and simulation purposes, probabilistic tools borrowed from Extreme Value Theory (EVT), such as the angular measure, can also be used to design novel statistical learning methods for Anomaly Detection/ranking. This paper proposes a new algorithm based on multivariate EVT to learn how to rank observations in a high dimensional space with respect to their degree of 'abnormality'. The procedure relies on an original dimension-reduction technique in the extreme domain that possibly produces a sparse representation of multivariate extremes and allows to gain insight into the dependence structure thereof, escaping the curse of dimensionality. The representation output by the unsupervised methodology we propose here can be combined with any Anomaly Detection technique tailored to non-extreme data. As it performs linearly with the dimension and almost linearly in the data (in O(dn log n)), it fits to large scale problems. The approach in this paper is novel in that EVT has never been used in its multivariate version in the field of Anomaly Detection. Illustrative experimental results provide strong empirical evidence of the relevance of our approach. version:1
arxiv-1509-01520 | An On-line Variational Bayesian Model for Multi-Person Tracking from Cluttered Scenes | http://arxiv.org/abs/1509.01520 | id:1509.01520 author:Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero, Radu Horaud category:cs.CV stat.ML  published:2015-09-04 summary:Object tracking is an ubiquitous problem that appears in many applications such as remote sensing, audio processing, computer vision, human-machine interfaces, human-robot interaction, etc. Although thoroughly investigated in computer vision, tracking a time-varying number of persons remains a challenging open problem. In this paper, we propose an on-line variational Bayesian model for multi-person tracking from cluttered visual observations provided by person detectors. The contributions of this paper are the followings. First, we propose a variational Bayesian framework for tracking an unknown and varying number of persons. Second, our model results in a variational expectation-maximization (VEM) algorithm with closed-form expressions for the posterior distributions of the latent variables and for the estimation of the model parameters. Third, the proposed model exploits observations from multiple detectors, and it is therefore multimodal by nature. Finally, we propose to embed both object-birth and object-visibility processes in an effort to robustly handle person appearances and disappearances over time. Evaluated on classical multiple person tracking datasets, our method shows competitive results with respect to state-of-the-art multiple-object tracking models, such as the probability hypothesis density (PHD) filter among others. version:2
arxiv-1603-09558 | Sub-pixel accuracy edge fitting by means of B-spline | http://arxiv.org/abs/1603.09558 | id:1603.09558 author:R. L. B. Breder, Vania V. Estrela, J. T. de Assis category:cs.CV  published:2016-03-31 summary:Local perturbations around contours strongly disturb the final result of computer vision tasks. It is common to introduce a priori information in the estimation process. Improvement can be achieved via a deformable model such as the snake model. In recent works, the deformable contour is modeled by means of B-spline snakes which allows local control, concise representation, and the use of fewer parameters. The estimation of the sub-pixel edges using a global B-spline model relies on the contour global determination according to a maximum likelihood framework and using the observed data likelihood. This procedure guarantees that the noisiest data will be filtered out. The data likelihood is computed as a consequence of the observation model which includes both orientation and position information. Comparative experiments of this algorithm and the classical spline interpolation have shown that the proposed algorithm outperforms the classical approach for Gaussian and Salt & Pepper noise. version:1
arxiv-1603-07893 | Investigation Into The Effectiveness Of Long Short Term Memory Networks For Stock Price Prediction | http://arxiv.org/abs/1603.07893 | id:1603.07893 author:Hengjian Jia category:cs.NE cs.LG  published:2016-03-25 summary:The effectiveness of long short term memory networks trained by backpropagation through time for stock price prediction is explored in this paper. A range of different architecture LSTM networks are constructed trained and tested. version:2
arxiv-1603-09473 | Monomer: Non-Metric Mixtures-of-Embeddings for Learning Visual Compatibility Across Categories | http://arxiv.org/abs/1603.09473 | id:1603.09473 author:Ruining He, Charles Packer, Julian McAuley category:cs.IR cs.CV cs.LG  published:2016-03-31 summary:Identifying relationships between items is a key task of an online recommender system, in order to help users discover items that are functionally complementary or visually compatible. In domains like clothing recommendation, this task is particularly challenging since a successful system should be capable of handling a large corpus of items, a huge amount of relationships among them, as well as the high-dimensional and semantically complicated features involved. Furthermore, the human notion of "compatibility" that we need to capture goes beyond mere similarity: For two items to be compatible---whether jeans and a t-shirt, or a laptop and a charger---they should be similar in some ways, but systematically different in others. In this paper we develop a method, Monomer, to uncover complicated and heterogeneous of relationships between items. Recently, scalable methods have been developed that address this task by learning embeddings of the visual and textual characteristics of the products involved, but which ultimately depend on a nearest-neighbor assumption between the learned embeddings. Here we show that richer notions of compatibility can be learned, principally by relaxing the metricity assumption inherent in previous work, so as to uncover ways in which related items should be systematically similar, and systematically different. Quantitatively, we show that our system achieves state-of-the-art performance on large-scale compatibility prediction tasks, especially in cases where there is substantial heterogeneity between related items. version:1
arxiv-1603-08538 | Hybrid Ant Colony Optimization in solving Multi-Skill Resource-Constrained Project Scheduling Problem | http://arxiv.org/abs/1603.08538 | id:1603.08538 author:Paweł B. Myszkowski, Marek E. Skowroński, Łukasz P. Olech, Krzysztof Oślizło category:cs.NE  published:2016-03-28 summary:In this paper Hybrid Ant Colony Optimization (HAntCO) approach in solving Multi--Skill Resource Constrained Project Scheduling Problem (MS--RCPSP) has been presented. We have proposed hybrid approach that links classical heuristic priority rules for project scheduling with Ant Colony Optimization (ACO). Furthermore, a novel approach for updating pheromone value has been proposed, based on both the best and worst solutions stored by ants. The objective of this paper is to research the usability and robustness of ACO and its hybrids with priority rules in solving MS--RCPSP. Experiments have been performed using artificially created dataset instances, based on real--world ones. We published those instances that can be used as a benchmark. Presented results show that ACO--based hybrid method is an efficient approach. More directed search process by hybrids makes this approach more stable and provides mostly better results than classical ACO. version:2
arxiv-1603-09469 | A ParaBoost Stereoscopic Image Quality Assessment (PBSIQA) System | http://arxiv.org/abs/1603.09469 | id:1603.09469 author:Hyunsuk Ko, Rui Song, C. -C. Jay Kuo category:cs.CV cs.LG  published:2016-03-31 summary:The problem of stereoscopic image quality assessment, which finds applications in 3D visual content delivery such as 3DTV, is investigated in this work. Specifically, we propose a new ParaBoost (parallel-boosting) stereoscopic image quality assessment (PBSIQA) system. The system consists of two stages. In the first stage, various distortions are classified into a few types, and individual quality scorers targeting at a specific distortion type are developed. These scorers offer complementary performance in face of a database consisting of heterogeneous distortion types. In the second stage, scores from multiple quality scorers are fused to achieve the best overall performance, where the fuser is designed based on the parallel boosting idea borrowed from machine learning. Extensive experimental results are conducted to compare the performance of the proposed PBSIQA system with those of existing stereo image quality assessment (SIQA) metrics. The developed quality metric can serve as an objective function to optimize the performance of a 3D content delivery system. version:1
arxiv-1511-01707 | Getting started with particle Metropolis-Hastings for inference in nonlinear dynamical models | http://arxiv.org/abs/1511.01707 | id:1511.01707 author:Johan Dahlin, Thomas B. Schön category:stat.CO q-fin.ST stat.ML  published:2015-11-05 summary:We provide a gentle introduction to the particle Metropolis-Hastings (PMH) algorithm for parameter inference in nonlinear state space models (SSMs) together with a software implementation in the statistical programming language R. Throughout this tutorial, we develop an implementation of the PMH algorithm (and the integrated particle filter), which is distributed as the package pmhtutorial available from the CRAN repository. Moreover, we provide the reader with some intuition for how the algorithm operates and discuss some solutions to numerical problems that might occur in practice. To illustrate the use of PMH, we consider parameter inference in a linear Gaussian SSM with synthetic data and a nonlinear stochastic volatility model with real-world data. We conclude the tutorial by discussing important possible improvements to the algorithm and we also list suitable references for further study. version:4
arxiv-1603-09462 | Robust Uncalibrated Stereo Rectification with Constrained Geometric Distortions (USR-CGD) | http://arxiv.org/abs/1603.09462 | id:1603.09462 author:Hyunsuk Ko, Han Suk Shim, Ouk Choi, C. -C. Jay Kuo category:cs.CV  published:2016-03-31 summary:A novel algorithm for uncalibrated stereo image-pair rectification under the constraint of geometric distortion, called USR-CGD, is presented in this work. Although it is straightforward to define a rectifying transformation (or homography) given the epipolar geometry, many existing algorithms have unwanted geometric distortions as a side effect. To obtain rectified images with reduced geometric distortions while maintaining a small rectification error, we parameterize the homography by considering the influence of various kinds of geometric distortions. Next, we define several geometric measures and incorporate them into a new cost function for parameter optimization. Finally, we propose a constrained adaptive optimization scheme to allow a balanced performance between the rectification error and the geometric error. Extensive experimental results are provided to demonstrate the superb performance of the proposed USR-CGD method, which outperforms existing algorithms by a significant margin. version:1
arxiv-1603-09460 | System Combination for Short Utterance Speaker Recognition | http://arxiv.org/abs/1603.09460 | id:1603.09460 author:Lantian Li, Dong Wang, Thomas Fang Zheng category:cs.CL cs.NE  published:2016-03-31 summary:Noticeable performance degradation is often observed in text-independent speaker recognition with short test utterances. This paper presents a combination approach to improve short utterance speaker recognition (SUSR), where two phonetic-aware systems are combined together: one is the DNN-based i-vector system and the other is the subregion-based GMM-UBM system proposed by us recently. The former employs phone posteriors to construct an i-vector model in which the shared statistics offer stronger robustness against limited test data. The latter establishes a phone-dependent GMM-UBM system which represents speaker characteristics with more details. A scorelevel system combination approach is proposed to integrate the respective advantages of the two systems. Experimental results confirm that on the text-independent SUSR task, both the DNN-based i-vector system and the subregion-based GMM-UBM system outperform their respective baselines, and the score-level system combination delivers significant performance improvement. version:1
arxiv-1510-05937 | Binary Speaker Embedding | http://arxiv.org/abs/1510.05937 | id:1510.05937 author:Lantian Li, Dong Wang, Chao Xing, Kaimin Yu, Thomas Fang Zheng category:cs.SD cs.LG  published:2015-10-20 summary:The popular i-vector model represents speakers as low-dimensional continuous vectors (i-vectors), and hence it is a way of continuous speaker embedding. In this paper, we investigate binary speaker embedding, which transforms i-vectors to binary vectors (codes) by a hash function. We start from locality sensitive hashing (LSH), a simple binarization approach where binary codes are derived from a set of random hash functions. A potential problem of LSH is that the randomly sampled hash functions might be suboptimal. We therefore propose an improved Hamming distance learning approach, where the hash function is learned by a variable-sized block training that projects each dimension of the original i-vectors to variable-sized binary codes independently. Our experiments show that binary speaker embedding can deliver competitive or even better results on both speaker verification and identification tasks, while the memory usage and the computation cost are significantly reduced. version:2
arxiv-1510-05940 | Max-margin Metric Learning for Speaker Recognition | http://arxiv.org/abs/1510.05940 | id:1510.05940 author:Lantian Li, Dong Wang, Chao Xing, Thomas Fang Zheng category:cs.SD cs.LG  published:2015-10-20 summary:Probabilistic linear discriminant analysis (PLDA) is a popular normalization approach for the i-vector model, and has delivered state-of-the-art performance in speaker recognition. A potential problem of the PLDA model, however, is that it essentially assumes Gaussian distributions over speaker vectors, which is not always true in practice. Additionally, the objective function is not directly related to the goal of the task, e.g., discriminating true speakers and imposters. In this paper, we propose a max-margin metric learning approach to solve the problems. It learns a linear transform with a criterion that the margin between target and imposter trials are maximized. Experiments conducted on the SRE08 core test show that compared to PLDA, the new approach can obtain comparable or even better performance, though the scoring is simply a cosine computation. version:2
arxiv-1603-09457 | LSTM based Conversation Models | http://arxiv.org/abs/1603.09457 | id:1603.09457 author:Yi Luan, Yangfeng Ji, Mari Ostendorf category:cs.CL  published:2016-03-31 summary:In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles. version:1
arxiv-1603-09454 | Exemplar-AMMs: Recognizing Crowd Movements from Pedestrian Trajectories | http://arxiv.org/abs/1603.09454 | id:1603.09454 author:Wenxi Liu, Rynson W. H. Lau, Xiaogang Wang, Dinesh Manocha category:cs.CV  published:2016-03-31 summary:In this paper, we present a novel method to recognize the types of crowd movement from crowd trajectories using agent-based motion models (AMMs). Our idea is to apply a number of AMMs, referred to as exemplar-AMMs, to describe the crowd movement. Specifically, we propose an optimization framework that filters out the unknown noise in the crowd trajectories and measures their similarity to the exemplar-AMMs to produce a crowd motion feature. We then address our real-world crowd movement recognition problem as a multi-label classification problem. Our experiments show that the proposed feature outperforms the state-of-the-art methods in recognizing both simulated and real-world crowd movements from their trajectories. Finally, we have created a synthetic dataset, SynCrowd, which contains 2D crowd trajectories in various scenarios, generated by various crowd simulators. This dataset can serve as a training set or benchmark for crowd analysis work. version:1
arxiv-1601-06116 | A Mathematical Formalization of Hierarchical Temporal Memory's Spatial Pooler | http://arxiv.org/abs/1601.06116 | id:1601.06116 author:James Mnatzaganian, Ernest Fokoué, Dhireesha Kudithipudi category:stat.ML cs.LG q-bio.NC  published:2016-01-22 summary:Hierarchical temporal memory (HTM) is an emerging machine learning algorithm, with the potential to provide a means to perform predictions on spatiotemporal data. The algorithm, inspired by the neocortex, currently does not have a comprehensive mathematical framework. This work brings together all aspects of the spatial pooler (SP), a critical learning component in HTM, under a single unifying framework. The primary learning mechanism is explored, where a maximum likelihood estimator for determining the degree of permanence update is proposed. The boosting mechanisms are studied and found to be only relevant during the initial few iterations of the network. Observations are made relating HTM to well-known algorithms such as competitive learning and attribute bagging. Methods are provided for using the SP for classification as well as dimensionality reduction. Empirical evidence verifies that given the proper parameterizations, the SP may be used for feature learning. version:2
arxiv-1603-09441 | A Stratified Analysis of Bayesian Optimization Methods | http://arxiv.org/abs/1603.09441 | id:1603.09441 author:Ian Dewancker, Michael McCourt, Scott Clark, Patrick Hayes, Alexandra Johnson, George Ke category:cs.LG stat.ML  published:2016-03-31 summary:Empirical analysis serves as an important complement to theoretical analysis for studying practical Bayesian optimization. Often empirical insights expose strengths and weaknesses inaccessible to theoretical analysis. We define two metrics for comparing the performance of Bayesian optimization methods and propose a ranking mechanism for summarizing performance within various genres or strata of test functions. These test functions serve to mimic the complexity of hyperparameter optimization problems, the most prominent application of Bayesian optimization, but with a closed form which allows for rapid evaluation and more predictable behavior. This offers a flexible and efficient way to investigate functions with specific properties of interest, such as oscillatory behavior or an optimum on the domain boundary. version:1
arxiv-1603-09016 | Rich Image Captioning in the Wild | http://arxiv.org/abs/1603.09016 | id:1603.09016 author:Kenneth Tran, Xiaodong He, Lei Zhang, Jian Sun, Cornelia Carapcea, Chris Thrasher, Chris Buehler, Chris Sienkiewicz category:cs.CV  published:2016-03-30 summary:We present an image caption system that addresses new challenges of automatically describing images in the wild. The challenges include high quality caption quality with respect to human judgments, out-of-domain data handling, and low latency required in many applications. Built on top of a state-of-the-art framework, we developed a deep vision model that detects a broad range of visual concepts, an entity recognition model that identifies celebrities and landmarks, and a confidence model for the caption output. Experimental results show that our caption engine outperforms previous state-of-the-art systems significantly on both in-domain dataset (i.e. MS COCO) and out of-domain datasets. version:2
arxiv-1602-08771 | Investigating practical linear temporal difference learning | http://arxiv.org/abs/1602.08771 | id:1602.08771 author:Adam White, Martha White category:cs.LG cs.AI stat.ML  published:2016-02-28 summary:Off-policy reinforcement learning has many applications including: learning from demonstration, learning multiple goal seeking policies in parallel, and representing predictive knowledge. Recently there has been an proliferation of new policy-evaluation algorithms that fill a longstanding algorithmic void in reinforcement learning: combining robustness to off-policy sampling, function approximation, linear complexity, and temporal difference (TD) updates. This paper contains two main contributions. First, we derive two new hybrid TD policy-evaluation algorithms, which fill a gap in this collection of algorithms. Second, we perform an empirical comparison to elicit which of these new linear TD methods should be preferred in different situations, and make concrete suggestions about practical use. version:2
arxiv-1603-09423 | Accurate Text Localization in Natural Image with Cascaded Convolutional Text Network | http://arxiv.org/abs/1603.09423 | id:1603.09423 author:Tong He, Weilin Huang, Yu Qiao, Jian Yao category:cs.CV  published:2016-03-31 summary:We introduce a new top-down pipeline for scene text detection. We propose a novel Cascaded Convolutional Text Network (CCTN) that joints two customized convolutional networks for coarse-to-fine text localization. The CCTN fast detects text regions roughly from a low-resolution image, and then accurately localizes text lines from each enlarged region. We cast previous character based detection into direct text region estimation, avoiding multiple bottom- up post-processing steps. It exhibits surprising robustness and discriminative power by considering whole text region as detection object which provides strong semantic information. We customize convolutional network by develop- ing rectangle convolutions and multiple in-network fusions. This enables it to handle multi-shape and multi-scale text efficiently. Furthermore, the CCTN is computationally efficient by sharing convolutional computations, and high-level property allows it to be invariant to various languages and multiple orientations. It achieves 0.84 and 0.86 F-measures on the ICDAR 2011 and ICDAR 2013, delivering substantial improvements over state-of-the-art results [23, 1]. version:1
arxiv-1603-09420 | Minimal Gated Unit for Recurrent Neural Networks | http://arxiv.org/abs/1603.09420 | id:1603.09420 author:Guo-Bing Zhou, Jianxin Wu, Chen-Lin Zhang, Zhi-Hua Zhou category:cs.NE cs.LG  published:2016-03-31 summary:Recently recurrent neural networks (RNN) has been very successful in handling sequence data. However, understanding RNN and finding the best practices for RNN is a difficult task, partly because there are many competing and complex hidden units (such as LSTM and GRU). We propose a gated unit for RNN, named as Minimal Gated Unit (MGU), since it only contains one gate, which is a minimal design among all gated hidden units. The design of MGU benefits from evaluation results on LSTM and GRU in the literature. Experiments on various sequence data show that MGU has comparable accuracy with GRU, but has a simpler structure, fewer parameters, and faster training. Hence, MGU is suitable in RNN's applications. Its simple architecture also means that it is easier to evaluate and tune, and in principle it is easier to study MGU's properties theoretically and empirically. version:1
arxiv-1603-09405 | Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding | http://arxiv.org/abs/1603.09405 | id:1603.09405 author:Peng Li, Heng Huang category:cs.CL cs.AI cs.NE  published:2016-03-30 summary:Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets. version:1
arxiv-1511-08119 | Higher Order Conditional Random Fields in Deep Neural Networks | http://arxiv.org/abs/1511.08119 | id:1511.08119 author:Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, Philip Torr category:cs.CV  published:2015-11-25 summary:We address the problem of semantic segmentation using deep learning. Most segmentation systems include a Conditional Random Field (CRF) to produce a structured output that is consistent with the image's visual features. Recent deep learning approaches have incorporated CRFs into Convolutional Neural Networks (CNNs), with some even training the CRF end-to-end with the rest of the network. However, these approaches have not employed higher order potentials, which have previously been shown to significantly improve segmentation performance. In this paper, we demonstrate that two types of higher order potential, based on object detections and superpixels, can be included in a CRF embedded within a deep network. We design these higher order potentials to allow inference with the differentiable mean field algorithm. As a result, all the parameters of our richer CRF model can be learned end-to-end with our pixelwise CNN classifier. We achieve state-of-the-art segmentation performance on the PASCAL VOC benchmark with these trainable higher order potentials. version:3
arxiv-1512-02325 | SSD: Single Shot MultiBox Detector | http://arxiv.org/abs/1512.02325 | id:1512.02325 author:Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg category:cs.CV  published:2015-12-08 summary:We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at \url{https://github.com/weiliu89/caffe/tree/ssd} . version:2
arxiv-1603-09381 | Clinical Information Extraction via Convolutional Neural Network | http://arxiv.org/abs/1603.09381 | id:1603.09381 author:Peng Li, Heng Huang category:cs.LG cs.CL cs.NE  published:2016-03-30 summary:We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their part-of-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines. version:1
arxiv-1603-09364 | Partial Face Detection for Continuous Authentication | http://arxiv.org/abs/1603.09364 | id:1603.09364 author:Upal Mahbub, Vishal M. Patel, Deepak Chandra, Brandon Barbello, Rama Chellappa category:cs.CV  published:2016-03-30 summary:In this paper, a part-based technique for real time detection of users' faces on mobile devices is proposed. This method is specifically designed for detecting partially cropped and occluded faces captured using a smartphone's front-facing camera for continuous authentication. The key idea is to detect facial segments in the frame and cluster the results to obtain the region which is most likely to contain a face. Extensive experimentation on a mobile dataset of 50 users shows that our method performs better than many state-of-the-art face detection methods in terms of accuracy and processing speed. version:1
arxiv-1603-09254 | A latent-observed dissimilarity measure | http://arxiv.org/abs/1603.09254 | id:1603.09254 author:Yasushi Terazono category:stat.ML  published:2016-03-30 summary:Quantitatively assessing relationships between latent variables and observed variables is important for understanding and developing generative models and representation learning. In this paper, we propose latent-observed dissimilarity (LOD) to evaluate the dissimilarity between the probabilistic characteristics of latent and observed variables. We also define four essential types of generative models with different independence/conditional independence configurations. Experiments using tractable real-world data show that LOD can effectively capture the differences between models and reflect the capability for higher layer learning. They also show that the conditional independence of latent variables given observed variables contributes to improving the transmission of information and characteristics from lower layers to higher layers. version:1
arxiv-1603-09246 | Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles | http://arxiv.org/abs/1603.09246 | id:1603.09246 author:Mehdi Noroozi, Paolo Favaro category:cs.CV  published:2016-03-30 summary:In this paper we study the problem of image representation learning without human annotation. Following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a Siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN is a more compact version of AlexNet, but with the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their cor-rect spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. The performance in object detection of features extracted from the CFN is the highest (51.8%) among unsupervisedly trained features, and very close to that of supervisedly trained features (56.5%). In object classification the CFN features achieve also the best accuracy (38.1%) among unsupervisedly trained features on the ImageNet 2012 dataset. version:1
arxiv-1603-09240 | Binary Quadratic Programing for Online Tracking of Hundreds of People in Extremely Crowded Scenes | http://arxiv.org/abs/1603.09240 | id:1603.09240 author:Afshin Dehghan, Mubarak Shah category:cs.CV cs.RO  published:2016-03-30 summary:Multi-object tracking has been studied for decades. However, when it comes to tracking pedestrians in extremely crowded scenes, we are limited to only few works. This is an important problem which gives rise to several challenges. Pre-trained object detectors fail to localize targets in crowded sequences. This consequently limits the use of data-association based multi-target tracking methods which rely on the outcome of an object detector. Additionally, the small apparent target size makes it challenging to extract features to discriminate targets from their surroundings. Finally, the large number of targets greatly increases computational complexity which in turn makes it hard to extend existing multi-target tracking approaches to high-density crowd scenarios. In this paper, we propose a tracker that addresses the aforementioned problems and is capable of tracking hundreds of people efficiently. We formulate online crowd tracking as Binary Quadratic Programing. Our formulation employs target's individual information in the form of appearance and motion as well as contextual cues in the form of neighborhood motion, spatial proximity and grouping constraints, and solves detection and data association simultaneously. In order to solve the proposed quadratic optimization efficiently, where state-of art commercial quadratic programing solvers fail to find the answer in a reasonable amount of time, we propose to use the most recent version of the Modified Frank Wolfe algorithm, which takes advantage of SWAP-steps to speed up the optimization. We show that the proposed formulation can track hundreds of targets efficiently and improves state-of-art results by significant margins on eleven challenging high density crowd sequences. version:1
arxiv-1603-09233 | Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs | http://arxiv.org/abs/1603.09233 | id:1603.09233 author:Rahul Meshram, Aditya Gopalan, D. Manjunath category:cs.LG  published:2016-03-30 summary:We describe and study a model for an Automated Online Recommendation System (AORS) in which a user's preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multi-armed bandit problem are also presented. version:1
arxiv-1511-08886 | Real-Time Depth Refinement for Specular Objects | http://arxiv.org/abs/1511.08886 | id:1511.08886 author:Roy Or - El, Rom Hershkovitz, Aaron Wetzler, Guy Rosman, Alfred M. Bruckstein, Ron Kimmel category:cs.CV  published:2015-11-28 summary:The introduction of consumer RGB-D scanners set off a major boost in 3D computer vision research. Yet, the precision of existing depth scanners is not accurate enough to recover fine details of a scanned object. While modern shading based depth refinement methods have been proven to work well with Lambertian objects, they break down in the presence of specularities. We present a novel shape from shading framework that addresses this issue and enhances both diffuse and specular objects' depth profiles. We take advantage of the built-in monochromatic IR projector and IR images of the RGB-D scanners and present a lighting model that accounts for the specular regions in the input image. Using this model, we reconstruct the depth map in real-time. Both quantitative tests and visual evaluations prove that the proposed method produces state of the art depth reconstruction results. version:2
arxiv-1603-09200 | Unsupervised Understanding of Location and Illumination Changes in Egocentric Videos | http://arxiv.org/abs/1603.09200 | id:1603.09200 author:Alejandro Betancourt, Natalia Díaz-Rodríguez, Emilia Barakova, Lucio Marcenaro, Matthias Rauterberg, Carlo Regazzoni category:cs.CV  published:2016-03-30 summary:Wearable cameras stand out as one of the most promising devices for the coming years, and as a consequence, the demand of computer algorithms to automatically understand these videos has been increasing quickly. An automatic understanding of these videos is not an easy task, and its mobile nature implies important challenges to be faced, such as the changing light conditions and the unrestricted locations recorded. This paper proposes an unsupervised strategy based on global features and manifold learning to endow wearable cameras with contextual information regarding the light conditions and the location recorded. Results show that non-linear manifold methods can capture contextual patterns from global features without compromising large computational resources. As an application case, the proposed unsupervised strategy is used as a switching mechanism to improve the hand-detection problem in egocentric videos under a multi-model approach. version:1
arxiv-1603-09188 | Unsupervised Visual Sense Disambiguation for Verbs using Multimodal Embeddings | http://arxiv.org/abs/1603.09188 | id:1603.09188 author:Spandana Gella, Mirella Lapata, Frank Keller category:cs.CL cs.CV  published:2016-03-30 summary:We introduce a new task, visual sense disambiguation for verbs: given an image and a verb, assign the correct sense of the verb, i.e., the one that describes the action depicted in the image. Just as textual word sense disambiguation is useful for a wide range of NLP tasks, visual sense disambiguation can be useful for multimodal tasks such as image retrieval, image description, and text illustration. We introduce VerSe, a new dataset that augments existing multimodal datasets (COCO and TUHOI) with sense labels. We propose an unsupervised algorithm based on Lesk which performs visual sense disambiguation using textual, visual, or multimodal embeddings. We find that textual embeddings perform well when gold-standard textual annotations (object labels and image descriptions) are available, while multimodal embeddings perform well on unannotated images. We also verify our findings by using the textual and multimodal embeddings as features in a supervised setting and analyse the performance of visual sense disambiguation task. VerSe is made publicly available and can be downloaded at: https://github.com/spandanagella/verse. version:1
arxiv-1412-1716 | Nonparametric modal regression | http://arxiv.org/abs/1412.1716 | id:1412.1716 author:Yen-Chi Chen, Christopher R. Genovese, Ryan J. Tibshirani, Larry Wasserman category:stat.ME math.ST stat.ML stat.TH  published:2014-12-04 summary:Modal regression estimates the local modes of the distribution of $Y$ given $X=x$, instead of the mean, as in the usual regression sense, and can hence reveal important structure missed by usual regression methods. We study a simple nonparametric method for modal regression, based on a kernel density estimate (KDE) of the joint distribution of $Y$ and $X$. We derive asymptotic error bounds for this method, and propose techniques for constructing confidence sets and prediction sets. The latter is used to select the smoothing bandwidth of the underlying KDE. The idea behind modal regression is connected to many others, such as mixture regression and density ridge estimation, and we discuss these ties as well. version:3
arxiv-1603-09129 | Exploiting Facial Landmarks for Emotion Recognition in the Wild | http://arxiv.org/abs/1603.09129 | id:1603.09129 author:Matthew Day category:cs.CV I.2.10  published:2016-03-30 summary:In this paper, we describe an entry to the third Emotion Recognition in the Wild Challenge, EmotiW2015. We detail the associated experiments and show that, through more accurately locating the facial landmarks, and considering only the distances between them, we can achieve a surprising level of performance. The resulting system is not only more accurate than the challenge baseline, but also much simpler. version:1
arxiv-1603-09128 | Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders | http://arxiv.org/abs/1603.09128 | id:1603.09128 author:Simon Šuster, Ivan Titov, Gertjan van Noord category:cs.CL cs.LG stat.ML  published:2016-03-30 summary:We present an approach to learning multi-sense word embeddings relying both on monolingual and bilingual information. Our model consists of an encoder, which uses monolingual and bilingual context (i.e. a parallel sentence) to choose a sense for a given word, and a decoder which predicts context words based on the chosen sense. The two components are estimated jointly. We observe that the word representations induced from bilingual data outperform the monolingual counterparts across a range of evaluation tasks, even though crosslingual information is not available at test time. version:1
arxiv-1603-09123 | deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks | http://arxiv.org/abs/1603.09123 | id:1603.09123 author:Byunghan Lee, Junghwan Baek, Seunghyun Park, Sungroh Yoon category:cs.LG q-bio.GN  published:2016-03-30 summary:MicroRNAs (miRNAs) are short sequences of ribonucleic acids that control the expression of target messenger RNAs (mRNAs) by binding them. Robust prediction of miRNA-mRNA pairs is of utmost importance in deciphering gene regulations but has been challenging because of high false positive rates, despite a deluge of computational tools that normally require laborious manual feature extraction. This paper presents an end-to-end machine learning framework for miRNA target prediction. Leveraged by deep recurrent neural networks-based auto-encoding and sequence-sequence interaction learning, our approach not only delivers an unprecedented level of accuracy but also eliminates the need for manual feature extraction. The performance gap between the proposed method and existing alternatives is substantial (over 25% increase in F-measure), and deepTarget delivers a quantum leap in the long-standing challenge of robust miRNA target prediction. version:1
arxiv-1603-09114 | LIFT: Learned Invariant Feature Transform | http://arxiv.org/abs/1603.09114 | id:1603.09114 author:Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, Pascal Fua category:cs.CV  published:2016-03-30 summary:We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining. version:1
arxiv-1602-06518 | Active Task Selection for Multi-Task Learning | http://arxiv.org/abs/1602.06518 | id:1602.06518 author:Anastasia Pentina, Christoph H. Lampert category:stat.ML cs.LG  published:2016-02-21 summary:In this paper we consider the problem of multi-task learning, in which a learner is given a collection of prediction tasks that need to be solved. In contrast to previous work, we give up on the assumption that labeled training data is available for all tasks. Instead, we propose an active task selection framework, where based only on the unlabeled data, the learner can choose a, typically small, subset of tasks for which he gets some labeled examples. For the remaining tasks, which have no available annotation, solutions are found by transferring information from the selected tasks. We analyze two transfer strategies and develop generalization bounds for each of them. Based on this theoretical analysis we propose two algorithms for making the choice of labeled tasks in a principled way and show their effectiveness on synthetic and real data. version:2
arxiv-1603-09065 | Structured Feature Learning for Pose Estimation | http://arxiv.org/abs/1603.09065 | id:1603.09065 author:Xiao Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang category:cs.CV  published:2016-03-30 summary:In this paper, we propose a structured feature learning framework to reason the correlations among body joints at the feature level in human pose estimation. Different from existing approaches of modelling structures on score maps or predicted labels, feature maps preserve substantially richer descriptions of body joints. The relationships between feature maps of joints are captured with the introduced geometrical transform kernels, which can be easily implemented with a convolution layer. Features and their relationships are jointly learned in an end-to-end learning system. A bi-directional tree structured model is proposed, so that the feature channels at a body joint can well receive information from other joints. The proposed framework improves feature learning substantially. With very simple post processing, it reaches the best mean PCP on the LSP and FLIC datasets. Compared with the baseline of learning features at each joint separately with ConvNet, the mean PCP has been improved by 18% on FLIC. The code is released to the public. version:1
arxiv-1603-09056 | Image Denoising Using Very Deep Fully Convolutional Encoder-Decoder Networks with Symmetric Skip Connections | http://arxiv.org/abs/1603.09056 | id:1603.09056 author:Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang category:cs.CV  published:2016-03-30 summary:Image denoising is a long-standing problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this paper, we propose a very deep encoding-decoding framework for image denoising. Instead of using image priors, the proposed framework learns end-to-end fully convolutional mappings from noisy images to the clean ones. The network is composed of multiple layers of convolution and de-convolution operators. With the observation that deeper networks improve denoising performance, we propose to use significantly deeper networks than those employed previously for low-level image processing tasks such as denoising. We propose to symmetrically link convolutional and de-convolutional layers with skip-layer connections, with which the training converges much faster and attains a higher-quality local optimum. From the image processing point of view, those symmetric connections help preserve image details. version:1
arxiv-1603-09054 | Unsupervised Measure of Word Similarity: How to Outperform Co-occurrence and Vector Cosine in VSMs | http://arxiv.org/abs/1603.09054 | id:1603.09054 author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL  published:2016-03-30 summary:In this paper, we claim that vector cosine, which is generally considered among the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by an unsupervised measure that calculates the extent of the intersection among the most mutually dependent contexts of the target words. To prove it, we describe and evaluate APSyn, a variant of the Average Precision that, without any optimization, outperforms the vector cosine and the co-occurrence on the standard ESL test set, with an improvement ranging between +9.00% and +17.98%, depending on the number of chosen top contexts. version:1
arxiv-1603-09051 | Phoenix: A Self-Optimizing Chess Engine | http://arxiv.org/abs/1603.09051 | id:1603.09051 author:Rahul A R, G Srinivasaraghavan category:cs.AI cs.NE  published:2016-03-30 summary:Since the advent of computers, many tasks which required humans to spend a lot of time and energy have been trivialized by the computers' ability to perform repetitive tasks extremely quickly. However there are still many areas in which humans excel in comparison with the machines. One such area is chess. Even with great advances in the speed and computational power of modern machines, Grandmasters often beat the best chess programs in the world with relative ease. This may be due to the fact that a game of chess cannot be won by pure calculation. There is more to the goodness of a chess position than some numerical value which apparently can be seen only by the human brain. Here an effort has been made to improve current chess engines by letting themselves evolve over a period of time. Firstly, the problem of learning is reduced into an optimization problem by defining Position Evaluation in terms of Positional Value Tables (PVTs). Next, the PVTs are optimized using Multi-Niche Crowding which successfully identifies the optima in a multimodal function, thereby arriving at distinctly different solutions which are close to the global optimum. version:1
arxiv-1603-08575 | Attend, Infer, Repeat: Fast Scene Understanding with Generative Models | http://arxiv.org/abs/1603.08575 | id:1603.08575 author:S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, Koray Kavukcuoglu, Geoffrey E. Hinton category:cs.CV cs.LG  published:2016-03-28 summary:We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization. version:2
arxiv-1603-09050 | Robustness of Bayesian Pool-based Active Learning Against Prior Misspecification | http://arxiv.org/abs/1603.09050 | id:1603.09050 author:Nguyen Viet Cuong, Nan Ye, Wee Sun Lee category:cs.LG stat.ML  published:2016-03-30 summary:We study the robustness of active learning (AL) algorithms against prior misspecification: whether an algorithm achieves similar performance using a perturbed prior as compared to using the true prior. In both the average and worst cases of the maximum coverage setting, we prove that all $\alpha$-approximate algorithms are robust (i.e., near $\alpha$-approximate) if the utility is Lipschitz continuous in the prior. We further show that robustness may not be achieved if the utility is non-Lipschitz. This suggests we should use a Lipschitz utility for AL if robustness is required. For the minimum cost setting, we can also obtain a robustness result for approximate AL algorithms. Our results imply that many commonly used AL algorithms are robust against perturbed priors. We then propose the use of a mixture prior to alleviate the problem of prior misspecification. We analyze the robustness of the uniform mixture prior and show experimentally that it performs reasonably well in practice. version:1
arxiv-1603-09046 | Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for Locally Robust Captioning | http://arxiv.org/abs/1603.09046 | id:1603.09046 author:Andrew Shin, Masataka Yamaguchi, Katsunori Ohnishi, Tatsuya Harada category:cs.CV  published:2016-03-30 summary:The workflow of extracting features from images using convolutional neural networks (CNN) and generating captions with recurrent neural networks (RNN) has become a de-facto standard for image captioning task. However, since CNN features are originally designed for classification task, it is mostly concerned with the main conspicuous element of the image, and often fails to correctly convey information on local, secondary elements. We propose to incorporate coding with vector of locally aggregated descriptors (VLAD) on spatial pyramid for CNN features of sub-regions in order to generate image representations that better reflect the local information of the images. Our results show that our method of compact VLAD coding can match CNN features with as little as 3% of dimensionality and, when combined with spatial pyramid, it results in image captions that more accurately take local elements into account. version:1
arxiv-1603-09045 | Performance of a community detection algorithm based on semidefinite programming | http://arxiv.org/abs/1603.09045 | id:1603.09045 author:Adel Javanmard, Andrea Montanari, Federico Ricci-Tersenghi category:stat.ML cond-mat.stat-mech cs.SI physics.soc-ph  published:2016-03-30 summary:The problem of detecting communities in a graph is maybe one the most studied inference problems, given its simplicity and widespread diffusion among several disciplines. A very common benchmark for this problem is the stochastic block model or planted partition problem, where a phase transition takes place in the detection of the planted partition by changing the signal-to-noise ratio. Optimal algorithms for the detection exist which are based on spectral methods, but we show these are extremely sensible to slight modification in the generative model. Recently Javanmard, Montanari and Ricci-Tersenghi (arXiv:1511.08769) have used statistical physics arguments, and numerical simulations to show that finding communities in the stochastic block model via semidefinite programming is quasi optimal. Further, the resulting semidefinite relaxation can be solved efficiently, and is very robust with respect to changes in the generative model. In this paper we study in detail several practical aspects of this new algorithm based on semidefinite programming for the detection of the planted partition. The algorithm turns out to be very fast, allowing the solution of problems with $O(10^5)$ variables in few second on a laptop computer. version:1
arxiv-1603-09335 | Möbius invariants of shapes and images | http://arxiv.org/abs/1603.09335 | id:1603.09335 author:Stephen Marsland, Robert McLachlan category:cs.CV math.MG  published:2016-03-30 summary:Identifying when different images are of the same object despite changes caused by imaging technologies, or processes such as growth, has many applications in fields such as computer vision and biological image analysis. One approach to this problem is to identify the group of possible transformations of the object and to find invariants to the action of that group, meaning that the object has the same values of the invariants despite the action of the group. In this paper we study the invariants of planar shapes and images under the M\"obius group $\mathrm{PSL}(2,\mathbb{C})$, which arises in the conformal camera model of vision and may also correspond to neurological aspects of vision, such as grouping of lines and circles. We survey the computational requirements of an invariant, and the known M\"obius invariants, and then develop an algorithm by which shapes can be recognised that is M\"obius- and parametrization-invariant, numerically stable, and robust to noise. We demonstrate the efficacy of this new invariant approach on sets of curves, and then develop a M\"obius-invariant signature of grey-scale images. version:1
arxiv-1603-09037 | Vector Quantization for Machine Vision | http://arxiv.org/abs/1603.09037 | id:1603.09037 author:Vincenzo Liguori category:cs.CV  published:2016-03-30 summary:This paper shows how to reduce the computational cost for a variety of common machine vision tasks by operating directly in the compressed domain, particularly in the context of hardware acceleration. Pyramid Vector Quantization (PVQ) is the compression technique of choice and its properties are exploited to simplify Support Vector Machines (SVM), Convolutional Neural Networks(CNNs), Histogram of Oriented Gradients (HOG) features, interest points matching and other algorithms. version:1
arxiv-1603-09035 | Towards Geo-Distributed Machine Learning | http://arxiv.org/abs/1603.09035 | id:1603.09035 author:Ignacio Cano, Markus Weimer, Dhruv Mahajan, Carlo Curino, Giovanni Matteo Fumarola category:cs.LG cs.DC stat.ML  published:2016-03-30 summary:Latency to end-users and regulatory requirements push large companies to build data centers all around the world. The resulting data is "born" geographically distributed. On the other hand, many machine learning applications require a global view of such data in order to achieve the best results. These types of applications form a new class of learning problems, which we call Geo-Distributed Machine Learning (GDML). Such applications need to cope with: 1) scarce and expensive cross-data center bandwidth, and 2) growing privacy concerns that are pushing for stricter data sovereignty regulations. Current solutions to learning from geo-distributed data sources revolve around the idea of first centralizing the data in one data center, and then training locally. As machine learning algorithms are communication-intensive, the cost of centralizing the data is thought to be offset by the lower cost of intra-data center communication during training. In this work, we show that the current centralized practice can be far from optimal, and propose a system for doing geo-distributed training. Furthermore, we argue that the geo-distributed approach is structurally more amenable to dealing with regulatory constraints, as raw data never leaves the source data center. Our empirical evaluation on three real datasets confirms the general validity of our approach, and shows that GDML is not only possible but also advisable in many scenarios. version:1
arxiv-1506-06289 | Filtrated Algebraic Subspace Clustering | http://arxiv.org/abs/1506.06289 | id:1506.06289 author:Manolis C. Tsakiris, Rene Vidal category:cs.CV  published:2015-06-20 summary:Subspace clustering is the problem of clustering data that lie close to a union of linear subspaces. In the abstract form of the problem, where no noise or other corruptions are present, the data are assumed to lie in general position inside the algebraic variety of a union of subspaces, and the objective is to decompose the variety into its constituent subspaces. Prior algebraic-geometric approaches to this problem require the subspaces to be of equal dimension, or the number of subspaces to be known. Subspaces of arbitrary dimensions can still be recovered in closed form, in terms of all homogeneous polynomials of degree $m$ that vanish on their union, when an upper bound m on the number of the subspaces is given. In this paper, we propose an alternative, provably correct, algorithm for addressing a union of at most $m$ arbitrary-dimensional subspaces, based on the idea of descending filtrations of subspace arrangements. Our algorithm uses the gradient of a vanishing polynomial at a point in the variety to find a hyperplane containing the subspace S passing through that point. By intersecting the variety with this hyperplane, we obtain a subvariety that contains S, and recursively applying the procedure until no non-trivial vanishing polynomial exists, our algorithm eventually identifies S. By repeating this procedure for other points, our algorithm eventually identifies all the subspaces by returning a basis for their orthogonal complement. Finally, we develop a variant of the abstract algorithm, suitable for computations with noisy data. We show by experiments on synthetic and real data that the proposed algorithm outperforms state-of-the-art methods on several occasions, thus demonstrating the merit of the idea of filtrations. version:4
arxiv-1603-09029 | Maximize Pointwise Cost-sensitively Submodular Functions With Budget Constraint | http://arxiv.org/abs/1603.09029 | id:1603.09029 author:Nguyen Viet Cuong, Huan Xu category:cs.AI cs.DM math.OC stat.ML  published:2016-03-30 summary:We study the worst-case adaptive optimization problem with budget constraint. Unlike previous works, we consider the general setting where the cost is a set function on sets of decisions. For this setting, we investigate the near-optimality of greedy policies when the utility function satisfies a novel property called pointwise cost-sensitive submodularity. This property is an extension of cost-sensitive submodularity, which in turn is a generalization of submodularity to general cost functions. We prove that two simple greedy policies for the problem are not near-optimal but the best between them is near-optimal. With this result, we propose a combined policy that is near-optimal with respect to the optimal worst-case policy that uses half of the budget. We discuss applications of our theoretical results and also report experimental results comparing the greedy policies on the active learning problem. version:1
arxiv-1603-09027 | Palmprint Recognition Using Deep Scattering Convolutional Network | http://arxiv.org/abs/1603.09027 | id:1603.09027 author:Shervin Minaee, Yao Wang category:cs.CV  published:2016-03-30 summary:Palmprint recognition has drawn a lot of attention during the recent years. Many algorithms have been proposed for palmprint recognition in the past, majority of them being based on features extracted from the transform domain. Many of these transform domain features are not translation or rotation invariant, and therefore a great deal of preprocessing is needed to align the images. In this paper, a powerful image representation, called scattering network/transform, is used for palmprint recognition. Scattering network is a convolutional network where its architecture and filters are predefined wavelet transforms. The first layer of scattering network captures similar features to SIFT descriptors and the higher-layer features capture higher-frequency content of the signal which are lost in SIFT and other similar descriptors. After extraction of the scattering features, their dimensionality is reduced by applying principal component analysis (PCA) which reduces the computational complexity of the recognition task. Two different classifiers are used for recognition: multi-class SVM and minimum-distance classifier. The proposed scheme has been tested on a well-known palmprint database and achieved accuracy rate of 99.95% and 100% using minimum distance classifier and SVM respectively. version:1
arxiv-1511-06435 | Comparative Study of Deep Learning Software Frameworks | http://arxiv.org/abs/1511.06435 | id:1511.06435 author:Soheil Bahrampour, Naveen Ramakrishnan, Lukas Schott, Mohak Shah category:cs.LG  published:2015-11-19 summary:Deep learning methods have resulted in significant performance improvements in several application domains and as such several software frameworks have been developed to facilitate their implementation. This paper presents a comparative study of five deep learning frameworks, namely Caffe, Neon, TensorFlow, Theano, and Torch, on three aspects: extensibility, hardware utilization, and speed. The study is performed on several types of deep learning architectures and we evaluate the performance of the above frameworks when employed on a single machine for both (multi-threaded) CPU and GPU (Nvidia Titan X) settings. The speed performance metrics used here include the gradient computation time, which is important during the training phase of deep networks, and the forward time, which is important from the deployment perspective of trained networks. For convolutional networks, we also report how each of these frameworks support various convolutional algorithms and their corresponding performance. From our experiments, we observe that Theano and Torch are the most easily extensible frameworks. We observe that Torch is best suited for any deep architecture on CPU, followed by Theano. It also achieves the best performance on the GPU for large convolutional and fully connected networks, followed closely by Neon. Theano achieves the best performance on GPU for training and deployment of LSTM networks. Caffe is the easiest for evaluating the performance of standard deep architectures. Finally, TensorFlow is a very flexible framework, similar to Theano, but its performance is currently not competitive compared to the other studied frameworks. version:3
arxiv-1603-09002 | Dataflow Matrix Machines as a Generalization of Recurrent Neural Networks | http://arxiv.org/abs/1603.09002 | id:1603.09002 author:Michael Bukatin, Steve Matthews, Andrey Radul category:cs.NE  published:2016-03-29 summary:Dataflow matrix machines are a powerful generalization of recurrent neural networks. They work with multiple types of arbitrary linear streams, multiple types of powerful neurons, and allow to incorporate higher-order constructions. We expect them to be useful in machine learning and probabilistic programming, and in the synthesis of dynamic systems and of deterministic and probabilistic programs. version:1
arxiv-1603-09000 | Online Rules for Control of False Discovery Rate and False Discovery Exceedance | http://arxiv.org/abs/1603.09000 | id:1603.09000 author:Adel Javanmard, Andrea Montanari category:math.ST cs.LG stat.AP stat.ME stat.ML stat.TH  published:2016-03-29 summary:Multiple hypothesis testing is a core problem in statistical inference and arises in almost every scientific field. Given a set of null hypotheses $\mathcal{H}(n) = (H_1,\dotsc, H_n)$, Benjamini and Hochberg introduced the false discovery rate (FDR), which is the expected proportion of false positives among rejected null hypotheses, and proposed a testing procedure that controls FDR below a pre-assigned significance level. Nowadays FDR is the criterion of choice for large scale multiple hypothesis testing. In this paper we consider the problem of controlling FDR in an "online manner". Concretely, we consider an ordered --possibly infinite-- sequence of null hypotheses $\mathcal{H} = (H_1,H_2,H_3,\dots )$ where, at each step $i$, the statistician must decide whether to reject hypothesis $H_i$ having access only to the previous decisions. This model was introduced by Foster and Stine. We study a class of "generalized alpha-investing" procedures and prove that any rule in this class controls online FDR, provided $p$-values corresponding to true nulls are independent from the other $p$-values. (Earlier work only established mFDR control.) Next, we obtain conditions under which generalized alpha-investing controls FDR in the presence of general $p$-values dependencies. Finally, we develop a modified set of procedures that also allow to control the false discovery exceedance (the tail of the proportion of false discoveries). Numerical simulations and analytical results indicate that online procedures do not incur a large loss in statistical power with respect to offline approaches, such as Benjamini-Hochberg. version:1
arxiv-1511-05297 | On the interplay of network structure and gradient convergence in deep learning | http://arxiv.org/abs/1511.05297 | id:1511.05297 author:Vamsi K Ithapu, Sathya Ravi, Vikas Singh category:cs.LG stat.ML  published:2015-11-17 summary:The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-\`a-vis their interaction with parameter convergence rates. We present a framework to address these questions based on the backpropagation convergence for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon the results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experiments we conducted as well as independent empirical observations reported by other groups in recent papers. version:5
arxiv-1603-08988 | Towards Practical Bayesian Parameter and State Estimation | http://arxiv.org/abs/1603.08988 | id:1603.08988 author:Yusuf Bugra Erol, Yi Wu, Lei Li, Stuart Russell category:cs.AI cs.LG stat.ML  published:2016-03-29 summary:Joint state and parameter estimation is a core problem for dynamic Bayesian networks. Although modern probabilistic inference toolkits make it relatively easy to specify large and practically relevant probabilistic models, the silver bullet---an efficient and general online inference algorithm for such problems---remains elusive, forcing users to write special-purpose code for each application. We propose a novel blackbox algorithm -- a hybrid of particle filtering for state variables and assumed density filtering for parameter variables. It has following advantages: (a) it is efficient due to its online nature, and (b) it is applicable to both discrete and continuous parameter spaces . On a variety of toy and real models, our system is able to generate more accurate results within a fixed computation budget. This preliminary evidence indicates that the proposed approach is likely to be of practical use. version:1
arxiv-1603-08984 | SMASH: Data-driven Reconstruction of Physically Valid Collisions | http://arxiv.org/abs/1603.08984 | id:1603.08984 author:Aron Monszpart, Nils Thuerey, Niloy J. Mitra category:cs.GR cs.CV  published:2016-03-29 summary:Collision sequences are commonly used in games and entertainment to add drama and excitement. Authoring even two body collisions in real world can be difficult as one has to get timing and the object trajectories to be correctly synchronized. After trial-and-error iterations, when objects can actually be made to collide, then they are difficult to acquire in 3D. In contrast, synthetically generating plausible collisions is difficult as it requires adjusting different collision parameters (e.g., object mass ratio, coefficient of restitution, etc.) and appropriate initial parameters. We present SMASH to directly `read off' appropriate collision parameters simply based on input video recordings. Specifically, we describe how to use laws of rigid body collision to regularize the problem of lifting 2D annotated poses to 3D reconstruction of collision sequences. The reconstructed sequences can then be modified and combined to easily author novel and plausible collision sequences. We demonstrate the system on various complex collision sequences. version:1
arxiv-1603-08981 | Detecting weak changes in dynamic events over networks | http://arxiv.org/abs/1603.08981 | id:1603.08981 author:Shuang Li, Yao Xie, Mehrdad Farajtabar, Le Song category:cs.LG stat.ML  published:2016-03-29 summary:Large volume of event data are becoming increasingly available in a wide variety of applications, such as social network analysis, Internet traffic monitoring and healthcare analytics. Event data are observed irregularly in continuous time, and the precise time interval between two events carries a great deal of information about the dynamics of the underlying systems. How to detect changes in these systems as quickly as possible based on such event data? In this paper, we present a novel online detection algorithm for high dimensional event data over networks. Our method is based on a likelihood ratio test for point processes, and achieve weak signal detection by aggregating local statistics over time and networks. We also design an online algorithm for efficiently updating the statistics using an EM-like algorithm, and derive highly accurate theoretical characterization of the false-alarm-rate. We demonstrate the good performance of our algorithm via numerical examples and real-world twitter and memetracker datasets. version:1
arxiv-1511-02825 | Multiple Instance Dictionary Learning using Functions of Multiple Instances | http://arxiv.org/abs/1511.02825 | id:1511.02825 author:Changzhe Jiao, Alina Zare category:cs.CV cs.LG stat.ML  published:2015-11-09 summary:A multiple instance dictionary learning method using functions of multiple instances (DL-FUMI) is proposed to address target detection and two-class classification problems with inaccurate training labels. Given inaccurate training labels, DL-FUMI learns a set of target dictionary atoms that describe the most distinctive and representative features of the true positive class as well as a set of nontarget dictionary atoms that account for the shared information found in both the positive and negative instances. Experimental results show that the estimated target dictionary atoms found by DL-FUMI are more representative prototypes and identify better discriminative features of the true positive class than existing methods in the literature. DL-FUMI is shown to have significantly better performance on several target detection and classification problems as compared to other multiple instance learning (MIL) dictionary learning algorithms on a variety of MIL problems. version:2
arxiv-1603-08968 | FAST: Free Adaptive Super-Resolution via Transfer for Compressed Videos | http://arxiv.org/abs/1603.08968 | id:1603.08968 author:Zhengdong Zhang, Vivienne Sze category:cs.CV  published:2016-03-29 summary:High resolution displays are increasingly popular, requiring most of the existing video content to be adapted to higher resolution. State-of-the-art super-resolution algorithms mainly address the visual quality of the output instead of real-time throughput. This paper introduces FAST, a framework to accelerate any image based super-resolution algorithm running on compressed videos. FAST leverages the similarity between adjacent frames in a video. Given the output of a super-resolution algorithm on one frame, the technique adaptively transfers it to the adjacent frames and skips running the super-resolution algorithm. The transferring process has negligible computation cost because the required information, including motion vectors, block size, and prediction residual, are embedded in the compressed video for free. In this work, we show that FAST accelerates state-of-the-art super-resolution algorithms by up to an order of magnitude with acceptable quality loss of up to 0.2 dB. Thus, we believe that the FAST framework is an important step towards enabling real-time super-resolution algorithms that upsample streamed videos for large displays. version:1
arxiv-1604-03426 | Sweep Distortion Removal from THz Images via Blind Demodulation | http://arxiv.org/abs/1604.03426 | id:1604.03426 author:Alireza Aghasi, Barmak Heshmat, Albert Redo-Sanchez, Justin Romberg, Ramesh Raskar category:cs.CV physics.optics  published:2016-03-29 summary:Heavy sweep distortion induced by alignments and inter-reflections of layers of a sample is a major burden in recovering 2D and 3D information in time resolved spectral imaging. This problem cannot be addressed by conventional denoising and signal processing techniques as it heavily depends on the physics of the acquisition. Here we propose and implement an algorithmic framework based on low-rank matrix recovery and alternating minimization that exploits the forward model for THz acquisition. The method allows recovering the original signal in spite of the presence of temporal-spatial distortions. We address a blind-demodulation problem, where based on several observations of the sample texture modulated by an undesired sweep pattern, the two classes of signals are separated. The performance of the method is examined in both synthetic and experimental data, and the successful reconstructions are demonstrated. The proposed general scheme can be implemented to advance inspection and imaging applications in THz and other time-resolved sensing modalities. version:1
arxiv-1603-08907 | Cross-modal Supervision for Learning Active Speaker Detection in Video | http://arxiv.org/abs/1603.08907 | id:1603.08907 author:Punarjay Chakravarty, Tinne Tuytelaars category:cs.CV  published:2016-03-29 summary:In this paper, we show how to use audio to supervise the learning of active speaker detection in video. Voice Activity Detection (VAD) guides the learning of the vision-based classifier in a weakly supervised manner. The classifier uses spatio-temporal features to encode upper body motion - facial expressions and gesticulations associated with speaking. We further improve a generic model for active speaker detection by learning person specific models. Finally, we demonstrate the online adaptation of generic models learnt on one dataset, to previously unseen people in a new dataset, again using audio (VAD) for weak supervision. The use of temporal continuity overcomes the lack of clean training data. We are the first to present an active speaker detection system that learns on one audio-visual dataset and automatically adapts to speakers in a new dataset. This work can be seen as an example of how the availability of multi-modal data allows us to learn a model without the need for supervision, by transferring knowledge from one modality to another. version:1
arxiv-1409-4327 | Zero Shot Recognition with Unreliable Attributes | http://arxiv.org/abs/1409.4327 | id:1409.4327 author:Dinesh Jayaraman, Kristen Grauman category:cs.CV stat.ML  published:2014-09-15 summary:In principle, zero-shot learning makes it possible to train a recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like \emph{striped} and \emph{four-legged}, one can construct a classifier for the zebra category by enumerating which properties it possesses---even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute's error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly. version:2
arxiv-1505-02206 | Learning image representations tied to ego-motion | http://arxiv.org/abs/1505.02206 | id:1505.02206 author:Dinesh Jayaraman, Kristen Grauman category:cs.CV cs.AI stat.ML  published:2015-05-08 summary:Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain. version:2
arxiv-1402-4893 | Anisotropic Mesh Adaptation for Image Representation | http://arxiv.org/abs/1402.4893 | id:1402.4893 author:Xianping Li category:cs.CV math.NA I.4.2  published:2014-02-20 summary:Triangular meshes have gained much interest in image representation and have been widely used in image processing. This paper introduces a framework of anisotropic mesh adaptation (AMA) methods to image representation and proposes a GPRAMA method that is based on AMA and greedy-point removal (GPR) scheme. Different than many other methods that triangulate sample points to form the mesh, the AMA methods start directly with a triangular mesh and then adapt the mesh based on a user-defined metric tensor to represent the image. The AMA methods have clear mathematical framework and provides flexibility for both image representation and image reconstruction. A mesh patching technique is developed for the implementation of the GPRAMA method, which leads to an improved version of the popular GPRFS-ED method. The GPRAMA method can achieve better quality than the GPRFS-ED method but with lower computational cost. version:4
arxiv-1603-08884 | A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data | http://arxiv.org/abs/1603.08884 | id:1603.08884 author:Adam Trischler, Zheng Ye, Xingdi Yuan, Jing He, Phillip Bachman, Kaheer Suleman category:cs.CL I.2.7  published:2016-03-29 summary:Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\it MCTest} benchmark. Partly because of its limited size, prior work on {\it MCTest} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\it MCTest}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15\% absolute). version:1
arxiv-1604-01683 | Fusing Face and Periocular biometrics using Canonical correlation analysis | http://arxiv.org/abs/1604.01683 | id:1604.01683 author:N. S. Lakshmiprabha category:cs.CV  published:2016-03-29 summary:This paper presents a novel face and periocular biometric fusion at feature level using canonical correlation analysis. Face recognition itself has limitations such as illumination, pose, expression, occlusion etc. Also, periocular biometrics has spectacles, head angle, hair and expression as its limitations. Unimodal biometrics cannot surmount all these limitations. The recognition accuracy can be increased by fusing dual information (face and periocular) from a single source (face image) using canonical correlation analysis (CCA). This work also proposes a new wavelet decomposed local binary pattern (WD-LBP) feature extractor which provides sufficient features for fusion. A detailed analysis on face and periocular biometrics shows that WD-LBP features are more accurate and faster than local binary pattern (LBP) and gabor wavelet. The experimental results using Muct face database reveals that the proposed multimodal biometrics performs better than the unimodal biometrics. version:1
arxiv-1603-08868 | A Readable Read: Automatic Assessment of Language Learning Materials based on Linguistic Complexity | http://arxiv.org/abs/1603.08868 | id:1603.08868 author:Ildikó Pilán, Sowmya Vajjala, Elena Volodina category:cs.CL  published:2016-03-29 summary:Corpora and web texts can become a rich language learning resource if we have a means of assessing whether they are linguistically appropriate for learners at a given proficiency level. In this paper, we aim at addressing this issue by presenting the first approach for predicting linguistic complexity for Swedish second language learning material on a 5-point scale. After showing that the traditional Swedish readability measure, L\"asbarhetsindex (LIX), is not suitable for this task, we propose a supervised machine learning model, based on a range of linguistic features, that can reliably classify texts according to their difficulty level. Our model obtained an accuracy of 81.3% and an F-score of 0.8, which is comparable to the state of the art in English and is considerably higher than previously reported results for other languages. We further studied the utility of our features with single sentences instead of full texts since sentences are a common linguistic unit in language learning exercises. We trained a separate model on sentence-level data with five classes, which yielded 63.4% accuracy. Although this is lower than the document level performance, we achieved an adjacent accuracy of 92%. Furthermore, we found that using a combination of different features, compared to using lexical features alone, resulted in 7% improvement in classification accuracy at the sentence level, whereas at the document level, lexical features were more dominant. Our models are intended for use in a freely accessible web-based language learning platform for the automatic generation of exercises. version:1
arxiv-1506-05032 | Histopathological Image Classification using Discriminative Feature-oriented Dictionary Learning | http://arxiv.org/abs/1506.05032 | id:1506.05032 author:Tiep Huu Vu, Hojjat Seyed Mousavi, Vishal Monga, Arvind UK Rao, Ganesh Rao category:cs.CV  published:2015-06-16 summary:In histopathological image analysis, feature extraction for classification is a challenging task due to the diversity of histology features suitable for each problem as well as presence of rich geometrical structures. In this paper, we propose an automatic feature discovery framework via learning class-specific dictionaries and present a low-complexity method for classification and disease grading in histopathology. Essentially, our Discriminative Feature-oriented Dictionary Learning (DFDL) method learns class-specific dictionaries such that under a sparsity constraint, the learned dictionaries allow representing a new image sample parsimoniously via the dictionary corresponding to the class identity of the sample. At the same time, the dictionary is designed to be poorly capable of representing samples from other classes. Experiments on three challenging real-world image databases: 1) histopathological images of intraductal breast lesions, 2) mammalian kidney, lung and spleen images provided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University, and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, reveal the merits of our proposal over state-of-the-art alternatives. {Moreover, we demonstrate that DFDL exhibits a more graceful decay in classification accuracy against the number of training images which is highly desirable in practice where generous training is often not available version:5
arxiv-1604-01684 | Face Image Analysis using AAM, Gabor, LBP and WD features for Gender, Age, Expression and Ethnicity Classification | http://arxiv.org/abs/1604.01684 | id:1604.01684 author:N. S. Lakshmiprabha category:cs.CV  published:2016-03-29 summary:The growth in electronic transactions and human machine interactions rely on the information such as gender, age, expression and ethnicity provided by the face image. In order to obtain these information, feature extraction plays a major role. In this paper, retrieval of age, gender, expression and race information from an individual face image is analysed using different feature extraction methods. The performance of four major feature extraction methods such as Active Appearance Model (AAM), Gabor wavelets, Local Binary Pattern (LBP) and Wavelet Decomposition (WD) are analyzed for gender recognition, age estimation, expression recognition and racial recognition in terms of accuracy (recognition rate), time for feature extraction, neural training and time to test an image. Each of this recognition system is compared with four feature extractors on same dataset (training and validation set) to get a better understanding in its performance. Experiments carried out on FG-NET, Cohn-Kanade, PAL face database shows that each method has its own merits and demerits. Hence it is practically impossible to define a method which is best at all circumstances with less computational complexity. Further, a detailed comparison of age estimation and age estimation using gender information is provided along with a solution to overcome aging effect in case of gender recognition. An attempt has been made in obtaining all (i.e. gender, age range, expression and ethnicity) information from a test image in a single go. version:1
arxiv-1603-08832 | Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community | http://arxiv.org/abs/1603.08832 | id:1603.08832 author:Ethan Fast, Tina Vachovsky, Michael S. Bernstein category:cs.CL cs.SI  published:2016-03-29 summary:Imagine a princess asleep in a castle, waiting for her prince to slay the dragon and rescue her. Tales like the famous Sleeping Beauty clearly divide up gender roles. But what about more modern stories, borne of a generation increasingly aware of social constructs like sexism and racism? Do these stories tend to reinforce gender stereotypes, or counter them? In this paper, we present a technique that combines natural language processing with a crowdsourced lexicon of stereotypes to capture gender biases in fiction. We apply this technique across 1.8 billion words of fiction from the Wattpad online writing community, investigating gender representation in stories, how male and female characters behave and are described, and how authors' use of gender stereotypes is associated with the community's ratings. We find that male over-representation and traditional gender stereotypes (e.g., dominant men and submissive women) are common throughout nearly every genre in our corpus. However, only some of these stereotypes, like sexual or violent men, are associated with highly rated stories. Finally, despite women often being the target of negative stereotypes, female authors are equally likely to write such stereotypes as men. version:1
arxiv-1603-08831 | Towards Understanding Sparse Filtering: A Theoretical Perspective | http://arxiv.org/abs/1603.08831 | id:1603.08831 author:Fabio Massimo Zennaro, Ke Chen category:cs.LG  published:2016-03-29 summary:In this paper we present our study on a recent and effective algorithm for unsupervised learning, that is, sparse filtering. The aim of this research is not to show whether or how well sparse filtering works, but to understand why and when sparse filtering does work. We provide a thorough study of this algorithm through a conceptual evaluation of feature distribution learning, a theoretical analysis of the properties of sparse filtering, and an experimental validation of our conclusions. We argue that sparse filtering works by explicitly maximizing the informativeness of the learned representation through the maximization of the proxy of sparsity, and by implicitly preserving information conveyed by the distribution of the original data through the constraint of structure preservation. In particular, we prove that sparse filtering preserves the cosine neighborhoodness of the data. We validate our statements on artificial and real data sets by applying our theoretical understanding to the explanation of the success of sparse filtering on real-world problems. Our work provides a strong theoretical framework for understanding sparse filtering, it highlights assumptions and conditions for success behind the algorithm, and it provides a fresh insight into developing new feature distribution learning algorithms. version:1
arxiv-1603-08815 | Spectral M-estimation with Applications to Hidden Markov Models | http://arxiv.org/abs/1603.08815 | id:1603.08815 author:Dustin Tran, Minjae Kim, Finale Doshi-Velez category:stat.CO cs.LG stat.ME  published:2016-03-29 summary:Method of moment estimators exhibit appealing statistical properties, such as asymptotic unbiasedness, for nonconvex problems. However, they typically require a large number of samples and are extremely sensitive to model misspecification. In this paper, we apply the framework of M-estimation to develop both a generalized method of moments procedure and a principled method for regularization. Our proposed M-estimator obtains optimal sample efficiency rates (in the class of moment-based estimators) and the same well-known rates on prediction accuracy as other spectral estimators. It also makes it straightforward to incorporate regularization into the sample moment conditions. We demonstrate empirically the gains in sample efficiency from our approach on hidden Markov models. version:1
arxiv-1603-08813 | Locally Epistatic Models for Genome-wide Prediction and Association by Importance Sampling | http://arxiv.org/abs/1603.08813 | id:1603.08813 author:Deniz Akdemir, Jean-Luc Jannink category:stat.AP q-bio.QM stat.ML  published:2016-03-29 summary:In statistical genetics an important task involves building predictive models for the genotype-phenotype relationships and thus attribute a proportion of the total phenotypic variance to the variation in genotypes. Numerous models have been proposed to incorporate additive genetic effects into models for prediction or association. However, there is a scarcity of models that can adequately account for gene by gene or other forms of genetical interactions. In addition, there is an increased interest in using marker annotations in genome-wide prediction and association. In this paper, we discuss an hybrid modeling methodology which combines the parametric mixed modeling approach and the non-parametric rule ensembles. This approach gives us a flexible class of models that can be used to capture additive, locally epistatic genetic effects, gene x background interactions and allows us to incorporate one or more annotations into the genomic selection or association models. We use benchmark data sets covering a range of organisms and traits in addition to simulated data sets to illustrate the strengths of this approach. The improvement of model accuracies and association results suggest that a part of the "missing heritability" in complex traits can be captured by modeling local epistasis. version:1
arxiv-1603-08810 | Scalable Solution for Approximate Nearest Subspace Search | http://arxiv.org/abs/1603.08810 | id:1603.08810 author:Masakazu Iwamura, Masataka Konishi, Koichi Kise category:cs.CV  published:2016-03-29 summary:Finding the nearest subspace is a fundamental problem and influential to many applications. In particular, a scalable solution that is fast and accurate for a large problem has a great impact. The existing methods for the problem are, however, useless in a large-scale problem with a large number of subspaces and high dimensionality of the feature space. A cause is that they are designed based on the traditional idea to represent a subspace by a single point. In this paper, we propose a scalable solution for the approximate nearest subspace search (ANSS) problem. Intuitively, the proposed method represents a subspace by multiple points unlike the existing methods. This makes a large-scale ANSS problem tractable. In the experiment with 3036 subspaces in the 1024-dimensional space, we confirmed that the proposed method was 7.3 times faster than the previous state-of-the-art without loss of accuracy. version:1
arxiv-1511-06939 | Session-based Recommendations with Recurrent Neural Networks | http://arxiv.org/abs/1511.06939 | id:1511.06939 author:Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos Tikk category:cs.LG cs.IR cs.NE  published:2015-11-21 summary:We apply recurrent neural networks (RNN) on a new domain, namely recommender systems. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an RNN-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic RNNs such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches. version:4
arxiv-1511-08418 | A Computational Model for Amodal Completion | http://arxiv.org/abs/1511.08418 | id:1511.08418 author:Maria Oliver, Gloria Haro, Mariella Dimiccoli, Baptiste Mazin, Coloma Ballester category:cs.CV  published:2015-11-26 summary:This paper presents a computational model to recover the most likely interpretation of the 3D scene structure from a planar image, where some objects may occlude others. The estimated scene interpretation is obtained by integrating some global and local cues and provides both the complete disoccluded objects that form the scene and their ordering according to depth. Our method first computes several distal scenes which are compatible with the proximal planar image. To compute these different hypothesized scenes, we propose a perceptually inspired object disocclusion method, which works by minimizing the Euler's elastica as well as by incorporating the relatability of partially occluded contours and the convexity of the disoccluded objects. Then, to estimate the preferred scene we rely on a Bayesian model and define probabilities taking into account the global complexity of the objects in the hypothesized scenes as well as the effort of bringing these objects in their relative position in the planar image, which is also measured by an Euler's elastica-based quantity. The model is illustrated with numerical experiments on, both, synthetic and real images showing the ability of our model to reconstruct the occluded objects and the preferred perceptual order among them. We also present results on images of the Berkeley dataset with provided figure-ground ground-truth labeling. version:2
arxiv-1503-05724 | A Neural Transfer Function for a Smooth and Differentiable Transition Between Additive and Multiplicative Interactions | http://arxiv.org/abs/1503.05724 | id:1503.05724 author:Sebastian Urban, Patrick van der Smagt category:stat.ML cs.LG cs.NE  published:2015-03-19 summary:Existing approaches to combine both additive and multiplicative neural units either use a fixed assignment of operations or require discrete optimization to determine what function a neuron should perform. This leads either to an inefficient distribution of computational resources or an extensive increase in the computational complexity of the training procedure. We present a novel, parameterizable transfer function based on the mathematical concept of non-integer functional iteration that allows the operation each neuron performs to be smoothly and, most importantly, differentiablely adjusted between addition and multiplication. This allows the decision between addition and multiplication to be integrated into the standard backpropagation training procedure. version:3
arxiv-1506-03059 | Deep SimNets | http://arxiv.org/abs/1506.03059 | id:1506.03059 author:Nadav Cohen, Or Sharir, Amnon Shashua category:cs.NE cs.LG  published:2015-06-09 summary:We present a deep layered architecture that generalizes convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators: (i) a similarity function that generalizes inner-product, and (ii) a log-mean-exp function called MEX that generalizes maximum and average. The two operators applied in succession give rise to a standard neuron but in "feature space". The feature spaces realized by SimNets depend on the choice of the similarity operator. The simplest setting, which corresponds to a convolution, realizes the feature space of the Exponential kernel, while other settings realize feature spaces of more powerful kernels (Generalized Gaussian, which includes as special cases RBF and Laplacian), or even dynamically learned feature spaces (Generalized Multiple Kernel Learning). As a result, the SimNet contains a higher abstraction level compared to a traditional ConvNet. We argue that enhanced expressiveness is important when the networks are small due to run-time constraints (such as those imposed by mobile applications). Empirical evaluation validates the superior expressiveness of SimNets, showing a significant gain in accuracy over ConvNets when computational resources at run-time are limited. We also show that in large-scale settings, where computational complexity is less of a concern, the additional capacity of SimNets can be controlled with proper regularization, yielding accuracies comparable to state of the art ConvNets. version:2
arxiv-1603-08767 | Machine Learning and Cloud Computing: Survey of Distributed and SaaS Solutions | http://arxiv.org/abs/1603.08767 | id:1603.08767 author:Daniel Pop category:cs.DC cs.LG  published:2016-03-29 summary:Applying popular machine learning algorithms to large amounts of data raised new challenges for the ML practitioners. Traditional ML libraries does not support well processing of huge datasets, so that new approaches were needed. Parallelization using modern parallel computing frameworks, such as MapReduce, CUDA, or Dryad gained in popularity and acceptance, resulting in new ML libraries developed on top of these frameworks. We will briefly introduce the most prominent industrial and academic outcomes, such as Apache Mahout, GraphLab or Jubatus. We will investigate how cloud computing paradigm impacted the field of ML. First direction is of popular statistics tools and libraries (R system, Python) deployed in the cloud. A second line of products is augmenting existing tools with plugins that allow users to create a Hadoop cluster in the cloud and run jobs on it. Next on the list are libraries of distributed implementations for ML algorithms, and on-premise deployments of complex systems for data analytics and data mining. Last approach on the radar of this survey is ML as Software-as-a-Service, several BigData start-ups (and large companies as well) already opening their solutions to the market. version:1
arxiv-1603-08754 | Multi-Cue Zero-Shot Learning with Strong Supervision | http://arxiv.org/abs/1603.08754 | id:1603.08754 author:Zeynep Akata, Mateusz Malinowski, Mario Fritz, Bernt Schiele category:cs.CV  published:2016-03-29 summary:Scaling up visual category recognition to large numbers of classes remains challenging. A promising research direction is zero-shot learning, which does not require any training data to recognize new classes, but rather relies on some form of auxiliary information describing the new classes. Ultimately, this may allow to use textbook knowledge that humans employ to learn about new classes by transferring knowledge from classes they know well. The most successful zero-shot learning approaches currently require a particular type of auxiliary information -- namely attribute annotations performed by humans -- that is not readily available for most classes. Our goal is to circumvent this bottleneck by substituting such annotations by extracting multiple pieces of information from multiple unstructured text sources readily available on the web. To compensate for the weaker form of auxiliary information, we incorporate stronger supervision in the form of semantic part annotations on the classes from which we transfer knowledge. We achieve our goal by a joint embedding framework that maps multiple text parts as well as multiple semantic parts into a common space. Our results consistently and significantly improve on the state-of-the-art in zero-short recognition and retrieval. version:1
arxiv-1603-08253 | Negative Learning Rates and P-Learning | http://arxiv.org/abs/1603.08253 | id:1603.08253 author:Devon Merrill category:cs.AI cs.LG  published:2016-03-27 summary:We present a method of training a differentiable function approximator for a regression task using negative examples. We effect this training using negative learning rates. We also show how this method can be used to perform direct policy learning in a reinforcement learning setting. version:2
arxiv-1603-08720 | Multi-Band Image Fusion Based on Spectral Unmixing | http://arxiv.org/abs/1603.08720 | id:1603.08720 author:Qi Wei, Jose Bioucas-Dias, Nicolas Dobigeon, Jean-Yves Tourneret, Marcus Chen, Simon Godsill category:cs.CV  published:2016-03-29 summary:This paper presents a multi-band image fusion algorithm based on unsupervised spectral unmixing for combining a high-spatial low-spectral resolution image and a low-spatial high-spectral resolution image. The widely used linear observation model (with additive Gaussian noise) is combined with the linear spectral mixture model to form the likelihoods of the observations. The non-negativity and sum-to-one constraints resulting from the intrinsic physical properties of the abundances are introduced as prior information to regularize this ill-posed problem. The joint fusion and unmixing problem is then formulated as maximizing the joint posterior distribution with respect to the endmember signatures and abundance maps, This optimization problem is attacked with an alternating optimization strategy. The two resulting sub-problems are convex and are solved efficiently using the alternating direction method of multipliers. Experiments are conducted for both synthetic and semi-real data. Simulation results show that the proposed unmixing based fusion scheme improves both the abundance and endmember estimation comparing with the state-of-the-art joint fusion and unmixing algorithms. version:1
arxiv-1411-6757 | Echo State Condition at the Critical Point | http://arxiv.org/abs/1411.6757 | id:1411.6757 author:Norbert Michael Mayer category:cs.NE  published:2014-11-25 summary:Recurrent networks that have transfer functions that fulfill the Lipschitz continuity with L=1, may be echo state networks if certain limitations on the recurrent connectivity are applied. Initially it has been shown that it is sufficient if the largest singular value of the recurrent connectivity S is smaller than 1. The main achievement of this paper is a proof under which conditions the network is an echo state network even if S=1. It turns out that in this critical case the exact shape of the transfer function plays a decisive role whether or not the network still fulfills the echo state condition. In addition, several intuitive examples with one neuron networks are outlined to illustrate effects of critical connectivity. version:7
arxiv-1603-08708 | Unified View of Matrix Completion under General Structural Constraints | http://arxiv.org/abs/1603.08708 | id:1603.08708 author:Suriya Gunasekar, Arindam Banerjee, Joydeep Ghosh category:stat.ML  published:2016-03-29 summary:In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by {\em any} norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on results from generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably the recently proposed spectral $k$-support norm. version:1
arxiv-1603-08705 | ROOT13: Spotting Hypernyms, Co-Hyponyms and Randoms | http://arxiv.org/abs/1603.08705 | id:1603.08705 author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL  published:2016-03-29 summary:In this paper, we describe ROOT13, a supervised system for the classification of hypernyms, co-hyponyms and random words. The system relies on a Random Forest algorithm and 13 unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT13 achieves an F1 score of 88.3%, against a baseline of 57.6% (vector cosine). When the classification is binary, ROOT13 achieves the following results: hypernyms-co-hyponyms (93.4% vs. 60.2%), hypernymsrandom (92.3% vs. 65.5%) and co-hyponyms-random (97.3% vs. 81.5%). Our results are competitive with stateof-the-art models. version:1
arxiv-1603-08704 | Interpretability of Multivariate Brain Maps in Brain Decoding: Definition and Quantification | http://arxiv.org/abs/1603.08704 | id:1603.08704 author:Seyed Mostafa Kia category:stat.ML cs.LG  published:2016-03-29 summary:Brain decoding is a popular multivariate approach for hypothesis testing in neuroimaging. It is well known that the brain maps derived from weights of linear classifiers are hard to interpret because of high correlations between predictors, low signal to noise ratios, and the high dimensionality of neuroimaging data. Therefore, improving the interpretability of brain decoding approaches is of primary interest in many neuroimaging studies. Despite extensive studies of this type, at present, there is no formal definition for interpretability of multivariate brain maps. As a consequence, there is no quantitative measure for evaluating the interpretability of different brain decoding methods. In this paper, first, we present a theoretical definition of interpretability in brain decoding; we show that the interpretability of multivariate brain maps can be decomposed into their reproducibility and representativeness. Second, as an application of the proposed theoretical definition, we formalize a heuristic method for approximating the interpretability of multivariate brain maps in a binary magnetoencephalography (MEG) decoding scenario. Third, we propose to combine the approximated interpretability and the performance of the brain decoding model into a new multi-objective criterion for model selection. Our results for the MEG data show that optimizing the hyper-parameters of the regularized linear classifier based on the proposed criterion results in more informative multivariate brain maps. More importantly, the presented definition provides the theoretical background for quantitative evaluation of interpretability, and hence, facilitates the development of more effective brain decoding algorithms in the future. version:1
arxiv-1603-08702 | Nine Features in a Random Forest to Learn Taxonomical Semantic Relations | http://arxiv.org/abs/1603.08702 | id:1603.08702 author:Enrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin Lu, Chu-Ren Huang category:cs.CL  published:2016-03-29 summary:ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%, hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias. version:1
arxiv-1603-08701 | What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets | http://arxiv.org/abs/1603.08701 | id:1603.08701 author:Enrico Santus, Tin-Shing Chiu, Qin Lu, Alessandro Lenci, Chu-Ren Huang category:cs.CL  published:2016-03-29 summary:In this paper, we claim that Vector Cosine, which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models, can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that, independently of the adopted parameters, outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches. version:1
arxiv-1603-08695 | Learning to Refine Object Segments | http://arxiv.org/abs/1603.08695 | id:1603.08695 author:Pedro O. Pinheiro, Tsung-Yi Lin, Ronan Collobert, Piotr Dollàr category:cs.CV  published:2016-03-29 summary:Object segmentation requires both object-level information and low-level pixel data. This presents a challenge for feedforward networks: lower layers in convolutional nets capture rich spatial information, while upper layers encode object-level knowledge but are invariant to factors such as pose and appearance. In this work we propose to augment feedforward nets for object segmentation with a novel top-down refinement approach. The resulting bottom-up/top-down architecture is capable of efficiently generating high-fidelity object masks. Similarly to skip connections, our approach leverages features at all layers of the net. Unlike skip connections, our approach does not attempt to output independent predictions at each layer. Instead, we first output a coarse `mask encoding' in a feedforward pass, then refine this mask encoding in a top-down pass utilizing features at successively lower layers. The approach is simple, fast, and effective. Building on the recent DeepMask network for generating object proposals, we show accuracy improvements of 10-20% in average recall for various setups and for small objects we improve recall by nearly 2 times. Additionally, by optimizing the overall network architecture, our approach, which we call SharpMask, is 50\% faster than the original DeepMask network (under .8s per image). version:1
arxiv-1603-08678 | Instance-sensitive Fully Convolutional Networks | http://arxiv.org/abs/1603.08678 | id:1603.08678 author:Jifeng Dai, Kaiming He, Yi Li, Shaoqing Ren, Jian Sun category:cs.CV  published:2016-03-29 summary:Fully convolutional networks (FCNs) have been proven very successful for semantic segmentation, but the FCN outputs are unaware of object instances. In this paper, we develop FCNs that are capable of proposing instance-level segment candidates. In contrast to the previous FCN that generates one score map, our FCN is designed to compute a small set of instance-sensitive score maps, each of which is the outcome of a pixel-wise classifier of a relative position to instances. On top of these instance-sensitive score maps, a simple assembling module is able to output instance candidate at each position. In contrast to the recent DeepMask method for segmenting instances, our method does not have any high-dimensional layer related to the mask resolution, but instead exploits image local coherence for estimating instances. We present competitive results of instance segment proposal on both PASCAL VOC and MS COCO. version:1
arxiv-1501-04308 | Some Insights About the Small Ball Probability Factorization for Hilbert Random Elements | http://arxiv.org/abs/1501.04308 | id:1501.04308 author:Enea Bongiorno, Aldo Goia category:math.PR math.ST stat.AP stat.ME stat.ML stat.TH 62G99  published:2015-01-18 summary:Asymptotic factorizations for the small-ball probability (SmBP) of a Hilbert valued random element $X$ are rigorously established and discussed. In particular, given the first $d$ principal components (PCs) and as the radius $\varepsilon$ of the ball tends to zero, the SmBP is asymptotically proportional to (a) the joint density of the first $d$ PCs, (b) the volume of the $d$-dimensional ball with radius $\varepsilon$, and (c) a correction factor weighting the use of a truncated version of the process expansion. Moreover, under suitable assumptions on the spectrum of the covariance operator of $X$ and as $d$ diverges to infinity when $\varepsilon$ vanishes, some simplifications occur. In particular, the SmBP factorizes asymptotically as the product of the joint density of the first $d$ PCs and a pure volume parameter. All the provided factorizations allow to define a surrogate intensity of the SmBP that, in some cases, leads to a genuine intensity. To operationalize the stated results, a non-parametric estimator for the surrogate intensity is introduced and it is proved that the use of estimated PCs, instead of the true ones, does not affect the rate of convergence. Finally, as an illustration, simulations in controlled frameworks are provided. version:2
arxiv-1506-07300 | Flexible Multi-layer Sparse Approximations of Matrices and Applications | http://arxiv.org/abs/1506.07300 | id:1506.07300 author:Luc Le Magoarou, Rémi Gribonval category:cs.LG  published:2015-06-24 summary:The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in non-convex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising, and the approximation of large matrices arising in inverse problems. version:2
arxiv-1602-05388 | Cross-Language Domain Adaptation for Classifying Crisis-Related Short Messages | http://arxiv.org/abs/1602.05388 | id:1602.05388 author:Muhammad Imran, Prasenjit Mitra, Jaideep Srivastava category:cs.CL  published:2016-02-17 summary:Rapid crisis response requires real-time analysis of messages. After a disaster happens, volunteers attempt to classify tweets to determine needs, e.g., supplies, infrastructure damage, etc. Given labeled data, supervised machine learning can help classify these messages. Scarcity of labeled data causes poor performance in machine training. Can we reuse old tweets to train classifiers? How can we choose labeled tweets for training? Specifically, we study the usefulness of labeled data of past events. Do labeled tweets in different language help? We observe the performance of our classifiers trained using different combinations of training sets obtained from past disasters. We perform extensive experimentation on real crisis datasets and show that the past labels are useful when both source and target events are of the same type (e.g. both earthquakes). For similar languages (e.g., Italian and Spanish), cross-language domain adaptation was useful, however, when for different languages (e.g., Italian and English), the performance decreased. version:2
arxiv-1603-04136 | On the Influence of Momentum Acceleration on Online Learning | http://arxiv.org/abs/1603.04136 | id:1603.04136 author:Kun Yuan, Bicheng Ying, Ali H. Sayed category:math.OC cs.LG stat.ML  published:2016-03-14 summary:The article examines in some detail the convergence rate and mean-square-error performance of momentum stochastic gradient methods in the constant step-size case and slow adaptation regime. The results establish that momentum methods are equivalent to the standard stochastic gradient method with a re-scaled (larger) step-size value. The size of the re-scaling is determined by the value of the momentum parameter. The equivalence result is established for all time instants and not only in steady-state. The analysis is carried out for general risk functions, and is not limited to quadratic risks. One notable conclusion is that the well-known benefits of momentum constructions for deterministic optimization problems do not necessarily carry over to the stochastic (online) setting when adaptation becomes necessary and when the true gradient vectors are not known beforehand. The analysis also suggests a method to retain some of the advantages of the momentum construction by employing a decaying momentum parameter, as opposed to a decaying step-size. In this way, the enhanced convergence rate during the initial stages of adaptation is preserved without the often-observed degradation in MSD performance. version:2
arxiv-1502-06105 | Regularization and Kernelization of the Maximin Correlation Approach | http://arxiv.org/abs/1502.06105 | id:1502.06105 author:Taehoon Lee, Taesup Moon, Seung Jean Kim, Sungroh Yoon category:cs.CV cs.LG  published:2015-02-21 summary:Robust classification becomes challenging when each class consists of multiple subclasses. Examples include multi-font optical character recognition and automated protein function prediction. In correlation-based nearest-neighbor classification, the maximin correlation approach (MCA) provides the worst-case optimal solution by minimizing the maximum misclassification risk through an iterative procedure. Despite the optimality, the original MCA has drawbacks that have limited its wide applicability in practice. That is, the MCA tends to be sensitive to outliers, cannot effectively handle nonlinearities in datasets, and suffers from having high computational complexity. To address these limitations, we propose an improved solution, named regularized maximin correlation approach (R-MCA). We first reformulate MCA as a quadratically constrained linear programming (QCLP) problem, incorporate regularization by introducing slack variables in the primal problem of the QCLP, and derive the corresponding Lagrangian dual. The dual formulation enables us to apply the kernel trick to R-MCA so that it can better handle nonlinearities. Our experimental results demonstrate that the regularization and kernelization make the proposed R-MCA more robust and accurate for various classification tasks than the original MCA. Furthermore, when the data size or dimensionality grows, R-MCA runs substantially faster by solving either the primal or dual (whichever has a smaller variable dimension) of the QCLP. version:2
arxiv-1603-08637 | Learning a Predictable and Generative Vector Representation for Objects | http://arxiv.org/abs/1603.08637 | id:1603.08637 author:Rohit Girdhar, David F. Fouhey, Mikel Rodriguez, Abhinav Gupta category:cs.CV  published:2016-03-29 summary:What is a good vector representation of an object? We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images. We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties. The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable. This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval. Extensive experimental analysis demonstrates the usefulness and versatility of this embedding. version:1
arxiv-1603-06041 | Learning Image Matching by Simply Watching Video | http://arxiv.org/abs/1603.06041 | id:1603.06041 author:Gucan Long, Laurent Kneip, Jose M. Alvarez, Hongdong Li category:cs.CV  published:2016-03-19 summary:This work presents an unsupervised learning based approach to the ubiquitous computer vision problem of image matching. We start from the insight that the problem of frame-interpolation implicitly solves for inter-frame correspondences. This permits the application of analysis-by-synthesis: we firstly train and apply a Convolutional Neural Network for frame-interpolation, then obtain correspondences by inverting the learned CNN. The key benefit behind this strategy is that the CNN for frame-interpolation can be trained in an unsupervised manner by exploiting the temporal coherency that is naturally contained in real-world video sequences. The present model therefore learns image matching by simply watching videos. Besides a promise to be more generally applicable, the presented approach achieves surprising performance comparable to traditional empirically designed methods. version:2
arxiv-1603-08636 | Towards an Automated Requirements-driven Development of Smart Cyber-Physical Systems | http://arxiv.org/abs/1603.08636 | id:1603.08636 author:Jiri Vinarek, Petr Hnetynka category:cs.SE cs.CL  published:2016-03-29 summary:The Invariant Refinement Method for Self Adaptation (IRM-SA) is a design method targeting development of smart Cyber-Physical Systems (sCPS). It allows for a systematic translation of the system requirements into the system architecture expressed as an ensemble-based component system (EBCS). However, since the requirements are captured using natural language, there exists the danger of their misinterpretation due to natural language requirements' ambiguity, which could eventually lead to design errors. Thus, automation and validation of the design process is desirable. In this paper, we (i) analyze the translation process of natural language requirements into the IRM-SA model, (ii) identify individual steps that can be automated and/or validated using natural language processing techniques, and (iii) propose suitable methods. version:1
arxiv-1603-08631 | Classification of Alzheimer's Disease using fMRI Data and Deep Learning Convolutional Neural Networks | http://arxiv.org/abs/1603.08631 | id:1603.08631 author:Saman Sarraf, Ghassem Tofighi category:cs.CV  published:2016-03-29 summary:Over the past decade, machine learning techniques especially predictive modeling and pattern recognition in biomedical sciences from drug delivery system to medical imaging has become one of the important methods which are assisting researchers to have deeper understanding of entire issue and to solve complex medical problems. Deep learning is power learning machine learning algorithm in classification while extracting high-level features. In this paper, we used convolutional neural network to classify Alzheimer's brain from normal healthy brain. The importance of classifying this kind of medical data is to potentially develop a predict model or system in order to recognize the type disease from normal subjects or to estimate the stage of the disease. Classification of clinical data such as Alzheimer's disease has been always challenging and most problematic part has been always selecting the most discriminative features. Using Convolutional Neural Network (CNN) and the famous architecture LeNet-5, we successfully classified functional MRI data of Alzheimer's subjects from normal controls where the accuracy of test data on trained data reached 96.85%. This experiment suggests us the shift and scale invariant features extracted by CNN followed by deep learning classification is most powerful method to distinguish clinical data from healthy data in fMRI. This approach also enables us to expand our methodology to predict more complicated systems. version:1
arxiv-1603-08232 | Exact Subsampling MCMC | http://arxiv.org/abs/1603.08232 | id:1603.08232 author:Matias Quiroz, Mattias Villani, Robert Kohn category:stat.CO stat.ME stat.ML  published:2016-03-27 summary:Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many observations by data subsampling has recently received considerable attention in the literature. Most of the proposed methods are approximate, and the only exact solution has been documented to be highly inefficient. We propose a simulation consistent subsampling method for estimating expectations of any function of the parameters using a combination of MCMC subsampling and the importance sampling correction for occasionally negative likelihood estimates in Lyne et al. (2015). Our algorithm is based on first obtaining an unbiased but not necessarily positive estimate of the likelihood. The estimator uses a soft lower bound such that the likelihood estimate is positive with a high probability, and computationally cheap control variables to lower variability. Second, we carry out a correlated pseudo marginal MCMC on the absolute value of the likelihood estimate. Third, the sign of the likelihood is corrected using an importance sampling step that has low variance by construction. We illustrate the usefulness of the method with two examples. version:2
arxiv-1603-08616 | Submodular Variational Inference for Network Reconstruction | http://arxiv.org/abs/1603.08616 | id:1603.08616 author:Lin Chen, Amin Karbasi, Forrest W Crawford category:cs.LG cs.DS cs.SI stat.ML  published:2016-03-29 summary:In real-world and online social networks, individuals receive and transmit information in real time. Cascading information transmissions --- phone calls, text messages, social media posts --- may be understood as a realization of a diffusion process operating on the network, and its branching path can be represented by a directed tree. One important feature of dynamic real-world diffusion processes is that the process may not traverse every edge in the network on which it operates. When the network itself is unknown, the path of the diffusion process may reveal some, but not all, of the edges connecting nodes that have received the diffusing information. The network reconstruction/inference problem is to estimate connections that are not revealed by the diffusion processes. This problem naturally arises in a many disciplines. Most of existing works on network reconstruction study this problem by deriving a likelihood function for the realized diffusion process given full knowledge of the network on which it operates, and attempting to find the network topology that maximizes this likelihood. The major challenge in this work is the intractability of the optimization problem. In this paper, we focus on the network reconstruction problem for a broad class of real-world diffusion processes, exemplified by a network diffusion scheme called respondent-driven sampling (RDS) that is widely used in epidemiology. We prove that under a reasonable model of network diffusion, the likelihood of an observed RDS realization is a Bayesian log-submodular model. We propose a novel, accurate, and computationally efficient variational inference algorithm for the network reconstruction problem under this model. In this algorithm, we allow for more flexibility for the possible deviation of the subjects' reported total degrees in the underlying graphical structure from the true ones. version:1
arxiv-1603-08604 | Classiffication-based Financial Markets Prediction using Deep Neural Networks | http://arxiv.org/abs/1603.08604 | id:1603.08604 author:Matthew Dixon, Diego Klabjan, Jin Hoon Bang category:cs.LG cs.CE  published:2016-03-29 summary:Deep neural networks (DNNs) are powerful types of artificial neural networks (ANNs) that use several hidden layers. They have recently gained considerable attention in the speech transcription and image recognition community (Krizhevsky et al., 2012) for their superior predictive properties including robustness to overfitting. However their application to algorithmic trading has not been previously researched, partly because of their computational complexity. This paper describes the application of DNNs to predicting financial market movement directions. In particular we describe the configuration and training approach and then demonstrate their application to backtesting a simple trading strategy over 43 different Commodity and FX future mid-prices at 5-minute intervals. All results in this paper are generated using a C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster than the serial version and a Python strategy backtesting environment both of which are available as open source code written by the authors. version:1
arxiv-1603-00489 | Weakly Supervised Localization using Deep Feature Maps | http://arxiv.org/abs/1603.00489 | id:1603.00489 author:Archith J. Bency, Heesung Kwon, Hyungtae Lee, S. Karthikeyan, B. S. Manjunath category:cs.CV  published:2016-03-01 summary:Object localization is an important computer vision problem with a variety of applications. The lack of large scale object-level annotations and the relative abundance of image-level labels makes a compelling case for weak supervision in the object localization task. Deep Convolutional Neural Networks are a class of state-of-the-art methods for the related problem of object recognition. In this paper, we describe a novel object localization algorithm which uses classification networks trained on only image labels. This weakly supervised method leverages local spatial and semantic patterns captured in the convolutional layers of classification networks. We propose an efficient beam search based approach to detect and localize multiple objects in images. The proposed method significantly outperforms the state-of-the-art in standard object localization data-sets with a 8 point increase in mAP scores. version:2
arxiv-1407-8339 | Combinatorial Multi-Armed Bandit and Its Extension to Probabilistically Triggered Arms | http://arxiv.org/abs/1407.8339 | id:1407.8339 author:Wei Chen, Yajun Wang, Yang Yuan, Qinshi Wang category:cs.LG  published:2014-07-31 summary:We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where subsets of base arms with unknown distributions form super arms. In each round, a super arm is played and the base arms contained in the super arm are played and their outcomes are observed. We further consider the extension in which more based arms could be probabilistically triggered based on the outcomes of already triggered arms. The reward of the super arm depends on the outcomes of all played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an offline (\alpha,\beta)-approximation oracle that takes the means of the outcome distributions of arms and outputs a super arm that with probability {\beta} generates an {\alpha} fraction of the optimal expected reward. The objective of an online learning algorithm for CMAB is to minimize (\alpha,\beta)-approximation regret, which is the difference between the \alpha{\beta} fraction of the expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves O(log n) distribution-dependent regret, where n is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound of UCB1 algorithm (up to a constant factor) for the classical MAB problem, and it significantly improves the regret bound in a earlier paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage and social influence maximization, both having nonlinear reward structures. In particular, application to social influence maximization requires our extension on probabilistically triggered arms. version:6
arxiv-1603-08597 | The Conditional Lucas & Kanade Algorithm | http://arxiv.org/abs/1603.08597 | id:1603.08597 author:Chen-Hsuan Lin, Rui Zhu, Simon Lucey category:cs.CV  published:2016-03-29 summary:The Lucas & Kanade (LK) algorithm is the method of choice for efficient dense image and object alignment. The approach is efficient as it attempts to model the connection between appearance and geometric displacement through a linear relationship that assumes independence across pixel coordinates. A drawback of the approach, however, is its generative nature. Specifically, its performance is tightly coupled with how well the linear model can synthesize appearance from geometric displacement, even though the alignment task itself is associated with the inverse problem. In this paper, we present a new approach, referred to as the Conditional LK algorithm, which: (i) directly learns linear models that predict geometric displacement as a function of appearance, and (ii) employs a novel strategy for ensuring that the generative pixel independence assumption can still be taken advantage of. We demonstrate that our approach exhibits superior performance to classical generative forms of the LK algorithm. Furthermore, we demonstrate its comparable performance to state-of-the-art methods such as the Supervised Descent Method with substantially less training examples, as well as the unique ability to "swap" geometric warp functions without having to retrain from scratch. Finally, from a theoretical perspective, our approach hints at possible redundancies that exist in current state-of-the-art methods for alignment that could be leveraged in vision systems of the future. version:1
arxiv-1506-09075 | Long-Range Motion Trajectories Extraction of Articulated Human Using Mesh Evolution | http://arxiv.org/abs/1506.09075 | id:1506.09075 author:Yuanyuan Wu, Xiaohai He, Byeongkeun Kang, Haiying Song, Truong Q. Nguyen category:cs.CV  published:2015-06-30 summary:This letter presents a novel approach to extract reliable dense and long-range motion trajectories of articulated human in a video sequence. Compared with existing approaches that emphasize temporal consistency of each tracked point, we also consider the spatial structure of tracked points on the articulated human. We treat points as a set of vertices, and build a triangle mesh to join them in image space. The problem of extracting long-range motion trajectories is changed to the issue of consistency of mesh evolution over time. First, self-occlusion is detected by a novel mesh-based method and an adaptive motion estimation method is proposed to initialize mesh between successive frames. Furthermore, we propose an iterative algorithm to efficiently adjust vertices of mesh for a physically plausible deformation, which can meet the local rigidity of mesh and silhouette constraints. Finally, we compare the proposed method with the state-of-the-art methods on a set of challenging sequences. Evaluations demonstrate that our method achieves favorable performance in terms of both accuracy and integrity of extracted trajectories. version:3
arxiv-1603-08594 | Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments | http://arxiv.org/abs/1603.08594 | id:1603.08594 author:Geetanjali Rakshit, Sagar Sontakke, Pushpak Bhattacharyya, Gholamreza Haffari category:cs.CL  published:2016-03-29 summary:In this paper, we attempt to solve the problem of Prepositional Phrase (PP) attachments in English. The motivation for the work comes from NLP applications like Machine Translation, for which, getting the correct attachment of prepositions is very crucial. The idea is to correct the PP-attachments for a sentence with the help of alignments from parallel data in another language. The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the English-Hindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained for English. version:1
arxiv-1603-08592 | Exploring Local Context for Multi-target Tracking in Wide Area Aerial Surveillance | http://arxiv.org/abs/1603.08592 | id:1603.08592 author:Bor-Jeng Chen, Gerard Medioni category:cs.CV  published:2016-03-28 summary:Tracking many vehicles in wide coverage aerial imagery is crucial for understanding events in a large field of view. Most approaches aim to associate detections from frame differencing into tracks. However, slow or stopped vehicles result in long-term missing detections and further cause tracking discontinuities. Relying merely on appearance clue to recover missing detections is difficult as targets are extremely small and in grayscale. In this paper, we address the limitations of detection association methods by coupling it with a local context tracker (LCT), which does not rely on motion detections. On one hand, our LCT learns neighboring spatial relation and tracks each target in consecutive frames using graph optimization. It takes the advantage of context constraints to avoid drifting to nearby targets. We generate hypotheses from sparse and dense flow efficiently to keep solutions tractable. On the other hand, we use detection association strategy to extract short tracks in batch processing. We explicitly handle merged detections by generating additional hypotheses from them. Our evaluation on wide area aerial imagery sequences shows significant improvement over state-of-the-art methods. version:1
arxiv-1603-08589 | Generalized Exponential Concentration Inequality for Rényi Divergence Estimation | http://arxiv.org/abs/1603.08589 | id:1603.08589 author:Shashank Singh, Barnabás Póczos category:cs.IT math.IT math.ST stat.ML stat.TH  published:2016-03-28 summary:Estimating divergences in a consistent way is of great importance in many machine learning tasks. Although this is a fundamental problem in nonparametric statistics, to the best of our knowledge there has been no finite sample exponential inequality convergence bound derived for any divergence estimators. The main contribution of our work is to provide such a bound for an estimator of R\'enyi-$\alpha$ divergence for a smooth H\"older class of densities on the $d$-dimensional unit cube $[0, 1]^d$. We also illustrate our theoretical results with a numerical experiment. version:1
arxiv-1603-08584 | Exponential Concentration of a Density Functional Estimator | http://arxiv.org/abs/1603.08584 | id:1603.08584 author:Shashank Singh, Barnabás P óczos category:math.ST cs.IT math.IT stat.ML stat.TH  published:2016-03-28 summary:We analyze a plug-in estimator for a large class of integral functionals of one or more continuous probability densities. This class includes important families of entropy, divergence, mutual information, and their conditional versions. For densities on the $d$-dimensional unit cube $[0,1]^d$ that lie in a $\beta$-H\"older smoothness class, we prove our estimator converges at the rate $O \left( n^{-\frac{\beta}{\beta + d}} \right)$. Furthermore, we prove the estimator is exponentially concentrated about its mean, whereas most previous related results have proven only expected error bounds on estimators. version:1
arxiv-1502-02590 | Analysis of classifiers' robustness to adversarial perturbations | http://arxiv.org/abs/1502.02590 | id:1502.02590 author:Alhussein Fawzi, Omar Fawzi, Pascal Frossard category:cs.LG cs.CV stat.ML  published:2015-02-09 summary:The goal of this paper is to analyze an intriguing phenomenon recently discovered in deep networks, namely their instability to adversarial perturbations (Szegedy et. al., 2014). We provide a theoretical framework for analyzing the robustness of classifiers to adversarial perturbations, and show fundamental upper bounds on the robustness of classifiers. Specifically, we establish a general upper bound on the robustness of classifiers to adversarial perturbations, and then illustrate the obtained upper bound on the families of linear and quadratic classifiers. In both cases, our upper bound depends on a distinguishability measure that captures the notion of difficulty of the classification task. Our results for both classes imply that in tasks involving small distinguishability, no classifier in the considered set will be robust to adversarial perturbations, even if a good accuracy is achieved. Our theoretical framework moreover suggests that the phenomenon of adversarial instability is due to the low flexibility of classifiers, compared to the difficulty of the classification task (captured by the distinguishability). Moreover, we show the existence of a clear distinction between the robustness of a classifier to random noise and its robustness to adversarial perturbations. Specifically, the former is shown to be larger than the latter by a factor that is proportional to \sqrt{d} (with d being the signal dimension) for linear classifiers. This result gives a theoretical explanation for the discrepancy between the two robustness properties in high dimensional problems, which was empirically observed in the context of neural networks. To the best of our knowledge, our results provide the first theoretical work that addresses the phenomenon of adversarial instability recently observed for deep networks. Our analysis is complemented by experimental results on controlled and real-world data. version:4
arxiv-1603-08578 | Analysis of k-Nearest Neighbor Distances with Application to Entropy Estimation | http://arxiv.org/abs/1603.08578 | id:1603.08578 author:Shashank Singh, Barnabás Póczos category:math.ST cs.IT math.IT stat.ML stat.TH  published:2016-03-28 summary:Estimating entropy and mutual information consistently is important for many machine learning applications. The Kozachenko-Leonenko (KL) estimator (Kozachenko & Leonenko, 1987) is a widely used nonparametric estimator for the entropy of multivariate continuous random variables, as well as the basis of the mutual information estimator of Kraskov et al. (2004), perhaps the most widely used estimator of mutual information in this setting. Despite the practical importance of these estimators, major theoretical questions regarding their finite-sample behavior remain open. This paper proves finite-sample bounds on the bias and variance of the KL estimator, showing that it achieves the minimax convergence rate for certain classes of smooth functions. In proving these bounds, we analyze finite-sample behavior of k-nearest neighbors (k-NN) distance statistics (on which the KL estimator is based). We derive concentration inequalities for k-NN distances and a general expectation bound for statistics of k-NN distances, which may be useful for other analyses of k-NN methods. version:1
arxiv-1511-06881 | Zoom Better to See Clearer: Human and Object Parsing with Hierarchical Auto-Zoom Net | http://arxiv.org/abs/1511.06881 | id:1511.06881 author:Fangting Xia, Peng Wang, Liang-Chieh Chen, Alan L. Yuille category:cs.CV cs.LG  published:2015-11-21 summary:Parsing articulated objects, e.g. humans and animals, into semantic parts (e.g. body, head and arms, etc.) from natural images is a challenging and fundamental problem for computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle these difficulties, we propose a "Hierarchical Auto-Zoom Net" (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two "Auto-Zoom Net" (AZNs), each employing fully convolutional networks that perform two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively "zoom" (resize) predicted image regions into their proper scales to refine the parsing. We conduct extensive experiments over the PASCAL part datasets on humans, horses, and cows. For humans, our approach significantly outperforms the state-of-the-arts by 5% mIOU and is especially better at segmenting small instances and small parts. We obtain similar improvements for parsing cows and horses over alternative methods. In summary, our strategy of first zooming into objects and then zooming into parts is very effective. It also enables us to process different regions of the image at different scales adaptively so that, for example, we do not need to waste computational resources scaling the entire image. version:5
arxiv-1506-01342 | One-to-many face recognition with bilinear CNNs | http://arxiv.org/abs/1506.01342 | id:1506.01342 author:Aruni RoyChowdhury, Tsung-Yu Lin, Subhransu Maji, Erik Learned-Miller category:cs.CV  published:2015-06-03 summary:The recent explosive growth in convolutional neural network (CNN) research has produced a variety of new architectures for deep learning. One intriguing new architecture is the bilinear CNN (B-CNN), which has shown dramatic performance gains on certain fine-grained recognition problems [15]. We apply this new CNN to the challenging new face recognition benchmark, the IARPA Janus Benchmark A (IJB-A) [12]. It features faces from a large number of identities in challenging real-world conditions. Because the face images were not identified automatically using a computerized face detection system, it does not have the bias inherent in such a database. We demonstrate the performance of the B-CNN model beginning from an AlexNet-style network pre-trained on ImageNet. We then show results for fine-tuning using a moderate-sized and public external database, FaceScrub [17]. We also present results with additional fine-tuning on the limited training data provided by the protocol. In each case, the fine-tuned bilinear model shows substantial improvements over the standard CNN. Finally, we demonstrate how a standard CNN pre-trained on a large face database, the recently released VGG-Face model [20], can be converted into a B-CNN without any additional feature training. This B-CNN improves upon the CNN performance on the IJB-A benchmark, achieving 89.5% rank-1 recall. version:5
arxiv-1603-08564 | Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation | http://arxiv.org/abs/1603.08564 | id:1603.08564 author:Satrajit Mukherjee, Bodhisattwa Prasad Majumder, Aritran Piplai, Swagatam Das category:cs.CV stat.ML  published:2016-03-28 summary:The paper proposes a novel Kernelized image segmentation scheme for noisy images that utilizes the concept of Smallest Univalue Segment Assimilating Nucleus (SUSAN) and incorporates spatial constraints by computing circular colour map induced weights. Fuzzy damping coefficients are obtained for each nucleus or center pixel on the basis of the corresponding weighted SUSAN area values, the weights being equal to the inverse of the number of horizontal and vertical moves required to reach a neighborhood pixel from the center pixel. These weights are used to vary the contributions of the different nuclei in the Kernel based framework. The paper also presents an edge quality metric obtained by fuzzy decision based edge candidate selection and final computation of the blurriness of the edges after their selection. The inability of existing algorithms to preserve edge information and structural details in their segmented maps necessitates the computation of the edge quality factor (EQF) for all the competing algorithms. Qualitative and quantitative analysis have been rendered with respect to state-of-the-art algorithms and for images ridden with varying types of noises. Speckle noise ridden SAR images and Rician noise ridden Magnetic Resonance Images have also been considered for evaluating the effectiveness of the proposed algorithm in extracting important segmentation information. version:1
arxiv-1603-08561 | Unsupervised Learning using Sequential Verification for Action Recognition | http://arxiv.org/abs/1603.08561 | id:1603.08561 author:Ishan Misra, C. Lawrence Zitnick, Martial Hebert category:cs.CV cs.AI cs.LG  published:2016-03-28 summary:In this paper, we consider the problem of learning a visual representation from the raw spatiotemporal signals in videos for use in action recognition. Our representation is learned without supervision from semantic labels. We formulate it as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful unsupervised representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. Our method can also be combined with supervised representations to provide an additional boost in accuracy for action recognition. Finally, to quantify its sensitivity to human pose, we show results for human pose estimation on the FLIC dataset that are competitive with approaches using significantly more supervised training data. version:1
arxiv-1603-08551 | Genetic cellular neural networks for generating three-dimensional geometry | http://arxiv.org/abs/1603.08551 | id:1603.08551 author:Hugo Martay category:cs.NE cs.GR 92B20  published:2016-03-28 summary:There are a number of ways to procedurally generate interesting three-dimensional shapes, and a method where a cellular neural network is combined with a mesh growth algorithm is presented here. The aim is to create a shape from a genetic code in such a way that a crude search can find interesting shapes. Identical neural networks are placed at each vertex of a mesh which can communicate with neural networks on neighboring vertices. The output of the neural networks determine how the mesh grows, allowing interesting shapes to be produced emergently, mimicking some of the complexity of biological organism development. Since the neural networks' parameters can be freely mutated, the approach is amenable for use in a genetic algorithm. version:1
arxiv-1603-08511 | Colorful Image Colorization | http://arxiv.org/abs/1603.08511 | id:1603.08511 author:Richard Zhang, Phillip Isola, Alexei A. Efros category:cs.CV  published:2016-03-28 summary:Given a grayscale photograph as input, this paper attacks the problem of hallucinating a {\em plausible} color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and explore using class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward operation in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test", asking human subjects to choose between a generated and ground truth color image. Our method successfully fools humans 20\% of the time, significantly higher than previous methods. version:1
arxiv-1603-08507 | Generating Visual Explanations | http://arxiv.org/abs/1603.08507 | id:1603.08507 author:Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell category:cs.CV cs.AI cs.CL  published:2016-03-28 summary:Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods. version:1
arxiv-1603-08486 | Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation | http://arxiv.org/abs/1603.08486 | id:1603.08486 author:Hoo-Chang Shin, Kirk Roberts, Le Lu, Dina Demner-Fushman, Jianhua Yao, Ronald M Summers category:cs.CV  published:2016-03-28 summary:Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normal-vs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/text dataset, to infer the joint image/text contexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account. version:1
arxiv-1603-08482 | Estimating Mixture Models via Mixtures of Polynomials | http://arxiv.org/abs/1603.08482 | id:1603.08482 author:Sida I. Wang, Arun Tejasvi Chaganty, Percy Liang category:stat.ML cs.LG  published:2016-03-28 summary:Mixture modeling is a general technique for making any simple model more expressive through weighted combination. This generality and simplicity in part explains the success of the Expectation Maximization (EM) algorithm, in which updates are easy to derive for a wide class of mixture models. However, the likelihood of a mixture model is non-convex, so EM has no known global convergence guarantees. Recently, method of moments approaches offer global guarantees for some mixture models, but they do not extend easily to the range of mixture models that exist. In this work, we present Polymom, an unifying framework based on method of moments in which estimation procedures are easily derivable, just as in EM. Polymom is applicable when the moments of a single mixture component are polynomials of the parameters. Our key observation is that the moments of the mixture model are a mixture of these polynomials, which allows us to cast estimation as a Generalized Moment Problem. We solve its relaxations using semidefinite optimization, and then extract parameters using ideas from computer algebra. This framework allows us to draw insights and apply tools from convex optimization, computer algebra and the theory of moments to study problems in statistical estimation. version:1
arxiv-1603-08474 | Deep Embedding for Spatial Role Labeling | http://arxiv.org/abs/1603.08474 | id:1603.08474 author:Oswaldo Ludwig, Xiao Liu, Parisa Kordjamshidi, Marie-Francine Moens category:cs.CL cs.CV cs.LG cs.NE  published:2016-03-28 summary:This paper introduces the visually informed embedding of word (VIEW), a continuous vector representation for a word extracted from a deep neural model trained using the Microsoft COCO data set to forecast the spatial arrangements between visual objects, given a textual description. The model is composed of a deep multilayer perceptron (MLP) stacked on the top of a Long Short Term Memory (LSTM) network, the latter being preceded by an embedding layer. The VIEW is applied to transferring multimodal background knowledge to Spatial Role Labeling (SpRL) algorithms, which recognize spatial relations between objects mentioned in the text. This work also contributes with a new method to select complementary features and a fine-tuning method for MLP that improves the $F1$ measure in classifying the words into spatial roles. The VIEW is evaluated with the Task 3 of SemEval-2013 benchmark data set, SpaceEval. version:1
arxiv-1602-06929 | Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm | http://arxiv.org/abs/1602.06929 | id:1602.06929 author:Prateek Jain, Chi Jin, Sham M. Kakade, Praneeth Netrapalli, Aaron Sidford category:cs.LG cs.DS cs.NE stat.ML  published:2016-02-22 summary:This work provides improved guarantees for streaming principle component analysis (PCA). Given $A_1, \ldots, A_n\in \mathbb{R}^{d\times d}$ sampled independently from distributions satisfying $\mathbb{E}[A_i] = \Sigma$ for $\Sigma \succeq \mathbf{0}$, this work provides an $O(d)$-space linear-time single-pass streaming algorithm for estimating the top eigenvector of $\Sigma$. The algorithm nearly matches (and in certain cases improves upon) the accuracy obtained by the standard batch method that computes top eigenvector of the empirical covariance $\frac{1}{n} \sum_{i \in [n]} A_i$ as analyzed by the matrix Bernstein inequality. Moreover, to achieve constant accuracy, our algorithm improves upon the best previous known sample complexities of streaming algorithms by either a multiplicative factor of $O(d)$ or $1/\mathrm{gap}$ where $\mathrm{gap}$ is the relative distance between the top two eigenvalues of $\Sigma$. These results are achieved through a novel analysis of the classic Oja's algorithm, one of the oldest and most popular algorithms for streaming PCA. In particular, this work shows that simply picking a random initial point $w_0$ and applying the update rule $w_{i + 1} = w_i + \eta_i A_i w_i$ suffices to accurately estimate the top eigenvector, with a suitable choice of $\eta_i$. We believe our result sheds light on how to efficiently perform streaming PCA both in theory and in practice and we hope that our analysis may serve as the basis for analyzing many variants and extensions of streaming PCA. version:2
arxiv-1510-06188 | Learning-based Compressive Subsampling | http://arxiv.org/abs/1510.06188 | id:1510.06188 author:Luca Baldassarre, Yen-Huan Li, Jonathan Scarlett, Baran Gözcü, Ilija Bogunovic, Volkan Cevher category:cs.IT cs.LG math.IT stat.ML  published:2015-10-21 summary:The problem of recovering a structured signal $\mathbf{x} \in \mathbb{C}^p$ from a set of dimensionality-reduced linear measurements $\mathbf{b} = \mathbf {A}\mathbf {x}$ arises in a variety of applications, such as medical imaging, spectroscopy, Fourier optics, and computerized tomography. Due to computational and storage complexity or physical constraints imposed by the problem, the measurement matrix $\mathbf{A} \in \mathbb{C}^{n \times p}$ is often of the form $\mathbf{A} = \mathbf{P}_{\Omega}\boldsymbol{\Psi}$ for some orthonormal basis matrix $\boldsymbol{\Psi}\in \mathbb{C}^{p \times p}$ and subsampling operator $\mathbf{P}_{\Omega}: \mathbb{C}^{p} \rightarrow \mathbb{C}^{n}$ that selects the rows indexed by $\Omega$. This raises the fundamental question of how best to choose the index set $\Omega$ in order to optimize the recovery performance. Previous approaches to addressing this question rely on non-uniform \emph{random} subsampling using application-specific knowledge of the structure of $\mathbf{x}$. In this paper, we instead take a principled learning-based approach in which a \emph{fixed} index set is chosen based on a set of training signals $\mathbf{x}_1,\dotsc,\mathbf{x}_m$. We formulate combinatorial optimization problems seeking to maximize the energy captured in these signals in an average-case or worst-case sense, and we show that these can be efficiently solved either exactly or approximately via the identification of modularity and submodularity structures. We provide both deterministic and statistical theoretical guarantees showing how the resulting measurement matrices perform on signals differing from the training signals, and we provide numerical examples showing our approach to be effective on a variety of data sets. version:3
arxiv-1509-07385 | Provable approximation properties for deep neural networks | http://arxiv.org/abs/1509.07385 | id:1509.07385 author:Uri Shaham, Alexander Cloninger, Ronald R. Coifman category:stat.ML cs.LG cs.NE  published:2015-09-24 summary:We discuss approximation of functions using deep neural nets. Given a function $f$ on a $d$-dimensional manifold $\Gamma \subset \mathbb{R}^m$, we construct a sparsely-connected depth-4 neural network and bound its error in approximating $f$. The size of the network depends on dimension and curvature of the manifold $\Gamma$, the complexity of $f$, in terms of its wavelet description, and only weakly on the ambient dimension $m$. Essentially, our network computes wavelet functions, which are computed from Rectified Linear Units (ReLU) version:3
arxiv-1603-08367 | Sparse Activity and Sparse Connectivity in Supervised Learning | http://arxiv.org/abs/1603.08367 | id:1603.08367 author:Markus Thom, Günther Palm category:cs.LG cs.CG cs.CV cs.NE  published:2016-03-28 summary:Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches. version:1
arxiv-1603-03381 | Temporally coherent 4D reconstruction of complex dynamic scenes | http://arxiv.org/abs/1603.03381 | id:1603.03381 author:Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, Adrian Hilton category:cs.CV  published:2016-03-10 summary:This paper presents an approach for reconstruction of 4D temporally coherent models of complex dynamic scenes. No prior knowledge is required of scene structure or camera calibration allowing reconstruction from multiple moving cameras. Sparse-to-dense temporal correspondence is integrated with joint multi-view segmentation and reconstruction to obtain a complete 4D representation of static and dynamic objects. Temporal coherence is exploited to overcome visual ambiguities resulting in improved reconstruction of complex scenes. Robust joint segmentation and reconstruction of dynamic objects is achieved by introducing a geodesic star convexity constraint. Comparative evaluation is performed on a variety of unstructured indoor and outdoor dynamic scenes with hand-held cameras and multiple people. This demonstrates reconstruction of complete temporally coherent 4D scene models with improved nonrigid object segmentation and shape reconstruction. version:2
arxiv-1601-00072 | GPU-Based Fuzzy C-Means Clustering Algorithm for Image Segmentation | http://arxiv.org/abs/1601.00072 | id:1601.00072 author:Mishal Almazrooie, Mogana Vadiveloo, Rosni Abdullah category:cs.DC cs.CV  published:2016-01-01 summary:In this paper, a fast and practical GPU-based implementation of Fuzzy C-Means(FCM) clustering algorithm for image segmentation is proposed. First, an extensive analysis is conducted to study the dependency among the image pixels in the algorithm for parallelization. The proposed GPU-based FCM has been tested on digital brain simulated dataset to segment white matter(WM), gray matter(GM) and cerebrospinal fluid (CSF) soft tissue regions. The execution time of the sequential FCM is 519 seconds for an image dataset with the size of 1MB. While the proposed GPU-based FCM requires only 2.33 seconds for the similar size of image dataset. An estimated 245-fold speedup is measured for the data size of 40 KB on a CUDA device that has 448 processors. version:3
arxiv-1603-08342 | Hierarchical Gaussian Mixture Model with Objects Attached to Terminal and Non-terminal Dendrogram Nodes | http://arxiv.org/abs/1603.08342 | id:1603.08342 author:Łukasz P. Olech, Mariusz Paradowski category:cs.LG cs.CV  published:2016-03-28 summary:A hierarchical clustering algorithm based on Gaussian mixture model is presented. The key difference to regular hierarchical mixture models is the ability to store objects in both terminal and nonterminal nodes. Upper levels of the hierarchy contain sparsely distributed objects, while lower levels contain densely represented ones. As it was shown by experiments, this ability helps in noise detection (modelling). Furthermore, compared to regular hierarchical mixture model, the presented method generates more compact dendrograms with higher quality measured by adopted F-measure. version:1
arxiv-1603-08328 | Continuous Stereo Matching using Local Expansion Moves | http://arxiv.org/abs/1603.08328 | id:1603.08328 author:Tatsunori Taniai, Yasuyuki Matsushita, Yoichi Sato, Takeshi Naemura category:cs.CV  published:2016-03-28 summary:We present an accurate and efficient stereo matching method using local expansion moves, a new move making scheme using graph cuts. The local expansion moves are presented as many alpha-expansions defined for small grid regions. The local expansion moves extend the traditional expansion moves by two ways: localization and spatial propagation. By localization, we use different candidate alpha-labels according to the locations of local alpha-expansions. By spatial propagation, we design our local alpha-expansions to propagate currently assigned labels for nearby regions. With this localization and spatial propagation, our method can efficiently infer Markov random field models with a huge or continuous label space using a randomized search scheme. Our local expansion move method has several advantages over previous approaches that are based on fusion moves or belief propagation; it produces submodular moves deriving a subproblem optimality; it helps find good, smooth, piecewise linear disparity maps; it is suitable for parallelization; it can use cost-volume filtering techniques for accelerating the matching cost computations. Our method is evaluated using the Middlebury stereo benchmark and shown to have the best performance in sub-pixel accuracy. version:1
arxiv-1603-08323 | Hierarchy of Groups Evaluation Using Different F-score Variants | http://arxiv.org/abs/1603.08323 | id:1603.08323 author:Michał Spytkowski, Łukasz P. Olech, Halina Kwaśnicka category:cs.CV  published:2016-03-28 summary:The paper presents a cursory examination of clustering, focusing on a rarely explored field of hierarchy of clusters. Based on this, a short discussion of clustering quality measures is presented and the F-score measure is examined more deeply. As there are no attempts to assess the quality for hierarchies of clusters, three variants of the F-Score based index are presented: classic, hierarchical and partial order. The partial order index is the authors' approach to the subject. Conducted experiments show the properties of the considered measures. In conclusions, the strong and weak sides of each variant are presented. version:1
arxiv-1603-08321 | Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention | http://arxiv.org/abs/1603.08321 | id:1603.08321 author:Linlin Chao, Jianhua Tao, Minghao Yang, Ya Li, Zhengqi Wen category:cs.CV cs.CL cs.LG  published:2016-03-28 summary:This paper focuses on two key problems for audio-visual emotion recognition in the video. One is the audio and visual streams temporal alignment for feature level fusion. The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition. The Long Short Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main classification architecture. Firstly, soft attention mechanism aligns the audio and visual streams. Secondly, seven emotion embedding vectors, which are corresponding to each classification emotion type, are added to locate the perception attentions. The locating and re-weighting process is also based on the soft attention mechanism. The experiment results on EmotiW2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques. version:1
arxiv-1511-04108 | LSTM-based Deep Learning Models for Non-factoid Answer Selection | http://arxiv.org/abs/1511.04108 | id:1511.04108 author:Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou category:cs.CL cs.LG  published:2015-11-12 summary:In this paper, we apply a general deep learning (DL) framework for the answer selection task, which does not depend on manually defined features or linguistic tools. The basic framework is to build the embeddings of questions and answers based on bidirectional long short-term memory (biLSTM) models, and measure their closeness by cosine similarity. We further extend this basic model in two directions. One direction is to define a more composite representation for questions and answers by combining convolutional neural network with the basic framework. The other direction is to utilize a simple but efficient attention mechanism in order to generate the answer representation according to the question context. Several variations of models are provided. The models are examined by two datasets, including TREC-QA and InsuranceQA. Experimental results demonstrate that the proposed models substantially outperform several strong baselines. version:4
arxiv-1603-08293 | Non-Greedy L21-Norm Maximization for Principal Component Analysis | http://arxiv.org/abs/1603.08293 | id:1603.08293 author:Feiping Nie, Heng Huang category:cs.LG  published:2016-03-28 summary:Principal Component Analysis (PCA) is one of the most important unsupervised methods to handle high-dimensional data. However, due to the high computational complexity of its eigen decomposition solution, it hard to apply PCA to the large-scale data with high dimensionality. Meanwhile, the squared L2-norm based objective makes it sensitive to data outliers. In recent research, the L1-norm maximization based PCA method was proposed for efficient computation and being robust to outliers. However, this work used a greedy strategy to solve the eigen vectors. Moreover, the L1-norm maximization based objective may not be the correct robust PCA formulation, because it loses the theoretical connection to the minimization of data reconstruction error, which is one of the most important intuitions and goals of PCA. In this paper, we propose to maximize the L21-norm based robust PCA objective, which is theoretically connected to the minimization of reconstruction error. More importantly, we propose the efficient non-greedy optimization algorithms to solve our objective and the more general L21-norm maximization problem with theoretically guaranteed convergence. Experimental results on real world data sets show the effectiveness of the proposed method for principal component analysis. version:1
arxiv-1512-08756 | Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems | http://arxiv.org/abs/1512.08756 | id:1512.08756 author:Colin Raffel, Daniel P. W. Ellis category:cs.LG cs.NE  published:2015-12-29 summary:We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic "addition" and "multiplication" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks. version:4
arxiv-1603-08262 | Towards Machine Intelligence | http://arxiv.org/abs/1603.08262 | id:1603.08262 author:Kamil Rocki category:cs.AI cs.LG cs.NE  published:2016-03-27 summary:There exists a theory of a single general-purpose learning algorithm which could explain the principles of its operation. This theory assumes that the brain has some initial rough architecture, a small library of simple innate circuits which are prewired at birth and proposes that all significant mental algorithms can be learned. Given current understanding and observations, this paper reviews and lists the ingredients of such an algorithm from both architectural and functional perspectives. version:1
arxiv-1407-4420 | Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyperspectral Images | http://arxiv.org/abs/1407.4420 | id:1407.4420 author:Fei Zhu, Paul Honeine, Maya Kallas category:cs.CV cs.IT cs.LG cs.NE math.IT stat.ML  published:2014-07-16 summary:The nonnegative matrix factorization (NMF) is widely used in signal and image processing, including bio-informatics, blind source separation and hyperspectral image analysis in remote sensing. A great challenge arises when dealing with a nonlinear formulation of the NMF. Within the framework of kernel machines, the models suggested in the literature do not allow the representation of the factorization matrices, which is a fallout of the curse of the pre-image. In this paper, we propose a novel kernel-based model for the NMF that does not suffer from the pre-image problem, by investigating the estimation of the factorization matrices directly in the input space. For different kernel functions, we describe two schemes for iterative algorithms: an additive update rule based on a gradient descent scheme and a multiplicative update rule in the same spirit as in the Lee and Seung algorithm. Within the proposed framework, we develop several extensions to incorporate constraints, including sparseness, smoothness, and spatial regularization with a total-variation-like penalty. The effectiveness of the proposed method is demonstrated with the problem of unmixing hyperspectral images, using well-known real images and results with state-of-the-art techniques. version:2
arxiv-1603-08240 | DeLight-Net: Decomposing Reflectance Maps into Specular Materials and Natural Illumination | http://arxiv.org/abs/1603.08240 | id:1603.08240 author:Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Luc Van Gool, Tinne Tuytelaars category:cs.CV  published:2016-03-27 summary:In this paper we are extracting surface reflectance and natural environmental illumination from a reflectance map, i.e. from a single 2D image of a sphere of one material under one illumination. This is a notoriously difficult problem, yet key to various re-rendering applications. With the recent advances in estimating reflectance maps from 2D images their further decomposition has become increasingly relevant. To this end, we propose a Convolutional Neural Network (CNN) architecture to reconstruct both material parameters (i.e. Phong) as well as illumination (i.e. high-resolution spherical illumination maps), that is solely trained on synthetic data. We demonstrate that decomposition of synthetic as well as real photographs of reflectance maps, both in High Dynamic Range (HDR), and, for the first time, on Low Dynamic Range (LDR) as well. Results are compared to previous approaches quantitatively as well as qualitatively in terms of re-renderings where illumination, material, view or shape are changed. version:1
arxiv-1603-04408 | U-CATCH: Using Color ATtribute of image patCHes in binary descriptors | http://arxiv.org/abs/1603.04408 | id:1603.04408 author:Ozgur Yilmaz, Alisher Abdulkhaev category:cs.CV  published:2016-03-14 summary:In this study, we propose a simple yet very effective method for extracting color information through binary feature description framework. Our method expands the dimension of binary comparisons into RGB and YCbCr spaces, showing more than 100% matching improve ment compared to non-color binary descriptors for a wide range of hard-to-match cases. The proposed method is general and can be applied to any binary descriptor to make it color sensitive. It is faster than classical binary descriptors for RGB sampling due to the abandonment of grayscale conversion and has almost identical complexity (insignificant compared to smoothing operation) for YCbCr sampling. version:2
arxiv-1603-08233 | Evolution of active categorical image classification via saccadic eye movement | http://arxiv.org/abs/1603.08233 | id:1603.08233 author:Randal S. Olson, Jason H. Moore, Christoph Adami category:cs.CV cs.LG cs.NE  published:2016-03-27 summary:Pattern recognition and classification is a central concern for modern information processing systems. In particular, one key challenge to image and video classification has been that the computational cost of image processing scales linearly with the number of pixels in the image or video. Here we present an intelligent machine (the "active categorical classifier," or ACC) that is inspired by the saccadic movements of the eye, and is capable of classifying images by selectively scanning only a portion of the image. We harness evolutionary computation to optimize the ACC on the MNIST hand-written digit classification task, and provide a proof-of-concept that the ACC works on noisy multi-class data. We further analyze the ACC and demonstrate its ability to classify images after viewing only a fraction of the pixels, and provide insight on future research paths to further improve upon the ACC presented here. version:1
arxiv-1603-08212 | Human Pose Estimation using Deep Consensus Voting | http://arxiv.org/abs/1603.08212 | id:1603.08212 author:Ita Lifshitz, Ethan Fetaya, Shimon Ullman category:cs.CV cs.LG  published:2016-03-27 summary:In this paper we consider the problem of human pose estimation from a single still image. We propose a novel approach where each location in the image votes for the position of each keypoint using a convolutional neural net. The voting scheme allows us to utilize information from the whole image, rather than rely on a sparse set of keypoint locations. Using dense, multi-target votes, not only produces good keypoint predictions, but also enables us to compute image-dependent joint keypoint probabilities by looking at consensus voting. This differs from most previous methods where joint probabilities are learned from relative keypoint locations and are independent of the image. We finally combine the keypoints votes and joint probabilities in order to identify the optimal pose configuration. We show our competitive performance on the MPII Human Pose and Leeds Sports Pose datasets. version:1
arxiv-1509-06720 | A Dual-Source Approach for 3D Pose Estimation from a Single Image | http://arxiv.org/abs/1509.06720 | id:1509.06720 author:Hashim Yasin, Umar Iqbal, Björn Krüger, Andreas Weber, Juergen Gall category:cs.CV  published:2015-09-22 summary:One major challenge for 3D pose estimation from a single RGB image is the acquisition of sufficient training data. In particular, collecting large amounts of training data that contain unconstrained images and are annotated with accurate 3D poses is infeasible. We therefore propose to use two independent training sources. The first source consists of images with annotated 2D poses and the second source consists of accurate 3D motion capture data. To integrate both sources, we propose a dual-source approach that combines 2D pose estimation with efficient and robust 3D pose retrieval. In our experiments, we show that our approach achieves state-of-the-art results and is even competitive when the skeleton structure of the two sources differ substantially. version:2
arxiv-1603-08182 | 3DMatch: Learning the Matching of Local 3D Geometry in Range Scans | http://arxiv.org/abs/1603.08182 | id:1603.08182 author:Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao category:cs.CV  published:2016-03-27 summary:Establishing correspondences between 3D geometries is essential to a large variety of graphics and vision applications, including 3D reconstruction, localization, and shape matching. Despite significant progress, geometric matching on real-world 3D data is still a challenging task due to the noisy, low-resolution, and incomplete nature of scanning data. These difficulties limit the performance of current state-of-art methods which are typically based on histograms over geometric properties. In this paper, we introduce 3DMatch, a data-driven local feature learner that jointly learns a geometric feature representation and an associated metric function from a large collection of real-world scanning data. We represent 3D geometry using accumulated distance fields around key-point locations. This representation is suited to handle noisy and partial scanning data, and concurrently supports deep learning with convolutional neural networks directly in 3D. To train the networks, we propose a way to automatically generate correspondence labels for deep learning by leveraging existing RGB-D reconstruction algorithms. In our results, we demonstrate that we are able to outperform state-of-the-art approaches by a significant margin. In addition, we show the robustness of our descriptor in a purely geometric sparse bundle adjustment pipeline for 3D reconstruction. version:1
arxiv-1603-00968 | MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification | http://arxiv.org/abs/1603.00968 | id:1603.00968 author:Ye Zhang, Stephen Roller, Byron Wallace category:cs.CL  published:2016-03-03 summary:We introduce a novel, simple convolution neural network (CNN) architecture - multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of word embeddings for sentence classification. MGNC-CNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a final feature vector. We then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is flexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models. version:2
arxiv-1603-08163 | Regularization Parameter Selection for a Bayesian Multi-Level Group Lasso Regression Model with Application to Imaging Genomics | http://arxiv.org/abs/1603.08163 | id:1603.08163 author:Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance category:stat.ML stat.AP stat.CO  published:2016-03-27 summary:We investigate the choice of tuning parameters for a Bayesian multi-level group lasso model developed for the joint analysis of neuroimaging and genetic data. The regression model we consider relates multivariate phenotypes consisting of brain summary measures (volumetric and cortical thickness values) to single nucleotide polymorphism (SNPs) data and imposes penalization at two nested levels, the first corresponding to genes and the second corresponding to SNPs. Associated with each level in the penalty is a tuning parameter which corresponds to a hyperparameter in the hierarchical Bayesian formulation. Following previous work on Bayesian lassos we consider the estimation of tuning parameters through either hierarchical Bayes based on hyperpriors and Gibbs sampling or through empirical Bayes based on maximizing the marginal likelihood using a Monte Carlo EM algorithm. For the specific model under consideration we find that these approaches can lead to severe overshrinkage of the regression parameter estimates in the high-dimensional setting or when the genetic effects are weak. We demonstrate these problems through simulation examples and study an approximation to the marginal likelihood which sheds light on the cause of this problem. We then suggest an alternative approach based on the widely applicable information criterion (WAIC), an asymptotic approximation to leave-one-out cross-validation that can be computed conveniently within an MCMC framework. version:1
arxiv-1603-08161 | VolumeDeform: Real-time Volumetric Non-rigid Reconstruction | http://arxiv.org/abs/1603.08161 | id:1603.08161 author:Matthias Innmann, Michael Zollhöfer, Matthias Nießner, Christian Theobalt, Marc Stamminger category:cs.CV  published:2016-03-27 summary:We present a novel approach for the reconstruction of dynamic geometric shapes using a single hand-held consumer-grade RGB-D sensor at real-time rates. Our method does not require a pre-defined shape template to start with and builds up the scene model from scratch during the scanning process. Geometry and motion are parameterized in a unified manner by a volumetric representation that encodes a distance field of the surface geometry as well as the non-rigid space deformation. Motion tracking is based on a set of extracted sparse color features in combination with a dense depth-based constraint formulation. This enables accurate tracking and drastically reduces drift inherent to standard model-to-depth alignment. We cast finding the optimal deformation of space as a non-linear regularized variational optimization problem by enforcing local smoothness and proximity to the input constraints. The problem is tackled in real-time at the camera's capture rate using a data-parallel flip-flop optimization strategy. Our results demonstrate robust tracking even for fast motion and scenes that lack geometric features. version:1
arxiv-1603-08155 | Perceptual Losses for Real-Time Style Transfer and Super-Resolution | http://arxiv.org/abs/1603.08155 | id:1603.08155 author:Justin Johnson, Alexandre Alahi, Li Fei-Fei category:cs.CV cs.LG  published:2016-03-27 summary:We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \emph{per-pixel} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \emph{perceptual} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results. version:1
arxiv-1508-00842 | Perceptron like Algorithms for Online Learning to Rank | http://arxiv.org/abs/1508.00842 | id:1508.00842 author:Sougata Chaudhuri, Ambuj Tewari category:cs.LG stat.ML  published:2015-08-04 summary:Perceptron is a classic online algorithm for learning a classification function. In this paper, we provide a novel extension of the perceptron algorithm to the learning to rank problem in information retrieval. We consider popular listwise performance measures such as Normalized Discounted Cumulative Gain (NDCG) and Average Precision (AP). A modern perspective on perceptron for classification is that it is simply an instance of online gradient descent (OGD), during mistake rounds, using the hinge loss function. Motivated by this interpretation, we propose a novel family of listwise, large margin ranking surrogates. Members of this family can be thought of as analogs of the hinge loss. Exploiting a certain self-bounding property of the proposed family, we provide a guarantee on the cumulative NDCG (or AP) induced loss incurred by our perceptron-like algorithm. We show that, if there exists a perfect oracle ranker which can correctly rank each instance in an online sequence of ranking data, with some margin, the cumulative loss of perceptron algorithm on that sequence is bounded by a constant, irrespective of the length of the sequence. This result is reminiscent of Novikoff's convergence theorem for the classification perceptron. Moreover, we prove a lower bound on the cumulative loss achievable by any deterministic algorithm, under the assumption of existence of perfect oracle ranker. The lower bound shows that our perceptron bound is not tight, and we propose another, \emph{purely online}, algorithm which achieves the lower bound. We provide empirical results on simulated and large commercial datasets to corroborate our theoretical results. version:3
arxiv-1603-08152 | How useful is photo-realistic rendering for visual learning? | http://arxiv.org/abs/1603.08152 | id:1603.08152 author:Yair Movshovitz-Attias, Takeo Kanade, Yaser Sheikh category:cs.CV  published:2016-03-26 summary:Data seems cheap to get, and in many ways it is, but the process of creating a high quality labeled dataset from a mass of data is time-consuming and expensive. With the advent of rich 3D repositories, photo-realistic rendering systems offer the opportunity to provide nearly limitless data. Yet, their primary value for visual learning may be the quality of the data they can provide rather than the quantity. Rendering engines offer the promise of perfect labels in addition to the data: what the precise camera pose is; what the precise lighting location, temperature, and distribution is; what the geometry of the object is. In this work we focus on semi-automating dataset creation through use of synthetic data and apply this method to an important task -- object viewpoint estimation. Using state-of-the-art rendering software we generate a large labeled dataset of cars rendered densely in viewpoint space. We investigate the effect of rendering parameters on estimation performance and show realism is important. We show that generalizing from synthetic data is not harder than the domain adaptation required between two real-image datasets and that combining synthetic images with a small amount of real data improves estimation accuracy. version:1
arxiv-1402-0240 | Graph Cuts with Interacting Edge Costs - Examples, Approximations, and Algorithms | http://arxiv.org/abs/1402.0240 | id:1402.0240 author:Stefanie Jegelka, Jeff Bilmes category:cs.DS cs.CV cs.DM math.OC  published:2014-02-02 summary:We study an extension of the classical graph cut problem, wherein we replace the modular (sum of edge weights) cost function by a submodular set function defined over graph edges. Special cases of this problem have appeared in different applications in signal processing, machine learning, and computer vision. In this paper, we connect these applications via the generic formulation of "cooperative graph cuts", for which we study complexity, algorithms, and connections to polymatroidal network flows. Finally, we compare the proposed algorithms empirically. version:4
arxiv-1603-08150 | Data-Driven Dynamic Decision Models | http://arxiv.org/abs/1603.08150 | id:1603.08150 author:John J. Nay, Jonathan M. Gilligan category:stat.ML cs.GT cs.MA cs.NE  published:2016-03-26 summary:This article outlines a method for automatically generating models of dynamic decision-making that both have strong predictive power and are interpretable in human terms. This is useful for designing empirically grounded agent-based simulations and for gaining direct insight into observed dynamic processes. We use an efficient model representation and a genetic algorithm-based estimation process to generate simple approximations that explain most of the structure of complex stochastic processes. This method, implemented in C++ and R, scales well to large data sets. We apply our methods to empirical data from human subjects game experiments and international relations. We also demonstrate the method's ability to recover known data-generating processes by simulating data with agent-based models and correctly deriving the underlying decision models for multiple agent models and degrees of stochasticity. version:1
arxiv-1603-08146 | A Draft Memory Model on Spiking Neural Assemblies | http://arxiv.org/abs/1603.08146 | id:1603.08146 author:João Ranhel, João H. Albuquerque, Bruno P. M. Azevedo, Nathalia M. Cunha, Pedro J. Ishimaru category:cs.NE  published:2016-03-26 summary:A draft memory model (DM) for neural networks with spike propagation delay (SNNwD) is described. Novelty in this approach are that the DM learns immediately, with stimuli presented once, without synaptic weight changes, and without external learning algorithm. Basal on this model is to trap spikes within neural loops. In order to construct the DM we developed two functional blocks, also described herein. The decoder block receives input from a single spikes source and connect it to one among many outputs. The selector block operates in the opposite direction, receiving many spikes sources and connecting one of them to a single output. We realized conceptual proofs by testing the DM in the prime numbers classifying task. This activation-based memory can be used as immediate and short-term memory. version:1
arxiv-1508-03337 | A Randomized Rounding Algorithm for Sparse PCA | http://arxiv.org/abs/1508.03337 | id:1508.03337 author:Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou, Petros Drineas category:cs.DS cs.LG stat.ML  published:2015-08-13 summary:We present and analyze a simple, two-step algorithm to approximate the optimal solution of the sparse PCA problem. Our approach first solves a L1 penalized version of the NP-hard sparse PCA optimization problem and then uses a randomized rounding strategy to sparsify the resulting dense solution. Our main theoretical result guarantees an additive error approximation and provides a tradeoff between sparsity and accuracy. Our experimental evaluation indicates that our approach is competitive in practice, even compared to state-of-the-art toolboxes such as Spasm. version:4
arxiv-1603-08124 | Video Interpolation using Optical Flow and Laplacian Smoothness | http://arxiv.org/abs/1603.08124 | id:1603.08124 author:Wenbin Li, Darren Cosker category:cs.CV  published:2016-03-26 summary:Non-rigid video interpolation is a common computer vision task. In this paper we present an optical flow approach which adopts a Laplacian Cotangent Mesh constraint to enhance the local smoothness. Similar to Li et al., our approach adopts a mesh to the image with a resolution up to one vertex per pixel and uses angle constraints to ensure sensible local deformations between image pairs. The Laplacian Mesh constraints are expressed wholly inside the optical flow optimization, and can be applied in a straightforward manner to a wide range of image tracking and registration problems. We evaluate our approach by testing on several benchmark datasets, including the Middlebury and Garg et al. datasets. In addition, we show application of our method for constructing 3D Morphable Facial Models from dynamic 3D data. version:1
arxiv-1603-08120 | Dense Nonrigid Ground Truth for Optical Flow in Real-World Scenes | http://arxiv.org/abs/1603.08120 | id:1603.08120 author:Wenbin Li, Darren Cosker, Zhihan Lv, Matthew Brown category:cs.CV  published:2016-03-26 summary:In this paper we present the first ground truth dataset of nonrigidly deforming real-world scenes (both long and short video sequences) in order to quantitatively evaluate RGB based tracking and registration methods. To construct ground truth for the RGB sequences, we simultaneously capture Near-Infrared (NIR) image sequences where dense markers - visible only in NIR - represent ground truth positions. This allows for comparison with automatically tracked RGB positions and the formation of error metrics. Most previous datasets containing nonrigidly deforming sequences are based on synthetic data. Our capture protocol enables us to acquire real-world deforming objects with realistic photometric effects - such as blur and illumination change - as well as occlusion and complex deformations. A public evaluation website is constructed to allow for ranking of RGB image based optical flow and other dense tracking algorithms, with various statistical measures. Furthermore, we present the first RGB-NIR multispectral optical flow model allowing for energy optimization by combining information from both the RGB and the complementary NIR channels. In our experiments we evaluate eight existing RGB based optical flow methods on our new dataset. We also evaluate our multispectral optical flow algorithm in real-world scenes by varying the input channels across RGB, NIR and RGB-NIR. version:1
arxiv-1603-08113 | Reconstructing undirected graphs from eigenspaces | http://arxiv.org/abs/1603.08113 | id:1603.08113 author:Yohann De Castro, Thibault Espinasse, Paul Rochet category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH  published:2016-03-26 summary:In this paper, we aim at recovering an undirected weighted graph of $N$ vertices from the knowledge of a perturbed version of the eigenspaces of its adjacency matrix $W$. Our approach is based on minimizing a cost function given by the Frobenius norm of the commutator $\mathsf{A} \mathsf{B}-\mathsf{B} \mathsf{A}$ between symmetric matrices $\mathsf{A}$ and $\mathsf{B}$. In the Erd\H{o}s-R\'enyi model with no self-loops, we show that identifiability (i.e. the ability to reconstruct $W$ from the knowledge of its eigenspaces) follows a sharp phase transition on the expected number of edges with threshold function $N\log N/2$. Given an estimation of the eigenspaces based on a $n$-sample, we provide backward-type support selection procedures from theoretical and practical point of views. In particular, deleting an edge from the active support, our study unveils that the empirical contrast is of the order of $\mathcal O(1/n)$ when we overestimate the true support and lower bounded by a positive constant when the estimated support is smaller than the true support. This feature leads to a powerful practical support estimation procedure when properly thresholding the empirical contrast. Simulated and real life numerical experiments assert our new methodology. version:1
arxiv-1603-08109 | Fast and Provably Accurate Bilateral Filtering | http://arxiv.org/abs/1603.08109 | id:1603.08109 author:Kunal N. Chaudhury, Swapnil D. Dabhade category:cs.CV  published:2016-03-26 summary:The bilateral filter is a non-linear filter that uses a range filter along with a spatial filter to perform edge-preserving smoothing of images. A direct computation of the bilateral filter requires $O(S)$ operations per pixel, where $S$ is the size of the support of the spatial filter. In this paper, we present a fast and provably accurate algorithm for approximating the bilateral filter when the range kernel is Gaussian. In particular, for box and Gaussian spatial filters, the proposed algorithm can cut down the complexity to $O(1)$ per pixel for any arbitrary $S$. The algorithm has a simple implementation involving $N+1$ spatial filterings, where $N$ is the approximation order. We give a detailed analysis of the filtering accuracy that can be achieved by the proposed approximation in relation to the target bilateral filter. This allows us to to estimate the order $N$ required to obtain a given accuracy. We also present comprehensive numerical results to demonstrate that the proposed algorithm is competitive with state-of-the-art methods in terms of speed and accuracy. version:1
arxiv-1603-08108 | Support Driven Wavelet Frame-based Image Deblurring | http://arxiv.org/abs/1603.08108 | id:1603.08108 author:Liangtian He, Yilun Wang, Zhaoyin Xiang category:cs.CV 90C26 I.4.3  published:2016-03-26 summary:The wavelet frame systems have been playing an active role in image restoration and many other image processing fields over the past decades, owing to the good capability of sparsely approximating piece-wise smooth functions such as images. In this paper, we propose a novel wavelet frame based sparse recovery model called \textit{Support Driven Sparse Regularization} (SDSR) for image deblurring, where the partial support information of frame coefficients is attained via a self-learning strategy and exploited via the proposed truncated $\ell_0$ regularization. Moreover, the state-of-the-art image restoration methods can be naturally incorporated into our proposed wavelet frame based sparse recovery framework. In particular, in order to achieve reliable support estimation of the frame coefficients, we make use of the state-of-the-art image restoration result such as that from the IDD-BM3D method as the initial reference image for support estimation. Our extensive experimental results have shown convincing improvements over existing state-of-the-art deblurring methods. version:1
arxiv-1603-08105 | Unsupervised Domain Adaptation in the Wild: Dealing with Asymmetric Label Sets | http://arxiv.org/abs/1603.08105 | id:1603.08105 author:Ayush Mittal, Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV  published:2016-03-26 summary:The goal of domain adaptation is to adapt models learned on a source domain to a particular target domain. Most methods for unsupervised domain adaptation proposed in the literature to date, assume that the set of classes present in the target domain is identical to the set of classes present in the source domain. This is a restrictive assumption that limits the practical applicability of unsupervised domain adaptation techniques in real world settings ("in the wild"). Therefore, we relax this constraint and propose a technique that allows the set of target classes to be a subset of the source classes. This way, large publicly available annotated datasets with a wide variety of classes can be used as source, even if the actual set of classes in target can be more limited and, maybe most importantly, unknown beforehand. To this end, we propose an algorithm that orders a set of source subspaces that are relevant to the target classification problem. Our method then chooses a restricted set from this ordered set of source subspaces. As an extension, even starting from multiple source datasets with varied sets of categories, this method automatically selects an appropriate subset of source categories relevant to a target dataset. Empirical analysis on a number of source and target domain datasets shows that restricting the source subspace to only a subset of categories does indeed substantially improve the eventual target classification accuracy over the baseline that considers all source classes. version:1
arxiv-1603-03183 | Exploring Context with Deep Structured models for Semantic Segmentation | http://arxiv.org/abs/1603.03183 | id:1603.03183 author:Guosheng Lin, Chunhua Shen, Anton van den Hengel, Ian Reid category:cs.CV  published:2016-03-10 summary:State-of-the-art semantic image segmentation methods are mostly based on training deep convolutional neural networks (CNNs). In this work, we proffer to improve semantic segmentation with the use of contextual information. In particular, we explore `patch-patch' context and `patch-background' context in deep CNNs. We formulate deep structured models by combining CNNs and Conditional Random Fields (CRFs) for learning the patch-patch context between image regions. Specifically, we formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied in order to avoid repeated expensive CRF inference during the course of back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image inputs and sliding pyramid pooling is very effective for improving performance. We perform comprehensive evaluation of the proposed method. We achieve new state-of-the-art performance on a number of challenging semantic segmentation datasets including $NYUDv2$, $PASCAL$-$VOC2012$, $Cityscapes$, $PASCAL$-$Context$, $SUN$-$RGBD$, $SIFT$-$flow$, and $KITTI$ datasets. Particularly, we report an intersection-over-union score of $77.8$ on the $PASCAL$-$VOC2012$ dataset. version:2
arxiv-1603-08095 | Blind signal separation and identification of mixtures of images | http://arxiv.org/abs/1603.08095 | id:1603.08095 author:Felipe P. do Carmo, Joaquim T. de Assis, Vania V. Estrela, Alessandra M. Coelho category:cs.CV  published:2016-03-26 summary:In this paper, a fresh procedure to handle image mixtures by means of blind signal separation relying on a combination of second order and higher order statistics techniques are introduced. The problem of blind signal separation is reassigned to the wavelet domain. The key idea behind this method is that the image mixture can be decomposed into the sum of uncorrelated and/or independent sub-bands using wavelet transform. Initially, the observed image is pre-whitened in the space domain. Afterwards, an initial separation matrix is estimated from the second order statistics de-correlation model in the wavelet domain. Later, this matrix will be used as an initial separation matrix for the higher order statistics stage in order to find the best separation matrix. The suggested algorithm was tested using natural images.Experiments have confirmed that the use of the proposed process provides promising outcomes in identifying an image from noisy mixtures of images. version:1
arxiv-1603-08092 | Learning Hough Regression Models via Bridge Partial Least Squares for Object Detection | http://arxiv.org/abs/1603.08092 | id:1603.08092 author:Jianyu Tang, Hanzi Wang, Yan Yan category:cs.CV  published:2016-03-26 summary:Popular Hough Transform-based object detection approaches usually construct an appearance codebook by clustering local image features. However, how to choose appropriate values for the parameters used in the clustering step remains an open problem. Moreover, some popular histogram features extracted from overlapping image blocks may cause a high degree of redundancy and multicollinearity. In this paper, we propose a novel Hough Transform-based object detection approach. First, to address the above issues, we exploit a Bridge Partial Least Squares (BPLS) technique to establish context-encoded Hough Regression Models (HRMs), which are linear regression models that cast probabilistic Hough votes to predict object locations. BPLS is an efficient variant of Partial Least Squares (PLS). PLS-based regression techniques (including BPLS) can reduce the redundancy and eliminate the multicollinearity of a feature set. And the appropriate value of the only parameter used in PLS (i.e., the number of latent components) can be determined by using a cross-validation procedure. Second, to efficiently handle object scale changes, we propose a novel multi-scale voting scheme. In this scheme, multiple Hough images corresponding to multiple object scales can be obtained simultaneously. Third, an object in a test image may correspond to multiple true and false positive hypotheses at different scales. Based on the proposed multi-scale voting scheme, a principled strategy is proposed to fuse hypotheses to reduce false positives by evaluating normalized pointwise mutual information between hypotheses. In the experiments, we also compare the proposed HRM approach with its several variants to evaluate the influences of its components on its performance. Experimental results show that the proposed HRM approach has achieved desirable performances on popular benchmark datasets. version:1
arxiv-1603-08091 | Measuring Book Impact Based on the Multi-granularity Online Review Mining | http://arxiv.org/abs/1603.08091 | id:1603.08091 author:Qingqing Zhou, Chengzhi Zhang, Star X. Zhao, Bikun Chen category:cs.DL cs.CL  published:2016-03-26 summary:As with articles and journals, the customary methods for measuring books' academic impact mainly involve citations, which is easy but limited to interrogating traditional citation databases and scholarly book reviews, Researchers have attempted to use other metrics, such as Google Books, libcitation, and publisher prestige. However, these approaches lack content-level information and cannot determine the citation intentions of users. Meanwhile, the abundant online review resources concerning academic books can be used to mine deeper information and content utilizing altmetric perspectives. In this study, we measure the impacts of academic books by multi-granularity mining online reviews, and we identify factors that affect a book's impact. First, online reviews of a sample of academic books on Amazon.cn are crawled and processed. Then, multi-granularity review mining is conducted to identify review sentiment polarities and aspects' sentiment values. Lastly, the numbers of positive reviews and negative reviews, aspect sentiment values, star values, and information regarding helpfulness are integrated via the entropy method, and lead to the calculation of the final book impact scores. The results of a correlation analysis of book impact scores obtained via our method versus traditional book citations show that, although there are substantial differences between subject areas, online book reviews tend to reflect the academic impact. Thus, we infer that online reviews represent a promising source for mining book impact within the altmetric perspective and at the multi-granularity content level. Moreover, our proposed method might also be a means by which to measure other books besides academic publications. version:1
arxiv-1603-08089 | Online shopping behavior study based on multi-granularity opinion mining: China vs. America | http://arxiv.org/abs/1603.08089 | id:1603.08089 author:Qingqing Zhou, Rui Xia, Chengzhi Zhang category:cs.CY cs.CL cs.HC  published:2016-03-26 summary:With the development of e-commerce, many products are now being sold worldwide, and manufacturers are eager to obtain a better understanding of customer behavior in various regions. To achieve this goal, most previous efforts have focused mainly on questionnaires, which are time-consuming and costly. The tremendous volume of product reviews on e-commerce websites has seen a new trend emerge, whereby manufacturers attempt to understand user preferences by analyzing online reviews. Following this trend, this paper addresses the problem of studying customer behavior by exploiting recently developed opinion mining techniques. This work is novel for three reasons. First, questionnaire-based investigation is automatically enabled by employing algorithms for template-based question generation and opinion mining-based answer extraction. Using this system, manufacturers are able to obtain reports of customer behavior featuring a much larger sample size, more direct information, a higher degree of automation, and a lower cost. Second, international customer behavior study is made easier by integrating tools for multilingual opinion mining. Third, this is the ?rst time an automatic questionnaire investigation has been conducted to compare customer behavior in China and America, where product reviews are written and read in Chinese and English, respectively. Our study on digital cameras, smartphones, and tablet computers yields three ?ndings. First, Chinese customers follow the Doctrine of the Mean, and often use euphemistic expressions, while American customers express their opinions more directly. Second, Chinese customers care more about general feelings, while American customers pay more attention to product details. Third, Chinese customers focus on external features, while American customers care more about the internal features of products. version:1
arxiv-1603-08081 | On Fast Bilateral Filtering using Fourier Kernels | http://arxiv.org/abs/1603.08081 | id:1603.08081 author:Sanjay Ghosh, Kunal N. Chaudhury category:cs.CV  published:2016-03-26 summary:It was demonstrated in earlier work that, by approximating its range kernel using shiftable functions, the non-linear bilateral filter can be computed using a series of fast convolutions. Previous approaches based on shiftable approximation have, however, been restricted to Gaussian range kernels. In this work, we propose a novel approximation that can be applied to any range kernel, provided it has a pointwise-convergent Fourier series. More specifically, we propose to approximate the Gaussian range kernel of the bilateral filter using a Fourier basis, where the coefficients of the basis are obtained by solving a series of least-squares problems. The coefficients can be efficiently computed using a recursive form of the QR decomposition. By controlling the cardinality of the Fourier basis, we can obtain a good tradeoff between the run-time and the filtering accuracy. In particular, we are able to guarantee sub-pixel accuracy for the overall filtering, which is not provided by most existing methods for fast bilateral filtering. We present simulation results to demonstrate the speed and accuracy of the proposed algorithm. version:1
arxiv-1603-08079 | Do You See What I Mean? Visual Resolution of Linguistic Ambiguities | http://arxiv.org/abs/1603.08079 | id:1603.08079 author:Yevgeni Berzak, Andrei Barbu, Daniel Harari, Boris Katz, Shimon Ullman category:cs.CV cs.AI cs.CL  published:2016-03-26 summary:Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types. version:1
arxiv-1603-08071 | Classification of Large-Scale Fundus Image Data Sets: A Cloud-Computing Framework | http://arxiv.org/abs/1603.08071 | id:1603.08071 author:Sohini Roychowdhury category:cs.CV  published:2016-03-26 summary:Large medical image data sets with high dimensionality require substantial amount of computation time for data creation and data processing. This paper presents a novel generalized method that finds optimal image-based feature sets that reduce computational time complexity while maximizing overall classification accuracy for detection of diabetic retinopathy (DR). First, region-based and pixel-based features are extracted from fundus images for classification of DR lesions and vessel-like structures. Next, feature ranking strategies are used to distinguish the optimal classification feature sets. DR lesion and vessel classification accuracies are computed using the boosted decision tree and decision forest classifiers in the Microsoft Azure Machine Learning Studio platform, respectively. For images from the DIARETDB1 data set, 40 of its highest-ranked features are used to classify four DR lesion types with an average classification accuracy of 90.1% in 792 seconds. Also, for classification of red lesion regions and hemorrhages from microaneurysms, accuracies of 85% and 72% are observed, respectively. For images from STARE data set, 40 high-ranked features can classify minor blood vessels with an accuracy of 83.5% in 326 seconds. Such cloud-based fundus image analysis systems can significantly enhance the borderline classification performances in automated screening systems. version:1
arxiv-1603-08070 | A generalized flow for multi-class and binary classification tasks: An Azure ML approach | http://arxiv.org/abs/1603.08070 | id:1603.08070 author:Matthew Bihis, Sohini Roychowdhury category:cs.CV  published:2016-03-26 summary:The constant growth in the present day real-world databases pose computational challenges for a single computer. Cloud-based platforms, on the other hand, are capable of handling large volumes of information manipulation tasks, thereby necessitating their use for large real-world data set computations. This work focuses on creating a novel Generalized Flow within the cloud-based computing platform: Microsoft Azure Machine Learning Studio (MAMLS) that accepts multi-class and binary classification data sets alike and processes them to maximize the overall classification accuracy. First, each data set is split into training and testing data sets, respectively. Then, linear and nonlinear classification model parameters are estimated using the training data set. Data dimensionality reduction is then performed to maximize classification accuracy. For multi-class data sets, data centric information is used to further improve overall classification accuracy by reducing the multi-class classification to a series of hierarchical binary classification tasks. Finally, the performance of optimized classification model thus achieved is evaluated and scored on the testing data set. The classification characteristics of the proposed flow are comparatively evaluated on 3 public data sets and a local data set with respect to existing state-of-the-art methods. On the 3 public data sets, the proposed flow achieves 78-97.5% classification accuracy. Also, the local data set, created using the information regarding presence of Diabetic Retinopathy lesions in fundus images, results in 85.3-95.7% average classification accuracy, which is higher than the existing methods. Thus, the proposed generalized flow can be useful for a wide range of application-oriented "big data sets". version:1
arxiv-1603-08067 | Recognizing Car Fluents from Video | http://arxiv.org/abs/1603.08067 | id:1603.08067 author:Bo Li, Tianfu Wu, Caiming Xiong, Song-Chun Zhu category:cs.CV  published:2016-03-26 summary:Physical fluents, a term originally used by Newton [40], refers to time-varying object states in dynamic scenes. In this paper, we are interested in inferring the fluents of vehicles from video. For example, a door (hood, trunk) is open or closed through various actions, light is blinking to turn. Recognizing these fluents has broad applications, yet have received scant attention in the computer vision literature. Car fluent recognition entails a unified framework for car detection, car part localization and part status recognition, which is made difficult by large structural and appearance variations, low resolutions and occlusions. This paper learns a spatial-temporal And-Or hierarchical model to represent car fluents. The learning of this model is formulated under the latent structural SVM framework. Since there are no publicly related dataset, we collect and annotate a car fluent dataset consisting of car videos with diverse fluents. In experiments, the proposed method outperforms several highly related baseline methods in terms of car fluent recognition and car part localization. version:1
arxiv-1603-08048 | "Did I Say Something Wrong?" A Word-Level Analysis of Wikipedia Articles for Deletion Discussions | http://arxiv.org/abs/1603.08048 | id:1603.08048 author:Michael Ruster category:cs.CL cs.SI stat.ML  published:2016-03-25 summary:This thesis focuses on gaining linguistic insights into textual discussions on a word level. It was of special interest to distinguish messages that constructively contribute to a discussion from those that are detrimental to them. Thereby, we wanted to determine whether "I"- and "You"-messages are indicators for either of the two discussion styles. These messages are nowadays often used in guidelines for successful communication. Although their effects have been successfully evaluated multiple times, a large-scale analysis has never been conducted. Thus, we used Wikipedia Articles for Deletion (short: AfD) discussions together with the records of blocked users and developed a fully automated creation of an annotated data set. In this data set, messages were labelled either constructive or disruptive. We applied binary classifiers to the data to determine characteristic words for both discussion styles. Thereby, we also investigated whether function words like pronouns and conjunctions play an important role in distinguishing the two. We found that "You"-messages were a strong indicator for disruptive messages which matches their attributed effects on communication. However, we found "I"-messages to be indicative for disruptive messages as well which is contrary to their attributed effects. The importance of function words could neither be confirmed nor refuted. Other characteristic words for either communication style were not found. Yet, the results suggest that a different model might represent disruptive and constructive messages in textual discussions better. version:1
arxiv-1409-5191 | Hamiltonian Monte Carlo Without Detailed Balance | http://arxiv.org/abs/1409.5191 | id:1409.5191 author:Jascha Sohl-Dickstein, Mayur Mudigonda, Michael R. DeWeese category:stat.CO stat.ML  published:2014-09-18 summary:We present a method for performing Hamiltonian Monte Carlo that largely eliminates sample rejection for typical hyperparameters. In situations that would normally lead to rejection, instead a longer trajectory is computed until a new state is reached that can be accepted. This is achieved using Markov chain transitions that satisfy the fixed point equation, but do not satisfy detailed balance. The resulting algorithm significantly suppresses the random walk behavior and wasted function evaluations that are typically the consequence of update rejection. We demonstrate a greater than factor of two improvement in mixing time on three test problems. We release the source code as Python and MATLAB packages. version:5
arxiv-1603-08039 | An Empirical Study of Dimensional Reduction Techniques for Facial Action Units Detection | http://arxiv.org/abs/1603.08039 | id:1603.08039 author:Zhuo Hui, Wen-Sheng Chu category:cs.CV  published:2016-03-25 summary:Biologically inspired features, such as Gabor filters, result in very high dimensional measurement. Does reducing the dimensionality of the feature space afford advantages beyond computational efficiency? Do some approaches to dimensionality reduction (DR) yield improved action unit detection? To answer these questions, we compared DR approaches in two relatively large databases of spontaneous facial behavior (45 participants in total with over 2 minutes of FACS-coded video per participant). Facial features were tracked and aligned using active appearance models (AAM). SIFT and Gabor features were extracted from local facial regions. We compared linear (PCA and KPCA), manifold (LPP and LLE), supervised (LDA and KDA) and hybrid approaches (LSDA) to DR with respect to AU detection. For further comparison, a no-DR control condition was included as well. Linear support vector machine classifiers with independent train and test sets were used for AU detection. AU detection was quantified using area under the ROC curve and F1. Baseline results for PCA with Gabor features were comparable with previous research. With some notable exceptions, DR improved AU detection relative to no-DR. Locality embedding approaches proved vulnerable to \emph{out-of-sample} problems. Gradient-based SIFT lead to better AU detection than the filter-based Gabor features. For area under the curve, few differences were found between linear and other DR approaches. For F1, results were mixed. For both metrics, the pattern of results varied among action units. These findings suggest that action unit detection may be optimized by using specific DR for specific action units. PCA and LDA were the most efficient approaches; KDA was the least efficient. version:1
