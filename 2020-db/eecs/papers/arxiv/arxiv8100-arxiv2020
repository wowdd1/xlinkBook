arxiv-1412-5764 | Image Dynamic Range Enhancement in the Context of Logarithmic Models | http://arxiv.org/abs/1412.5764 | id:1412.5764 author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV  published:2014-12-18 summary:Images of a scene observed under a variable illumination or with a variable optical aperture are not identical. Does a privileged representant exist? In which mathematical context? How to obtain it? The authors answer to such questions in the context of logarithmic models for images. After a short presentation of the model, the paper presents two image transforms: one performs an optimal enhancement of the dynamic range, and the other does the same for the mean dynamic range. Experimental results are shown. version:1
arxiv-1411-6387 | Deep Convolutional Neural Fields for Depth Estimation from a Single Image | http://arxiv.org/abs/1411.6387 | id:1411.6387 author:Fayao Liu, Chunhua Shen, Guosheng Lin category:cs.CV  published:2014-11-24 summary:We consider the problem of depth estimation from a single monocular image in this work. It is a challenging task as no reliable depth cues are available, e.g., stereo correspondences, motions, etc. Previous efforts have been focusing on exploiting geometric priors or additional sources of information, with all using hand-crafted features. Recently, there is mounting evidence that features from deep convolutional neural networks (CNN) are setting new records for various vision applications. On the other hand, considering the continuous characteristic of the depth values, depth estimations can be naturally formulated into a continuous conditional random field (CRF) learning problem. Therefore, we in this paper present a deep convolutional neural field model for estimating depths from a single image, aiming to jointly explore the capacity of deep CNN and continuous CRF. Specifically, we propose a deep structured learning scheme which learns the unary and pairwise potentials of continuous CRF in a unified deep CNN framework. The proposed method can be used for depth estimations of general scenes with no geometric priors nor any extra information injected. In our case, the integral of the partition function can be analytically calculated, thus we can exactly solve the log-likelihood optimization. Moreover, solving the MAP problem for predicting depths of a new image is highly efficient as closed-form solutions exist. We experimentally demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on both indoor and outdoor scene datasets. version:2
arxiv-1412-5710 | Multiobjective Optimization of Classifiers by Means of 3-D Convex Hull Based Evolutionary Algorithm | http://arxiv.org/abs/1412.5710 | id:1412.5710 author:Jiaqi Zhao, Vitor Basto Fernandes, Licheng Jiao, Iryna Yevseyeva, Asep Maulana, Rui Li, Thomas BÃ¤ck, Michael T. M. Emmerich category:cs.NE cs.LG  published:2014-12-18 summary:Finding a good classifier is a multiobjective optimization problem with different error rates and the costs to be minimized. The receiver operating characteristic is widely used in the machine learning community to analyze the performance of parametric classifiers or sets of Pareto optimal classifiers. In order to directly compare two sets of classifiers the area (or volume) under the convex hull can be used as a scalar indicator for the performance of a set of classifiers in receiver operating characteristic space. Recently, the convex hull based multiobjective genetic programming algorithm was proposed and successfully applied to maximize the convex hull area for binary classification problems. The contribution of this paper is to extend this algorithm for dealing with higher dimensional problem formulations. In particular, we discuss problems where parsimony (or classifier complexity) is stated as a third objective and multi-class classification with three different true classification rates to be maximized. The design of the algorithm proposed in this paper is inspired by indicator-based evolutionary algorithms, where first a performance indicator for a solution set is established and then a selection operator is designed that complies with the performance indicator. In this case, the performance indicator will be the volume under the convex hull. The algorithm is tested and analyzed in a proof of concept study on different benchmarks that are designed for measuring its capability to capture relevant parts of a convex hull. Further benchmark and application studies on email classification and feature selection round up the analysis and assess robustness and usefulness of the new algorithm in real world settings. version:1
arxiv-1412-5490 | High Frequency Content based Stimulus for Perceptual Sharpness Assessment in Natural Images | http://arxiv.org/abs/1412.5490 | id:1412.5490 author:Ashirbani Saha, Q. M. Jonathan Wu category:cs.CV  published:2014-12-17 summary:A blind approach to evaluate the perceptual sharpness present in a natural image is proposed. Though the literature demonstrates a set of variegated visual cues to detect or evaluate the absence or presence of sharpness, we emphasize in the current work that high frequency content and local standard deviation can form strong features to compute perceived sharpness in any natural image, and can be considered an able alternative for the existing cues. Unsharp areas in a natural image happen to exhibit uniform intensity or lack of sharp changes between regions. Sharp region transitions in an image are caused by the presence of spatial high frequency content. Therefore, in the proposed approach, we hypothesize that using the high frequency content as the principal stimulus, the perceived sharpness can be quantified in an image. When an image is convolved with a high pass filter, higher values at any pixel location signify the presence of high frequency content at those locations. Considering these values as the stimulus, the exponent of the stimulus is weighted by local standard deviation to impart the contribution of the local contrast within the formation of the sharpness map. The sharpness map highlights the relatively sharper regions in the image and is used to calculate the perceived sharpness score of the image. The advantages of the proposed method lie in its use of simple visual cues of high frequency content and local contrast to arrive at the perceptual score, and requiring no training with the images. The promise of the proposed method is demonstrated by its ability to compute perceived sharpness for within image and across image sharpness changes and for blind evaluation of perceptual degradation resulting due to presence of blur. Experiments conducted on several databases demonstrate improved performance of the proposed method over that of the state-of-the-art techniques. version:2
arxiv-1410-0759 | cuDNN: Efficient Primitives for Deep Learning | http://arxiv.org/abs/1410.0759 | id:1410.0759 author:Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, Evan Shelhamer category:cs.NE cs.LG cs.MS  published:2014-10-03 summary:We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption. version:3
arxiv-1312-6077 | Efficient Visual Coding: From Retina To V2 | http://arxiv.org/abs/1312.6077 | id:1312.6077 author:Honghao Shan, Garrison Cottrell category:cs.CV q-bio.NC  published:2013-12-20 summary:The human visual system has a hierarchical structure consisting of layers of processing, such as the retina, V1, V2, etc. Understanding the functional roles of these visual processing layers would help to integrate the psychophysiological and neurophysiological models into a consistent theory of human vision, and would also provide insights to computer vision research. One classical theory of the early visual pathway hypothesizes that it serves to capture the statistical structure of the visual inputs by efficiently coding the visual information in its outputs. Until recently, most computational models following this theory have focused upon explaining the receptive field properties of one or two visual layers. Recent work in deep networks has eliminated this concern, however, there is till the retinal layer to consider. Here we improve on a previously-described hierarchical model Recursive ICA (RICA) [1] which starts with PCA, followed by a layer of sparse coding or ICA, followed by a component-wise nonlinearity derived from considerations of the variable distributions expected by ICA. This process is then repeated. In this work, we improve on this model by using a new version of sparse PCA (sPCA), which results in biologically-plausible receptive fields for both the sPCA and ICA/sparse coding. When applied to natural image patches, our model learns visual features exhibiting the receptive field properties of retinal ganglion cells/lateral geniculate nucleus (LGN) cells, V1 simple cells, V1 complex cells, and V2 cells. Our work provides predictions for experimental neuroscience studies. For example, our result suggests that a previous neurophysiological study improperly discarded some of their recorded neurons; we predict that their discarded neurons capture the shape contour of objects. version:2
arxiv-1412-3506 | Road Detection by One-Class Color Classification: Dataset and Experiments | http://arxiv.org/abs/1412.3506 | id:1412.3506 author:Jose M. Alvarez, Theo Gevers, Antonio M. Lopez category:cs.CV  published:2014-12-11 summary:Detecting traversable road areas ahead a moving vehicle is a key process for modern autonomous driving systems. A common approach to road detection consists of exploiting color features to classify pixels as road or background. These algorithms reduce the effect of lighting variations and weather conditions by exploiting the discriminant/invariant properties of different color representations. Furthermore, the lack of labeled datasets has motivated the development of algorithms performing on single images based on the assumption that the bottom part of the image belongs to the road surface. In this paper, we first introduce a dataset of road images taken at different times and in different scenarios using an onboard camera. Then, we devise a simple online algorithm and conduct an exhaustive evaluation of different classifiers and the effect of using different color representation to characterize pixels. version:2
arxiv-1412-5687 | Towards Open World Recognition | http://arxiv.org/abs/1412.5687 | id:1412.5687 author:Abhijit Bendale, Terrance Boult category:cs.CV  published:2014-12-18 summary:With the of advent rich classification models and high computational power visual recognition systems have found many operational applications. Recognition in the real world poses multiple challenges that are not apparent in controlled lab environments. The datasets are dynamic and novel categories must be continuously detected and then added. At prediction time, a trained system has to deal with myriad unseen categories. Operational systems require minimum down time, even to learn. To handle these operational issues, we present the problem of Open World recognition and formally define it. We prove that thresholding sums of monotonically decreasing functions of distances in linearly transformed feature space can balance "open space risk" and empirical risk. Our theory extends existing algorithms for open world recognition. We present a protocol for evaluation of open world recognition systems. We present the Nearest Non-Outlier (NNO) algorithm which evolves model efficiently, adding object categories incrementally while detecting outliers and managing open space risk. We perform experiments on the ImageNet dataset with 1.2M+ images to validate the effectiveness of our method on large scale visual recognition tasks. NNO consistently yields superior results on open world recognition. version:1
arxiv-1412-5676 | Optimal Triggering of Networked Control Systems | http://arxiv.org/abs/1412.5676 | id:1412.5676 author:Ali Heydari category:cs.SY math.OC stat.ML  published:2014-12-17 summary:The problem of resource allocation of nonlinear networked control systems is investigated, where, unlike the well discussed case of triggering for stability, the objective is optimal triggering. An approximate dynamic programming approach is developed for solving problems with fixed final times initially and then it is extended to infinite horizon problems. Different cases including Zero-Order-Hold, Generalized Zero-Order-Hold, and stochastic networks are investigated. Afterwards, the developments are extended to the case of problems with unknown dynamics and a model-free scheme is presented for learning the (approximate) optimal solution. After detailed analyses of convergence, optimality, and stability of the results, the performance of the method is demonstrated through different numerical examples. version:1
arxiv-1412-5659 | Effective sampling for large-scale automated writing evaluation systems | http://arxiv.org/abs/1412.5659 | id:1412.5659 author:Nicholas Dronen, Peter W. Foltz, Kyle Habermehl category:cs.CL cs.LG  published:2014-12-17 summary:Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. This requirement limits large-scale adoption of AWE since human-scoring essays is costly. Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays. Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy. We conclude with a discussion of how to integrate this approach into large-scale AWE systems. version:1
arxiv-1412-5632 | Support recovery without incoherence: A case for nonconvex regularization | http://arxiv.org/abs/1412.5632 | id:1412.5632 author:Po-Ling Loh, Martin J. Wainwright category:math.ST cs.IT math.IT stat.ML stat.TH 62F12  published:2014-12-17 summary:We demonstrate that the primal-dual witness proof method may be used to establish variable selection consistency and $\ell_\infty$-bounds for sparse regression problems, even when the loss function and/or regularizer are nonconvex. Using this method, we derive two theorems concerning support recovery and $\ell_\infty$-guarantees for the regression estimator in a general setting. Our results provide rigorous theoretical justification for the use of nonconvex regularization: For certain nonconvex regularizers with vanishing derivative away from the origin, support recovery consistency may be guaranteed without requiring the typical incoherence conditions present in $\ell_1$-based methods. We then derive several corollaries that illustrate the wide applicability of our method to analyzing composite objective functions involving losses such as least squares, nonconvex modified least squares for errors-in variables linear regression, the negative log likelihood for generalized linear models, and the graphical Lasso. We conclude with empirical studies to corroborate our theoretical predictions. version:1
arxiv-1412-5627 | Feature extraction from complex networks: A case of study in genomic sequences classification | http://arxiv.org/abs/1412.5627 | id:1412.5627 author:Bruno Mendes Moro Conque, AndrÃ© Yoshiaki Kashiwabara, FabrÃ­cio Martins Lopes category:cs.CE cs.LG q-bio.QM  published:2014-12-17 summary:This work presents a new approach for classification of genomic sequences from measurements of complex networks and information theory. For this, it is considered the nucleotides, dinucleotides and trinucleotides of a genomic sequence. For each of them, the entropy, sum entropy and maximum entropy values are calculated.For each of them is also generated a network, in which the nodes are the nucleotides, dinucleotides or trinucleotides and its edges are estimated by observing the respective adjacency among them in the genomic sequence. In this way, it is generated three networks, for which measures of complex networks are extracted.These measures together with measures of information theory comprise a feature vector representing a genomic sequence. Thus, the feature vector is used for classification by methods such as SVM, MultiLayer Perceptron, J48, IBK, Naive Bayes and Random Forest in order to evaluate the proposed approach.It was adopted coding sequences, intergenic sequences and TSS (Transcriptional Starter Sites) as datasets, for which the better results were obtained by the Random Forest with 91.2%, followed by J48 with 89.1% and SVM with 84.8% of accuracy. These results indicate that the new approach of feature extraction has its value, reaching good levels of classification even considering only the genomic sequences, i.e., no other a priori knowledge about them is considered. version:1
arxiv-1412-5617 | Learning from Data with Heterogeneous Noise using SGD | http://arxiv.org/abs/1412.5617 | id:1412.5617 author:Shuang Song, Kamalika Chaudhuri, Anand D. Sarwate category:cs.LG  published:2014-12-17 summary:We consider learning from data of variable quality that may be obtained from different heterogeneous sources. Addressing learning from heterogeneous data in its full generality is a challenging problem. In this paper, we adopt instead a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem arises naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Experiments on real data show that our method performs better than using a single learning rate and using only the less noisy of the two datasets when the noise level is low to moderate. version:1
arxiv-1412-5513 | Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction | http://arxiv.org/abs/1412.5513 | id:1412.5513 author:Cyrine Arouri, Engelbert Mephu Nguifo, Sabeur Aridhi, CÃ©cile Roucelle, Gaelle Bonnet-Loosli, Norbert TsopzÃ© category:cs.NE cs.AI  published:2014-12-17 summary:The choice of architecture of artificial neuron network (ANN) is still a challenging task that users face every time. It greatly affects the accuracy of the built network. In fact there is no optimal method that is applicable to various implementations at the same time. In this paper we propose a method to construct ANN based on clustering, that resolves the problems of random and ad hoc approaches for multilayer ANN architecture. Our method can be applied to regression problems. Experimental results obtained with different datasets, reveals the efficiency of our method. version:1
arxiv-1412-5488 | Full-reference image quality assessment by combining global and local distortion measures | http://arxiv.org/abs/1412.5488 | id:1412.5488 author:Ashirbani Saha, Q. M. Jonathan Wu category:cs.CV  published:2014-12-17 summary:Full-reference image quality assessment (FR-IQA) techniques compare a reference and a distorted/test image and predict the perceptual quality of the test image in terms of a scalar value representing an objective score. The evaluation of FR-IQA techniques is carried out by comparing the objective scores from the techniques with the subjective scores (obtained from human observers) provided in the image databases used for the IQA. Hence, we reasonably assume that the goal of a human observer is to rate the distortion present in the test image. The goal oriented tasks are processed by the human visual system (HVS) through top-down processing which actively searches for local distortions driven by the goal. Therefore local distortion measures in an image are important for the top-down processing. At the same time, bottom-up processing also takes place signifying spontaneous visual functions in the HVS. To account for this, global perceptual features can be used. Therefore, we hypothesize that the resulting objective score for an image can be derived from the combination of local and global distortion measures calculated from the reference and test images. We calculate the local distortion by measuring the local correlation differences from the gradient and contrast information. For global distortion, dissimilarity of the saliency maps computed from a bottom-up model of saliency is used. The motivation behind the proposed approach has been thoroughly discussed, accompanied by an intuitive analysis. Finally, experiments are conducted in six benchmark databases suggesting the effectiveness of the proposed approach that achieves competitive performance with the state-of-the-art methods providing an improvement in the overall performance. version:1
arxiv-1412-5477 | Computational Model to Generate Case-Inflected Forms of Masculine Nouns for Word Search in Sanskrit E-Text | http://arxiv.org/abs/1412.5477 | id:1412.5477 author:S V Kasmir Raja, V Rajitha, Lakshmanan Meenakshi category:cs.CL  published:2014-12-17 summary:The problem of word search in Sanskrit is inseparable from complexities that include those caused by euphonic conjunctions and case-inflections. The case-inflectional forms of a noun normally number 24 owing to the fact that in Sanskrit there are eight cases and three numbers-singular, dual and plural. The traditional method of generating these inflectional forms is rather elaborate owing to the fact that there are differences in the forms generated between even very similar words and there are subtle nuances involved. Further, it would be a cumbersome exercise to generate and search for 24 forms of a word during a word search in a large text, using the currently available case-inflectional form generators. This study presents a new approach to generating case-inflectional forms that is simpler to compute. Further, an optimized model that is sufficient for generating only those word forms that are required in a word search and is more than 80% efficient compared to the complete case-inflectional forms generator, is presented in this study for the first time. version:1
arxiv-1412-5448 | Extended Recommendation Framework: Generating the Text of a User Review as a Personalized Summary | http://arxiv.org/abs/1412.5448 | id:1412.5448 author:MickaÃ«l Poussevin, Vincent Guigue, Patrick Gallinari category:cs.IR cs.CL  published:2014-12-17 summary:We propose to augment rating based recommender systems by providing the user with additional information which might help him in his choice or in the understanding of the recommendation. We consider here as a new task, the generation of personalized reviews associated to items. We use an extractive summary formulation for generating these reviews. We also show that the two information sources, ratings and items could be used both for estimating ratings and for generating summaries, leading to improved performance for each system compared to the use of a single source. Besides these two contributions, we show how a personalized polarity classifier can integrate the rating and textual aspects. Overall, the proposed system offers the user three personalized hints for a recommendation: rating, text and polarity. We evaluate these three components on two datasets using appropriate measures for each task. version:1
arxiv-1412-5404 | Word Network Topic Model: A Simple but General Solution for Short and Imbalanced Texts | http://arxiv.org/abs/1412.5404 | id:1412.5404 author:Yuan Zuo, Jichang Zhao, Ke Xu category:cs.CL cs.IR  published:2014-12-17 summary:The short text has been the prevalent format for information of Internet in recent decades, especially with the development of online social media, whose millions of users generate a vast number of short messages everyday. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhance the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the word-word space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling with LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And finally we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages. version:1
arxiv-1412-5384 | Representation of Evolutionary Algorithms in FPGA Cluster for Project of Large-Scale Networks | http://arxiv.org/abs/1412.5384 | id:1412.5384 author:Andre B. Perina, Marcilyanne M. Gois, Paulo Matias, Joao M. P. Cardoso, Alexandre C. B. Delbem, Vanderlei Bonato category:cs.DC cs.NE  published:2014-12-17 summary:Many problems are related to network projects, such as electric distribution, telecommunication and others. Most of them can be represented by graphs, which manipulate thousands or millions of nodes, becoming almost an impossible task to obtain real-time solutions. Many efficient solutions use Evolutionary Algorithms (EA), where researches show that performance of EAs can be substantially raised by using an appropriate representation, such as the Node-Depth Encoding (NDE). The objective of this work was to partition an implementation on single-FPGA (Field-Programmable Gate Array) based on NDE from 512 nodes to a multi-FPGAs approach, expanding the system to 4096 nodes. version:1
arxiv-1502-05988 | Deep Learning for Multi-label Classification | http://arxiv.org/abs/1502.05988 | id:1502.05988 author:Jesse Read, Fernando Perez-Cruz category:cs.LG cs.AI  published:2014-12-17 summary:In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature version:1
arxiv-1412-5334 | The Affine Transforms for Image Enhancement in the Context of Logarithmic Models | http://arxiv.org/abs/1412.5334 | id:1412.5334 author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV  published:2014-12-17 summary:The logarithmic model offers new tools for image processing. An efficient method for image enhancement is to use an affine transformation with the logarithmic operations: addition and scalar multiplication. We define some criteria for automatically determining the parameters of the processing and this is done via mean and variance computed by logarithmic operations. version:1
arxiv-1412-5328 | A Mathematical Model for Logarithmic Image Processing | http://arxiv.org/abs/1412.5328 | id:1412.5328 author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV  published:2014-12-17 summary:In this paper, we propose a new mathematical model for image processing. It is a logarithmical one. We consider the bounded interval (-1, 1) as the set of gray levels. Firstly, we define two operations: addition <+> and real scalar multiplication <x>. With these operations, the set of gray levels becomes a real vector space. Then, defining the scalar product (. .) and the norm . , we obtain an Euclidean space of the gray levels. Secondly, we extend these operations and functions for color images. We finally show the effect of various simple operations on an image. version:1
arxiv-1412-5325 | Color Image Enhancement In the Framework of Logarithmic Models | http://arxiv.org/abs/1412.5325 | id:1412.5325 author:Vasile Patrascu, Vasile Buzuloiu category:cs.CV  published:2014-12-17 summary:In this paper, we propose a mathematical model for color image processing. It is a logarithmical one. We consider the cube (-1,1)x(-1,1)x(-1,1) as the set of values for the color space. We define two operations: addition <+> and real scalar multiplication <x>. With these operations the space of colors becomes a real vector space. Then, defining the scalar product (. .) and the norm . , we obtain a (logarithmic) Euclidean space. We show how we can use this model for color image enhancement and we present some experimental results. version:1
arxiv-1412-5323 | Gene Similarity-based Approaches for Determining Core-Genes of Chloroplasts | http://arxiv.org/abs/1412.5323 | id:1412.5323 author:Bassam AlKindy, Christophe Guyeux, Jean-FranÃ§ois Couchot, Michel Salomon, Jacques M. Bahi category:cs.NE q-bio.GN  published:2014-12-17 summary:In computational biology and bioinformatics, the manner to understand evolution processes within various related organisms paid a lot of attention these last decades. However, accurate methodologies are still needed to discover genes content evolution. In a previous work, two novel approaches based on sequence similarities and genes features have been proposed. More precisely, we proposed to use genes names, sequence similarities, or both, insured either from NCBI or from DOGMA annotation tools. Dogma has the advantage to be an up-to-date accurate automatic tool specifically designed for chloroplasts, whereas NCBI possesses high quality human curated genes (together with wrongly annotated ones). The key idea of the former proposal was to take the best from these two tools. However, the first proposal was limited by name variations and spelling errors on the NCBI side, leading to core trees of low quality. In this paper, these flaws are fixed by improving the comparison of NCBI and DOGMA results, and by relaxing constraints on gene names while adding a stage of post-validation on gene sequences. The two stages of similarity measures, on names and sequences, are thus proposed for sequence clustering. This improves results that can be obtained using either NCBI or DOGMA alone. Results obtained with this quality control test are further investigated and compared with previously released ones, on both computational and biological aspects, considering a set of 99 chloroplastic genomes. version:1
arxiv-1412-5322 | An Algebraical Model for Gray Level Images | http://arxiv.org/abs/1412.5322 | id:1412.5322 author:Vasile Patrascu category:cs.CV  published:2014-12-17 summary:In this paper we propose a new algebraical model for the gray level images. It can be used for digital image processing. The model adresses to those images which are generated in improper light conditions (very low or high level). The vector space structure is able to illustrate some features into the image using modified level of contrast and luminosity. Also, the defined structure could be used in image enhancement. The general approach is presented with experimental results to demonstrate image enhancement. version:1
arxiv-1412-5275 | Iranian cashes recognition using mobile | http://arxiv.org/abs/1412.5275 | id:1412.5275 author:Ismail Nojavani, Azade Rezaeezade, Amirhassan Monadjemi category:cs.CV  published:2014-12-17 summary:In economical societies of today, using cash is an inseparable aspect of human life. People use cashes for marketing, services, entertainments, bank operations and so on. This huge amount of contact with cash and the necessity of knowing the monetary value of it caused one of the most challenging problems for visually impaired people. In this paper we propose a mobile phone based approach to identify monetary value of a picture taken from cashes using some image processing and machine vision techniques. While the developed approach is very fast, it can recognize the value of cash by average accuracy of about 95% and can overcome different challenges like rotation, scaling, collision, illumination changes, perspective, and some others. version:1
arxiv-1412-5272 | Consistency Analysis of an Empirical Minimum Error Entropy Algorithm | http://arxiv.org/abs/1412.5272 | id:1412.5272 author:Jun Fan, Ting Hu, Qiang Wu, Ding-Xuan Zhou category:cs.LG stat.ML  published:2014-12-17 summary:In this paper we study the consistency of an empirical minimum error entropy (MEE) algorithm in a regression setting. We introduce two types of consistency. The error entropy consistency, which requires the error entropy of the learned function to approximate the minimum error entropy, is shown to be always true if the bandwidth parameter tends to 0 at an appropriate rate. The regression consistency, which requires the learned function to approximate the regression function, however, is a complicated issue. We prove that the error entropy consistency implies the regression consistency for homoskedastic models where the noise is independent of the input variable. But for heteroskedastic models, a counterexample is used to show that the two types of consistency do not coincide. A surprising result is that the regression consistency is always true, provided that the bandwidth parameter tends to infinity at an appropriate rate. Regression consistency of two classes of special models is shown to hold with fixed bandwidth parameter, which further illustrates the complexity of regression consistency of MEE. Fourier transform plays crucial roles in our analysis. version:1
arxiv-1502-06434 | ANN Model to Predict Stock Prices at Stock Exchange Markets | http://arxiv.org/abs/1502.06434 | id:1502.06434 author:B. W. Wanjawa, L. Muchemi category:q-fin.ST cs.CE cs.LG cs.NE  published:2014-12-17 summary:Stock exchanges are considered major players in financial sectors of many countries. Most Stockbrokers, who execute stock trade, use technical, fundamental or time series analysis in trying to predict stock prices, so as to advise clients. However, these strategies do not usually guarantee good returns because they guide on trends and not the most likely price. It is therefore necessary to explore improved methods of prediction. The research proposes the use of Artificial Neural Network that is feedforward multi-layer perceptron with error backpropagation and develops a model of configuration 5:21:21:1 with 80% training data in 130,000 cycles. The research develops a prototype and tests it on 2008-2012 data from stock markets e.g. Nairobi Securities Exchange and New York Stock Exchange, where prediction results show MAPE of between 0.71% and 2.77%. Validation done with Encog and Neuroph realized comparable results. The model is thus capable of prediction on typical stock markets. version:1
arxiv-1412-5244 | Learning unbiased features | http://arxiv.org/abs/1412.5244 | id:1412.5244 author:Yujia Li, Kevin Swersky, Richard Zemel category:cs.LG cs.AI cs.NE stat.ML  published:2014-12-17 summary:A key element in transfer learning is representation learning; if representations can be developed that expose the relevant factors underlying the data, then new tasks and domains can be learned readily based on mappings of these salient factors. We propose that an important aim for these representations are to be unbiased. Different forms of representation learning can be derived from alternative definitions of unwanted bias, e.g., bias to particular tasks, domains, or irrelevant underlying data dimensions. One very useful approach to estimating the amount of bias in a representation comes from maximum mean discrepancy (MMD) [5], a measure of distance between probability distributions. We are not the first to suggest that MMD can be a useful criterion in developing representations that apply across multiple domains or tasks [1]. However, in this paper we describe a number of novel applications of this criterion that we have devised, all based on the idea of developing unbiased representations. These formulations include: a standard domain adaptation framework; a method of learning invariant representations; an approach based on noise-insensitive autoencoders; and a novel form of generative model. version:1
arxiv-1412-5236 | The supervised hierarchical Dirichlet process | http://arxiv.org/abs/1412.5236 | id:1412.5236 author:Andrew M. Dai, Amos J. Storkey category:stat.ML cs.LG  published:2014-12-17 summary:We propose the supervised hierarchical Dirichlet process (sHDP), a nonparametric generative model for the joint distribution of a group of observations and a response variable directly associated with that whole group. We compare the sHDP with another leading method for regression on grouped data, the supervised latent Dirichlet allocation (sLDA) model. We evaluate our method on two real-world classification problems and two real-world regression problems. Bayesian nonparametric regression models based on the Dirichlet process, such as the Dirichlet process-generalised linear models (DP-GLM) have previously been explored; these models allow flexibility in modelling nonlinear relationships. However, until now, Hierarchical Dirichlet Process (HDP) mixtures have not seen significant use in supervised problems with grouped data since a straightforward application of the HDP on the grouped data results in learnt clusters that are not predictive of the responses. The sHDP solves this problem by allowing for clusters to be learnt jointly from the group structure and from the label assigned to each group. version:1
arxiv-1412-5218 | Testing MCMC code | http://arxiv.org/abs/1412.5218 | id:1412.5218 author:Roger B. Grosse, David K. Duvenaud category:cs.SE cs.LG stat.ML  published:2014-12-16 summary:Markov Chain Monte Carlo (MCMC) algorithms are a workhorse of probabilistic modeling and inference, but are difficult to debug, and are prone to silent failure if implemented naively. We outline several strategies for testing the correctness of MCMC algorithms. Specifically, we advocate writing code in a modular way, where conditional probability calculations are kept separate from the logic of the sampler. We discuss strategies for both unit testing and integration testing. As a running example, we show how a Python implementation of Gibbs sampling for a mixture of Gaussians model can be tested. version:1
arxiv-1408-0553 | Sample Complexity Analysis for Learning Overcomplete Latent Variable Models through Tensor Methods | http://arxiv.org/abs/1408.0553 | id:1408.0553 author:Animashree Anandkumar, Rong Ge, Majid Janzamin category:cs.LG math.PR stat.ML  published:2014-08-03 summary:We provide guarantees for learning latent variable models emphasizing on the overcomplete regime, where the dimensionality of the latent space can exceed the observed dimensionality. In particular, we consider multiview mixtures, spherical Gaussian mixtures, ICA, and sparse coding models. We provide tight concentration bounds for empirical moments through novel covering arguments. We analyze parameter recovery through a simple tensor power update algorithm. In the semi-supervised setting, we exploit the label or prior information to get a rough estimate of the model parameters, and then refine it using the tensor method on unlabeled samples. We establish that learning is possible when the number of components scales as $k=o(d^{p/2})$, where $d$ is the observed dimension, and $p$ is the order of the observed moment employed in the tensor method. Our concentration bound analysis also leads to minimax sample complexity for semi-supervised learning of spherical Gaussian mixtures. In the unsupervised setting, we use a simple initialization algorithm based on SVD of the tensor slices, and provide guarantees under the stricter condition that $k\le \beta d$ (where constant $\beta$ can be larger than $1$), where the tensor method recovers the components under a polynomial running time (and exponential in $\beta$). Our analysis establishes that a wide range of overcomplete latent variable models can be learned efficiently with low computational and sample complexity through tensor decomposition methods. version:2
arxiv-1412-5212 | Application of Topic Models to Judgments from Public Procurement Domain | http://arxiv.org/abs/1412.5212 | id:1412.5212 author:MichaÅ ÅopuszyÅski category:cs.CL  published:2014-12-16 summary:In this work, automatic analysis of themes contained in a large corpora of judgments from public procurement domain is performed. The employed technique is unsupervised latent Dirichlet allocation (LDA). In addition, it is proposed, to use LDA in conjunction with recently developed method of unsupervised keyword extraction. Such an approach improves the interpretability of the automatically obtained topics and allows for better computational performance. The described analysis illustrates a potential of the method in detecting recurring themes and discovering temporal trends in lodged contract appeals. These results may be in future applied to improve information retrieval from repositories of legal texts or as auxiliary material for legal analyses carried out by human experts. version:1
arxiv-1412-5158 | Testing and Confidence Intervals for High Dimensional Proportional Hazards Model | http://arxiv.org/abs/1412.5158 | id:1412.5158 author:Ethan X. Fang, Yang Ning, Han Liu category:stat.ML math.ST stat.TH  published:2014-12-16 summary:This paper proposes a decorrelation-based approach to test hypotheses and construct confidence intervals for the low dimensional component of high dimensional proportional hazards models. Motivated by the geometric projection principle, we propose new decorrelated score, Wald and partial likelihood ratio statistics. Without assuming model selection consistency, we prove the asymptotic normality of these test statistics, establish their semiparametric optimality. We also develop new procedures for constructing pointwise confidence intervals for the baseline hazard function and baseline survival function. Thorough numerical results are provided to back up our theory. version:1
arxiv-1412-5104 | Locally Scale-Invariant Convolutional Neural Networks | http://arxiv.org/abs/1412.5104 | id:1412.5104 author:Angjoo Kanazawa, Abhishek Sharma, David Jacobs category:cs.CV cs.LG cs.NE  published:2014-12-16 summary:Convolutional Neural Networks (ConvNets) have shown excellent results on many visual classification tasks. With the exception of ImageNet, these datasets are carefully crafted such that objects are well-aligned at similar scales. Naturally, the feature learning problem gets more challenging as the amount of variation in the data increases, as the models have to learn to be invariant to certain changes in appearance. Recent results on the ImageNet dataset show that given enough data, ConvNets can learn such invariances producing very discriminative features [1]. But could we do more: use less parameters, less data, learn more discriminative features, if certain invariances were built into the learning process? In this paper we present a simple model that allows ConvNets to learn features in a locally scale-invariant manner without increasing the number of model parameters. We show on a modified MNIST dataset that when faced with scale variation, building in scale-invariance allows ConvNets to learn more discriminative features with reduced chances of over-fitting. version:1
arxiv-1411-5379 | Type-Driven Incremental Semantic Parsing with Polymorphism | http://arxiv.org/abs/1411.5379 | id:1411.5379 author:Kai Zhao, Liang Huang category:cs.CL  published:2014-11-19 summary:Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GeoQuery, Jobs and Atis domains. version:3
arxiv-1412-5067 | Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling Problem with Setups | http://arxiv.org/abs/1412.5067 | id:1412.5067 author:A. V. Eremeev, Ju. V. Kovalenko category:cs.NE  published:2014-12-16 summary:In this paper, we perform an experimental study of optimal recombination operator for makespan minimization problem on single machine with sequence-dependent setup times ($1 s_{vu} C_{\max}$). The computational experiment on benchmark problems from TSPLIB library indicates practical applicability of optimal recombination in crossover operator of genetic algorithm for $1 s_{vu} C_{\max}$. version:1
arxiv-1401-0872 | Binary Linear Classification and Feature Selection via Generalized Approximate Message Passing | http://arxiv.org/abs/1401.0872 | id:1401.0872 author:Justin Ziniel, Philip Schniter, Per Sederberg category:cs.IT math.IT stat.ML  published:2014-01-05 summary:For the problem of binary linear classification and feature selection, we propose algorithmic approaches to classifier design based on the generalized approximate message passing (GAMP) algorithm, recently proposed in the context of compressive sensing. We are particularly motivated by problems where the number of features greatly exceeds the number of training examples, but where only a few features suffice for accurate classification. We show that sum-product GAMP can be used to (approximately) minimize the classification error rate and max-sum GAMP can be used to minimize a wide variety of regularized loss functions. Furthermore, we describe an expectation-maximization (EM)-based scheme to learn the associated model parameters online, as an alternative to cross-validation, and we show that GAMP's state-evolution framework can be used to accurately predict the misclassification rate. Finally, we present a detailed numerical study to confirm the accuracy, speed, and flexibility afforded by our GAMP-based approaches to binary linear classification and feature selection. version:3
arxiv-1409-4256 | Machine learning for ultrafast X-ray diffraction patterns on large-scale GPU clusters | http://arxiv.org/abs/1409.4256 | id:1409.4256 author:Tomas Ekeberg, Stefan Engblom, Jing Liu category:q-bio.BM cs.DC cs.LG physics.bio-ph q-bio.QM 68W10  68W15  68U10  published:2014-09-11 summary:The classical method of determining the atomic structure of complex molecules by analyzing diffraction patterns is currently undergoing drastic developments. Modern techniques for producing extremely bright and coherent X-ray lasers allow a beam of streaming particles to be intercepted and hit by an ultrashort high energy X-ray beam. Through machine learning methods the data thus collected can be transformed into a three-dimensional volumetric intensity map of the particle itself. The computational complexity associated with this problem is very high such that clusters of data parallel accelerators are required. We have implemented a distributed and highly efficient algorithm for inversion of large collections of diffraction patterns targeting clusters of hundreds of GPUs. With the expected enormous amount of diffraction data to be produced in the foreseeable future, this is the required scale to approach real time processing of data at the beam site. Using both real and synthetic data we look at the scaling properties of the application and discuss the overall computational viability of this exciting and novel imaging technique. version:2
arxiv-1412-4986 | A Scalable Asynchronous Distributed Algorithm for Topic Modeling | http://arxiv.org/abs/1412.4986 | id:1412.4986 author:Hsiang-Fu Yu, Cho-Jui Hsieh, Hyokun Yun, S. V. N Vishwanathan, Inderjit S. Dhillon category:cs.DC cs.IR cs.LG  published:2014-12-16 summary:Learning meaningful topic models with massive document collections which contain millions of documents and billions of tokens is challenging because of two reasons: First, one needs to deal with a large number of topics (typically in the order of thousands). Second, one needs a scalable and efficient way of distributing the computation across multiple machines. In this paper we present a novel algorithm F+Nomad LDA which simultaneously tackles both these problems. In order to handle large number of topics we use an appropriately modified Fenwick tree. This data structure allows us to sample from a multinomial distribution over $T$ items in $O(\log T)$ time. Moreover, when topic counts change the data structure can be updated in $O(\log T)$ time. In order to distribute the computation across multiple processor we present a novel asynchronous framework inspired by the Nomad algorithm of \cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform state-of-the-art on massive problems which involve millions of documents, billions of words, and thousands of topics. version:1
arxiv-1412-4967 | Sparse, guided feature connections in an Abstract Deep Network | http://arxiv.org/abs/1412.4967 | id:1412.4967 author:Anthony Knittel, Alan Blair category:cs.NE  published:2014-12-16 summary:We present a technique for developing a network of re-used features, where the topology is formed using a coarse learning method, that allows gradient-descent fine tuning, known as an Abstract Deep Network (ADN). New features are built based on observed co-occurrences, and the network is maintained using a selection process related to evolutionary algorithms. This allows coarse ex- ploration of the problem space, effective for irregular domains, while gradient descent allows pre- cise solutions. Accuracy on standard UCI and Protein-Structure Prediction problems is comparable with benchmark SVM and optimized GBML approaches, and shows scalability for addressing large problems. The discrete implementation is symbolic, allowing interpretability, while the continuous method using fine-tuning shows improved accuracy. The binary multiplexer problem is explored, as an irregular domain that does not support gradient descent learning, showing solution to the bench- mark 135-bit problem. A convolutional implementation is demonstrated on image classification, showing an error-rate of 0.79% on the MNIST problem, without a pre-defined topology. The ADN system provides a method for developing a very sparse, deep feature topology, based on observed relationships between features, that is able to find solutions in irregular domains, and initialize a network prior to gradient descent learning. version:1
arxiv-1412-4944 | Efficient GPU Implementation for Single Block Orthogonal Dictionary Learning | http://arxiv.org/abs/1412.4944 | id:1412.4944 author:Paul Irofti category:cs.CV cs.DC  published:2014-12-16 summary:Dictionary training for sparse representations involves dealing with large chunks of data and complex algorithms that determine time consuming implementations. SBO is an iterative dictionary learning algorithm based on constructing unions of orthonormal bases via singular value decomposition, that represents each data item through a single best fit orthobase. In this paper we present a GPGPU approach of implementing SBO in OpenCL. We provide a lock-free solution that ensures full-occupancy of the GPU by following the map-reduce model for the sparse-coding stage and by making use of the Partitioned Global Address Space (PGAS) model for developing parallel dictionary updates. The resulting implementation achieves a favourable trade-off between algorithm complexity and data representation quality compared to PAK-SVD which is the standard overcomplete dictionary learning approach. We present and discuss numerical results showing a significant acceleration of the execution time for the dictionary learning process. version:1
arxiv-1412-4940 | Discovering beautiful attributes for aesthetic image analysis | http://arxiv.org/abs/1412.4940 | id:1412.4940 author:Luca Marchesotti, Naila Murray, Florent Perronnin category:cs.CV  published:2014-12-16 summary:Aesthetic image analysis is the study and assessment of the aesthetic properties of images. Current computational approaches to aesthetic image analysis either provide accurate or interpretable results. To obtain both accuracy and interpretability by humans, we advocate the use of learned and nameable visual attributes as mid-level features. For this purpose, we propose to discover and learn the visual appearance of attributes automatically, using a recently introduced database, called AVA, which contains more than 250,000 images together with their aesthetic scores and textual comments given by photography enthusiasts. We provide a detailed analysis of these annotations as well as the context in which they were given. We then describe how these three key components of AVA - images, scores, and comments - can be effectively leveraged to learn visual attributes. Lastly, we show that these learned attributes can be successfully used in three applications: aesthetic quality prediction, image tagging and retrieval. version:1
arxiv-1407-0202 | SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives | http://arxiv.org/abs/1407.0202 | id:1407.0202 author:Aaron Defazio, Francis Bach, Simon Lacoste-Julien category:cs.LG math.OC stat.ML  published:2014-07-01 summary:In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method. version:3
arxiv-1312-5439 | Asynchronous Adaptation and Learning over Networks - Part III: Comparison Analysis | http://arxiv.org/abs/1312.5439 | id:1312.5439 author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC  published:2013-12-19 summary:In Part II [3] we carried out a detailed mean-square-error analysis of the performance of asynchronous adaptation and learning over networks under a fairly general model for asynchronous events including random topologies, random link failures, random data arrival times, and agents turning on and off randomly. In this Part III, we compare the performance of synchronous and asynchronous networks. We also compare the performance of decentralized adaptation against centralized stochastic-gradient (batch) solutions. Two interesting conclusions stand out. First, the results establish that the performance of adaptive networks is largely immune to the effect of asynchronous events: the mean and mean-square convergence rates and the asymptotic bias values are not degraded relative to synchronous or centralized implementations. Only the steady-state mean-square-deviation suffers a degradation in the order of $\nu$, which represents the small step-size parameters used for adaptation. Second, the results show that the adaptive distributed network matches the performance of the centralized solution. These conclusions highlight another critical benefit of cooperation by networked agents: cooperation does not only enhance performance in comparison to stand-alone single-agent processing, but it also endows the network with remarkable resilience to various forms of random failure events and is able to deliver performance that is as powerful as batch solutions. version:3
arxiv-1312-5438 | Asynchronous Adaptation and Learning over Networks - Part II: Performance Analysis | http://arxiv.org/abs/1312.5438 | id:1312.5438 author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC  published:2013-12-19 summary:In Part I \cite{Zhao13TSPasync1}, we introduced a fairly general model for asynchronous events over adaptive networks including random topologies, random link failures, random data arrival times, and agents turning on and off randomly. We performed a stability analysis and established the notable fact that the network is still able to converge in the mean-square-error sense to the desired solution. Once stable behavior is guaranteed, it becomes important to evaluate how fast the iterates converge and how close they get to the optimal solution. This is a demanding task due to the various asynchronous events and due to the fact that agents influence each other. In this Part II, we carry out a detailed analysis of the mean-square-error performance of asynchronous strategies for solving distributed optimization and adaptation problems over networks. We derive analytical expressions for the mean-square convergence rate and the steady-state mean-square-deviation. The expressions reveal how the various parameters of the asynchronous behavior influence network performance. In the process, we establish the interesting conclusion that even under the influence of asynchronous events, all agents in the adaptive network can still reach an $O(\nu^{1 + \gamma_o'})$ near-agreement with some $\gamma_o' > 0$ while approaching the desired solution within $O(\nu)$ accuracy, where $\nu$ is proportional to the small step-size parameter for adaptation. version:3
arxiv-1312-5434 | Asynchronous Adaptation and Learning over Networks --- Part I: Modeling and Stability Analysis | http://arxiv.org/abs/1312.5434 | id:1312.5434 author:Xiaochuan Zhao, Ali H. Sayed category:cs.SY cs.IT cs.LG math.IT math.OC  published:2013-12-19 summary:In this work and the supporting Parts II [2] and III [3], we provide a rather detailed analysis of the stability and performance of asynchronous strategies for solving distributed optimization and adaptation problems over networks. We examine asynchronous networks that are subject to fairly general sources of uncertainties, such as changing topologies, random link failures, random data arrival times, and agents turning on and off randomly. Under this model, agents in the network may stop updating their solutions or may stop sending or receiving information in a random manner and without coordination with other agents. We establish in Part I conditions on the first and second-order moments of the relevant parameter distributions to ensure mean-square stable behavior. We derive in Part II expressions that reveal how the various parameters of the asynchronous behavior influence network performance. We compare in Part III the performance of asynchronous networks to the performance of both centralized solutions and synchronous networks. One notable conclusion is that the mean-square-error performance of asynchronous networks shows a degradation only of the order of $O(\nu)$, where $\nu$ is a small step-size parameter, while the convergence rate remains largely unaltered. The results provide a solid justification for the remarkable resilience of cooperative networks in the face of random failures at multiple levels: agents, links, data arrivals, and topology. version:3
arxiv-1412-4313 | Combining the Best of Graphical Models and ConvNets for Semantic Segmentation | http://arxiv.org/abs/1412.4313 | id:1412.4313 author:Michael Cogswell, Xiao Lin, Senthil Purushwalkam, Dhruv Batra category:cs.CV  published:2014-12-14 summary:We present a two-module approach to semantic segmentation that incorporates Convolutional Networks (CNNs) and Graphical Models. Graphical models are used to generate a small (5-30) set of diverse segmentations proposals, such that this set has high recall. Since the number of required proposals is so low, we can extract fairly complex features to rank them. Our complex feature of choice is a novel CNN called SegNet, which directly outputs a (coarse) semantic segmentation. Importantly, SegNet is specifically trained to optimize the corpus-level PASCAL IOU loss function. To the best of our knowledge, this is the first CNN specifically designed for semantic segmentation. This two-module approach achieves $52.5\%$ on the PASCAL 2012 segmentation challenge. version:2
arxiv-1410-0291 | A Morphological Analyzer for Japanese Nouns, Verbs and Adjectives | http://arxiv.org/abs/1410.0291 | id:1410.0291 author:Yanchuan Sim category:cs.CL  published:2014-10-01 summary:We present an open source morphological analyzer for Japanese nouns, verbs and adjectives. The system builds upon the morphological analyzing capabilities of MeCab to incorporate finer details of classification such as politeness, tense, mood and voice attributes. We implemented our analyzer in the form of a finite state transducer using the open source finite state compiler FOMA toolkit. The source code and tool is available at https://bitbucket.org/skylander/yc-nlplab/. version:2
arxiv-1412-4864 | Learning with Pseudo-Ensembles | http://arxiv.org/abs/1412.4864 | id:1412.4864 author:Philip Bachman, Ouais Alsharif, Doina Precup category:stat.ML cs.LG cs.NE  published:2014-12-16 summary:We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark. version:1
arxiv-1412-4863 | Max-Margin based Discriminative Feature Learning | http://arxiv.org/abs/1412.4863 | id:1412.4863 author:Changsheng Li, Qingshan Liu, Weishan Dong, Xin Zhang, Lin Yang category:cs.LG  published:2014-12-16 summary:In this paper, we propose a new max-margin based discriminative feature learning method. Specifically, we aim at learning a low-dimensional feature representation, so as to maximize the global margin of the data and make the samples from the same class as close as possible. In order to enhance the robustness to noise, a $l_{2,1}$ norm constraint is introduced to make the transformation matrix in group sparsity. In addition, for multi-class classification tasks, we further intend to learn and leverage the correlation relationships among multiple class tasks for assisting in learning discriminative features. The experimental results demonstrate the power of the proposed method against the related state-of-the-art methods. version:1
arxiv-1412-4526 | Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification | http://arxiv.org/abs/1412.4526 | id:1412.4526 author:Hongsheng Li, Rui Zhao, Xiaogang Wang category:cs.CV  published:2014-12-15 summary:We present highly efficient algorithms for performing forward and backward propagation of Convolutional Neural Network (CNN) for pixelwise classification on images. For pixelwise classification tasks, such as image segmentation and object detection, surrounding image patches are fed into CNN for predicting the classes of centered pixels via forward propagation and for updating CNN parameters via backward propagation. However, forward and backward propagation was originally designed for whole-image classification. Directly applying it to pixelwise classification in a patch-by-patch scanning manner is extremely inefficient, because surrounding patches of pixels have large overlaps, which lead to a lot of redundant computation. The proposed algorithms eliminate all the redundant computation in convolution and pooling on images by introducing novel d-regularly sparse kernels. It generates exactly the same results as those by patch-by-patch scanning. Convolution and pooling operations with such kernels are able to continuously access memory and can run efficiently on GPUs. A fraction of patches of interest can be chosen from each training image for backward propagation by applying a mask to the error map at the last CNN layer. Its computation complexity is constant with respect to the number of patches sampled from the image. Experiments have shown that our proposed algorithms speed up commonly used patch-by-patch scanning over 1500 times in both forward and backward propagation. The speedup increases with the sizes of images and patches. version:2
arxiv-1209-2194 | Cooperative learning in multi-agent systems from intermittent measurements | http://arxiv.org/abs/1209.2194 | id:1209.2194 author:Naomi Ehrich Leonard, Alex Olshevsky category:math.OC cs.LG cs.MA cs.SY  published:2012-09-11 summary:Motivated by the problem of tracking a direction in a decentralized way, we consider the general problem of cooperative learning in multi-agent systems with time-varying connectivity and intermittent measurements. We propose a distributed learning protocol capable of learning an unknown vector $\mu$ from noisy measurements made independently by autonomous nodes. Our protocol is completely distributed and able to cope with the time-varying, unpredictable, and noisy nature of inter-agent communication, and intermittent noisy measurements of $\mu$. Our main result bounds the learning speed of our protocol in terms of the size and combinatorial features of the (time-varying) networks connecting the nodes. version:5
arxiv-1412-4682 | Rule-based Emotion Detection on Social Media: Putting Tweets on Plutchik's Wheel | http://arxiv.org/abs/1412.4682 | id:1412.4682 author:Erik Tromp, Mykola Pechenizkiy category:cs.CL  published:2014-12-15 summary:We study sentiment analysis beyond the typical granularity of polarity and instead use Plutchik's wheel of emotions model. We introduce RBEM-Emo as an extension to the Rule-Based Emission Model algorithm to deduce such emotions from human-written messages. We evaluate our approach on two different datasets and compare its performance with the current state-of-the-art techniques for emotion detection, including a recursive auto-encoder. The results of the experimental study suggest that RBEM-Emo is a promising approach advancing the current state-of-the-art in emotion detection. version:1
arxiv-1408-5810 | Kernel-based Information Criterion | http://arxiv.org/abs/1408.5810 | id:1408.5810 author:Somayeh Danafar, Kenji Fukumizu, Faustino Gomez category:stat.ML  published:2014-08-25 summary:This paper introduces Kernel-based Information Criterion (KIC) for model selection in regression analysis. The novel kernel-based complexity measure in KIC efficiently computes the interdependency between parameters of the model using a variable-wise variance and yields selection of better, more robust regressors. Experimental results show superior performance on both simulated and real data sets compared to Leave-One-Out Cross-Validation (LOOCV), kernel-based Information Complexity (ICOMP), and maximum log of marginal likelihood in Gaussian Process Regression (GPR). version:2
arxiv-1412-4616 | A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems | http://arxiv.org/abs/1412.4616 | id:1412.4616 author:Felix Weninger, BjÃ¶rn Schuller, Florian Eyben, Martin WÃ¶llmer, Gerhard Rigoll category:cs.CL cs.SD  published:2014-12-15 summary:Transcription of broadcast news is an interesting and challenging application for large-vocabulary continuous speech recognition (LVCSR). We present in detail the structure of a manually segmented and annotated corpus including over 160 hours of German broadcast news, and propose it as an evaluation framework of LVCSR systems. We show our own experimental results on the corpus, achieved with a state-of-the-art LVCSR decoder, measuring the effect of different feature sets and decoding parameters, and thereby demonstrate that real-time decoding of our test set is feasible on a desktop PC at 9.2% word error rate. version:1
arxiv-1308-1479 | Challenges of Big Data Analysis | http://arxiv.org/abs/1308.1479 | id:1308.1479 author:Jianqing Fan, Fang Han, Han Liu category:stat.ML  published:2013-08-07 summary:Big Data bring new opportunities to modern society and challenges to data scientists. On one hand, Big Data hold great promises for discovering subtle population patterns and heterogeneities that are not possible with small-scale data. On the other hand, the massive sample size and high dimensionality of Big Data introduce unique computational and statistical challenges, including scalability and storage bottleneck, noise accumulation, spurious correlation, incidental endogeneity, and measurement errors. These challenges are distinguished and require new computational and statistical paradigm. This article give overviews on the salient features of Big Data and how these features impact on paradigm change on statistical and computational methods as well as computing architectures. We also provide various new perspectives on the Big Data analysis and computation. In particular, we emphasis on the viability of the sparsest solution in high-confidence set and point out that exogeneous assumptions in most statistical methods for Big Data can not be validated due to incidental endogeneity. They can lead to wrong statistical inferences and consequently wrong scientific conclusions. version:2
arxiv-1412-6061 | CITlab ARGUS for Arabic Handwriting | http://arxiv.org/abs/1412.6061 | id:1412.6061 author:Gundram Leifert, Roger Labahn, Tobias StrauÃ category:cs.CV cs.NE 68T10  68T05  published:2014-12-15 summary:In the recent years it turned out that multidimensional recurrent neural networks (MDRNN) perform very well for offline handwriting recognition tasks like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and dictionary lookup, our ARGUS software completed this task with an error rate of 26.27% in its primary setup. version:1
arxiv-1412-6012 | CITlab ARGUS for historical data tables | http://arxiv.org/abs/1412.6012 | id:1412.6012 author:Gundram Leifert, Tobias GrÃ¼ning, Tobias StrauÃ, Roger Labahn, for the University o category:cs.CV cs.NE 68T05  68T10  published:2014-12-15 summary:We describe CITlab's recognition system for the ANWRESH-2014 competition attached to the 14. International Conference on Frontiers in Handwriting Recognition, ICFHR 2014. The task comprises word recognition from segmented historical documents. The core components of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing. version:1
arxiv-1412-4470 | Automatic video scene segmentation based on spatial-temporal clues and rhythm | http://arxiv.org/abs/1412.4470 | id:1412.4470 author:Walid Mahdi, Liming Chen, Mohsen Ardebilian category:cs.CV  published:2014-12-15 summary:With ever increasing computing power and data storage capacity, the potential for large digital video libraries is growing rapidly.However, the massive use of video for the moment is limited by its opaque characteristics. Indeed, a user who has to handle and retrieve sequentially needs too much time in order to find out segments of interest within a video. Therefore, providing an environment both convenient and efficient for video storing and retrieval, especially for content-based searching as this exists in traditional textbased database systems, has been the focus of recent and important efforts of a large research community In this paper, we propose a new automatic video scene segmentation method that explores two main video features; these are spatial-temporal relationship and rhythm of shots. The experimental evidence we obtained from a 80 minutevideo showed that our prototype provides very high accuracy for video segmentation. version:1
arxiv-1412-4438 | Fixed Point Algorithm Based on Quasi-Newton Method for Convex Minimization Problem with Application to Image Deblurring | http://arxiv.org/abs/1412.4438 | id:1412.4438 author:Dai-Qiang Chen category:cs.CV 68U10  90C90  65T60  published:2014-12-15 summary:Solving an optimization problem whose objective function is the sum of two convex functions has received considerable interests in the context of image processing recently. In particular, we are interested in the scenario when a non-differentiable convex function such as the total variation (TV) norm is included in the objective function due to many variational models established in image processing have this nature. In this paper, we propose a fast fixed point algorithm based on the quasi-Newton method for solving this class of problem, and apply it in the field of TV-based image deblurring. The novel method is derived from the idea of the quasi-Newton method, and the fixed-point algorithms based on the proximity operator, which were widely investigated very recently. Utilizing the non-expansion property of the proximity operator we further investigate the global convergence of the proposed algorithm. Numerical experiments on image deblurring problem with additive or multiplicative noise are presented to demonstrate that the proposed algorithm is superior to the recently developed fixed-point algorithm in the computational efficiency. version:1
arxiv-1412-6144 | The Computational Theory of Intelligence: Applications to Genetic Programming and Turing Machines | http://arxiv.org/abs/1412.6144 | id:1412.6144 author:Daniel Kovach category:cs.NE 92DXX  68TXX  03Dxx I.2.0; J.3; F.1.1  published:2014-12-14 summary:In this paper, we continue the efforts of the Computational Theory of Intelligence (CTI) by extending concepts to include computational processes in terms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive, and Hybrid Computational Intelligence processes are also introduced and discussed. We consider the ramifications of the assumptions of CTI with regard to the qualities of reproduction and virility. Applications to Biology, Computer Science and Cyber Security are also discussed. version:1
arxiv-1409-3215 | Sequence to Sequence Learning with Neural Networks | http://arxiv.org/abs/1409.3215 | id:1409.3215 author:Ilya Sutskever, Oriol Vinyals, Quoc V. Le category:cs.CL cs.LG  published:2014-09-10 summary:Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier. version:3
arxiv-1412-4401 | Tools for Terminology Processing | http://arxiv.org/abs/1412.4401 | id:1412.4401 author:C. Enguehard, B. Daille, E. Morin category:cs.CY cs.CL  published:2014-12-14 summary:Automatic terminology processing appeared 10 years ago when electronic corpora became widely available. Such processing may be statistically or linguistically based and produces terminology resources that can be used in a number of applications : indexing, information retrieval, technology watch, etc. We present the tools that have been developed in the IRIN Institute. They all take as input texts (or collection of texts) and reflect different states of terminology processing: term acquisition, term recognition and term structuring. version:1
arxiv-1412-3191 | Bach in 2014: Music Composition with Recurrent Neural Network | http://arxiv.org/abs/1412.3191 | id:1412.3191 author:I-Ting Liu, Bhiksha Ramakrishnan category:cs.AI cs.NE  published:2014-12-10 summary:We propose a framework for computer music composition that uses resilient propagation (RProp) and long short term memory (LSTM) recurrent neural network. In this paper, we show that LSTM network learns the structure and characteristics of music pieces properly by demonstrating its ability to recreate music. We also show that predicting existing music using RProp outperforms Back propagation through time (BPTT). version:2
arxiv-1412-4237 | First order algorithms in variational image processing | http://arxiv.org/abs/1412.4237 | id:1412.4237 author:Martin Burger, Alex Sawatzky, Gabriele Steidl category:math.OC cs.CV stat.ML  published:2014-12-13 summary:Variational methods in imaging are nowadays developing towards a quite universal and flexible tool, allowing for highly successful approaches on tasks like denoising, deblurring, inpainting, segmentation, super-resolution, disparity, and optical flow estimation. The overall structure of such approaches is of the form ${\cal D}(Ku) + \alpha {\cal R} (u) \rightarrow \min_u$ ; where the functional ${\cal D}$ is a data fidelity term also depending on some input data $f$ and measuring the deviation of $Ku$ from such and ${\cal R}$ is a regularization functional. Moreover $K$ is a (often linear) forward operator modeling the dependence of data on an underlying image, and $\alpha$ is a positive regularization parameter. While ${\cal D}$ is often smooth and (strictly) convex, the current practice almost exclusively uses nonsmooth regularization functionals. The majority of successful techniques is using nonsmooth and convex functionals like the total variation and generalizations thereof or $\ell_1$-norms of coefficients arising from scalar products with some frame system. The efficient solution of such variational problems in imaging demands for appropriate algorithms. Taking into account the specific structure as a sum of two very different terms to be minimized, splitting algorithms are a quite canonical choice. Consequently this field has revived the interest in techniques like operator splittings or augmented Lagrangians. Here we shall provide an overview of methods currently developed and recent results as well as some computational studies providing a comparison of different methods and also illustrating their success in applications. version:1
arxiv-1412-4218 | Optimization of Reliability of Network of Given Connectivity using Genetic Algorithm | http://arxiv.org/abs/1412.4218 | id:1412.4218 author:Ho Tat Lam, Kwok Yip Szeto category:physics.soc-ph cs.NE cs.SI  published:2014-12-13 summary:Reliability is one of the important measures of how well the system meets its design objective, and mathematically is the probability that a system will perform satisfactorily for at least a given period of time. When the system is described by a connected network of N components (nodes) and their L connection (links), the reliability of the system becomes a difficult network design problem which solutions are of great practical interest in science and engineering. This paper discusses the numerical method of finding the most reliable network for a given N and L using genetic algorithm. For a given topology of the network, the reliability is numerically computed using adjacency matrix. For a search in the space of all possible topologies of the connected network with N nodes and L links, genetic operators such as mutation and crossover are applied to the adjacency matrix through a string representation. In the context of graphs, the mutation of strings in genetic algorithm corresponds to the rewiring of graphs, while crossover corresponds to the interchange of the sub-graphs. For small networks where the most reliable network can be found by exhaustive search, genetic algorithm is very efficient. For larger networks, our results not only demonstrate the efficiency of our algorithm, but also suggest that the most reliable network will have high symmetry. version:1
arxiv-1412-4217 | A Study of Sindhi Related and Arabic Script Adapted languages Recognition | http://arxiv.org/abs/1412.4217 | id:1412.4217 author:Dil Nawaz Hakro, A. Z. Talib, Zeeshan Bhatti, G. N. Moja category:cs.CV  published:2014-12-13 summary:A large number of publications are available for the Optical Character Recognition (OCR). Significant researches, as well as articles are present for the Latin, Chinese and Japanese scripts. Arabic script is also one of mature script from OCR perspective. The adaptive languages which share Arabic script or its extended characters; still lacking the OCRs for their language. In this paper we present the efforts of researchers on Arabic and its related and adapted languages. This survey is organized in different sections, in which introduction is followed by properties of Sindhi Language. OCR process techniques and methods used by various researchers are presented. The last section is dedicated for future work and conclusion is also discussed. version:1
arxiv-1412-4205 | The application of the Bayes Ying Yang harmony based GMMs in on-line signature verification | http://arxiv.org/abs/1412.4205 | id:1412.4205 author:Xiaosha Zhao, Mandan Liu category:cs.CV  published:2014-12-13 summary:In this contribution, a Bayes Ying Yang(BYY) harmony based approach for on-line signature verification is presented. In the proposed method, a simple but effective Gaussian Mixture Models(GMMs) is used to represent for each user's signature model based on the prior information collected. Different from the early works, in this paper, we use the Bayes Ying Yang machine combined with the harmony function to achieve Automatic Model Selection(AMS) during the parameter learning for the GMMs, so that a better approximation of the user model is assured. Experiments on a database from the First International Signature Verification Competition(SVC 2004) confirm that this combined algorithm yields quite satisfactory results. version:1
arxiv-1412-4196 | Descriptor Ensemble: An Unsupervised Approach to Descriptor Fusion in the Homography Space | http://arxiv.org/abs/1412.4196 | id:1412.4196 author:Yuan-Ting Hu, Yen-Yu Lin, Hsin-Yi Chen, Kuang-Jui Hsu, Bing-Yu Chen category:cs.CV  published:2014-12-13 summary:With the aim to improve the performance of feature matching, we present an unsupervised approach to fuse various local descriptors in the space of homographies. Inspired by the observation that the homographies of correct feature correspondences vary smoothly along the spatial domain, our approach stands on the unsupervised nature of feature matching, and can select a good descriptor for matching each feature point. Specifically, the homography space serves as the common domain, in which a correspondence obtained by any descriptor is considered as a point, for integrating various heterogeneous descriptors. Both geometric coherence and spatial continuity among correspondences are considered via computing their geodesic distances in the space. In this way, mutual verification across different descriptors is allowed, and correct correspondences will be highlighted with a high degree of consistency (i.e., short geodesic distances here). It follows that one-class SVM can be applied to identifying these correct correspondences, and boosts the performance of feature matching. The proposed approach is comprehensively compared with the state-of-the-art approaches, and evaluated on four benchmarks of image matching. The promising results manifest its effectiveness. version:1
arxiv-1412-4186 | An Evaluation of Support Vector Machines as a Pattern Recognition Tool | http://arxiv.org/abs/1412.4186 | id:1412.4186 author:Eugene Borovikov category:cs.LG 62-07  published:2014-12-13 summary:The purpose of this report is in examining the generalization performance of Support Vector Machines (SVM) as a tool for pattern recognition and object classification. The work is motivated by the growing popularity of the method that is claimed to guarantee a good generalization performance for the task in hand. The method is implemented in MATLAB. SVMs based on various kernels are tested for classifying data from various domains. version:1
arxiv-1412-4183 | A survey of modern optical character recognition techniques | http://arxiv.org/abs/1412.4183 | id:1412.4183 author:Eugene Borovikov category:cs.CV 62-04  published:2014-12-13 summary:This report explores the latest advances in the field of digital document recognition. With the focus on printed document imagery, we discuss the major developments in optical character recognition (OCR) and document image enhancement/restoration in application to Latin and non-Latin scripts. In addition, we review and discuss the available technologies for hand-written document recognition. In this report, we also provide some company-accumulated benchmark results on available OCR engines. version:1
arxiv-1412-4182 | The Statistics of Streaming Sparse Regression | http://arxiv.org/abs/1412.4182 | id:1412.4182 author:Jacob Steinhardt, Stefan Wager, Percy Liang category:math.ST cs.LG stat.ML stat.TH  published:2014-12-13 summary:We present a sparse analogue to stochastic gradient descent that is guaranteed to perform well under similar conditions to the lasso. In the linear regression setup with irrepresentable noise features, our algorithm recovers the support set of the optimal parameter vector with high probability, and achieves a statistically quasi-optimal rate of convergence of Op(k log(d)/T), where k is the sparsity of the solution, d is the number of features, and T is the number of training examples. Meanwhile, our algorithm does not require any more computational resources than stochastic gradient descent. In our experiments, we find that our method substantially out-performs existing streaming algorithms on both real and simulated data. version:1
arxiv-1412-4175 | Optimizing Over Radial Kernels on Compact Manifolds | http://arxiv.org/abs/1412.4175 | id:1412.4175 author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV  published:2014-12-13 summary:We tackle the problem of optimizing over all possible positive definite radial kernels on Riemannian manifolds for classification. Kernel methods on Riemannian manifolds have recently become increasingly popular in computer vision. However, the number of known positive definite kernels on manifolds remain very limited. Furthermore, most kernels typically depend on at least one parameter that needs to be tuned for the problem at hand. A poor choice of kernel, or of parameter value, may yield significant performance drop-off. Here, we show that positive definite radial kernels on the unit n-sphere, the Grassmann manifold and Kendall's shape manifold can be expressed in a simple form whose parameters can be automatically optimized within a support vector machine framework. We demonstrate the benefits of our kernel learning algorithm on object, face, action and shape recognition. version:1
arxiv-1412-3714 | Feature Weight Tuning for Recursive Neural Networks | http://arxiv.org/abs/1412.3714 | id:1412.3714 author:Jiwei Li category:cs.NE cs.AI cs.CL cs.LG  published:2014-12-11 summary:This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform "weight tuning" for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models. version:2
arxiv-1412-4174 | A Framework for Shape Analysis via Hilbert Space Embedding | http://arxiv.org/abs/1412.4174 | id:1412.4174 author:Sadeep Jayasumana, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV  published:2014-12-13 summary:We propose a framework for 2D shape analysis using positive definite kernels defined on Kendall's shape manifold. Different representations of 2D shapes are known to generate different nonlinear spaces. Due to the nonlinearity of these spaces, most existing shape classification algorithms resort to nearest neighbor methods and to learning distances on shape spaces. Here, we propose to map shapes on Kendall's shape manifold to a high dimensional Hilbert space where Euclidean geometry applies. To this end, we introduce a kernel on this manifold that permits such a mapping, and prove its positive definiteness. This kernel lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the benefits of our approach over the state-of-the-art methods on shape classification, clustering and retrieval. version:1
arxiv-1412-4172 | Kernel Methods on the Riemannian Manifold of Symmetric Positive Definite Matrices | http://arxiv.org/abs/1412.4172 | id:1412.4172 author:Sadeep Jayasumana, Richard Hartley, Mathieu Salzmann, Hongdong Li, Mehrtash Harandi category:cs.CV  published:2014-12-13 summary:Symmetric Positive Definite (SPD) matrices have become popular to encode image information. Accounting for the geometry of the Riemannian manifold of SPD matrices has proven key to the success of many algorithms. However, most existing methods only approximate the true shape of the manifold locally by its tangent plane. In this paper, inspired by kernel methods, we propose to map SPD matrices to a high dimensional Hilbert space where Euclidean geometry applies. To encode the geometry of the manifold in the mapping, we introduce a family of provably positive definite kernels on the Riemannian manifold of SPD matrices. These kernels are derived from the Gaussian ker- nel, but exploit different metrics on the manifold. This lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM and kernel PCA, to the Riemannian manifold of SPD matrices. We demonstrate the benefits of our approach on the problems of pedestrian detection, ob- ject categorization, texture analysis, 2D motion segmentation and Diffusion Tensor Imaging (DTI) segmentation. version:1
arxiv-1412-4128 | Expanded Alternating Optimization of Nonconvex Functions with Applications to Matrix Factorization and Penalized Regression | http://arxiv.org/abs/1412.4128 | id:1412.4128 author:W. James Murdoch, Mu Zhu category:stat.CO stat.ML  published:2014-12-12 summary:We propose a general technique for improving alternating optimization (AO) of nonconvex functions. Starting from the solution given by AO, we conduct another sequence of searches over subspaces that are both meaningful to the optimization problem at hand and different from those used by AO. To demonstrate the utility of our approach, we apply it to the matrix factorization (MF) algorithm for recommender systems and the coordinate descent algorithm for penalized regression (PR), and show meaningful improvements using both real-world (for MF) and simulated (for PR) data sets. Moreover, we demonstrate for MF that, by constructing search spaces customized to the given data set, we can significantly increase the convergence rate of our technique. version:1
arxiv-1412-4102 | Representing Data by a Mixture of Activated Simplices | http://arxiv.org/abs/1412.4102 | id:1412.4102 author:Chunyu Wang, John Flynn, Yizhou Wang, Alan L. Yuille category:cs.CV  published:2014-12-12 summary:We present a new model which represents data as a mixture of simplices. Simplices are geometric structures that generalize triangles. We give a simple geometric understanding that allows us to learn a simplicial structure efficiently. Our method requires that the data are unit normalized (and thus lie on the unit sphere). We show that under this restriction, building a model with simplices amounts to constructing a convex hull inside the sphere whose boundary facets is close to the data. We call the boundary facets of the convex hull that are close to the data Activated Simplices. While the total number of bases used to build the simplices is a parameter of the model, the dimensions of the individual activated simplices are learned from the data. Simplices can have different dimensions, which facilitates modeling of inhomogeneous data sources. The simplicial structure is bounded --- this is appropriate for modeling data with constraints, such as human elbows can not bend more than 180 degrees. The simplices are easy to interpret and extremes within the data can be discovered among the vertices. The method provides good reconstruction and regularization. It supports good nearest neighbor classification and it allows realistic generative models to be constructed. It achieves state-of-the-art results on benchmark datasets, including 3D poses and digits. version:1
arxiv-1406-0924 | Multiscale Fields of Patterns | http://arxiv.org/abs/1406.0924 | id:1406.0924 author:Pedro F. Felzenszwalb, John G. Oberlin category:cs.CV  published:2014-06-04 summary:We describe a framework for defining high-order image models that can be used in a variety of applications. The approach involves modeling local patterns in a multiscale representation of an image. Local properties of a coarsened image reflect non-local properties of the original image. In the case of binary images local properties are defined by the binary patterns observed over small neighborhoods around each pixel. With the multiscale representation we capture the frequency of patterns observed at different scales of resolution. This framework leads to expressive priors that depend on a relatively small number of parameters. For inference and learning we use an MCMC method for block sampling with very large blocks. We evaluate the approach with two example applications. One involves contour detection. The other involves binary segmentation. version:3
arxiv-1412-4080 | Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso | http://arxiv.org/abs/1412.4080 | id:1412.4080 author:Antoine Bonnefoy, Valentin Emiya, Liva Ralaivola, RÃ©mi Gribonval category:stat.ML cs.LG  published:2014-12-12 summary:Recent computational strategies based on screening tests have been proposed to accelerate algorithms addressing penalized sparse regression problems such as the Lasso. Such approaches build upon the idea that it is worth dedicating some small computational effort to locate inactive atoms and remove them from the dictionary in a preprocessing stage so that the regression algorithm working with a smaller dictionary will then converge faster to the solution of the initial problem. We believe that there is an even more efficient way to screen the dictionary and obtain a greater acceleration: inside each iteration of the regression algorithm, one may take advantage of the algorithm computations to obtain a new screening test for free with increasing screening effects along the iterations. The dictionary is henceforth dynamically screened instead of being screened statically, once and for all, before the first iteration. We formalize this dynamic screening principle in a general algorithmic scheme and apply it by embedding inside a number of first-order algorithms adapted existing screening tests to solve the Lasso or new screening tests to solve the Group-Lasso. Computational gains are assessed in a large set of experiments on synthetic data as well as real-world sounds and images. They show both the screening efficiency and the gain in terms running times. version:1
arxiv-1410-3059 | Computabilities of Validity and Satisfiability in Probability Logics over Finite and Countable Models | http://arxiv.org/abs/1410.3059 | id:1410.3059 author:Greg Yang category:cs.LO cs.LG math.LO math.PR  published:2014-10-12 summary:The $\epsilon$-logic (which is called $\epsilon$E-logic in this paper) of Kuyper and Terwijn is a variant of first order logic with the same syntax, in which the models are equipped with probability measures and in which the $\forall x$ quantifier is interpreted as "there exists a set $A$ of measure $\ge 1 - \epsilon$ such that for each $x \in A$, ...." Previously, Kuyper and Terwijn proved that the general satisfiability and validity problems for this logic are, i) for rational $\epsilon \in (0, 1)$, respectively $\Sigma^1_1$-complete and $\Pi^1_1$-hard, and ii) for $\epsilon = 0$, respectively decidable and $\Sigma^0_1$-complete. The adjective "general" here means "uniformly over all languages." We extend these results in the scenario of finite models. In particular, we show that the problems of satisfiability by and validity over finite models in $\epsilon$E-logic are, i) for rational $\epsilon \in (0, 1)$, respectively $\Sigma^0_1$- and $\Pi^0_1$-complete, and ii) for $\epsilon = 0$, respectively decidable and $\Pi^0_1$-complete. Although partial results toward the countable case are also achieved, the computability of $\epsilon$E-logic over countable models still remains largely unsolved. In addition, most of the results, of this paper and of Kuyper and Terwijn, do not apply to individual languages with a finite number of unary predicates. Reducing this requirement continues to be a major point of research. On the positive side, we derive the decidability of the corresponding problems for monadic relational languages --- equality- and function-free languages with finitely many unary and zero other predicates. This result holds for all three of the unrestricted, the countable, and the finite model cases. Applications in computational learning theory, weighted graphs, and neural networks are discussed in the context of these decidability and undecidability results. version:2
arxiv-1411-7942 | Using Sentence Plausibility to Learn the Semantics of Transitive Verbs | http://arxiv.org/abs/1411.7942 | id:1411.7942 author:Tamara Polajnar, Laura Rimell, Stephen Clark category:cs.CL  published:2014-11-28 summary:The functional approach to compositional distributional semantics considers transitive verbs to be linear maps that transform the distributional vectors representing nouns into a vector representing a sentence. We conduct an initial investigation that uses a matrix consisting of the parameters of a logistic regression classifier trained on a plausibility task as a transitive verb function. We compare our method to a commonly used corpus-based method for constructing a verb matrix and find that the plausibility training may be more effective for disambiguation tasks. version:2
arxiv-1412-3958 | An Automatic Seeded Region Growing for 2D Biomedical Image Segmentation | http://arxiv.org/abs/1412.3958 | id:1412.3958 author:Mohammed M. Abdelsamea category:cs.CV  published:2014-12-12 summary:In this paper, an automatic seeded region growing algorithm is proposed for cellular image segmentation. First, the regions of interest (ROIs) extracted from the preprocessed image. Second, the initial seeds are automatically selected based on ROIs extracted from the image. Third, the most reprehensive seeds are selected using a machine learning algorithm. Finally, the cellular image is segmented into regions where each region corresponds to a seed. The aim of the proposed is to automatically extract the Region of Interests (ROI) from the cellular images in terms of overcoming the explosion, under segmentation and over segmentation problems. Experimental results show that the proposed algorithm can improve the segmented image and the segmented results are less noisy as compared to some existing algorithms. version:1
arxiv-1412-3949 | CITlab ARGUS for historical handwritten documents | http://arxiv.org/abs/1412.3949 | id:1412.3949 author:Tobias StrauÃ, Tobias GrÃ¼ning, Gundram Leifert, Roger Labahn, for the University o category:cs.CV cs.NE 68T05  68T10  published:2014-12-12 summary:We describe CITlab's recognition system for the HTRtS competition attached to the 14. International Conference on Frontiers in Handwriting Recognition, ICFHR 2014. The task comprises the recognition of historical handwritten documents. The core algorithms of our system are based on multi-dimensional recurrent neural networks (MDRNN) and connectionist temporal classification (CTC). The software modules behind that as well as the basic utility technologies are essentially powered by PLANET's ARGUS framework for intelligent text recognition and image processing. version:1
arxiv-1408-6257 | Sparse Graph-based Transduction for Image Classification | http://arxiv.org/abs/1408.6257 | id:1408.6257 author:Sheng Huang, Dan Yang, Jia Zhou, Luwen Huangfu, Xiaohong Zhang category:cs.CV  published:2014-08-26 summary:Motivated by the remarkable successes of Graph-based Transduction (GT) and Sparse Representation (SR), we present a novel Classifier named Sparse Graph-based Classifier (SGC) for image classification. In SGC, SR is leveraged to measure the correlation (similarity) of each two samples and a graph is constructed for encoding these correlations. Then the Laplacian eigenmapping is adopted for deriving the graph Laplacian of the graph. Finally, SGC can be obtained by plugging the graph Laplacian into the conventional GT framework. In the image classification procedure, SGC utilizes the correlations, which are encoded in the learned graph Laplacian, to infer the labels of unlabeled images. SGC inherits the merits of both GT and SR. Compared to SR, SGC improves the robustness and the discriminating power of GT. Compared to GT, SGC sufficiently exploits the whole data. Therefore it alleviates the undercomplete dictionary issue suffered by SR. Four popular image databases are employed for evaluation. The results demonstrate that SGC can achieve a promising performance in comparison with the state-of-the-art classifiers, particularly in the small training sample size case and the noisy sample case. version:2
arxiv-1412-3925 | Region segmentation for sparse decompositions: better brain parcellations from rest fMRI | http://arxiv.org/abs/1412.3925 | id:1412.3925 author:Alexandre Abraham, Elvis Dohmatob, Bertrand Thirion, Dimitris Samaras, Gael Varoquaux category:q-bio.NC cs.CV  published:2014-12-12 summary:Functional Magnetic Resonance Images acquired during resting-state provide information about the functional organization of the brain through measuring correlations between brain areas. Independent components analysis is the reference approach to estimate spatial components from weakly structured data such as brain signal time courses; each of these components may be referred to as a brain network and the whole set of components can be conceptualized as a brain functional atlas. Recently, new methods using a sparsity prior have emerged to deal with low signal-to-noise ratio data. However, even when using sophisticated priors, the results may not be very sparse and most often do not separate the spatial components into brain regions. This work presents post-processing techniques that automatically sparsify brain maps and separate regions properly using geometric operations, and compares these techniques according to faithfulness to data and stability metrics. In particular, among threshold-based approaches, hysteresis thresholding and random walker segmentation, the latter improves significantly the stability of both dense and sparse models. version:1
arxiv-1412-3922 | Size sensitive packing number for Hamming cube and its consequences | http://arxiv.org/abs/1412.3922 | id:1412.3922 author:Kunal Dutta, Arijit Ghosh category:cs.DM cs.CG cs.LG math.CO  published:2014-12-12 summary:We prove a size-sensitive version of Haussler's Packing lemma~\cite{Haussler92spherepacking} for set-systems with bounded primal shatter dimension, which have an additional {\em size-sensitive property}. This answers a question asked by Ezra~\cite{Ezra-sizesendisc-soda-14}. We also partially address another point raised by Ezra regarding overcounting of sets in her chaining procedure. As a consequence of these improvements, we get an improvement on the size-sensitive discrepancy bounds for set systems with the above property. Improved bounds on the discrepancy for these special set systems also imply an improvement in the sizes of {\em relative $(\varepsilon, \delta)$-approximations} and $(\nu, \alpha)$-samples. version:1
arxiv-1412-3919 | Machine Learning for Neuroimaging with Scikit-Learn | http://arxiv.org/abs/1412.3919 | id:1412.3919 author:Alexandre Abraham, Fabian Pedregosa, Michael Eickenberg, Philippe Gervais, Andreas Muller, Jean Kossaifi, Alexandre Gramfort, Bertrand Thirion, GÃ¤el Varoquaux category:cs.LG cs.CV stat.ML  published:2014-12-12 summary:Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g. multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g. resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain. version:1
arxiv-1412-3914 | Edge Preserving Multi-Modal Registration Based On Gradient Intensity Self-Similarity | http://arxiv.org/abs/1412.3914 | id:1412.3914 author:Tamar Rott, Dorin Shriki, Tamir Bendory category:cs.CV  published:2014-12-12 summary:Image registration is a challenging task in the world of medical imaging. Particularly, accurate edge registration plays a central role in a variety of clinical conditions. The Modality Independent Neighbourhood Descriptor (MIND) demonstrates state of the art alignment, based on the image self-similarity. However, this method appears to be less accurate regarding edge registration. In this work, we propose a new registration method, incorporating gradient intensity and MIND self-similarity metric. Experimental results show the superiority of this method in edge registration tasks, while preserving the original MIND performance for other image features and textures. version:1
arxiv-1412-2863 | Score Function Features for Discriminative Learning: Matrix and Tensor Framework | http://arxiv.org/abs/1412.2863 | id:1412.2863 author:Majid Janzamin, Hanie Sedghi, Anima Anandkumar category:cs.LG stat.ML  published:2014-12-09 summary:Feature learning forms the cornerstone for tackling challenging learning problems in domains such as speech, computer vision and natural language processing. In this paper, we consider a novel class of matrix and tensor-valued features, which can be pre-trained using unlabeled samples. We present efficient algorithms for extracting discriminative information, given these pre-trained features and labeled samples for any related task. Our class of features are based on higher-order score functions, which capture local variations in the probability density function of the input. We establish a theoretical framework to characterize the nature of discriminative information that can be extracted from score-function features, when used in conjunction with labeled samples. We employ efficient spectral decomposition algorithms (on matrices and tensors) for extracting discriminative components. The advantage of employing tensor-valued features is that we can extract richer discriminative information in the form of an overcomplete representations. Thus, we present a novel framework for employing generative models of the input for discriminative learning. version:2
arxiv-1412-3635 | Simulating a perceptron on a quantum computer | http://arxiv.org/abs/1412.3635 | id:1412.3635 author:Maria Schuld, Ilya Sinayskiy, Francesco Petruccione category:quant-ph cs.LG cs.NE  published:2014-12-11 summary:Perceptrons are the basic computational unit of artificial neural networks, as they model the activation mechanism of an output neuron due to incoming signals from its neighbours. As linear classifiers, they play an important role in the foundations of machine learning. In the context of the emerging field of quantum machine learning, several attempts have been made to develop a corresponding unit using quantum information theory. Based on the quantum phase estimation algorithm, this paper introduces a quantum perceptron model imitating the step-activation function of a classical perceptron. This scheme requires resources in $\mathcal{O}(n)$ (where $n$ is the size of the input) and promises efficient applications for more complex structures such as trainable quantum neural networks. version:1
arxiv-1412-3617 | Efficient penalty search for multiple changepoint problems | http://arxiv.org/abs/1412.3617 | id:1412.3617 author:Kaylea Haynes, Idris A. Eckley, Paul Fearnhead category:stat.CO stat.ML  published:2014-12-11 summary:In the multiple changepoint setting, various search methods have been proposed which involve optimising either a constrained or penalised cost function over possible numbers and locations of changepoints using dynamic programming. Such methods are typically computationally intensive. Recent work in the penalised optimisation setting has focussed on developing a pruning-based approach which gives an improved computational cost that, under certain conditions, is linear in the number of data points. Such an approach naturally requires the specification of a penalty to avoid under/over-fitting. Work has been undertaken to identify the appropriate penalty choice for data generating processes with known distributional form, but in many applications the model assumed for the data is not correct and these penalty choices are not always appropriate. Consequently it is desirable to have an approach that enables us to compare segmentations for different choices of penalty. To this end we present a method to obtain optimal changepoint segmentations of data sequences for all penalty values across a continuous range. This permits an evaluation of the various segmentations to identify a suitably parsimonious penalty choice. The computational complexity of this approach can be linear in the number of data points and linear in the difference between the number of changepoints in the optimal segmentations for the smallest and largest penalty values. This can be orders of magnitude faster than alternative approaches that find optimal segmentations for a range of the number of changepoints. version:1
arxiv-1412-4031 | High-level numerical simulations of noise in CCD and CMOS photosensors: review and tutorial | http://arxiv.org/abs/1412.4031 | id:1412.4031 author:Mikhail Konnik, James Welsh category:astro-ph.IM cs.CV  published:2014-12-11 summary:In many applications, such as development and testing of image processing algorithms, it is often necessary to simulate images containing realistic noise from solid-state photosensors. A high-level model of CCD and CMOS photosensors based on a literature review is formulated in this paper. The model includes photo-response non-uniformity, photon shot noise, dark current Fixed Pattern Noise, dark current shot noise, offset Fixed Pattern Noise, source follower noise, sense node reset noise, and quantisation noise. The model also includes voltage-to-voltage, voltage-to-electrons, and analogue-to-digital converter non-linearities. The formulated model can be used to create synthetic images for testing and validation of image processing algorithms in the presence of realistic images noise. An example of the simulated CMOS photosensor and a comparison with a custom-made CMOS hardware sensor is presented. Procedures for characterisation from both light and dark noises are described. Experimental results that confirm the validity of the numerical model are provided. The paper addresses the issue of the lack of comprehensive high-level photosensor models that enable engineers to simulate realistic effects of noise on the images obtained from solid-state photosensors. version:1
arxiv-1412-3555 | Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling | http://arxiv.org/abs/1412.3555 | id:1412.3555 author:Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio category:cs.NE cs.LG  published:2014-12-11 summary:In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM. version:1
arxiv-1501-01209 | Reinforcement Learning and Nonparametric Detection of Game-Theoretic Equilibrium Play in Social Networks | http://arxiv.org/abs/1501.01209 | id:1501.01209 author:Omid Namvar Gharehshiran, William Hoiles, Vikram Krishnamurthy category:cs.GT cs.LG cs.SI stat.ML  published:2014-12-11 summary:This paper studies two important signal processing aspects of equilibrium behavior in non-cooperative games arising in social networks, namely, reinforcement learning and detection of equilibrium play. The first part of the paper presents a reinforcement learning (adaptive filtering) algorithm that facilitates learning an equilibrium by resorting to diffusion cooperation strategies in a social network. Agents form homophilic social groups, within which they exchange past experiences over an undirected graph. It is shown that, if all agents follow the proposed algorithm, their global behavior is attracted to the correlated equilibria set of the game. The second part of the paper provides a test to detect if the actions of agents are consistent with play from the equilibrium of a concave potential game. The theory of revealed preference from microeconomics is used to construct a non-parametric decision test and statistical test which only require the probe and associated actions of agents. A stochastic gradient algorithm is given to optimize the probe in real time to minimize the Type-II error probabilities of the detection test subject to specified Type-I error probability. We provide a real-world example using the energy market, and a numerical example to detect malicious agents in an online social network. version:1
arxiv-1405-5170 | The ROMES method for statistical modeling of reduced-order-model error | http://arxiv.org/abs/1405.5170 | id:1405.5170 author:Martin Drohmann, Kevin Carlberg category:cs.NA math.NA stat.ML 65G99  65Y20  62M86  published:2014-05-20 summary:This work presents a technique for statistically modeling errors introduced by reduced-order models. The method employs Gaussian-process regression to construct a mapping from a small number of computationally inexpensive `error indicators' to a distribution over the true error. The variance of this distribution can be interpreted as the (epistemic) uncertainty introduced by the reduced-order model. To model normed errors, the method employs existing rigorous error bounds and residual norms as indicators; numerical experiments show that the method leads to a near-optimal expected effectivity in contrast to typical error bounds. To model errors in general outputs, the method uses dual-weighted residuals---which are amenable to uncertainty control---as indicators. Experiments illustrate that correcting the reduced-order-model output with this surrogate can improve prediction accuracy by an order of magnitude; this contrasts with existing `multifidelity correction' approaches, which often fail for reduced-order models and suffer from the curse of dimensionality. The proposed error surrogates also lead to a notion of `probabilistic rigor', i.e., the surrogate bounds the error with specified probability. version:3
arxiv-1412-3474 | Deep Domain Confusion: Maximizing for Domain Invariance | http://arxiv.org/abs/1412.3474 | id:1412.3474 author:Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell category:cs.CV  published:2014-12-10 summary:Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task. version:1
arxiv-1412-8109 | Complex support vector machines regression for robust channel estimation in LTE downlink system | http://arxiv.org/abs/1412.8109 | id:1412.8109 author:Anis Charrada, Abdelaziz Samet category:cs.IT cs.LG math.IT  published:2014-12-10 summary:In this paper, the problem of channel estimation for LTE Downlink system in the environment of high mobility presenting non-Gaussian impulse noise interfering with reference signals is faced. The estimation of the frequency selective time varying multipath fading channel is performed by using a channel estimator based on a nonlinear complex Support Vector Machine Regression (SVR) which is applied to Long Term Evolution (LTE) downlink. The estimation algorithm makes use of the pilot signals to estimate the total frequency response of the highly selective fading multipath channel. Thus, the algorithm maps trained data into a high dimensional feature space and uses the structural risk minimization principle to carry out the regression estimation for the frequency response function of the fading channel. The obtained results show the effectiveness of the proposed method which has better performance than the conventional Least Squares (LS) and Decision Feedback methods to track the variations of the fading multipath channel. version:1
arxiv-1412-3411 | GP-select: Accelerating EM using adaptive subspace preselection | http://arxiv.org/abs/1412.3411 | id:1412.3411 author:Jacquelyn A. Shelton, Jan Gasthaus, Zhenwen Dai, Joerg Luecke, Arthur Gretton category:stat.ML cs.LG  published:2014-12-10 summary:We propose a nonparametric procedure to achieve fast inference in generative graphical models when the number of latent states is very large. The approach is based on iterative latent variable preselection, where we alternate between learning a 'selection function' to reveal the relevant latent variables, and use this to obtain a compact approximation of the posterior distribution for EM; this can make inference possible where the number of possible latent states is e.g. exponential in the number of latent variables, whereas an exact approach would be computationally unfeasible. We learn the selection function entirely from the observed data and current EM state via Gaussian process regression: this is by contrast with earlier approaches, where selections were hand-designed for each problem setting. We show our approach to perform as well as these bespoke selection functions on a wide variety of inference problems: in particular, for the challenging case of a hierarchical model for object localization with occlusion, we achieve results that match a customized state-of-the-art selection method, at a far lower computational cost. version:1
arxiv-1412-3684 | Object Recognition Using Deep Neural Networks: A Survey | http://arxiv.org/abs/1412.3684 | id:1412.3684 author:Soren Goyal, Paul Benjamin category:cs.CV cs.LG cs.NE  published:2014-12-10 summary:Recognition of objects using Deep Neural Networks is an active area of research and many breakthroughs have been made in the last few years. The paper attempts to indicate how far this field has progressed. The paper briefly describes the history of research in Neural Networks and describe several of the recent advances in this field. The performances of recently developed Neural Network Algorithm over benchmark datasets have been tabulated. Finally, some the applications of this field have been provided. version:1
arxiv-1304-5530 | Inexact Coordinate Descent: Complexity and Preconditioning | http://arxiv.org/abs/1304.5530 | id:1304.5530 author:Rachael Tappenden, Peter RichtÃ¡rik, Jacek Gondzio category:math.OC cs.AI stat.ML  published:2013-04-19 summary:In this paper we consider the problem of minimizing a convex function using a randomized block coordinate descent method. One of the key steps at each iteration of the algorithm is determining the update to a block of variables. Existing algorithms assume that in order to compute the update, a particular subproblem is solved exactly. In his work we relax this requirement, and allow for the subproblem to be solved inexactly, leading to an inexact block coordinate descent method. Our approach incorporates the best known results for exact updates as a special case. Moreover, these theoretical guarantees are complemented by practical considerations: the use of iterative techniques to determine the update as well as the use of preconditioning for further acceleration. version:2
arxiv-1412-3369 | Candidate Constrained CRFs for Loss-Aware Structured Prediction | http://arxiv.org/abs/1412.3369 | id:1412.3369 author:Faruk Ahmed, Daniel Tarlow, Dhruv Batra category:cs.CV  published:2014-12-10 summary:When evaluating computer vision systems, we are often concerned with performance on a task-specific evaluation measure such as the Intersection-Over-Union score used in the PASCAL VOC image segmentation challenge. Ideally, our systems would be tuned specifically to these evaluation measures. However, despite much work on loss-aware structured prediction, top performing systems do not use these techniques. In this work, we seek to address this problem, incorporating loss-aware prediction in a manner that is amenable to the approaches taken by top performing systems. Our main idea is to simultaneously leverage two systems: a highly tuned pipeline system as is found on top of leaderboards, and a traditional CRF. We show how to combine high quality candidate solutions from the pipeline with the probabilistic approach of the CRF that is amenable to loss-aware prediction. The result is that we can use loss-aware prediction methodology to improve performance of the highly tuned pipeline system. version:1
arxiv-1410-5401 | Neural Turing Machines | http://arxiv.org/abs/1410.5401 | id:1410.5401 author:Alex Graves, Greg Wayne, Ivo Danihelka category:cs.NE  published:2014-10-20 summary:We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples. version:2
arxiv-1411-3128 | Deep Multi-Instance Transfer Learning | http://arxiv.org/abs/1411.3128 | id:1411.3128 author:Dimitrios Kotzias, Misha Denil, Phil Blunsom, Nando de Freitas category:cs.LG stat.ML  published:2014-11-12 summary:We present a new approach for transferring knowledge from groups to individuals that comprise them. We evaluate our method in text, by inferring the ratings of individual sentences using full-review ratings. This approach, which combines ideas from transfer learning, deep learning and multi-instance learning, reduces the need for laborious human labelling of fine-grained data when abundant labels are available at the group level. version:2
arxiv-1412-3336 | Statistical Patterns in Written Language | http://arxiv.org/abs/1412.3336 | id:1412.3336 author:DamiÃ¡n H. Zanette category:cs.CL  published:2014-12-10 summary:Quantitative linguistics has been allowed, in the last few decades, within the admittedly blurry boundaries of the field of complex systems. A growing host of applied mathematicians and statistical physicists devote their efforts to disclose regularities, correlations, patterns, and structural properties of language streams, using techniques borrowed from statistics and information theory. Overall, results can still be categorized as modest, but the prospects are promising: medium- and long-range features in the organization of human language -which are beyond the scope of traditional linguistics- have already emerged from this kind of analysis and continue to be reported, contributing a new perspective to our understanding of this most complex communication system. This short book is intended to review some of these recent contributions. version:1
arxiv-1412-3297 | Convergence and rate of convergence of some greedy algorithms in convex optimization | http://arxiv.org/abs/1412.3297 | id:1412.3297 author:Vladimir Temlyakov category:stat.ML math.NA  published:2014-12-10 summary:The paper gives a systematic study of the approximate versions of three greedy-type algorithms that are widely used in convex optimization. By approximate version we mean the one where some of evaluations are made with an error. Importance of such versions of greedy-type algorithms in convex optimization and in approximation theory was emphasized in previous literature. version:1
arxiv-1412-3276 | Generalised Entropy MDPs and Minimax Regret | http://arxiv.org/abs/1412.3276 | id:1412.3276 author:Emmanouil G. Androulakis, Christos Dimitrakakis category:cs.LG stat.ML  published:2014-12-10 summary:Bayesian methods suffer from the problem of how to specify prior beliefs. One interesting idea is to consider worst-case priors. This requires solving a stochastic zero-sum game. In this paper, we extend well-known results from bandit theory in order to discover minimax-Bayes policies and discuss when they are practical. version:1
arxiv-1412-3161 | Object-centric Sampling for Fine-grained Image Classification | http://arxiv.org/abs/1412.3161 | id:1412.3161 author:Xiaoyu Wang, Tianbao Yang, Guobin Chen, Yuanqing Lin category:cs.CV  published:2014-12-10 summary:This paper proposes to go beyond the state-of-the-art deep convolutional neural network (CNN) by incorporating the information from object detection, focusing on dealing with fine-grained image classification. Unfortunately, CNN suffers from over-fiting when it is trained on existing fine-grained image classification benchmarks, which typically only consist of less than a few tens of thousands training images. Therefore, we first construct a large-scale fine-grained car recognition dataset that consists of 333 car classes with more than 150 thousand training images. With this large-scale dataset, we are able to build a strong baseline for CNN with top-1 classification accuracy of 81.6%. One major challenge in fine-grained image classification is that many classes are very similar to each other while having large within-class variation. One contributing factor to the within-class variation is cluttered image background. However, the existing CNN training takes uniform window sampling over the image, acting as blind on the location of the object of interest. In contrast, this paper proposes an \emph{object-centric sampling} (OCS) scheme that samples image windows based on the object location information. The challenge in using the location information lies in how to design powerful object detector and how to handle the imperfectness of detection results. To that end, we design a saliency-aware object detection approach specific for the setting of fine-grained image classification, and the uncertainty of detection results are naturally handled in our OCS scheme. Our framework is demonstrated to be very effective, improving top-1 accuracy to 89.3% (from 81.6%) on the large-scale fine-grained car classification dataset. version:1
arxiv-1412-3159 | Road Detection via On--line Label Transfer | http://arxiv.org/abs/1412.3159 | id:1412.3159 author:JosÃ© M. Ãlvarez, Ferran Diego, Joan Serrat, Antonio M. LÃ³pez category:cs.CV  published:2014-12-10 summary:Vision-based road detection is an essential functionality for supporting advanced driver assistance systems (ADAS) such as road following and vehicle and pedestrian detection. The major challenges of road detection are dealing with shadows and lighting variations and the presence of other objects in the scene. Current road detection algorithms characterize road areas at pixel level and group pixels accordingly. However, these algorithms fail in presence of strong shadows and lighting variations. Therefore, we propose a road detection algorithm based on video alignment. The key idea of the algorithm is to exploit the similarities occurred when a vehicle follows the same trajectory more than once. In this way, road areas are learned in a first ride and then, this road knowledge is used to infer areas depicting drivable road surfaces in subsequent rides. Two different experiments are conducted to validate the proposal on different video sequences taken at different scenarios and different daytime. The former aims to perform on-line road detection. The latter aims to perform off-line road detection and is applied to automatically generate the ground-truth necessary to validate road detection algorithms. Qualitative and quantitative evaluations prove that the proposed algorithm is a valid road detection approach. version:1
arxiv-1412-3100 | Semi-Supervised Learning with Heterophily | http://arxiv.org/abs/1412.3100 | id:1412.3100 author:Wolfgang Gatterbauer category:cs.LG cs.DB  published:2014-12-09 summary:We propose a novel linear semi-supervised learning formulation that is derived from a solid probabilistic framework: belief propagation. We show that our formulation generalizes a number of label propagation algorithms described in the literature by allowing them to propagate generalized assumptions about influences between classes of neighboring nodes. We call this formulation Semi-Supervised Learning with Heterophily (SSL-H). We also show how the affinity matrix can be learned from observed data with a simple convex optimization framework that is inspired by locally linear embedding. We call this approach Linear Heterophily Estimation (LHE). Experiments on synthetic data show that both approaches combined can learn heterophily of a graph with 1M nodes, 10M edges and few labels in under 1min, and give better labeling accuracies than a baseline method in the case of small fraction of explicitly labeled nodes. version:1
arxiv-1412-3078 | Hierarchical Mixture-of-Experts Model for Large-Scale Gaussian Process Regression | http://arxiv.org/abs/1412.3078 | id:1412.3078 author:Jun Wei Ng, Marc Peter Deisenroth category:stat.ML cs.AI cs.LG stat.CO  published:2014-12-09 summary:We propose a practical and scalable Gaussian process model for large-scale nonlinear probabilistic regression. Our mixture-of-experts model is conceptually simple and hierarchically recombines computations for an overall approximation of a full Gaussian process. Closed-form and distributed computations allow for efficient and massive parallelisation while keeping the memory consumption small. Given sufficient computing resources, our model can handle arbitrarily large data sets, without explicit sparse approximations. We provide strong experimental evidence that our model can be applied to large data sets of sizes far beyond millions. Hence, our model has the potential to lay the foundation for general large-scale Gaussian process research. version:1
arxiv-1412-3051 | POPE: Post Optimization Posterior Evaluation of Likelihood Free Models | http://arxiv.org/abs/1412.3051 | id:1412.3051 author:Edward Meeds, Michael Chiang, Mary Lee, Olivier Cinquin, John Lowengrub, Max Welling category:stat.ML q-bio.QM  published:2014-12-09 summary:In many domains, scientists build complex simulators of natural phenomena that encode their hypotheses about the underlying processes. These simulators can be deterministic or stochastic, fast or slow, constrained or unconstrained, and so on. Optimizing the simulators with respect to a set of parameter values is common practice, resulting in a single parameter setting that minimizes an objective subject to constraints. We propose a post optimization posterior analysis that computes and visualizes all the models that can generate equally good or better simulation results, subject to constraints. These optimization posteriors are desirable for a number of reasons among which easy interpretability, automatic parameter sensitivity and correlation analysis and posterior predictive analysis. We develop a new sampling framework based on approximate Bayesian computation (ABC) with one-sided kernels. In collaboration with two groups of scientists we applied POPE to two important biological simulators: a fast and stochastic simulator of stem-cell cycling and a slow and deterministic simulator of tumor growth patterns. version:1
arxiv-1411-5879 | A Unified Semantic Embedding: Relating Taxonomies and Attributes | http://arxiv.org/abs/1411.5879 | id:1411.5879 author:Sung Ju Hwang, Leonid Sigal category:cs.CV  published:2014-11-18 summary:We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be represented by a supercategory + sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition. version:2
arxiv-1412-3009 | Brain Tumor Detection Based on Bilateral Symmetry Information | http://arxiv.org/abs/1412.3009 | id:1412.3009 author:Narkhede Sachin, Deven Shah, Vaishali Khairnar, Sujata Kadu category:cs.CV  published:2014-12-09 summary:Advances in computing technology have allowed researchers across many fields of endeavor to collect and maintain vast amounts of observational statistical data such as clinical data,biological patient data,data regarding access of web sites,financial data,and the like.Brain Magnetic Resonance Imaging(MRI)segmentation is a complex problem in the field of medical imaging despite various presented methods.MR image of human brain can be divided into several sub regions especially soft tissues such as gray matter,white matter and cerebrospinal fluid.Although edge information is the main clue in image segmentation,it can not get a better result in analysis the content of images without combining other information.The segmentation of brain tissue in the magnetic resonance imaging(MRI)is very important for detecting the existence and outlines of tumors.In this paper,an algorithm about segmentation based on the symmetry character of brain MRI image is presented.Our goal is to detect the position and boundary of tumors automatically.Experiments were conducted on real pictures,and the results show that the algorithm is flexible and convenient. version:1
arxiv-1412-2444 | An Approach for Reducing Outliers of Non Local Means Image Denoising Filter | http://arxiv.org/abs/1412.2444 | id:1412.2444 author:Raka Kundu, Amlan Chakrabarti, Prasanna Lenka category:cs.CV  published:2014-12-08 summary:We propose an adaptive approach for non local means (NLM) image filtering termed as non local adaptive clipped means (NLACM), which reduces the effect of outliers and improves the denoising quality as compared to traditional NLM. Common method to neglect outliers from a data population is computation of mean in a range defined by mean and standard deviation. In NLACM we perform the median within the defined range based on statistical estimation of the neighbourhood region of a pixel to be denoised. As parameters of the range are independent of any additional input and is based on local intensity values, hence the approach is adaptive. Experimental results for NLACM show better estimation of true intensity from noisy neighbourhood observation as compared to NLM at high noise levels. We have verified the technique for speckle noise reduction and we have tested it on ultrasound (US) image of lumbar spine. These ultrasound images act as guidance for injection therapy for treatment of lumbar radiculopathy. We believe that the proposed approach for image denoising is first of its kind and its efficiency can be well justified as it shows better performance in image restoration. version:2
arxiv-1412-4005 | Sparsity and adaptivity for the blind separation of partially correlated sources | http://arxiv.org/abs/1412.4005 | id:1412.4005 author:Jerome Bobin, Jeremy Rapin, Anthony Larue, Jean-Luc Starck category:stat.AP cs.LG stat.ML  published:2014-12-09 summary:Blind source separation (BSS) is a very popular technique to analyze multichannel data. In this context, the data are modeled as the linear combination of sources to be retrieved. For that purpose, standard BSS methods all rely on some discrimination principle, whether it is statistical independence or morphological diversity, to distinguish between the sources. However, dealing with real-world data reveals that such assumptions are rarely valid in practice: the signals of interest are more likely partially correlated, which generally hampers the performances of standard BSS methods. In this article, we introduce a novel sparsity-enforcing BSS method coined Adaptive Morphological Component Analysis (AMCA), which is designed to retrieve sparse and partially correlated sources. More precisely, it makes profit of an adaptive re-weighting scheme to favor/penalize samples based on their level of correlation. Extensive numerical experiments have been carried out which show that the proposed method is robust to the partial correlation of sources while standard BSS techniques fail. The AMCA algorithm is evaluated in the field of astrophysics for the separation of physical components from microwave data. version:1
arxiv-1412-2929 | Bayesian Fisher's Discriminant for Functional Data | http://arxiv.org/abs/1412.2929 | id:1412.2929 author:Yao-Hsiang Yang, Lu-Hung Chen, Chieh-Chih Wang, Chu-Song Chen category:cs.LG stat.ML  published:2014-12-09 summary:We propose a Bayesian framework of Gaussian process in order to extend Fisher's discriminant to classify functional data such as spectra and images. The probability structure for our extended Fisher's discriminant is explicitly formulated, and we utilize the smoothness assumptions of functional data as prior probabilities. Existing methods which directly employ the smoothness assumption of functional data can be shown as special cases within this framework given corresponding priors while their estimates of the unknowns are one-step approximations to the proposed MAP estimates. Empirical results on various simulation studies and different real applications show that the proposed method significantly outperforms the other Fisher's discriminant methods for functional data. version:1
arxiv-1406-2227 | Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition | http://arxiv.org/abs/1406.2227 | id:1406.2227 author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV  published:2014-06-09 summary:In this work we present a framework for the recognition of natural scene text. Our framework does not require any human-labelled data, and performs word recognition on the whole image holistically, departing from the character based recognition systems of the past. The deep neural network models at the centre of this framework are trained solely on data produced by a synthetic text generation engine -- synthetic data that is highly realistic and sufficient to replace real data, giving us infinite amounts of training data. This excess of data exposes new possibilities for word recognition models, and here we consider three models, each one "reading" words in a different way: via 90k-way dictionary encoding, character sequence encoding, and bag-of-N-grams encoding. In the scenarios of language based and completely unconstrained text recognition we greatly improve upon state-of-the-art performance on standard datasets, using our fast, simple machinery and requiring zero data-acquisition costs. version:4
arxiv-1310-5791 | ROP: Matrix recovery via rank-one projections | http://arxiv.org/abs/1310.5791 | id:1310.5791 author:T. Tony Cai, Anru Zhang category:math.ST cs.IT math.IT stat.ME stat.ML stat.TH  published:2013-10-22 summary:Estimation of low-rank matrices is of significant interest in a range of contemporary applications. In this paper, we introduce a rank-one projection model for low-rank matrix recovery and propose a constrained nuclear norm minimization method for stable recovery of low-rank matrices in the noisy case. The procedure is adaptive to the rank and robust against small perturbations. Both upper and lower bounds for the estimation accuracy under the Frobenius norm loss are obtained. The proposed estimator is shown to be rate-optimal under certain conditions. The estimator is easy to implement via convex programming and performs well numerically. The techniques and main results developed in the paper also have implications to other related statistical problems. An application to estimation of spiked covariance matrices from one-dimensional random projections is considered. The results demonstrate that it is still possible to accurately estimate the covariance matrix of a high-dimensional distribution based only on one-dimensional projections. version:3
arxiv-1412-2873 | Cancer Detection with Multiple Radiologists via Soft Multiple Instance Logistic Regression and $L_1$ Regularization | http://arxiv.org/abs/1412.2873 | id:1412.2873 author:Inna Stainvas, Alexandra Manevitch, Isaac Leichter category:cs.CV  published:2014-12-09 summary:This paper deals with the multiple annotation problem in medical application of cancer detection in digital images. The main assumption is that though images are labeled by many experts, the number of images read by the same expert is not large. Thus differing with the existing work on modeling each expert and ground truth simultaneously, the multi annotation information is used in a soft manner. The multiple labels from different experts are used to estimate the probability of the findings to be marked as malignant. The learning algorithm minimizes the Kullback Leibler (KL) divergence between the modeled probabilities and desired ones constraining the model to be compact. The probabilities are modeled by logit regression and multiple instance learning concept is used by us. Experiments on a real-life computer aided diagnosis (CAD) problem for CXR CAD lung cancer detection demonstrate that the proposed algorithm leads to similar results as learning with a binary RVMMIL classifier or a mixture of binary RVMMIL models per annotator. However, this model achieves a smaller complexity and is more preferable in practice. version:1
arxiv-1407-8147 | Stochastic Coordinate Coding and Its Application for Drosophila Gene Expression Pattern Annotation | http://arxiv.org/abs/1407.8147 | id:1407.8147 author:Binbin Lin, Qingyang Li, Qian Sun, Ming-Jun Lai, Ian Davidson, Wei Fan, Jieping Ye category:cs.LG cs.CE  published:2014-07-30 summary:\textit{Drosophila melanogaster} has been established as a model organism for investigating the fundamental principles of developmental gene interactions. The gene expression patterns of \textit{Drosophila melanogaster} can be documented as digital images, which are annotated with anatomical ontology terms to facilitate pattern discovery and comparison. The automated annotation of gene expression pattern images has received increasing attention due to the recent expansion of the image database. The effectiveness of gene expression pattern annotation relies on the quality of feature representation. Previous studies have demonstrated that sparse coding is effective for extracting features from gene expression images. However, solving sparse coding remains a computationally challenging problem, especially when dealing with large-scale data sets and learning large size dictionaries. In this paper, we propose a novel algorithm to solve the sparse coding problem, called Stochastic Coordinate Coding (SCC). The proposed algorithm alternatively updates the sparse codes via just a few steps of coordinate descent and updates the dictionary via second order stochastic gradient descent. The computational cost is further reduced by focusing on the non-zero components of the sparse codes and the corresponding columns of the dictionary only in the updating procedure. Thus, the proposed algorithm significantly improves the efficiency and the scalability, making sparse coding applicable for large-scale data sets and large dictionary sizes. Our experiments on Drosophila gene expression data sets demonstrate the efficiency and the effectiveness of the proposed algorithm. version:2
arxiv-1412-2066 | Learning Multi-target Tracking with Quadratic Object Interactions | http://arxiv.org/abs/1412.2066 | id:1412.2066 author:Shaofei Wang, Charless C. Fowlkes category:cs.CV cs.LG  published:2014-12-05 summary:We describe a model for multi-target tracking based on associating collections of candidate detections across frames of a video. In order to model pairwise interactions between different tracks, such as suppression of overlapping tracks and contextual cues about co-occurence of different objects, we augment a standard min-cost flow objective with quadratic terms between detection variables. We learn the parameters of this model using structured prediction and a loss function which approximates the multi-target tracking accuracy. We evaluate two different approaches to finding an optimal set of tracks under model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves equivalent performance to the LP relaxation while being 2-7x faster than a commercial solver. The resulting model with learned parameters outperforms existing methods across several categories on the KITTI tracking benchmark. version:2
arxiv-1412-2859 | Circumventing the Curse of Dimensionality in Prediction: Causal Rate-Distortion for Infinite-Order Markov Processes | http://arxiv.org/abs/1412.2859 | id:1412.2859 author:Sarah Marzen, James P. Crutchfield category:cond-mat.stat-mech cs.LG nlin.CD q-bio.NC stat.ML  published:2014-12-09 summary:Predictive rate-distortion analysis suffers from the curse of dimensionality: clustering arbitrarily long pasts to retain information about arbitrarily long futures requires resources that typically grow exponentially with length. The challenge is compounded for infinite-order Markov processes, since conditioning on finite sequences cannot capture all of their past dependencies. Spectral arguments show that algorithms which cluster finite-length sequences fail dramatically when the underlying process has long-range temporal correlations and can fail even for processes generated by finite-memory hidden Markov models. We circumvent the curse of dimensionality in rate-distortion analysis of infinite-order processes by casting predictive rate-distortion objective functions in terms of the forward- and reverse-time causal states of computational mechanics. Examples demonstrate that the resulting causal rate-distortion theory substantially improves current predictive rate-distortion analyses. version:1
arxiv-1410-7835 | Fast Learning of Relational Dependency Networks | http://arxiv.org/abs/1410.7835 | id:1410.7835 author:Oliver Schulte, Zhensong Qian, Arthur E. Kirkpatrick, Xiaoqian Yin, Yan Sun category:cs.LG  published:2014-10-28 summary:A Relational Dependency Network (RDN) is a directed graphical model widely used for multi-relational data. These networks allow cyclic dependencies, necessary to represent relational autocorrelations. We describe an approach for learning both the RDN's structure and its parameters, given an input relational database: First learn a Bayesian network (BN), then transform the Bayesian network to an RDN. Thus fast Bayes net learning can provide fast RDN learning. The BN-to-RDN transform comprises a simple, local adjustment of the Bayes net structure and a closed-form transform of the Bayes net parameters. This method can learn an RDN for a dataset with a million tuples in minutes. We empirically compare our approach to state-of-the art RDN learning methods that use functional gradient boosting, on five benchmark datasets. Learning RDNs via BNs scales much better to large datasets than learning RDNs with boosting, and provides competitive accuracy in predictions. version:2
arxiv-1412-2821 | Zipf's Law and the Frequency of Characters or Words of Oracles | http://arxiv.org/abs/1412.2821 | id:1412.2821 author:Xiuli Wang category:cs.CL math.ST stat.TH  published:2014-12-09 summary:The article discusses the frequency of characters of Oracle,concluding that the frequency and the rank of a word or character is fit to Zipf-Mandelboit Law or Zipf's law with three parameters,and figuring out the parameters based on the frequency,and pointing out that what some researchers of Oracle call the assembling on the two ends is just a description by their impression about the Oracle data. version:1
arxiv-1412-5027 | What is a salient object? A dataset and a baseline model for salient object detection | http://arxiv.org/abs/1412.5027 | id:1412.5027 author:Ali Borji category:cs.CV  published:2014-12-08 summary:Salient object detection or salient region detection models, diverging from fixation prediction models, have traditionally been dealing with locating and segmenting the most salient object or region in a scene. While the notion of most salient object is sensible when multiple objects exist in a scene, current datasets for evaluation of saliency detection approaches often have scenes with only one single object. We introduce three main contributions in this paper: First, we take an indepth look at the problem of salient object detection by studying the relationship between where people look in scenes and what they choose as the most salient object when they are explicitly asked. Based on the agreement between fixations and saliency judgments, we then suggest that the most salient object is the one that attracts the highest fraction of fixations. Second, we provide two new less biased benchmark datasets containing scenes with multiple objects that challenge existing saliency models. Indeed, we observed a severe drop in performance of 8 state-of-the-art models on our datasets (40% to 70%). Third, we propose a very simple yet powerful model based on superpixels to be used as a baseline for model evaluation and comparison. While on par with the best models on MSRA-5K dataset, our model wins over other models on our data highlighting a serious drawback of existing models, which is convoluting the processes of locating the most salient object and its segmentation. We also provide a review and statistical analysis of some labeled scene datasets that can be used for evaluating salient object detection models. We believe that our work can greatly help remedy the over-fitting of models to existing biased datasets and opens new venues for future research in this fast-evolving field. version:1
arxiv-1412-2812 | Unsupervised Induction of Semantic Roles within a Reconstruction-Error Minimization Framework | http://arxiv.org/abs/1412.2812 | id:1412.2812 author:Ivan Titov, Ehsan Khoddam category:cs.CL cs.AI cs.LG stat.ML  published:2014-12-08 summary:We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages. version:1
arxiv-1410-6853 | Covariance Matrices for Mean Field Variational Bayes | http://arxiv.org/abs/1410.6853 | id:1410.6853 author:Ryan Giordano, Tamara Broderick category:stat.ML cs.LG stat.ME  published:2014-10-24 summary:Mean Field Variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, it is well known that a major failing of MFVB is its (sometimes severe) underestimates of the uncertainty of model variables and lack of information about model variable covariance. We develop a fast, general methodology for exponential families that augments MFVB to deliver accurate uncertainty estimates for model variables -- both for individual variables and coherently across variables. MFVB for exponential families defines a fixed-point equation in the means of the approximating posterior, and our approach yields a covariance estimate by perturbing this fixed point. Inspired by linear response theory, we call our method linear response variational Bayes (LRVB). We demonstrate the accuracy of our method on simulated data sets. version:2
arxiv-1408-2873 | First-Pass Large Vocabulary Continuous Speech Recognition using Bi-Directional Recurrent DNNs | http://arxiv.org/abs/1408.2873 | id:1408.2873 author:Awni Y. Hannun, Andrew L. Maas, Daniel Jurafsky, Andrew Y. Ng category:cs.CL cs.LG cs.NE  published:2014-08-12 summary:We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence. version:2
arxiv-1412-2697 | Image quality assessment measure based on natural image statistics in the Tetrolet domain | http://arxiv.org/abs/1412.2697 | id:1412.2697 author:Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi, Driss Aboutajdine category:cs.CV  published:2014-12-08 summary:This paper deals with a reduced reference (RR) image quality measure based on natural image statistics modeling. For this purpose, Tetrolet transform is used since it provides a convenient way to capture local geometric structures. This transform is applied to both reference and distorted images. Then, Gaussian Scale Mixture (GSM) is proposed to model subbands in order to take account statistical dependencies between tetrolet coefficients. In order to quantify the visual degradation, a measure based on Kullback Leibler Divergence (KLD) is provided. The proposed measure was tested on the Cornell VCL A-57 dataset and compared with other measures according to FR-TV1 VQEG framework. version:1
arxiv-1412-2689 | A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic | http://arxiv.org/abs/1412.2689 | id:1412.2689 author:Ali Aajli, Karim Afdel category:cs.CY cs.AI cs.LG  published:2014-12-08 summary:In recent years, adaptive learning systems rely increasingly on learning hierarchy to customize the educational logic developed in their courses. Most approaches do not consider that the relationships of prerequisites between the skills are fuzzy relationships. In this article, we describe a new approach of a practical application of fuzzy logic techniques to the construction of learning hierarchies. For this, we use a learning hierarchy predefined by one or more experts of a specific field. However, the relationships of prerequisites between the skills in the learning hierarchy are not definitive and they are fuzzy relationships. Indeed, we measure relevance degree of all relationships existing in this learning hierarchy and we try to answer to the following question: Is the relationships of prerequisites predefined in initial learning hierarchy are correctly established or not? version:1
arxiv-1502-07243 | Real-Time System of Hand Detection And Gesture Recognition In Cyber Presence Interactive System For E-Learning | http://arxiv.org/abs/1502.07243 | id:1502.07243 author:Bousaaid Mourad, Ayaou Tarik, Afdel Karim, Estraillier Pascal category:cs.CV  published:2014-12-08 summary:The development of technologies of multimedia, linked to that of Internet and democratization of high outflow, has made henceforth E-learning possible for learners being in virtual classes and geographically distributed. The quality and quantity of asynchronous and synchronous communications are the key elements for E-learning success. It is important to have a propitious supervision to reduce the feeling of isolation in E-learning. This feeling of isolation is among the main causes of loss and high rates of stalling in E-learning. The researches to be conducted in this domain aim to bring solutions of convergence coming from real time image for the capture and recognition of hand gestures. These gestures will be analyzed by the system and transformed as indicator of participation. This latter is displayed in the table of performance of the tutor as a curve according to the time. In case of isolation of learner, the indicator of participation will become red and the tutor will be informed of learners with difficulties to participate during learning session. version:1
arxiv-1412-2672 | When Computer Vision Gazes at Cognition | http://arxiv.org/abs/1412.2672 | id:1412.2672 author:Tao Gao, Daniel Harari, Joshua Tenenbaum, Shimon Ullman category:cs.AI cs.CV  published:2014-12-08 summary:Joint attention is a core, early-developing form of social interaction. It is based on our ability to discriminate the third party objects that other people are looking at. While it has been shown that people can accurately determine whether another person is looking directly at them versus away, little is known about human ability to discriminate a third person gaze directed towards objects that are further away, especially in unconstraint cases where the looker can move her head and eyes freely. In this paper we address this question by jointly exploring human psychophysics and a cognitively motivated computer vision model, which can detect the 3D direction of gaze from 2D face images. The synthesis of behavioral study and computer vision yields several interesting discoveries. (1) Human accuracy of discriminating targets 8{\deg}-10{\deg} of visual angle apart is around 40% in a free looking gaze task; (2) The ability to interpret gaze of different lookers vary dramatically; (3) This variance can be captured by the computational model; (4) Human outperforms the current model significantly. These results collectively show that the acuity of human joint attention is indeed highly impressive, given the computational challenge of the natural looking task. Moreover, the gap between human and model performance, as well as the variability of gaze interpretation across different lookers, require further understanding of the underlying mechanisms utilized by humans for this challenging task. version:1
arxiv-1412-2632 | Probabilistic low-rank matrix completion on finite alphabets | http://arxiv.org/abs/1412.2632 | id:1412.2632 author:Jean Lafond, Olga Klopp, Eric Moulines, Jospeh Salmon category:math.ST stat.ML stat.TH  published:2014-12-08 summary:The task of reconstructing a matrix given a sample of observedentries is known as the matrix completion problem. It arises ina wide range of problems, including recommender systems, collaborativefiltering, dimensionality reduction, image processing, quantum physics or multi-class classificationto name a few. Most works have focused on recovering an unknown real-valued low-rankmatrix from randomly sub-sampling its entries.Here, we investigate the case where the observations take a finite number of values, corresponding for examples to ratings in recommender systems or labels in multi-class classification.We also consider a general sampling scheme (not necessarily uniform) over the matrix entries.The performance of a nuclear-norm penalized estimator is analyzed theoretically.More precisely, we derive bounds for the Kullback-Leibler divergence between the true and estimated distributions.In practice, we have also proposed an efficient algorithm based on lifted coordinate gradient descent in order to tacklepotentially high dimensional settings. version:1
arxiv-1407-1598 | Low Complexity Regularization of Linear Inverse Problems | http://arxiv.org/abs/1407.1598 | id:1407.1598 author:Samuel Vaiter, Gabriel PeyrÃ©, Jalal M. Fadili category:math.OC cs.IT math.IT stat.ML  published:2014-07-07 summary:Inverse problems and regularization theory is a central theme in contemporary signal processing, where the goal is to reconstruct an unknown signal from partial indirect, and possibly noisy, measurements of it. A now standard method for recovering the unknown signal is to solve a convex optimization problem that enforces some prior knowledge about its structure. This has proved efficient in many problems routinely encountered in imaging sciences, statistics and machine learning. This chapter delivers a review of recent advances in the field where the regularization prior promotes solutions conforming to some notion of simplicity/low-complexity. These priors encompass as popular examples sparsity and group sparsity (to capture the compressibility of natural signals and images), total variation and analysis sparsity (to promote piecewise regularity), and low-rank (as natural extension of sparsity to matrix-valued data). Our aim is to provide a unified treatment of all these regularizations under a single umbrella, namely the theory of partial smoothness. This framework is very general and accommodates all low-complexity regularizers just mentioned, as well as many others. Partial smoothness turns out to be the canonical way to encode low-dimensional models that can be linear spaces or more general smooth manifolds. This review is intended to serve as a one stop shop toward the understanding of the theoretical properties of the so-regularized solutions. It covers a large spectrum including: (i) recovery guarantees and stability to noise, both in terms of $\ell^2$-stability and model (manifold) identification; (ii) sensitivity analysis to perturbations of the parameters involved (in particular the observations), with applications to unbiased risk estimation ; (iii) convergence properties of the forward-backward proximal splitting scheme, that is particularly well suited to solve the corresponding large-scale regularized optimization problem. version:2
arxiv-1412-3352 | Web image annotation by diffusion maps manifold learning algorithm | http://arxiv.org/abs/1412.3352 | id:1412.3352 author:Neda Pourali category:cs.CV cs.IR cs.LG 68T10  published:2014-12-08 summary:Automatic image annotation is one of the most challenging problems in machine vision areas. The goal of this task is to predict number of keywords automatically for images captured in real data. Many methods are based on visual features in order to calculate similarities between image samples. But the computation cost of these approaches is very high. These methods require many training samples to be stored in memory. To lessen this burden, a number of techniques have been developed to reduce the number of features in a dataset. Manifold learning is a popular approach to nonlinear dimensionality reduction. In this paper, we investigate Diffusion maps manifold learning method for web image auto-annotation task. Diffusion maps manifold learning method is used to reduce the dimension of some visual features. Extensive experiments and analysis on NUS-WIDE-LITE web image dataset with different visual features show how this manifold learning dimensionality reduction method can be applied effectively to image annotation. version:1
arxiv-1412-2486 | Optimization models of natural communication | http://arxiv.org/abs/1412.2486 | id:1412.2486 author:Ramon Ferrer-i-Cancho category:physics.soc-ph cs.CL physics.data-an  published:2014-12-08 summary:A family of information theoretic models of communication was introduced more than a decade ago to explain the origins of Zipf's law for word frequencies. The family is a based on a combination of two information theoretic principles: maximization of mutual information between forms and meanings and minimization of form entropy. The family also sheds light on the origins of three other patterns: the principle of contrast, a related a vocabulary learning bias and the meaning-frequency law. Here two important components of the family, namely the information theoretic principles and the energy function that combines them linearly, are reviewed from the perspective of psycholinguistics, language learning, information theory and synergetic linguistics. The minimization of this linear function resembles a sort of agnostic information theoretic model selection that might be tuned by self-organization. version:1
arxiv-1412-2485 | Accurate Streaming Support Vector Machines | http://arxiv.org/abs/1412.2485 | id:1412.2485 author:Vikram Nathan, Sharath Raghvendra category:cs.LG  published:2014-12-08 summary:A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the "maximum margin" linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation. version:1
arxiv-1311-6556 | Double Ramp Loss Based Reject Option Classifier | http://arxiv.org/abs/1311.6556 | id:1311.6556 author:Naresh Manwani, Kalpit Desai, Sanand Sasidharan, Ramasubramanian Sundararajan category:cs.LG  published:2013-11-26 summary:We consider the problem of learning reject option classifiers. The goodness of a reject option classifier is quantified using $0-d-1$ loss function wherein a loss $d \in (0,.5)$ is assigned for rejection. In this paper, we propose {\em double ramp loss} function which gives a continuous upper bound for $(0-d-1)$ loss. Our approach is based on minimizing regularized risk under the double ramp loss using {\em difference of convex (DC) programming}. We show the effectiveness of our approach through experiments on synthetic and benchmark datasets. Our approach performs better than the state of the art reject option classification approaches. version:2
arxiv-1412-2457 | Weighted Polynomial Approximations: Limits for Learning and Pseudorandomness | http://arxiv.org/abs/1412.2457 | id:1412.2457 author:Mark Bun, Thomas Steinke category:cs.CC cs.LG  published:2014-12-08 summary:Polynomial approximations to boolean functions have led to many positive results in computer science. In particular, polynomial approximations to the sign function underly algorithms for agnostically learning halfspaces, as well as pseudorandom generators for halfspaces. In this work, we investigate the limits of these techniques by proving inapproximability results for the sign function. Firstly, the polynomial regression algorithm of Kalai et al. (SIAM J. Comput. 2008) shows that halfspaces can be learned with respect to log-concave distributions on $\mathbb{R}^n$ in the challenging agnostic learning model. The power of this algorithm relies on the fact that under log-concave distributions, halfspaces can be approximated arbitrarily well by low-degree polynomials. We ask whether this technique can be extended beyond log-concave distributions, and establish a negative result. We show that polynomials of any degree cannot approximate the sign function to within arbitrarily low error for a large class of non-log-concave distributions on the real line, including those with densities proportional to $\exp(- x ^{0.99})$. Secondly, we investigate the derandomization of Chernoff-type concentration inequalities. Chernoff-type tail bounds on sums of independent random variables have pervasive applications in theoretical computer science. Schmidt et al. (SIAM J. Discrete Math. 1995) showed that these inequalities can be established for sums of random variables with only $O(\log(1/\delta))$-wise independence, for a tail probability of $\delta$. We show that their results are tight up to constant factors. These results rely on techniques from weighted approximation theory, which studies how well functions on the real line can be approximated by polynomials under various distributions. We believe that these techniques will have further applications in other areas of computer science. version:1
arxiv-1412-2442 | Rediscovering the Alphabet - On the Innate Universal Grammar | http://arxiv.org/abs/1412.2442 | id:1412.2442 author:M. Yahia Kaadan, Asaad Kaadan category:cs.CL  published:2014-12-08 summary:Universal Grammar (UG) theory has been one of the most important research topics in linguistics since introduced five decades ago. UG specifies the restricted set of languages learnable by human brain, and thus, many researchers believe in its biological roots. Numerous empirical studies of neurobiological and cognitive functions of the human brain, and of many natural languages, have been conducted to unveil some aspects of UG. This, however, resulted in different and sometimes contradicting theories that do not indicate a universally unique grammar. In this research, we tackle the UG problem from an entirely different perspective. We search for the Unique Universal Grammar (UUG) that facilitates communication and knowledge transfer, the sole purpose of a language. We formulate this UG and show that it is unique, intrinsic, and cosmic, rather than humanistic. Initial analysis on a widespread natural language already showed some positive results. version:1
arxiv-1412-2378 | Learning Word Representations from Relational Graphs | http://arxiv.org/abs/1412.2378 | id:1412.2378 author:Danushka Bollegala, Takanori Maehara, Yuichi Yoshida, Ken-ichi Kawarabayashi category:cs.CL  published:2014-12-07 summary:Attributes of words and relations between two words are central to numerous tasks in Artificial Intelligence such as knowledge representation, similarity measurement, and analogy detection. Often when two words share one or more attributes in common, they are connected by some semantic relations. On the other hand, if there are numerous semantic relations between two words, we can expect some of the attributes of one of the words to be inherited by the other. Motivated by this close connection between attributes and relations, given a relational graph in which words are inter- connected via numerous semantic relations, we propose a method to learn a latent representation for the individual words. The proposed method considers not only the co-occurrences of words as done by existing approaches for word representation learning, but also the semantic relations in which two words co-occur. To evaluate the accuracy of the word representations learnt using the proposed method, we use the learnt word representations to solve semantic word analogy problems. Our experimental results show that it is possible to learn better word representations by using semantic semantics between words. version:1
arxiv-1410-0781 | SimNets: A Generalization of Convolutional Networks | http://arxiv.org/abs/1410.0781 | id:1410.0781 author:Nadav Cohen, Amnon Shashua category:cs.NE cs.LG  published:2014-10-03 summary:We present a deep layered architecture that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators, one being a similarity function whose family contains the convolution operator used in ConvNets, and the other is a new soft max-min-mean operator called MEX that realizes classical operators like ReLU and max pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Three interesting properties emerge from the architecture: (i) the basic input to hidden layer to output machinery contains as special cases kernel machines with the Exponential and Generalized Gaussian kernels, the output units being "neurons in feature space" (ii) in its general form, the basic machinery has a higher abstraction level than kernel machines, and (iii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are an order of magnitude smaller than comparable ConvNets. version:3
arxiv-1412-5902 | A Physically Inspired Clustering Algorithm: to Evolve Like Particles | http://arxiv.org/abs/1412.5902 | id:1412.5902 author:Teng Qiu, Kaifu Yang, Chaoyi Li, Yongjie Li category:cs.LG cs.CV  published:2014-12-07 summary:Clustering analysis is a method to organize raw data into categories based on a measure of similarity. It has been successfully applied to diverse fields from science to business and engineering. By endowing data points with physical meaning like particles in the physical world and then leaning their evolving tendency of moving from higher to lower potentials, data points in the proposed clustering algorithm sequentially hop to the locations of their transfer points and gather, after a few steps, at the locations of cluster centers with the locally lowest potentials, where cluster members can be easily identified. The whole clustering process is simple and efficient, and can be performed either automatically or interactively, with reliable performances on test data of diverse shapes, attributes, and dimensionalities. version:1
arxiv-1412-2342 | Bayesian Image Restoration for Poisson Corrupted Image using a Latent Variational Method with Gaussian MRF | http://arxiv.org/abs/1412.2342 | id:1412.2342 author:Hayaru Shouno category:cs.CV  published:2014-12-07 summary:We treat an image restoration problem with a Poisson noise chan- nel using a Bayesian framework. The Poisson randomness might be appeared in observation of low contrast object in the field of imaging. The noise observation is often hard to treat in a theo- retical analysis. In our formulation, we interpret the observation through the Poisson noise channel as a likelihood, and evaluate the bound of it with a Gaussian function using a latent variable method. We then introduce a Gaussian Markov random field (GMRF) as the prior for the Bayesian approach, and derive the posterior as a Gaussian distribution. The latent parameters in the likelihood and the hyperparameter in the GMRF prior could be treated as hid- den parameters, so that, we propose an algorithm to infer them in the expectation maximization (EM) framework using loopy belief propagation(LBP). We confirm the ability of our algorithm in the computer simulation, and compare it with the results of other im- age restoration frameworks. version:1
arxiv-1402-1515 | Dictionary Learning over Distributed Models | http://arxiv.org/abs/1402.1515 | id:1402.1515 author:Jianshu Chen, Zaid J. Towfic, Ali H. Sayed category:cs.LG cs.DC  published:2014-02-06 summary:In this paper, we consider learning dictionary models over a network of agents, where each agent is only in charge of a portion of the dictionary elements. This formulation is relevant in Big Data scenarios where large dictionary models may be spread over different spatial locations and it is not feasible to aggregate all dictionaries in one location due to communication and privacy considerations. We first show that the dual function of the inference problem is an aggregation of individual cost functions associated with different agents, which can then be minimized efficiently by means of diffusion strategies. The collaborative inference step generates dual variables that are used by the agents to update their dictionaries without the need to share these dictionaries or even the coefficient models for the training data. This is a powerful property that leads to an effective distributed procedure for learning dictionaries over large networks (e.g., hundreds of agents in our experiments). Furthermore, the proposed learning strategy operates in an online manner and is able to respond to streaming data, where each data sample is presented to the network once. version:2
arxiv-1412-2316 | Iterative Bayesian Reconstruction of Non-IID Block-Sparse Signals | http://arxiv.org/abs/1412.2316 | id:1412.2316 author:Mehdi Korki, Jingxin Zhang, Cishen Zhang, Hadi Zayyani category:stat.ML cs.IT math.IT  published:2014-12-07 summary:This paper presents a novel Block Iterative Bayesian Algorithm (Block-IBA) for reconstructing block-sparse signals with unknown block structures. Unlike the existing algorithms for block sparse signal recovery which assume the cluster structure of the nonzero elements of the unknown signal to be independent and identically distributed (i.i.d.), we use a more realistic Bernoulli-Gaussian hidden Markov model (BGHMM) to characterize the non-i.i.d. block-sparse signals commonly encountered in practice. The Block-IBA iteratively estimates the amplitudes and positions of the block-sparse signal using the steepest-ascent based Expectation-Maximization (EM), and optimally selects the nonzero elements of the block-sparse signal by adaptive thresholding. The global convergence of Block-IBA is analyzed and proved, and the effectiveness of Block-IBA is demonstrated by numerical experiments and simulations on synthetic and real-life data. version:1
arxiv-1403-6212 | Selectable Factor Extraction in High Dimensions | http://arxiv.org/abs/1403.6212 | id:1403.6212 author:Yiyuan She category:stat.ME stat.ML  published:2014-03-25 summary:This paper studies how to perform joint feature selection and extraction for both supervised and unsupervised learning of high-dimensional data. We propose a novel selectable reduced rank regression (SEL-RRR) which can construct multiple explanatory factors from a guaranteed small subset of input features. Sharp oracle inequalities are proved to reveal its power in predictive learning. We develop a fast and simple-to-implement algorithm which provides a computational framework for a wide family of sparsity-inducing penalties. It also adapts to rank constrained variable screening in ultrahigh dimensions. Moreover, a predictive information criterion (PIC) is proposed for model selection, with a theoretical guarantee of achieving the non-asymptotic optimal error rate. Experiments demonstrate the efficacy and efficiency of simultaneous rank reduction and variable selection in various applications. version:2
arxiv-1411-3230 | Sparse Modeling for Image and Vision Processing | http://arxiv.org/abs/1411.3230 | id:1411.3230 author:Julien Mairal, Francis Bach, Jean Ponce category:cs.CV  published:2014-11-12 summary:In recent years, a large amount of multi-disciplinary research has been conducted on sparse models and their applications. In statistics and machine learning, the sparsity principle is used to perform model selection---that is, automatically selecting a simple model among a large collection of them. In signal processing, sparse coding consists of representing data with linear combinations of a few dictionary elements. Subsequently, the corresponding tools have been widely adopted by several scientific communities such as neuroscience, bioinformatics, or computer vision. The goal of this monograph is to offer a self-contained view of sparse modeling for visual recognition and image processing. More specifically, we focus on applications where the dictionary is learned and adapted to data, yielding a compact representation that has been successful in various contexts. version:2
arxiv-1401-7413 | Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted Least Squares Minimization | http://arxiv.org/abs/1401.7413 | id:1401.7413 author:Canyi Lu, Zhouchen Lin, Shuicheng Yan category:cs.LG cs.CV stat.ML  published:2014-01-29 summary:This work presents a general framework for solving the low rank and/or sparse matrix minimization problems, which may involve multiple non-smooth terms. The Iteratively Reweighted Least Squares (IRLS) method is a fast solver, which smooths the objective function and minimizes it by alternately updating the variables and their weights. However, the traditional IRLS can only solve a sparse only or low rank only minimization problem with squared loss or an affine constraint. This work generalizes IRLS to solve joint/mixed low rank and sparse minimization problems, which are essential formulations for many tasks. As a concrete example, we solve the Schatten-$p$ norm and $\ell_{2,q}$-norm regularized Low-Rank Representation (LRR) problem by IRLS, and theoretically prove that the derived solution is a stationary point (globally optimal if $p,q\geq1$). Our convergence proof of IRLS is more general than previous one which depends on the special properties of the Schatten-$p$ norm and $\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data sets demonstrate that our IRLS is much more efficient. version:2
arxiv-1412-2231 | Generalized Singular Value Thresholding | http://arxiv.org/abs/1412.2231 | id:1412.2231 author:Canyi Lu, Changbo Zhu, Chunyan Xu, Shuicheng Yan, Zhouchen Lin category:cs.CV cs.LG cs.NA math.NA  published:2014-12-06 summary:This work studies the Generalized Singular Value Thresholding (GSVT) operator ${\Prox}_{g}^{\bm{\sigma}}(\cdot)$, \begin{equation*} {\Prox}_{g}^{\bm{\sigma}}(\B)=\arg\min\limits_{\X}\sum_{i=1}^{m}g(\sigma_{i}(\X)) + \frac{1}{2} \X-\B _{F}^{2}, \end{equation*} associated with a nonconvex function $g$ defined on the singular values of $\X$. We prove that GSVT can be obtained by performing the proximal operator of $g$ (denoted as $\Prox_g(\cdot)$) on the singular values since $\Prox_g(\cdot)$ is monotone when $g$ is lower bounded. If the nonconvex $g$ satisfies some conditions (many popular nonconvex surrogate functions, e.g., $\ell_p$-norm, $0<p<1$, of $\ell_0$-norm are special cases), a general solver to find $\Prox_g(b)$ is proposed for any $b\geq0$. GSVT greatly generalizes the known Singular Value Thresholding (SVT) which is a basic subroutine in many convex low rank minimization methods. We are able to solve the nonconvex low rank minimization problem by using GSVT in place of SVT. version:1
arxiv-1404-3023 | Markov Chain Analysis of Evolution Strategies on a Linear Constraint Optimization Problem | http://arxiv.org/abs/1404.3023 | id:1404.3023 author:Alexandre Chotard, Anne Auger, Nikolaus Hansen category:cs.NE math.OC  published:2014-04-11 summary:This paper analyses a $(1,\lambda)$-Evolution Strategy, a randomised comparison-based adaptive search algorithm, on a simple constraint optimisation problem. The algorithm uses resampling to handle the constraint and optimizes a linear function with a linear constraint. Two cases are investigated: first the case where the step-size is constant, and second the case where the step-size is adapted using path length control. We exhibit for each case a Markov chain whose stability analysis would allow us to deduce the divergence of the algorithm depending on its internal parameters. We show divergence at a constant rate when the step-size is constant. We sketch that with step-size adaptation geometric divergence takes place. Our results complement previous studies where stability was assumed. version:2
arxiv-1412-2196 | Relations among Some Low Rank Subspace Recovery Models | http://arxiv.org/abs/1412.2196 | id:1412.2196 author:Hongyang Zhang, Zhouchen Lin, Chao Zhang, Junbin Gao category:cs.LG math.OC  published:2014-12-06 summary:Recovering intrinsic low dimensional subspaces from data distributed on them is a key preprocessing step to many applications. In recent years, there has been a lot of work that models subspace recovery as low rank minimization problems. We find that some representative models, such as Robust Principal Component Analysis (R-PCA), Robust Low Rank Representation (R-LRR), and Robust Latent Low Rank Representation (R-LatLRR), are actually deeply connected. More specifically, we discover that once a solution to one of the models is obtained, we can obtain the solutions to other models in closed-form formulations. Since R-PCA is the simplest, our discovery makes it the center of low rank subspace recovery models. Our work has two important implications. First, R-PCA has a solid theoretical foundation. Under certain conditions, we could find better solutions to these low rank models at overwhelming probabilities, although these models are non-convex. Second, we can obtain significantly faster algorithms for these models by solving R-PCA first. The computation cost can be further cut by applying low complexity randomized algorithms, e.g., our novel $\ell_{2,1}$ filtering algorithm, to R-PCA. Experiments verify the advantages of our algorithms over other state-of-the-art ones that are based on the alternating direction method. version:1
arxiv-1412-2186 | Using Artificial Neural Network Techniques for Prediction of Electric Energy Consumption | http://arxiv.org/abs/1412.2186 | id:1412.2186 author:Hasan M. H. Owda, Babatunji Omoniwa, Ahmad R. Shahid, Sheikh Ziauddin category:cs.NE cs.AI  published:2014-12-06 summary:Due to imprecision and uncertainties in predicting real world problems, artificial neural network (ANN) techniques have become increasingly useful for modeling and optimization. This paper presents an artificial neural network approach for forecasting electric energy consumption. For effective planning and operation of power systems, optimal forecasting tools are needed for energy operators to maximize profit and also to provide maximum satisfaction to energy consumers. Monthly data for electric energy consumed in the Gaza strip was collected from year 1994 to 2013. Data was trained and the proposed model was validated using 2-Fold and K-Fold cross validation techniques. The model has been tested with actual energy consumption data and yields satisfactory performance. version:1
arxiv-1406-1485 | Iterative Neural Autoregressive Distribution Estimator (NADE-k) | http://arxiv.org/abs/1406.1485 | id:1406.1485 author:Tapani Raiko, Li Yao, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG  published:2014-06-05 summary:Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in $k$ steps rather than to learn to reconstruct in a single inference step. The proposed model is an unsupervised building block for deep learning that combines the desirable properties of NADE and multi-predictive training: (1) Its test likelihood can be computed analytically, (2) it is easy to generate independent samples from it, and (3) it uses an inference engine that is a superset of variational inference for Boltzmann machines. The proposed NADE-k is competitive with the state-of-the-art in density estimation on the two datasets tested. version:3
arxiv-1411-1434 | On the Information Theoretic Limits of Learning Ising Models | http://arxiv.org/abs/1411.1434 | id:1411.1434 author:Karthikeyan Shanmugam, Rashish Tandon, Alexandros G. Dimakis, Pradeep Ravikumar category:cs.LG  published:2014-11-05 summary:We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting. version:2
arxiv-1412-2106 | Consistent optimization of AMS by logistic loss minimization | http://arxiv.org/abs/1412.2106 | id:1412.2106 author:Wojciech KotÅowski category:cs.LG  published:2014-12-05 summary:In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, a threshold is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting (thresholded) classifier measured with respect to the squared AMS, is upperbounded by the regret of the underlying real-valued function measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS. version:1
arxiv-1406-3010 | "Mental Rotation" by Optimizing Transforming Distance | http://arxiv.org/abs/1406.3010 | id:1406.3010 author:Weiguang Ding, Graham W. Taylor category:cs.LG cs.CV  published:2014-06-11 summary:The human visual system is able to recognize objects despite transformations that can drastically alter their appearance. To this end, much effort has been devoted to the invariance properties of recognition systems. Invariance can be engineered (e.g. convolutional nets), or learned from data explicitly (e.g. temporal coherence) or implicitly (e.g. by data augmentation). One idea that has not, to date, been explored is the integration of latent variables which permit a search over a learned space of transformations. Motivated by evidence that people mentally simulate transformations in space while comparing examples, so-called "mental rotation", we propose a transforming distance. Here, a trained relational model actively transforms pairs of examples so that they are maximally similar in some feature space yet respect the learned transformational constraints. We apply our method to nearest-neighbour problems on the Toronto Face Database and NORB. version:2
arxiv-1412-2041 | Multi-Target Shrinkage | http://arxiv.org/abs/1412.2041 | id:1412.2041 author:Daniel Bartz, Johannes HÃ¶hne, Klaus-Robert MÃ¼ller category:stat.ME stat.ML  published:2014-12-05 summary:Stein showed that the multivariate sample mean is outperformed by "shrinking" to a constant target vector. Ledoit and Wolf extended this approach to the sample covariance matrix and proposed a multiple of the identity as shrinkage target. In a general framework, independent of a specific estimator, we extend the shrinkage concept by allowing simultaneous shrinkage to a set of targets. Application scenarios include settings with (A) additional data sets from potentially similar distributions, (B) non-stationarity, (C) a natural grouping of the data or (D) multiple alternative estimators which could serve as targets. We show that this Multi-Target Shrinkage can be translated into a quadratic program and derive conditions under which the estimation of the shrinkage intensities yields optimal expected squared error in the limit. For the sample mean and the sample covariance as specific instances, we derive conditions under which the optimality of MTS is applicable. We consider two asymptotic settings: the large dimensional limit (LDL), where the dimensionality and the number of observations go to infinity at the same rate, and the finite observations large dimensional limit (FOLDL), where only the dimensionality goes to infinity while the number of observations remains constant. We then show the effectiveness in extensive simulations and on real world data. version:1
arxiv-1412-1957 | CoMIC: Good features for detection and matching at object boundaries | http://arxiv.org/abs/1412.1957 | id:1412.1957 author:Swarna Kamlam Ravindran, Anurag Mittal category:cs.CV  published:2014-12-05 summary:Feature or interest points typically use information aggregation in 2D patches which does not remain stable at object boundaries when there is object motion against a significantly varying background. Level or iso-intensity curves are much more stable under such conditions, especially the longer ones. In this paper, we identify stable portions on long iso-curves and detect corners on them. Further, the iso-curve associated with a corner is used to discard portions from the background and improve matching. Such CoMIC (Corners on Maximally-stable Iso-intensity Curves) points yield superior results at the object boundary regions compared to state-of-the-art detectors while performing comparably at the interior regions as well. This is illustrated in exhaustive matching experiments for both boundary and non-boundary regions in applications such as stereo and point tracking for structure from motion in video sequences. version:1
arxiv-1412-1947 | A parallel sampling based clustering | http://arxiv.org/abs/1412.1947 | id:1412.1947 author:Aditya AV Sastry, Kalyan Netti category:cs.LG 68Q32  published:2014-12-05 summary:The problem of automatically clustering data is an age old problem. People have created numerous algorithms to tackle this problem. The execution time of any of this algorithm grows with the number of input points and the number of cluster centers required. To reduce the number of input points we could average the points locally and use the means or the local centers as the input for clustering. However since the required number of local centers is very high, running the clustering algorithm on the entire dataset to obtain these representational points is very time consuming. To remedy this problem, in this paper we are proposing two subclustering schemes where by we subdivide the dataset into smaller sets and run the clustering algorithm on the smaller datasets to obtain the required number of datapoints to run our clustering algorithm with. As we are subdividing the given dataset, we could run clustering algorithm on each smaller piece of the dataset in parallel. We found that both parallel and serial execution of this method to be much faster than the original clustering algorithm and error in running the clustering algorithm on a reduced set to be very less. version:1
arxiv-1404-3368 | Near-optimal sample compression for nearest neighbors | http://arxiv.org/abs/1404.3368 | id:1404.3368 author:Lee-Ad Gottlieb, Aryeh Kontorovich, Pinhas Nisnevitch category:cs.LG cs.CC  published:2014-04-13 summary:We present the first sample compression algorithm for nearest neighbors with non-trivial performance guarantees. We complement these guarantees by demonstrating almost matching hardness lower bounds, which show that our bound is nearly optimal. Our result yields new insight into margin-based nearest neighbor classification in metric spaces and allows us to significantly sharpen and simplify existing bounds. Some encouraging empirical results are also presented. version:3
arxiv-1412-1927 | Quantile universal threshold: model selection at the detection edge for high-dimensional linear regression | http://arxiv.org/abs/1412.1927 | id:1412.1927 author:Jairo Diaz Rodriguez, Sylvain Sardy category:stat.ML stat.ME  published:2014-12-05 summary:To estimate a sparse linear model from data with Gaussian noise, consilience from lasso and compressed sensing literatures is that thresholding estimators like lasso and the Dantzig selector have the ability in some situations to identify with high probability part of the significant covariates asymptotically, and are numerically tractable thanks to convexity. Yet, the selection of a threshold parameter $\lambda$ remains crucial in practice. To that aim we propose Quantile Universal Thresholding, a selection of $\lambda$ at the detection edge. We show with extensive simulations and real data that an excellent compromise between high true positive rate and low false discovery rate is achieved, leading also to good predictive risk. version:1
arxiv-1412-1908 | Person Re-identification by Saliency Learning | http://arxiv.org/abs/1412.1908 | id:1412.1908 author:Rui Zhao, Wanli Ouyang, Xiaogang Wang category:cs.CV  published:2014-12-05 summary:Human eyes can recognize person identities based on small salient regions, i.e. human saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on human saliency, we propose a novel perspective for person re-identification based on learning human saliency and matching saliency distribution. The proposed saliency learning and matching framework consists of four steps: (1) To handle misalignment caused by drastic viewpoint change and pose variations, we apply adjacency constrained patch matching to build dense correspondence between image pairs. (2) We propose two alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a saliency score for each image patch, through which distinctive features stand out without using identity labels in the training procedure. (3) saliency matching is proposed based on patch matching. Matching patches with inconsistent saliency brings penalty, and images of the same identity are recognized by minimizing the saliency matching cost. (4) Furthermore, saliency matching is tightly integrated with patch matching in a unified structural RankSVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the state-of-the-art person re-identification methods on both datasets. version:1
arxiv-1410-7454 | Deep Structured learning for mass segmentation from Mammograms | http://arxiv.org/abs/1410.7454 | id:1410.7454 author:Neeraj Dhungel, Gustavo Carneiro, Andrew P. Bradley category:cs.CV  published:2014-10-27 summary:In this paper, we present a novel method for the segmentation of breast masses from mammograms exploring structured and deep learning. Specifically, using structured support vector machine (SSVM), we formulate a model that combines different types of potential functions, including one that classifies image regions using deep learning. Our main goal with this work is to show the accuracy and efficiency improvements that these relatively new techniques can provide for the segmentation of breast masses from mammograms. We also propose an easily reproducible quantitative analysis to as- sess the performance of breast mass segmentation methodologies based on widely accepted accuracy and running time measurements on public datasets, which will facilitate further comparisons for this segmentation problem. In particular, we use two publicly available datasets (DDSM-BCRP and INbreast) and propose the computa- tion of the running time taken for the methodology to produce a mass segmentation given an input image and the use of the Dice index to quantitatively measure the segmentation accuracy. For both databases, we show that our proposed methodology produces competitive results in terms of accuracy and running time. version:2
arxiv-1412-1871 | A higher homotopic extension of persistent (co)homology | http://arxiv.org/abs/1412.1871 | id:1412.1871 author:Estanislao Herscovich category:math.AT cs.CG cs.CV math.KT  published:2014-12-05 summary:Our objective in this article is to show a possibly interesting structure of homotopic nature appearing in persistent (co)homology. Assuming that the filtration of the (say) simplicial set embedded in a finite dimensional vector space induces a multiplicative filtration (which would not be a so harsh hypothesis in our setting) on the dg algebra given by the complex of simplicial cochains, we may use a result by T. Kadeishvili to get a unique (up to noncanonical equivalence) A_infinity-algebra structure on the complete persistent cohomology of the filtered simplicial (or topological) set. We then provide a construction of a (pseudo)metric on the set of all (generalized) barcodes (that is, of all cohomological degrees) enriched with the A_infinity-algebra structure stated before, refining the usual bottleneck metric, and which is also independent of the particular A_infinity-algebra structure chosen (among those equivalent to each other). We think that this distance might deserve some attention for topological data analysis, for it in particular can recognize different linking or foldings patterns, as in the Borromean rings. As an aside, we give a simple proof of a result relating the barcode structure between persistent homology and cohomology. This result was observed in a recent article by V. de Silva, D. Morozov and M. Vejdemo-Johansson under some restricted assumptions, which we do not suppose. version:1
arxiv-1408-5845 | Analysis of a Reduced-Communication Diffusion LMS Algorithm | http://arxiv.org/abs/1408.5845 | id:1408.5845 author:Reza Arablouei, Stefan Werner, KutluyÄ±l DoÄanÃ§ay, Yih-Fang Huang category:cs.DC cs.LG cs.SY math.OC  published:2014-08-25 summary:In diffusion-based algorithms for adaptive distributed estimation, each node of an adaptive network estimates a target parameter vector by creating an intermediate estimate and then combining the intermediate estimates available within its closed neighborhood. We analyze the performance of a reduced-communication diffusion least mean-square (RC-DLMS) algorithm, which allows each node to receive the intermediate estimates of only a subset of its neighbors at each iteration. This algorithm eases the usage of network communication resources and delivers a trade-off between estimation performance and communication cost. We show analytically that the RC-DLMS algorithm is stable and convergent in both mean and mean-square senses. We also calculate its theoretical steady-state mean-square deviation. Simulation results demonstrate a good match between theory and experiment. version:2
arxiv-1411-1490 | Efficient Representations for Life-Long Learning and Autoencoding | http://arxiv.org/abs/1411.1490 | id:1411.1490 author:Maria-Florina Balcan, Avrim Blum, Santosh Vempala category:cs.LG  published:2014-11-06 summary:It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, "learning to learn" as they do so. In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal. Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm. Our aim is to learn new internal representations as the algorithm learns new target functions, that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data. We develop efficient algorithms for two very different kinds of commonalities that target functions might share: one based on learning common low-dimensional and unions of low-dimensional subspaces and one based on learning nonlinear Boolean combinations of features. Our algorithms for learning Boolean feature combinations additionally have a dual interpretation, and can be viewed as giving an efficient procedure for constructing near-optimal sparse Boolean autoencoders under a natural "anchor-set" assumption. version:2
arxiv-1412-1842 | Reading Text in the Wild with Convolutional Neural Networks | http://arxiv.org/abs/1412.1842 | id:1412.1842 author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV  published:2014-12-04 summary:In this work we present an end-to-end system for text spotting -- localising and recognising text in natural scene images -- and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query. version:1
arxiv-1412-1788 | Primal-Dual Algorithms for Non-negative Matrix Factorization with the Kullback-Leibler Divergence | http://arxiv.org/abs/1412.1788 | id:1412.1788 author:Felipe Yanez, Francis Bach category:cs.LG math.OC  published:2014-12-04 summary:Non-negative matrix factorization (NMF) approximates a given matrix as a product of two non-negative matrices. Multiplicative algorithms deliver reliable results, but they show slow convergence for high-dimensional data and may be stuck away from local minima. Gradient descent methods have better behavior, but only apply to smooth losses such as the least-squares loss. In this article, we propose a first-order primal-dual algorithm for non-negative decomposition problems (where one factor is fixed) with the KL divergence, based on the Chambolle-Pock algorithm. All required computations may be obtained in closed form and we provide an efficient heuristic way to select step-sizes. By using alternating optimization, our algorithm readily extends to NMF and, on synthetic examples, face recognition or music source separation datasets, it is either faster than existing algorithms, or leads to improved local optima, or both. version:1
arxiv-1411-5271 | Quantifying error in estimates of human brain fiber directions using Earth Mover's Distance | http://arxiv.org/abs/1411.5271 | id:1411.5271 author:Charles Zheng, Franco Pestilli, Ariel Rokem category:stat.ML  published:2014-11-19 summary:Diffusion-weighted MR imaging (DWI) is the only method we currently have to measure connections between different parts of the human brain in vivo. To elucidate the structure of these connections, algorithms for tracking bundles of axonal fibers through the subcortical white matter rely on local estimates of the fiber orientation distribution function (fODF) in different parts of the brain. These functions describe the relative abundance of populations of axonal fibers crossing each other in each location. Multiple models exist for estimating fODFs. The quality of the resulting estimates can be quantified by means of a suitable measure of distance on the space of fODFs. However, there are multiple distance metrics that can be applied for this purpose, including smoothed $L_p$ distances and the Wasserstein metrics. Here, we give four reasons for the use of the Earth Mover's Distance (EMD) equipped with the arc-length, as a distance metric. (continued) version:2
arxiv-1409-1801 | Annotating Synapses in Large EM Datasets | http://arxiv.org/abs/1409.1801 | id:1409.1801 author:Stephen M. Plaza, Toufiq Parag, Gary B. Huang, Donald J. Olbris, Mathew A. Saunders, Patricia K. Rivlin category:q-bio.QM cs.CV q-bio.NC  published:2014-09-05 summary:Reconstructing neuronal circuits at the level of synapses is a central problem in neuroscience and becoming a focus of the emerging field of connectomics. To date, electron microscopy (EM) is the most proven technique for identifying and quantifying synaptic connections. As advances in EM make acquiring larger datasets possible, subsequent manual synapse identification ({\em i.e.}, proofreading) for deciphering a connectome becomes a major time bottleneck. Here we introduce a large-scale, high-throughput, and semi-automated methodology to efficiently identify synapses. We successfully applied our methodology to the Drosophila medulla optic lobe, annotating many more synapses than previous connectome efforts. Our approaches are extensible and will make the often complicated process of synapse identification accessible to a wider-community of potential proofreaders. version:2
arxiv-1412-1710 | Convolutional Neural Networks at Constrained Time Cost | http://arxiv.org/abs/1412.1710 | id:1412.1710 author:Kaiming He, Jian Sun category:cs.CV  published:2014-12-04 summary:Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than "AlexNet" (16.0% top-5 error, 10-view test). version:1
arxiv-1412-1632 | Deep Learning for Answer Sentence Selection | http://arxiv.org/abs/1412.1632 | id:1412.1632 author:Lei Yu, Karl Moritz Hermann, Phil Blunsom, Stephen Pulman category:cs.CL  published:2014-12-04 summary:Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task. version:1
arxiv-1407-5158 | Tight convex relaxations for sparse matrix factorization | http://arxiv.org/abs/1407.5158 | id:1407.5158 author:Emile Richard, Guillaume Obozinski, Jean-Philippe Vert category:stat.ML cs.LG math.ST stat.TH  published:2014-07-19 summary:Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual $\ell\_1$-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results. version:2
arxiv-1412-1602 | End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results | http://arxiv.org/abs/1412.1602 | id:1412.1602 author:Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio category:cs.NE cs.LG stat.ML  published:2014-12-04 summary:We replace the Hidden Markov Model (HMM) which is traditionally used in in continuous speech recognition with a bi-directional recurrent neural network encoder coupled to a recurrent neural network decoder that directly emits a stream of phonemes. The alignment between the input and output sequences is established using an attention mechanism: the decoder emits each symbol based on a context created with a subset of input symbols elected by the attention mechanism. We report initial results demonstrating that this new approach achieves phoneme error rates that are comparable to the state-of-the-art HMM-based decoders, on the TIMIT dataset. version:1
arxiv-1412-1576 | LightLDA: Big Topic Models on Modest Compute Clusters | http://arxiv.org/abs/1412.1576 | id:1412.1576 author:Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng, Eric P. Xing, Tie-Yan Liu, Wei-Ying Ma category:stat.ML cs.DC cs.IR cs.LG  published:2014-12-04 summary:When building large-scale machine learning (ML) programs, such as big topic models or deep neural nets, one usually assumes such tasks can only be attempted with industrial-sized clusters with thousands of nodes, which are out of reach for most practitioners or academic researchers. We consider this challenge in the context of topic modeling on web-scale corpora, and show that with a modest cluster of as few as 8 machines, we can train a topic model with 1 million topics and a 1-million-word vocabulary (for a total of 1 trillion parameters), on a document collection with 200 billion tokens -- a scale not yet reported even with thousands of machines. Our major contributions include: 1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whose running cost is (surprisingly) agnostic of model size, and empirically converges nearly an order of magnitude faster than current state-of-the-art Gibbs samplers; 2) a structure-aware model-parallel scheme, which leverages dependencies within the topic model, yielding a sampling strategy that is frugal on machine memory and network communication; 3) a differential data-structure for model storage, which uses separate data structures for high- and low-frequency words to allow extremely large models to fit in memory, while maintaining high inference speed; and 4) a bounded asynchronous data-parallel scheme, which allows efficient distributed processing of massive data via a parameter server. Our distribution strategy is an instance of the model-and-data-parallel programming model underlying the Petuum framework for general distributed ML, and was implemented on top of the Petuum open-source system. We provide experimental evidence showing how this development puts massive models within reach on a small cluster while still enjoying proportional time cost reductions with increasing cluster size, in comparison with alternative options. version:1
arxiv-1412-1574 | Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking | http://arxiv.org/abs/1412.1574 | id:1412.1574 author:Liming Zhao, Xi Li, Jun Xiao, Fei Wu, Yueting Zhuang category:cs.CV cs.LG  published:2014-12-04 summary:As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker. version:1
arxiv-1412-1463 | On the String Kernel Pre-Image Problem with Applications in Drug Discovery | http://arxiv.org/abs/1412.1463 | id:1412.1463 author:SÃ©bastien GiguÃ¨re, AmÃ©lie Rolland, FranÃ§ois Laviolette, Mario Marchand category:cs.LG cs.CE I.2.6; K.3.2  published:2014-12-03 summary:The pre-image problem has to be solved during inference by most structured output predictors. For string kernels, this problem corresponds to finding the string associated to a given input. An algorithm capable of solving or finding good approximations to this problem would have many applications in computational biology and other fields. This work uses a recent result on combinatorial optimization of linear predictors based on string kernels to develop, for the pre-image, a low complexity upper bound valid for many string kernels. This upper bound is used with success in a branch and bound searching algorithm. Applications and results in the discovery of druggable peptides are presented and discussed. version:2
arxiv-1410-3596 | Detection of cheating by decimation algorithm | http://arxiv.org/abs/1410.3596 | id:1410.3596 author:Shogo Yamanaka, Masayuki Ohzeki, Aurelien Decelle category:stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG  published:2014-10-14 summary:We expand the item response theory to study the case of "cheating students" for a set of exams, trying to detect them by applying a greedy algorithm of inference. This extended model is closely related to the Boltzmann machine learning. In this paper we aim to infer the correct biases and interactions of our model by considering a relatively small number of sets of training data. Nevertheless, the greedy algorithm that we employed in the present study exhibits good performance with a few number of training data. The key point is the sparseness of the interactions in our problem in the context of the Boltzmann machine learning: the existence of cheating students is expected to be very rare (possibly even in real world). We compare a standard approach to infer the sparse interactions in the Boltzmann machine learning to our greedy algorithm and we find the latter to be superior in several aspects. version:2
arxiv-1412-1820 | Context-Dependent Fine-Grained Entity Type Tagging | http://arxiv.org/abs/1412.1820 | id:1412.1820 author:Dan Gillick, Nevena Lazic, Kuzman Ganchev, Jesse Kirchner, David Huynh category:cs.CL  published:2014-12-03 summary:Entity type tagging is the task of assigning category labels to each mention of an entity in a document. While standard systems focus on a small set of types, recent work (Ling and Weld, 2012) suggests that using a large fine-grained label set can lead to dramatic improvements in downstream tasks. In the absence of labeled training data, existing fine-grained tagging systems obtain examples automatically, using resolved entities and their types extracted from a knowledge base. However, since the appropriate type often depends on context (e.g. Washington could be tagged either as city or government), this procedure can result in spurious labels, leading to poorer generalization. We propose the task of context-dependent fine type tagging, where the set of acceptable labels for a mention is restricted to only those deducible from the local context (e.g. sentence or document). We introduce new resources for this task: 11,304 mentions annotated with their context-dependent fine types, and we provide baseline experimental results on this data. version:1
arxiv-1412-1506 | Textural Approach for Mass Abnormality Segmentation in Mammographic Images | http://arxiv.org/abs/1412.1506 | id:1412.1506 author:Khamsa Djaroudib, Abdelmalik Taleb Ahmed, Abdelmadjid Zidani category:cs.CV 68U10  published:2014-12-03 summary:Mass abnormality segmentation is a vital step for the medical diagnostic process and is attracting more and more the interest of many research groups. Currently, most of the works achieved in this area have used the Gray Level Co-occurrence Matrix (GLCM) as texture features with a region-based approach. These features come in previous phase for segmentation stage or are using as inputs to classification stage. The work discussed in this paper attempts to experiment the GLCM method under a contour-based approach. Besides, we experiment the proposed approach on various tissues densities to bring more significant results. At this end, we explored some challenging breast images from BIRADS medical Data Base. Our first experimentations showed promising results with regard to the edges mass segmentation methods. This paper discusses first the main works achieved in this area. Sections 2 and 3 present materials and our methodology. The main results are showed and evaluated before concluding our paper. version:1
arxiv-1406-5429 | Playing with Duality: An Overview of Recent Primal-Dual Approaches for Solving Large-Scale Optimization Problems | http://arxiv.org/abs/1406.5429 | id:1406.5429 author:Nikos Komodakis, Jean-Christophe Pesquet category:cs.NA cs.CV cs.LG math.OC G.1.6; I.4; I.5  published:2014-06-20 summary:Optimization methods are at the core of many problems in signal/image processing, computer vision, and machine learning. For a long time, it has been recognized that looking at the dual of an optimization problem may drastically simplify its solution. Deriving efficient strategies which jointly brings into play the primal and the dual problems is however a more recent idea which has generated many important new contributions in the last years. These novel developments are grounded on recent advances in convex analysis, discrete optimization, parallel processing, and non-smooth optimization with emphasis on sparsity issues. In this paper, we aim at presenting the principles of primal-dual approaches, while giving an overview of numerical methods which have been proposed in different contexts. We show the benefits which can be drawn from primal-dual algorithms both for solving large-scale convex optimization problems and discrete ones, and we provide various application examples to illustrate their usefulness. version:2
arxiv-1412-1443 | Structure learning of antiferromagnetic Ising models | http://arxiv.org/abs/1412.1443 | id:1412.1443 author:Guy Bresler, David Gamarnik, Devavrat Shah category:stat.ML cs.IT cs.LG math.IT  published:2014-12-03 summary:In this paper we investigate the computational complexity of learning the graph structure underlying a discrete undirected graphical model from i.i.d. samples. We first observe that the notoriously difficult problem of learning parities with noise can be captured as a special case of learning graphical models. This leads to an unconditional computational lower bound of $\Omega (p^{d/2})$ for learning general graphical models on $p$ nodes of maximum degree $d$, for the class of so-called statistical algorithms recently introduced by Feldman et al (2013). The lower bound suggests that the $O(p^d)$ runtime required to exhaustively search over neighborhoods cannot be significantly improved without restricting the class of models. Aside from structural assumptions on the graph such as it being a tree, hypertree, tree-like, etc., many recent papers on structure learning assume that the model has the correlation decay property. Indeed, focusing on ferromagnetic Ising models, Bento and Montanari (2009) showed that all known low-complexity algorithms fail to learn simple graphs when the interaction strength exceeds a number related to the correlation decay threshold. Our second set of results gives a class of repelling (antiferromagnetic) models that have the opposite behavior: very strong interaction allows efficient learning in time $O(p^2)$. We provide an algorithm whose performance interpolates between $O(p^2)$ and $O(p^{d+2})$ depending on the strength of the repulsion. version:1
arxiv-1412-1442 | Memory Bounded Deep Convolutional Networks | http://arxiv.org/abs/1412.1442 | id:1412.1442 author:Maxwell D. Collins, Pushmeet Kohli category:cs.CV  published:2014-12-03 summary:In this work, we investigate the use of sparsity-inducing regularizers during training of Convolution Neural Networks (CNNs). These regularizers encourage that fewer connections in the convolution and fully connected layers take non-zero values and in effect result in sparse connectivity between hidden units in the deep network. This in turn reduces the memory and runtime cost involved in deploying the learned CNNs. We show that training with such regularization can still be performed using stochastic gradient descent implying that it can be used easily in existing codebases. Experimental evaluation of our approach on MNIST, CIFAR, and ImageNet datasets shows that our regularizers can result in dramatic reductions in memory requirements. For instance, when applied on AlexNet, our method can reduce the memory consumption by a factor of four with minimal loss in accuracy. version:1
arxiv-1411-7405 | A note relating ridge regression and OLS p-values to preconditioned sparse penalized regression | http://arxiv.org/abs/1411.7405 | id:1411.7405 author:Karl Rohe category:stat.ML stat.ME  published:2014-11-26 summary:When the design matrix has orthonormal columns, "soft thresholding" the ordinary least squares (OLS) solution produces the Lasso solution [Tibshirani, 1996]. If one uses the Puffer preconditioned Lasso [Jia and Rohe, 2012], then this result generalizes from orthonormal designs to full rank designs (Theorem 1). Theorem 2 refines the Puffer preconditioner to make the Lasso select the same model as removing the elements of the OLS solution with the largest p-values. Using a generalized Puffer preconditioner, Theorem 3 relates ridge regression to the preconditioned Lasso; this result is for the high dimensional setting, p > n. Where the standard Lasso is akin to forward selection [Efron et al., 2004], Theorems 1, 2, and 3 suggest that the preconditioned Lasso is more akin to backward elimination. These results hold for sparse penalties beyond l1; for a broad class of sparse and non-convex techniques (e.g. SCAD and MC+), the results hold for all local minima. version:2
arxiv-1412-1370 | Nested Variational Compression in Deep Gaussian Processes | http://arxiv.org/abs/1412.1370 | id:1412.1370 author:James Hensman, Neil D. Lawrence category:stat.ML  published:2014-12-03 summary:Deep Gaussian processes provide a flexible approach to probabilistic modelling of data using either supervised or unsupervised learning. For tractable inference approximations to the marginal likelihood of the model must be made. The original approach to approximate inference in these models used variational compression to allow for approximate variational marginalization of the hidden variables leading to a lower bound on the marginal likelihood of the model [Damianou and Lawrence, 2013]. In this paper we extend this idea with a nested variational compression. The resulting lower bound on the likelihood can be easily parallelized or adapted for stochastic variational inference. version:1
arxiv-1412-1353 | Curriculum Learning of Multiple Tasks | http://arxiv.org/abs/1412.1353 | id:1412.1353 author:Anastasia Pentina, Viktoriia Sharmanska, Christoph H. Lampert category:stat.ML cs.LG  published:2014-12-03 summary:Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover the favourable order of tasks. version:1
arxiv-1412-1342 | A perspective on the advancement of natural language processing tasks via topological analysis of complex networks | http://arxiv.org/abs/1412.1342 | id:1412.1342 author:Diego R. Amancio category:cs.CL  published:2014-12-03 summary:Comment on "Approaching human language with complex networks" by Cong and Liu (Physics of Life Reviews, Volume 11, Issue 4, December 2014, Pages 598-618). version:1
arxiv-1412-1285 | The inductive theory of natural selection | http://arxiv.org/abs/1412.1285 | id:1412.1285 author:Steven A. Frank category:q-bio.PE cs.NE physics.bio-ph  published:2014-12-03 summary:The theory of natural selection has two forms. Deductive theory describes how populations change over time. One starts with an initial population and some rules for change. From those assumptions, one calculates the future state of the population. Deductive theory predicts how populations adapt to environmental challenge. Inductive theory describes the causes of change in populations. One starts with a given amount of change. One then assigns different parts of the total change to particular causes. Inductive theory analyzes alternative causal models for how populations have adapted to environmental challenge. This chapter emphasizes the inductive analysis of cause. version:1
arxiv-1412-1265 | Deeply learned face representations are sparse, selective, and robust | http://arxiv.org/abs/1412.1265 | id:1412.1265 author:Yi Sun, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-12-03 summary:This paper designs a high-performance deep convolutional network (DeepID2+) for face recognition. It is learned with the identification-verification supervisory signal. By increasing the dimension of hidden representations and adding supervision to early convolutional layers, DeepID2+ achieves new state-of-the-art on LFW and YouTube Faces benchmarks. Through empirical studies, we have discovered three properties of its deep neural activations critical for the high performance: sparsity, selectiveness and robustness. (1) It is observed that neural activations are moderately sparse. Moderate sparsity maximizes the discriminative power of the deep net as well as the distance between images. It is surprising that DeepID2+ still can achieve high recognition accuracy even after the neural responses are binarized. (2) Its neurons in higher layers are highly selective to identities and identity-related attributes. We can identify different subsets of neurons which are either constantly excited or inhibited when different identities or attributes are present. Although DeepID2+ is not taught to distinguish attributes during training, it has implicitly learned such high-level concepts. (3) It is much more robust to occlusions, although occlusion patterns are not included in the training set. version:1
arxiv-1412-1219 | Colorisation et texturation temps rÃ©el d'environnements urbains par systÃ¨me mobile avec scanner laser et camÃ©ra fish-eye | http://arxiv.org/abs/1412.1219 | id:1412.1219 author:Jean-Emmanuel Deschaud, Xavier Brun, FranÃ§ois Goulette category:cs.RO cs.CV  published:2014-12-03 summary:We present here a real time mobile mapping system mounted on a vehicle. The terrestrial acquisition system is based on a geolocation system and two sensors, namely, a laser scanner and a camera with a fish-eye lens. We produce 3D colored points cloud and textured models of the environment. Once the system has been calibrated, the data acquisition and processing are done "on the way". This article mainly presents our methods of colorization of point cloud, triangulation and texture mapping. version:1
arxiv-1412-1216 | Simple Two-Dimensional Object Tracking based on a Graph Algorithm | http://arxiv.org/abs/1412.1216 | id:1412.1216 author:Alexandra Heidsieck category:cs.CV  published:2014-12-03 summary:The visual observation and tracking of cells and other micrometer-sized objects has many different biomedical applications. The automation of those tasks based on computer methods helps in the evaluation of such measurements. In this work, we present a general purpose algorithm that excels at evaluating deterministic behavior of micrometer-sized objects. Our concrete application is the tracking of fast moving objects over large distances along deterministic trajectories in a microscopic video. Thereby, we are able to determine characteristic properties of the objects. For this purpose, we use a set of basic algorithms, including blob recognition, feature-based shape recognition and a graph algorithm, and combined them in a novel way. An evaluation of the algorithms performance shows a high accuracy in the recognition of objects as well as of complete trajectories. Moreover, a direct comparison to a similar algorithm shows superior recognition rates. version:1
arxiv-1412-1215 | Mary Astell's words in A Serious Proposal to the Ladies (part I), a lexicographic inquiry with NooJ | http://arxiv.org/abs/1412.1215 | id:1412.1215 author:HÃ©lÃ¨ne Pignot, Odile Piton category:cs.CL  published:2014-12-03 summary:In the following article we elected to study with NooJ the lexis of a 17 th century text, Mary Astell's seminal essay, A Serious Proposal to the Ladies, part I, published in 1694. We first focused on the semantics to see how Astell builds her vindication of the female sex, which words she uses to sensitise women to their alienated condition and promote their education. Then we studied the morphology of the lexemes (which is different from contemporary English) used by the author, thanks to the NooJ tools we have devised for this purpose. NooJ has great functionalities for lexicographic work. Its commands and graphs prove to be most efficient in the spotting of archaic words or variants in spelling. Introduction In our previous articles, we have studied the singularities of 17 th century English within the framework of a diachronic analysis thanks to syntactical and morphological graphs and thanks to the dictionaries we have compiled from a corpus that may be expanded overtime. Our early work was based on a limited corpus of English travel literature to Greece in the 17 th century. This article deals with a late seventeenth century text written by a woman philosopher and essayist, Mary Astell (1666--1731), considered as one of the first English feminists. Astell wrote her essay at a time in English history when women were "the weaker vessel" and their main business in life was to charm and please men by their looks and submissiveness. In this essay we will see how NooJ can help us analyse Astell's rhetoric (what point of view does she adopt, does she speak in her own name, in the name of all women, what is her representation of men and women and their relationships in the text, what are the goals of education?). Then we will turn our attention to the morphology of words in the text and use NooJ commands and graphs to carry out a lexicographic inquiry into Astell's lexemes. version:1
arxiv-1412-1194 | Gradient Boundary Histograms for Action Recognition | http://arxiv.org/abs/1412.1194 | id:1412.1194 author:Feng Shi, Robert Laganiere, Emil Petriu category:cs.CV  published:2014-12-03 summary:This paper introduces a high efficient local spatiotemporal descriptor, called gradient boundary histograms (GBH). The proposed GBH descriptor is built on simple spatio-temporal gradients, which are fast to compute. We demonstrate that it can better represent local structure and motion than other gradient-based descriptors, and significantly outperforms them on large realistic datasets. A comprehensive evaluation shows that the recognition accuracy is preserved while the spatial resolution is greatly reduced, which yields both high efficiency and low memory usage. version:1
arxiv-1412-1138 | Highly comparative fetal heart rate analysis | http://arxiv.org/abs/1412.1138 | id:1412.1138 author:B. D. Fulcher, A. E. Georgieva, C. W. G. Redman, Nick S. Jones category:cs.LG cs.AI q-bio.QM  published:2014-12-03 summary:A database of fetal heart rate (FHR) time series measured from 7221 patients during labor is analyzed with the aim of learning the types of features of these recordings that are informative of low cord pH. Our 'highly comparative' analysis involves extracting over 9000 time-series analysis features from each FHR time series, including measures of autocorrelation, entropy, distribution, and various model fits. This diverse collection of features was developed in previous work, and is publicly available. We describe five features that most accurately classify a balanced training set of 59 'low pH' and 59 'normal pH' FHR recordings. We then describe five of the features with the strongest linear correlation to cord pH across the full dataset of FHR time series. The features identified in this work may be used as part of a system for guiding intervention during labor in future. This work successfully demonstrates the utility of comparing across a large, interdisciplinary literature on time-series analysis to automatically contribute new scientific results for specific biomedical signal processing challenges. version:1
arxiv-1412-1135 | Detector Discovery in the Wild: Joint Multiple Instance and Representation Learning | http://arxiv.org/abs/1412.1135 | id:1412.1135 author:Judy Hoffman, Deepak Pathak, Trevor Darrell, Kate Saenko category:cs.CV  published:2014-12-02 summary:We develop methods for detector learning which exploit joint training over both weak and strong labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. Previous methods for weak-label learning often learn detector models independently using latent variable optimization, but fail to share deep representation knowledge across classes and usually require strong initialization. Other previous methods transfer deep representations from domains with strong labels to those with only weak labels, but do not optimize over individual latent boxes, and thus may miss specific salient structures for a particular category. We propose a model that subsumes these previous approaches, and simultaneously trains a representation and detectors for categories with either weak or strong labels present. We provide a novel formulation of a joint multiple instance learning method that includes examples from classification-style data when available, and also performs domain transfer learning to improve the underlying detector representation. Our model outperforms known methods on ImageNet-200 detection with weak labels. version:1
arxiv-1412-6122 | Spread Unary Coding | http://arxiv.org/abs/1412.6122 | id:1412.6122 author:Subhash Kak category:cs.NE cs.IT math.IT  published:2014-12-02 summary:Unary coding is useful but it is redundant in its standard form. Unary coding can also be seen as spatial coding where the value of the number is determined by its place in an array. Motivated by biological finding that several neurons in the vicinity represent the same number, we propose a variant of unary numeration in its spatial form, where each number is represented by several 1s. We call this spread unary coding where the number of 1s used is the spread of the code. Spread unary coding is associated with saturation of the Hamming distance between code words. version:1
arxiv-1412-1114 | Easy Hyperparameter Search Using Optunity | http://arxiv.org/abs/1412.1114 | id:1412.1114 author:Marc Claesen, Jaak Simm, Dusan Popovic, Yves Moreau, Bart De Moor category:cs.LG  published:2014-12-02 summary:Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net. version:1
arxiv-1310-5665 | Learning Theory and Algorithms for Revenue Optimization in Second-Price Auctions with Reserve | http://arxiv.org/abs/1310.5665 | id:1310.5665 author:Mehryar Mohri, Andres MuÃ±oz Medina category:cs.LG  published:2013-10-21 summary:Second-price auctions with reserve play a critical role for modern search engine and popular online sites since the revenue of these companies often directly de- pends on the outcome of such auctions. The choice of the reserve price is the main mechanism through which the auction revenue can be influenced in these electronic markets. We cast the problem of selecting the reserve price to optimize revenue as a learning problem and present a full theoretical analysis dealing with the complex properties of the corresponding loss function. We further give novel algorithms for solving this problem and report the results of several experiments in both synthetic and real data demonstrating their effectiveness. version:3
arxiv-1412-0439 | Fuzzy human motion analysis: A review | http://arxiv.org/abs/1412.0439 | id:1412.0439 author:Chern Hong Lim, Ekta Vats, Chee Seng Chan category:cs.CV cs.AI  published:2014-12-01 summary:Human Motion Analysis (HMA) is currently one of the most popularly active research domains as such significant research interests are motivated by a number of real world applications such as video surveillance, sports analysis, healthcare monitoring and so on. However, most of these real world applications face high levels of uncertainties that can affect the operations of such applications. Hence, the fuzzy set theory has been applied and showed great success in the recent past. In this paper, we aim at reviewing the fuzzy set oriented approaches for HMA, individuating how the fuzzy set may improve the HMA, envisaging and delineating the future perspectives. To the best of our knowledge, there is not found a single survey in the current literature that has discussed and reviewed fuzzy approaches towards the HMA. For ease of understanding, we conceptually classify the human motion into three broad levels: Low-Level (LoL), Mid-Level (MiL), and High-Level (HiL) HMA. version:2
arxiv-1411-5799 | Group Factor Analysis | http://arxiv.org/abs/1411.5799 | id:1411.5799 author:Arto Klami, Seppo Virtanen, Eemeli LeppÃ¤aho, Samuel Kaski category:stat.ML  published:2014-11-21 summary:Factor analysis provides linear factors that describe relationships between individual variables of a data set. We extend this classical formulation into linear factors that describe relationships between groups of variables, where each group represents either a set of related variables or a data set. The model also naturally extends canonical correlation analysis to more than two sets, in a way that is more flexible than previous extensions. Our solution is formulated as variational inference of a latent variable model with structural sparsity, and it consists of two hierarchical levels: The higher level models the relationships between the groups, whereas the lower models the observed variables given the higher level. We show that the resulting solution solves the group factor analysis problem accurately, outperforming alternative factor analysis based solutions as well as more straightforward implementations of group factor analysis. The method is demonstrated on two life science data sets, one on brain activation and the other on systems biology, illustrating its applicability to the analysis of different types of high-dimensional data sources. version:2
arxiv-1412-1074 | Learning interpretable models of phenotypes from whole genome sequences with the Set Covering Machine | http://arxiv.org/abs/1412.1074 | id:1412.1074 author:Alexandre Drouin, SÃ©bastien GiguÃ¨re, Vladana Sagatovich, Maxime DÃ©raspe, FranÃ§ois Laviolette, Mario Marchand, Jacques Corbeil category:q-bio.GN cs.CE cs.LG stat.ML  published:2014-12-02 summary:The increased affordability of whole genome sequencing has motivated its use for phenotypic studies. We address the problem of learning interpretable models for discrete phenotypes from whole genomes. We propose a general approach that relies on the Set Covering Machine and a k-mer representation of the genomes. We show results for the problem of predicting the resistance of Pseudomonas Aeruginosa, an important human pathogen, against 4 antibiotics. Our results demonstrate that extremely sparse models which are biologically relevant can be learnt using this approach. version:1
arxiv-1411-7596 | Convex Techniques for Model Selection | http://arxiv.org/abs/1411.7596 | id:1411.7596 author:Dustin Tran category:math.OC stat.ML  published:2014-11-27 summary:We develop a robust convex algorithm to select the regularization parameter in model selection. In practice this would be automated in order to save practitioners time from having to tune it manually. In particular, we implement and test the convex method for $K$-fold cross validation on ridge regression, although the same concept extends to more complex models. We then compare its performance with standard methods. version:2
arxiv-1412-0879 | Watsonsim: Overview of a Question Answering Engine | http://arxiv.org/abs/1412.0879 | id:1412.0879 author:Sean Gallagher, Wlodek Zadrozny, Walid Shalaby, Adarsh Avadhani category:cs.CL cs.IR  published:2014-12-02 summary:The objective of the project is to design and run a system similar to Watson, designed to answer Jeopardy questions. In the course of a semester, we developed an open source question answering system using the Indri, Lucene, Bing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among additional modules. By the end of the semester, we achieved 18% accuracy on Jeopardy questions, and work has not stopped since then. version:1
arxiv-1412-0826 | Hashing on Nonlinear Manifolds | http://arxiv.org/abs/1412.0826 | id:1412.0826 author:Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin Tang, Heng Tao Shen category:cs.CV  published:2014-12-02 summary:Learning based hashing methods have attracted considerable attention due to their ability to greatly increase the scale at which existing algorithms may operate. Most of these methods are designed to generate binary codes preserving the Euclidean similarity in the original space. Manifold learning techniques, in contrast, are better able to model the intrinsic structure embedded in the original high-dimensional data. The complexities of these models, and the problems with out-of-sample data, have previously rendered them unsuitable for application to large-scale embedding, however. In this work, how to learn compact binary embeddings on their intrinsic manifolds is considered. In order to address the above-mentioned difficulties, an efficient, inductive solution to the out-of-sample data problem, and a process by which non-parametric manifold learning may be used as the basis of a hashing method is proposed. The proposed approach thus allows the development of a range of new hashing techniques exploiting the flexibility of the wide variety of manifold learning approaches available. It is particularly shown that hashing on the basis of t-SNE outperforms state-of-the-art hashing methods on large-scale benchmark datasets, and is very effective for image classification with very short code lengths. The proposed hashing framework is shown to be easily improved, for example, by minimizing the quantization error with learned orthogonal rotations. In addition, a supervised inductive manifold hashing framework is developed by incorporating the label information, which is shown to greatly advance the semantic retrieval performance. version:1
arxiv-1412-0801 | Analytical Comparison of Noise Reduction Filters for Image Restoration Using SNR Estimation | http://arxiv.org/abs/1412.0801 | id:1412.0801 author:Poorna Banerjee Dasgupta category:cs.CV  published:2014-12-02 summary:Noise removal from images is a part of image restoration in which we try to reconstruct or recover an image that has been degraded by using apriori knowledge of the degradation phenomenon. Noises present in images can be of various types with their characteristic Probability Distribution Functions (PDF). Noise removal techniques depend on the kind of noise present in the image rather than on the image itself. This paper explores the effects of applying noise reduction filters having similar properties on noisy images with emphasis on Signal-to-Noise-Ratio (SNR) value estimation for comparing the results. version:1
arxiv-1411-1804 | Beta Process Non-negative Matrix Factorization with Stochastic Structured Mean-Field Variational Inference | http://arxiv.org/abs/1411.1804 | id:1411.1804 author:Dawen Liang, Matthew D. Hoffman category:stat.ML cs.LG  published:2014-11-07 summary:Beta process is the standard nonparametric Bayesian prior for latent factor model. In this paper, we derive a structured mean-field variational inference algorithm for a beta process non-negative matrix factorization (NMF) model with Poisson likelihood. Unlike the linear Gaussian model, which is well-studied in the nonparametric Bayesian literature, NMF model with beta process prior does not enjoy the conjugacy. We leverage the recently developed stochastic structured mean-field variational inference to relax the conjugacy constraint and restore the dependencies among the latent variables in the approximating variational distribution. Preliminary results on both synthetic and real examples demonstrate that the proposed inference algorithm can reasonably recover the hidden structure of the data. version:2
arxiv-1412-0774 | Feedforward semantic segmentation with zoom-out features | http://arxiv.org/abs/1412.0774 | id:1412.0774 author:Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich category:cs.CV  published:2014-12-02 summary:We introduce a purely feed-forward architecture for semantic segmentation. We map small image elements (superpixels) to rich feature representations extracted from a sequence of nested regions of increasing extent. These regions are obtained by "zooming out" from the superpixel all the way to scene-level resolution. This approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead superpixels are classified by a feedforward multilayer network. Our architecture achieves new state of the art performance in semantic segmentation, obtaining 64.4% average accuracy on the PASCAL VOC 2012 test set. version:1
arxiv-1412-0751 | Tiered Clustering to Improve Lexical Entailment | http://arxiv.org/abs/1412.0751 | id:1412.0751 author:John Wieting category:cs.CL  published:2014-12-02 summary:Many tasks in Natural Language Processing involve recognizing lexical entailment. Two different approaches to this problem have been proposed recently that are quite different from each other. The first is an asymmetric similarity measure designed to give high scores when the contexts of the narrower term in the entailment are a subset of those of the broader term. The second is a supervised approach where a classifier is learned to predict entailment given a concatenated latent vector representation of the word. Both of these approaches are vector space models that use a single context vector as a representation of the word. In this work, I study the effects of clustering words into senses and using these multiple context vectors to infer entailment using extensions of these two algorithms. I find that this approach offers some improvement to these entailment algorithms. version:1
arxiv-1412-0680 | Fast Sublinear Sparse Representation using Shallow Tree Matching Pursuit | http://arxiv.org/abs/1412.0680 | id:1412.0680 author:Ali Ayremlou, Thomas Goldstein, Ashok Veeraraghavan, Richard Baraniuk category:cs.CV  published:2014-12-01 summary:Sparse approximations using highly over-complete dictionaries is a state-of-the-art tool for many imaging applications including denoising, super-resolution, compressive sensing, light-field analysis, and object recognition. Unfortunately, the applicability of such methods is severely hampered by the computational burden of sparse approximation: these algorithms are linear or super-linear in both the data dimensionality and size of the dictionary. We propose a framework for learning the hierarchical structure of over-complete dictionaries that enables fast computation of sparse representations. Our method builds on tree-based strategies for nearest neighbor matching, and presents domain-specific enhancements that are highly efficient for the analysis of image patches. Contrary to most popular methods for building spatial data structures, out methods rely on shallow, balanced trees with relatively few layers. We show an extensive array of experiments on several applications such as image denoising/superresolution, compressive video/light-field sensing where we practically achieve 100-1000x speedup (with a less than 1dB loss in accuracy). version:1
arxiv-1403-1942 | Predictive Overlapping Co-Clustering | http://arxiv.org/abs/1403.1942 | id:1403.1942 author:Chandrima Sarkar, Jaideep Srivastava category:cs.LG  published:2014-03-08 summary:In the past few years co-clustering has emerged as an important data mining tool for two way data analysis. Co-clustering is more advantageous over traditional one dimensional clustering in many ways such as, ability to find highly correlated sub-groups of rows and columns. However, one of the overlooked benefits of co-clustering is that, it can be used to extract meaningful knowledge for various other knowledge extraction purposes. For example, building predictive models with high dimensional data and heterogeneous population is a non-trivial task. Co-clusters extracted from such data, which shows similar pattern in both the dimension, can be used for a more accurate predictive model building. Several applications such as finding patient-disease cohorts in health care analysis, finding user-genre groups in recommendation systems and community detection problems can benefit from co-clustering technique that utilizes the predictive power of the data to generate co-clusters for improved data analysis. In this paper, we present the novel idea of Predictive Overlapping Co-Clustering (POCC) as an optimization problem for a more effective and improved predictive analysis. Our algorithm generates optimal co-clusters by maximizing predictive power of the co-clusters subject to the constraints on the number of row and column clusters. In this paper precision, recall and f-measure have been used as evaluation measures of the resulting co-clusters. Results of our algorithm has been compared with two other well-known techniques - K-means and Spectral co-clustering, over four real data set namely, Leukemia, Internet-Ads, Ovarian cancer and MovieLens data set. The results demonstrate the effectiveness and utility of our algorithm POCC in practice. version:2
arxiv-1412-0607 | How to monitor and mitigate stair-casing in l1 trend filtering | http://arxiv.org/abs/1412.0607 | id:1412.0607 author:Cristian R. Rojas, Bo Wahlberg category:math.ST cs.SY stat.ML stat.TH  published:2014-12-01 summary:In this paper we study the estimation of changing trends in time-series using $\ell_1$ trend filtering. This method generalizes 1D Total Variation (TV) denoising for detection of step changes in means to detecting changes in trends, and it relies on a convex optimization problem for which there are very efficient numerical algorithms. It is known that TV denoising suffers from the so-called stair-case effect, which leads to detecting false change points. The objective of this paper is to show that $\ell_1$ trend filtering also suffers from a certain stair-case problem. The analysis is based on an interpretation of the dual variables of the optimization problem in the method as integrated random walk. We discuss consistency conditions for $\ell_1$ trend filtering, how to monitor their fulfillment, and how to modify the algorithm to avoid the stair-case false detection problem. version:1
arxiv-1412-0595 | Scalability and Optimization Strategies for GPU Enhanced Neural Networks (GeNN) | http://arxiv.org/abs/1412.0595 | id:1412.0595 author:Naresh Balaji, Esin Yavuz, Thomas Nowotny category:cs.DC cs.NE q-bio.NC  published:2014-12-01 summary:Simulation of spiking neural networks has been traditionally done on high-performance supercomputers or large-scale clusters. Utilizing the parallel nature of neural network computation algorithms, GeNN (GPU Enhanced Neural Network) provides a simulation environment that performs on General Purpose NVIDIA GPUs with a code generation based approach. GeNN allows the users to design and simulate neural networks by specifying the populations of neurons at different stages, their synapse connection densities and the model of individual neurons. In this report we describe work on how to scale synaptic weights based on the configuration of the user-defined network to ensure sufficient spiking and subsequent effective learning. We also discuss optimization strategies particular to GPU computing: sparse representation of synapse connections and occupancy based block-size determination. version:1
arxiv-1412-0543 | Game-theoretical control with continuous action sets | http://arxiv.org/abs/1412.0543 | id:1412.0543 author:Steven Perkins, Panayotis Mertikopoulos, David S. Leslie category:math.OC cs.GT cs.MA stat.ML  published:2014-12-01 summary:Motivated by the recent applications of game-theoretical learning techniques to the design of distributed control systems, we study a class of control problems that can be formulated as potential games with continuous action sets, and we propose an actor-critic reinforcement learning algorithm that provably converges to equilibrium in this class of problems. The method employed is to analyse the learning process under study through a mean-field dynamical system that evolves in an infinite-dimensional function space (the space of probability distributions over the players' continuous controls). To do so, we extend the theory of finite-dimensional two-timescale stochastic approximation to an infinite-dimensional, Banach space setting, and we prove that the continuous dynamics of the process converge to equilibrium in the case of potential games. These results combine to give a provably-convergent learning algorithm in which players do not need to keep track of the controls selected by the other agents. version:1
arxiv-1412-6145 | Study of the Influence of the Number Normalization Scheme Used in Two Chaotic Pseudo Random Number Generators Used as the Source of Randomness in Differential Evolution | http://arxiv.org/abs/1412.6145 | id:1412.6145 author:Lenka Skanderova, Tomas Fabian category:cs.NE cs.CR  published:2014-12-01 summary:In many publications, authors showed that chaotic pseudo random number generators (PRNGs) may improve performance of the evolutionary algorithms. In this paper, we use two chaotic maps Gingerbread man and Tinkerbell as the chaotic PRNGs instead of the classical PRNG in the differential evolution. Numbers generated by this maps are normalized to the unit interval by three different methods -- operation modulo, straightforward number normalization where we know minimal and maximal generated number and arctangent of the two variables $x$ and $y$, where numbers $x$ and $y$ are generated by the Gingerbread man map and Tinkerbell map. The first goal of this paper is to show whether the differential evolution convergence speed might be affected by the way how we normalize number generated by the chaotic map. The second goal is to find out the influence of the probability distribution function of the selected chaotic PRNGs. The results mentioned below showed that the selected normalization method may improve differential evolution convergence speed, especially in the case of arctangent and straightforward number normalization, where we know the minimal and maximal generated numbers. version:1
arxiv-1310-1533 | CAM: Causal additive models, high-dimensional order search and penalized regression | http://arxiv.org/abs/1310.1533 | id:1310.1533 author:Peter BÃ¼hlmann, Jonas Peters, Jan Ernest category:stat.ME cs.LG stat.ML  published:2013-10-06 summary:We develop estimation for potentially high-dimensional additive structural equation models. A key component of our approach is to decouple order search among the variables from feature or edge selection in a directed acyclic graph encoding the causal structure. We show that the former can be done with nonregularized (restricted) maximum likelihood estimation while the latter can be efficiently addressed using sparse regression techniques. Thus, we substantially simplify the problem of structure search and estimation for an important class of causal models. We establish consistency of the (restricted) maximum likelihood estimator for low- and high-dimensional scenarios, and we also allow for misspecification of the error distribution. Furthermore, we develop an efficient computational algorithm which can deal with many variables, and the new method's accuracy and performance is illustrated on simulated and real data. version:2
arxiv-1411-6156 | Efficiently learning Ising models on arbitrary graphs | http://arxiv.org/abs/1411.6156 | id:1411.6156 author:Guy Bresler category:cs.LG cs.IT math.IT stat.ML  published:2014-11-22 summary:We consider the problem of reconstructing the graph underlying an Ising model from i.i.d. samples. Over the last fifteen years this problem has been of significant interest in the statistics, machine learning, and statistical physics communities, and much of the effort has been directed towards finding algorithms with low computational cost for various restricted classes of models. Nevertheless, for learning Ising models on general graphs with $p$ nodes of degree at most $d$, it is not known whether or not it is possible to improve upon the $p^{d}$ computation needed to exhaustively search over all possible neighborhoods for each node. In this paper we show that a simple greedy procedure allows to learn the structure of an Ising model on an arbitrary bounded-degree graph in time on the order of $p^2$. We make no assumptions on the parameters except what is necessary for identifiability of the model, and in particular the results hold at low-temperatures as well as for highly non-uniform models. The proof rests on a new structural property of Ising models: we show that for any node there exists at least one neighbor with which it has a high mutual information. This structural property may be of independent interest. version:2
arxiv-1412-0307 | Seeding the Initial Population of Multi-Objective Evolutionary Algorithms: A Computational Study | http://arxiv.org/abs/1412.0307 | id:1412.0307 author:Tobias Friedrich, Markus Wagner category:cs.NE  published:2014-11-30 summary:Most experimental studies initialize the population of evolutionary algorithms with random genotypes. In practice, however, optimizers are typically seeded with good candidate solutions either previously known or created according to some problem-specific method. This "seeding" has been studied extensively for single-objective problems. For multi-objective problems, however, very little literature is available on the approaches to seeding and their individual benefits and disadvantages. In this article, we are trying to narrow this gap via a comprehensive computational study on common real-valued test functions. We investigate the effect of two seeding techniques for five algorithms on 48 optimization problems with 2, 3, 4, 6, and 8 objectives. We observe that some functions (e.g., DTLZ4 and the LZ family) benefit significantly from seeding, while others (e.g., WFG) profit less. The advantage of seeding also depends on the examined algorithm. version:1
arxiv-1412-0296 | Untangling Local and Global Deformations in Deep Convolutional Networks for Image Classification and Sliding Window Detection | http://arxiv.org/abs/1412.0296 | id:1412.0296 author:George Papandreou, Iasonas Kokkinos, Pierre-AndrÃ© Savalle category:cs.CV  published:2014-11-30 summary:Deep Convolutional Neural Networks (DCNNs) commonly use generic `max-pooling' (MP) layers to extract deformation-invariant features, but we argue in favor of a more refined treatment. First, we introduce epitomic convolution as a building block alternative to the common convolution-MP cascade of DCNNs; while having identical complexity to MP, Epitomic Convolution allows for parameter sharing across different filters, resulting in faster convergence and better generalization. Second, we introduce a Multiple Instance Learning approach to explicitly accommodate global translation and scaling when training a DCNN exclusively with class labels. For this we rely on a `patchwork' data structure that efficiently lays out all image scales and positions as candidates to a DCNN. Factoring global and local deformations allows a DCNN to `focus its resources' on the treatment of non-rigid deformations and yields a substantial classification accuracy improvement. Third, further pursuing this idea, we develop an efficient DCNN sliding window object detector that employs explicit search over position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on the Pascal VOC 2007 benchmark. version:1
arxiv-1412-0251 | A Clearer Picture of Blind Deconvolution | http://arxiv.org/abs/1412.0251 | id:1412.0251 author:Daniele Perrone, Paolo Favaro category:cs.CV  published:2014-11-30 summary:Blind deconvolution is the problem of recovering a sharp image and a blur kernel from a noisy blurry image. Recently, there has been a significant effort on understanding the basic mechanisms to solve blind deconvolution. While this effort resulted in the deployment of effective algorithms, the theoretical findings generated contrasting views on why these approaches worked. On the one hand, one could observe experimentally that alternating energy minimization algorithms converge to the desired solution. On the other hand, it has been shown that such alternating minimization algorithms should fail to converge and one should instead use a so-called Variational Bayes approach. To clarify this conundrum, recent work showed that a good image and blur prior is instead what makes a blind deconvolution algorithm work. Unfortunately, this analysis did not apply to algorithms based on total variation regularization. In this manuscript, we provide both analysis and experiments to get a clearer picture of blind deconvolution. Our analysis reveals the very reason why an algorithm based on total variation works. We also introduce an implementation of this algorithm and show that, in spite of its extreme simplicity, it is very robust and achieves a performance comparable to the state of the art. version:1
arxiv-1412-0218 | Simple pairs of points in digital spaces. Topology-preserving transformations of digital spaces by contracting simple pairs of points | http://arxiv.org/abs/1412.0218 | id:1412.0218 author:Alexander V. Evako category:cs.DM cs.CV  published:2014-11-30 summary:Transformations of digital spaces preserving local and global topology play an important role in thinning, skeletonization and simplification of digital images. In the present paper, we introduce and study contractions of simple pair of points based on the notions of a digital contractible space and contractible transformations of digital spaces. We show that the contraction of a simple pair of points preserves local and global topology of a digital space. Relying on the obtained results, we study properties if digital manifolds. In particular, we show that a digital n-manifold can be transformed to its compressed form with the minimal number of points by sequential contractions of simple pairs. Key Words: Graph, digital space, contraction, splitting, simple pair, homotopy, thinning version:1
arxiv-1311-2115 | Fast large-scale optimization by unifying stochastic gradient and quasi-Newton methods | http://arxiv.org/abs/1311.2115 | id:1311.2115 author:Jascha Sohl-Dickstein, Ben Poole, Surya Ganguli category:cs.LG 90C26 G.1.6  published:2013-11-09 summary:We present an algorithm for minimizing a sum of functions that combines the computational efficiency of stochastic gradient descent (SGD) with the second order curvature information leveraged by quasi-Newton methods. We unify these disparate approaches by maintaining an independent Hessian approximation for each contributing function in the sum. We maintain computational tractability and limit memory requirements even for high dimensional optimization problems by storing and manipulating these quadratic approximations in a shared, time evolving, low dimensional subspace. Each update step requires only a single contributing function or minibatch evaluation (as in SGD), and each step is scaled using an approximate inverse Hessian and little to no adjustment of hyperparameters is required (as is typical for quasi-Newton methods). This algorithm contrasts with earlier stochastic second order techniques that treat the Hessian of each contributing function as a noisy approximation to the full Hessian, rather than as a target for direct estimation. We experimentally demonstrate improved convergence on seven diverse optimization problems. The algorithm is released as open source Python and MATLAB packages. version:7
arxiv-1412-0156 | Constant Step Size Least-Mean-Square: Bias-Variance Trade-offs and Optimal Sampling Distributions | http://arxiv.org/abs/1412.0156 | id:1412.0156 author:Alexandre DÃ©fossez, Francis Bach category:cs.LG math.OC stat.ML  published:2014-11-29 summary:We consider the least-squares regression problem and provide a detailed asymptotic analysis of the performance of averaged constant-step-size stochastic gradient descent (a.k.a. least-mean-squares). In the strongly-convex case, we provide an asymptotic expansion up to explicit exponentially decaying terms. Our analysis leads to new insights into stochastic approximation algorithms: (a) it gives a tighter bound on the allowed step-size; (b) the generalization error may be divided into a variance term which is decaying as O(1/n), independently of the step-size $\gamma$, and a bias term that decays as O(1/$\gamma$ 2 n 2); (c) when allowing non-uniform sampling, the choice of a good sampling density depends on whether the variance or bias terms dominate. In particular, when the variance term dominates, optimal sampling densities do not lead to much gain, while when the bias term dominates, we can choose larger step-sizes that leads to significant improvements. version:1
arxiv-1405-1359 | Latent semantics of action verbs reflect phonetic parameters of intensity and emotional content | http://arxiv.org/abs/1405.1359 | id:1405.1359 author:Michael Kai Petersen category:cs.CL 68T50 I.2.4; I.2.7  published:2014-05-06 summary:Conjuring up our thoughts, language reflects statistical patterns of word co-occurrences which in turn come to describe how we perceive the world. Whether counting how frequently nouns and verbs combine in Google search queries, or extracting eigenvectors from term document matrices made up of Wikipedia lines and Shakespeare plots, the resulting latent semantics capture not only the associative links which form concepts, but also spatial dimensions embedded within the surface structure of language. As both the shape and movements of objects have been found to be associated with phonetic contrasts already in toddlers, this study explores whether articulatory and acoustic parameters may likewise differentiate the latent semantics of action verbs. Selecting 3 x 20 emotion, face, and hand related verbs known to activate premotor areas in the brain, their mutual cosine similarities were computed using latent semantic analysis LSA, and the resulting adjacency matrices were compared based on two different large scale text corpora; HAWIK and TASA. Applying hierarchical clustering to identify common structures across the two text corpora, the verbs largely divide into combined mouth and hand movements versus emotional expressions. Transforming the verbs into their constituent phonemes, the clustered small and large size movements appear differentiated by front versus back vowels corresponding to increasing levels of arousal. Whereas the clustered emotional verbs seem characterized by sequences of close versus open jaw produced phonemes, generating up- or downwards shifts in formant frequencies that may influence their perceived valence. Suggesting, that the latent semantics of action verbs reflect parameters of intensity and emotional polarity that appear correlated with the articulatory contrasts and acoustic characteristics of phonemes version:3
arxiv-1412-0111 | Color image quality assessment measure using multivariate generalized Gaussian distribution | http://arxiv.org/abs/1412.0111 | id:1412.0111 author:Mounir Omari, Abdelkaher Ait Abdelouahad, Mohammed El Hassouni, Hocine Cherifi category:cs.CV  published:2014-11-29 summary:This paper deals with color image quality assessment in the reduced-reference framework based on natural scenes statistics. In this context, we propose to model the statistics of the steerable pyramid coefficients by a Multivariate Generalized Gaussian distribution (MGGD). This model allows taking into account the high correlation between the components of the RGB color space. For each selected scale and orientation, we extract a parameter matrix from the three color components subbands. In order to quantify the visual degradation, we use a closed-form of Kullback-Leibler Divergence (KLD) between two MGGDs. Using "TID 2008" benchmark, the proposed measure has been compared with the most influential methods according to the FRTV1 VQEG framework. Results demonstrates its effectiveness for a great variety of distortion type. Among other benefits this measure uses only very little information about the original image. version:1
arxiv-1412-0100 | Multiple Instance Reinforcement Learning for Efficient Weakly-Supervised Detection in Images | http://arxiv.org/abs/1412.0100 | id:1412.0100 author:Stefan Mathe, Cristian Sminchisescu category:cs.CV cs.LG  published:2014-11-29 summary:State-of-the-art visual recognition and detection systems increasingly rely on large amounts of training data and complex classifiers. Therefore it becomes increasingly expensive both to manually annotate datasets and to keep running times at levels acceptable for practical applications. In this paper, we propose two solutions to address these issues. First, we introduce a weakly supervised, segmentation-based approach to learn accurate detectors and image classifiers from weak supervisory signals that provide only approximate constraints on target localization. We illustrate our system on the problem of action detection in static images (Pascal VOC Actions 2012), using human visual search patterns as our training signal. Second, inspired from the saccade-and-fixate operating principle of the human visual system, we use reinforcement learning techniques to train efficient search models for detection. Our sequential method is weakly supervised and general (it does not require eye movements), finds optimal search strategies for any given detection confidence function and achieves performance similar to exhaustive sliding window search at a fraction of its computational cost. version:1
arxiv-1312-1931 | Multi-frame denoising of high speed optical coherence tomography data using inter-frame and intra-frame priors | http://arxiv.org/abs/1312.1931 | id:1312.1931 author:Liheng Bian, Jinli Suo, Feng Chen, Qionghai Dai category:cs.CV  published:2013-12-06 summary:Optical coherence tomography (OCT) is an important interferometric diagnostic technique which provides cross-sectional views of the subsurface microstructure of biological tissues. However, the imaging quality of high-speed OCT is limited due to the large speckle noise. To address this problem, this paper proposes a multi-frame algorithmic method to denoise OCT volume. Mathematically, we build an optimization model which forces the temporally registered frames to be low rank, and the gradient in each frame to be sparse, under logarithmic image formation and noise variance constraints. Besides, a convex optimization algorithm based on the augmented Lagrangian method is derived to solve the above model. The results reveal that our approach outperforms the other methods in terms of both speckle noise suppression and crucial detail preservation. version:2
arxiv-1412-0069 | Pedestrian Detection aided by Deep Learning Semantic Tasks | http://arxiv.org/abs/1412.0069 | id:1412.0069 author:Yonglong Tian, Ping Luo, Xiaogang Wang, Xiaoou Tang category:cs.CV  published:2014-11-29 summary:Deep learning methods have achieved great success in pedestrian detection, owing to its ability to learn features from raw pixels. However, they mainly capture middle-level representations, such as pose of pedestrian, but confuse positive with hard negative samples, which have large ambiguity, e.g. the shape and appearance of `tree trunk' or `wire pole' are similar to pedestrian in certain viewpoint. This ambiguity can be distinguished by high-level representation. To this end, this work jointly optimizes pedestrian detection with semantic tasks, including pedestrian attributes (e.g. `carrying backpack') and scene attributes (e.g. `road', `tree', and `horizontal'). Rather than expensively annotating scene attributes, we transfer attributes information from existing scene segmentation datasets to the pedestrian dataset, by proposing a novel deep model to learn high-level features from multiple tasks and multiple data sources. Since distinct tasks have distinct convergence rates and data from different datasets have different distributions, a multi-task objective function is carefully designed to coordinate tasks and reduce discrepancies among datasets. The importance coefficients of tasks and network parameters in this objective function can be iteratively estimated. Extensive evaluations show that the proposed approach outperforms the state-of-the-art on the challenging Caltech and ETH datasets, where it reduces the miss rates of previous deep models by 17 and 5.5 percent, respectively. version:1
arxiv-1412-0065 | 3D Hand Pose Detection in Egocentric RGB-D Images | http://arxiv.org/abs/1412.0065 | id:1412.0065 author:Gregory Rogez, James S. Supancic III, Maryam Khademi, Jose Maria Martinez Montiel, Deva Ramanan category:cs.CV  published:2014-11-29 summary:We focus on the task of everyday hand pose estimation from egocentric viewpoints. For this task, we show that depth sensors are particularly informative for extracting near-field interactions of the camera wearer with his/her environment. Despite the recent advances in full-body pose estimation using Kinect-like sensors, reliable monocular hand pose estimation in RGB-D images is still an unsolved problem. The problem is considerably exacerbated when analyzing hands performing daily activities from a first-person viewpoint, due to severe occlusions arising from object manipulations and a limited field-of-view. Our system addresses these difficulties by exploiting strong priors over viewpoint and pose in a discriminative tracking-by-detection framework. Our priors are operationalized through a photorealistic synthetic model of egocentric scenes, which is used to generate training data for learning depth-based pose classifiers. We evaluate our approach on an annotated dataset of real egocentric object manipulation scenes and compare to both commercial and academic approaches. Our method provides state-of-the-art performance for both hand detection and pose estimation in egocentric RGB-D images. version:1
arxiv-1412-0062 | A Bayesian Framework for Sparse Representation-Based 3D Human Pose Estimation | http://arxiv.org/abs/1412.0062 | id:1412.0062 author:Behnam Babagholami-Mohamadabadi, Amin Jourabloo, Ali Zarghami, Shohreh Kasaei category:cs.CV  published:2014-11-29 summary:A Bayesian framework for 3D human pose estimation from monocular images based on sparse representation (SR) is introduced. Our probabilistic approach aims at simultaneously learning two overcomplete dictionaries (one for the visual input space and the other for the pose space) with a shared sparse representation. Existing SR-based pose estimation approaches only offer a point estimation of the dictionary and the sparse codes. Therefore, they might be unreliable when the number of training examples is small. Our Bayesian framework estimates a posterior distribution for the sparse codes and the dictionaries from labeled training data. Hence, it is robust to overfitting on small-size training data. Experimental results on various human activities show that the proposed method is superior to the state of-the-art pose estimation algorithms. version:1
arxiv-1410-7659 | Learning graphical models from the Glauber dynamics | http://arxiv.org/abs/1410.7659 | id:1410.7659 author:Guy Bresler, David Gamarnik, Devavrat Shah category:cs.LG cs.IT math.IT stat.CO stat.ML  published:2014-10-28 summary:In this paper we consider the problem of learning undirected graphical models from data generated according to the Glauber dynamics. The Glauber dynamics is a Markov chain that sequentially updates individual nodes (variables) in a graphical model and it is frequently used to sample from the stationary distribution (to which it converges given sufficient time). Additionally, the Glauber dynamics is a natural dynamical model in a variety of settings. This work deviates from the standard formulation of graphical model learning in the literature, where one assumes access to i.i.d. samples from the distribution. Much of the research on graphical model learning has been directed towards finding algorithms with low computational cost. As the main result of this work, we establish that the problem of reconstructing binary pairwise graphical models is computationally tractable when we observe the Glauber dynamics. Specifically, we show that a binary pairwise graphical model on $p$ nodes with maximum degree $d$ can be learned in time $f(d)p^2\log p$, for a function $f(d)$, using nearly the information-theoretic minimum number of samples. version:2
arxiv-1412-0060 | Egocentric Pose Recognition in Four Lines of Code | http://arxiv.org/abs/1412.0060 | id:1412.0060 author:Gregory Rogez, James S. Supancic III, Deva Ramanan category:cs.CV  published:2014-11-29 summary:We tackle the problem of estimating the 3D pose of an individual's upper limbs (arms+hands) from a chest mounted depth-camera. Importantly, we consider pose estimation during everyday interactions with objects. Past work shows that strong pose+viewpoint priors and depth-based features are crucial for robust performance. In egocentric views, hands and arms are observable within a well defined volume in front of the camera. We call this volume an egocentric workspace. A notable property is that hand appearance correlates with workspace location. To exploit this correlation, we classify arm+hand configurations in a global egocentric coordinate frame, rather than a local scanning window. This greatly simplify the architecture and improves performance. We propose an efficient pipeline which 1) generates synthetic workspace exemplars for training using a virtual chest-mounted camera whose intrinsic parameters match our physical camera, 2) computes perspective-aware depth features on this entire volume and 3) recognizes discrete arm+hand pose classes through a sparse multi-class SVM. Our method provides state-of-the-art hand pose recognition performance from egocentric RGB-D images in real-time. version:1
arxiv-1407-7294 | Online Learning and Profit Maximization from Revealed Preferences | http://arxiv.org/abs/1407.7294 | id:1407.7294 author:Kareem Amin, Rachel Cummings, Lili Dworkin, Michael Kearns, Aaron Roth category:cs.DS cs.GT cs.LG  published:2014-07-27 summary:We consider the problem of learning from revealed preferences in an online setting. In our framework, each period a consumer buys an optimal bundle of goods from a merchant according to her (linear) utility function and current prices, subject to a budget constraint. The merchant observes only the purchased goods, and seeks to adapt prices to optimize his profits. We give an efficient algorithm for the merchant's problem that consists of a learning phase in which the consumer's utility function is (perhaps partially) inferred, followed by a price optimization step. We also consider an alternative online learning algorithm for the setting where prices are set exogenously, but the merchant would still like to predict the bundle that will be bought by the consumer for purposes of inventory or supply chain management. In contrast with most prior work on the revealed preferences problem, we demonstrate that by making stronger assumptions on the form of utility functions, efficient algorithms for both learning and profit maximization are possible, even in adaptive, online settings. version:2
arxiv-1411-7973 | Bus Travel Time Predictions Using Additive Models | http://arxiv.org/abs/1411.7973 | id:1411.7973 author:Matthias Kormaksson, Luciano Barbosa, Marcos R. Vieira, Bianca Zadrozny category:cs.LG stat.AP  published:2014-11-28 summary:Many factors can affect the predictability of public bus services such as traffic, weather and local events. Other aspects, such as day of week or hour of day, may influence bus travel times as well, either directly or in conjunction with other variables. However, the exact nature of such relationships between travel times and predictor variables is, in most situations, not known. In this paper we develop a framework that allows for flexible modeling of bus travel times through the use of Additive Models. In particular, we model travel times as a sum of linear as well as nonlinear terms that are modeled as smooth functions of predictor variables. The proposed class of models provides a principled statistical framework that is highly flexible in terms of model building. The experimental results demonstrate uniformly superior performance of our best model as compared to previous prediction methods when applied to a very large GPS data set obtained from buses operating in the city of Rio de Janeiro. version:1
arxiv-1411-7964 | Effective Face Frontalization in Unconstrained Images | http://arxiv.org/abs/1411.7964 | id:1411.7964 author:Tal Hassner, Shai Harel, Eran Paz, Roee Enbar category:cs.CV  published:2014-11-28 summary:"Frontalization" is the process of synthesizing frontal facing views of faces appearing in single unconstrained photos. Recent reports have suggested that this process may substantially boost the performance of face recognition systems. This, by transforming the challenging problem of recognizing faces viewed from unconstrained viewpoints to the easier problem of recognizing faces in constrained, forward facing poses. Previous frontalization methods did this by attempting to approximate 3D facial shapes for each query image. We observe that 3D face shape estimation from unconstrained photos may be a harder problem than frontalization and can potentially introduce facial misalignments. Instead, we explore the simpler approach of using a single, unmodified, 3D surface as an approximation to the shape of all input faces. We show that this leads to a straightforward, efficient and easy to implement method for frontalization. More importantly, it produces aesthetic new frontal views and is surprisingly effective when used for face recognition and gender estimation. version:1
arxiv-1403-2923 | Adaptive Representations for Tracking Breaking News on Twitter | http://arxiv.org/abs/1403.2923 | id:1403.2923 author:Igor Brigadir, Derek Greene, PÃ¡draig Cunningham category:cs.IR cs.NE I.5.4; I.5.1; H.3.3  published:2014-03-12 summary:Twitter is often the most up-to-date source for finding and tracking breaking news stories. Therefore, there is considerable interest in developing filters for tweet streams in order to track and summarize stories. This is a non-trivial text analytics task as tweets are short, and standard retrieval methods often fail as stories evolve over time. In this paper we examine the effectiveness of adaptive mechanisms for tracking and summarizing breaking news stories. We evaluate the effectiveness of these mechanisms on a number of recent news events for which manually curated timelines are available. Assessments based on ROUGE metrics indicate that an adaptive approaches are best suited for tracking evolving stories on Twitter. version:3
arxiv-1411-7924 | Predicting clicks in online display advertising with latent features and side-information | http://arxiv.org/abs/1411.7924 | id:1411.7924 author:Bjarne Ãrum Fruergaard category:stat.ML cs.LG stat.AP  published:2014-11-28 summary:We review a method for click-through rate prediction based on the work of Menon et al. [11], which combines collaborative filtering and matrix factorization with a side-information model and fuses the outputs to proper probabilities in [0,1]. In addition we provide details, both for the modeling as well as the experimental part, that are not found elsewhere. We rigorously test the performance on several test data sets from consecutive days in a click-through rate prediction setup, in a manner which reflects a real-world pipeline. Our results confirm that performance can be increased using latent features, albeit the differences in the measures are small but significant. version:1
arxiv-1411-7923 | Learning Face Representation from Scratch | http://arxiv.org/abs/1411.7923 | id:1411.7923 author:Dong Yi, Zhen Lei, Shengcai Liao, Stan Z. Li category:cs.CV  published:2014-11-28 summary:Pushing by big data and deep convolutional neural network (CNN), the performance of face recognition is becoming comparable to human. Using private large scale training datasets, several groups achieve very high performance on LFW, i.e., 97% to 99%. While there are many open source implementations of CNN, none of large scale face dataset is publicly available. The current situation in the field of face recognition is that data is more important than algorithm. To solve this problem, this paper proposes a semi-automatical way to collect face images from Internet and builds a large scale dataset containing about 10,000 subjects and 500,000 images, called CASIAWebFace. Based on the database, we use a 11-layer CNN to learn discriminative representation and obtain state-of-theart accuracy on LFW and YTF. The publication of CASIAWebFace will attract more research groups entering this field and accelerate the development of face recognition in the wild. version:1
arxiv-1411-7911 | On Rendering Synthetic Images for Training an Object Detector | http://arxiv.org/abs/1411.7911 | id:1411.7911 author:Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV  published:2014-11-28 summary:We propose a novel approach to synthesizing images that are effective for training object detectors. Starting from a small set of real images, our algorithm estimates the rendering parameters required to synthesize similar images given a coarse 3D model of the target object. These parameters can then be reused to generate an unlimited number of training images of the object of interest in arbitrary 3D poses, which can then be used to increase classification performances. A key insight of our approach is that the synthetically generated images should be similar to real images, not in terms of image quality, but rather in terms of features used during the detector training. We show in the context of drone, plane, and car detection that using such synthetically generated images yields significantly better performances than simply perturbing real images or even synthesizing images in such way that they look very realistic, as is often done when only limited amounts of training data are available. version:1
arxiv-1410-5485 | A stronger null hypothesis for crossing dependencies | http://arxiv.org/abs/1410.5485 | id:1410.5485 author:Ramon Ferrer-i-Cancho category:cs.CL cs.SI physics.soc-ph  published:2014-10-20 summary:The syntactic structure of a sentence can be modeled as a tree where vertices are words and edges indicate syntactic dependencies between words. It is well-known that those edges normally do not cross when drawn over the sentence. Here a new null hypothesis for the number of edge crossings of a sentence is presented. That null hypothesis takes into account the length of the pair of edges that may cross and predicts the relative number of crossings in random trees with a small error, suggesting that a ban of crossings or a principle of minimization of crossings are not needed in general to explain the origins of non-crossing dependencies. Our work paves the way for more powerful null hypotheses to investigate the origins of non-crossing dependencies in nature. version:2
arxiv-1411-7864 | Efficient inference of overlapping communities in complex networks | http://arxiv.org/abs/1411.7864 | id:1411.7864 author:Bjarne Ãrum Fruergaard, Tue Herlau category:stat.ML cs.SI physics.soc-ph  published:2014-11-28 summary:We discuss two views on extending existing methods for complex network modeling which we dub the communities first and the networks first view, respectively. Inspired by the networks first view that we attribute to White, Boorman, and Breiger (1976)[1], we formulate the multiple-networks stochastic blockmodel (MNSBM), which seeks to separate the observed network into subnetworks of different types and where the problem of inferring structure in each subnetwork becomes easier. We show how this model is specified in a generative Bayesian framework where parameters can be inferred efficiently using Gibbs sampling. The result is an effective multiple-membership model without the drawbacks of introducing complex definitions of "groups" and how they interact. We demonstrate results on the recovery of planted structure in synthetic networks and show very encouraging results on link prediction performances using multiple-networks models on a number of real-world network data sets. version:1
arxiv-1411-7855 | V-variable image compression | http://arxiv.org/abs/1411.7855 | id:1411.7855 author:Franklin Mendivil, Ãrjan Stenflo category:cs.CV 28A80  68U10  94A08  published:2014-11-28 summary:V-variable fractals, where $V$ is a positive integer, are intuitively fractals with at most $V$ different "forms" or "shapes" at all levels of magnification. In this paper we describe how V-variable fractals can be used for the purpose of image compression. version:1
arxiv-1409-7938 | Lazier Than Lazy Greedy | http://arxiv.org/abs/1409.7938 | id:1409.7938 author:Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, Andreas Krause category:cs.LG cs.DS cs.IR  published:2014-09-28 summary:Is it possible to maximize a monotone submodular function faster than the widely used lazy greedy algorithm (also known as accelerated greedy), both in theory and practice? In this paper, we develop the first linear-time algorithm for maximizing a general monotone submodular function subject to a cardinality constraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can achieve a $(1-1/e-\varepsilon)$ approximation guarantee, in expectation, to the optimum solution in time linear in the size of the data and independent of the cardinality constraint. We empirically demonstrate the effectiveness of our algorithm on submodular functions arising in data summarization, including training large-scale kernel methods, exemplar-based clustering, and sensor placement. We observe that STOCHASTIC-GREEDY practically achieves the same utility value as lazy greedy but runs much faster. More surprisingly, we observe that in many practical scenarios STOCHASTIC-GREEDY does not evaluate the whole fraction of data points even once and still achieves indistinguishable results compared to lazy greedy. version:3
arxiv-1411-7820 | Coarse-grained Cross-lingual Alignment of Comparable Texts with Topic Models and Encyclopedic Knowledge | http://arxiv.org/abs/1411.7820 | id:1411.7820 author:Vivi Nastase, Angela Fahrni category:cs.CL  published:2014-11-28 summary:We present a method for coarse-grained cross-lingual alignment of comparable texts: segments consisting of contiguous paragraphs that discuss the same theme (e.g. history, economy) are aligned based on induced multilingual topics. The method combines three ideas: a two-level LDA model that filters out words that do not convey themes, an HMM that models the ordering of themes in the collection of documents, and language-independent concept annotations to serve as a cross-language bridge and to strengthen the connection between paragraphs in the same segment through concept relations. The method is evaluated on English and French data previously used for monolingual alignment. The results show state-of-the-art performance in both monolingual and cross-lingual settings. version:1
arxiv-1411-7817 | Learning with Algebraic Invariances, and the Invariant Kernel Trick | http://arxiv.org/abs/1411.7817 | id:1411.7817 author:Franz J. KirÃ¡ly, Andreas Ziehe, Klaus-Robert MÃ¼ller category:stat.ML cs.LG math.ST stat.TH  published:2014-11-28 summary:When solving data analysis problems it is important to integrate prior knowledge and/or structural invariances. This paper contributes by a novel framework for incorporating algebraic invariance structure into kernels. In particular, we show that algebraic properties such as sign symmetries in data, phase independence, scaling etc. can be included easily by essentially performing the kernel trick twice. We demonstrate the usefulness of our theory in simulations on selected applications such as sign-invariant spectral clustering and underdetermined ICA. version:1
arxiv-1411-7806 | Two Gaussian Approaches to Black-Box Optomization | http://arxiv.org/abs/1411.7806 | id:1411.7806 author:LukÃ¡Å¡ Bajer, Martin HoleÅa category:cs.NE cs.AI  published:2014-11-28 summary:Outline of several strategies for using Gaussian processes as surrogate models for the covariance matrix adaptation evolution strategy (CMA-ES). version:1
arxiv-1411-7798 | Cross-Modal Learning via Pairwise Constraints | http://arxiv.org/abs/1411.7798 | id:1411.7798 author:Ran He, Man Zhang, Liang Wang, Ye Ji, Qiyue Yin category:cs.CV  published:2014-11-28 summary:In multimedia applications, the text and image components in a web document form a pairwise constraint that potentially indicates the same semantic concept. This paper studies cross-modal learning via the pairwise constraint, and aims to find the common structure hidden in different modalities. We first propose a compound regularization framework to deal with the pairwise constraint, which can be used as a general platform for developing cross-modal algorithms. For unsupervised learning, we propose a cross-modal subspace clustering method to learn a common structure for different modalities. For supervised learning, to reduce the semantic gap and the outliers in pairwise constraints, we propose a cross-modal matching method based on compound ?21 regularization along with an iteratively reweighted algorithm to find the global optimum. Extensive experiments demonstrate the benefits of joint text and image modeling with semantically induced pairwise constraints, and show that the proposed cross-modal methods can further reduce the semantic gap between different modalities and improve the clustering/retrieval accuracy. version:1
arxiv-1411-7715 | Flying Objects Detection from a Single Moving Camera | http://arxiv.org/abs/1411.7715 | id:1411.7715 author:Artem Rozantsev, Vincent Lepetit, Pascal Fua category:cs.CV  published:2014-11-27 summary:We propose an approach to detect flying objects such as UAVs and aircrafts when they occupy a small portion of the field of view, possibly moving against complex backgrounds, and are filmed by a camera that itself moves. Solving such a difficult problem requires combining both appearance and motion cues. To this end we propose a regression-based approach to motion stabilization of local image patches that allows us to achieve effective classification on spatio-temporal image cubes and outperform state-of-the-art techniques. As the problem is relatively new, we collected two challenging datasets for UAVs and Aircrafts, which can be used as benchmarks for flying objects detection and vision-guided collision avoidance. version:1
arxiv-1411-7714 | Features in Concert: Discriminative Feature Selection meets Unsupervised Clustering | http://arxiv.org/abs/1411.7714 | id:1411.7714 author:Marius Leordeanu, Alexandra Radu, Rahul Sukthankar category:cs.CV  published:2014-11-27 summary:Feature selection is an essential problem in computer vision, important for category learning and recognition. Along with the rapid development of a wide variety of visual features and classifiers, there is a growing need for efficient feature selection and combination methods, to construct powerful classifiers for more complex and higher-level recognition tasks. We propose an algorithm that efficiently discovers sparse, compact representations of input features or classifiers, from a vast sea of candidates, with important optimality properties, low computational cost and excellent accuracy in practice. Different from boosting, we start with a discriminant linear classification formulation that encourages sparse solutions. Then we obtain an equivalent unsupervised clustering problem that jointly discovers ensembles of diverse features. They are independently valuable but even more powerful when united in a cluster of classifiers. We evaluate our method on the task of large-scale recognition in video and show that it significantly outperforms classical selection approaches, such as AdaBoost and greedy forward-backward selection, and powerful classifiers such as SVMs, in speed of training and performance, especially in the case of limited training data. version:1
arxiv-1411-7706 | A Nonparametric Bayesian Approach to Uncovering Rat Hippocampal Population Codes During Spatial Navigation | http://arxiv.org/abs/1411.7706 | id:1411.7706 author:Scott W. Linderman, Matthew J. Johnson, Matthew A. Wilson, Zhe Chen category:stat.ML q-bio.NC  published:2014-11-27 summary:Rodent hippocampal population codes represent important spatial information about the environment during navigation. Several computational methods have been developed to uncover the neural representation of spatial topology embedded in rodent hippocampal ensemble spike activity. Here we extend our previous work and propose a nonparametric Bayesian approach to infer rat hippocampal population codes during spatial navigation. To tackle the model selection problem, we leverage a nonparametric Bayesian model. Specifically, to analyze rat hippocampal ensemble spiking activity, we apply a hierarchical Dirichlet process-hidden Markov model (HDP-HMM) using two Bayesian inference methods, one based on Markov chain Monte Carlo (MCMC) and the other based on variational Bayes (VB). We demonstrate the effectiveness of our Bayesian approaches on recordings from a freely-behaving rat navigating in an open field environment. We find that MCMC-based inference with Hamiltonian Monte Carlo (HMC) hyperparameter sampling is flexible and efficient, and outperforms VB and MCMC approaches with hyperparameters set by empirical Bayes. version:1
arxiv-1411-7682 | On color image quality assessment using natural image statistics | http://arxiv.org/abs/1411.7682 | id:1411.7682 author:Mounir Omari, Mohammed El Hassouni, Hocine Cherifi, Abdelkaher Ait Abdelouahad category:cs.CV  published:2014-11-27 summary:Color distortion can introduce a significant damage in visual quality perception, however, most of existing reduced-reference quality measures are designed for grayscale images. In this paper, we consider a basic extension of well-known image-statistics based quality assessment measures to color images. In order to evaluate the impact of color information on the measures efficiency, two color spaces are investigated: RGB and CIELAB. Results of an extensive evaluation using TID 2013 benchmark demonstrates that significant improvement can be achieved for a great number of distortion type when the CIELAB color representation is used. version:1
arxiv-1405-0514 | Complexity of Equivalence and Learning for Multiplicity Tree Automata | http://arxiv.org/abs/1405.0514 | id:1405.0514 author:Ines Marusic, James Worrell category:cs.LG cs.FL  published:2014-05-02 summary:We consider the complexity of equivalence and learning for multiplicity tree automata, i.e., weighted tree automata over a field. We first show that the equivalence problem is logspace equivalent to polynomial identity testing, the complexity of which is a longstanding open problem. Secondly, we derive lower bounds on the number of queries needed to learn multiplicity tree automata in Angluin's exact learning model, over both arbitrary and fixed fields. Habrard and Oncina (2006) give an exact learning algorithm for multiplicity tree automata, in which the number of queries is proportional to the size of the target automaton and the size of a largest counterexample, represented as a tree, that is returned by the Teacher. However, the smallest tree-counterexample may be exponential in the size of the target automaton. Thus the above algorithm does not run in time polynomial in the size of the target automaton, and has query complexity exponential in the lower bound. Assuming a Teacher that returns minimal DAG representations of counterexamples, we give a new exact learning algorithm whose query complexity is quadratic in the target automaton size, almost matching the lower bound, and improving the best previously-known algorithm by an exponential factor. version:2
arxiv-1411-7655 | A statistical reduced-reference method for color image quality assessment | http://arxiv.org/abs/1411.7655 | id:1411.7655 author:Mounir Omari, Mohammed El Hassouni, Abdelkaher Ait Abdelouahad, Hocine Cherifi category:cs.CV  published:2014-11-27 summary:Although color is a fundamental feature of human visual perception, it has been largely unexplored in the reduced-reference (RR) image quality assessment (IQA) schemes. In this paper, we propose a natural scene statistic (NSS) method, which efficiently uses this information. It is based on the statistical deviation between the steerable pyramid coefficients of the reference color image and the degraded one. We propose and analyze the multivariate generalized Gaussian distribution (MGGD) to model the underlying statistics. In order to quantify the degradation, we develop and evaluate two measures based respectively on the Geodesic distance between two MGGDs and on the closed-form of the Kullback Leibler divergence. We performed an extensive evaluation of both metrics in various color spaces (RGB, HSV, CIELAB and YCrCb) using the TID 2008 benchmark and the FRTV Phase I validation process. Experimental results demonstrate the effectiveness of the proposed framework to achieve a good consistency with human visual perception. Furthermore, the best configuration is obtained with CIELAB color space associated to KLD deviation measure. version:1
arxiv-1406-2839 | The Poisson transform for unnormalised statistical models | http://arxiv.org/abs/1406.2839 | id:1406.2839 author:Simon BarthelmÃ©, Nicolas Chopin category:stat.CO stat.ML 62F99 G.3  published:2014-06-11 summary:Contrary to standard statistical models, unnormalised statistical models only specify the likelihood function up to a constant. While such models are natural and popular, the lack of normalisation makes inference much more difficult. Here we show that inferring the parameters of a unnormalised model on a space $\Omega$ can be mapped onto an equivalent problem of estimating the intensity of a Poisson point process on $\Omega$. The unnormalised statistical model now specifies an intensity function that does not need to be normalised. Effectively, the normalisation constant may now be inferred as just another parameter, at no loss of information. The result can be extended to cover non-IID models, which includes for example unnormalised models for sequences of graphs (dynamical graphs), or for sequences of binary vectors. As a consequence, we prove that unnormalised parameteric inference in non-IID models can be turned into a semi-parametric estimation problem. Moreover, we show that the noise-contrastive divergence of Gutmann & Hyv\"arinen (2012) can be understood as an approximation of the Poisson transform, and extended to non-IID settings. We use our results to fit spatial Markov chain models of eye movements, where the Poisson transform allows us to turn a highly non-standard model into vanilla semi-parametric logistic regression. version:2
arxiv-1411-7612 | A Parallel Genetic Algorithm for Generalized Vertex Cover Problem | http://arxiv.org/abs/1411.7612 | id:1411.7612 author:Drona Pratap Chandu category:cs.DC cs.NE  published:2014-11-27 summary:This paper presents a parallel genetic algorithm for generalised vertex cover problem (GVCP) using Hadoop Map-Reduce framework. The proposed Map-Reduce implementation helps to run the genetic algorithm for generalized vertex cover problem (GVCP) on multiple machines parallely and computes the solution in relatively short time. version:1
arxiv-1411-7582 | Graph Sensitive Indices for Comparing Clusterings | http://arxiv.org/abs/1411.7582 | id:1411.7582 author:Zaeem Hussain, Marina Meila category:cs.LG  published:2014-11-27 summary:This report discusses two new indices for comparing clusterings of a set of points. The motivation for looking at new ways for comparing clusterings stems from the fact that the existing clustering indices are based on set cardinality alone and do not consider the positions of data points. The new indices, namely, the Random Walk index (RWI) and Variation of Information with Neighbors (VIN), are both inspired by the clustering metric Variation of Information (VI). VI possesses some interesting theoretical properties which are also desirable in a metric for comparing clusterings. We define our indices and discuss some of their explored properties which appear relevant for a clustering index. We also include the results of these indices on clusterings of some example data sets. version:1
arxiv-1409-5623 | Interactive Visual Exploration of Topic Models using Graphs | http://arxiv.org/abs/1409.5623 | id:1409.5623 author:Samuel RÃ¶nnqvist, Xiaolu Wang, Peter Sarlin category:cs.IR cs.CL I.5.5  published:2014-09-19 summary:Probabilistic topic modeling is a popular and powerful family of tools for uncovering thematic structure in large sets of unstructured text documents. While much attention has been directed towards the modeling algorithms and their various extensions, comparatively few studies have concerned how to present or visualize topic models in meaningful ways. In this paper, we present a novel design that uses graphs to visually communicate topic structure and meaning. By connecting topic nodes via descriptive keyterms, the graph representation reveals topic similarities, topic meaning and shared, ambiguous keyterms. At the same time, the graph can be used for information retrieval purposes, to find documents by topic or topic subsets. To exemplify the utility of the design, we illustrate its use for organizing and exploring corpora of financial patents. version:2
arxiv-1408-1717 | Matrix Completion on Graphs | http://arxiv.org/abs/1408.1717 | id:1408.1717 author:Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre Vandergheynst category:cs.LG stat.ML  published:2014-08-07 summary:The problem of finding the missing values of a matrix given a few of its entries, called matrix completion, has gathered a lot of attention in the recent years. Although the problem under the standard low rank assumption is NP-hard, Cand\`es and Recht showed that it can be exactly relaxed if the number of observed entries is sufficiently large. In this work, we introduce a novel matrix completion model that makes use of proximity information about rows and columns by assuming they form communities. This assumption makes sense in several real-world problems like in recommender systems, where there are communities of people sharing preferences, while products form clusters that receive similar ratings. Our main goal is thus to find a low-rank solution that is structured by the proximities of rows and columns encoded by graphs. We borrow ideas from manifold learning to constrain our solution to be smooth on these graphs, in order to implicitly force row and column proximities. Our matrix recovery model is formulated as a convex non-smooth optimization problem, for which a well-posed iterative scheme is provided. We study and evaluate the proposed matrix completion on synthetic and real data, showing that the proposed structured low-rank recovery model outperforms the standard matrix completion model in many situations. version:3
arxiv-1411-7542 | Scalability of using Restricted Boltzmann Machines for Combinatorial Optimization | http://arxiv.org/abs/1411.7542 | id:1411.7542 author:Malte Probst, Franz Rothlauf, JÃ¶rn Grahl category:cs.NE I.2.6; I.2.8  published:2014-11-27 summary:Estimation of Distribution Algorithms (EDAs) require flexible probability models that can be efficiently learned and sampled. Restricted Boltzmann Machines (RBMs) are generative neural networks with these desired properties. We integrate an RBM into an EDA and evaluate the performance of this system in solving combinatorial optimization problems with a single objective. We assess how the number of fitness evaluations and the CPU time scale with problem size and with problem complexity. The results are compared to the Bayesian Optimization Algorithm, a state-of-the-art EDA. Although RBM-EDA requires larger population sizes and a larger number of fitness evaluations, it outperforms BOA in terms of CPU times, in particular if the problem is large or complex. RBM-EDA requires less time for model building than BOA. These results highlight the potential of using generative neural networks for combinatorial optimization. version:1
arxiv-1411-7508 | Forecasting the Colorado River Discharge Using an Artificial Neural Network (ANN) Approach | http://arxiv.org/abs/1411.7508 | id:1411.7508 author:Amirhossein Mehrkesh, Maryam Ahmadi category:stat.ML physics.soc-ph  published:2014-11-27 summary:Artificial Neural Network (ANN) based model is a computational approach commonly used for modeling the complex relationships between input and output parameters. Prediction of the flow rate of a river is a requisite for any successful water resource management and river basin planning. In the current survey, the effectiveness of an Artificial Neural Network was examined to predict the Colorado River discharge. In this modeling process, an ANN model was used to relate the discharge of the Colorado River to such parameters as the amount of precipitation, ambient temperature and snowpack level at a specific time of the year. The model was able to precisely study the impact of climatic parameters on the flow rate of the Colorado River. version:1
arxiv-1411-7466 | The Treasure beneath Convolutional Layers: Cross-convolutional-layer Pooling for Image Classification | http://arxiv.org/abs/1411.7466 | id:1411.7466 author:Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2014-11-27 summary:A number of recent studies have shown that a Deep Convolutional Neural Network (DCNN) pretrained on a large dataset can be adopted as a universal image description which leads to astounding performance in many visual classification tasks. Most of these studies, if not all, adopt activations of the fully-connected layer of a DCNN as the image or region representation and it is believed that convolutional layer activations are less discriminative. This paper, however, advocates that if used appropriately convolutional layer activations can be turned into a powerful image representation which enjoys many advantages over fully-connected layer activations. This is achieved by adopting a new technique proposed in this paper called cross-convolutional-layer pooling. More specifically, it extracts subarrays of feature maps of one convolutional layer as local features and pools the extracted features with the guidance of feature maps of the successive convolutional layer. Compared with exising methods that apply DCNNs in the local feature setting, the proposed method is significantly faster since it requires much fewer times of DCNN forward computation. Moreover, it avoids the domain mismatch issue which is usually encountered when applying fully connected layer activations to describe local regions. By applying our method to four popular visual classification tasks, it is demonstrated that the proposed method can achieve comparable or in some cases significantly better performance than existing fully-connected layer based image representations while incurring much lower computational cost. version:1
arxiv-1411-7450 | Worst-Case Linear Discriminant Analysis as Scalable Semidefinite Feasibility Problems | http://arxiv.org/abs/1411.7450 | id:1411.7450 author:Hui Li, Chunhua Shen, Anton van den Hengel, Qinfeng Shi category:cs.LG  published:2014-11-27 summary:In this paper, we propose an efficient semidefinite programming (SDP) approach to worst-case linear discriminant analysis (WLDA). Compared with the traditional LDA, WLDA considers the dimensionality reduction problem from the worst-case viewpoint, which is in general more robust for classification. However, the original problem of WLDA is non-convex and difficult to optimize. In this paper, we reformulate the optimization problem of WLDA into a sequence of semidefinite feasibility problems. To efficiently solve the semidefinite feasibility problems, we design a new scalable optimization method with quasi-Newton methods and eigen-decomposition being the core components. The proposed method is orders of magnitude faster than standard interior-point based SDP solvers. Experiments on a variety of classification problems demonstrate that our approach achieves better performance than standard LDA. Our method is also much faster and more scalable than standard interior-point SDP solvers based WLDA. The computational complexity for an SDP with $m$ constraints and matrices of size $d$ by $d$ is roughly reduced from $\mathcal{O}(m^3+md^3+m^2d^2)$ to $\mathcal{O}(d^3)$ ($m>d$ in our case). version:1
arxiv-1411-7445 | Bi-objective Optimization for Robust RGB-D Visual Odometry | http://arxiv.org/abs/1411.7445 | id:1411.7445 author:Tao Han, Chao Xu, Ryan Loxton, Lei Xie category:cs.RO cs.CV  published:2014-11-27 summary:This paper considers a new bi-objective optimization formulation for robust RGB-D visual odometry. We investigate two methods for solving the proposed bi-objective optimization problem: the weighted sum method (in which the objective functions are combined into a single objective function) and the bounded objective method (in which one of the objective functions is optimized and the value of the other objective function is bounded via a constraint). Our experimental results for the open source TUM RGB-D dataset show that the new bi-objective optimization formulation is superior to several existing RGB-D odometry methods. In particular, the new formulation yields more accurate motion estimates and is more robust when textural or structural features in the image sequence are lacking. version:1
arxiv-1411-7441 | Pattern Decomposition with Complex Combinatorial Constraints: Application to Materials Discovery | http://arxiv.org/abs/1411.7441 | id:1411.7441 author:Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire, Carla Gomes, Bart Selman, Robert B. van Dover category:cs.AI cs.LG stat.ML  published:2014-11-27 summary:Identifying important components or factors in large amounts of noisy data is a key problem in machine learning and data mining. Motivated by a pattern decomposition problem in materials discovery, aimed at discovering new materials for renewable energy, e.g. for fuel and solar cells, we introduce CombiFD, a framework for factor based pattern decomposition that allows the incorporation of a-priori knowledge as constraints, including complex combinatorial constraints. In addition, we propose a new pattern decomposition algorithm, called AMIQO, based on solving a sequence of (mixed-integer) quadratic programs. Our approach considerably outperforms the state of the art on the materials discovery problem, scaling to larger datasets and recovering more precise and physically meaningful decompositions. We also show the effectiveness of our approach for enforcing background knowledge on other application domains. version:1
arxiv-1411-7432 | Metrics for Probabilistic Geometries | http://arxiv.org/abs/1411.7432 | id:1411.7432 author:Alessandra Tosi, SÃ¸ren Hauberg, Alfredo Vellido, Neil D. Lawrence category:stat.ML cs.LG  published:2014-11-27 summary:We investigate the geometrical structure of probabilistic generative dimensionality reduction models using the tools of Riemannian geometry. We explicitly define a distribution over the natural metric given by the models. We provide the necessary algorithms to compute expected metric tensors where the distribution over mappings is given by a Gaussian process. We treat the corresponding latent variable model as a Riemannian manifold and we use the expectation of the metric under the Gaussian process prior to define interpolating paths and measure distance between latent points. We show how distances that respect the expected metric lead to more appropriate generation of new data. version:1
arxiv-1412-0003 | 3D-Assisted Image Feature Synthesis for Novel Views of an Object | http://arxiv.org/abs/1412.0003 | id:1412.0003 author:Hao Su, Fan Wang, Li Yi, Leonidas Guibas category:cs.CV  published:2014-11-26 summary:Comparing two images in a view-invariant way has been a challenging problem in computer vision for a long time, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize new features for other views of the same object. To accomplish this, we introduce an aligned set of 3D models in the same class as the input object image. Each 3D model is represented by a set of views, and we study the correlation of image patches between different views, seeking what we call surrogates --- patches in one view whose feature content predicts well the features of a patch in another view. In particular, for each patch in the novel desired view, we seek surrogates from the observed view of the given image. For a given surrogate, we predict that surrogate using linear combination of the corresponding patches of the 3D model views, learn the coefficients, and then transfer these coefficients on a per patch basis to synthesize the features of the patch in the novel view. In this way we can create feature sets for all views of the latent object, providing us a multi-view representation of the object. View-invariant object comparisons are achieved simply by computing the $L^2$ distances between the features of corresponding views. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the proposed view-agnostic distance (VAD) in fine-grained image retrieval (100 object classes) and classification tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than traditional image features in this respect. version:1
arxiv-1411-7336 | Edge direction matrixes-based local binar patterns descriptor for shape pattern recognition | http://arxiv.org/abs/1411.7336 | id:1411.7336 author:Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh category:cs.CV cs.IR  published:2014-11-26 summary:Shapes and texture image recognition usage is an essential branch of pattern recognition. It is made up of techniques that aim at extracting information from images via human knowledge and works. Local Binary Pattern (LBP) ensures encoding global and local information and scaling invariance by introducing a look-up table to reflect the uniformity structure of an object. However, edge direction matrixes (EDMS) only apply global invariant descriptor which employs first and secondary order relationships. The main idea behind this methodology is the need of improved recognition capabilities, a goal achieved by the combinative use of these descriptors. This collaboration aims to make use of the major advantages each one presents, by simultaneously complementing each other, in order to elevate their weak points. By using multiple classifier approaches such as random forest and multi-layer perceptron neural network, the proposed combinative descriptor are compared with the state of the art combinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS), LBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS image, Enghlishfnt and Arabic calligraphy . The experiments have shown the superiority of the introduced descriptor over the GLCM with EDMS, LBP and moment invariants and other well-known descriptor such as Scale Invariant Feature Transform from the literature. version:1
arxiv-1412-0035 | Understanding Deep Image Representations by Inverting Them | http://arxiv.org/abs/1412.0035 | id:1412.0035 author:Aravindh Mahendran, Andrea Vedaldi category:cs.CV  published:2014-11-26 summary:Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance. version:1
arxiv-1411-7889 | Open-source code for manifold-based 3D rotation recovery of X-ray scattering patterns | http://arxiv.org/abs/1411.7889 | id:1411.7889 author:Aliakbar Jafarpour category:physics.optics cs.CV  published:2014-11-26 summary:Single particle 3D imaging with ultrashort X-ray laser pulses is based on collecting and combining the information content of 2D scattering patterns of an object at different orientations. Typical sample-delivery schemes leave little or no room for controlling the orientations. As such, the orientation associated with a given snapshot should be estimated after the experiment. Here we present an open-source code for the most rigorous technique having been reported in this context. Some practical issues along with proposed solutions are also discussed. version:1
arxiv-1411-7245 | Heuristics for Exact Nonnegative Matrix Factorization | http://arxiv.org/abs/1411.7245 | id:1411.7245 author:Arnaud Vandaele, Nicolas Gillis, FranÃ§ois Glineur, Daniel Tuyttens category:math.OC cs.LG cs.NA stat.ML  published:2014-11-26 summary:The exact nonnegative matrix factorization (exact NMF) problem is the following: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank $r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$ nonnegative matrix $H$ such that $X = WH$. In this paper, we propose two heuristics for exact NMF, one inspired from simulated annealing and the other from the greedy randomized adaptive search procedure. We show that these two heuristics are able to compute exact nonnegative factorizations for several classes of nonnegative matrices (namely, linear Euclidean distance matrices, slack matrices, unique-disjointness matrices, and randomly generated matrices) and as such demonstrate their superiority over standard multi-start strategies. We also consider a hybridization between these two heuristics that allows us to combine the advantages of both methods. Finally, we discuss the use of these heuristics to gain insight on the behavior of the nonnegative rank, i.e., the minimum factorization rank such that an exact NMF exists. In particular, we disprove a conjecture on the nonnegative rank of a Kronecker product, propose a new upper bound on the extension complexity of generic $n$-gons and conjecture the exact value of (i) the extension complexity of regular $n$-gons and (ii) the nonnegative rank of a submatrix of the slack matrix of the correlation polytope. version:1
arxiv-1411-7200 | Localized Complexities for Transductive Learning | http://arxiv.org/abs/1411.7200 | id:1411.7200 author:Ilya Tolstikhin, Gilles Blanchard, Marius Kloft category:stat.ML cs.LG  published:2014-11-26 summary:We show two novel concentration inequalities for suprema of empirical processes when sampling without replacement, which both take the variance of the functions into account. While these inequalities may potentially have broad applications in learning theory in general, we exemplify their significance by studying the transductive setting of learning theory. For which we provide the first excess risk bounds based on the localized complexity of the hypothesis class, which can yield fast rates of convergence also in the transductive learning setting. We give a preliminary analysis of the localized complexities for the prominent case of kernel classes. version:1
arxiv-1403-6600 | How Crossover Speeds Up Building-Block Assembly in Genetic Algorithms | http://arxiv.org/abs/1403.6600 | id:1403.6600 author:Dirk Sudholt category:cs.NE cs.DS  published:2014-03-26 summary:We re-investigate a fundamental question: how effective is crossover in Genetic Algorithms in combining building blocks of good solutions? Although this has been discussed controversially for decades, we are still lacking a rigorous and intuitive answer. We provide such answers for royal road functions and OneMax, where every bit is a building block. For the latter we show that using crossover makes every ($\mu$+$\lambda$) Genetic Algorithm at least twice as fast as the fastest evolutionary algorithm using only standard bit mutation, up to small-order terms and for moderate $\mu$ and $\lambda$. Crossover is beneficial because it effectively turns fitness-neutral mutations into improvements by combining the right building blocks at a later stage. Compared to mutation-based evolutionary algorithms, this makes multi-bit mutations more useful. Introducing crossover changes the optimal mutation rate on OneMax from $1/n$ to $(1+\sqrt{5})/2 \cdot 1/n \approx 1.618/n$. This holds both for uniform crossover and $k$-point crossover. Experiments and statistical tests confirm that our findings apply to a broad class of building-block functions. version:2
arxiv-1411-5595 | Linking GloVe with word2vec | http://arxiv.org/abs/1411.5595 | id:1411.5595 author:Tianze Shi, Zhiyuan Liu category:cs.CL cs.LG stat.ML  published:2014-11-20 summary:The Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al. is reported to be an efficient and effective method for learning vector representations of words. State-of-the-art performance is also provided by skip-gram with negative-sampling (SGNS) implemented in the word2vec tool. In this note, we explain the similarities between the training objectives of the two models, and show that the objective of SGNS is similar to the objective of a specialized form of GloVe, though their cost functions are defined differently. version:2
arxiv-1411-7113 | Real time Detection of Lane Markers in Urban Streets | http://arxiv.org/abs/1411.7113 | id:1411.7113 author:Mohamed Aly category:cs.CV cs.RO  published:2014-11-26 summary:We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques. version:1
arxiv-1404-4114 | Structured Stochastic Variational Inference | http://arxiv.org/abs/1404.4114 | id:1404.4114 author:Matthew D. Hoffman, David M. Blei category:cs.LG  published:2014-04-16 summary:Stochastic variational inference makes it possible to approximate posterior distributions induced by large datasets quickly using stochastic optimization. The algorithm relies on the use of fully factorized variational distributions. However, this "mean-field" independence approximation limits the fidelity of the posterior approximation, and introduces local optima. We show how to relax the mean-field approximation to allow arbitrary dependencies between global parameters and local hidden variables, producing better parameter estimates by reducing bias, sensitivity to local optima, and sensitivity to hyperparameters. version:3
arxiv-1312-5857 | A Generative Product-of-Filters Model of Audio | http://arxiv.org/abs/1312.5857 | id:1312.5857 author:Dawen Liang, Matthew D. Hoffman, Gautham J. Mysore category:stat.ML cs.LG  published:2013-12-20 summary:We propose the product-of-filters (PoF) model, a generative model that decomposes audio spectra as sparse linear combinations of "filters" in the log-spectral domain. PoF makes similar assumptions to those used in the classic homomorphic filtering approach to signal processing, but replaces hand-designed decompositions built of basic signal processing operations with a learned decomposition based on statistical inference. This paper formulates the PoF model and derives a mean-field method for posterior inference and a variational EM algorithm to estimate the model's free parameters. We demonstrate PoF's potential for audio processing on a bandwidth expansion task, and show that PoF can serve as an effective unsupervised feature extractor for a speaker identification task. version:5
arxiv-1411-1971 | Power-Law Graph Cuts | http://arxiv.org/abs/1411.1971 | id:1411.1971 author:Xiangyang Zhou, Jiaxin Zhang, Brian Kulis category:cs.CV cs.LG stat.ML  published:2014-10-29 summary:Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human-segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the resulting objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms. version:2
arxiv-1409-7985 | The Utility of Text: The Case of Amicus Briefs and the Supreme Court | http://arxiv.org/abs/1409.7985 | id:1409.7985 author:Yanchuan Sim, Bryan Routledge, Noah A. Smith category:cs.CL cs.AI cs.GT cs.LG  published:2014-09-29 summary:We explore the idea that authoring a piece of text is an act of maximizing one's expected utility. To make this idea concrete, we consider the societally important decisions of the Supreme Court of the United States. Extensive past work in quantitative political science provides a framework for empirically modeling the decisions of justices and how they relate to text. We incorporate into such a model texts authored by amici curiae ("friends of the court" separate from the litigants) who seek to weigh in on the decision, then explicitly model their goals in a random utility model. We demonstrate the benefits of this approach in improved vote prediction and the ability to perform counterfactual analysis. version:5
arxiv-1411-7014 | Efficient Algorithms for Bayesian Network Parameter Learning from Incomplete Data | http://arxiv.org/abs/1411.7014 | id:1411.7014 author:Guy Van den Broeck, Karthika Mohan, Arthur Choi, Judea Pearl category:cs.LG cs.AI  published:2014-11-25 summary:We propose an efficient family of algorithms to learn the parameters of a Bayesian network from incomplete data. In contrast to textbook approaches such as EM and the gradient method, our approach is non-iterative, yields closed form parameter estimates, and eliminates the need for inference in a Bayesian network. Our approach provides consistent parameter estimates for missing data problems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach is orders of magnitude faster than EM (as our approach requires no inference). Given sufficient data, we learn parameters that can be orders of magnitude more accurate. version:1
arxiv-1411-6948 | PLUTO: Penalized Unbiased Logistic Regression Trees | http://arxiv.org/abs/1411.6948 | id:1411.6948 author:Wenwen Zhang, Wei-Yin Loh category:stat.ML stat.ME  published:2014-11-25 summary:We propose a new algorithm called PLUTO for building logistic regression trees to binary response data. PLUTO can capture the nonlinear and interaction patterns in messy data by recursively partitioning the sample space. It fits a simple or a multiple linear logistic regression model in each partition. PLUTO employs the cyclical coordinate descent method for estimation of multiple linear logistic regression models with elastic net penalties, which allows it to deal with high-dimensional data efficiently. The tree structure comprises a graphical description of the data. Together with the logistic regression models, it provides an accurate classifier as well as a piecewise smooth estimate of the probability of "success". PLUTO controls selection bias by: (1) separating split variable selection from split point selection; (2) applying an adjusted chi-squared test to find the split variable instead of exhaustive search. A bootstrap calibration technique is employed to further correct selection bias. Comparison on real datasets shows that on average, the multiple linear PLUTO models predict more accurately than other algorithms. version:1
arxiv-1411-6912 | Short-Term Memory Through Persistent Activity: Evolution of Self-Stopping and Self-Sustaining Activity in Spiking Neural Networks | http://arxiv.org/abs/1411.6912 | id:1411.6912 author:Julien Hubert, Takashi Ikegami category:cs.NE q-bio.NC  published:2014-11-25 summary:Memories in the brain are separated in two categories: short-term and long-term memories. Long-term memories remain for a lifetime, while short-term ones exist from a few milliseconds to a few minutes. Within short-term memory studies, there is debate about what neural structure could implement it. Indeed, mechanisms responsible for long-term memories appear inadequate for the task. Instead, it has been proposed that short-term memories could be sustained by the persistent activity of a group of neurons. In this work, we explore what topology could sustain short-term memories, not by designing a model from specific hypotheses, but through Darwinian evolution in order to obtain new insights into its implementation. We evolved 10 networks capable of retaining information for a fixed duration between 2 and 11s. Our main finding has been that the evolution naturally created two functional modules in the network: one which sustains the information containing primarily excitatory neurons, while the other, which is responsible for forgetting, was composed mainly of inhibitory neurons. This demonstrates how the balance between inhibition and excitation plays an important role in cognition. version:1
arxiv-1411-6909 | Image Classification and Retrieval from User-Supplied Tags | http://arxiv.org/abs/1411.6909 | id:1411.6909 author:Hamid Izadinia, Ali Farhadi, Aaron Hertzmann, Matthew D. Hoffman category:cs.CV  published:2014-11-25 summary:This paper proposes direct learning of image classification from user-supplied tags, without filtering. Each tag is supplied by the user who shared the image online. Enormous numbers of these tags are freely available online, and they give insight about the image categories important to users and to image classification. Our approach is complementary to the conventional approach of manual annotation, which is extremely costly. We analyze of the Flickr 100 Million Image dataset, making several useful observations about the statistics of these tags. We introduce a large-scale robust classification algorithm, in order to handle the inherent noise in these tags, and a calibration procedure to better predict objective annotations. We show that freely available, user-supplied tags can obtain similar or superior results to large databases of costly manual annotations. version:1
arxiv-1204-3573 | Learning Sets with Separating Kernels | http://arxiv.org/abs/1204.3573 | id:1204.3573 author:Ernesto De Vito, Lorenzo Rosasco, Alessandro Toigo category:stat.ML  published:2012-04-16 summary:We consider the problem of learning a set from random samples. We show how relevant geometric and topological properties of a set can be studied analytically using concepts from the theory of reproducing kernel Hilbert spaces. A new kind of reproducing kernel, that we call separating kernel, plays a crucial role in our study and is analyzed in detail. We prove a new analytic characterization of the support of a distribution, that naturally leads to a family of provably consistent regularized learning algorithms and we discuss the stability of these methods with respect to random sampling. Numerical experiments show that the approach is competitive, and often better, than other state of the art techniques. version:2
arxiv-1412-6149 | Design, Implementation and Simulation of a Cloud Computing System for Enhancing Real-time Video Services by using VANET and Onboard Navigation Systems | http://arxiv.org/abs/1412.6149 | id:1412.6149 author:Karim Hammoudi, Nabil Ajam, Mohamed Kasraoui, Fadi Dornaika, Karan Radhakrishnan, Karthik Bandi, Qing Cai, Sai Liu category:cs.NI cs.CV  published:2014-11-25 summary:In this paper, we propose a design for novel and experimental cloud computing systems. The proposed system aims at enhancing computational, communicational and annalistic capabilities of road navigation services by merging several independent technologies, namely vision-based embedded navigation systems, prominent Cloud Computing Systems (CCSs) and Vehicular Ad-hoc NETwork (VANET). This work presents our initial investigations by describing the design of a global generic system. The designed system has been experimented with various scenarios of video-based road services. Moreover, the associated architecture has been implemented on a small-scale simulator of an in-vehicle embedded system. The implemented architecture has been experimented in the case of a simulated road service to aid the police agency. The goal of this service is to recognize and track searched individuals and vehicles in a real-time monitoring system remotely connected to moving cars. The presented work demonstrates the potential of our system for efficiently enhancing and diversifying real-time video services in road environments. version:1
arxiv-1411-6850 | Similarity- based approach for outlier detection | http://arxiv.org/abs/1411.6850 | id:1411.6850 author:Amina Dik, Khalid Jebari, Abdelaziz Bouroumi, Aziz Ettouhami category:cs.CV  published:2014-11-25 summary:This paper presents a new approach for detecting outliers by introducing the notion of object's proximity. The main idea is that normal point has similar characteristics with several neighbors. So the point in not an outlier if it has a high degree of proximity and its neighbors are several. The performance of this approach is illustrated through real datasets version:1
arxiv-1407-2170 | Orientation covariant aggregation of local descriptors with embeddings | http://arxiv.org/abs/1407.2170 | id:1407.2170 author:Giorgos Tolias, Teddy Furon, HervÃ© JÃ©gou category:cs.CV  published:2014-07-08 summary:Image search systems based on local descriptors typically achieve orientation invariance by aligning the patches on their dominant orientations. Albeit successful, this choice introduces too much invariance because it does not guarantee that the patches are rotated consistently. This paper introduces an aggregation strategy of local descriptors that achieves this covariance property by jointly encoding the angle in the aggregation stage in a continuous manner. It is combined with an efficient monomial embedding to provide a codebook-free method to aggregate local descriptors into a single vector representation. Our strategy is also compatible and employed with several popular encoding methods, in particular bag-of-words, VLAD and the Fisher vector. Our geometric-aware aggregation strategy is effective for image search, as shown by experiments performed on standard benchmarks for image and particular object retrieval, namely Holidays and Oxford buildings. version:2
arxiv-1411-6768 | Hypotheses of neural code and the information model of the neuron-detector | http://arxiv.org/abs/1411.6768 | id:1411.6768 author:Yuri Parzhin category:cs.NE cs.AI q-bio.NC I.2.0; I.2.6; I.5.1  published:2014-11-25 summary:This paper deals with the problem of neural code solving. On the basis of the formulated hypotheses the information model of a neuron-detector is suggested, the detector being one of the basic elements of an artificial neural network (ANN). The paper subjects the connectionist paradigm of ANN building to criticism and suggests a new presentation paradigm for ANN building and neuroelements (NE) learning. The adequacy of the suggested model is proved by the fact that is does not contradict the modern propositions of neuropsychology and neurophysiology. version:1
arxiv-1411-5881 | Hardware-Amenable Structural Learning for Spike-based Pattern Classification using a Simple Model of Active Dendrites | http://arxiv.org/abs/1411.5881 | id:1411.5881 author:Shaista Hussain, Shih-Chii Liu, Arindam Basu category:cs.NE q-bio.NC  published:2014-11-20 summary:This paper presents a spike-based model which employs neurons with functionally distinct dendritic compartments for classifying high dimensional binary patterns. The synaptic inputs arriving on each dendritic subunit are nonlinearly processed before being linearly integrated at the soma, giving the neuron a capacity to perform a large number of input-output mappings. The model utilizes sparse synaptic connectivity; where each synapse takes a binary value. The optimal connection pattern of a neuron is learned by using a simple hardware-friendly, margin enhancing learning algorithm inspired by the mechanism of structural plasticity in biological neurons. The learning algorithm groups correlated synaptic inputs on the same dendritic branch. Since the learning results in modified connection patterns, it can be incorporated into current event-based neuromorphic systems with little overhead. This work also presents a branch-specific spike-based version of this structural plasticity rule. The proposed model is evaluated on benchmark binary classification problems and its performance is compared against that achieved using Support Vector Machine (SVM) and Extreme Learning Machine (ELM) techniques. Our proposed method attains comparable performance while utilizing 10 to 50% less computational resources than the other reported techniques. version:2
arxiv-1411-6725 | Accelerated Parallel Optimization Methods for Large Scale Machine Learning | http://arxiv.org/abs/1411.6725 | id:1411.6725 author:Haipeng Luo, Patrick Haffner, Jean-Francois Paiement category:cs.LG  published:2014-11-25 summary:The growing amount of high dimensional data in different machine learning applications requires more efficient and scalable optimization algorithms. In this work, we consider combining two techniques, parallelism and Nesterov's acceleration, to design faster algorithms for L1-regularized loss. We first simplify BOOM, a variant of gradient descent, and study it in a unified framework, which allows us to not only propose a refined measurement of sparsity to improve BOOM, but also show that BOOM is provably slower than FISTA. Moving on to parallel coordinate descent methods, we then propose an efficient accelerated version of Shotgun, improving the convergence rate from $O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysis compared to previous work, and also allows one to study several connected work in a unified way. version:1
arxiv-1406-5549 | Fast Edge Detection Using Structured Forests | http://arxiv.org/abs/1406.5549 | id:1406.5549 author:Piotr DollÃ¡r, C. Lawrence Zitnick category:cs.CV  published:2014-06-20 summary:Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains realtime performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets. version:2
arxiv-1411-6699 | One Vector is Not Enough: Entity-Augmented Distributional Semantics for Discourse Relations | http://arxiv.org/abs/1411.6699 | id:1411.6699 author:Yangfeng Ji, Jacob Eisenstein category:cs.CL cs.LG  published:2014-11-25 summary:Discourse relations bind smaller linguistic units into coherent texts. However, automatically identifying discourse relations is difficult, because it requires understanding the semantics of the linked arguments. A more subtle challenge is that it is not enough to represent the meaning of each argument of a discourse relation, because the relation may depend on links between lower-level components, such as entity mentions. Our solution computes distributional meaning representations by composition up the syntactic parse tree. A key difference from previous work on compositional distributional semantics is that we also compute representations for entity mentions, using a novel downward compositional pass. Discourse relations are predicted from the distributional representations of the arguments, and also of their coreferent entity mentions. The resulting system obtains substantial improvements over the previous state-of-the-art in predicting implicit discourse relations in the Penn Discourse Treebank. version:1
arxiv-1411-6651 | A Greedy, Flexible Algorithm to Learn an Optimal Bayesian Network Structure | http://arxiv.org/abs/1411.6651 | id:1411.6651 author:Amir Arsalan Soltani category:cs.AI stat.ML  published:2014-11-24 summary:In this report paper we first present a report of the Advanced Machine Learning Course Project on the provided data set and then present a novel heuristic algorithm for exact Bayesian network (BN) structure discovery that uses decomposable scoring functions. Our algorithm follows a different approach to solve the problem of BN structure discovery than the previously used methods such as Dynamic Programming (DP) and Branch and Bound to reduce the search space and find the global optima space for the problem. The algorithm we propose has some degree of flexibility that can make it more or less greedy. The more the algorithm is set to be greedy, the more the speed of the algorithm will be, and the less optimal the final structure. Our algorithm runs in a much less time than the previously known methods and guarantees to have an optimality of close to 99%. Therefore, it sacrifices less than one percent of score of an optimal structure in order to gain a much lower running time and make the algorithm feasible for large data sets (we may note that we never used any toolbox except for result validation) version:1
arxiv-1402-4069 | Application of the Ring Theory in the Segmentation of Digital Images | http://arxiv.org/abs/1402.4069 | id:1402.4069 author:Yasel GarcÃ©s, Esley Torres, Osvaldo Pereira, Roberto RodrÃ­guez category:cs.CV  published:2014-02-17 summary:Ring theory is one of the branches of the abstract algebra that has been broadly used in images. However, ring theory has not been very related with image segmentation. In this paper, we propose a new index of similarity among images using Zn rings and the entropy function. This new index was applied as a new stopping criterion to the Mean Shift Iterative Algorithm with the goal to reach a better segmentation. An analysis on the performance of the algorithm with this new stopping criterion is carried out. The obtained results proved that the new index is a suitable tool to compare images. version:2
arxiv-1411-6590 | Consistency of Cheeger and Ratio Graph Cuts | http://arxiv.org/abs/1411.6590 | id:1411.6590 author:Nicolas Garcia Trillos, Dejan Slepcev, James von Brecht, Thomas Laurent, Xavier Bresson category:stat.ML cs.LG math.ST stat.TH  published:2014-11-24 summary:This paper establishes the consistency of a family of graph-cut-based algorithms for clustering of data clouds. We consider point clouds obtained as samples of a ground-truth measure. We investigate approaches to clustering based on minimizing objective functionals defined on proximity graphs of the given sample. Our focus is on functionals based on graph cuts like the Cheeger and ratio cuts. We show that minimizers of the these cuts converge as the sample size increases to a minimizer of a corresponding continuum cut (which partitions the ground truth measure). Moreover, we obtain sharp conditions on how the connectivity radius can be scaled with respect to the number of sample points for the consistency to hold. We provide results for two-way and for multiway cuts. Furthermore we provide numerical experiments that illustrate the results and explore the optimality of scaling in dimension two. version:1
arxiv-1312-2171 | bartMachine: Machine Learning with Bayesian Additive Regression Trees | http://arxiv.org/abs/1312.2171 | id:1312.2171 author:Adam Kapelner, Justin Bleich category:stat.ML cs.LG  published:2013-12-08 summary:We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data. version:3
arxiv-1411-6622 | Noise Benefits in Expectation-Maximization Algorithms | http://arxiv.org/abs/1411.6622 | id:1411.6622 author:Osonde Adekorede Osoba category:stat.ML cs.LG math.ST stat.TH  published:2014-11-24 summary:This dissertation shows that careful injection of noise into sample data can substantially speed up Expectation-Maximization algorithms. Expectation-Maximization algorithms are a class of iterative algorithms for extracting maximum likelihood estimates from corrupted or incomplete data. The convergence speed-up is an example of a noise benefit or "stochastic resonance" in statistical signal processing. The dissertation presents derivations of sufficient conditions for such noise-benefits and demonstrates the speed-up in some ubiquitous signal-processing algorithms. These algorithms include parameter estimation for mixture models, the $k$-means clustering algorithm, the Baum-Welch algorithm for training hidden Markov models, and backpropagation for training feedforward artificial neural networks. This dissertation also analyses the effects of data and model corruption on the more general Bayesian inference estimation framework. The main finding is a theorem guaranteeing that uniform approximators for Bayesian model functions produce uniform approximators for the posterior pdf via Bayes theorem. This result also applies to hierarchical and multidimensional Bayesian models. version:1
arxiv-1411-6520 | Distributed Coordinate Descent for L1-regularized Logistic Regression | http://arxiv.org/abs/1411.6520 | id:1411.6520 author:Ilya Trofimov, Alexander Genkin category:stat.ML cs.LG  published:2014-11-24 summary:Solving logistic regression with L1-regularization in distributed settings is an important problem. This problem arises when training dataset is very large and cannot fit the memory of a single machine. We present d-GLMNET, a new algorithm solving logistic regression with L1-regularization in the distributed settings. We empirically show that it is superior over distributed online learning via truncated gradient. version:1
arxiv-1411-6509 | Persistent Evidence of Local Image Properties in Generic ConvNets | http://arxiv.org/abs/1411.6509 | id:1411.6509 author:Ali Sharif Razavian, Hossein Azizpour, Atsuto Maki, Josephine Sullivan, Carl Henrik Ek, Stefan Carlsson category:cs.CV  published:2014-11-24 summary:Supervised training of a convolutional network for object classification should make explicit any information related to the class of objects and disregard any auxiliary information associated with the capture of the image or the variation within the object class. Does this happen in practice? Although this seems to pertain to the very final layers in the network, if we look at earlier layers we find that this is not the case. Surprisingly, strong spatial information is implicit. This paper addresses this, in particular, exploiting the image representation at the first fully connected layer, i.e. the global image descriptor which has been recently shown to be most effective in a range of visual recognition tasks. We empirically demonstrate evidences for the finding in the contexts of four different tasks: 2d landmark detection, 2d object keypoints prediction, estimation of the RGB values of input image, and recovery of semantic label of each pixel. We base our investigation on a simple framework with ridge rigression commonly across these tasks, and show results which all support our insight. Such spatial information can be used for computing correspondence of landmarks to a good accuracy, but should potentially be useful for improving the training of the convolutional nets for classification purposes. version:1
