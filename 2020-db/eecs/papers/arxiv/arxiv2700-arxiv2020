arxiv-1301-1576 | Optical Flow on Evolving Surfaces with an Application to the Analysis of 4D Microscopy Data | http://arxiv.org/abs/1301.1576 | id:1301.1576 author:Clemens Kirisits, Lukas F. Lang, Otmar Scherzer category:math.OC cs.CV  published:2013-01-08 summary:We extend the concept of optical flow to a dynamic non-Euclidean setting. Optical flow is traditionally computed from a sequence of flat images. It is the purpose of this paper to introduce variational motion estimation for images that are defined on an evolving surface. Volumetric microscopy images depicting a live zebrafish embryo serve as both biological motivation and test data. version:2
arxiv-1305-4757 | Power to the Points: Validating Data Memberships in Clusterings | http://arxiv.org/abs/1305.4757 | id:1305.4757 author:Parasaran Raman, Suresh Venkatasubramanian category:cs.LG cs.CG  published:2013-05-21 summary:A clustering is an implicit assignment of labels of points, based on proximity to other points. It is these labels that are then used for downstream analysis (either focusing on individual clusters, or identifying representatives of clusters and so on). Thus, in order to trust a clustering as a first step in exploratory data analysis, we must trust the labels assigned to individual data. Without supervision, how can we validate this assignment? In this paper, we present a method to attach affinity scores to the implicit labels of individual points in a clustering. The affinity scores capture the confidence level of the cluster that claims to "own" the point. This method is very general: it can be used with clusterings derived from Euclidean data, kernelized data, or even data derived from information spaces. It smoothly incorporates importance functions on clusters, allowing us to eight different clusters differently. It is also efficient: assigning an affinity score to a point depends only polynomially on the number of clusters and is independent of the number of points in the data. The dimensionality of the underlying space only appears in preprocessing. We demonstrate the value of our approach with an experimental study that illustrates the use of these scores in different data analysis tasks, as well as the efficiency and flexibility of the method. We also demonstrate useful visualizations of these scores; these might prove useful within an interactive analytics framework. version:1
arxiv-1305-4723 | On the Complexity Analysis of Randomized Block-Coordinate Descent Methods | http://arxiv.org/abs/1305.4723 | id:1305.4723 author:Zhaosong Lu, Lin Xiao category:math.OC cs.LG cs.NA math.NA stat.ML  published:2013-05-21 summary:In this paper we analyze the randomized block-coordinate descent (RBCD) methods proposed in [8,11] for minimizing the sum of a smooth convex function and a block-separable convex function. In particular, we extend Nesterov's technique developed in [8] for analyzing the RBCD method for minimizing a smooth convex function over a block-separable closed convex set to the aforementioned more general problem and obtain a sharper expected-value type of convergence rate than the one implied in [11]. Also, we obtain a better high-probability type of iteration complexity, which improves upon the one in [11] by at least the amount $O(n/\epsilon)$, where $\epsilon$ is the target solution accuracy and $n$ is the number of problem blocks. In addition, for unconstrained smooth convex minimization, we develop a new technique called {\it randomized estimate sequence} to analyze the accelerated RBCD method proposed by Nesterov [11] and establish a sharper expected-value type of convergence rate than the one given in [11]. version:1
arxiv-1210-0645 | Nonparametric Unsupervised Classification | http://arxiv.org/abs/1210.0645 | id:1210.0645 author:Yingzhen Yang, Thomas S. Huang category:cs.LG stat.ML  published:2012-10-02 summary:Unsupervised classification methods learn a discriminative classifier from unlabeled data, which has been proven to be an effective way of simultaneously clustering the data and training a classifier from the data. Various unsupervised classification methods obtain appealing results by the classifiers learned in an unsupervised manner. However, existing methods do not consider the misclassification error of the unsupervised classifiers except unsupervised SVM, so the performance of the unsupervised classifiers is not fully evaluated. In this work, we study the misclassification error of two popular classifiers, i.e. the nearest neighbor classifier (NN) and the plug-in classifier, in the setting of unsupervised classification. version:5
arxiv-1302-3268 | Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem | http://arxiv.org/abs/1302.3268 | id:1302.3268 author:Ittai Abraham, Omar Alonso, Vasilis Kandylas, Aleksandrs Slivkins category:cs.LG  published:2013-02-13 summary:Very recently crowdsourcing has become the de facto platform for distributing and collecting human computation for a wide range of tasks and applications such as information retrieval, natural language processing and machine learning. Current crowdsourcing platforms have some limitations in the area of quality control. Most of the effort to ensure good quality has to be done by the experimenter who has to manage the number of workers needed to reach good results. We propose a simple model for adaptive quality control in crowdsourced multiple-choice tasks which we call the \emph{bandit survey problem}. This model is related to, but technically different from the well-known multi-armed bandit problem. We present several algorithms for this problem, and support them with analysis and simulations. Our approach is based in our experience conducting relevance evaluation for a large commercial search engine. version:2
arxiv-1305-4544 | Efficient Image Retargeting for High Dynamic Range Scenes | http://arxiv.org/abs/1305.4544 | id:1305.4544 author:Govind Salvi, Puneet Sharma, Shanmuganathan Raman category:cs.CV  published:2013-05-20 summary:Most of the real world scenes have a very high dynamic range (HDR). The mobile phone cameras and the digital cameras available in markets are limited in their capability in both the range and spatial resolution. Same argument can be posed about the limited dynamic range display devices which also differ in the spatial resolution and aspect ratios. In this paper, we address the problem of displaying the high contrast low dynamic range (LDR) image of a HDR scene in a display device which has different spatial resolution compared to that of the capturing digital camera. The optimal solution proposed in this work can be employed with any camera which has the ability to shoot multiple differently exposed images of a scene. Further, the proposed solutions provide the flexibility in the depiction of entire contrast of the HDR scene as a LDR image with an user specified spatial resolution. This task is achieved through an optimized content aware retargeting framework which preserves salient features along with the algorithm to combine multi-exposure images. We show the proposed approach performs exceedingly well in the generation of high contrast LDR image of varying spatial resolution compared to an alternate approach. version:1
arxiv-1203-0970 | Infinite Shift-invariant Grouped Multi-task Learning for Gaussian Processes | http://arxiv.org/abs/1203.0970 | id:1203.0970 author:Yuyang Wang, Roni Khardon, Pavlos Protopapas category:cs.LG astro-ph.IM stat.ML  published:2012-03-05 summary:Multi-task learning leverages shared information among data sets to improve the learning performance of individual tasks. The paper applies this framework for data where each task is a phase-shifted periodic time series. In particular, we develop a novel Bayesian nonparametric model capturing a mixture of Gaussian processes where each task is a sum of a group-specific function and a component capturing individual variation, in addition to each task being phase shifted. We develop an efficient \textsc{em} algorithm to learn the parameters of the model. As a special case we obtain the Gaussian mixture model and \textsc{em} algorithm for phased-shifted periodic time series. Furthermore, we extend the proposed model by using a Dirichlet Process prior and thereby leading to an infinite mixture model that is capable of doing automatic model selection. A Variational Bayesian approach is developed for inference in this model. Experiments in regression, classification and class discovery demonstrate the performance of the proposed models using both synthetic data and real-world time series data from astrophysics. Our methods are particularly useful when the time series are sparsely and non-synchronously sampled. version:2
arxiv-1305-4433 | Meta Path-Based Collective Classification in Heterogeneous Information Networks | http://arxiv.org/abs/1305.4433 | id:1305.4433 author:Xiangnan Kong, Bokai Cao, Philip S. Yu, Ying Ding, David J. Wild category:cs.LG stat.ML  published:2013-05-20 summary:Collective classification has been intensively studied due to its impact in many important applications, such as web mining, bioinformatics and citation analysis. Collective classification approaches exploit the dependencies of a group of linked objects whose class labels are correlated and need to be predicted simultaneously. In this paper, we focus on studying the collective classification problem in heterogeneous networks, which involves multiple types of data objects interconnected by multiple types of links. Intuitively, two objects are correlated if they are linked by many paths in the network. However, most existing approaches measure the dependencies among objects through directly links or indirect links without considering the different semantic meanings behind different paths. In this paper, we study the collective classification problem taht is defined among the same type of objects in heterogenous networks. Moreover, by considering different linkage paths in the network, one can capture the subtlety of different types of dependencies among objects. We introduce the concept of meta-path based dependencies among objects, where a meta path is a path consisting a certain sequence of linke types. We show that the quality of collective classification results strongly depends upon the meta paths used. To accommodate the large network size, a novel solution, called HCC (meta-path based Heterogenous Collective Classification), is developed to effectively assign labels to a group of instances that are interconnected through different meta-paths. The proposed HCC model can capture different types of dependencies among objects with respect to different meta paths. Empirical studies on real-world networks demonstrate that effectiveness of the proposed meta path-based collective classification approach. version:1
arxiv-1303-2430 | Quantum and Concept Combination, Entangled Measurements and Prototype Theory | http://arxiv.org/abs/1303.2430 | id:1303.2430 author:Diederik Aerts category:cs.AI cs.CL quant-ph  published:2013-03-11 summary:We analyze the meaning of the violation of the marginal probability law for situations of correlation measurements where entanglement is identified. We show that for quantum theory applied to the cognitive realm such a violation does not lead to the type of problems commonly believed to occur in situations of quantum theory applied to the physical realm. We briefly situate our quantum approach for modeling concepts and their combinations with respect to the notions of 'extension' and 'intension' in theories of meaning, and in existing concept theories. version:2
arxiv-1305-4345 | Ensembles of Classifiers based on Dimensionality Reduction | http://arxiv.org/abs/1305.4345 | id:1305.4345 author:Alon Schclar, Lior Rokach, Amir Amit category:cs.LG  published:2013-05-19 summary:We present a novel approach for the construction of ensemble classifiers based on dimensionality reduction. Dimensionality reduction methods represent datasets using a small number of attributes while preserving the information conveyed by the original dataset. The ensemble members are trained based on dimension-reduced versions of the training set. These versions are obtained by applying dimensionality reduction to the original training set using different values of the input parameters. This construction meets both the diversity and accuracy criteria which are required to construct an ensemble classifier where the former criterion is obtained by the various input parameter values and the latter is achieved due to the decorrelation and noise reduction properties of dimensionality reduction. In order to classify a test sample, it is first embedded into the dimension reduced space of each individual classifier by using an out-of-sample extension algorithm. Each classifier is then applied to the embedded sample and the classification is obtained via a voting scheme. We present three variations of the proposed approach based on the Random Projections, the Diffusion Maps and the Random Subspaces dimensionality reduction algorithms. We also present a multi-strategy ensemble which combines AdaBoost and Diffusion Maps. A comparison is made with the Bagging, AdaBoost, Rotation Forest ensemble classifiers and also with the base classifier which does not incorporate dimensionality reduction. Our experiments used seventeen benchmark datasets from the UCI repository. The results obtained by the proposed algorithms were superior in many cases to other algorithms. version:1
arxiv-1305-4339 | Generalized Centroid Estimators in Bioinformatics | http://arxiv.org/abs/1305.4339 | id:1305.4339 author:Michiaki Hamada, Hisanori Kiryu, Wataru Iwasaki, Kiyoshi Asai category:q-bio.QM cs.LG  published:2013-05-19 summary:In a number of estimation problems in bioinformatics, accuracy measures of the target problem are usually given, and it is important to design estimators that are suitable to those accuracy measures. However, there is often a discrepancy between an employed estimator and a given accuracy measure of the problem. In this study, we introduce a general class of efficient estimators for estimation problems on high-dimensional binary spaces, which representmany fundamental problems in bioinformatics. Theoretical analysis reveals that the proposed estimators generally fit with commonly-used accuracy measures (e.g. sensitivity, PPV, MCC and F-score) as well as it can be computed efficiently in many cases, and cover a wide range of problems in bioinformatics from the viewpoint of the principle of maximum expected accuracy (MEA). It is also shown that some important algorithms in bioinformatics can be interpreted in a unified manner. Not only the concept presented in this paper gives a useful framework to design MEA-based estimators but also it is highly extendable and sheds new light on many problems in bioinformatics. version:1
arxiv-1305-4325 | Quantum Annealing for Dirichlet Process Mixture Models with Applications to Network Clustering | http://arxiv.org/abs/1305.4325 | id:1305.4325 author:Issei Sato, Shu Tanaka, Kenichi Kurihara, Seiji Miyashita, Hiroshi Nakagawa category:cond-mat.dis-nn cond-mat.stat-mech quant-ph stat.ML  published:2013-05-19 summary:We developed a new quantum annealing (QA) algorithm for Dirichlet process mixture (DPM) models based on the Chinese restaurant process (CRP). QA is a parallelized extension of simulated annealing (SA), i.e., it is a parallel stochastic optimization technique. Existing approaches [Kurihara et al. UAI2009, Sato et al. UAI2009] and cannot be applied to the CRP because their QA framework is formulated using a fixed number of mixture components. The proposed QA algorithm can handle an unfixed number of classes in mixture models. We applied QA to a DPM model for clustering vertices in a network where a CRP seating arrangement indicates a network partition. A multi core processor was used for running QA in experiments, the results of which show that QA is better than SA, Markov chain Monte Carlo inference, and beam search at finding a maximum a posteriori estimation of a seating arrangement in the CRP. Since our QA algorithm is as easy as to implement the SA algorithm, it is suitable for a wide range of applications. version:1
arxiv-1305-4324 | Horizon-Independent Optimal Prediction with Log-Loss in Exponential Families | http://arxiv.org/abs/1305.4324 | id:1305.4324 author:Peter Bartlett, Peter Grunwald, Peter Harremoes, Fares Hedayati, Wojciech Kotlowski category:cs.LG stat.ML  published:2013-05-19 summary:We study online learning under logarithmic loss with regular parametric models. Hedayati and Bartlett (2012b) showed that a Bayesian prediction strategy with Jeffreys prior and sequential normalized maximum likelihood (SNML) coincide and are optimal if and only if the latter is exchangeable, and if and only if the optimal strategy can be calculated without knowing the time horizon in advance. They put forward the question what families have exchangeable SNML strategies. This paper fully answers this open problem for one-dimensional exponential families. The exchangeability can happen only for three classes of natural exponential family distributions, namely the Gaussian, Gamma, and the Tweedie exponential family of order 3/2. Keywords: SNML Exchangeability, Exponential Family, Online Learning, Logarithmic Loss, Bayesian Strategy, Jeffreys Prior, Fisher Information1 version:1
arxiv-1302-2157 | Passive Learning with Target Risk | http://arxiv.org/abs/1302.2157 | id:1302.2157 author:Mehrdad Mahdavi, Rong Jin category:cs.LG  published:2013-02-08 summary:In this paper we consider learning in passive setting but with a slight modification. We assume that the target expected loss, also referred to as target risk, is provided in advance for learner as prior knowledge. Unlike most studies in the learning theory that only incorporate the prior knowledge into the generalization bounds, we are able to explicitly utilize the target risk in the learning process. Our analysis reveals a surprising result on the sample complexity of learning: by exploiting the target risk in the learning algorithm, we show that when the loss function is both strongly convex and smooth, the sample complexity reduces to $\O(\log (\frac{1}{\epsilon}))$, an exponential improvement compared to the sample complexity $\O(\frac{1}{\epsilon})$ for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful. version:2
arxiv-1305-4298 | Blockwise SURE Shrinkage for Non-Local Means | http://arxiv.org/abs/1305.4298 | id:1305.4298 author:Yue Wu, Brian Tracey, Premkumar Natarajan, Joseph P. Noonan category:cs.CV  published:2013-05-18 summary:In this letter, we investigate the shrinkage problem for the non-local means (NLM) image denoising. In particular, we derive the closed-form of the optimal blockwise shrinkage for NLM that minimizes the Stein's unbiased risk estimator (SURE). We also propose a constant complexity algorithm allowing fast blockwise shrinkage. Simulation results show that the proposed blockwise shrinkage method improves NLM performance in attaining higher peak signal noise ratio (PSNR) and structural similarity index (SSIM), and makes NLM more robust against parameter changes. Similar ideas can be applicable to other patchwise image denoising techniques. version:1
arxiv-1305-4232 | Embedding Riemannian Manifolds by the Heat Kernel of the Connection Laplacian | http://arxiv.org/abs/1305.4232 | id:1305.4232 author:Hau-tieng Wu category:math.DG math.SP math.ST stat.ML stat.TH  published:2013-05-18 summary:Given a class of closed Riemannian manifolds with prescribed geometric conditions, we introduce an embedding of the manifolds into $\ell^2$ based on the heat kernel of the Connection Laplacian associated with the Levi-Civita connection on the tangent bundle. As a result, we can construct a distance in this class which leads to a pre-compactness theorem on the class under consideration. version:1
arxiv-1305-4204 | Machine learning on images using a string-distance | http://arxiv.org/abs/1305.4204 | id:1305.4204 author:Uzi Chester, Joel Ratsaby category:cs.LG cs.CV  published:2013-05-17 summary:We present a new method for image feature-extraction which is based on representing an image by a finite-dimensional vector of distances that measure how different the image is from a set of image prototypes. We use the recently introduced Universal Image Distance (UID) \cite{RatsabyChesterIEEE2012} to compare the similarity between an image and a prototype image. The advantage in using the UID is the fact that no domain knowledge nor any image analysis need to be done. Each image is represented by a finite dimensional feature vector whose components are the UID values between the image and a finite set of image prototypes from each of the feature categories. The method is automatic since once the user selects the prototype images, the feature vectors are automatically calculated without the need to do any image analysis. The prototype images can be of different size, in particular, different than the image size. Based on a collection of such cases any supervised or unsupervised learning algorithm can be used to train and produce an image classifier or image cluster analysis. In this paper we present the image feature-extraction method and use it on several supervised and unsupervised learning experiments for satellite image data. version:1
arxiv-1208-0787 | A Random Walk Based Model Incorporating Social Information for Recommendations | http://arxiv.org/abs/1208.0787 | id:1208.0787 author:Shang Shang, Sanjeev R. Kulkarni, Paul W. Cuff, Pan Hui category:cs.IR cs.LG  published:2012-08-03 summary:Collaborative filtering (CF) is one of the most popular approaches to build a recommendation system. In this paper, we propose a hybrid collaborative filtering model based on a Makovian random walk to address the data sparsity and cold start problems in recommendation systems. More precisely, we construct a directed graph whose nodes consist of items and users, together with item content, user profile and social network information. We incorporate user's ratings into edge settings in the graph model. The model provides personalized recommendations and predictions to individuals and groups. The proposed algorithms are evaluated on MovieLens and Epinions datasets. Experimental results show that the proposed methods perform well compared with other graph-based methods, especially in the cold start case. version:2
arxiv-1208-0782 | Wisdom of the Crowd: Incorporating Social Influence in Recommendation Models | http://arxiv.org/abs/1208.0782 | id:1208.0782 author:Shang Shang, Pan Hui, Sanjeev R. Kulkarni, Paul W. Cuff category:cs.IR cs.LG cs.SI physics.soc-ph  published:2012-08-03 summary:Recommendation systems have received considerable attention recently. However, most research has been focused on improving the performance of collaborative filtering (CF) techniques. Social networks, indispensably, provide us extra information on people's preferences, and should be considered and deployed to improve the quality of recommendations. In this paper, we propose two recommendation models, for individuals and for groups respectively, based on social contagion and social influence network theory. In the recommendation model for individuals, we improve the result of collaborative filtering prediction with social contagion outcome, which simulates the result of information cascade in the decision-making process. In the recommendation model for groups, we apply social influence network theory to take interpersonal influence into account to form a settled pattern of disagreement, and then aggregate opinions of group members. By introducing the concept of susceptibility and interpersonal influence, the settled rating results are flexible, and inclined to members whose ratings are "essential". version:2
arxiv-1305-4168 | Flying Triangulation - towards the 3D movie camera | http://arxiv.org/abs/1305.4168 | id:1305.4168 author:Florian Willomitzer, Svenja Ettl, Christian Faber, Gerd Häusler category:cs.CV physics.optics  published:2013-05-17 summary:Flying Triangulation sensors enable a free-hand and motion-robust 3D data acquisition of complex shaped objects. The measurement principle is based on a multi-line light-sectioning approach and uses sophisticated algorithms for real-time registration (S. Ettl et al., Appl. Opt. 51 (2012) 281-289). As "single-shot principle", light sectioning enables the option to get surface data from one single camera exposure. But there is a drawback: A pixel-dense measurement is not possible because of fundamental information-theoretical reasons. By "pixel-dense" we understand that each pixel displays individually measured distance information, neither interpolated from its neighbour pixels nor using lateral context information. Hence, for monomodal single-shot principles, the 3D data generated from one 2D raw image display a significantly lower space-bandwidth than the camera permits. This is the price one must pay for motion robustness. Currently, our sensors project about 10 lines (each with 1000 pixels), reaching an considerable lower data efficiency than theoretically possible for a single-shot sensor. Our aim is to push Flying Triangulation to its information-theoretical limits. Therefore, the line density as well as the measurement depth needs to be significantly increased. This causes serious indexing ambiguities. On the road to a single-shot 3D movie camera, we are working on solutions to overcome the problem of false line indexing by utilizing yet unexploited information. We will present several approaches and will discuss profound information-theoretical questions about the information efficiency of 3D sensors. version:1
arxiv-1305-4153 | Factored expectation propagation for input-output FHMM models in systems biology | http://arxiv.org/abs/1305.4153 | id:1305.4153 author:Botond Cseke, Guido Sanguinetti category:stat.ML  published:2013-05-17 summary:We consider the problem of joint modelling of metabolic signals and gene expression in systems biology applications. We propose an approach based on input-output factorial hidden Markov models and propose a structured variational inference approach to infer the structure and states of the model. We start from the classical free form structured variational mean field approach and use a expectation propagation to approximate the expectations needed in the variational loop. We show that this corresponds to a factored expectation constrained approximate inference. We validate our model through extensive simulations and demonstrate its applicability on a real world bacterial data set. version:1
arxiv-1305-3882 | Rule-Based Semantic Tagging. An Application Undergoing Dictionary Glosses | http://arxiv.org/abs/1305.3882 | id:1305.3882 author:Daniel Christen category:cs.CL  published:2013-05-16 summary:The project presented in this article aims to formalize criteria and procedures in order to extract semantic information from parsed dictionary glosses. The actual purpose of the project is the generation of a semantic network (nearly an ontology) issued from a monolingual Italian dictionary, through unsupervised procedures. Since the project involves rule-based Parsing, Semantic Tagging and Word Sense Disambiguation techniques, its outcomes may find an interest also beyond this immediate intent. The cooperation of both syntactic and semantic features in meaning construction are investigated, and procedures which allows a translation of syntactic dependencies in semantic relations are discussed. The procedures that rise from this project can be applied also to other text types than dictionary glosses, as they convert the output of a parsing process into a semantic representation. In addition some mechanism are sketched that may lead to a kind of procedural semantics, through which multiple paraphrases of an given expression can be generated. Which means that these techniques may find an application also in 'query expansion' strategies, interesting Information Retrieval, Search Engines and Question Answering Systems. version:2
arxiv-1302-1422 | Sémantique des déterminants dans un cadre richement typé | http://arxiv.org/abs/1302.1422 | id:1302.1422 author:Christian Retoré category:cs.CL  published:2013-02-06 summary:The variation of word meaning according to the context leads us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantifiers play a fundamental role in the construction of those formulae. But in our rich type system the usual semantic terms do not work. We propose a solution ins- pired by the tau and epsilon operators of Hilbert, kinds of generic elements and choice functions. This approach unifies the treatment of the different determi- ners and quantifiers as well as the dynamic binding of pronouns. Above all, this fully computational view fits in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint. version:2
arxiv-1305-4081 | Conditions for Convergence in Regularized Machine Learning Objectives | http://arxiv.org/abs/1305.4081 | id:1305.4081 author:Patrick Hop, Xinghao Pan category:cs.LG cs.NA math.OC  published:2013-05-17 summary:Analysis of the convergence rates of modern convex optimization algorithms can be achived through binary means: analysis of emperical convergence, or analysis of theoretical convergence. These two pathways of capturing information diverge in efficacy when moving to the world of distributed computing, due to the introduction of non-intuitive, non-linear slowdowns associated with broadcasting, and in some cases, gathering operations. Despite these nuances in the rates of convergence, we can still show the existence of convergence, and lower bounds for the rates. This paper will serve as a helpful cheat-sheet for machine learning practitioners encountering this problem class in the field. version:1
arxiv-1305-4064 | Font Acknowledgment and Character Extraction of Digital and Scanned Images | http://arxiv.org/abs/1305.4064 | id:1305.4064 author:Syed Muhammad Arsalan Bashir category:cs.CV  published:2013-05-17 summary:The font recognition and character extraction is of immense importance as these are many scenarios where data are in such a form, which cannot be processed like in image form or as a hard copy. So the procedure developed in this paper is basically related to identifying the font (Times New Roman, Arial and Comic Sans MS) and afterwards recovering the text using simple correlation based method where the binary templates are correlated to the input image text characters. All of this extraction is done in the presence of a little noise as images may have noisy patterns due to photocopying. The significance of this method exists in extraction of data from various monitoring (Surveillance) camera footages or even more. The method is developed on Matlab\c{opyright} which takes input image and recovers text and font information from it in a text file. version:1
arxiv-1305-3981 | Binary Tree based Chinese Word Segmentation | http://arxiv.org/abs/1305.3981 | id:1305.3981 author:Kaixu Zhang, Can Wang, Maosong Sun category:cs.CL  published:2013-05-17 summary:Chinese word segmentation is a fundamental task for Chinese language processing. The granularity mismatch problem is the main cause of the errors. This paper showed that the binary tree representation can store outputs with different granularity. A binary tree based framework is also designed to overcome the granularity mismatch problem. There are two steps in this framework, namely tree building and tree pruning. The tree pruning step is specially designed to focus on the granularity problem. Previous work for Chinese word segmentation such as the sequence tagging can be easily employed in this framework. This framework can also provide quantitative error analysis methods. The experiments showed that after using a more sophisticated tree pruning function for a state-of-the-art conditional random field based baseline, the error reduction can be up to 20%. version:1
arxiv-1305-3971 | Sparse Norm Filtering | http://arxiv.org/abs/1305.3971 | id:1305.3971 author:Chengxi Ye, Dacheng Tao, Mingli Song, David W. Jacobs, Min Wu category:cs.GR cs.CV cs.MM  published:2013-05-17 summary:Optimization-based filtering smoothes an image by minimizing a fidelity function and simultaneously preserves edges by exploiting a sparse norm penalty over gradients. It has obtained promising performance in practical problems, such as detail manipulation, HDR compression and deblurring, and thus has received increasing attentions in fields of graphics, computer vision and image processing. This paper derives a new type of image filter called sparse norm filter (SNF) from optimization-based filtering. SNF has a very simple form, introduces a general class of filtering techniques, and explains several classic filters as special implementations of SNF, e.g. the averaging filter and the median filter. It has advantages of being halo free, easy to implement, and low time and memory costs (comparable to those of the bilateral filter). Thus, it is more generic than a smoothing operator and can better adapt to different tasks. We validate the proposed SNF by a wide variety of applications including edge-preserving smoothing, outlier tolerant filtering, detail manipulation, HDR compression, non-blind deconvolution, image segmentation, and colorization. version:1
arxiv-1305-3939 | Analysis Of Interest Points Of Curvelet Coefficients Contributions Of Microscopic Images And Improvement Of Edges | http://arxiv.org/abs/1305.3939 | id:1305.3939 author:A. Djimeli, D. Tchiotsop, R. Tchinda category:cs.CV  published:2013-05-16 summary:This paper focuses on improved edge model based on Curvelet coefficients analysis. Curvelet transform is a powerful tool for multiresolution representation of object with anisotropic edge. Curvelet coefficients contributions have been analyzed using Scale Invariant Feature Transform (SIFT), commonly used to study local structure in images. The permutation of Curvelet coefficients from original image and edges image obtained from gradient operator is used to improve original edges. Experimental results show that this method brings out details on edges when the decomposition scale increases. version:1
arxiv-1305-3885 | Geometric primitive feature extraction - concepts, algorithms, and applications | http://arxiv.org/abs/1305.3885 | id:1305.3885 author:Dilip K. Prasad category:cs.CV cs.CG  published:2013-05-16 summary:This thesis presents important insights and concepts related to the topic of the extraction of geometric primitives from the edge contours of digital images. Three specific problems related to this topic have been studied, viz., polygonal approximation of digital curves, tangent estimation of digital curves, and ellipse fitting anddetection from digital curves. For the problem of polygonal approximation, two fundamental problems have been addressed. First, the nature of the performance evaluation metrics in relation to the local and global fitting characteristics has been studied. Second, an explicit error bound of the error introduced by digitizing a continuous line segment has been derived and used to propose a generic non-heuristic parameter independent framework which can be used in several dominant point detection methods. For the problem of tangent estimation for digital curves, a simple method of tangent estimation has been proposed. It is shown that the method has a definite upper bound of the error for conic digital curves. It has been shown that the method performs better than almost all (seventy two) existing tangent estimation methods for conic as well as several non-conic digital curves. For the problem of fitting ellipses on digital curves, a geometric distance minimization model has been considered. An unconstrained, linear, non-iterative, and numerically stable ellipse fitting method has been proposed and it has been shown that the proposed method has better selectivity for elliptic digital curves (high true positive and low false positive) as compared to several other ellipse fitting methods. For the problem of detecting ellipses in a set of digital curves, several innovative and fast pre-processing, grouping, and hypotheses evaluation concepts applicable for digital curves have been proposed and combined to form an ellipse detection method. version:1
arxiv-1305-1363 | One-Pass AUC Optimization | http://arxiv.org/abs/1305.1363 | id:1305.1363 author:Wei Gao, Rong Jin, Shenghuo Zhu, Zhi-Hua Zhou category:cs.LG  published:2013-05-07 summary:AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm. version:2
arxiv-1305-3640 | Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data | http://arxiv.org/abs/1305.3640 | id:1305.3640 author:Jan-Willem van de Meent, Jonathan E. Bronson, Frank Wood, Ruben L. Gonzalez Jr., Chris H. Wiggins category:stat.ML physics.bio-ph q-bio.QM  published:2013-05-15 summary:We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process. version:1
arxiv-1305-3616 | Modeling Information Propagation with Survival Theory | http://arxiv.org/abs/1305.3616 | id:1305.3616 author:Manuel Gomez Rodriguez, Jure Leskovec, Bernhard Schoelkopf category:cs.SI cs.DS physics.soc-ph stat.ML  published:2013-05-15 summary:Networks provide a skeleton for the spread of contagions, like, information, ideas, behaviors and diseases. Many times networks over which contagions diffuse are unobserved and need to be inferred. Here we apply survival theory to develop general additive and multiplicative risk models under which the network inference problems can be solved efficiently by exploiting their convexity. Our additive risk model generalizes several existing network inference models. We show all these models are particular cases of our more general model. Our multiplicative model allows for modeling scenarios in which a node can either increase or decrease the risk of activation of another node, in contrast with previous approaches, which consider only positive risk increments. We evaluate the performance of our network inference algorithms on large synthetic and real cascade datasets, and show that our models are able to predict the length and duration of cascades in real data. version:1
arxiv-1304-4086 | Hubiness, length, crossings and their relationships in dependency trees | http://arxiv.org/abs/1304.4086 | id:1304.4086 author:Ramon Ferrer-i-Cancho category:cs.CL cs.DM cs.SI physics.soc-ph  published:2013-04-15 summary:Here tree dependency structures are studied from three different perspectives: their degree variance (hubiness), the mean dependency length and the number of dependency crossings. Bounds that reveal pairwise dependencies among these three metrics are derived. Hubiness (the variance of degrees) plays a central role: the mean dependency length is bounded below by hubiness while the number of crossings is bounded above by hubiness. Our findings suggest that the online memory cost of a sentence might be determined not just by the ordering of words but also by the hubiness of the underlying structure. The 2nd moment of degree plays a crucial role that is reminiscent of its role in large complex networks. version:5
arxiv-1305-3384 | Transfer Learning for Content-Based Recommender Systems using Tree Matching | http://arxiv.org/abs/1305.3384 | id:1305.3384 author:Naseem Biadsy, Lior Rokach, Armin Shmilovici category:cs.LG cs.IR  published:2013-05-15 summary:In this paper we present a new approach to content-based transfer learning for solving the data sparsity problem in cases when the users' preferences in the target domain are either scarce or unavailable, but the necessary information on the preferences exists in another domain. We show that training a system to use such information across domains can produce better performance. Specifically, we represent users' behavior patterns based on topological graph structures. Each behavior pattern represents the behavior of a set of users, when the users' behavior is defined as the items they rated and the items' rating values. In the next step we find a correlation between behavior patterns in the source domain and behavior patterns in the target domain. This mapping is considered a bridge between the two domains. Based on the correlation and content-attributes of the items, we train a machine learning model to predict users' ratings in the target domain. When we compare our approach to the popularity approach and KNN-cross-domain on a real world dataset, the results show that on an average of 83$%$ of the cases our approach outperforms both methods. version:1
arxiv-1304-5587 | Color image denoising by chromatic edges based vector valued diffusion | http://arxiv.org/abs/1304.5587 | id:1304.5587 author:V. B. Surya Prasath, Juan C. Moreno, K. Palaniappan category:cs.CV 68U10 I.4.3  published:2013-04-20 summary:In this letter we propose to denoise digital color images via an improved geometric diffusion scheme. By introducing edges detected from all three color channels into the diffusion the proposed scheme avoids color smearing artifacts. Vector valued diffusion is used to control the smoothing and the geometry of color images are taken into consideration. Color edge strength function computed from different planes is introduced and it stops the diffusion spread across chromatic edges. Experimental results indicate that the scheme achieves good denoising with edge preservation when compared to other related schemes. version:2
arxiv-1305-3334 | Online Learning in a Contract Selection Problem | http://arxiv.org/abs/1305.3334 | id:1305.3334 author:Cem Tekin, Mingyan Liu category:cs.LG cs.GT math.OC stat.ML  published:2013-05-15 summary:In an online contract selection problem there is a seller which offers a set of contracts to sequentially arriving buyers whose types are drawn from an unknown distribution. If there exists a profitable contract for the buyer in the offered set, i.e., a contract with payoff higher than the payoff of not accepting any contracts, the buyer chooses the contract that maximizes its payoff. In this paper we consider the online contract selection problem to maximize the sellers profit. Assuming that a structural property called ordered preferences holds for the buyer's payoff function, we propose online learning algorithms that have sub-linear regret with respect to the best set of contracts given the distribution over the buyer's type. This problem has many applications including spectrum contracts, wireless service provider data plans and recommendation systems. version:1
arxiv-1302-4297 | Feature Multi-Selection among Subjective Features | http://arxiv.org/abs/1302.4297 | id:1302.4297 author:Sivan Sabato, Adam Kalai category:cs.LG stat.ML  published:2013-02-18 summary:When dealing with subjective, noisy, or otherwise nebulous features, the "wisdom of crowds" suggests that one may benefit from multiple judgments of the same feature on the same object. We give theoretically-motivated `feature multi-selection' algorithms that choose, among a large set of candidate features, not only which features to judge but how many times to judge each one. We demonstrate the effectiveness of this approach for linear regression on a crowdsourced learning task of predicting people's height and weight from photos, using features such as 'gender' and 'estimated weight' as well as culturally fraught ones such as 'attractive'. version:3
arxiv-1305-3207 | Efficient Density Estimation via Piecewise Polynomial Approximation | http://arxiv.org/abs/1305.3207 | id:1305.3207 author:Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun category:cs.LG cs.DS stat.ML  published:2013-05-14 summary:We give a highly efficient "semi-agnostic" algorithm for learning univariate probability distributions that are well approximated by piecewise polynomial density functions. Let $p$ be an arbitrary distribution over an interval $I$ which is $\tau$-close (in total variation distance) to an unknown probability distribution $q$ that is defined by an unknown partition of $I$ into $t$ intervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each of the intervals. We give an algorithm that draws $\tilde{O}(t\new{(d+1)}/\eps^2)$ samples from $p$, runs in time $\poly(t,d,1/\eps)$, and with high probability outputs a piecewise polynomial hypothesis distribution $h$ that is $(O(\tau)+\eps)$-close (in total variation distance) to $p$. This sample complexity is essentially optimal; we show that even for $\tau=0$, any algorithm that learns an unknown $t$-piecewise degree-$d$ probability distribution over $I$ to accuracy $\eps$ must use $\Omega({\frac {t(d+1)} {\poly(1 + \log(d+1))}} \cdot {\frac 1 {\eps^2}})$ samples from the distribution, regardless of its running time. Our algorithm combines tools from approximation theory, uniform convergence, linear programming, and dynamic programming. We apply this general algorithm to obtain a wide range of results for many natural problems in density estimation over both continuous and discrete domains. These include state-of-the-art results for learning mixtures of log-concave distributions; mixtures of $t$-modal distributions; mixtures of Monotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions; mixtures of Gaussians; and mixtures of $k$-monotone densities. Our general technique yields computationally efficient algorithms for all these problems, in many cases with provably optimal sample complexities (up to logarithmic factors) in all parameters. version:1
arxiv-1305-3189 | A Bag of Words Approach for Semantic Segmentation of Monitored Scenes | http://arxiv.org/abs/1305.3189 | id:1305.3189 author:Wassim Bouachir, Atousa Torabi, Guillaume-Alexandre Bilodeau, Pascal Blais category:cs.CV  published:2013-05-14 summary:This paper proposes a semantic segmentation method for outdoor scenes captured by a surveillance camera. Our algorithm classifies each perceptually homogenous region as one of the predefined classes learned from a collection of manually labelled images. The proposed approach combines two different types of information. First, color segmentation is performed to divide the scene into perceptually similar regions. Then, the second step is based on SIFT keypoints and uses the bag of words representation of the regions for the classification. The prediction is done using a Na\"ive Bayesian Network as a generative classifier. Compared to existing techniques, our method provides more compact representations of scene contents and the segmentation result is more consistent with human perception due to the combination of the color information with the image keypoints. The experiments conducted on a publicly available data set demonstrate the validity of the proposed method. version:1
arxiv-1210-3569 | Autonomous Reinforcement of Behavioral Sequences in Neural Dynamics | http://arxiv.org/abs/1210.3569 | id:1210.3569 author:Sohrob Kazerounian, Matthew Luciw, Mathis Richter, Yulia Sandamirskaya category:cs.NE  published:2012-10-12 summary:We introduce a dynamic neural algorithm called Dynamic Neural (DN) SARSA(\lambda) for learning a behavioral sequence from delayed reward. DN-SARSA(\lambda) combines Dynamic Field Theory models of behavioral sequence representation, classical reinforcement learning, and a computational neuroscience model of working memory, called Item and Order working memory, which serves as an eligibility trace. DN-SARSA(\lambda) is implemented on both a simulated and real robot that must learn a specific rewarding sequence of elementary behaviors from exploration. Results show DN-SARSA(\lambda) performs on the level of the discrete SARSA(\lambda), validating the feasibility of general reinforcement learning without compromising neural dynamics. version:2
arxiv-1305-3149 | Qualitative detection of oil adulteration with machine learning approaches | http://arxiv.org/abs/1305.3149 | id:1305.3149 author:Xiao-Bo Jin, Qiang Lu, Feng Wang, Quan-gong Huo category:cs.CE cs.LG  published:2013-05-14 summary:The study focused on the machine learning analysis approaches to identify the adulteration of 9 kinds of edible oil qualitatively and answered the following three questions: Is the oil sample adulterant? How does it constitute? What is the main ingredient of the adulteration oil? After extracting the high-performance liquid chromatography (HPLC) data on triglyceride from 370 oil samples, we applied the adaptive boosting with multi-class Hamming loss (AdaBoost.MH) to distinguish the oil adulteration in contrast with the support vector machine (SVM). Further, we regarded the adulterant oil and the pure oil samples as ones with multiple labels and with only one label, respectively. Then multi-label AdaBoost.MH and multi-label learning vector quantization (ML-LVQ) model were built to determine the ingredients and their relative ratio in the adulteration oil. The experimental results on six measures show that ML-LVQ achieves better performance than multi-label AdaBoost.MH. version:1
arxiv-1305-3120 | Optimization with First-Order Surrogate Functions | http://arxiv.org/abs/1305.3120 | id:1305.3120 author:Julien Mairal category:stat.ML cs.LG math.OC  published:2013-05-14 summary:In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning. version:1
arxiv-1305-3107 | I Wish I Didn't Say That! Analyzing and Predicting Deleted Messages in Twitter | http://arxiv.org/abs/1305.3107 | id:1305.3107 author:Sasa Petrovic, Miles Osborne, Victor Lavrenko category:cs.SI cs.CL  published:2013-05-14 summary:Twitter has become a major source of data for social media researchers. One important aspect of Twitter not previously considered are {\em deletions} -- removal of tweets from the stream. Deletions can be due to a multitude of reasons such as privacy concerns, rashness or attempts to undo public statements. We show how deletions can be automatically predicted ahead of time and analyse which tweets are likely to be deleted and how. version:1
arxiv-1203-1515 | Multiple Change Point Estimation in Stationary Ergodic Time Series | http://arxiv.org/abs/1203.1515 | id:1203.1515 author:Azadeh Khaleghi, Daniil Ryabko category:stat.ML cs.IT math.IT math.ST stat.TH  published:2012-03-07 summary:Given a heterogeneous time-series sample, the objective is to find points in time (called change points) where the probability distribution generating the data has changed. The data are assumed to have been generated by arbitrary unknown stationary ergodic distributions. No modelling, independence or mixing assumptions are made. A novel, computationally efficient, nonparametric method is proposed, and is shown to be asymptotically consistent in this general framework. The theoretical results are complemented with experimental evaluations. version:10
arxiv-1305-2713 | Early Detection of Alzheimer's - A Crucial Requirement | http://arxiv.org/abs/1305.2713 | id:1305.2713 author:Ijaz Bukhari category:cs.CV physics.med-ph  published:2013-05-13 summary:Alzheimer's, an old age disease of people over 65 years causes problems with memory, thinking and behavior. This disease progresses very slow and its identification in early stages is very difficult. The symptoms of Alzheimer's appear slowly and gradually will have worse effects. In its early stages, not only the patients themselves but their loved ones are generally unable to accept that the patient is suffering from disease. In this paper, we have proposed a new algorithm to detect patients of Alzheimer's at early stages by comparing the Magnetic Resonance Images (MRI) of the patients with normal persons of their age. The progress of the disease can also be monitored by periodic comparison of the previous and current MRI. version:2
arxiv-1305-3014 | Scalable Audience Reach Estimation in Real-time Online Advertising | http://arxiv.org/abs/1305.3014 | id:1305.3014 author:Ali Jalali, Santanu Kolay, Peter Foldes, Ali Dasdan category:cs.LG cs.DB  published:2013-05-14 summary:Online advertising has been introduced as one of the most efficient methods of advertising throughout the recent years. Yet, advertisers are concerned about the efficiency of their online advertising campaigns and consequently, would like to restrict their ad impressions to certain websites and/or certain groups of audience. These restrictions, known as targeting criteria, limit the reachability for better performance. This trade-off between reachability and performance illustrates a need for a forecasting system that can quickly predict/estimate (with good accuracy) this trade-off. Designing such a system is challenging due to (a) the huge amount of data to process, and, (b) the need for fast and accurate estimates. In this paper, we propose a distributed fault tolerant system that can generate such estimates fast with good accuracy. The main idea is to keep a small representative sample in memory across multiple machines and formulate the forecasting problem as queries against the sample. The key challenge is to find the best strata across the past data, perform multivariate stratified sampling while ensuring fuzzy fall-back to cover the small minorities. Our results show a significant improvement over the uniform and simple stratified sampling strategies which are currently widely used in the industry. version:1
arxiv-1305-3013 | Novel variational model for inpainting in the wavelet domain | http://arxiv.org/abs/1305.3013 | id:1305.3013 author:Dai-Qiang Chen, Li-Zhi Cheng category:cs.CV 68U10  65J22  65T60  published:2013-05-14 summary:Wavelet domain inpainting refers to the process of recovering the missing coefficients during the image compression or transmission stage. Recently, an efficient algorithm framework which is called Bregmanized operator splitting (BOS) was proposed for solving the classical variational model of wavelet inpainting. However, it is still time-consuming to some extent due to the inner iteration. In this paper, a novel variational model is established to formulate this reconstruction problem from the view of image decomposition. Then an efficient iterative algorithm based on the split-Bregman method is adopted to calculate an optimal solution, and it is also proved to be convergent. Compared with the BOS algorithm the proposed algorithm avoids the inner iteration and hence is more simple. Numerical experiments demonstrate that the proposed method is very efficient and outperforms the current state-of-the-art methods, especially in the computational time. version:1
arxiv-1305-3011 | Real Time Bid Optimization with Smooth Budget Delivery in Online Advertising | http://arxiv.org/abs/1305.3011 | id:1305.3011 author:Kuang-Chih Lee, Ali Jalali, Ali Dasdan category:cs.GT cs.LG  published:2013-05-14 summary:Today, billions of display ad impressions are purchased on a daily basis through a public auction hosted by real time bidding (RTB) exchanges. A decision has to be made for advertisers to submit a bid for each selected RTB ad request in milliseconds. Restricted by the budget, the goal is to buy a set of ad impressions to reach as many targeted users as possible. A desired action (conversion), advertiser specific, includes purchasing a product, filling out a form, signing up for emails, etc. In addition, advertisers typically prefer to spend their budget smoothly over the time in order to reach a wider range of audience accessible throughout a day and have a sustainable impact. However, since the conversions occur rarely and the occurrence feedback is normally delayed, it is very challenging to achieve both budget and performance goals at the same time. In this paper, we present an online approach to the smooth budget delivery while optimizing for the conversion performance. Our algorithm tries to select high quality impressions and adjust the bid price based on the prior performance distribution in an adaptive manner by distributing the budget optimally across time. Our experimental results from real advertising campaigns demonstrate the effectiveness of our proposed approach. version:1
arxiv-1305-3006 | Fast Linearized Alternating Direction Minimization Algorithm with Adaptive Parameter Selection for Multiplicative Noise Removal | http://arxiv.org/abs/1305.3006 | id:1305.3006 author:Dai-Qiang Chen, Li-Zhi Cheng category:cs.CV math.NA 68U10  published:2013-05-14 summary:Owing to the edge preserving ability and low computational cost of the total variation (TV), variational models with the TV regularization have been widely investigated in the field of multiplicative noise removal. The key points of the successful application of these models lie in: the optimal selection of the regularization parameter which balances the data-fidelity term with the TV regularizer; the efficient algorithm to compute the solution. In this paper, we propose two fast algorithms based on the linearized technique, which are able to estimate the regularization parameter and recover the image simultaneously. In the iteration step of the proposed algorithms, the regularization parameter is adjusted by a special discrepancy function defined for multiplicative noise. The convergence properties of the proposed algorithms are proved under certain conditions, and numerical experiments demonstrate that the proposed algorithms overall outperform some state-of-the-art methods in the PSNR values and computational time. version:1
arxiv-1305-2982 | Estimating or Propagating Gradients Through Stochastic Neurons | http://arxiv.org/abs/1305.2982 | id:1305.2982 author:Yoshua Bengio category:cs.LG  published:2013-05-14 summary:Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients. version:1
arxiv-1305-2788 | HRF estimation improves sensitivity of fMRI encoding and decoding models | http://arxiv.org/abs/1305.2788 | id:1305.2788 author:Fabian Pedregosa, Michael Eickenberg, Bertrand Thirion, Alexandre Gramfort category:cs.LG stat.AP  published:2013-05-13 summary:Extracting activation patterns from functional Magnetic Resonance Images (fMRI) datasets remains challenging in rapid-event designs due to the inherent delay of blood oxygen level-dependent (BOLD) signal. The general linear model (GLM) allows to estimate the activation from a design matrix and a fixed hemodynamic response function (HRF). However, the HRF is known to vary substantially between subjects and brain regions. In this paper, we propose a model for jointly estimating the hemodynamic response function (HRF) and the activation patterns via a low-rank representation of task effects.This model is based on the linearity assumption behind the GLM and can be computed using standard gradient-based solvers. We use the activation patterns computed by our model as input data for encoding and decoding studies and report performance improvement in both settings. version:1
arxiv-1302-4922 | Structure Discovery in Nonparametric Regression through Compositional Kernel Search | http://arxiv.org/abs/1302.4922 | id:1302.4922 author:David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani category:stat.ML cs.LG stat.ME G.3; I.2.6  published:2013-02-20 summary:Despite its importance, choosing the structural form of the kernel in nonparametric regression remains a black art. We define a space of kernel structures which are built compositionally by adding and multiplying a small number of base kernels. We present a method for searching over this space of structures which mirrors the scientific discovery process. The learned structures can often decompose functions into interpretable components and enable long-range extrapolation on time-series datasets. Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks. version:4
arxiv-1305-2732 | An efficient algorithm for learning with semi-bandit feedback | http://arxiv.org/abs/1305.2732 | id:1305.2732 author:Gergely Neu, Gábor Bartók category:cs.LG  published:2013-05-13 summary:We consider the problem of online combinatorial optimization under semi-bandit feedback. The goal of the learner is to sequentially select its actions from a combinatorial decision set so as to minimize its cumulative loss. We propose a learning algorithm for this problem based on combining the Follow-the-Perturbed-Leader (FPL) prediction method with a novel loss estimation procedure called Geometric Resampling (GR). Contrary to previous solutions, the resulting algorithm can be efficiently implemented for any decision set where efficient offline combinatorial optimization is possible at all. Assuming that the elements of the decision set can be described with d-dimensional binary vectors with at most m non-zero entries, we show that the expected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As a side result, we also improve the best known regret bounds for FPL in the full information setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m) over previous bounds for this algorithm. version:1
arxiv-1305-2687 | Automatic Parameter Adaptation for Multi-object Tracking | http://arxiv.org/abs/1305.2687 | id:1305.2687 author:Duc Phu Chau, Monique Thonnat, François Bremond category:cs.CV  published:2013-05-13 summary:Object tracking quality usually depends on video context (e.g. object occlusion level, object density). In order to decrease this dependency, this paper presents a learning approach to adapt the tracker parameters to the context variations. In an offline phase, satisfactory tracking parameters are learned for video context clusters. In the online control phase, once a context change is detected, the tracking parameters are tuned using the learned values. The experimental results show that the proposed approach outperforms the recent trackers in state of the art. This paper brings two contributions: (1) a classification method of video sequences to learn offline tracking parameters, (2) a new method to tune online tracking parameters using tracking context. version:1
arxiv-1305-2680 | A study for the effect of the Emphaticness and language and dialect for Voice Onset Time (VOT) in Modern Standard Arabic (MSA) | http://arxiv.org/abs/1305.2680 | id:1305.2680 author:Sulaiman S. AlDahri category:cs.CL cs.SD  published:2013-05-13 summary:The signal sound contains many different features, including Voice Onset Time (VOT), which is a very important feature of stop sounds in many languages. The only application of VOT values is stopping phoneme subsets. This subset of consonant sounds is stop phonemes exist in the Arabic language, and in fact, all languages. The pronunciation of these sounds is hard and unique especially for less-educated Arabs and non-native Arabic speakers. VOT can be utilized by the human auditory system to distinguish between voiced and unvoiced stops such as /p/ and /b/ in English.This search focuses on computing and analyzing VOT of Modern Standard Arabic (MSA), within the Arabic language, for all pairs of non-emphatic (namely, /d/ and /t/) and emphatic pairs (namely, /d?/ and /t?/) depending on carrier words. This research uses a database built by ourselves, and uses the carrier words syllable structure: CV-CV-CV. One of the main outcomes always found is the emphatic sounds (/d?/, /t?/) are less than 50% of non-emphatic (counter-part) sounds ( /d/, /t/).Also, VOT can be used to classify or detect for a dialect ina language. version:1
arxiv-1305-2667 | Mean field variational Bayesian inference for support vector machine classification | http://arxiv.org/abs/1305.2667 | id:1305.2667 author:Jan Luts, John T. Ormerod category:stat.ME stat.ML  published:2013-05-13 summary:A mean field variational Bayes approach to support vector machines (SVMs) using the latent variable representation on Polson & Scott (2012) is presented. This representation allows circumvention of many of the shortcomings associated with classical SVMs including automatic penalty parameter selection, the ability to handle dependent samples, missing data and variable selection. We demonstrate on simulated and real datasets that our approach is easily extendable to non-standard situations and outperforms the classical SVM approach whilst remaining computationally efficient. version:1
arxiv-1305-2648 | Boosting with the Logistic Loss is Consistent | http://arxiv.org/abs/1305.2648 | id:1305.2648 author:Matus Telgarsky category:cs.LG stat.ML  published:2013-05-13 summary:This manuscript provides optimization guarantees, generalization bounds, and statistical consistency results for AdaBoost variants which replace the exponential loss with the logistic and similar losses (specifically, twice differentiable convex losses which are Lipschitz and tend to zero on one side). The heart of the analysis is to show that, in lieu of explicit regularization and constraints, the structure of the problem is fairly rigidly controlled by the source distribution itself. The first control of this type is in the separable case, where a distribution-dependent relaxed weak learning rate induces speedy convergence with high probability over any sample. Otherwise, in the nonseparable case, the convex surrogate risk itself exhibits distribution-dependent levels of curvature, and consequently the algorithm's output has small norm with high probability. version:1
arxiv-1306-0541 | Identifying Pairs in Simulated Bio-Medical Time-Series | http://arxiv.org/abs/1306.0541 | id:1306.0541 author:Uri Kartoun category:cs.LG cs.CE  published:2013-05-12 summary:The paper presents a time-series-based classification approach to identify similarities in pairs of simulated human-generated patterns. An example for a pattern is a time-series representing a heart rate during a specific time-range, wherein the time-series is a sequence of data points that represent the changes in the heart rate values. A bio-medical simulator system was developed to acquire a collection of 7,871 price patterns of financial instruments. The financial instruments traded in real-time on three American stock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. The system simulates a human in which each price pattern represents one bio-medical sensor. Data provided during trading hours from the stock exchanges allowed real-time classification. Classification is based on new machine learning techniques: self-labeling, which allows the application of supervised learning methods on unlabeled time-series and similarity ranking, which applied on a decision tree learning algorithm to classify time-series regardless of type and quantity. version:1
arxiv-1305-2581 | Accelerated Mini-Batch Stochastic Dual Coordinate Ascent | http://arxiv.org/abs/1305.2581 | id:1305.2581 author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG  published:2013-05-12 summary:Stochastic dual coordinate ascent (SDCA) is an effective technique for solving regularized loss minimization problems in machine learning. This paper considers an extension of SDCA under the mini-batch setting that is often used in practice. Our main contribution is to introduce an accelerated mini-batch version of SDCA and prove a fast convergence rate for this method. We discuss an implementation of our method over a parallel computing system, and compare the results to both the vanilla stochastic dual coordinate ascent and to the accelerated deterministic gradient descent method of \cite{nesterov2007gradient}. version:1
arxiv-1305-2532 | Learning Policies for Contextual Submodular Prediction | http://arxiv.org/abs/1305.2532 | id:1305.2532 author:Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, J. Andrew Bagnell category:cs.LG stat.ML  published:2013-05-11 summary:Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on no-regret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization. version:1
arxiv-1305-2505 | On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions | http://arxiv.org/abs/1305.2505 | id:1305.2505 author:Purushottam Kar, Bharath K Sriperumbudur, Prateek Jain, Harish C Karnick category:cs.LG stat.ML  published:2013-05-11 summary:In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly convex pairwise loss functions. We are also able to analyze a class of memory efficient online learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees. version:1
arxiv-1305-2504 | Geiringer Theorems: From Population Genetics to Computational Intelligence, Memory Evolutive Systems and Hebbian Learning | http://arxiv.org/abs/1305.2504 | id:1305.2504 author:Boris Mitavskiy, Elio Tuci, Chris Cannings, Chris Cannings, Jonathan Rowe, Jun He category:cs.NE  published:2013-05-11 summary:The classical Geiringer theorem addresses the limiting frequency of occurrence of various alleles after repeated application of crossover. It has been adopted to the setting of evolutionary algorithms and, a lot more recently, reinforcement learning and Monte-Carlo tree search methodology to cope with a rather challenging question of action evaluation at the chance nodes. The theorem motivates novel dynamic parallel algorithms that are explicitly described in the current paper for the first time. The algorithms involve independent agents traversing a dynamically constructed directed graph that possibly has loops. A rather elegant and profound category-theoretic model of cognition in biological neural networks developed by a well-known French mathematician, professor Andree Ehresmann jointly with a neurosurgeon, Jan Paul Vanbremeersch over the last thirty years provides a hint at the connection between such algorithms and Hebbian learning. version:1
arxiv-1301-3627 | Two SVDs produce more focal deep learning representations | http://arxiv.org/abs/1301.3627 | id:1301.3627 author:Hinrich Schuetze, Christian Scheible category:cs.CL cs.LG  published:2013-01-16 summary:A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012). version:2
arxiv-1305-2473 | Affine Invariant Divergences associated with Composite Scores and its Applications | http://arxiv.org/abs/1305.2473 | id:1305.2473 author:Takafumi Kanamori, Hironori Fujisawa category:math.ST stat.ML stat.TH  published:2013-05-11 summary:In statistical analysis, measuring a score of predictive performance is an important task. In many scientific fields, appropriate scores were tailored to tackle the problems at hand. A proper score is a popular tool to obtain statistically consistent forecasts. Furthermore, a mathematical characterization of the proper score was studied. As a result, it was revealed that the proper score corresponds to a Bregman divergence, which is an extension of the squared distance over the set of probability distributions. In the present paper, we introduce composite scores as an extension of the typical scores in order to obtain a wider class of probabilistic forecasting. Then, we propose a class of composite scores, named Holder scores, that induce equivariant estimators. The equivariant estimators have a favorable property, implying that the estimator is transformed in a consistent way, when the data is transformed. In particular, we deal with the affine transformation of the data. By using the equivariant estimators under the affine transformation, one can obtain estimators that do no essentially depend on the choice of the system of units in the measurement. Conversely, we prove that the Holder score is characterized by the invariance property under the affine transformations. Furthermore, we investigate statistical properties of the estimators using Holder scores for the statistical problems including estimation of regression functions and robust parameter estimation, and illustrate the usefulness of the newly introduced scores for statistical forecasting. version:1
arxiv-1305-2452 | Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation | http://arxiv.org/abs/1305.2452 | id:1305.2452 author:James Foulds, Levi Boyles, Christopher Dubois, Padhraic Smyth, Max Welling category:cs.LG  published:2013-05-10 summary:In the internet era there has been an explosion in the amount of digital text information available, leading to difficulties of scale for traditional inference algorithms for topic models. Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model. We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method. We show connections between collapsed variational Bayesian inference and MAP estimation for LDA, and leverage these connections to prove convergence properties of the proposed algorithm. In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than the previous method. Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software. version:1
arxiv-1207-3718 | MARFCAT: Transitioning to Binary and Larger Data Sets of SATE IV | http://arxiv.org/abs/1207.3718 | id:1207.3718 author:Serguei A. Mokhov, Joey Paquet, Mourad Debbabi, Yankui Sun category:cs.CR cs.PL cs.SE stat.ML K.6.5; D.3  published:2012-07-16 summary:We present a second iteration of a machine learning approach to static code analysis and fingerprinting for weaknesses related to security, software engineering, and others using the open-source MARF framework and the MARFCAT application based on it for the NIST's SATE IV static analysis tool exposition workshop's data sets that include additional test cases, including new large synthetic cases. To aid detection of weak or vulnerable code, including source or binary on different platforms the machine learning approach proved to be fast and accurate to for such tasks where other tools are either much slower or have much smaller recall of known vulnerabilities. We use signal and NLP processing techniques in our approach to accomplish the identification and classification tasks. MARFCAT's design from the beginning in 2010 made is independent of the language being analyzed, source code, bytecode, or binary. In this follow up work with explore some preliminary results in this area. We evaluated also additional algorithms that were used to process the data. version:2
arxiv-1305-2395 | Shape Reconstruction and Recognition with Isolated Non-directional Cues | http://arxiv.org/abs/1305.2395 | id:1305.2395 author:Toshiro Kubota, Jessica Ranck, Briley Acker, Herman De Haan category:cs.CV  published:2013-05-10 summary:The paper investigates a hypothesis that our visual system groups visual cues based on how they form a surface, or more specifically triangulation derived from the visual cues. To test our hypothesis, we compare shape recognition with three different representations of visual cues: a set of isolated dots delineating the outline of the shape, a set of triangles obtained from Delaunay triangulation of the set of dots, and a subset of Delaunay triangles excluding those outside of the shape. Each participant was assigned to one particular representation type and increased the number of dots (and consequentially triangles) until the underlying shape could be identified. We compare the average number of dots needed for identification among three types of representations. Our hypothesis predicts that the results from the three representations will be similar. However, they show statistically significant differences. The paper also presents triangulation based algorithms for reconstruction and recognition of a shape from a set of isolated dots. Experiments showed that the algorithms were more effective and perceptually agreeable than similar contour based ones. From these experiments, we conclude that triangulation does affect our shape recognition. However, the surface based approach presents a number of computational advantages over the contour based one and should be studied further. version:1
arxiv-1305-2362 | Revisiting Bayesian Blind Deconvolution | http://arxiv.org/abs/1305.2362 | id:1305.2362 author:David Wipf, Haichao Zhang category:cs.CV cs.LG stat.ML  published:2013-05-10 summary:Blind deconvolution involves the estimation of a sharp signal or image given only a blurry observation. Because this problem is fundamentally ill-posed, strong priors on both the sharp image and blur kernel are required to regularize the solution space. While this naturally leads to a standard MAP estimation framework, performance is compromised by unknown trade-off parameter settings, optimization heuristics, and convergence issues stemming from non-convexity and/or poor prior selections. To mitigate some of these problems, a number of authors have recently proposed substituting a variational Bayesian (VB) strategy that marginalizes over the high-dimensional image space leading to better estimates of the blur kernel. However, the underlying cost function now involves both integrals with no closed-form solution and complex, function-valued arguments, thus losing the transparency of MAP. Beyond standard Bayesian-inspired intuitions, it thus remains unclear by exactly what mechanism these methods are able to operate, rendering understanding, improvements and extensions more difficult. To elucidate these issues, we demonstrate that the VB methodology can be recast as an unconventional MAP problem with a very particular penalty/prior that couples the image, blur kernel, and noise level in a principled way. This unique penalty has a number of useful characteristics pertaining to relative concavity, local minima avoidance, and scale-invariance that allow us to rigorously explain the success of VB including its existing implementational heuristics and approximations. It also provides strict criteria for choosing the optimal image prior that, perhaps counter-intuitively, need not reflect the statistics of natural scenes. In so doing we challenge the prevailing notion of why VB is successful for blind deconvolution while providing a transparent platform for introducing enhancements. version:1
arxiv-1108-4531 | Novel Analysis of Population Scalability in Evolutionary Algorithms | http://arxiv.org/abs/1108.4531 | id:1108.4531 author:Jun He, Tianshi Chen, Boris Mitavskiy category:cs.NE  published:2011-08-23 summary:Population-based evolutionary algorithms (EAs) have been widely applied to solve various optimization problems. The question of how the performance of a population-based EA depends on the population size arises naturally. The performance of an EA may be evaluated by different measures, such as the average convergence rate to the optimal set per generation or the expected number of generations to encounter an optimal solution for the first time. Population scalability is the performance ratio between a benchmark EA and another EA using identical genetic operators but a larger population size. Although intuitively the performance of an EA may improve if its population size increases, currently there exist only a few case studies for simple fitness functions. This paper aims at providing a general study for discrete optimisation. A novel approach is introduced to analyse population scalability using the fundamental matrix. The following two contributions summarize the major results of the current article. (1) We demonstrate rigorously that for elitist EAs with identical global mutation, using a lager population size always increases the average rate of convergence to the optimal set; and yet, sometimes, the expected number of generations needed to find an optimal solution (measured by either the maximal value or the average value) may increase, rather than decrease. (2) We establish sufficient and/or necessary conditions for the superlinear scalability, that is, when the average convergence rate of a $(\mu+\mu)$ EA (where $\mu\ge2$) is bigger than $\mu$ times that of a $(1+1)$ EA. version:4
arxiv-1305-2827 | Human Mood Detection For Human Computer Interaction | http://arxiv.org/abs/1305.2827 | id:1305.2827 author:Preeti Badar, Urmila Shrawankar category:cs.CV  published:2013-05-10 summary:In this paper we propose an easiest approach for facial expression recognition. Here we are using concept of SVM for Expression Classification. Main problem is sub divided in three main modules. First one is Face detection in which we are using skin filter and Face segmentation. We are given more stress on feature Extraction. This method is effective enough for application where fast execution is required. Second, Facial Feature Extraction which is essential part for expression recognition. In this module we used Edge Projection Analysis. Finally extracted features vector is passed towards SVM classifier for Expression Recognition. We are considering six basic Expressions (Anger, Fear, Disgust, Joy, Sadness, and Surprise) version:1
arxiv-1305-2828 | Image Optimization and Prediction | http://arxiv.org/abs/1305.2828 | id:1305.2828 author:Shweta Jain, Urmila Shrawankar category:cs.CV  published:2013-05-10 summary:Image Processing, Optimization and Prediction of an Image play a key role in Computer Science. Image processing provides a way to analyze and identify an image .Many areas like medical image processing, Satellite images, natural images and artificial images requires lots of analysis and research on optimization. In Image Optimization and Prediction we are combining the features of Query Optimization, Image Processing and Prediction . Image optimization is used in Pattern analysis, object recognition, in medical Image processing to predict the type of diseases, in satellite images for predicting weather forecast, availability of water or mineral etc. Image Processing, Optimization and analysis is a wide open area for research .Lots of research has been conducted in the area of Image analysis and many techniques are available for image analysis but, a single technique is not yet identified for image analysis and prediction .our research is focused on identifying a global technique for image analysis and Prediction. version:1
arxiv-1305-2830 | Performance Enhancement of Distributed Quasi Steady-State Genetic Algorithm | http://arxiv.org/abs/1305.2830 | id:1305.2830 author:Rahila Patel, Urmila Shrawankar, MM. Raghuwanshi, Anil N. Jaiswal category:cs.NE  published:2013-05-10 summary:This paper proposes a new scheme for performance enhancement of distributed genetic algorithm (DGA). Initial population is divided in two classes i.e. female and male. Simple distance based clustering is used for cluster formation around females. For reclustering self-adaptive K-means is used, which produces well distributed and well separated clusters. The self-adaptive K-means used for reclustering automatically locates initial position of centroids and number of clusters. Four plans of co-evolution are applied on these clusters independently. Clusters evolve separately. Merging of clusters takes place depending on their performance. For experimentation unimodal and multimodal test functions have been used. Test result show that the new scheme of distribution of population has given better performance. version:1
arxiv-1305-2269 | Beyond Physical Connections: Tree Models in Human Pose Estimation | http://arxiv.org/abs/1305.2269 | id:1305.2269 author:Fang Wang, Yi Li category:cs.CV  published:2013-05-10 summary:Simple tree models for articulated objects prevails in the last decade. However, it is also believed that these simple tree models are not capable of capturing large variations in many scenarios, such as human pose estimation. This paper attempts to address three questions: 1) are simple tree models sufficient? more specifically, 2) how to use tree models effectively in human pose estimation? and 3) how shall we use combined parts together with single parts efficiently? Assuming we have a set of single parts and combined parts, and the goal is to estimate a joint distribution of their locations. We surprisingly find that no latent variables are introduced in the Leeds Sport Dataset (LSP) during learning latent trees for deformable model, which aims at approximating the joint distributions of body part locations using minimal tree structure. This suggests one can straightforwardly use a mixed representation of single and combined parts to approximate their joint distribution in a simple tree model. As such, one only needs to build Visual Categories of the combined parts, and then perform inference on the learned latent tree. Our method outperformed the state of the art on the LSP, both in the scenarios when the training images are from the same dataset and from the PARSE dataset. Experiments on animal images from the VOC challenge further support our findings. version:1
arxiv-1305-2876 | Multi-q Pattern Classification of Polarization Curves | http://arxiv.org/abs/1305.2876 | id:1305.2876 author:Ricardo Fabbri, Ivan N. Bastos, Francisco D. Moura Neto, Francisco J. P. Lopes, Wesley N. Goncalves, Odemir M. Bruno category:cs.CE cs.CV  published:2013-05-10 summary:Several experimental measurements are expressed in the form of one-dimensional profiles, for which there is a scarcity of methodologies able to classify the pertinence of a given result to a specific group. The polarization curves that evaluate the corrosion kinetics of electrodes in corrosive media are an application where the behavior is chiefly analyzed from profiles. Polarization curves are indeed a classic method to determine the global kinetics of metallic electrodes, but the strong nonlinearity from different metals and alloys can overlap and the discrimination becomes a challenging problem. Moreover, even finding a typical curve from replicated tests requires subjective judgement. In this paper we used the so-called multi-q approach based on the Tsallis statistics in a classification engine to separate multiple polarization curve profiles of two stainless steels. We collected 48 experimental polarization curves in aqueous chloride medium of two stainless steel types, with different resistance against localized corrosion. Multi-q pattern analysis was then carried out on a wide potential range, from cathodic up to anodic regions. An excellent classification rate was obtained, at a success rate of 90%, 80%, and 83% for low (cathodic), high (anodic), and both potential ranges, respectively, using only 2% of the original profile data. These results show the potential of the proposed approach towards efficient, robust, systematic and automatic classification of highly non-linear profile curves. version:1
arxiv-1305-2238 | Multivariate Regression with Calibration | http://arxiv.org/abs/1305.2238 | id:1305.2238 author:Han Liu, Lie Wang, Tuo Zhao category:stat.ML  published:2013-05-10 summary:We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods, CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite sample performance. Computationally, we develop an efficient smoothed proximal gradient algorithm with a worst-case numerical rate of convergence $O(1/\epsilon)$, where $\epsilon$ is a pre-specified accuracy. Theoretically, we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms existing multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR even outperforms the handcrafted models created by human experts. version:1
arxiv-1305-1956 | Joint Topic Modeling and Factor Analysis of Textual Information and Graded Response Data | http://arxiv.org/abs/1305.1956 | id:1305.1956 author:Andrew S. Lan, Christoph Studer, Andrew E. Waters, Richard G. Baraniuk category:stat.ML cs.LG  published:2013-05-08 summary:Modern machine learning methods are critical to the development of large-scale personalized learning systems that cater directly to the needs of individual learners. The recently developed SPARse Factor Analysis (SPARFA) framework provides a new statistical model and algorithms for machine learning-based learning analytics, which estimate a learner's knowledge of the latent concepts underlying a domain, and content analytics, which estimate the relationships among a collection of questions and the latent concepts. SPARFA estimates these quantities given only the binary-valued graded responses to a collection of questions. In order to better interpret the estimated latent concepts, SPARFA relies on a post-processing step that utilizes user-defined tags (e.g., topics or keywords) available for each question. In this paper, we relax the need for user-defined tags by extending SPARFA to jointly process both graded learner responses and the text of each question and its associated answer(s) or other feedback. Our purely data-driven approach (i) enhances the interpretability of the estimated latent concepts without the need of explicitly generating a set of tags or performing a post-processing step, (ii) improves the prediction performance of SPARFA, and (iii) scales to large test/assessments where human annotation would prove burdensome. We demonstrate the efficacy of the proposed approach on two real educational datasets. version:2
arxiv-1305-2221 | Repairing and Inpainting Damaged Images using Diffusion Tensor | http://arxiv.org/abs/1305.2221 | id:1305.2221 author:Faouzi Benzarti, Hamid Amiri category:cs.CV  published:2013-05-09 summary:Removing or repairing the imperfections of a digital images or videos is a very active and attractive field of research belonging to the image inpainting technique. This later has a wide range of applications, such as removing scratches in old photographic image, removing text and logos or creating cartoon and artistic effects. In this paper, we propose an efficient method to repair a damaged image based on a non linear diffusion tensor. The idea is to track perfectly the local geometry of the damaged image and allowing diffusion only in the isophotes curves direction. To illustrate the effective performance of our method, we present some experimental results on test and real photographic color images version:1
arxiv-1305-2218 | Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates | http://arxiv.org/abs/1305.2218 | id:1305.2218 author:Shenghuo Zhu category:cs.LG cs.AI  published:2013-05-09 summary:With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O({\kappa}/T) for strongly convex functions, instead of O({\kappa} ln(T)/T). We also prove that an accelerated SGD algorithm also achieves a rate of O({\kappa}/T). version:1
arxiv-1301-6648 | Generalized Bregman Divergence and Gradient of Mutual Information for Vector Poisson Channels | http://arxiv.org/abs/1301.6648 | id:1301.6648 author:Liming Wang, Miguel Rodrigues, Lawrence Carin category:cs.IT math.IT stat.ML  published:2013-01-28 summary:We investigate connections between information-theoretic and estimation-theoretic quantities in vector Poisson channel models. In particular, we generalize the gradient of mutual information with respect to key system parameters from the scalar to the vector Poisson channel model. We also propose, as another contribution, a generalization of the classical Bregman divergence that offers a means to encapsulate under a unifying framework the gradient of mutual information results for scalar and vector Poisson and Gaussian channel models. The so-called generalized Bregman divergence is also shown to exhibit various properties akin to the properties of the classical version. The vector Poisson channel model is drawing considerable attention in view of its application in various domains: as an example, the availability of the gradient of mutual information can be used in conjunction with gradient descent methods to effect compressive-sensing projection designs in emerging X-ray and document classification applications. version:3
arxiv-1305-2038 | A Rank Minrelation - Majrelation Coefficient | http://arxiv.org/abs/1305.2038 | id:1305.2038 author:Patrick E. Meyer category:stat.ML cs.AI  published:2013-05-09 summary:Improving the detection of relevant variables using a new bivariate measure could importantly impact variable selection and large network inference methods. In this paper, we propose a new statistical coefficient that we call the rank minrelation coefficient. We define a minrelation of X to Y (or equivalently a majrelation of Y to X) as a measure that estimate p(Y > X) when X and Y are continuous random variables. The approach is similar to Lin's concordance coefficient that rather focuses on estimating p(X = Y). In other words, if a variable X exhibits a minrelation to Y then, as X increases, Y is likely to increases too. However, on the contrary to concordance or correlation, the minrelation is not symmetric. More explicitly, if X decreases, little can be said on Y values (except that the uncertainty on Y actually increases). In this paper, we formally define this new kind of bivariate dependencies and propose a new statistical coefficient in order to detect those dependencies. We show through several key examples that this new coefficient has many interesting properties in order to select relevant variables, in particular when compared to correlation. version:1
arxiv-1305-2959 | Automatic Speech Recognition Using Template Model for Man-Machine Interface | http://arxiv.org/abs/1305.2959 | id:1305.2959 author:Neema Mishra, Urmila Shrawankar, V M Thakare category:cs.SD cs.CL  published:2013-05-09 summary:Speech is a natural form of communication for human beings, and computers with the ability to understand speech and speak with a human voice are expected to contribute to the development of more natural man-machine interfaces. Computers with this kind of ability are gradually becoming a reality, through the evolution of speech recognition technologies. Speech is being an important mode of interaction with computers. In this paper Feature extraction is implemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern matching is done using Dynamic time warping (DTW) algorithm. version:1
arxiv-1305-2847 | An Overview of Hindi Speech Recognition | http://arxiv.org/abs/1305.2847 | id:1305.2847 author:Neema Mishra, Urmila Shrawankar, V M Thakare category:cs.CL cs.SD  published:2013-05-09 summary:In this age of information technology, information access in a convenient manner has gained importance. Since speech is a primary mode of communication among human beings, it is natural for people to expect to be able to carry out spoken dialogue with computer. Speech recognition system permits ordinary people to speak to the computer to retrieve information. It is desirable to have a human computer dialogue in local language. Hindi being the most widely spoken Language in India is the natural primary human language candidate for human machine interaction. There are five pairs of vowels in Hindi languages; one member is longer than the other one. This paper describes an overview of speech recognition system that includes how speech is produced and the properties and characteristics of Hindi Phoneme. version:1
arxiv-1305-2846 | Opportunities & Challenges In Automatic Speech Recognition | http://arxiv.org/abs/1305.2846 | id:1305.2846 author:Rashmi Makhijani, Urmila Shrawankar, V M Thakare category:cs.CL cs.SD  published:2013-05-09 summary:Automatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures. version:1
arxiv-1305-2352 | Speech Enhancement Using Pitch Detection Approach For Noisy Environment | http://arxiv.org/abs/1305.2352 | id:1305.2352 author:Rashmi Makhijani, Urmila Shrawankar, V M Thakare category:cs.SD cs.CL  published:2013-05-09 summary:Acoustical mismatch among training and testing phases degrades outstandingly speech recognition results. This problem has limited the development of real-world nonspecific applications, as testing conditions are highly variant or even unpredictable during the training process. Therefore the background noise has to be removed from the noisy speech signal to increase the signal intelligibility and to reduce the listener fatigue. Enhancement techniques applied, as pre-processing stages; to the systems remarkably improve recognition results. In this paper, a novel approach is used to enhance the perceived quality of the speech signal when the additive noise cannot be directly controlled. Instead of controlling the background noise, we propose to reinforce the speech signal so that it can be heard more clearly in noisy environments. The subjective evaluation shows that the proposed method improves perceptual quality of speech in various noisy environments. As in some cases speaking may be more convenient than typing, even for rapid typists: many mathematical symbols are missing from the keyboard but can be easily spoken and recognized. Therefore, the proposed system can be used in an application designed for mathematical symbol recognition (especially symbols not available on the keyboard) in schools. version:1
arxiv-1305-1998 | Inferring Team Strengths Using a Discrete Markov Random Field | http://arxiv.org/abs/1305.1998 | id:1305.1998 author:John Zech, Frank Wood category:stat.ML  published:2013-05-09 summary:We propose an original model for inferring team strengths using a Markov Random Field, which can be used to generate historical estimates of the offensive and defensive strengths of a team over time. This model was designed to be applied to sports such as soccer or hockey, in which contest outcomes take value in a limited discrete space. We perform inference using a combination of Expectation Maximization and Loopy Belief Propagation. The challenges of working with a non-convex optimization problem and a high-dimensional parameter space are discussed. The performance of the model is demonstrated on professional soccer data from the English Premier League. version:1
arxiv-1211-1275 | Kernelized Bayesian Matrix Factorization | http://arxiv.org/abs/1211.1275 | id:1211.1275 author:Mehmet Gönen, Suleiman A. Khan, Samuel Kaski category:stat.ML  published:2012-11-06 summary:We extend kernelized matrix factorization with a fully Bayesian treatment and with an ability to work with multiple side information sources expressed as different kernels. Kernel functions have been introduced to matrix factorization to integrate side information about the rows and columns (e.g., objects and users in recommender systems), which is necessary for making out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite graph inference, where the output matrix is binary, but extensions to more general matrices are straightforward. We extend the state of the art in two key aspects: (i) A fully conjugate probabilistic formulation of the kernelized matrix factorization problem enables an efficient variational approximation, whereas fully Bayesian treatments are not computationally feasible in the earlier approaches. (ii) Multiple side information sources are included, treated as different kernels in multiple kernel learning that additionally reveals which side information sources are informative. Our method outperforms alternatives in predicting drug-protein interactions on two data sets. We then show that our framework can also be used for solving multilabel learning problems by considering samples and labels as the two domains where matrix factorization operates on. Our algorithm obtains the lowest Hamming loss values on 10 out of 14 multilabel classification data sets compared to five state-of-the-art multilabel learning algorithms. version:3
arxiv-1305-1958 | The Dynamically Extended Mind -- A Minimal Modeling Case Study | http://arxiv.org/abs/1305.1958 | id:1305.1958 author:Tom Froese, Carlos Gershenson, David A. Rosenblueth category:cs.AI cs.NE nlin.CD I.2.0  published:2013-05-08 summary:The extended mind hypothesis has stimulated much interest in cognitive science. However, its core claim, i.e. that the process of cognition can extend beyond the brain via the body and into the environment, has been heavily criticized. A prominent critique of this claim holds that when some part of the world is coupled to a cognitive system this does not necessarily entail that the part is also constitutive of that cognitive system. This critique is known as the "coupling-constitution fallacy". In this paper we respond to this reductionist challenge by using an evolutionary robotics approach to create a minimal model of two acoustically coupled agents. We demonstrate how the interaction process as a whole has properties that cannot be reduced to the contributions of the isolated agents. We also show that the neural dynamics of the coupled agents has formal properties that are inherently impossible for those neural networks in isolation. By keeping the complexity of the model to an absolute minimum, we are able to illustrate how the coupling-constitution fallacy is in fact based on an inadequate understanding of the constitutive role of nonlinear interactions in dynamical systems theory. version:1
arxiv-1302-4853 | Consistency of Online Random Forests | http://arxiv.org/abs/1302.4853 | id:1302.4853 author:Misha Denil, David Matheson, Nando de Freitas category:stat.ML  published:2013-02-20 summary:As a testament to their success, the theory of random forests has long been outpaced by their application in practice. In this paper, we take a step towards narrowing this gap by providing a consistency result for online random forests. version:2
arxiv-1207-6910 | Gaussian process regression as a predictive model for Quality-of-Service in Web service systems | http://arxiv.org/abs/1207.6910 | id:1207.6910 author:Jakub M. Tomczak, Jerzy Swiatek, Krzysztof Latawiec category:cs.NI cs.LG 60G15  68M11  97R40 G.3; H.3.5; I.5.4  published:2012-07-30 summary:In this paper, we present the Gaussian process regression as the predictive model for Quality-of-Service (QoS) attributes in Web service systems. The goal is to predict performance of the execution system expressed as QoS attributes given existing execution system, service repository, and inputs, e.g., streams of requests. In order to evaluate the performance of Gaussian process regression the simulation environment was developed. Two quality indexes were used, namely, Mean Absolute Error and Mean Squared Error. The results obtained within the experiment show that the Gaussian process performed the best with linear kernel and statistically significantly better comparing to Classification and Regression Trees (CART) method. version:2
arxiv-1305-1443 | Standard Fingerprint Databases: Manual Minutiae Labeling and Matcher Performance Analyses | http://arxiv.org/abs/1305.1443 | id:1305.1443 author:Mehmet Kayaoglu, Berkay Topcu, Umut Uludag category:cs.CV  published:2013-05-07 summary:Fingerprint verification and identification algorithms based on minutiae features are used in many biometric systems today (e.g., governmental e-ID programs, border control, AFIS, personal authentication for portable devices). Researchers in industry/academia are now able to utilize many publicly available fingerprint databases (e.g., Fingerprint Verification Competition (FVC) & NIST databases) to compare/evaluate their feature extraction and/or matching algorithm performances against those of others. The results from these evaluations are typically utilized by decision makers responsible for implementing the cited biometric systems, in selecting/tuning specific sensors, feature extractors and matchers. In this study, for a subset of the cited public fingerprint databases, we report fingerprint minutiae matching results, which are based on (i) minutiae extracted automatically from fingerprint images, and (ii) minutiae extracted manually by human subjects. By doing so, we are able to (i) quantitatively judge the performance differences between these two cases, (ii) elaborate on performance upper bounds of minutiae matching, utilizing what can be termed as "ground truth" minutiae features, (iii) analyze minutiae matching performance, without coupling it with the minutiae extraction performance beforehand. Further, as we will freely distribute the minutiae templates, originating from this manual labeling study, in a standard minutiae template exchange format (ISO 19794-2), we believe that other researchers in the biometrics community will be able to utilize the associated results & templates to create their own evaluations pertaining to their fingerprint minutiae extractors/matchers. version:2
arxiv-1305-1925 | Speech: A Challenge to Digital Signal Processing Technology for Human-to-Computer Interaction | http://arxiv.org/abs/1305.1925 | id:1305.1925 author:Urmila Shrawankar, Anjali Mahajan category:cs.HC cs.CL  published:2013-05-08 summary:This software project based paper is for a vision of the near future in which computer interaction is characterized by natural face-to-face conversations with lifelike characters that speak, emote, and gesture. The first step is speech. The dream of a true virtual reality, a complete human-computer interaction system will not come true unless we try to give some perception to machine and make it perceive the outside world as humans communicate with each other. This software project is under development for listening and replying machine (Computer) through speech. The Speech interface is developed to convert speech input into some parametric form (Speech-to-Text) for further processing and the results, text output to speech synthesis (Text-to-Speech) version:1
arxiv-1305-1707 | Class Imbalance Problem in Data Mining Review | http://arxiv.org/abs/1305.1707 | id:1305.1707 author:Rushi Longadge, Snehalata Dongre category:cs.LG  published:2013-05-08 summary:In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, data-preprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem. version:1
arxiv-1305-1704 | The Extended Parameter Filter | http://arxiv.org/abs/1305.1704 | id:1305.1704 author:Yusuf Erol, Lei Li, Bharath Ramsundar, Stuart J. Russell category:stat.ML cs.AI  published:2013-05-08 summary:The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvik's filter and a Kalman filter in parameter space and establish more general conditions under which Storvik's filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik's method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods. version:1
arxiv-1208-4662 | Automatic Segmentation of Fluorescence Lifetime Microscopy Images of Cells Using Multi-Resolution Community Detection | http://arxiv.org/abs/1208.4662 | id:1208.4662 author:Dandan Hu, Pinaki Sarder, Peter Ronhovde, Sandra Orthaus, Samuel Achilefu, Zohar Nussinov category:physics.med-ph cond-mat.stat-mech cs.CV physics.data-an  published:2012-08-23 summary:We have developed an automatic method for segmenting fluorescence lifetime (FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolution community detection (MCD) based network segmentation method. The image processing problem is framed as identifying segments with respective average FLTs against a background in FLIM images. The proposed method segments a FLIM image for a given resolution of the network composed using image pixels as the nodes and similarity between the pixels as the edges. In the resulting segmentation, low network resolution leads to larger segments and high network resolution leads to smaller segments. Further, the mean-square error (MSE) in estimating the FLT segments in a FLIM image using the proposed method was found to be consistently decreasing with increasing resolution of the corresponding network. The proposed MCD method outperformed a popular spectral clustering based method in performing FLIM image segmentation. The spectral segmentation method introduced noisy segments in its output at high resolution. It was unable to offer a consistent decrease in MSE with increasing resolution. version:2
arxiv-1209-5467 | Minimizing inter-subject variability in fNIRS based Brain Computer Interfaces via multiple-kernel support vector learning | http://arxiv.org/abs/1209.5467 | id:1209.5467 author:Berdakh Abibullaev, Jinung An, Seung-Hyun Lee, Sang-Hyeon Jin, Jeon-Il Moon category:stat.ML cs.LG  published:2012-09-25 summary:Brain signal variability in the measurements obtained from different subjects during different sessions significantly deteriorates the accuracy of most brain-computer interface (BCI) systems. Moreover these variabilities, also known as inter-subject or inter-session variabilities, require lengthy calibration sessions before the BCI system can be used. Furthermore, the calibration session has to be repeated for each subject independently and before use of the BCI due to the inter-session variability. In this study, we present an algorithm in order to minimize the above-mentioned variabilities and to overcome the time-consuming and usually error-prone calibration time. Our algorithm is based on linear programming support-vector machines and their extensions to a multiple kernel learning framework. We tackle the inter-subject or -session variability in the feature spaces of the classifiers. This is done by incorporating each subject- or session-specific feature spaces into much richer feature spaces with a set of optimal decision boundaries. Each decision boundary represents the subject- or a session specific spatio-temporal variabilities of neural signals. Consequently, a single classifier with multiple feature spaces will generalize well to new unseen test patterns even without the calibration steps. We demonstrate that classifiers maintain good performances even under the presence of a large degree of BCI variability. The present study analyzes BCI variability related to oxy-hemoglobin neural signals measured using a functional near-infrared spectroscopy. version:4
arxiv-1305-1679 | High Level Pattern Classification via Tourist Walks in Networks | http://arxiv.org/abs/1305.1679 | id:1305.1679 author:Thiago Christiano Silva, Liang Zhao category:cs.AI cs.LG  published:2013-05-07 summary:Complex networks refer to large-scale graphs with nontrivial connection patterns. The salient and interesting features that the complex network study offer in comparison to graph theory are the emphasis on the dynamical properties of the networks and the ability of inherently uncovering pattern formation of the vertices. In this paper, we present a hybrid data classification technique combining a low level and a high level classifier. The low level term can be equipped with any traditional classification techniques, which realize the classification task considering only physical features (e.g., geometrical or statistical features) of the input data. On the other hand, the high level term has the ability of detecting data patterns with semantic meanings. In this way, the classification is realized by means of the extraction of the underlying network's features constructed from the input data. As a result, the high level classification process measures the compliance of the test instances with the pattern formation of the training data. Out of various high level perspectives that can be utilized to capture semantic meaning, we utilize the dynamical features that are generated from a tourist walker in a networked environment. Specifically, a weighted combination of transient and cycle lengths generated by the tourist walk is employed for that end. Interestingly, our study shows that the proposed technique is able to further improve the already optimized performance of traditional classification techniques. version:1
arxiv-1206-2627 | Image Similarity Using Sparse Representation and Compression Distance | http://arxiv.org/abs/1206.2627 | id:1206.2627 author:Tanaya Guha, Rabab K. Ward category:cs.CV  published:2012-06-12 summary:A new line of research uses compression methods to measure the similarity between signals. Two signals are considered similar if one can be compressed significantly when the information of the other is known. The existing compression-based similarity methods, although successful in the discrete one dimensional domain, do not work well in the context of images. This paper proposes a sparse representation-based approach to encode the information content of an image using information from the other image, and uses the compactness (sparsity) of the representation as a measure of its compressibility (how much can the image be compressed) with respect to the other image. The more sparse the representation of an image, the better it can be compressed and the more it is similar to the other image. The efficacy of the proposed measure is demonstrated through the high accuracies achieved in image clustering, retrieval and classification. version:2
arxiv-1305-1520 | A Method for Visuo-Spatial Classification of Freehand Shapes Freely Sketched | http://arxiv.org/abs/1305.1520 | id:1305.1520 author:Ney Renau-Ferrer, Céline Remi category:cs.CV  published:2013-05-07 summary:We present the principle and the main steps of a new method for the visuo-spatial analysis of geometrical sketches recorded online. Visuo-spatial analysis is a necessary step for multi-level analysis. Multi-level analysis simultaneously allows classification, comparison or clustering of the constituent parts of a pattern according to their visuo-spatial properties, their procedural strategies, their structural or temporal parameters, or any combination of two or more of those parameters. The first results provided by this method concern the comparison of sketches to some perfect patterns of simple geometrical figures and the measure of dissimilarity between real sketches. The mean rates of good decision higher than 95% obtained are promising in both cases. version:1
arxiv-1305-1459 | EURETILE 2010-2012 summary: first three years of activity of the European Reference Tiled Experiment | http://arxiv.org/abs/1305.1459 | id:1305.1459 author:Pier Stanislao Paolucci, Iuliana Bacivarov, Gert Goossens, Rainer Leupers, Frédéric Rousseau, Christoph Schumacher, Lothar Thiele, Piero Vicini category:cs.DC cs.AR cs.NE cs.OS cs.PL  published:2013-05-07 summary:This is the summary of first three years of activity of the EURETILE FP7 project 247846. EURETILE investigates and implements brain-inspired and fault-tolerant foundational innovations to the system architecture of massively parallel tiled computer architectures and the corresponding programming paradigm. The execution targets are a many-tile HW platform, and a many-tile simulator. A set of SW process - HW tile mapping candidates is generated by the holistic SW tool-chain using a combination of analytic and bio-inspired methods. The Hardware dependent Software is then generated, providing OS services with maximum efficiency/minimal overhead. The many-tile simulator collects profiling data, closing the loop of the SW tool chain. Fine-grain parallelism inside processes is exploited by optimized intra-tile compilation techniques, but the project focus is above the level of the elementary tile. The elementary HW tile is a multi-processor, which includes a fault tolerant Distributed Network Processor (for inter-tile communication) and ASIP accelerators. Furthermore, EURETILE investigates and implements the innovations for equipping the elementary HW tile with high-bandwidth, low-latency brain-like inter-tile communication emulating 3 levels of connection hierarchy, namely neural columns, cortical areas and cortex, and develops a dedicated cortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking Neural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages on the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES Integrated Project (2006-2009). version:1
arxiv-1305-1426 | Speech Enhancement Modeling Towards Robust Speech Recognition System | http://arxiv.org/abs/1305.1426 | id:1305.1426 author:Urmila Shrawankar, V. M. Thakare category:cs.SD cs.CL  published:2013-05-07 summary:Form about four decades human beings have been dreaming of an intelligent machine which can master the natural speech. In its simplest form, this machine should consist of two subsystems, namely automatic speech recognition (ASR) and speech understanding (SU). The goal of ASR is to transcribe natural speech while SU is to understand the meaning of the transcription. Recognizing and understanding a spoken sentence is obviously a knowledge-intensive process, which must take into account all variable information about the speech communication process, from acoustics to semantics and pragmatics. While developing an Automatic Speech Recognition System, it is observed that some adverse conditions degrade the performance of the Speech Recognition System. In this contribution, speech enhancement system is introduced for enhancing speech signals corrupted by additive noise and improving the performance of Automatic Speech Recognizers in noisy conditions. Automatic speech recognition experiments show that replacing noisy speech signals by the corresponding enhanced speech signals leads to an improvement in the recognition accuracies. The amount of improvement varies with the type of the corrupting noise. version:1
arxiv-1305-1359 | A Differential Equations Approach to Optimizing Regret Trade-offs | http://arxiv.org/abs/1305.1359 | id:1305.1359 author:Alexandr Andoni, Rina Panigrahy category:cs.LG  published:2013-05-07 summary:We consider the classical question of predicting binary sequences and study the {\em optimal} algorithms for obtaining the best possible regret and payoff functions for this problem. The question turns out to be also equivalent to the problem of optimal trade-offs between the regrets of two experts in an "experts problem", studied before by \cite{kearns-regret}. While, say, a regret of $\Theta(\sqrt{T})$ is known, we argue that it important to ask what is the provably optimal algorithm for this problem --- both because it leads to natural algorithms, as well as because regret is in fact often comparable in magnitude to the final payoffs and hence is a non-negligible term. In the basic setting, the result essentially follows from a classical result of Cover from '65. Here instead, we focus on another standard setting, of time-discounted payoffs, where the final "stopping time" is not specified. We exhibit an explicit characterization of the optimal regret for this setting. To obtain our main result, we show that the optimal payoff functions have to satisfy the Hermite differential equation, and hence are given by the solutions to this equation. It turns out that characterization of the payoff function is qualitatively different from the classical (non-discounted) setting, and, namely, there's essentially a unique optimal solution. version:1
arxiv-1305-1344 | Speckle Noise Reduction in Medical Ultrasound Images | http://arxiv.org/abs/1305.1344 | id:1305.1344 author:Faouzi Benzarti, Hamid Amiri category:cs.CV  published:2013-05-06 summary:Ultrasound imaging is an incontestable vital tool for diagnosis, it provides in non-invasive manner the internal structure of the body to detect eventually diseases or abnormalities tissues. Unfortunately, the presence of speckle noise in these images affects edges and fine details which limit the contrast resolution and make diagnostic more difficult. In this paper, we propose a denoising approach which combines logarithmic transformation and a non linear diffusion tensor. Since speckle noise is multiplicative and nonwhite process, the logarithmic transformation is a reasonable choice to convert signaldependent or pure multiplicative noise to an additive one. The key idea from using diffusion tensor is to adapt the flow diffusion towards the local orientation by applying anisotropic diffusion along the coherent structure direction of interesting features in the image. To illustrate the effective performance of our algorithm, we present some experimental results on synthetically and real echographic images. version:1
arxiv-1305-1343 | Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings | http://arxiv.org/abs/1305.1343 | id:1305.1343 author:Arnim Bleier, Andreas Strotmann category:cs.DL cs.CL cs.IR  published:2013-05-06 summary:Author co-citation studies employ factor analysis to reduce high-dimensional co-citation matrices to low-dimensional and possibly interpretable factors, but these studies do not use any information from the text bodies of publications. We hypothesise that term frequencies may yield useful information for scientometric analysis. In our work we ask if word features in combination with Bayesian analysis allow well-founded science mapping studies. This work goes back to the roots of Mosteller and Wallace's (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals. To answer our research question we (i) introduce a new data set on which the experiments are carried out, (ii) describe the Bayesian model employed for inference and (iii) present first results of the analysis. version:1
arxiv-1305-1319 | New Alignment Methods for Discriminative Book Summarization | http://arxiv.org/abs/1305.1319 | id:1305.1319 author:David Bamman, Noah A. Smith category:cs.CL  published:2013-05-06 summary:We consider the unsupervised alignment of the full text of a book with a human-written summary. This presents challenges not seen in other text alignment problems, including a disparity in length and, consequent to this, a violation of the expectation that individual words and phrases should align, since large passages and chapters can be distilled into a single summary phrase. We present two new methods, based on hidden Markov models, specifically targeted to this problem, and demonstrate gains on an extractive book summarization task. While there is still much room for improvement, unsupervised alignment holds intrinsic value in offering insight into what features of a book are deemed worthy of summarization. version:1
arxiv-1305-1256 | A Convex Functional for Image Denoising based on Patches with Constrained Overlaps and its vectorial application to Low Dose Differential Phase Tomography | http://arxiv.org/abs/1305.1256 | id:1305.1256 author:Alessandro Mirone, Emmanuel Brun, Paola Coan category:math.NA cs.CV  published:2013-05-06 summary:We solve the image denoising problem with a dictionary learning technique by writing a convex functional of a new form. This functional contains beside the usual sparsity inducing term and fidelity term, a new term which induces similarity between overlapping patches in the overlap regions. The functional depends on two free regularization parameters: a coefficient multiplying the sparsity-inducing $L_{1}$ norm of the patch basis functions coefficients, and a coefficient multiplying the $L_{2}$ norm of the differences between patches in the overlapping regions. The solution is found by applying the iterative proximal gradient descent method with FISTA acceleration. In the case of tomography reconstruction we calculate the gradient by applying projection of the solution and its error backprojection at each iterative step. We study the quality of the solution, as a function of the regularization parameters and noise, on synthetic datas for which the solution is a-priori known. We apply the method on experimental data in the case of Differential Phase Tomography. For this case we use an original approach which consists in using vectorial patches, each patch having two components: one per each gradient component. The resulting algorithm, implemented in the ESRF tomography reconstruction code PyHST, results to be robust, efficient, and well adapted to strongly reduce the required dose and the number of projections in medical tomography. version:1
arxiv-1305-1206 | A Contrario Selection of Optimal Partitions for Image Segmentation | http://arxiv.org/abs/1305.1206 | id:1305.1206 author:Juan Cardelino, Vicent Caselles, Marcelo Bertalmio, Gregory Randall category:cs.CV  published:2013-05-06 summary:We present a novel segmentation algorithm based on a hierarchical representation of images. The main contribution of this work is to explore the capabilities of the A Contrario reasoning when applied to the segmentation problem, and to overcome the limitations of current algorithms within that framework. This exploratory approach has three main goals. Our first goal is to extend the search space of greedy merging algorithms to the set of all partitions spanned by a certain hierarchy, and to cast the segmentation as a selection problem within this space. In this way we increase the number of tested partitions and thus we potentially improve the segmentation results. In addition, this space is considerably smaller than the space of all possible partitions, thus we still keep the complexity controlled. Our second goal aims to improve the locality of region merging algorithms, which usually merge pairs of neighboring regions. In this work, we overcome this limitation by introducing a validation procedure for complete partitions, rather than for pairs of regions. The third goal is to perform an exhaustive experimental evaluation methodology in order to provide reproducible results. Finally, we embed the selection process on a statistical A Contrario framework which allows us to have only one free parameter related to the desired scale. version:1
arxiv-1305-1172 | Gromov-Hausdorff Approximation of Metric Spaces with Linear Structure | http://arxiv.org/abs/1305.1172 | id:1305.1172 author:Frédéric Chazal, Jian Sun category:cs.CG cs.LG math.MG  published:2013-05-06 summary:In many real-world applications data come as discrete metric spaces sampled around 1-dimensional filamentary structures that can be seen as metric graphs. In this paper we address the metric reconstruction problem of such filamentary structures from data sampled around them. We prove that they can be approximated, with respect to the Gromov-Hausdorff distance by well-chosen Reeb graphs (and some of their variants) and we provide an efficient and easy to implement algorithm to compute such approximations in almost linear time. We illustrate the performances of our algorithm on a few synthetic and real data sets. version:1
arxiv-1305-1163 | A Computer Vision System for Attention Mapping in SLAM based 3D Models | http://arxiv.org/abs/1305.1163 | id:1305.1163 author:Lucas Paletta, Katrin Santner, Gerald Fritz, Albert Hofmann, Gerald Lodron, Georg Thallinger, Heinz Mayer category:cs.CV  published:2013-05-06 summary:The study of human factors in the frame of interaction studies has been relevant for usability engi-neering and ergonomics for decades. Today, with the advent of wearable eye-tracking and Google glasses, monitoring of human factors will soon become ubiquitous. This work describes a computer vision system that enables pervasive mapping and monitoring of human attention. The key contribu-tion is that our methodology enables full 3D recovery of the gaze pointer, human view frustum and associated human centred measurements directly into an automatically computed 3D model in real-time. We apply RGB-D SLAM and descriptor matching methodologies for the 3D modelling, locali-zation and fully automated annotation of ROIs (regions of interest) within the acquired 3D model. This innovative methodology will open new avenues for attention studies in real world environments, bringing new potential into automated processing for human factors technologies. version:1
arxiv-1305-1145 | Techniques for Feature Extraction In Speech Recognition System : A Comparative Study | http://arxiv.org/abs/1305.1145 | id:1305.1145 author:Urmila Shrawankar, V M Thakare category:cs.SD cs.CL  published:2013-05-06 summary:The time domain waveform of a speech signal carries all of the auditory information. From the phonological point of view, it little can be said on the basis of the waveform itself. However, past research in mathematics, acoustics, and speech technology have provided many methods for converting data that can be considered as information if interpreted correctly. In order to find some statistically relevant information from incoming data, it is important to have mechanisms for reducing the information of each segment in the audio signal into a relatively small number of parameters, or features. These features should describe each segment in such a characteristic way that other similar segments can be grouped together by comparing their features. There are enormous interesting and exceptional ways to describe the speech signal in terms of parameters. Though, they all have their strengths and weaknesses, we have presented some of the most used methods with their importance. version:1
arxiv-1210-1207 | Learning Human Activities and Object Affordances from RGB-D Videos | http://arxiv.org/abs/1210.1207 | id:1210.1207 author:Hema Swetha Koppula, Rudhir Gupta, Ashutosh Saxena category:cs.RO cs.AI cs.CV  published:2012-10-04 summary:Understanding human activities and object affordances are two very important skills, especially for personal robots which operate in human environments. In this work, we consider the problem of extracting a descriptive labeling of the sequence of sub-activities being performed by a human, and more importantly, of their interactions with the objects in the form of associated affordances. Given a RGB-D video, we jointly model the human activities and object affordances as a Markov random field where the nodes represent objects and sub-activities, and the edges represent the relationships between object affordances, their relations with sub-activities, and their evolution over time. We formulate the learning problem using a structural support vector machine (SSVM) approach, where labelings over various alternate temporal segmentations are considered as latent variables. We tested our method on a challenging dataset comprising 120 activity videos collected from 4 subjects, and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and 75.0% for high-level activity labeling. We then demonstrate the use of such descriptive labeling in performing assistive tasks by a PR2 robot. version:2
arxiv-1205-4220 | Diffusion Adaptation over Networks | http://arxiv.org/abs/1205.4220 | id:1205.4220 author:Ali H. Sayed category:cs.MA cs.LG  published:2012-05-18 summary:Adaptive networks are well-suited to perform decentralized information processing and optimization tasks and to model various types of self-organized and complex behavior encountered in nature. Adaptive networks consist of a collection of agents with processing and learning abilities. The agents are linked together through a connection topology, and they cooperate with each other through local interactions to solve distributed optimization, estimation, and inference problems in real-time. The continuous diffusion of information across the network enables agents to adapt their performance in relation to streaming data and network conditions; it also results in improved adaptation and learning performance relative to non-cooperative agents. This article provides an overview of diffusion strategies for adaptation and learning over networks. The article is divided into several sections: 1. Motivation; 2. Mean-Square-Error Estimation; 3. Distributed Optimization via Diffusion Strategies; 4. Adaptive Diffusion Strategies; 5. Performance of Steepest-Descent Diffusion Strategies; 6. Performance of Adaptive Diffusion Strategies; 7. Comparing the Performance of Cooperative Strategies; 8. Selecting the Combination Weights; 9. Diffusion with Noisy Information Exchanges; 10. Extensions and Further Considerations; Appendix A: Properties of Kronecker Products; Appendix B: Graph Laplacian and Network Connectivity; Appendix C: Stochastic Matrices; Appendix D: Block Maximum Norm; Appendix E: Comparison with Consensus Strategies; References. version:2
arxiv-1305-1052 | Hybridization of Otsu Method and Median Filter for Color Image Segmentation | http://arxiv.org/abs/1305.1052 | id:1305.1052 author:Firas Ajil Jassim, Fawzi H. Altaani category:cs.CV  published:2013-05-05 summary:In this article a novel algorithm for color image segmentation has been developed. The proposed algorithm based on combining two existing methods in such a novel way to obtain a significant method to partition the color image into significant regions. On the first phase, the traditional Otsu method for gray channel image segmentation were applied for each of the R,G, and B channels separately to determine the suitable automatic threshold for each channel. After that, the new modified channels are integrated again to formulate a new color image. The resulted image suffers from some kind of distortion. To get rid of this distortion, the second phase is arise which is the median filter to smooth the image and increase the segmented regions. This process looks very significant by the ocular eye. Experimental results were presented on a variety of test images to support the proposed algorithm. version:1
arxiv-1305-1040 | On the Convergence and Consistency of the Blurring Mean-Shift Process | http://arxiv.org/abs/1305.1040 | id:1305.1040 author:Ting-Li Chen category:stat.ML cs.LG  published:2013-05-05 summary:The mean-shift algorithm is a popular algorithm in computer vision and image processing. It can also be cast as a minimum gamma-divergence estimation. In this paper we focus on the "blurring" mean shift algorithm, which is one version of the mean-shift process that successively blurs the dataset. The analysis of the blurring mean-shift is relatively more complicated compared to the nonblurring version, yet the algorithm convergence and the estimation consistency have not been well studied in the literature. In this paper we prove both the convergence and the consistency of the blurring mean-shift. We also perform simulation studies to compare the efficiency of the blurring and the nonblurring versions of the mean-shift algorithms. Our results show that the blurring mean-shift has more efficiency. version:1
arxiv-1212-2278 | Inverting and Visualizing Features for Object Detection | http://arxiv.org/abs/1212.2278 | id:1212.2278 author:Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba category:cs.CV  published:2012-12-11 summary:We introduce algorithms to visualize feature spaces used by object detectors. The tools in this paper allow a human to put on `HOG goggles' and perceive the visual world as a HOG based object detector sees it. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and indicates that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of our detection systems. version:2
arxiv-1305-1002 | Efficient Estimation of the number of neighbours in Probabilistic K Nearest Neighbour Classification | http://arxiv.org/abs/1305.1002 | id:1305.1002 author:Ji Won Yoon, Nial Friel category:cs.LG stat.ML  published:2013-05-05 summary:Probabilistic k-nearest neighbour (PKNN) classification has been introduced to improve the performance of original k-nearest neighbour (KNN) classification algorithm by explicitly modelling uncertainty in the classification of each feature vector. However, an issue common to both KNN and PKNN is to select the optimal number of neighbours, $k$. The contribution of this paper is to incorporate the uncertainty in $k$ into the decision making, and in so doing use Bayesian model averaging to provide improved classification. Indeed the problem of assessing the uncertainty in $k$ can be viewed as one of statistical model selection which is one of the most important technical issues in the statistics and machine learning domain. In this paper, a new functional approximation algorithm is proposed to reconstruct the density of the model (order) without relying on time consuming Monte Carlo simulations. In addition, this algorithm avoids cross validation by adopting Bayesian framework. The performance of this algorithm yielded very good performance on several real experimental datasets. version:1
arxiv-1205-4591 | Forecastable Component Analysis (ForeCA) | http://arxiv.org/abs/1205.4591 | id:1205.4591 author:Georg M. Goerg category:stat.ME stat.ML  published:2012-05-21 summary:I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classification. The R package ForeCA (http://cran.r-project.org/web/packages/ForeCA/index.html) accompanies this work and is publicly available on CRAN. version:3
arxiv-1103-1013 | A Feature Selection Method for Multivariate Performance Measures | http://arxiv.org/abs/1103.1013 | id:1103.1013 author:Qi Mao, Ivor W. Tsang category:cs.LG  published:2011-03-05 summary:Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a two-layer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-of-the-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms $l_1$-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM$^{perf}$ in terms of $F_1$-score. version:2
arxiv-1305-0922 | On Comparison between Evolutionary Programming Network-based Learning and Novel Evolution Strategy Algorithm-based Learning | http://arxiv.org/abs/1305.0922 | id:1305.0922 author:M. A. Khayer Azad, Md. Shafiqul Islam, M. M. A. Hashem category:cs.NE cs.LG  published:2013-05-04 summary:This paper presents two different evolutionary systems - Evolutionary Programming Network (EPNet) and Novel Evolutions Strategy (NES) Algorithm. EPNet does both training and architecture evolution simultaneously, whereas NES does a fixed network and only trains the network. Five mutation operators proposed in EPNet to reflect the emphasis on evolving ANNs behaviors. Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting. On the other hand, NES uses two new genetic operators - subpopulation-based max-mean arithmetical crossover and time-variant mutation. The above-mentioned two algorithms have been tested on a number of benchmark problems, such as the medical diagnosis problems (breast cancer, diabetes, and heart disease). The results and the comparison between them are also presented in this paper. version:1
arxiv-1103-0890 | Efficient Multi-Template Learning for Structured Prediction | http://arxiv.org/abs/1103.0890 | id:1103.0890 author:Qi Mao, Ivor W. Tsang category:cs.LG cs.CL  published:2011-03-04 summary:Conditional random field (CRF) and Structural Support Vector Machine (Structural SVM) are two state-of-the-art methods for structured prediction which captures the interdependencies among output variables. The success of these methods is attributed to the fact that their discriminative models are able to account for overlapping features on the whole input observations. These features are usually generated by applying a given set of templates on labeled data, but improper templates may lead to degraded performance. To alleviate this issue, in this paper, we propose a novel multiple template learning paradigm to learn structured prediction and the importance of each template simultaneously, so that hundreds of arbitrary templates could be added into the learning model without caution. This paradigm can be formulated as a special multiple kernel learning problem with exponential number of constraints. Then we introduce an efficient cutting plane algorithm to solve this problem in the primal, and its convergence is presented. We also evaluate the proposed learning paradigm on two widely-studied structured prediction tasks, \emph{i.e.} sequence labeling and dependency parsing. Extensive experimental results show that the proposed method outperforms CRFs and Structural SVMs due to exploiting the importance of each template. Our complexity analysis and empirical results also show that our proposed method is more efficient than OnlineMKL on very sparse and high-dimensional data. We further extend this paradigm for structured prediction using generalized $p$-block norm regularization with $p>1$, and experiments show competitive performances when $p \in [1,2)$. version:2
arxiv-1211-0889 | APPLE: Approximate Path for Penalized Likelihood Estimators | http://arxiv.org/abs/1211.0889 | id:1211.0889 author:Yi Yu, Yang Feng category:stat.ML cs.LG  published:2012-11-02 summary:In high-dimensional data analysis, penalized likelihood estimators are shown to provide superior results in both variable selection and parameter estimation. A new algorithm, APPLE, is proposed for calculating the Approximate Path for Penalized Likelihood Estimators. Both the convex penalty (such as LASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered. The APPLE efficiently computes the solution path for the penalized likelihood estimator using a hybrid of the modified predictor-corrector method and the coordinate-descent algorithm. APPLE is compared with several well-known packages via simulation and analysis of two gene expression data sets. version:3
arxiv-1305-0871 | Dictionary learning based image enhancement for rarity detection | http://arxiv.org/abs/1305.0871 | id:1305.0871 author:Weifeng Liu, Xiaomeng Wang, Yanjiang Wang category:cs.CV  published:2013-05-04 summary:Image enhancement is an important image processing technique that processes images suitably for a specific application e.g. image editing. The conventional solutions of image enhancement are grouped into two categories which are spatial domain processing method and transform domain processing method such as contrast manipulation, histogram equalization, homomorphic filtering. This letter proposes a new image enhance method based on dictionary learning. Particularly, the proposed method adjusts the image by manipulating the rarity of dictionary atoms. Firstly, learn the dictionary through sparse coding algorithms on divided sub-image blocks. Secondly, compute the rarity of dictionary atoms on statistics of the corresponding sparse coefficients. Thirdly, adjust the rarity according to specific application and form a new dictionary. Finally, reconstruct the image using the updated dictionary and sparse coefficients. Compared with the traditional techniques, the proposed method enhances image based on the image content not on distribution of pixel grey value or frequency. The advantages of the proposed method lie in that it is in better correspondence with the response of the human visual system and more suitable for salient objects extraction. The experimental results demonstrate the effectiveness of the proposed image enhance method. version:1
arxiv-1302-2645 | Geometrical complexity of data approximators | http://arxiv.org/abs/1302.2645 | id:1302.2645 author:E. M. Mirkes, A. Zinovyev, A. N. Gorban category:stat.ML cs.LG  published:2013-02-11 summary:There are many methods developed to approximate a cloud of vectors embedded in high-dimensional space by simpler objects: starting from principal points and linear manifolds to self-organizing maps, neural gas, elastic maps, various types of principal curves and principal trees, and so on. For each type of approximators the measure of the approximator complexity was developed too. These measures are necessary to find the balance between accuracy and complexity and to define the optimal approximations of a given type. We propose a measure of complexity (geometrical complexity) which is applicable to approximators of several types and which allows comparing data approximations of different types. version:2
arxiv-1305-0855 | Inference in Kingman's Coalescent with Particle Markov Chain Monte Carlo Method | http://arxiv.org/abs/1305.0855 | id:1305.0855 author:Yifei Chen, Xiaohui Xie category:stat.ML q-bio.PE  published:2013-05-03 summary:We propose a new algorithm to do posterior sampling of Kingman's coalescent, based upon the Particle Markov Chain Monte Carlo methodology. Specifically, the algorithm is an instantiation of the Particle Gibbs Sampling method, which alternately samples coalescent times conditioned on coalescent tree structures, and tree structures conditioned on coalescent times via the conditional Sequential Monte Carlo procedure. We implement our algorithm as a C++ package, and demonstrate its utility via a parameter estimation task in population genetics on both single- and multiple-locus data. The experiment results show that the proposed algorithm performs comparable to or better than several well-developed methods. version:1
arxiv-1301-0254 | Group theory, group actions, evolutionary algorithms, and global optimization | http://arxiv.org/abs/1301.0254 | id:1301.0254 author:Andrew Clark category:cs.NE math.DS math.OC math.RA  published:2012-12-27 summary:In this paper we use group, action and orbit to understand how evolutionary solve nonconvex optimization problems. version:3
arxiv-1305-0698 | Learning from Imprecise and Fuzzy Observations: Data Disambiguation through Generalized Loss Minimization | http://arxiv.org/abs/1305.0698 | id:1305.0698 author:Eyke Hüllermeier category:cs.LG  published:2013-05-03 summary:Methods for analyzing or learning from "fuzzy data" have attracted increasing attention in recent years. In many cases, however, existing methods (for precise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner, and without carefully considering the interpretation of a fuzzy set when being used for modeling data. Distinguishing between an ontic and an epistemic interpretation of fuzzy set-valued data, and focusing on the latter, we argue that a "fuzzification" of learning algorithms based on an application of the generic extension principle is not appropriate. In fact, the extension principle fails to properly exploit the inductive bias underlying statistical and machine learning methods, although this bias, at least in principle, offers a means for "disambiguating" the fuzzy data. Alternatively, we therefore propose a method which is based on the generalization of loss functions in empirical risk minimization, and which performs model identification and data disambiguation simultaneously. Elaborating on the fuzzification of specific types of losses, we establish connections to well-known loss functions in regression and classification. We compare our approach with related methods and illustrate its use in logistic regression for binary classification. version:1
arxiv-1305-0638 | Feature Selection Based on Term Frequency and T-Test for Text Categorization | http://arxiv.org/abs/1305.0638 | id:1305.0638 author:Deqing Wang, Hui Zhang, Rui Liu, Weifeng Lv category:cs.LG cs.IR stat.ML  published:2013-05-03 summary:Much work has been done on feature selection. Existing methods are based on document frequency, such as Chi-Square Statistic, Information Gain etc. However, these methods have two shortcomings: one is that they are not reliable for low-frequency terms, and the other is that they only count whether one term occurs in a document and ignore the term frequency. Actually, high-frequency terms within a specific category are often regards as discriminators. This paper focuses on how to construct the feature selection function based on term frequency, and proposes a new approach based on $t$-test, which is used to measure the diversity of the distributions of a term between the specific category and the entire corpus. Extensive comparative experiments on two text corpora using three classifiers show that our new approach is comparable to or or slightly better than the state-of-the-art feature selection methods (i.e., $\chi^2$, and IG) in terms of macro-$F_1$ and micro-$F_1$. version:1
arxiv-1305-0630 | Anisotropic oracle inequalities in noisy quantization | http://arxiv.org/abs/1305.0630 | id:1305.0630 author:Sébastien Loustau category:math.ST stat.ML stat.TH  published:2013-05-03 summary:The effect of errors in variables in quantization is investigated. We prove general exact and non-exact oracle inequalities with fast rates for an empirical minimization based on a noisy sample $Z_i=X_i+\epsilon_i,i=1,\ldots,n$, where $X_i$ are i.i.d. with density $f$ and $\epsilon_i$ are i.i.d. with density $\eta$. These rates depend on the geometry of the density $f$ and the asymptotic behaviour of the characteristic function of $\eta$. This general study can be applied to the problem of $k$-means clustering with noisy data. For this purpose, we introduce a deconvolution $k$-means stochastic minimization which reaches fast rates of convergence under standard Pollard's regularity assumptions. version:1
arxiv-1305-0626 | An Improved EM algorithm | http://arxiv.org/abs/1305.0626 | id:1305.0626 author:Fuqiang Chen category:cs.LG cs.AI stat.ML  published:2013-05-03 summary:In this paper, we firstly give a brief introduction of expectation maximization (EM) algorithm, and then discuss the initial value sensitivity of expectation maximization algorithm. Subsequently, we give a short proof of EM's convergence. Then, we implement experiments with the expectation maximization algorithm (We implement all the experiments on Gaussion mixture model (GMM)). Our experiment with expectation maximization is performed in the following three cases: initialize randomly; initialize with result of K-means; initialize with result of K-medoids. The experiment result shows that expectation maximization algorithm depend on its initial state or parameters. And we found that EM initialized with K-medoids performed better than both the one initialized with K-means and the one initialized randomly. version:1
arxiv-1305-0625 | CONATION: English Command Input/Output System for Computers | http://arxiv.org/abs/1305.0625 | id:1305.0625 author:Kamlesh Sharma, Dr. T. V. Prasad category:cs.HC cs.CL  published:2013-05-03 summary:In this information technology age, a convenient and user friendly interface is required to operate the computer system on very fast rate. In the human being, speech being a natural mode of communication has potential to being a fast and convenient mode of interaction with computer. Speech recognition will play an important role in taking technology to them. It is the need of this era to access the information within seconds. This paper describes the design and development of speaker independent and English command interpreted system for computers. HMM model is used to represent the phoneme like speech commands. Experiments have been done on real world data and system has been trained in normal condition for real world subject. version:1
arxiv-1211-3760 | Mixed LICORS: A Nonparametric Algorithm for Predictive State Reconstruction | http://arxiv.org/abs/1211.3760 | id:1211.3760 author:Georg M. Goerg, Cosma Rohilla Shalizi category:stat.ME stat.ML  published:2012-11-15 summary:We introduce 'mixed LICORS', an algorithm for learning nonlinear, high-dimensional dynamics from spatio-temporal data, suitable for both prediction and simulation. Mixed LICORS extends the recent LICORS algorithm (Goerg and Shalizi, 2012) from hard clustering of predictive distributions to a non-parametric, EM-like soft clustering. This retains the asymptotic predictive optimality of LICORS, but, as we show in simulations, greatly improves out-of-sample forecasts with limited data. The new method is implemented in the publicly-available R package "LICORS" (http://cran.r-project.org/web/packages/LICORS/). version:2
arxiv-1305-0423 | Testing Hypotheses by Regularized Maximum Mean Discrepancy | http://arxiv.org/abs/1305.0423 | id:1305.0423 author:Somayeh Danafar, Paola M. V. Rancoita, Tobias Glasmachers, Kevin Whittingstall, Juergen Schmidhuber category:cs.LG cs.AI stat.ML  published:2013-05-02 summary:Do two data samples come from different distributions? Recent studies of this fundamental problem focused on embedding probability distributions into sufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to compare distributions by the distance between their embeddings. We show that Regularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based hypothesis testing, yields substantial improvements even when sample sizes are small, and excels at hypothesis tests involving multiple comparisons with power control. We derive asymptotic distributions under the null and alternative hypotheses, and assess power control. Outstanding results are obtained on: challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar dataset. version:1
arxiv-1305-0395 | Tensor Decompositions: A New Concept in Brain Data Analysis? | http://arxiv.org/abs/1305.0395 | id:1305.0395 author:Andrzej Cichocki category:cs.NA cs.LG q-bio.NC stat.ML  published:2013-05-02 summary:Matrix factorizations and their extensions to tensor factorizations and decompositions have become prominent techniques for linear and multilinear blind source separation (BSS), especially multiway Independent Component Analysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), Smooth Component Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover, tensor decompositions have many other potential applications beyond multilinear BSS, especially feature extraction, classification, dimensionality reduction and multiway clustering. In this paper, we briefly overview new and emerging models and approaches for tensor decompositions in applications to group and linked multiway BSS/ICA, feature extraction, classification andMultiway Partial Least Squares (MPLS) regression problems. Keywords: Multilinear BSS, linked multiway BSS/ICA, tensor factorizations and decompositions, constrained Tucker and CP models, Penalized Tensor Decompositions (PTD), feature extraction, classification, multiway PLS and CCA. version:1
arxiv-1301-4566 | Sparse/Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory | http://arxiv.org/abs/1301.4566 | id:1301.4566 author:Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto category:stat.ML math.OC math.ST stat.TH 62F35  65K10  published:2013-01-19 summary:We introduce a class of quadratic support (QS) functions, many of which play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and Kalman smoothing. Well known examples include the l2, Huber, l1 and Vapnik losses. We build on a dual representation for QS functions using convex analysis, revealing the structure necessary for a QS function to be interpreted as the negative log of a probability density, and providing the foundation for statistical interpretation and analysis of QS loss functions. For a subclass of QS functions called piecewise linear quadratic (PLQ) penalties, we also develop efficient numerical estimation schemes. These components form a flexible statistical modeling framework for a variety of learning applications, together with a toolbox of efficient numerical methods for inference. In particular, for PLQ densities, interior point (IP) methods can be used. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. The efficiency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaussian errors are assumed in the process and measurement models. The extended framework allows arbitrary PLQ densities to be used, and the proposed IP approach solves the generalized Kalman smoothing problem while maintaining the linear complexity in the size of the time series, just as in the Gaussian case. This extends the computational efficiency of classic algorithms to a much broader nonsmooth setting, and includes many recently proposed robust and sparse smoothers as special cases. version:2
arxiv-1305-0355 | Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition | http://arxiv.org/abs/1305.0355 | id:1305.0355 author:Adel Javanmard, Andrea Montanari category:math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH  published:2013-05-02 summary:In the high-dimensional regression model a response variable is linearly related to $p$ covariates, but the sample size $n$ is smaller than $p$. We assume that only a small subset of covariates is `active' (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso ($\ell_1$-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called `irrepresentability' condition. In this paper we study the `Gauss-Lasso' selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate `generalized irrepresentability condition' (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. version:1
arxiv-1305-0311 | An Adaptive Descriptor Design for Object Recognition in the Wild | http://arxiv.org/abs/1305.0311 | id:1305.0311 author:Zhenyu Guo, Z. Jane Wang category:cs.CV  published:2013-05-01 summary:Digital images nowadays have various styles of appearance, in the aspects of color tones, contrast, vignetting, and etc. These 'picture styles' are directly related to the scene radiance, image pipeline of the camera, and post processing functions. Due to the complexity and nonlinearity of these causes, popular gradient-based image descriptors won't be invariant to different picture styles, which will decline the performance of object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we present the first study on the influence of picture styles for object recognition, and propose an adaptive approach based on the kernel view of gradient descriptors and multiple kernel learning, without estimating or specifying the styles of images used in training and testing. We conduct experiments on Domain Adaptation data set and Oxford Flower data set. The experiments also include several variants of the flower data set by processing the images with popular photo effects. The results demonstrate that our proposed method improve from standard descriptors in all cases. version:1
arxiv-1209-1380 | The Sample Complexity of Search over Multiple Populations | http://arxiv.org/abs/1209.1380 | id:1209.1380 author:Matthew L. Malloy, Gongguo Tang, Robert D. Nowak category:cs.IT math.IT stat.ML  published:2012-09-06 summary:This paper studies the sample complexity of searching over multiple populations. We consider a large number of populations, each corresponding to either distribution P0 or P1. The goal of the search problem studied here is to find one population corresponding to distribution P1 with as few samples as possible. The main contribution is to quantify the number of samples needed to correctly find one such population. We consider two general approaches: non-adaptive sampling methods, which sample each population a predetermined number of times until a population following P1 is found, and adaptive sampling methods, which employ sequential sampling schemes for each population. We first derive a lower bound on the number of samples required by any sampling scheme. We then consider an adaptive procedure consisting of a series of sequential probability ratio tests, and show it comes within a constant factor of the lower bound. We give explicit expressions for this constant when samples of the populations follow Gaussian and Bernoulli distributions. An alternative adaptive scheme is discussed which does not require full knowledge of P1, and comes within a constant factor of the optimal scheme. For comparison, a lower bound on the sampling requirements of any non-adaptive scheme is presented. version:2
arxiv-1305-0218 | Video Segmentation via Diffusion Bases | http://arxiv.org/abs/1305.0218 | id:1305.0218 author:Dina Dushnik, Alon Schclar, Amir Averbuch category:cs.CV cs.MM  published:2013-05-01 summary:Identifying moving objects in a video sequence, which is produced by a static camera, is a fundamental and critical task in many computer-vision applications. A common approach performs background subtraction, which identifies moving objects as the portion of a video frame that differs significantly from a background model. A good background subtraction algorithm has to be robust to changes in the illumination and it should avoid detecting non-stationary background objects such as moving leaves, rain, snow, and shadows. In addition, the internal background model should quickly respond to changes in background such as objects that start to move or stop. We present a new algorithm for video segmentation that processes the input video sequence as a 3D matrix where the third axis is the time domain. Our approach identifies the background by reducing the input dimension using the \emph{diffusion bases} methodology. Furthermore, we describe an iterative method for extracting and deleting the background. The algorithm has two versions and thus covers the complete range of backgrounds: one for scenes with static backgrounds and the other for scenes with dynamic (moving) backgrounds. version:1
arxiv-1305-0194 | MATAWS: A Multimodal Approach for Automatic WS Semantic Annotation | http://arxiv.org/abs/1305.0194 | id:1305.0194 author:Cihan Aksoy, Vincent Labatut, Chantal Cherifi, Jean-François Santucci category:cs.SE cs.CL cs.IR  published:2013-05-01 summary:Many recent works aim at developing methods and tools for the processing of semantic Web services. In order to be properly tested, these tools must be applied to an appropriate benchmark, taking the form of a collection of semantic WS descriptions. However, all of the existing publicly available collections are limited by their size or their realism (use of randomly generated or resampled descriptions). Larger and realistic syntactic (WSDL) collections exist, but their semantic annotation requires a certain level of automation, due to the number of operations to be processed. In this article, we propose a fully automatic method to semantically annotate such large WS collections. Our approach is multimodal, in the sense it takes advantage of the latent semantics present not only in the parameter names, but also in the type names and structures. Concept-to-word association is performed by using Sigma, a mapping of WordNet to the SUMO ontology. After having described in details our annotation method, we apply it to the larger collection of real-world syntactic WS descriptions we could find, and assess its efficiency. version:1
arxiv-1304-8020 | Semi-Supervised Information-Maximization Clustering | http://arxiv.org/abs/1304.8020 | id:1304.8020 author:Daniele Calandriello, Gang Niu, Masashi Sugiyama category:cs.LG stat.ML  published:2013-04-30 summary:Semi-supervised clustering aims to introduce prior knowledge in the decision process of a clustering algorithm. In this paper, we propose a novel semi-supervised clustering algorithm based on the information-maximization principle. The proposed method is an extension of a previous unsupervised information-maximization clustering algorithm based on squared-loss mutual information to effectively incorporate must-links and cannot-links. The proposed method is computationally efficient because the clustering solution can be obtained analytically via eigendecomposition. Furthermore, the proposed method allows systematic optimization of tuning parameters such as the kernel width, given the degree of belief in the must-links and cannot-links. The usefulness of the proposed method is demonstrated through experiments. version:2
arxiv-1209-1033 | The Annealing Sparse Bayesian Learning Algorithm | http://arxiv.org/abs/1209.1033 | id:1209.1033 author:Benyuan Liu, Hongqi Fan, Zaiqi Lu, Qiang Fu category:cs.IT cs.LG math.IT  published:2012-09-05 summary:In this paper we propose a two-level hierarchical Bayesian model and an annealing schedule to re-enable the noise variance learning capability of the fast marginalized Sparse Bayesian Learning Algorithms. The performance such as NMSE and F-measure can be greatly improved due to the annealing technique. This algorithm tends to produce the most sparse solution under moderate SNR scenarios and can outperform most concurrent SBL algorithms while pertains small computational load. version:4
arxiv-1301-3641 | Training Neural Networks with Stochastic Hessian-Free Optimization | http://arxiv.org/abs/1301.3641 | id:1301.3641 author:Ryan Kiros category:cs.LG cs.NE stat.ML  published:2013-01-16 summary:Hessian-free (HF) optimization has been successfully used for training deep autoencoders and recurrent networks. HF uses the conjugate gradient algorithm to construct update directions through curvature-vector products that can be computed on the same order of time as gradients. In this paper we exploit this property and study stochastic HF with gradient and curvature mini-batches independent of the dataset size. We modify Martens' HF for these settings and integrate dropout, a method for preventing co-adaptation of feature detectors, to guard against overfitting. Stochastic Hessian-free optimization gives an intermediary between SGD and HF that achieves competitive performance on both classification and deep autoencoder experiments. version:3
arxiv-1305-0103 | Clustering Unclustered Data: Unsupervised Binary Labeling of Two Datasets Having Different Class Balances | http://arxiv.org/abs/1305.0103 | id:1305.0103 author:Marthinus Christoffel du Plessis, Masashi Sugiyama category:cs.LG  published:2013-05-01 summary:We consider the unsupervised learning problem of assigning labels to unlabeled data. A naive approach is to use clustering methods, but this works well only when data is properly clustered and each cluster corresponds to an underlying class. In this paper, we first show that this unsupervised labeling problem in balanced binary cases can be solved if two unlabeled datasets having different class balances are available. More specifically, estimation of the sign of the difference between probability densities of two unlabeled datasets gives the solution. We then introduce a new method to directly estimate the sign of the density difference without density estimation. Finally, we demonstrate the usefulness of the proposed method against several clustering methods on various toy problems and real-world datasets. version:1
arxiv-1301-3568 | Joint Training Deep Boltzmann Machines for Classification | http://arxiv.org/abs/1301.3568 | id:1301.3568 author:Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG  published:2013-01-16 summary:We introduce a new method for training deep Boltzmann machines jointly. Prior methods of training DBMs require an initial learning pass that trains the model greedily, one layer at a time, or do not perform well on classification tasks. In our approach, we train all layers of the DBM simultaneously, using a novel training procedure called multi-prediction training. The resulting model can either be interpreted as a single generative model trained to maximize a variational approximation to the generalized pseudolikelihood, or as a family of recurrent networks that share parameters and may be approximately averaged together using a novel technique we call the multi-inference trick. We show that our approach performs competitively for classification and outperforms previous methods in terms of accuracy of approximate inference and classification with missing inputs. version:3
arxiv-1304-4610 | Spectral Compressed Sensing via Structured Matrix Completion | http://arxiv.org/abs/1304.4610 | id:1304.4610 author:Yuxin Chen, Yuejie Chi category:cs.IT cs.LG math.IT math.NA stat.ML  published:2013-04-16 summary:The paper studies the problem of recovering a spectrally sparse object from a small number of time domain samples. Specifically, the object of interest with ambient dimension $n$ is assumed to be a mixture of $r$ complex multi-dimensional sinusoids, while the underlying frequencies can assume any value in the unit disk. Conventional compressed sensing paradigms suffer from the {\em basis mismatch} issue when imposing a discrete dictionary on the Fourier representation. To address this problem, we develop a novel nonparametric algorithm, called enhanced matrix completion (EMaC), based on structured matrix completion. The algorithm starts by arranging the data into a low-rank enhanced form with multi-fold Hankel structure, then attempts recovery via nuclear norm minimization. Under mild incoherence conditions, EMaC allows perfect recovery as soon as the number of samples exceeds the order of $\mathcal{O}(r\log^{2} n)$. We also show that, in many instances, accurate completion of a low-rank multi-fold Hankel matrix is possible when the number of observed entries is proportional to the information theoretical limits (except for a logarithmic gap). The robustness of EMaC against bounded noise and its applicability to super resolution are further demonstrated by numerical experiments. version:2
arxiv-1305-0051 | Revealing social networks of spammers through spectral clustering | http://arxiv.org/abs/1305.0051 | id:1305.0051 author:Kevin S. Xu, Mark Kliger, Yilun Chen, Peter J. Woolf, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ML  published:2013-04-30 summary:To date, most studies on spam have focused only on the spamming phase of the spam cycle and have ignored the harvesting phase, which consists of the mass acquisition of email addresses. It has been observed that spammers conceal their identity to a lesser degree in the harvesting phase, so it may be possible to gain new insights into spammers' behavior by studying the behavior of harvesters, which are individuals or bots that collect email addresses. In this paper, we reveal social networks of spammers by identifying communities of harvesters with high behavioral similarity using spectral clustering. The data analyzed was collected through Project Honey Pot, a distributed system for monitoring harvesting and spamming. Our main findings are (1) that most spammers either send only phishing emails or no phishing emails at all, (2) that most communities of spammers also send only phishing emails or no phishing emails at all, and (3) that several groups of spammers within communities exhibit coherent temporal behavior and have similar IP addresses. Our findings reveal some previously unknown behavior of spammers and suggest that there is indeed social structure between spammers to be discovered. version:1
arxiv-1305-0020 | Image Compression By Embedding Five Modulus Method Into JPEG | http://arxiv.org/abs/1305.0020 | id:1305.0020 author:Firas A. Jassim category:cs.CV cs.MM  published:2013-04-30 summary:The standard JPEG format is almost the optimum format in image compression. The compression ratio in JPEG sometimes reaches 30:1. The compression ratio of JPEG could be increased by embedding the Five Modulus Method (FMM) into the JPEG algorithm. The novel algorithm gives twice the time as the standard JPEG algorithm or more. The novel algorithm was called FJPEG (Five-JPEG). The quality of the reconstructed image after compression is approximately approaches the JPEG. Standard test images have been used to support and implement the suggested idea in this paper and the error metrics have been computed and compared with JPEG. version:1
arxiv-1305-0015 | Inferring ground truth from multi-annotator ordinal data: a probabilistic approach | http://arxiv.org/abs/1305.0015 | id:1305.0015 author:Balaji Lakshminarayanan, Yee Whye Teh category:stat.ML cs.LG  published:2013-04-30 summary:A popular approach for large scale data annotation tasks is crowdsourcing, wherein each data point is labeled by multiple noisy annotators. We consider the problem of inferring ground truth from noisy ordinal labels obtained from multiple annotators of varying and unknown expertise levels. Annotation models for ordinal data have been proposed mostly as extensions of their binary/categorical counterparts and have received little attention in the crowdsourcing literature. We propose a new model for crowdsourced ordinal data that accounts for instance difficulty as well as annotator expertise, and derive a variational Bayesian inference algorithm for parameter estimation. We analyze the ordinal extensions of several state-of-the-art annotator models for binary/categorical labels and evaluate the performance of all the models on two real world datasets containing ordinal query-URL relevance scores, collected through Amazon's Mechanical Turk. Our results indicate that the proposed model performs better or as well as existing state-of-the-art methods and is more resistant to `spammy' annotators (i.e., annotators who assign labels randomly without actually looking at the instance) than popular baselines such as mean, median, and majority vote which do not account for annotator expertise. version:1
arxiv-1304-8092 | Fractal-Based Detection of Microcalcification Clusters in Digital Mammograms | http://arxiv.org/abs/1304.8092 | id:1304.8092 author:P. Shanmugavadivu, V. Sivakumar category:cs.CV  published:2013-04-30 summary:In this paper, a novel method for edge detection of microcalcification clusters in mammogram images is presented using the concept of Fractal Dimension and Hurst co-efficient that enables to locate the microcalcifications in the mammograms. This technique detects the edges accurately than the ones obtained by the conventional Sobel method. Generally, Sobel method detects the edges of the regions/objects in an image using the Fudge factor that assumes its value as 0.5, by default. In this proposed technique, the Fudge factor is suitably replaced with Hurst Co-efficient, which is computed as the difference of Fractal dimension and the topological dimension of a given input image. These two dimensions are image-dependent, and hence the respective Hurst co-efficient too varies with respect to images. Hence, the image-dependent Hurst co-efficient based Sobel method is proved to produce better results than the Fudge factor based Sobel method. The results of the proposed method substantiate the merit of the proposed technique. version:1
arxiv-1304-8087 | Uniqueness of Tensor Decompositions with Applications to Polynomial Identifiability | http://arxiv.org/abs/1304.8087 | id:1304.8087 author:Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan category:cs.DS cs.LG math.ST stat.TH  published:2013-04-30 summary:We give a robust version of the celebrated result of Kruskal on the uniqueness of tensor decompositions: we prove that given a tensor whose decomposition satisfies a robust form of Kruskal's rank condition, it is possible to approximately recover the decomposition if the tensor is known up to a sufficiently small (inverse polynomial) error. Kruskal's theorem has found many applications in proving the identifiability of parameters for various latent variable models and mixture models such as Hidden Markov models, topic models etc. Our robust version immediately implies identifiability using only polynomially many samples in many of these settings. This polynomial identifiability is an essential first step towards efficient learning algorithms for these models. Recently, algorithms based on tensor decompositions have been used to estimate the parameters of various hidden variable models efficiently in special cases as long as they satisfy certain "non-degeneracy" properties. Our methods give a way to go beyond this non-degeneracy barrier, and establish polynomial identifiability of the parameters under much milder conditions. Given the importance of Kruskal's theorem in the tensor literature, we expect that this robust version will have several applications beyond the settings we explore in this work. version:1
arxiv-1304-7993 | Digenes: genetic algorithms to discover conjectures about directed and undirected graphs | http://arxiv.org/abs/1304.7993 | id:1304.7993 author:Romain Absil, Hadrien Mélot category:cs.DM cs.NE  published:2013-04-30 summary:We present Digenes, a new discovery system that aims to help researchers in graph theory. While its main task is to find extremal graphs for a given (function of) invariants, it also provides some basic support in proof conception. This has already been proved to be very useful to find new conjectures since the AutoGraphiX system of Caporossi and Hansen (Discrete Math. 212-2000). However, unlike existing systems, Digenes can be used both with directed or undirected graphs. In this paper, we present the principles and functionality of Digenes, describe the genetic algorithms that have been designed to achieve them, and give some computational results and open questions. This do arise some interesting questions regarding genetic algorithms design particular to this field, such as crossover definition. version:1
arxiv-1304-7942 | ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge | http://arxiv.org/abs/1304.7942 | id:1304.7942 author:Michele Filannino, Gavin Brown, Goran Nenadic category:cs.CL I.2.7; I.2.4; I.2.6  published:2013-04-30 summary:This paper describes a temporal expression identification and normalization system, ManTIME, developed for the TempEval-3 challenge. The identification phase combines the use of conditional random fields along with a post-processing identification pipeline, whereas the normalization phase is carried out using NorMA, an open-source rule-based temporal normalizer. We investigate the performance variation with respect to different feature types. Specifically, we show that the use of WordNet-based features in the identification task negatively affects the overall performance, and that there is no statistically significant difference in using gazetteers, shallow parsing and propositional noun phrases labels on top of the morphological features. On the test data, the best run achieved 0.95 (P), 0.85 (R) and 0.90 (F1) in the identification phase. Normalization accuracies are 0.84 (type attribute) and 0.77 (value attribute). Surprisingly, the use of the silver data (alone or in addition to the gold annotated ones) does not improve the performance. version:1
arxiv-1005-0826 | Clustering processes | http://arxiv.org/abs/1005.0826 | id:1005.0826 author:Daniil Ryabko category:cs.LG cs.IT math.IT stat.ML  published:2010-05-05 summary:The problem of clustering is considered, for the case when each data point is a sample generated by a stationary ergodic process. We propose a very natural asymptotic notion of consistency, and show that simple consistent algorithms exist, under most general non-parametric assumptions. The notion of consistency is as follows: two samples should be put into the same cluster if and only if they were generated by the same distribution. With this notion of consistency, clustering generalizes such classical statistical problems as homogeneity testing and process classification. We show that, for the case of a known number of clusters, consistency can be achieved under the only assumption that the joint distribution of the data is stationary ergodic (no parametric or Markovian assumptions, no assumptions of independence, neither between nor within the samples). If the number of clusters is unknown, consistency can be achieved under appropriate assumptions on the mixing rates of the processes. (again, no parametric or independence assumptions). In both cases we give examples of simple (at most quadratic in each argument) algorithms which are consistent. version:2
arxiv-1304-7230 | Learning Densities Conditional on Many Interacting Features | http://arxiv.org/abs/1304.7230 | id:1304.7230 author:David C. Kessler, Jack Taylor, David B. Dunson category:stat.ML cs.LG  published:2013-04-26 summary:Learning a distribution conditional on a set of discrete-valued features is a commonly encountered task. This becomes more challenging with a high-dimensional feature set when there is the possibility of interaction between the features. In addition, many frequently applied techniques consider only prediction of the mean, but the complete conditional density is needed to answer more complex questions. We demonstrate a novel nonparametric Bayes method based upon a tensor factorization of feature-dependent weights for Gaussian kernels. The method makes use of multistage feature selection for dimension reduction. The resulting conditional density morphs flexibly with the selected features. version:2
arxiv-1206-1623 | Proximal Newton-type methods for minimizing composite functions | http://arxiv.org/abs/1206.1623 | id:1206.1623 author:Jason D. Lee, Yuekai Sun, Michael A. Saunders category:stat.ML cs.DS cs.LG cs.NA math.OC  published:2012-06-07 summary:We generalize Newton-type methods for minimizing smooth functions to handle a sum of two convex functions: a smooth function and a nonsmooth function with a simple proximal mapping. We show that the resulting proximal Newton-type methods inherit the desirable convergence behavior of Newton-type methods for minimizing smooth functions, even when search directions are computed inexactly. Many popular methods tailored to problems arising in bioinformatics, signal processing, and statistical learning are special cases of proximal Newton-type methods, and our analysis yields new convergence results for some of these methods. version:13
arxiv-1209-2388 | On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization | http://arxiv.org/abs/1209.2388 | id:1209.2388 author:Ohad Shamir category:cs.LG math.OC stat.ML  published:2012-09-11 summary:The problem of stochastic convex optimization with bandit feedback (in the learning community) or without knowledge of gradients (in the optimization community) has received much attention in recent years, in the form of algorithms and performance upper bounds. However, much less is known about the inherent complexity of these problems, and there are few lower bounds in the literature, especially for nonlinear functions. In this paper, we investigate the attainable error/regret in the bandit and derivative-free settings, as a function of the dimension d and the available number of queries T. We provide a precise characterization of the attainable performance for strongly-convex and smooth functions, which also imply a non-trivial lower bound for more general problems. Moreover, we prove that in both the bandit and derivative-free setting, the required number of queries must scale at least quadratically with the dimension. Finally, we show that on the natural class of quadratic functions, it is possible to obtain a "fast" O(1/T) error rate in terms of T, under mild assumptions, even without having access to gradients. To the best of our knowledge, this is the first such rate in a derivative-free stochastic setting, and holds despite previous results which seem to imply the contrary. version:3
arxiv-1304-7728 | Machine Translation Systems in India | http://arxiv.org/abs/1304.7728 | id:1304.7728 author:Sugata Sanyal, Rajdeep Borgohain category:cs.CL cs.CY  published:2013-04-29 summary:Machine Translation is the translation of one natural language into another using automated and computerized means. For a multilingual country like India, with the huge amount of information exchanged between various regions and in different languages in digitized format, it has become necessary to find an automated process from one language to another. In this paper, we take a look at the various Machine Translation System in India which is specifically built for the purpose of translation between the Indian languages. We discuss the various approaches taken for building the machine translation system and then discuss some of the Machine Translation Systems in India along with their features. version:1
arxiv-1304-7713 | Markovian models for one dimensional structure estimation on heavily noisy imagery | http://arxiv.org/abs/1304.7713 | id:1304.7713 author:Ana Georgina Flesia, Javier Gimenez, Elena Rufeil Fiori category:cs.CV stat.AP  published:2013-04-29 summary:Radar (SAR) images often exhibit profound appearance variations due to a variety of factors including clutter noise produced by the coherent nature of the illumination. Ultrasound images and infrared images have similar cluttered appearance, that make 1 dimensional structures, as edges and object boundaries difficult to locate. Structure information is usually extracted in two steps: first, building and edge strength mask classifying pixels as edge points by hypothesis testing, and secondly estimating from that mask, pixel wide connected edges. With constant false alarm rate (CFAR) edge strength detectors for speckle clutter, the image needs to be scanned by a sliding window composed of several differently oriented splitting sub-windows. The accuracy of edge location for these ratio detectors depends strongly on the orientation of the sub-windows. In this work we propose to transform the edge strength detection problem into a binary segmentation problem in the undecimated wavelet domain, solvable using parallel 1d Hidden Markov Models. For general dependency models, exact estimation of the state map becomes computationally complex, but in our model, exact MAP is feasible. The effectiveness of our approach is demonstrated on simulated noisy real-life natural images with available ground truth, while the strength of our output edge map is measured with Pratt's, Baddeley an Kappa proficiency measures. Finally, analysis and experiments on three different types of SAR images, with different polarizations, resolutions and textures, illustrate that the proposed method can detect structure on SAR images effectively, providing a very good start point for active contour methods. version:1
arxiv-1304-7710 | Learning Geo-Temporal Non-Stationary Failure and Recovery of Power Distribution | http://arxiv.org/abs/1304.7710 | id:1304.7710 author:Yun Wei, Chuanyi Ji, Floyd Galvan, Stephen Couvillon, George Orellana, James Momoh category:cs.SY cs.LG physics.soc-ph  published:2013-04-29 summary:Smart energy grid is an emerging area for new applications of machine learning in a non-stationary environment. Such a non-stationary environment emerges when large-scale failures occur at power distribution networks due to external disturbances such as hurricanes and severe storms. Power distribution networks lie at the edge of the grid, and are especially vulnerable to external disruptions. Quantifiable approaches are lacking and needed to learn non-stationary behaviors of large-scale failure and recovery of power distribution. This work studies such non-stationary behaviors in three aspects. First, a novel formulation is derived for an entire life cycle of large-scale failure and recovery of power distribution. Second, spatial-temporal models of failure and recovery of power distribution are developed as geo-location based multivariate non-stationary GI(t)/G(t)/Infinity queues. Third, the non-stationary spatial-temporal models identify a small number of parameters to be learned. Learning is applied to two real-life examples of large-scale disruptions. One is from Hurricane Ike, where data from an operational network is exact on failures and recoveries. The other is from Hurricane Sandy, where aggregated data is used for inferring failure and recovery processes at one of the impacted areas. Model parameters are learned using real data. Two findings emerge as results of learning: (a) Failure rates behave similarly at the two different provider networks for two different hurricanes but differently at the geographical regions. (b) Both rapid- and slow-recovery are present for Hurricane Ike but only slow recovery is shown for a regional distribution network from Hurricane Sandy. version:1
arxiv-1303-0417 | On the convergence of the IRLS algorithm in Non-Local Patch Regression | http://arxiv.org/abs/1303.0417 | id:1303.0417 author:Kunal N. Chaudhury category:cs.CV stat.ML  published:2013-03-02 summary:Recently, it was demonstrated in [CS2012,CS2013] that the robustness of the classical Non-Local Means (NLM) algorithm [BCM2005] can be improved by incorporating $\ell^p (0 < p \leq 2)$ regression into the NLM framework. This general optimization framework, called Non-Local Patch Regression (NLPR), contains NLM as a special case. Denoising results on synthetic and natural images show that NLPR consistently performs better than NLM beyond a moderate noise level, and significantly so when $p$ is close to zero. An iteratively reweighted least-squares (IRLS) algorithm was proposed for solving the regression problem in NLPR, where the NLM output was used to initialize the iterations. Based on exhaustive numerical experiments, we observe that the IRLS algorithm is globally convergent (for arbitrary initialization) in the convex regime $1 \leq p \leq 2$, and locally convergent (fails very rarely using NLM initialization) in the non-convex regime $0 < p < 1$. In this letter, we adapt the "majorize-minimize" framework introduced in [Voss1980] to explain these observations. [CS2012] Chaudhury et al. (2012), "Non-local Euclidean medians," IEEE Signal Processing Letters. [CS2013] Chaudhury et al. (2013), "Non-local patch regression: Robust image denoising in patch space," IEEE ICASSP. [BCM2005] Buades et al. (2005), "A review of image denoising algorithms, with a new one," Multiscale Modeling and Simulation. [Voss1980] Voss et al. (1980), "Linear convergence of generalized Weiszfeld's method," Computing. version:2
arxiv-1304-7607 | A Discrete State Transition Algorithm for Generalized Traveling Salesman Problem | http://arxiv.org/abs/1304.7607 | id:1304.7607 author:Xiaolin Tang, Chunhua Yang, Xiaojun Zhou, Weihua Gui category:math.OC cs.AI cs.NE  published:2013-04-29 summary:Generalized traveling salesman problem (GTSP) is an extension of classical traveling salesman problem (TSP), which is a combinatorial optimization problem and an NP-hard problem. In this paper, an efficient discrete state transition algorithm (DSTA) for GTSP is proposed, where a new local search operator named \textit{K-circle}, directed by neighborhood information in space, has been introduced to DSTA to shrink search space and strengthen search ability. A novel robust update mechanism, restore in probability and risk in probability (Double R-Probability), is used in our work to escape from local minima. The proposed algorithm is tested on a set of GTSP instances. Compared with other heuristics, experimental results have demonstrated the effectiveness and strong adaptability of DSTA and also show that DSTA has better search ability than its competitors. version:1
arxiv-1304-7577 | Optimal amortized regret in every interval | http://arxiv.org/abs/1304.7577 | id:1304.7577 author:Rina Panigrahy, Preyas Popat category:cs.LG cs.DS stat.ML  published:2013-04-29 summary:Consider the classical problem of predicting the next bit in a sequence of bits. A standard performance measure is {\em regret} (loss in payoff) with respect to a set of experts. For example if we measure performance with respect to two constant experts one that always predicts 0's and another that always predicts 1's it is well known that one can get regret $O(\sqrt T)$ with respect to the best expert by using, say, the weighted majority algorithm. But this algorithm does not provide performance guarantee in any interval. There are other algorithms that ensure regret $O(\sqrt {x \log T})$ in any interval of length $x$. In this paper we show a randomized algorithm that in an amortized sense gets a regret of $O(\sqrt x)$ for any interval when the sequence is partitioned into intervals arbitrarily. We empirically estimated the constant in the $O()$ for $T$ upto 2000 and found it to be small -- around 2.1. We also experimentally evaluate the efficacy of this algorithm in predicting high frequency stock data. version:1
arxiv-1304-7576 | Fractal structures in Adversarial Prediction | http://arxiv.org/abs/1304.7576 | id:1304.7576 author:Rina Panigrahy, Preyas Popat category:cs.LG  published:2013-04-29 summary:Fractals are self-similar recursive structures that have been used in modeling several real world processes. In this work we study how "fractal-like" processes arise in a prediction game where an adversary is generating a sequence of bits and an algorithm is trying to predict them. We will see that under a certain formalization of the predictive payoff for the algorithm it is most optimal for the adversary to produce a fractal-like sequence to minimize the algorithm's ability to predict. Indeed it has been suggested before that financial markets exhibit a fractal-like behavior. We prove that a fractal-like distribution arises naturally out of an optimization from the adversary's perspective. In addition, we give optimal trade-offs between predictability and expected deviation (i.e. sum of bits) for our formalization of predictive payoff. This result is motivated by the observation that several time series data exhibit higher deviations than expected for a completely random walk. version:1
arxiv-1304-7528 | Semi-supervised Eigenvectors for Large-scale Locally-biased Learning | http://arxiv.org/abs/1304.7528 | id:1304.7528 author:Toke J. Hansen, Michael W. Mahoney category:cs.LG math.SP stat.ML  published:2013-04-28 summary:In many applications, one has side information, e.g., labels that are provided in a semi-supervised manner, about a specific target region of a large data set, and one wants to perform machine learning and data analysis tasks "nearby" that prespecified target region. For example, one might be interested in the clustering structure of a data graph near a prespecified "seed set" of nodes, or one might be interested in finding partitions in an image that are near a prespecified "ground truth" set of pixels. Locally-biased problems of this sort are particularly challenging for popular eigenvector-based machine learning and data analysis tools. At root, the reason is that eigenvectors are inherently global quantities, thus limiting the applicability of eigenvector-based methods in situations where one is interested in very local properties of the data. In this paper, we address this issue by providing a methodology to construct semi-supervised eigenvectors of a graph Laplacian, and we illustrate how these locally-biased eigenvectors can be used to perform locally-biased machine learning. These semi-supervised eigenvectors capture successively-orthogonalized directions of maximum variance, conditioned on being well-correlated with an input seed set of nodes that is assumed to be provided in a semi-supervised manner. We show that these semi-supervised eigenvectors can be computed quickly as the solution to a system of linear equations; and we also describe several variants of our basic method that have improved scaling properties. We provide several empirical examples demonstrating how these semi-supervised eigenvectors can be used to perform locally-biased learning; and we discuss the relationship between our results and recent machine learning algorithms that use global eigenvectors of the graph Laplacian. version:1
arxiv-1304-5823 | Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors | http://arxiv.org/abs/1304.5823 | id:1304.5823 author:Edward Grefenstette category:math.LO cs.CL cs.LO 68T50  03B10 F.4.1; I.2.7  published:2013-04-22 summary:The development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature. This paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models, such as tensors and matrices, can be used to simulate different aspects of predicate logic. This paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors. It provides tensor interpretations of the set of logical connectives required to model propositional calculi. It suggests a variant of these tensor calculi capable of modelling quantifiers, using few non-linear operations. It finally discusses the relation between these variants, and how this relation should constitute the subject of future work. version:2
arxiv-1304-7507 | Measuring Cultural Relativity of Emotional Valence and Arousal using Semantic Clustering and Twitter | http://arxiv.org/abs/1304.7507 | id:1304.7507 author:Eugene Yuta Bann, Joanna J. Bryson category:cs.CL cs.AI  published:2013-04-28 summary:Researchers since at least Darwin have debated whether and to what extent emotions are universal or culture-dependent. However, previous studies have primarily focused on facial expressions and on a limited set of emotions. Given that emotions have a substantial impact on human lives, evidence for cultural emotional relativity might be derived by applying distributional semantics techniques to a text corpus of self-reported behaviour. Here, we explore this idea by measuring the valence and arousal of the twelve most popular emotion keywords expressed on the micro-blogging site Twitter. We do this in three geographical regions: Europe, Asia and North America. We demonstrate that in our sample, the valence and arousal levels of the same emotion keywords differ significantly with respect to these geographical regions --- Europeans are, or at least present themselves as more positive and aroused, North Americans are more negative and Asians appear to be more positive but less aroused when compared to global valence and arousal levels of the same emotion keywords. Our work is the first in kind to programatically map large text corpora to a dimensional model of affect. version:1
arxiv-1304-7465 | Deterministic Initialization of the K-Means Algorithm Using Hierarchical Clustering | http://arxiv.org/abs/1304.7465 | id:1304.7465 author:M. Emre Celebi, Hassan A. Kingravi category:cs.LG cs.CV I.5.3; H.2.8  published:2013-04-28 summary:K-means is undoubtedly the most widely used partitional clustering algorithm. Unfortunately, due to its gradient descent nature, this algorithm is highly sensitive to the initial placement of the cluster centers. Numerous initialization methods have been proposed to address this problem. Many of these methods, however, have superlinear complexity in the number of data points, making them impractical for large data sets. On the other hand, linear methods are often random and/or order-sensitive, which renders their results unrepeatable. Recently, Su and Dy proposed two highly successful hierarchical initialization methods named Var-Part and PCA-Part that are not only linear, but also deterministic (non-random) and order-invariant. In this paper, we propose a discriminant analysis based approach that addresses a common deficiency of these two methods. Experiments on a large and diverse collection of data sets from the UCI Machine Learning Repository demonstrate that Var-Part and PCA-Part are highly competitive with one of the best random initialization methods to date, i.e., k-means++, and that the proposed approach significantly improves the performance of both hierarchical methods. version:1
arxiv-1211-4520 | Storing cycles in Hopfield-type networks with pseudoinverse learning rule: admissibility and network topology | http://arxiv.org/abs/1211.4520 | id:1211.4520 author:Chuan Zhang, Gerhard Dangelmayr, Iuliana Oprea category:cs.NE 15A04  published:2012-11-19 summary:Cyclic patterns of neuronal activity are ubiquitous in animal nervous systems, and partially responsible for generating and controlling rhythmic movements such as locomotion, respiration, swallowing and so on. Clarifying the role of the network connectivities for generating cyclic patterns is fundamental for understanding the generation of rhythmic movements. In this paper, the storage of binary cycles in neural networks is investigated. We call a cycle $\Sigma$ admissible if a connectivity matrix satisfying the cycle's transition conditions exists, and construct it using the pseudoinverse learning rule. Our main focus is on the structural features of admissible cycles and corresponding network topology. We show that $\Sigma$ is admissible if and only if its discrete Fourier transform contains exactly $r={rank}(\Sigma)$ nonzero columns. Based on the decomposition of the rows of $\Sigma$ into loops, where a loop is the set of all cyclic permutations of a row, cycles are classified as simple cycles, separable or inseparable composite cycles. Simple cycles contain rows from one loop only, and the network topology is a feedforward chain with feedback to one neuron if the loop-vectors in $\Sigma$ are cyclic permutations of each other. Composite cycles contain rows from at least two disjoint loops, and the neurons corresponding to the rows in $\Sigma$ from the same loop are identified with a cluster. Networks constructed from separable composite cycles decompose into completely isolated clusters. For inseparable composite cycles at least two clusters are connected, and the cluster-connectivity is related to the intersections of the spaces spanned by the loop-vectors of the clusters. Simulations showing successfully retrieved cycles in continuous-time Hopfield-type networks and in networks of spiking neurons are presented. version:2
arxiv-1304-7423 | On Integrating Fuzzy Knowledge Using a Novel Evolutionary Algorithm | http://arxiv.org/abs/1304.7423 | id:1304.7423 author:Nafisa Afrin Chowdhury, Murshida Khatun, M. M. A. Hashem category:cs.NE cs.AI  published:2013-04-28 summary:Fuzzy systems may be considered as knowledge-based systems that incorporates human knowledge into their knowledge base through fuzzy rules and fuzzy membership functions. The intent of this study is to present a fuzzy knowledge integration framework using a Novel Evolutionary Strategy (NES), which can simultaneously integrate multiple fuzzy rule sets and their membership function sets. The proposed approach consists of two phases: fuzzy knowledge encoding and fuzzy knowledge integration. Four application domains, the hepatitis diagnosis, the sugarcane breeding prediction, Iris plants classification, and Tic-tac-toe endgame were used to show the performance ofthe proposed knowledge approach. Results show that the fuzzy knowledge base derived using our approach performs better than Genetic Algorithm based approach. version:1
arxiv-1304-7399 | Bingham Procrustean Alignment for Object Detection in Clutter | http://arxiv.org/abs/1304.7399 | id:1304.7399 author:Jared Glover, Sanja Popovic category:cs.CV cs.RO stat.AP  published:2013-04-27 summary:A new system for object detection in cluttered RGB-D images is presented. Our main contribution is a new method called Bingham Procrustean Alignment (BPA) to align models with the scene. BPA uses point correspondences between oriented features to derive a probability distribution over possible model poses. The orientation component of this distribution, conditioned on the position, is shown to be a Bingham distribution. This result also applies to the classic problem of least-squares alignment of point sets, when point features are orientation-less, and gives a principled, probabilistic way to measure pose uncertainty in the rigid alignment problem. Our detection system leverages BPA to achieve more reliable object detections in clutter. version:1
arxiv-1304-7289 | TimeML-strict: clarifying temporal annotation | http://arxiv.org/abs/1304.7289 | id:1304.7289 author:Leon Derczynski, Hector Llorens, Naushad UzZaman category:cs.CL I.2.7  published:2013-04-26 summary:TimeML is an XML-based schema for annotating temporal information over discourse. The standard has been used to annotate a variety of resources and is followed by a number of tools, the creation of which constitute hundreds of thousands of man-hours of research work. However, the current state of resources is such that many are not valid, or do not produce valid output, or contain ambiguous or custom additions and removals. Difficulties arising from these variances were highlighted in the TempEval-3 exercise, which included its own extra stipulations over conventional TimeML as a response. To unify the state of current resources, and to make progress toward easy adoption of its current incarnation ISO-TimeML, this paper introduces TimeML-strict: a valid, unambiguous, and easy-to-process subset of TimeML. We also introduce three resources -- a schema for TimeML-strict; a validator tool for TimeML-strict, so that one may ensure documents are in the correct form; and a repair tool that corrects common invalidating errors and adds disambiguating markup in order to convert documents from the laxer TimeML standard to TimeML-strict. version:1
arxiv-1211-1082 | Active and passive learning of linear separators under log-concave distributions | http://arxiv.org/abs/1211.1082 | id:1211.1082 author:Maria Florina Balcan, Philip M. Long category:cs.LG math.ST stat.ML stat.TH  published:2012-11-06 summary:We provide new results concerning label efficient, polynomial time, passive and active learning of linear separators. We prove that active learning provides an exponential improvement over PAC (passive) learning of homogeneous linear separators under nearly log-concave distributions. Building on this, we provide a computationally efficient PAC algorithm with optimal (up to a constant factor) sample complexity for such problems. This resolves an open question concerning the sample complexity of efficient PAC algorithms under the uniform distribution in the unit ball. Moreover, it provides the first bound for a polynomial-time PAC algorithm that is tight for an interesting infinite class of hypothesis functions under a general and natural class of data-distributions, providing significant progress towards a longstanding open question. We also provide new bounds for active and passive learning in the case that the data might not be linearly separable, both in the agnostic case and and under the Tsybakov low-noise condition. To derive our results, we provide new structural results for (nearly) log-concave distributions, which might be of independent interest as well. version:3
arxiv-1304-7236 | In the sight of my wearable camera: Classifying my visual experience | http://arxiv.org/abs/1304.7236 | id:1304.7236 author:Alessandro Perina, Nebojsa Jojic category:cs.CV  published:2013-04-26 summary:We introduce and we analyze a new dataset which resembles the input to biological vision systems much more than most previously published ones. Our analysis leaded to several important conclusions. First, it is possible to disambiguate over dozens of visual scenes (locations) encountered over the course of several weeks of a human life with accuracy of over 80%, and this opens up possibility for numerous novel vision applications, from early detection of dementia to everyday use of wearable camera streams for automatic reminders, and visual stream exchange. Second, our experimental results indicate that, generative models such as Latent Dirichlet Allocation or Counting Grids, are more suitable to such types of data, as they are more robust to overtraining and comfortable with images at low resolution, blurred and characterized by relatively random clutter and a mix of objects. version:1
arxiv-1212-2287 | Runtime Optimizations for Prediction with Tree-Based Models | http://arxiv.org/abs/1212.2287 | id:1212.2287 author:Nima Asadi, Jimmy Lin, Arjen P. de Vries category:cs.DB cs.IR cs.LG  published:2012-12-11 summary:Tree-based models have proven to be an effective solution for web ranking as well as other problems in diverse domains. This paper focuses on optimizing the runtime performance of applying such models to make predictions, given an already-trained model. Although exceedingly simple conceptually, most implementations of tree-based models do not efficiently utilize modern superscalar processor architectures. By laying out data structures in memory in a more cache-conscious fashion, removing branches from the execution flow using a technique called predication, and micro-batching predictions using a technique called vectorization, we are able to better exploit modern processor architectures and significantly improve the speed of tree-based models over hard-coded if-else blocks. Our work contributes to the exploration of architecture-conscious runtime implementations of machine learning algorithms. version:2
arxiv-1304-7211 | Algorithmic Optimisations for Iterative Deconvolution Methods | http://arxiv.org/abs/1304.7211 | id:1304.7211 author:Martin Welk, Martin Erler category:cs.CV I.4.4; F.2.1  published:2013-04-26 summary:We investigate possibilities to speed up iterative algorithms for non-blind image deconvolution. We focus on algorithms in which convolution with the point-spread function to be deconvolved is used in each iteration, and aim at accelerating these convolution operations as they are typically the most expensive part of the computation. We follow two approaches: First, for some practically important specific point-spread functions, algorithmically efficient sliding window or list processing techniques can be used. In some constellations this allows faster computation than via the Fourier domain. Second, as iterations progress, computation of convolutions can be restricted to subsets of pixels. For moderate thinning rates this can be done with almost no impact on the reconstruction quality. Both approaches are demonstrated in the context of Richardson-Lucy deconvolution but are not restricted to this method. version:1
arxiv-1304-0828 | Computational Lower Bounds for Sparse PCA | http://arxiv.org/abs/1304.0828 | id:1304.0828 author:Quentin Berthet, Philippe Rigollet category:math.ST cs.CC stat.ML stat.TH 62C20  published:2013-04-03 summary:In the context of sparse principal component detection, we bring evidence towards the existence of a statistical price to pay for computational efficiency. We measure the performance of a test by the smallest signal strength that it can detect and we propose a computationally efficient method based on semidefinite programming. We also prove that the statistical performance of this test cannot be strictly improved by any computationally efficient method. Our results can be viewed as complexity theoretic lower bounds conditionally on the assumptions that some instances of the planted clique problem cannot be solved in randomized polynomial time. version:2
arxiv-1304-7184 | Reading Ancient Coin Legends: Object Recognition vs. OCR | http://arxiv.org/abs/1304.7184 | id:1304.7184 author:Albert Kavelar, Sebastian Zambanini, Martin Kampel category:cs.CV  published:2013-04-26 summary:Standard OCR is a well-researched topic of computer vision and can be considered solved for machine-printed text. However, when applied to unconstrained images, the recognition rates drop drastically. Therefore, the employment of object recognition-based techniques has become state of the art in scene text recognition applications. This paper presents a scene text recognition method tailored to ancient coin legends and compares the results achieved in character and word recognition experiments to a standard OCR engine. The conducted experiments show that the proposed method outperforms the standard OCR engine on a set of 180 cropped coin legend words. version:1
arxiv-1304-7158 | Irreflexive and Hierarchical Relations as Translations | http://arxiv.org/abs/1304.7158 | id:1304.7158 author:Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko category:cs.LG  published:2013-04-26 summary:We consider the problem of embedding entities and relations of knowledge bases in low-dimensional vector spaces. Unlike most existing approaches, which are primarily efficient for modeling equivalence relations, our approach is designed to explicitly model irreflexive relations, such as hierarchies, by interpreting them as translations operating on the low-dimensional embeddings of the entities. Preliminary experiments show that, despite its simplicity and a smaller number of parameters than previous approaches, our approach achieves state-of-the-art performance according to standard evaluation protocols on data from WordNet and Freebase. version:1
arxiv-1304-7157 | Question Answering Against Very-Large Text Collections | http://arxiv.org/abs/1304.7157 | id:1304.7157 author:Leon Derczynski, Richard Shaw, Ben Solway, Jun Wang category:cs.CL cs.IR  published:2013-04-26 summary:Question answering involves developing methods to extract useful information from large collections of documents. This is done with specialised search engines such as Answer Finder. The aim of Answer Finder is to provide an answer to a question rather than a page listing related documents that may contain the correct answer. So, a question such as "How tall is the Eiffel Tower" would simply return "325m" or "1,063ft". Our task was to build on the current version of Answer Finder by improving information retrieval, and also improving the pre-processing involved in question series analysis. version:1
arxiv-1304-7153 | A Convex Approach for Image Hallucination | http://arxiv.org/abs/1304.7153 | id:1304.7153 author:Peter Innerhofer, Thomas Pock category:cs.CV  published:2013-04-26 summary:In this paper we propose a global convex approach for image hallucination. Altering the idea of classical multi image super resolution (SU) systems to single image SU, we incorporate aligned images to hallucinate the output. Our work is based on the paper of Tappen et al. where they use a non-convex model for image hallucination. In comparison we formulate a convex primal optimization problem and derive a fast converging primal-dual algorithm with a global optimal solution. We use a database with face images to incorporate high-frequency details to the high-resolution output. We show that we can achieve state-of-the-art results by using a convex approach. version:1
arxiv-1301-2811 | Cutting Recursive Autoencoder Trees | http://arxiv.org/abs/1301.2811 | id:1301.2811 author:Christian Scheible, Hinrich Schuetze category:cs.CL cs.AI  published:2013-01-13 summary:Deep Learning models enjoy considerable success in Natural Language Processing. While deep architectures produce useful representations that lead to improvements in various tasks, they are often difficult to interpret. This makes the analysis of learned structures particularly difficult. In this paper, we rely on empirical tests to see whether a particular structure makes sense. We present an analysis of the Semi-Supervised Recursive Autoencoder, a well-known model that produces structural representations of text. We show that for certain tasks, the structure of the autoencoder can be significantly reduced without loss of classification accuracy and we evaluate the produced structures using human judgment. version:3
arxiv-1304-7140 | Pulmonary Vascular Tree Segmentation from Contrast-Enhanced CT Images | http://arxiv.org/abs/1304.7140 | id:1304.7140 author:M. Helmberger, M. Urschler, M. Pienn, Z. Balint, A. Olschewski, H. Bischof category:cs.CV physics.med-ph  published:2013-04-26 summary:We present a pulmonary vessel segmentation algorithm, which is fast, fully automatic and robust. It uses a coarse segmentation of the airway tree and a left and right lung labeled volume to restrict a vessel enhancement filter, based on an offset medialness function, to the lungs. We show the application of our algorithm on contrast-enhanced CT images, where we derive a clinical parameter to detect pulmonary hypertension (PH) in patients. Results on a dataset of 24 patients show that quantitative indices derived from the segmentation are applicable to distinguish patients with and without PH. Further work-in-progress results are shown on the VESSEL12 challenge dataset, which is composed of non-contrast-enhanced scans, where we range in the midfield of participating contestants. version:1
arxiv-1304-5063 | Combinaison d'information visuelle, conceptuelle, et contextuelle pour la construction automatique de hierarchies semantiques adaptees a l'annotation d'images | http://arxiv.org/abs/1304.5063 | id:1304.5063 author:Hichem Bannour, Céline Hudelot category:cs.CV cs.LG cs.MM 68T45 I.4.10  published:2013-04-18 summary:This paper proposes a new methodology to automatically build semantic hierarchies suitable for image annotation and classification. The building of the hierarchy is based on a new measure of semantic similarity. The proposed measure incorporates several sources of information: visual, conceptual and contextual as we defined in this paper. The aim is to provide a measure that best represents image semantics. We then propose rules based on this measure, for the building of the final hierarchy, and which explicitly encode hierarchical relationships between different concepts. Therefore, the built hierarchy is used in a semantic hierarchical classification framework for image annotation. Our experiments and results show that the hierarchy built improves classification results. Ce papier propose une nouvelle methode pour la construction automatique de hierarchies semantiques adaptees a la classification et a l'annotation d'images. La construction de la hierarchie est basee sur une nouvelle mesure de similarite semantique qui integre plusieurs sources d'informations: visuelle, conceptuelle et contextuelle que nous definissons dans ce papier. L'objectif est de fournir une mesure qui est plus proche de la semantique des images. Nous proposons ensuite des regles, basees sur cette mesure, pour la construction de la hierarchie finale qui encode explicitement les relations hierarchiques entre les differents concepts. La hierarchie construite est ensuite utilisee dans un cadre de classification semantique hierarchique d'images en concepts visuels. Nos experiences et resultats montrent que la hierarchie construite permet d'ameliorer les resultats de la classification. version:2
arxiv-1304-7132 | Filament and Flare Detection in Hα image sequences | http://arxiv.org/abs/1304.7132 | id:1304.7132 author:Gernot Riegler, Thomas Pock, Werner Pötzi, Astrid Veronig category:cs.CV astro-ph.IM  published:2013-04-26 summary:Solar storms can have a major impact on the infrastructure of the earth. Some of the causing events are observable from ground in the H{\alpha} spectral line. In this paper we propose a new method for the simultaneous detection of flares and filaments in H{\alpha} image sequences. Therefore we perform several preprocessing steps to enhance and normalize the images. Based on the intensity values we segment the image by a variational approach. In a final postprecessing step we derive essential properties to classify the events and further demonstrate the performance by comparing our obtained results to the data annotated by an expert. The information produced by our method can be used for near real-time alerts and the statistical analysis of existing data by solar physicists. version:1
arxiv-1304-7118 | Synthesis of neural networks for spatio-temporal spike pattern recognition and processing | http://arxiv.org/abs/1304.7118 | id:1304.7118 author:J. Tapson, G. Cohen, S. Afshar, K. Stiefel, Y. Buskila, R. Wang, T. J. Hamilton, A. van Schaik category:cs.NE q-bio.NC  published:2013-04-26 summary:The advent of large scale neural computational platforms has highlighted the lack of algorithms for synthesis of neural structures to perform predefined cognitive tasks. The Neural Engineering Framework offers one such synthesis, but it is most effective for a spike rate representation of neural information, and it requires a large number of neurons to implement simple functions. We describe a neural network synthesis method that generates synaptic connectivity for neurons which process time-encoded neural signals, and which makes very sparse use of neurons. The method allows the user to specify, arbitrarily, neuronal characteristics such as axonal and dendritic delays, and synaptic transfer functions, and then solves for the optimal input-output relationship using computed dendritic weights. The method may be used for batch or online learning and has an extremely fast optimization process. We demonstrate its use in generating a network to recognize speech which is sparsely encoded as spike times. version:1
arxiv-1304-6933 | Digit Recognition in Handwritten Weather Records | http://arxiv.org/abs/1304.6933 | id:1304.6933 author:Manuel Keglevic, Robert Sablatnig category:cs.CV  published:2013-04-25 summary:This paper addresses the automatic recognition of handwritten temperature values in weather records. The localization of table cells is based on line detection using projection profiles. Further, a stroke-preserving line removal method which is based on gradient images is proposed. The presented digit recognition utilizes features which are extracted using a set of filters and a Support Vector Machine classifier. It was evaluated on the MNIST and the USPS dataset and our own database with about 17,000 RGB digit images. An accuracy of 99.36% per digit is achieved for the entire system using a set of 84 weather records. version:2
arxiv-1301-6915 | An Impossibility Result for High Dimensional Supervised Learning | http://arxiv.org/abs/1301.6915 | id:1301.6915 author:Mohammad Hossein Rohban, Prakash Ishwar, Birant Orten, William C. Karl, Venkatesh Saligrama category:stat.ML  published:2013-01-29 summary:We study high-dimensional asymptotic performance limits of binary supervised classification problems where the class conditional densities are Gaussian with unknown means and covariances and the number of signal dimensions scales faster than the number of labeled training samples. We show that the Bayes error, namely the minimum attainable error probability with complete distributional knowledge and equally likely classes, can be arbitrarily close to zero and yet the limiting minimax error probability of every supervised learning algorithm is no better than a random coin toss. In contrast to related studies where the classification difficulty (Bayes error) is made to vanish, we hold it constant when taking high-dimensional limits. In contrast to VC-dimension based minimax lower bounds that consider the worst case error probability over all distributions that have a fixed Bayes error, our worst case is over the family of Gaussian distributions with constant Bayes error. We also show that a nontrivial asymptotic minimax error probability can only be attained for parametric subsets of zero measure (in a suitable measure space). These results expose the fundamental importance of prior knowledge and suggest that unless we impose strong structural constraints, such as sparsity, on the parametric space, supervised learning may be ineffective in high dimensional small sample settings. version:2
arxiv-1304-6990 | Euclidean Upgrade from a Minimal Number of Segments | http://arxiv.org/abs/1304.6990 | id:1304.6990 author:Tanja Schilling, Tomas Pajdla category:cs.CV  published:2013-04-25 summary:In this paper, we propose an algebraic approach to upgrade a projective reconstruction to a Euclidean one, and aim at computing the rectifying homography from a minimal number of 9 segments of known length. Constraints are derived from these segments which yield a set of polynomial equations that we solve by means of Gr\"obner bases. We explain how a solver for such a system of equations can be constructed from simplified template data. Moreover, we present experiments that demonstrate that the given problem can be solved in this way. version:1
arxiv-1205-5920 | On latent position inference from doubly stochastic messaging activities | http://arxiv.org/abs/1205.5920 | id:1205.5920 author:Nam H. Lee, Jordan Yoder, Minh Tang, Carey E Priebe category:stat.ML math.ST stat.ME stat.TH 62M0  60G35  60G55  published:2012-05-26 summary:We model messaging activities as a hierarchical doubly stochastic point process with three main levels, and develop an iterative algorithm for inferring actors' relative latent positions from a stream of messaging activity data. Each of the message-exchanging actors is modeled as a process in a latent space. The actors' latent positions are assumed to be influenced by the distribution of a much larger population over the latent space. Each actor's movement in the latent space is modeled as being governed by two parameters that we call confidence and visibility, in addition to dependence on the population distribution. The messaging frequency between a pair of actors is assumed to be inversely proportional to the distance between their latent positions. Our inference algorithm is based on a projection approach to an online filtering problem. The algorithm associates each actor with a probability density-valued process, and each probability density is assumed to be a mixture of basis functions. For efficient numerical experiments, we further develop our algorithm for the case where the basis functions are obtained by translating and scaling a standard Gaussian density. version:5
arxiv-1301-2840 | Unsupervised Feature Learning for low-level Local Image Descriptors | http://arxiv.org/abs/1301.2840 | id:1301.2840 author:Christian Osendorfer, Justin Bayer, Sebastian Urban, Patrick van der Smagt category:cs.CV cs.LG stat.ML  published:2013-01-14 summary:Unsupervised feature learning has shown impressive results for a wide range of input modalities, in particular for object classification tasks in computer vision. Using a large amount of unlabeled data, unsupervised feature learning methods are utilized to construct high-level representations that are discriminative enough for subsequently trained supervised classification algorithms. However, it has never been \emph{quantitatively} investigated yet how well unsupervised learning methods can find \emph{low-level representations} for image patches without any additional supervision. In this paper we examine the performance of pure unsupervised methods on a low-level correspondence task, a problem that is central to many Computer Vision applications. We find that a special type of Restricted Boltzmann Machines (RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple binarization scheme produces compact representations that perform better than several state-of-the-art descriptors. version:4
arxiv-1304-6899 | An implementation of the relational k-means algorithm | http://arxiv.org/abs/1304.6899 | id:1304.6899 author:Balázs Szalkai category:cs.LG cs.CV cs.MS  published:2013-04-25 summary:A C# implementation of a generalized k-means variant called relational k-means is described here. Relational k-means is a generalization of the well-known k-means clustering method which works for non-Euclidean scenarios as well. The input is an arbitrary distance matrix, as opposed to the traditional k-means method, where the clustered objects need to be identified with vectors. version:1
arxiv-1304-5575 | Inverse Density as an Inverse Problem: The Fredholm Equation Approach | http://arxiv.org/abs/1304.5575 | id:1304.5575 author:Qichao Que, Mikhail Belkin category:cs.LG stat.ML  published:2013-04-20 summary:In this paper we address the problem of estimating the ratio $\frac{q}{p}$ where $p$ is a density function and $q$ is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as {\it importance sampling} in statistical inference and is also closely related to the problem of {\it covariate shift} in transfer learning as well as to various MCMC methods. It may also be useful for separating the underlying geometry of a space, say a manifold, from the density function defined on it. Our approach is based on reformulating the problem of estimating $\frac{q}{p}$ as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on $\R^d$, compact domains in $\R^d$ and smooth $d$-dimensional sub-manifolds of the Euclidean space. We also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons. We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner. version:2
arxiv-1304-7282 | An Improved Approach for Word Ambiguity Removal | http://arxiv.org/abs/1304.7282 | id:1304.7282 author:Priti Saktel, Urmila Shrawankar category:cs.CL  published:2013-04-25 summary:Word ambiguity removal is a task of removing ambiguity from a word, i.e. correct sense of word is identified from ambiguous sentences. This paper describes a model that uses Part of Speech tagger and three categories for word sense disambiguation (WSD). Human Computer Interaction is very needful to improve interactions between users and computers. For this, the Supervised and Unsupervised methods are combined. The WSD algorithm is used to find the efficient and accurate sense of a word based on domain information. The accuracy of this work is evaluated with the aim of finding best suitable domain of word. version:1
arxiv-1304-6663 | Low-rank optimization for distance matrix completion | http://arxiv.org/abs/1304.6663 | id:1304.6663 author:B. Mishra, G. Meyer, R. Sepulchre category:math.OC cs.LG stat.ML  published:2013-04-24 summary:This paper addresses the problem of low-rank distance matrix completion. This problem amounts to recover the missing entries of a distance matrix when the dimension of the data embedding space is possibly unknown but small compared to the number of considered data points. The focus is on high-dimensional problems. We recast the considered problem into an optimization problem over the set of low-rank positive semidefinite matrices and propose two efficient algorithms for low-rank distance matrix completion. In addition, we propose a strategy to determine the dimension of the embedding space. The resulting algorithms scale to high-dimensional problems and monotonically converge to a global solution of the problem. Finally, numerical experiments illustrate the good performance of the proposed algorithms on benchmarks. version:2
arxiv-1304-6810 | Inference and learning in probabilistic logic programs using weighted Boolean formulas | http://arxiv.org/abs/1304.6810 | id:1304.6810 author:Daan Fierens, Guy Van den Broeck, Joris Renkens, Dimitar Shterionov, Bernd Gutmann, Ingo Thon, Gerda Janssens, Luc De Raedt category:cs.AI cs.LG cs.LO  published:2013-04-25 summary:Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. This paper investigates how classical inference and learning tasks known from the graphical model community can be tackled for probabilistic logic programs. Several such tasks such as computing the marginals given evidence and learning from (partial) interpretations have not really been addressed for probabilistic logic programs before. The first contribution of this paper is a suite of efficient algorithms for various inference tasks. It is based on a conversion of the program and the queries and evidence to a weighted Boolean formula. This allows us to reduce the inference tasks to well-studied tasks such as weighted model counting, which can be solved using state-of-the-art methods known from the graphical model and knowledge compilation literature. The second contribution is an algorithm for parameter estimation in the learning from interpretations setting. The algorithm employs Expectation Maximization, and is built on top of the developed inference algorithms. The proposed approach is experimentally evaluated. The results show that the inference algorithms improve upon the state-of-the-art in probabilistic logic programming and that it is indeed possible to learn the parameters of a probabilistic logic program from interpretations. version:1
arxiv-1304-6759 | k-Modulus Method for Image Transformation | http://arxiv.org/abs/1304.6759 | id:1304.6759 author:Firas A. Jassim category:cs.CV  published:2013-04-24 summary:In this paper, we propose a new algorithm to make a novel spatial image transformation. The proposed approach aims to reduce the bit depth used for image storage. The basic technique for the proposed transformation is based of the modulus operator. The goal is to transform the whole image into multiples of predefined integer. The division of the whole image by that integer will guarantee that the new image surely less in size from the original image. The k-Modulus Method could not be used as a stand alone transform for image compression because of its high compression ratio. It could be used as a scheme embedded in other image processing fields especially compression. According to its high PSNR value, it could be amalgamated with other methods to facilitate the redundancy criterion. version:1
arxiv-1304-6655 | Comparison of several reweighted l1-algorithms for solving cardinality minimization problems | http://arxiv.org/abs/1304.6655 | id:1304.6655 author:Mohammad Javad Abdi category:math.OC stat.ML  published:2013-04-24 summary:Reweighted l1-algorithms have attracted a lot of attention in the field of applied mathematics. A unified framework of such algorithms has been recently proposed by Zhao and Li. In this paper we construct a few new examples of reweighted l1-methods. These functions are certain concave approximations of the l0-norm function. We focus on the numerical comparison between some new and existing reweighted l1-algorithms. We show how the change of parameters in reweighted algorithms may affect the performance of the algorithms for finding the solution of the cardinality minimization problem. In our experiments, the problem data were generated according to different statistical distributions, and we test the algorithms on different sparsity level of the solution of the problem. Our numerical results demonstrate that the reweighted l1-method is one of the efficient methods for locating the solution of the cardinality minimization problem. version:1
arxiv-1209-0430 | Fixed-rank matrix factorizations and Riemannian low-rank optimization | http://arxiv.org/abs/1209.0430 | id:1209.0430 author:B. Mishra, G. Meyer, S. Bonnabel, R. Sepulchre category:cs.LG math.OC  published:2012-09-03 summary:Motivated by the problem of learning a linear regression model whose parameter is a large fixed-rank non-symmetric matrix, we consider the optimization of a smooth cost function defined on the set of fixed-rank matrices. We adopt the geometric framework of optimization on Riemannian quotient manifolds. We study the underlying geometries of several well-known fixed-rank matrix factorizations and then exploit the Riemannian quotient geometry of the search space in the design of a class of gradient descent and trust-region algorithms. The proposed algorithms generalize our previous results on fixed-rank symmetric positive semidefinite matrices, apply to a broad range of applications, scale to high-dimensional problems and confer a geometric basis to recent contributions on the learning of fixed-rank non-symmetric matrices. We make connections with existing algorithms in the context of low-rank matrix completion and discuss relative usefulness of the proposed framework. Numerical experiments suggest that the proposed algorithms compete with the state-of-the-art and that manifold optimization offers an effective and versatile framework for the design of machine learning algorithms that learn a fixed-rank matrix. version:2
arxiv-1304-6487 | Locally linear representation for subspace learning and clustering | http://arxiv.org/abs/1304.6487 | id:1304.6487 author:Liangli Zhen, Zhang Yi, Xi Peng, Dezhong Peng category:cs.LG stat.ML  published:2013-04-24 summary:It is a key to construct a similarity graph in graph-oriented subspace learning and clustering. In a similarity graph, each vertex denotes a data point and the edge weight represents the similarity between two points. There are two popular schemes to construct a similarity graph, i.e., pairwise distance based scheme and linear representation based scheme. Most existing works have only involved one of the above schemes and suffered from some limitations. Specifically, pairwise distance based methods are sensitive to the noises and outliers compared with linear representation based methods. On the other hand, there is the possibility that linear representation based algorithms wrongly select inter-subspaces points to represent a point, which will degrade the performance. In this paper, we propose an algorithm, called Locally Linear Representation (LLR), which integrates pairwise distance with linear representation together to address the problems. The proposed algorithm can automatically encode each data point over a set of points that not only could denote the objective point with less residual error, but also are close to the point in Euclidean space. The experimental results show that our approach is promising in subspace learning and subspace clustering. version:1
arxiv-1304-6480 | A Theoretical Analysis of NDCG Type Ranking Measures | http://arxiv.org/abs/1304.6480 | id:1304.6480 author:Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen category:cs.LG cs.IR stat.ML  published:2013-04-24 summary:A central problem in ranking is to design a ranking measure for evaluation of ranking functions. In this paper we study, from a theoretical perspective, the widely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures. Although there are extensive empirical studies of NDCG, little is known about its theoretical properties. We first show that, whatever the ranking function is, the standard NDCG which adopts a logarithmic discount, converges to 1 as the number of items to rank goes to infinity. On the first sight, this result is very surprising. It seems to imply that NDCG cannot differentiate good and bad ranking functions, contradicting to the empirical success of NDCG in many applications. In order to have a deeper understanding of ranking measures in general, we propose a notion referred to as consistent distinguishability. This notion captures the intuition that a ranking measure should have such a property: For every pair of substantially different ranking functions, the ranking measure can decide which one is better in a consistent manner on almost all datasets. We show that NDCG with logarithmic discount has consistent distinguishability although it converges to the same limit for all ranking functions. We next characterize the set of all feasible discount functions for NDCG according to the concept of consistent distinguishability. Specifically we show that whether NDCG has consistent distinguishability depends on how fast the discount decays, and 1/r is a critical point. We then turn to the cut-off version of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for various choices of k and the discount functions. Experimental results on real Web search datasets agree well with the theory. version:1
arxiv-1304-6478 | The K-modes algorithm for clustering | http://arxiv.org/abs/1304.6478 | id:1304.6478 author:Miguel Á. Carreira-Perpiñán, Weiran Wang category:cs.LG stat.ME stat.ML  published:2013-04-24 summary:Many clustering algorithms exist that estimate a cluster centroid, such as K-means, K-medoids or mean-shift, but no algorithm seems to exist that clusters data by returning exactly K meaningful modes. We propose a natural definition of a K-modes objective function by combining the notions of density and cluster assignment. The algorithm becomes K-means and K-medoids in the limit of very large and very small scales. Computationally, it is slightly slower than K-means but much faster than mean-shift or K-medoids. Unlike K-means, it is able to find centroids that are valid patterns, truly representative of a cluster, even with nonconvex clusters, and appears robust to outliers and misspecification of the scale and number of clusters. version:1
arxiv-1209-1171 | Solving Support Vector Machines in Reproducing Kernel Banach Spaces with Positive Definite Functions | http://arxiv.org/abs/1209.1171 | id:1209.1171 author:Gregory E. Fasshauer, Fred J. Hickernell, Qi Ye category:stat.ML math.NA math.OC 46E22  46E15  62K05  published:2012-09-06 summary:In this paper we solve support vector machines in reproducing kernel Banach spaces with reproducing kernels defined on nonsymmetric domains instead of the traditional methods in reproducing kernel Hilbert spaces. Using the orthogonality of semi-inner-products, we can obtain the explicit representations of the dual (normalized-duality-mapping) elements of support vector machine solutions. In addition, we can introduce the reproduction property in a generalized native space by Fourier transform techniques such that it becomes a reproducing kernel Banach space, which can be even embedded into Sobolev spaces, and its reproducing kernel is set up by the related positive definite function. The representations of the optimal solutions of support vector machines (regularized empirical risks) in these reproducing kernel Banach spaces are formulated explicitly in terms of positive definite functions, and their finite numbers of coefficients can be computed by fixed point iteration. We also give some typical examples of reproducing kernel Banach spaces induced by Mat\'ern functions (Sobolev splines) so that their support vector machine solutions are well computable as the classical algorithms. Moreover, each of their reproducing bases includes information from multiple training data points. The concept of reproducing kernel Banach spaces offers us a new numerical tool for solving support vector machines. version:3
arxiv-1304-8016 | On Semantic Word Cloud Representation | http://arxiv.org/abs/1304.8016 | id:1304.8016 author:Lukas Barth, Stephen Kobourov, Sergey Pupyrev, Torsten Ueckerdt category:cs.DS cs.CL  published:2013-04-23 summary:We study the problem of computing semantic-preserving word clouds in which semantically related words are close to each other. While several heuristic approaches have been described in the literature, we formalize the underlying geometric algorithm problem: Word Rectangle Adjacency Contact (WRAC). In this model each word is associated with rectangle with fixed dimensions, and the goal is to represent semantically related words by ensuring that the two corresponding rectangles touch. We design and analyze efficient polynomial-time algorithms for some variants of the WRAC problem, show that several general variants are NP-hard, and describe a number of approximation algorithms. Finally, we experimentally demonstrate that our theoretically-sound algorithms outperform the early heuristics. version:1
arxiv-1304-6379 | Semi-Optimal Edge Detector based on Simple Standard Deviation with Adjusted Thresholding | http://arxiv.org/abs/1304.6379 | id:1304.6379 author:Firas A. Jassim category:cs.CV  published:2013-04-23 summary:This paper proposes a novel method which combines both median filter and simple standard deviation to accomplish an excellent edge detector for image processing. First of all, a denoising process must be applied on the grey scale image using median filter to identify pixels which are likely to be contaminated by noise. The benefit of this step is to smooth the image and get rid of the noisy pixels. After that, the simple statistical standard deviation could be computed for each 2X2 window size. If the value of the standard deviation inside the 2X2 window size is greater than a predefined threshold, then the upper left pixel in the 2?2 window represents an edge. The visual differences between the proposed edge detector and the standard known edge detectors have been shown to support the contribution in this paper. version:1
arxiv-1212-1617 | Similarity of Polygonal Curves in the Presence of Outliers | http://arxiv.org/abs/1212.1617 | id:1212.1617 author:Jean-Lou De Carufel, Amin Gheibi, Anil Maheshwari, Jörg-Rüdiger Sack, Christian Scheffer category:cs.CG cs.CV cs.GR  published:2012-12-07 summary:The Fr\'{e}chet distance is a well studied and commonly used measure to capture the similarity of polygonal curves. Unfortunately, it exhibits a high sensitivity to the presence of outliers. Since the presence of outliers is a frequently occurring phenomenon in practice, a robust variant of Fr\'{e}chet distance is required which absorbs outliers. We study such a variant here. In this modified variant, our objective is to minimize the length of subcurves of two polygonal curves that need to be ignored (MinEx problem), or alternately, maximize the length of subcurves that are preserved (MaxIn problem), to achieve a given Fr\'{e}chet distance. An exact solution to one problem would imply an exact solution to the other problem. However, we show that these problems are not solvable by radicals over $\mathbb{Q}$ and that the degree of the polynomial equations involved is unbounded in general. This motivates the search for approximate solutions. We present an algorithm, which approximates, for a given input parameter $\delta$, optimal solutions for the \MinEx\ and \MaxIn\ problems up to an additive approximation error $\delta$ times the length of the input curves. The resulting running time is upper bounded by $\mathcal{O} \left(\frac{n^3}{\delta} \log \left(\frac{n}{\delta} \right)\right)$, where $n$ is the complexity of the input polygonal curves. version:2
arxiv-1304-6291 | Learning Visual Symbols for Parsing Human Poses in Images | http://arxiv.org/abs/1304.6291 | id:1304.6291 author:Fang Wang, Yi Li category:cs.CV  published:2013-04-23 summary:Parsing human poses in images is fundamental in extracting critical visual information for artificial intelligent agents. Our goal is to learn self-contained body part representations from images, which we call visual symbols, and their symbol-wise geometric contexts in this parsing process. Each symbol is individually learned by categorizing visual features leveraged by geometric information. In the categorization, we use Latent Support Vector Machine followed by an efficient cross validation procedure to learn visual symbols. Then, these symbols naturally define geometric contexts of body parts in a fine granularity. When the structure of the compositional parts is a tree, we derive an efficient approach to estimating human poses in images. Experiments on two large datasets suggest our approach outperforms state of the art methods. version:1
arxiv-1104-0896 | On Identifying Significant Edges in Graphical Models of Molecular Networks | http://arxiv.org/abs/1104.0896 | id:1104.0896 author:Marco Scutari, Radhakrishnan Nagarajan category:stat.ML stat.ME  published:2011-04-05 summary:Objective: Modelling the associations from high-throughput experimental molecular data has provided unprecedented insights into biological pathways and signalling mechanisms. Graphical models and networks have especially proven to be useful abstractions in this regard. Ad-hoc thresholds are often used in conjunction with structure learning algorithms to determine significant associations. The present study overcomes this limitation by proposing a statistically-motivated approach for identifying significant associations in a network. Methods and Materials: A new method that identifies significant associations in graphical models by estimating the threshold minimising the $L_{\mathrm{1}}$ norm between the cumulative distribution function (CDF) of the observed edge confidences and those of its asymptotic counterpart is proposed. The effectiveness of the proposed method is demonstrated on popular synthetic data sets as well as publicly available experimental molecular data corresponding to gene and protein expression profiles. Results: The improved performance of the proposed approach is demonstrated across the synthetic data sets using sensitivity, specificity and accuracy as performance metrics. The results are also demonstrated across varying sample sizes and three different structure learning algorithms with widely varying assumptions. In all cases, the proposed approach has specificity and accuracy close to 1, while sensitivity increases linearly in the logarithm of the sample size. The estimated threshold systematically outperforms common ad-hoc ones in terms of sensitivity while maintaining comparable levels of specificity and accuracy. Networks from experimental data sets are reconstructed accurately with respect to the results from the original papers. version:5
arxiv-1304-6213 | Counting people from above: Airborne video based crowd analysis | http://arxiv.org/abs/1304.6213 | id:1304.6213 author:Roland Perko, Thomas Schnabel, Gerald Fritz, Alexander Almer, Lucas Paletta category:cs.CV  published:2013-04-23 summary:Crowd monitoring and analysis in mass events are highly important technologies to support the security of attending persons. Proposed methods based on terrestrial or airborne image/video data often fail in achieving sufficiently accurate results to guarantee a robust service. We present a novel framework for estimating human count, density and motion from video data based on custom tailored object detection techniques, a regression based density estimate and a total variation based optical flow extraction. From the gathered features we present a detailed accuracy analysis versus ground truth measurements. In addition, all information is projected into world coordinates to enable a direct integration with existing geo-information systems. The resulting human counts demonstrate a mean error of 4% to 9% and thus represent a most efficient measure that can be robustly applied in security critical services. version:1
arxiv-1304-5594 | Dew Point modelling using GEP based multi objective optimization | http://arxiv.org/abs/1304.5594 | id:1304.5594 author:Siddharth Shroff, Vipul Dabhi category:cs.NE  published:2013-04-20 summary:Different techniques are used to model the relationship between temperatures, dew point and relative humidity. Gene expression programming is capable of modelling complex realities with great accuracy, allowing at the same time, the extraction of knowledge from the evolved models compared to other learning algorithms. We aim to use Gene Expression Programming for modelling of dew point. Generally, accuracy of the model is the only objective used by selection mechanism of GEP. This will evolve large size models with low training error. To avoid this situation, use of multiple objectives, like accuracy and size of the model are preferred by Genetic Programming practitioners. Solution to a multi-objective problem is a set of solutions which satisfies the objectives given by decision maker. Multi objective based GEP will be used to evolve simple models. Various algorithms widely used for multi objective optimization, like NSGA II and SPEA 2, are tested on different test problems. The results obtained thereafter gives idea that SPEA 2 is better than NSGA II based on the features like execution time, number of solutions obtained and convergence rate. We selected SPEA 2 for dew point prediction. The multi-objective base GEP produces accurate and simpler (smaller) solutions compared to solutions produced by plain GEP for dew point predictions. Thus multi objective base GEP produces better solutions by considering the dual objectives of fitness and size of the solution. These simple models can be used to predict future values of dew point. version:2
arxiv-1304-5894 | Bayesian crack detection in ultra high resolution multimodal images of paintings | http://arxiv.org/abs/1304.5894 | id:1304.5894 author:Bruno Cornelis, Yun Yang, Joshua T. Vogelstein, Ann Dooms, Ingrid Daubechies, David Dunson category:cs.CV cs.LG  published:2013-04-22 summary:The preservation of our cultural heritage is of paramount importance. Thanks to recent developments in digital acquisition techniques, powerful image analysis algorithms are developed which can be useful non-invasive tools to assist in the restoration and preservation of art. In this paper we propose a semi-supervised crack detection method that can be used for high-dimensional acquisitions of paintings coming from different modalities. Our dataset consists of a recently acquired collection of images of the Ghent Altarpiece (1432), one of Northern Europe's most important art masterpieces. Our goal is to build a classifier that is able to discern crack pixels from the background consisting of non-crack pixels, making optimal use of the information that is provided by each modality. To accomplish this we employ a recently developed non-parametric Bayesian classifier, that uses tensor factorizations to characterize any conditional probability. A prior is placed on the parameters of the factorization such that every possible interaction between predictors is allowed while still identifying a sparse subset among these predictors. The proposed Bayesian classifier, which we will refer to as conditional Bayesian tensor factorization or CBTF, is assessed by visually comparing classification results with the Random Forest (RF) algorithm. version:2
arxiv-1301-3461 | Factorized Topic Models | http://arxiv.org/abs/1301.3461 | id:1301.3461 author:Cheng Zhang, Carl Henrik Ek, Andreas Damianou, Hedvig Kjellstrom category:cs.LG cs.CV cs.IR  published:2013-01-15 summary:In this paper we present a modification to a latent topic model, which makes the model exploit supervision to produce a factorized representation of the observed data. The structured parameterization separately encodes variance that is shared between classes from variance that is private to each class by the introduction of a new prior over the topic space. The approach allows for a more eff{}icient inference and provides an intuitive interpretation of the data in terms of an informative signal together with structured noise. The factorized representation is shown to enhance inference performance for image, text, and video classification. version:7
arxiv-1304-6192 | A Bag of Visual Words Approach for Symbols-Based Coarse-Grained Ancient Coin Classification | http://arxiv.org/abs/1304.6192 | id:1304.6192 author:Hafeez Anwar, Sebastian Zambanini, Martin Kampel category:cs.CV  published:2013-04-23 summary:The field of Numismatics provides the names and descriptions of the symbols minted on the ancient coins. Classification of the ancient coins aims at assigning a given coin to its issuer. Various issuers used various symbols for their coins. We propose to use these symbols for a framework that will coarsely classify the ancient coins. Bag of visual words (BoVWs) is a well established visual recognition technique applied to various problems in computer vision like object and scene recognition. Improvements have been made by incorporating the spatial information to this technique. We apply the BoVWs technique to our problem and use three symbols for coarse-grained classification. We use rectangular tiling, log-polar tiling and circular tiling to incorporate spatial information to BoVWs. Experimental results show that the circular tiling proves superior to the rest of the methods for our problem. version:1
arxiv-1304-6108 | The varifold representation of non-oriented shapes for diffeomorphic registration | http://arxiv.org/abs/1304.6108 | id:1304.6108 author:Nicolas Charon, Alain Trouvé category:cs.CG cs.CV math.DG  published:2013-04-22 summary:In this paper, we address the problem of orientation that naturally arises when representing shapes like curves or surfaces as currents. In the field of computational anatomy, the framework of currents has indeed proved very efficient to model a wide variety of shapes. However, in such approaches, orientation of shapes is a fundamental issue that can lead to several drawbacks in treating certain kind of datasets. More specifically, problems occur with structures like acute pikes because of canceling effects of currents or with data that consists in many disconnected pieces like fiber bundles for which currents require a consistent orientation of all pieces. As a promising alternative to currents, varifolds, introduced in the context of geometric measure theory by F. Almgren, allow the representation of any non-oriented manifold (more generally any non-oriented rectifiable set). In particular, we explain how varifolds can encode numerically non-oriented objects both from the discrete and continuous point of view. We show various ways to build a Hilbert space structure on the set of varifolds based on the theory of reproducing kernels. We show that, unlike the currents' setting, these metrics are consistent with shape volume (theorem 4.1) and we derive a formula for the variation of metric with respect to the shape (theorem 4.2). Finally, we propose a generalization to non-oriented shapes of registration algorithms in the context of Large Deformations Metric Mapping (LDDMM), which we detail with a few examples in the last part of the paper. version:1
arxiv-1206-7051 | Stochastic Variational Inference | http://arxiv.org/abs/1206.7051 | id:1206.7051 author:Matt Hoffman, David M. Blei, Chong Wang, John Paisley category:stat.ML cs.AI stat.CO stat.ME  published:2012-06-29 summary:We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets. version:3
arxiv-1210-2771 | Cost-Sensitive Tree of Classifiers | http://arxiv.org/abs/1210.2771 | id:1210.2771 author:Zhixiang Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen category:stat.ML cs.LG  published:2012-10-09 summary:Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test time must be budgeted and accounted for. In this paper, we address the challenge of balancing the test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across eatures. We decrease this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost. version:3
arxiv-1304-5974 | Dynamic stochastic blockmodels: Statistical models for time-evolving networks | http://arxiv.org/abs/1304.5974 | id:1304.5974 author:Kevin S. Xu, Alfred O. Hero III category:cs.SI cs.LG physics.soc-ph stat.ME G.3; G.2.2  published:2013-04-22 summary:Significant efforts have gone into the development of statistical models for analyzing data in the form of networks, such as social networks. Most existing work has focused on modeling static networks, which represent either a single time snapshot or an aggregate view over time. There has been recent interest in statistical modeling of dynamic networks, which are observed at multiple points in time and offer a richer representation of many complex phenomena. In this paper, we propose a state-space model for dynamic networks that extends the well-known stochastic blockmodel for static networks to the dynamic setting. We then propose a procedure to fit the model using a modification of the extended Kalman filter augmented with a local search. We apply the procedure to analyze a dynamic social network of email communication. version:1
arxiv-1203-3887 | Learning loopy graphical models with latent variables: Efficient methods and guarantees | http://arxiv.org/abs/1203.3887 | id:1203.3887 author:Animashree Anandkumar, Ragupathyraj Valluvan category:stat.ML cs.AI cs.LG math.ST stat.TH  published:2012-03-17 summary:The problem of structure estimation in graphical models with latent variables is considered. We characterize conditions for tractable graph estimation and develop efficient methods with provable guarantees. We consider models where the underlying Markov graph is locally tree-like, and the model is in the regime of correlation decay. For the special case of the Ising model, the number of samples $n$ required for structural consistency of our method scales as $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is the number of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ is the depth (i.e., distance from a hidden node to the nearest observed nodes), and $\eta$ is a parameter which depends on the bounds on node and edge potentials in the Ising model. Necessary conditions for structural consistency under any algorithm are derived and our method nearly matches the lower bound on sample requirements. Further, the proposed method is practical to implement and provides flexibility to control the number of latent variables and the cycle lengths in the output graph. version:4
arxiv-1109-3318 | Distributed User Profiling via Spectral Methods | http://arxiv.org/abs/1109.3318 | id:1109.3318 author:Dan-Cristian Tomozei, Laurent Massoulié category:cs.LG 60B20 G.3  published:2011-09-15 summary:User profiling is a useful primitive for constructing personalised services, such as content recommendation. In the present paper we investigate the feasibility of user profiling in a distributed setting, with no central authority and only local information exchanges between users. We compute a profile vector for each user (i.e., a low-dimensional vector that characterises her taste) via spectral transformation of observed user-produced ratings for items. Our two main contributions follow: i) We consider a low-rank probabilistic model of user taste. More specifically, we consider that users and items are partitioned in a constant number of classes, such that users and items within the same class are statistically identical. We prove that without prior knowledge of the compositions of the classes, based solely on few random observed ratings (namely $O(N\log N)$ such ratings for $N$ users), we can predict user preference with high probability for unrated items by running a local vote among users with similar profile vectors. In addition, we provide empirical evaluations characterising the way in which spectral profiling performance depends on the dimension of the profile space. Such evaluations are performed on a data set of real user ratings provided by Netflix. ii) We develop distributed algorithms which provably achieve an embedding of users into a low-dimensional space, based on spectral transformation. These involve simple message passing among users, and provably converge to the desired embedding. Our method essentially relies on a novel combination of gossiping and the algorithm proposed by Oja and Karhunen. version:2
arxiv-1304-5880 | Dealing with natural language interfaces in a geolocation context | http://arxiv.org/abs/1304.5880 | id:1304.5880 author:M. -A. Abchir, Isis Truck, Anna Pappa category:cs.CL  published:2013-04-22 summary:In the geolocation field where high-level programs and low-level devices coexist, it is often difficult to find a friendly user inter- face to configure all the parameters. The challenge addressed in this paper is to propose intuitive and simple, thus natural lan- guage interfaces to interact with low-level devices. Such inter- faces contain natural language processing and fuzzy represen- tations of words that facilitate the elicitation of business-level objectives in our context. version:1
arxiv-1208-3030 | Asymptotic Generalization Bound of Fisher's Linear Discriminant Analysis | http://arxiv.org/abs/1208.3030 | id:1208.3030 author:Wei Bian, Dacheng Tao category:stat.ML cs.LG  published:2012-08-15 summary:Fisher's linear discriminant analysis (FLDA) is an important dimension reduction method in statistical pattern recognition. It has been shown that FLDA is asymptotically Bayes optimal under the homoscedastic Gaussian assumption. However, this classical result has the following two major limitations: 1) it holds only for a fixed dimensionality $D$, and thus does not apply when $D$ and the training sample size $N$ are proportionally large; 2) it does not provide a quantitative description on how the generalization ability of FLDA is affected by $D$ and $N$. In this paper, we present an asymptotic generalization analysis of FLDA based on random matrix theory, in a setting where both $D$ and $N$ increase and $D/N\longrightarrow\gamma\in[0,1)$. The obtained lower bound of the generalization discrimination power overcomes both limitations of the classical result, i.e., it is applicable when $D$ and $N$ are proportionally large and provides a quantitative description of the generalization ability of FLDA in terms of the ratio $\gamma=D/N$ and the population discrimination power. Besides, the discrimination power bound also leads to an upper bound on the generalization error of binary-classification with FLDA. version:2
arxiv-1303-7297 | Infinitely imbalanced binomial regression and deformed exponential families | http://arxiv.org/abs/1303.7297 | id:1303.7297 author:Tomonari Sei category:math.ST stat.ML stat.TH 62J12  62H30  62E20  published:2013-03-29 summary:The logistic regression model is known to converge to a Poisson point process model if the binary response tends to infinitely imbalanced. In this paper, it is shown that this phenomenon is universal in a wide class of link functions on binomial regression. The proof relies on the extreme value theory. For the logit, probit and complementary log-log link functions, the intensity measure of the point process becomes an exponential family. For some other link functions, deformed exponential families appear. A penalized maximum likelihood estimator for the Poisson point process model is suggested. version:2
arxiv-1208-3779 | Multiple graph regularized protein domain ranking | http://arxiv.org/abs/1208.3779 | id:1208.3779 author:Jim Jing-Yan Wang, Halima Bensmail, Xin Gao category:cs.LG cs.CE cs.IR q-bio.QM  published:2012-08-18 summary:Background Protein domain ranking is a fundamental task in structural biology. Most protein domain ranking methods rely on the pairwise comparison of protein domains while neglecting the global manifold structure of the protein domain database. Recently, graph regularized ranking that exploits the global structure of the graph defined by the pairwise similarities has been proposed. However, the existing graph regularized ranking methods are very sensitive to the choice of the graph model and parameters, and this remains a difficult problem for most of the protein domain ranking methods. Results To tackle this problem, we have developed the Multiple Graph regularized Ranking algorithm, MultiG- Rank. Instead of using a single graph to regularize the ranking scores, MultiG-Rank approximates the intrinsic manifold of protein domain distribution by combining multiple initial graphs for the regularization. Graph weights are learned with ranking scores jointly and automatically, by alternately minimizing an ob- jective function in an iterative algorithm. Experimental results on a subset of the ASTRAL SCOP protein domain database demonstrate that MultiG-Rank achieves a better ranking performance than single graph regularized ranking methods and pairwise similarity based ranking methods. Conclusion The problem of graph model and parameter selection in graph regularized protein domain ranking can be solved effectively by combining multiple graphs. This aspect of generalization introduces a new frontier in applying multiple graphs to solving protein domain ranking applications. version:3
arxiv-1304-5678 | Analytic Feature Selection for Support Vector Machines | http://arxiv.org/abs/1304.5678 | id:1304.5678 author:Carly Stambaugh, Hui Yang, Felix Breuer category:cs.LG stat.ML I.2.6; I.2.7  published:2013-04-20 summary:Support vector machines (SVMs) rely on the inherent geometry of a data set to classify training data. Because of this, we believe SVMs are an excellent candidate to guide the development of an analytic feature selection algorithm, as opposed to the more commonly used heuristic methods. We propose a filter-based feature selection algorithm based on the inherent geometry of a feature set. Through observation, we identified six geometric properties that differ between optimal and suboptimal feature sets, and have statistically significant correlations to classifier performance. Our algorithm is based on logistic and linear regression models using these six geometric properties as predictor variables. The proposed algorithm achieves excellent results on high dimensional text data sets, with features that can be organized into a handful of feature types; for example, unigrams, bigrams or semantic structural features. We believe this algorithm is a novel and effective approach to solving the feature selection problem for linear SVMs. version:1
arxiv-1210-7665 | Graph Estimation From Multi-attribute Data | http://arxiv.org/abs/1210.7665 | id:1210.7665 author:Mladen Kolar, Han Liu, Eric P. Xing category:stat.ML  published:2012-10-29 summary:Many real world network problems often concern multivariate nodal attributes such as image, textual, and multi-view feature vectors on nodes, rather than simple univariate nodal attributes. The existing graph estimation methods built on Gaussian graphical models and covariance selection algorithms can not handle such data, neither can the theories developed around such methods be directly applied. In this paper, we propose a new principled framework for estimating graphs from multi-attribute data. Instead of estimating the partial correlation as in current literature, our method estimates the partial canonical correlations that naturally accommodate complex nodal features. Computationally, we provide an efficient algorithm which utilizes the multi-attribute structure. Theoretically, we provide sufficient conditions which guarantee consistent graph recovery. Extensive simulation studies demonstrate performance of our method under various conditions. Furthermore, we provide illustrative applications to uncovering gene regulatory networks from gene and protein profiles, and uncovering brain connectivity graph from functional magnetic resonance imaging data. version:2
arxiv-1304-5634 | A Survey on Multi-view Learning | http://arxiv.org/abs/1304.5634 | id:1304.5634 author:Chang Xu, Dacheng Tao, Chao Xu category:cs.LG  published:2013-04-20 summary:In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning. version:1
arxiv-1212-1744 | Computational Capabilities of Random Automata Networks for Reservoir Computing | http://arxiv.org/abs/1212.1744 | id:1212.1744 author:David Snyder, Alireza Goudarzi, Christof Teuscher category:nlin.AO cond-mat.dis-nn cs.NE  published:2012-12-08 summary:This paper underscores the conjecture that intrinsic computation is maximal in systems at the "edge of chaos." We study the relationship between dynamics and computational capability in Random Boolean Networks (RBN) for Reservoir Computing (RC). RC is a computational paradigm in which a trained readout layer interprets the dynamics of an excitable component (called the reservoir) that is perturbed by external input. The reservoir is often implemented as a homogeneous recurrent neural network, but there has been little investigation into the properties of reservoirs that are discrete and heterogeneous. Random Boolean networks are generic and heterogeneous dynamical systems and here we use them as the reservoir. An RBN is typically a closed system; to use it as a reservoir we extend it with an input layer. As a consequence of perturbation, the RBN does not necessarily fall into an attractor. Computational capability in RC arises from a trade-off between separability and fading memory of inputs. We find the balance of these properties predictive of classification power and optimal at critical connectivity. These results are relevant to the construction of devices which exploit the intrinsic dynamics of complex heterogeneous systems, such as biomolecular substrates. version:2
arxiv-1304-4889 | Hands-free Evolution of 3D-printable Objects via Eye Tracking | http://arxiv.org/abs/1304.4889 | id:1304.4889 author:Nick Cheney, Jeff Clune, Jason Yosinski, Hod Lipson category:cs.NE cs.HC  published:2013-04-17 summary:Interactive evolution has shown the potential to create amazing and complex forms in both 2-D and 3-D settings. However, the algorithm is slow and users quickly become fatigued. We propose that the use of eye tracking for interactive evolution systems will both reduce user fatigue and improve evolutionary success. We describe a systematic method for testing the hypothesis that eye tracking driven interactive evolution will be a more successful and easier-to-use design method than traditional interactive evolution methods driven by mouse clicks. We provide preliminary results that support the possibility of this proposal, and lay out future work to investigate these advantages in extensive clinical trials. version:3
arxiv-1304-5457 | Personalized Academic Research Paper Recommendation System | http://arxiv.org/abs/1304.5457 | id:1304.5457 author:Joonseok Lee, Kisung Lee, Jennifer G. Kim category:cs.IR cs.DL cs.LG  published:2013-04-19 summary:A huge number of academic papers are coming out from a lot of conferences and journals these days. In these circumstances, most researchers rely on key-based search or browsing through proceedings of top conferences and journals to find their related work. To ease this difficulty, we propose a Personalized Academic Research Paper Recommendation System, which recommends related articles, for each researcher, that may be interesting to her/him. In this paper, we first introduce our web crawler to retrieve research papers from the web. Then, we define similarity between two research papers based on the text similarity between them. Finally, we propose our recommender system developed using collaborative filtering methods. Our evaluation results demonstrate that our system recommends good quality research papers. version:1
arxiv-1304-5417 | Analytic Expressions for Stochastic Distances Between Relaxed Complex Wishart Distributions | http://arxiv.org/abs/1304.5417 | id:1304.5417 author:Alejandro C. Frery, Abraão D. C. Nascimento, Renato J. Cintra category:stat.ML  published:2013-04-19 summary:The scaled complex Wishart distribution is a widely used model for multilook full polarimetric SAR data whose adequacy has been attested in the literature. Classification, segmentation, and image analysis techniques which depend on this model have been devised, and many of them employ some type of dissimilarity measure. In this paper we derive analytic expressions for four stochastic distances between relaxed scaled complex Wishart distributions in their most general form and in important particular cases. Using these distances, inequalities are obtained which lead to new ways of deriving the Bartlett and revised Wishart distances. The expressiveness of the four analytic distances is assessed with respect to the variation of parameters. Such distances are then used for deriving new tests statistics, which are proved to have asymptotic chi-square distribution. Adopting the test size as a comparison criterion, a sensitivity study is performed by means of Monte Carlo experiments suggesting that the Bhattacharyya statistic outperforms all the others. The power of the tests is also assessed. Applications to actual data illustrate the discrimination and homogeneity identification capabilities of these distances. version:1
arxiv-1304-5319 | A Joint Intensity and Depth Co-Sparse Analysis Model for Depth Map Super-Resolution | http://arxiv.org/abs/1304.5319 | id:1304.5319 author:Martin Kiechle, Simon Hawe, Martin Kleinsteuber category:cs.CV  published:2013-04-19 summary:High-resolution depth maps can be inferred from low-resolution depth measurements and an additional high-resolution intensity image of the same scene. To that end, we introduce a bimodal co-sparse analysis model, which is able to capture the interdependency of registered intensity and depth information. This model is based on the assumption that the co-supports of corresponding bimodal image structures are aligned when computed by a suitable pair of analysis operators. No analytic form of such operators exist and we propose a method for learning them from a set of registered training signals. This learning process is done offline and returns a bimodal analysis operator that is universally applicable to natural scenes. We use this to exploit the bimodal co-sparse analysis model as a prior for solving inverse problems, which leads to an efficient algorithm for depth map super-resolution. version:1
arxiv-1304-5212 | Object Tracking in Videos: Approaches and Issues | http://arxiv.org/abs/1304.5212 | id:1304.5212 author:Duc Phu Chau, François Bremond, Monique Thonnat category:cs.CV  published:2013-04-18 summary:Mobile object tracking has an important role in the computer vision applications. In this paper, we use a tracked target-based taxonomy to present the object tracking algorithms. The tracked targets are divided into three categories: points of interest, appearance and silhouette of mobile objects. Advantages and limitations of the tracking approaches are also analyzed to find the future directions in the object tracking domain. version:1
arxiv-1304-5168 | Image Retrieval based on Bag-of-Words model | http://arxiv.org/abs/1304.5168 | id:1304.5168 author:Jialu Liu category:cs.IR cs.LG  published:2013-04-18 summary:This article gives a survey for bag-of-words (BoW) or bag-of-features model in image retrieval system. In recent years, large-scale image retrieval shows significant potential in both industry applications and research problems. As local descriptors like SIFT demonstrate great discriminative power in solving vision problems like object recognition, image classification and annotation, more and more state-of-the-art large scale image retrieval systems are trying to rely on them. A common way to achieve this is first quantizing local descriptors into visual words, and then applying scalable textual indexing and retrieval schemes. We call this model as bag-of-words or bag-of-features model. The goal of this survey is to give an overview of this model and introduce different strategies when building the system based on this model. version:1
arxiv-1304-4055 | Multiobjective optimization in Gene Expression Programming for Dew Point | http://arxiv.org/abs/1304.4055 | id:1304.4055 author:Siddharth Shroff, Vipul Dabhi category:cs.NE  published:2013-04-15 summary:The processes occurring in climatic change evolution and their variations play a major role in environmental engineering. Different techniques are used to model the relationship between temperatures, dew point and relative humidity. Gene expression programming is capable of modelling complex realities with great accuracy, allowing, at the same time, the extraction of knowledge from the evolved models compared to other learning algorithms. This research aims to use Gene Expression Programming for modelling of dew point. Generally, accuracy of the model is the only objective used by selection mechanism of GEP. This will evolve large size models with low training error. To avoid this situation, use of multiple objectives, like accuracy and size of the model are preferred by Genetic Programming practitioners. Multi-objective problem finds a set of solutions satisfying the objectives given by decision maker. Multiobjective based GEP will be used to evolve simple models. Various algorithms widely used for multi objective optimization like NSGA II and SPEA 2 are tested for different test cases. The results obtained thereafter gives idea that SPEA 2 is better algorithm compared to NSGA II based on the features like execution time, number of solutions obtained and convergence rate. Thus compared to models obtained by GEP, multi-objective algorithms fetch better solutions considering the dual objectives of fitness and size of the equation. These simple models can be used to predict dew point. version:2
arxiv-1211-2315 | Efficient network-guided multi-locus association mapping with graph cuts | http://arxiv.org/abs/1211.2315 | id:1211.2315 author:Chloé-Agathe Azencott, Dominik Grimm, Mahito Sugiyama, Yoshinobu Kawahara, Karsten M. Borgwardt category:stat.ML q-bio.QM  published:2012-11-10 summary:As an increasing number of genome-wide association studies reveal the limitations of attempting to explain phenotypic heritability by single genetic loci, there is growing interest for associating complex phenotypes with sets of genetic loci. While several methods for multi-locus mapping have been proposed, it is often unclear how to relate the detected loci to the growing knowledge about gene pathways and networks. The few methods that take biological pathways or networks into account are either restricted to investigating a limited number of predetermined sets of loci, or do not scale to genome-wide settings. We present SConES, a new efficient method to discover sets of genetic loci that are maximally associated with a phenotype, while being connected in an underlying network. Our approach is based on a minimum cut reformulation of the problem of selecting features under sparsity and connectivity constraints that can be solved exactly and rapidly. SConES outperforms state-of-the-art competitors in terms of runtime, scales to hundreds of thousands of genetic loci, and exhibits higher power in detecting causal SNPs in simulation studies than existing methods. On flowering time phenotypes and genotypes from Arabidopsis thaliana, SConES detects loci that enable accurate phenotype prediction and that are supported by the literature. Matlab code for SConES is available at http://webdav.tuebingen.mpg.de/u/karsten/Forschung/scones/ version:5
arxiv-1205-1183 | On the Complexity of Trial and Error | http://arxiv.org/abs/1205.1183 | id:1205.1183 author:Xiaohui Bei, Ning Chen, Shengyu Zhang category:cs.CC cs.DS cs.LG  published:2012-05-06 summary:Motivated by certain applications from physics, biochemistry, economics, and computer science, in which the objects under investigation are not accessible because of various limitations, we propose a trial-and-error model to examine algorithmic issues in such situations. Given a search problem with a hidden input, we are asked to find a valid solution, to find which we can propose candidate solutions (trials), and use observed violations (errors), to prepare future proposals. In accordance with our motivating applications, we consider the fairly broad class of constraint satisfaction problems, and assume that errors are signaled by a verification oracle in the format of the index of a violated constraint (with the content of the constraint still hidden). Our discoveries are summarized as follows. On one hand, despite the seemingly very little information provided by the verification oracle, efficient algorithms do exist for a number of important problems. For the Nash, Core, Stable Matching, and SAT problems, the unknown-input versions are as hard as the corresponding known-input versions, up to a factor of polynomial. We further give almost tight bounds on the latter two problems' trial complexities. On the other hand, there are problems whose complexities are substantially increased in the unknown-input model. In particular, no time-efficient algorithms exist (under standard hardness assumptions) for Graph Isomorphism and Group Isomorphism problems. The tools used to achieve these results include order theory, strong ellipsoid method, and some non-standard reductions. Our model investigates the value of information, and our results demonstrate that the lack of input information can introduce various levels of extra difficulty. The model exhibits intimate connections with (and we hope can also serve as a useful supplement to) certain existing learning and complexity theories. version:2
arxiv-1304-4994 | Polygon Matching and Indexing Under Affine Transformations | http://arxiv.org/abs/1304.4994 | id:1304.4994 author:Edgar Chávez, Ana C. Chávez-Cáliz, Jorge L. López-López category:cs.CV 51N10  published:2013-04-18 summary:Given a collection $\{Z_1,Z_2,\ldots,Z_m\}$ of $n$-sided polygons in the plane and a query polygon $W$ we give algorithms to find all $Z_\ell$ such that $W=f(Z_\ell)$ with $f$ an unknown similarity transformation in time independent of the size of the collection. If $f$ is a known affine transformation, we show how to find all $Z_\ell$ such that $W=f(Z_\ell)$ in $O(n+\log(m))$ time. For a pair $W,W^\prime$ of polygons we can find all the pairs $Z_\ell,Z_{\ell^\prime}$ such that $W=f(Z_\ell)$ and $W^\prime=f(Z_{\ell^\prime})$ for an unknown affine transformation $f$ in $O(m+n)$ time. For the case of triangles we also give bounds for the problem of matching triangles with variable vertices, which is equivalent to affine matching triangles in noisy conditions. version:1
arxiv-1211-0996 | Learning using Local Membership Queries | http://arxiv.org/abs/1211.0996 | id:1211.0996 author:Pranjal Awasthi, Vitaly Feldman, Varun Kanade category:cs.LG cs.AI  published:2012-11-05 summary:We introduce a new model of membership query (MQ) learning, where the learning algorithm is restricted to query points that are \emph{close} to random examples drawn from the underlying distribution. The learning model is intermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (where the queries are allowed to be arbitrary points). Membership query algorithms are not popular among machine learning practitioners. Apart from the obvious difficulty of adaptively querying labelers, it has also been observed that querying \emph{unnatural} points leads to increased noise from human labelers (Lang and Baum, 1992). This motivates our study of learning algorithms that make queries that are close to examples generated from the data distribution. We restrict our attention to functions defined on the $n$-dimensional Boolean hypercube and say that a membership query is local if its Hamming distance from some example in the (random) training data is at most $O(\log(n))$. We show the following results in this model: (i) The class of sparse polynomials (with coefficients in R) over $\{0,1\}^n$ is polynomial time learnable under a large class of \emph{locally smooth} distributions using $O(\log(n))$-local queries. This class also includes the class of $O(\log(n))$-depth decision trees. (ii) The class of polynomial-sized decision trees is polynomial time learnable under product distributions using $O(\log(n))$-local queries. (iii) The class of polynomial size DNF formulas is learnable under the uniform distribution using $O(\log(n))$-local queries in time $n^{O(\log(\log(n)))}$. (iv) In addition we prove a number of results relating the proposed model to the traditional PAC model and the PAC+MQ model. version:2
arxiv-1304-4786 | The Mahalanobis distance for functional data with applications to classification | http://arxiv.org/abs/1304.4786 | id:1304.4786 author:Esdras Joseph, Pedro Galeano, Rosa E. Lillo category:math.ST stat.CO stat.ME stat.ML stat.TH  published:2013-04-17 summary:This paper presents a general notion of Mahalanobis distance for functional data that extends the classical multivariate concept to situations where the observed data are points belonging to curves generated by a stochastic process. More precisely, a new semi-distance for functional observations that generalize the usual Mahalanobis distance for multivariate datasets is introduced. For that, the development uses a regularized square root inverse operator in Hilbert spaces. Some of the main characteristics of the functional Mahalanobis semi-distance are shown. Afterwards, new versions of several well known functional classification procedures are developed using the Mahalanobis distance for functional data as a measure of proximity between functional observations. The performance of several well known functional classification procedures are compared with those methods used in conjunction with the Mahalanobis distance for functional data, with positive results, through a Monte Carlo study and the analysis of two real data examples. version:1
arxiv-1304-4765 | Robust Noise Filtering in Image Sequences | http://arxiv.org/abs/1304.4765 | id:1304.4765 author:Soumaya Hichri, Faouzi Benzarti, Hamid Amiri category:cs.CV  published:2013-04-17 summary:Image sequences filtering have recently become a very important technical problem especially with the advent of new technology in multimedia and video systems applications. Often image sequences are corrupted by some amount of noise introduced by the image sensor and therefore inherently present in the imaging process. The main problem in the image sequences is how to deal with spatio-temporal and non stationary signals. In this paper, we propose a robust method for noise removal of image sequence based on coupled spatial and temporal anisotropic diffusion. The idea is to achieve an adaptive smoothing in both spatial and temporal directions, by solving a nonlinear diffusion equation. This allows removing noise while preserving all spatial and temporal discontinuities version:1
arxiv-1304-4711 | Automated Switching System for Skin Pixel Segmentation in Varied Lighting | http://arxiv.org/abs/1304.4711 | id:1304.4711 author:Ankit Chaudhary, Ankur Gupta category:cs.CV  published:2013-04-17 summary:In Computer Vision, colour-based spatial techniquesoften assume a static skin colour model. However, skin colour perceived by a camera can change when lighting changes. In common real environment multiple light sources impinge on the skin. Moreover, detection techniques may vary when the image under study is taken under different lighting condition than the one that was earlier under consideration. Therefore, for robust skin pixel detection, a dynamic skin colour model that can cope with the changes must be employed. This paper shows that skin pixel detection in a digital colour image can be significantly improved by employing automated colour space switching methods. In the root of the switching technique which is employed in this study, lies the statistical mean of value of the skin pixels in the image which in turn has been derived from the Value, measures as a third component of the HSV. The study is based on experimentations on a set of images where capture time conditions varying from highly illuminated to almost dark. version:1
arxiv-1304-4662 | Tracking of Fingertips and Centres of Palm using KINECT | http://arxiv.org/abs/1304.4662 | id:1304.4662 author:J. L. Raheja, A. Chaudhary, K Singal category:cs.CV  published:2013-04-17 summary:Hand Gesture is a popular way to interact or control machines and it has been implemented in many applications. The geometry of hand is such that it is hard to construct in virtual environment and control the joints but the functionality and DOF encourage researchers to make a hand like instrument. This paper presents a novel method for fingertips detection and centres of palms detection distinctly for both hands using MS KINECT in 3D from the input image. KINECT facilitates us by providing the depth information of foreground objects. The hands were segmented using the depth vector and centres of palms were detected using distance transformation on inverse image. This result would be used to feed the inputs to the robotic hands to emulate human hands operation. version:1
arxiv-1304-4652 | A Health Monitoring System for Elder and Sick Persons | http://arxiv.org/abs/1304.4652 | id:1304.4652 author:Ankit Chaudhary, Jagdish L. Raheja category:cs.CV cs.HC  published:2013-04-17 summary:This paper discusses a vision based health monitoring system which would be very easy in use and deployment. Elder and sick people who are not able to talk or walk they are dependent on other human beings for their daily needs and need continuous monitoring. The developed system provides facility to the sick or elder person to describe his or her need to their caretaker in lingual description by showing particular hand gesture with the developed system. This system uses fingertip detection technique for gesture extraction and artificial neural network for gesture classification and recognition. The system is able to work in different light conditions and can be connected to different devices to announce users need on a distant location. version:1
arxiv-1211-5614 | A Hash based Approach for Secure Keyless Steganography in Lossless RGB Images | http://arxiv.org/abs/1211.5614 | id:1211.5614 author:Ankit Chaudhary, J. Vasavada, J. L. Raheja, S. Kumar, M. Sharma category:cs.CR cs.CV cs.MM  published:2012-11-23 summary:This paper proposes an improved steganography approach for hiding text messages in lossless RGB images. The objective of this work is to increase the security level and to improve the storage capacity with compression techniques. The security level is increased by randomly distributing the text message over the entire image instead of clustering within specific image portions. Storage capacity is increased by utilizing all the color channels for storing information and providing the source text message compression. The degradation of the images can be minimized by changing only one least significant bit per color channel for hiding the message, incurring a very little change in the original image. Using steganography alone with simple LSB has a potential problem that the secret message is easily detectable from the histogram analysis method. To improve the security as well as the image embedding capacity indirectly, a compression based scheme is introduced. Various tests have been done to check the storage capacity and message distribution. These testes show the superiority of the proposed approach with respect to other existing approaches. version:5
arxiv-1304-4642 | Easy and hard functions for the Boolean hidden shift problem | http://arxiv.org/abs/1304.4642 | id:1304.4642 author:Andrew M. Childs, Robin Kothari, Maris Ozols, Martin Roetteler category:quant-ph cs.CC cs.LG  published:2013-04-16 summary:We study the quantum query complexity of the Boolean hidden shift problem. Given oracle access to f(x+s) for a known Boolean function f, the task is to determine the n-bit string s. The quantum query complexity of this problem depends strongly on f. We demonstrate that the easiest instances of this problem correspond to bent functions, in the sense that an exact one-query algorithm exists if and only if the function is bent. We partially characterize the hardest instances, which include delta functions. Moreover, we show that the problem is easy for random functions, since two queries suffice. Our algorithm for random functions is based on performing the pretty good measurement on several copies of a certain state; its analysis relies on the Fourier transform. We also use this approach to improve the quantum rejection sampling approach to the Boolean hidden shift problem. version:1
arxiv-1304-4634 | Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances and Nonlocal Means | http://arxiv.org/abs/1304.4634 | id:1304.4634 author:Leonardo Torres, Sidnei J. S. Sant'Anna, Corina C. Freitas, Alejandro C. Frery category:cs.IT cs.CV cs.GR math.IT stat.AP stat.ML  published:2013-04-16 summary:This paper presents a technique for reducing speckle in Polarimetric Synthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and a statistical test based on stochastic divergences. The main objective is to select homogeneous pixels in the filtering area through statistical tests between distributions. This proposal uses the complex Wishart model to describe PolSAR data, but the technique can be extended to other models. The weights of the location-variant linear filter are function of the p-values of tests which verify the hypothesis that two samples come from the same distribution and, therefore, can be used to compute a local mean. The test stems from the family of (h-phi) divergences which originated in Information Theory. This novel technique was compared with the Boxcar, Refined Lee and IDAN filters. Image quality assessment methods on simulated and real data are employed to validate the performance of this approach. We show that the proposed filter also enhances the polarimetric entropy and preserves the scattering information of the targets. version:1
arxiv-1304-4633 | PAC Quasi-automatizability of Resolution over Restricted Distributions | http://arxiv.org/abs/1304.4633 | id:1304.4633 author:Brendan Juba category:cs.DS cs.LG cs.LO  published:2013-04-16 summary:We consider principled alternatives to unsupervised learning in data mining by situating the learning task in the context of the subsequent analysis task. Specifically, we consider a query-answering (hypothesis-testing) task: In the combined task, we decide whether an input query formula is satisfied over a background distribution by using input examples directly, rather than invoking a two-stage process in which (i) rules over the distribution are learned by an unsupervised learning algorithm and (ii) a reasoning algorithm decides whether or not the query formula follows from the learned rules. In a previous work (2013), we observed that the learning task could satisfy numerous desirable criteria in this combined context -- effectively matching what could be achieved by agnostic learning of CNFs from partial information -- that are not known to be achievable directly. In this work, we show that likewise, there are reasoning tasks that are achievable in such a combined context that are not known to be achievable directly (and indeed, have been seriously conjectured to be impossible, cf. (Alekhnovich and Razborov, 2008)). Namely, we test for a resolution proof of the query formula of a given size in quasipolynomial time (that is, "quasi-automatizing" resolution). The learning setting we consider is a partial-information, restricted-distribution setting that generalizes learning parities over the uniform distribution from partial information, another task that is known not to be achievable directly in various models (cf. (Ben-David and Dichterman, 1998) and (Michael, 2010)). version:1
arxiv-1304-4549 | Learning Heteroscedastic Models by Convex Programming under Group Sparsity | http://arxiv.org/abs/1304.4549 | id:1304.4549 author:Arnak S. Dalalyan, Mohamed Hebiri, Katia Méziani, Joseph Salmon category:stat.ML  published:2013-04-16 summary:Popular sparse estimation methods based on $\ell_1$-relaxation, such as the Lasso and the Dantzig selector, require the knowledge of the variance of the noise in order to properly tune the regularization parameter. This constitutes a major obstacle in applying these methods in several frameworks---such as time series, random fields, inverse problems---for which the noise is rarely homoscedastic and its level is hard to know in advance. In this paper, we propose a new approach to the joint estimation of the conditional mean and the conditional variance in a high-dimensional (auto-) regression setting. An attractive feature of the proposed estimator is that it is efficiently computable even for very large scale problems by solving a second-order cone program (SOCP). We present theoretical analysis and numerical results assessing the performance of the proposed procedure. version:1
arxiv-1304-4535 | Heterogeneous patterns enhancing static and dynamic texture classification | http://arxiv.org/abs/1304.4535 | id:1304.4535 author:Núbia Rosa da Silva, Odemir Martinez Bruno category:cs.CV  published:2013-04-16 summary:Some mixtures, such as colloids like milk, blood, and gelatin, have homogeneous appearance when viewed with the naked eye, however, to observe them at the nanoscale is possible to understand the heterogeneity of its components. The same phenomenon can occur in pattern recognition in which it is possible to see heterogeneous patterns in texture images. However, current methods of texture analysis can not adequately describe such heterogeneous patterns. Common methods used by researchers analyse the image information in a global way, taking all its features in an integrated manner. Furthermore, multi-scale analysis verifies the patterns at different scales, but still preserving the homogeneous analysis. On the other hand various methods use textons to represent the texture, breaking texture down into its smallest unit. To tackle this problem, we propose a method to identify texture patterns not small as textons at distinct scales enhancing the separability among different types of texture. We find sub patterns of texture according to the scale and then group similar patterns for a more refined analysis. Tests were performed in four static texture databases and one dynamic one. Results show that our method provides better classification rate compared with conventional approaches both in static and in dynamic texture. version:1
arxiv-1304-4520 | Sentiment Analysis : A Literature Survey | http://arxiv.org/abs/1304.4520 | id:1304.4520 author:Subhabrata Mukherjee, Pushpak Bhattacharyya category:cs.CL  published:2013-04-16 summary:Our day-to-day life has always been influenced by what people think. Ideas and opinions of others have always affected our own opinions. The explosion of Web 2.0 has led to increased activity in Podcasting, Blogging, Tagging, Contributing to RSS, Social Bookmarking, and Social Networking. As a result there has been an eruption of interest in people to mine these vast resources of data for opinions. Sentiment Analysis or Opinion Mining is the computational treatment of opinions, sentiments and subjectivity of text. In this report, we take a look at the various challenges and applications of Sentiment Analysis. We will discuss in details various approaches to perform a computational treatment of sentiments and opinions. Various supervised or data-driven techniques to SA like Na\"ive Byes, Maximum Entropy, SVM, and Voted Perceptrons will be discussed and their strengths and drawbacks will be touched upon. We will also see a new dimension of analyzing sentiments by Cognitive Psychology mainly through the work of Janyce Wiebe, where we will see ways to detect subjectivity, perspective in narrative and understanding the discourse structure. We will also study some specific topics in Sentiment Analysis and the contemporary works in those areas. version:1
arxiv-1007-3622 | A generalized risk approach to path inference based on hidden Markov models | http://arxiv.org/abs/1007.3622 | id:1007.3622 author:Jüri Lember, Alexey A. Koloydenko category:stat.ML cs.LG stat.CO  published:2010-07-21 summary:Motivated by the unceasing interest in hidden Markov models (HMMs), this paper re-examines hidden path inference in these models, using primarily a risk-based framework. While the most common maximum a posteriori (MAP), or Viterbi, path estimator and the minimum error, or Posterior Decoder (PD), have long been around, other path estimators, or decoders, have been either only hinted at or applied more recently and in dedicated applications generally unfamiliar to the statistical learning community. Over a decade ago, however, a family of algorithmically defined decoders aiming to hybridize the two standard ones was proposed (Brushe et al., 1998). The present paper gives a careful analysis of this hybridization approach, identifies several problems and issues with it and other previously proposed approaches, and proposes practical resolutions of those. Furthermore, simple modifications of the classical criteria for hidden path recognition are shown to lead to a new class of decoders. Dynamic programming algorithms to compute these decoders in the usual forward-backward manner are presented. A particularly interesting subclass of such estimators can be also viewed as hybrids of the MAP and PD estimators. Similar to previously proposed MAP-PD hybrids, the new class is parameterized by a small number of tunable parameters. Unlike their algorithmic predecessors, the new risk-based decoders are more clearly interpretable, and, most importantly, work "out of the box" in practice, which is demonstrated on some real bioinformatics tasks and data. Some further generalizations and applications are discussed in conclusion. version:4
arxiv-1304-4344 | Sparse Coding and Dictionary Learning for Symmetric Positive Definite Matrices: A Kernel Approach | http://arxiv.org/abs/1304.4344 | id:1304.4344 author:Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, Brian C. Lovell category:cs.LG cs.CV stat.ML  published:2013-04-16 summary:Recent advances suggest that a wide range of computer vision problems can be addressed more appropriately by considering non-Euclidean geometry. This paper tackles the problem of sparse coding and dictionary learning in the space of symmetric positive definite matrices, which form a Riemannian manifold. With the aid of the recently introduced Stein kernel (related to a symmetric version of Bregman matrix divergence), we propose to perform sparse coding by embedding Riemannian manifolds into reproducing kernel Hilbert spaces. This leads to a convex and kernel version of the Lasso problem, which can be solved efficiently. We furthermore propose an algorithm for learning a Riemannian dictionary (used for sparse coding), closely tied to the Stein kernel. Experiments on several classification tasks (face recognition, texture classification, person re-identification) show that the proposed sparse coding approach achieves notable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as tensor sparse coding, Riemannian locality preserving projection, and symmetry-driven accumulation of local features. version:1
arxiv-1301-5348 | Why Size Matters: Feature Coding as Nystrom Sampling | http://arxiv.org/abs/1301.5348 | id:1301.5348 author:Oriol Vinyals, Yangqing Jia, Trevor Darrell category:cs.LG cs.CV  published:2013-01-15 summary:Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points. Furthermore, since bounds are known on the approximation power of Nystrom sampling as a function of how many samples (i.e. dictionary size) we consider, we can derive bounds on the approximation of the exact (but expensive to compute) kernel matrix, and use it as a proxy to predict accuracy as a function of the dictionary size, which has been observed to increase but also to saturate as we increase its size. This model may help explaining the positive effect of the codebook size and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity. version:2
arxiv-1302-0494 | Local Structure Matching Driven by Joint-Saliency-Structure Adaptive Kernel Regression | http://arxiv.org/abs/1302.0494 | id:1302.0494 author:Binjie Qin, Zhuangming Shen, Zien Zhou, Jiawei Zhou, Jiuai Sun, Hui Zhang, Mingxing Hu, Yisong Lv category:cs.CV  published:2013-02-03 summary:For nonrigid image registration, matching the particular structures (or the outliers) that have missing correspondence and/or local large deformations, can be more difficult than matching the common structures with small deformations in the two images. Most existing works depend heavily on the outlier segmentation to remove the outlier effect in the registration. Moreover, these works do not handle simultaneously the missing correspondences and local large deformations. In this paper, we defined the nonrigid image registration as a local adaptive kernel regression which locally reconstruct the moving image's dense deformation vectors from the sparse deformation vectors in the multi-resolution block matching. The kernel function of the kernel regression adapts its shape and orientation to the reference image's structure to gather more deformation vector samples of the same structure for the iterative regression computation, whereby the moving image's local deformations could be compliant with the reference image's local structures. To estimate the local deformations around the outliers, we use joint saliency map that highlights the corresponding saliency structures (called Joint Saliency Structures, JSSs) in the two images to guide the dense deformation reconstruction by emphasizing those JSSs' sparse deformation vectors in the kernel regression. The experimental results demonstrate that by using local JSS adaptive kernel regression, the proposed method achieves almost the best performance in alignment of all challenging image pairs with outlier structures compared with other five state-of-the-art nonrigid registration algorithms. version:4
arxiv-1304-4112 | Shadow Estimation Method for "The Episolar Constraint: Monocular Shape from Shadow Correspondence" | http://arxiv.org/abs/1304.4112 | id:1304.4112 author:Austin Abrams, Chris Hawley, Kylia Miskell, Adina Stoica, Nathan Jacobs, Robert Pless category:cs.CV  published:2013-04-15 summary:Recovering shadows is an important step for many vision algorithms. Current approaches that work with time-lapse sequences are limited to simple thresholding heuristics. We show these approaches only work with very careful tuning of parameters, and do not work well for long-term time-lapse sequences taken over the span of many months. We introduce a parameter-free expectation maximization approach which simultaneously estimates shadows, albedo, surface normals, and skylight. This approach is more accurate than previous methods, works over both very short and very long sequences, and is robust to the effects of nonlinear camera response. Finally, we demonstrate that the shadow masks derived through this algorithm substantially improve the performance of sun-based photometric stereo compared to earlier shadow mask estimation. version:1
arxiv-1304-4058 | Link Prediction with Social Vector Clocks | http://arxiv.org/abs/1304.4058 | id:1304.4058 author:Conrad Lee, Bobo Nick, Ulrik Brandes, Pádraig Cunningham category:cs.SI physics.soc-ph stat.ML  published:2013-04-15 summary:State-of-the-art link prediction utilizes combinations of complex features derived from network panel data. We here show that computationally less expensive features can achieve the same performance in the common scenario in which the data is available as a sequence of interactions. Our features are based on social vector clocks, an adaptation of the vector-clock concept introduced in distributed computing to social interaction networks. In fact, our experiments suggest that by taking into account the order and spacing of interactions, social vector clocks exploit different aspects of link formation so that their combination with previous approaches yields the most accurate predictor to date. version:1
arxiv-1304-4051 | Coordinating metaheuristic agents with swarm intelligence | http://arxiv.org/abs/1304.4051 | id:1304.4051 author:Mehmet Emin Aydin category:cs.MA cs.NE  published:2013-04-15 summary:Coordination of multi agent systems remains as a problem since there is no prominent method to completely solve this problem. Metaheuristic agents are specific implementations of multi-agent systems, which imposes working together to solve optimisation problems with metaheuristic algorithms. The idea borrowed from swarm intelligence seems working much better than those implementations suggested before. This paper reports the performance of swarms of simulated annealing agents collaborating with particle swarm optimization algorithm. The proposed approach is implemented for multidimensional knapsack problem and has resulted much better than some other works published before. version:1
arxiv-1304-4041 | Multispectral Spatial Characterization: Application to Mitosis Detection in Breast Cancer Histopathology | http://arxiv.org/abs/1304.4041 | id:1304.4041 author:H. Irshad, A. Gouaillard, L. Roux, D. Racoceanu category:cs.CV  published:2013-04-15 summary:Accurate detection of mitosis plays a critical role in breast cancer histopathology. Manual detection and counting of mitosis is tedious and subject to considerable inter- and intra-reader variations. Multispectral imaging is a recent medical imaging technology, proven successful in increasing the segmentation accuracy in other fields. This study aims at improving the accuracy of mitosis detection by developing a specific solution using multispectral and multifocal imaging of breast cancer histopathological data. We propose to enable clinical routine-compliant quality of mitosis discrimination from other objects. The proposed framework includes comprehensive analysis of spectral bands and z-stack focus planes, detection of expected mitotic regions (candidates) in selected focus planes and spectral bands, computation of multispectral spatial features for each candidate, selection of multispectral spatial features and a study of different state-of-the-art classification methods for candidates classification as mitotic or non mitotic figures. This framework has been evaluated on MITOS multispectral medical dataset and achieved 60% detection rate and 57% F-Measure. Our results indicate that multispectral spatial features have more information for mitosis classification in comparison with white spectral band features, being therefore a very promising exploration area to improve the quality of the diagnosis assistance in histopathology. version:1
arxiv-1304-3577 | Identifying cancer subtypes in glioblastoma by combining genomic, transcriptomic and epigenomic data | http://arxiv.org/abs/1304.3577 | id:1304.3577 author:Richard S. Savage, Zoubin Ghahramani, Jim E. Griffin, Paul Kirk, David L. Wild category:q-bio.GN stat.ML  published:2013-04-12 summary:We present a nonparametric Bayesian method for disease subtype discovery in multi-dimensional cancer data. Our method can simultaneously analyse a wide range of data types, allowing for both agreement and disagreement between their underlying clustering structure. It includes feature selection and infers the most likely number of disease subtypes, given the data. We apply the method to 277 glioblastoma samples from The Cancer Genome Atlas, for which there are gene expression, copy number variation, methylation and microRNA data. We identify 8 distinct consensus subtypes and study their prognostic value for death, new tumour events, progression and recurrence. The consensus subtypes are prognostic of tumour recurrence (log-rank p-value of $3.6 \times 10^{-4}$ after correction for multiple hypothesis tests). This is driven principally by the methylation data (log-rank p-value of $2.0 \times 10^{-3}$) but the effect is strengthened by the other 3 data types, demonstrating the value of integrating multiple data types. Of particular note is a subtype of 47 patients characterised by very low levels of methylation. This subtype has very low rates of tumour recurrence and no new events in 10 years of follow up. We also identify a small gene expression subtype of 6 patients that shows particularly poor survival outcomes. Additionally, we note a consensus subtype that showly a highly distinctive data signature and suggest that it is therefore a biologically distinct subtype of glioblastoma. The code is available from https://sites.google.com/site/multipledatafusion/ version:2
arxiv-1304-3992 | GPU Acclerated Automated Feature Extraction from Satellite Images | http://arxiv.org/abs/1304.3992 | id:1304.3992 author:K. Phani Tejaswi, D. Shanmukha Rao, Thara Nair, A. V. V. Prasad category:cs.DC cs.CV  published:2013-04-15 summary:The availability of large volumes of remote sensing data insists on higher degree of automation in feature extraction, making it a need of the hour.The huge quantum of data that needs to be processed entails accelerated processing to be enabled.GPUs, which were originally designed to provide efficient visualization, are being massively employed for computation intensive parallel processing environments. Image processing in general and hence automated feature extraction, is highly computation intensive, where performance improvements have a direct impact on societal needs. In this context, an algorithm has been formulated for automated feature extraction from a panchromatic or multispectral image based on image processing techniques. Two Laplacian of Guassian (LoG) masks were applied on the image individually followed by detection of zero crossing points and extracting the pixels based on their standard deviation with the surrounding pixels. The two extracted images with different LoG masks were combined together which resulted in an image with the extracted features and edges. Finally the user is at liberty to apply the image smoothing step depending on the noise content in the extracted image. The image is passed through a hybrid median filter to remove the salt and pepper noise from the image. This paper discusses the aforesaid algorithm for automated feature extraction, necessity of deployment of GPUs for the same; system-level challenges and quantifies the benefits of integrating GPUs in such environment. The results demonstrate that substantial enhancement in performance margin can be achieved with the best utilization of GPU resources and an efficient parallelization strategy. Performance results in comparison with the conventional computing scenario have provided a speedup of 20x, on realization of this parallelizing strategy. version:1
arxiv-1210-7053 | Managing sparsity, time, and quality of inference in topic models | http://arxiv.org/abs/1210.7053 | id:1210.7053 author:Khoat Than, Tu Bao Ho category:stat.ML cs.AI cs.CV stat.ME  published:2012-10-26 summary:Inference is an integral part of probabilistic topic models, but is often non-trivial to derive an efficient algorithm for a specific model. It is even much more challenging when we want to find a fast inference algorithm which always yields sparse latent representations of documents. In this article, we introduce a simple framework for inference in probabilistic topic models, denoted by FW. This framework is general and flexible enough to be easily adapted to mixture models. It has a linear convergence rate, offers an easy way to incorporate prior knowledge, and provides us an easy way to directly trade off sparsity against quality and time. We demonstrate the goodness and flexibility of FW over existing inference methods by a number of tasks. Finally, we show how inference in topic models with nonconjugate priors can be done efficiently. version:2
arxiv-1304-3915 | Single View Depth Estimation from Examples | http://arxiv.org/abs/1304.3915 | id:1304.3915 author:Tal Hassner, Ronen Basri category:cs.CV 68T45  published:2013-04-14 summary:We describe a non-parametric, "example-based" method for estimating the depth of an object, viewed in a single photo. Our method consults a database of example 3D geometries, searching for those which look similar to the object in the photo. The known depths of the selected database objects act as shape priors which constrain the process of estimating the object's depth. We show how this process can be performed by optimizing a well defined target likelihood function, via a hard-EM procedure. We address the problem of representing the (possibly infinite) variability of viewing conditions with a finite (and often very small) example set, by proposing an on-the-fly example update scheme. We further demonstrate the importance of non-stationarity in avoiding misleading examples when estimating structured shapes. We evaluate our method and present both qualitative as well as quantitative results for challenging object classes. Finally, we show how this same technique may be readily applied to a number of related problems. These include the novel task of estimating the occluded depth of an object's backside and the task of tailoring custom fitting image-maps for input depths. version:1
arxiv-1304-3892 | An accelerated CLPSO algorithm | http://arxiv.org/abs/1304.3892 | id:1304.3892 author:Muhammad Omer Bin Saeed, Muhammad Saqib Sohail, Syed Zeeshan Rizvi, Mobien Shoaib, Asrar Ul Haq Sheikh category:cs.NE  published:2013-04-14 summary:The particle swarm approach provides a low complexity solution to the optimization problem among various existing heuristic algorithms. Recent advances in the algorithm resulted in improved performance at the cost of increased computational complexity, which is undesirable. Literature shows that the particle swarm optimization algorithm based on comprehensive learning provides the best complexity-performance trade-off. We show how to reduce the complexity of this algorithm further, with a slight but acceptable performance loss. This enhancement allows the application of the algorithm in time critical applications, such as, real-time tracking, equalization etc. version:1
arxiv-1304-3879 | Automatic case acquisition from texts for process-oriented case-based reasoning | http://arxiv.org/abs/1304.3879 | id:1304.3879 author:Valmi Dufour-Lussier, Florence Le Ber, Jean Lieber, Emmanuel Nauer category:cs.AI cs.CL  published:2013-04-14 summary:This paper introduces a method for the automatic acquisition of a rich case representation from free text for process-oriented case-based reasoning. Case engineering is among the most complicated and costly tasks in implementing a case-based reasoning system. This is especially so for process-oriented case-based reasoning, where more expressive case representations are generally used and, in our opinion, actually required for satisfactory case adaptation. In this context, the ability to acquire cases automatically from procedural texts is a major step forward in order to reason on processes. We therefore detail a methodology that makes case acquisition from processes described as free text possible, with special attention given to assembly instruction texts. This methodology extends the techniques we used to extract actions from cooking recipes. We argue that techniques taken from natural language processing are required for this task, and that they give satisfactory results. An evaluation based on our implemented prototype extracting workflows from recipe texts is provided. version:1
arxiv-1304-3840 | A New Homogeneity Inter-Clusters Measure in SemiSupervised Clustering | http://arxiv.org/abs/1304.3840 | id:1304.3840 author:Badreddine Meftahi, Ourida Ben Boubaker Saidi category:cs.LG  published:2013-04-13 summary:Many studies in data mining have proposed a new learning called semi-Supervised. Such type of learning combines unlabeled and labeled data which are hard to obtain. However, in unsupervised methods, the only unlabeled data are used. The problem of significance and the effectiveness of semi-supervised clustering results is becoming of main importance. This paper pursues the thesis that muchgreater accuracy can be achieved in such clustering by improving the similarity computing. Hence, we introduce a new approach of semisupervised clustering using an innovative new homogeneity measure of generated clusters. Our experimental results demonstrate significantly improved accuracy as a result. version:1
arxiv-1304-3792 | Solving Linear Equations Using a Jacobi Based Time-Variant Adaptive Hybrid Evolutionary Algorithm | http://arxiv.org/abs/1304.3792 | id:1304.3792 author:A. R. M. Jalal Uddin Jamali, M. M. A. Hashem, Md. Bazlar Rahman category:cs.NE  published:2013-04-13 summary:Large set of linear equations, especially for sparse and structured coefficient (matrix) equations, solutions using classical methods become arduous. And evolutionary algorithms have mostly been used to solve various optimization and learning problems. Recently, hybridization of classical methods (Jacobi method and Gauss-Seidel method) with evolutionary computation techniques have successfully been applied in linear equation solving. In the both above hybrid evolutionary methods, uniform adaptation (UA) techniques are used to adapt relaxation factor. In this paper, a new Jacobi Based Time-Variant Adaptive (JBTVA) hybrid evolutionary algorithm is proposed. In this algorithm, a Time-Variant Adaptive (TVA) technique of relaxation factor is introduced aiming at both improving the fine local tuning and reducing the disadvantage of uniform adaptation of relaxation factors. This algorithm integrates the Jacobi based SR method with time variant adaptive evolutionary algorithm. The convergence theorems of the proposed algorithm are proved theoretically. And the performance of the proposed algorithm is compared with JBUA hybrid evolutionary algorithm and classical methods in the experimental domain. The proposed algorithm outperforms both the JBUA hybrid algorithm and classical methods in terms of convergence speed and effectiveness. version:1
arxiv-1304-3779 | Improving Generalization Ability of Genetic Programming: Comparative Study | http://arxiv.org/abs/1304.3779 | id:1304.3779 author:Tejashvi R. Naik, Vipul K. Dabhi category:cs.NE  published:2013-04-13 summary:In the field of empirical modeling using Genetic Programming (GP), it is important to evolve solution with good generalization ability. Generalization ability of GP solutions get affected by two important issues: bloat and over-fitting. Bloat is uncontrolled growth of code without any gain in fitness and important issue in GP. We surveyed and classified existing literature related to different techniques used by GP research community to deal with the issue of bloat. Moreover, the classifications of different bloat control approaches and measures for bloat are discussed. Next, we tested four bloat control methods: Tarpeian, double tournament, lexicographic parsimony pressure with direct bucketing and ratio bucketing on six different problems and identified where each bloat control method performs well on per problem basis. Based on the analysis of each method, we combined two methods: double tournament (selection method) and Tarpeian method (works before evaluation) to avoid bloated solutions and compared with the results obtained from individual performance of double tournament method. It was found that the results were improved with this combination of two methods. version:1
arxiv-1304-3763 | An Improved ACS Algorithm for the Solutions of Larger TSP Problems | http://arxiv.org/abs/1304.3763 | id:1304.3763 author:Md. Rakib Hassan, Md. Kamrul Hasan, M. M. A. Hashem category:cs.AI cs.DS cs.NE  published:2013-04-13 summary:Solving large traveling salesman problem (TSP) in an efficient way is a challenging area for the researchers of computer science. This paper presents a modified version of the ant colony system (ACS) algorithm called Red-Black Ant Colony System (RB-ACS) for the solutions of TSP which is the most prominent member of the combinatorial optimization problem. RB-ACS uses the concept of ant colony system together with the parallel search of genetic algorithm for obtaining the optimal solutions quickly. In this paper, it is shown that the proposed RB-ACS algorithm yields significantly better performance than the existing best-known algorithms. version:1
arxiv-1304-3745 | Towards more accurate clustering method by using dynamic time warping | http://arxiv.org/abs/1304.3745 | id:1304.3745 author:Khadoudja Ghanem category:cs.LG stat.ML  published:2013-04-12 summary:An intrinsic problem of classifiers based on machine learning (ML) methods is that their learning time grows as the size and complexity of the training dataset increases. For this reason, it is important to have efficient computational methods and algorithms that can be applied on large datasets, such that it is still possible to complete the machine learning tasks in reasonable time. In this context, we present in this paper a more accurate simple process to speed up ML methods. An unsupervised clustering algorithm is combined with Expectation, Maximization (EM) algorithm to develop an efficient Hidden Markov Model (HMM) training. The idea of the proposed process consists of two steps. In the first step, training instances with similar inputs are clustered and a weight factor which represents the frequency of these instances is assigned to each representative cluster. Dynamic Time Warping technique is used as a dissimilarity function to cluster similar examples. In the second step, all formulas in the classical HMM training algorithm (EM) associated with the number of training instances are modified to include the weight factor in appropriate terms. This process significantly accelerates HMM training while maintaining the same initial, transition and emission probabilities matrixes as those obtained with the classical HMM training algorithm. Accordingly, the classification accuracy is preserved. Depending on the size of the training set, speedups of up to 2200 times is possible when the size is about 100.000 instances. The proposed approach is not limited to training HMMs, but it can be employed for a large variety of MLs methods. version:1
arxiv-1304-3708 | Advice-Efficient Prediction with Expert Advice | http://arxiv.org/abs/1304.3708 | id:1304.3708 author:Yevgeny Seldin, Peter Bartlett, Koby Crammer category:cs.LG stat.ML  published:2013-04-12 summary:Advice-efficient prediction with expert advice (in analogy to label-efficient prediction) is a variant of prediction with expert advice game, where on each round of the game we are allowed to ask for advice of a limited number $M$ out of $N$ experts. This setting is especially interesting when asking for advice of every expert on every round is expensive. We present an algorithm for advice-efficient prediction with expert advice that achieves $O(\sqrt{\frac{N}{M}T\ln N})$ regret on $T$ rounds of the game. version:1
arxiv-1304-3612 | A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems | http://arxiv.org/abs/1304.3612 | id:1304.3612 author:V. Ravibabu category:cs.NE  published:2013-04-12 summary:This paper represents the metaheuristics proposed for solving a class of Shop Scheduling problem. The Bacterial Foraging Optimization algorithm is featured with Ant Colony Optimization algorithm and proposed as a natural inspired computing approach to solve the Mixed Shop Scheduling problem. The Mixed Shop is the combination of Job Shop, Flow Shop and Open Shop scheduling problems. The sample instances for all mentioned Shop problems are used as test data and Mixed Shop survive its computational complexity to minimize the makespan. The computational results show that the proposed algorithm is gentler to solve and performs better than the existing algorithms. version:1
arxiv-1304-3610 | Modified Soft Brood Crossover in Genetic Programming | http://arxiv.org/abs/1304.3610 | id:1304.3610 author:Hardik M. Parekh, Vipul K. Dabhi category:cs.NE  published:2013-04-12 summary:Premature convergence is one of the important issues while using Genetic Programming for data modeling. It can be avoided by improving population diversity. Intelligent genetic operators can help to improve the population diversity. Crossover is an important operator in Genetic Programming. So, we have analyzed number of intelligent crossover operators and proposed an algorithm with the modification of soft brood crossover operator. It will help to improve the population diversity and reduce the premature convergence. We have performed experiments on three different symbolic regression problems. Then we made the performance comparison of our proposed crossover (Modified Soft Brood Crossover) with the existing soft brood crossover and subtree crossover operators. version:1
arxiv-1101-1057 | Sparsity regret bounds for individual sequences in online linear regression | http://arxiv.org/abs/1101.1057 | id:1101.1057 author:Sébastien Gerchinovitz category:stat.ML cs.LG math.ST stat.TH  published:2011-01-05 summary:We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T. We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same flavor as in Dalalyan and Tsybakov (2011) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with fixed design. version:3
arxiv-1304-3573 | Astronomical Image Denoising Using Dictionary Learning | http://arxiv.org/abs/1304.3573 | id:1304.3573 author:Simon Beckouche, Jean-Luc Starck, Jalal Fadili category:astro-ph.IM cs.CV  published:2013-04-12 summary:Astronomical images suffer a constant presence of multiple defects that are consequences of the intrinsic properties of the acquisition equipments, and atmospheric conditions. One of the most frequent defects in astronomical imaging is the presence of additive noise which makes a denoising step mandatory before processing data. During the last decade, a particular modeling scheme, based on sparse representations, has drawn the attention of an ever growing community of researchers. Sparse representations offer a promising framework to many image and signal processing tasks, especially denoising and restoration applications. At first, the harmonics, wavelets, and similar bases and overcomplete representations have been considered as candidate domains to seek the sparsest representation. A new generation of algorithms, based on data-driven dictionaries, evolved rapidly and compete now with the off-the-shelf fixed dictionaries. While designing a dictionary beforehand leans on a guess of the most appropriate representative elementary forms and functions, the dictionary learning framework offers to construct the dictionary upon the data themselves, which provides us with a more flexible setup to sparse modeling and allows to build more sophisticated dictionaries. In this paper, we introduce the Centered Dictionary Learning (CDL) method and we study its performances for astronomical image denoising. We show how CDL outperforms wavelet or classic dictionary learning denoising techniques on astronomical images, and we give a comparison of the effect of these different algorithms on the photometry of the denoised images. version:1
arxiv-1304-3568 | Distributed dictionary learning over a sensor network | http://arxiv.org/abs/1304.3568 | id:1304.3568 author:Pierre Chainais, Cédric Richard category:stat.ML cs.LG stat.AP  published:2013-04-12 summary:We consider the problem of distributed dictionary learning, where a set of nodes is required to collectively learn a common dictionary from noisy measurements. This approach may be useful in several contexts including sensor networks. Diffusion cooperation schemes have been proposed to solve the distributed linear regression problem. In this work we focus on a diffusion-based adaptive dictionary learning strategy: each node records observations and cooperates with its neighbors by sharing its local dictionary. The resulting algorithm corresponds to a distributed block coordinate descent (alternate optimization). Beyond dictionary learning, this strategy could be adapted to many matrix factorization problems and generalized to various settings. This article presents our approach and illustrates its efficiency on some numerical examples. version:1
arxiv-1304-3406 | Merging Satellite Measurements of Rainfall Using Multi-scale Imagery Technique | http://arxiv.org/abs/1304.3406 | id:1304.3406 author:Seyed Hamed Alemohammad, Dara Entekhabi category:cs.CV cs.IR I.4; I.4.5; I.5.4  published:2013-04-11 summary:Several passive microwave satellites orbit the Earth and measure rainfall. These measurements have the advantage of almost full global coverage when compared to surface rain gauges. However, these satellites have low temporal revisit and missing data over some regions. Image fusion is a useful technique to fill in the gaps of one image (one satellite measurement) using another one. The proposed algorithm uses an iterative fusion scheme to integrate information from two satellite measurements. The algorithm is implemented on two datasets for 7 years of half-hourly data. The results show significant improvements in rain detection and rain intensity in the merged measurements. version:1
arxiv-1304-3393 | Generic Behaviour Similarity Measures for Evolutionary Swarm Robotics | http://arxiv.org/abs/1304.3393 | id:1304.3393 author:Jorge Gomes, Anders Lyhne Christensen category:cs.NE  published:2013-04-11 summary:Novelty search has shown to be a promising approach for the evolution of controllers for swarm robotics. In existing studies, however, the experimenter had to craft a domain dependent behaviour similarity measure to use novelty search in swarm robotics applications. The reliance on hand-crafted similarity measures places an additional burden to the experimenter and introduces a bias in the evolutionary process. In this paper, we propose and compare two task-independent, generic behaviour similarity measures: combined state count and sampled average state. The proposed measures use the values of sensors and effectors recorded for each individual robot of the swarm. The characterisation of the group-level behaviour is then obtained by combining the sensor-effector values from all the robots. We evaluate the proposed measures in an aggregation task and in a resource sharing task. We show that the generic measures match the performance of domain dependent measures in terms of solution quality. Our results indicate that the proposed generic measures operate as effective behaviour similarity measures, and that it is possible to leverage the benefits of novelty search without having to craft domain specific similarity measures. version:1
arxiv-1304-3362 | Evolution of Swarm Robotics Systems with Novelty Search | http://arxiv.org/abs/1304.3362 | id:1304.3362 author:Jorge Gomes, Paulo Urbano, Anders Lyhne Christensen category:cs.NE  published:2013-04-11 summary:Novelty search is a recent artificial evolution technique that challenges traditional evolutionary approaches. In novelty search, solutions are rewarded based on their novelty, rather than their quality with respect to a predefined objective. The lack of a predefined objective precludes premature convergence caused by a deceptive fitness function. In this paper, we apply novelty search combined with NEAT to the evolution of neural controllers for homogeneous swarms of robots. Our empirical study is conducted in simulation, and we use a common swarm robotics task - aggregation, and a more challenging task - sharing of an energy recharging station. Our results show that novelty search is unaffected by deception, is notably effective in bootstrapping the evolution, can find solutions with lower complexity than fitness-based evolution, and can find a broad diversity of solutions for the same task. Even in non-deceptive setups, novelty search achieves solution qualities similar to those obtained in traditional fitness-based evolution. Our study also encompasses variants of novelty search that work in concert with fitness-based evolution to combine the exploratory character of novelty search with the exploitatory character of objective-based evolution. We show that these variants can further improve the performance of novelty search. Overall, our study shows that novelty search is a promising alternative for the evolution of controllers for robotic swarms. version:1
arxiv-1304-3345 | Probabilistic Classification using Fuzzy Support Vector Machines | http://arxiv.org/abs/1304.3345 | id:1304.3345 author:Marzieh Parandehgheibi category:cs.LG math.ST stat.TH  published:2013-04-11 summary:In medical applications such as recognizing the type of a tumor as Malignant or Benign, a wrong diagnosis can be devastating. Methods like Fuzzy Support Vector Machines (FSVM) try to reduce the effect of misplaced training points by assigning a lower weight to the outliers. However, there are still uncertain points which are similar to both classes and assigning a class by the given information will cause errors. In this paper, we propose a two-phase classification method which probabilistically assigns the uncertain points to each of the classes. The proposed method is applied to the Breast Cancer Wisconsin (Diagnostic) Dataset which consists of 569 instances in 2 classes of Malignant and Benign. This method assigns certain instances to their appropriate classes with probability of one, and the uncertain instances to each of the classes with associated probabilities. Therefore, based on the degree of uncertainty, doctors can suggest further examinations before making the final diagnosis. version:1
arxiv-1304-3265 | Extension of hidden markov model for recognizing large vocabulary of sign language | http://arxiv.org/abs/1304.3265 | id:1304.3265 author:Maher Jebali, Patrice Dalle, Mohamed Jemni category:cs.CL  published:2013-04-11 summary:Computers still have a long way to go before they can interact with users in a truly natural fashion. From a users perspective, the most natural way to interact with a computer would be through a speech and gesture interface. Although speech recognition has made significant advances in the past ten years, gesture recognition has been lagging behind. Sign Languages (SL) are the most accomplished forms of gestural communication. Therefore, their automatic analysis is a real challenge, which is interestingly implied to their lexical and syntactic organization levels. Statements dealing with sign language occupy a significant interest in the Automatic Natural Language Processing (ANLP) domain. In this work, we are dealing with sign language recognition, in particular of French Sign Language (FSL). FSL has its own specificities, such as the simultaneity of several parameters, the important role of the facial expression or movement and the use of space for the proper utterance organization. Unlike speech recognition, Frensh sign language (FSL) events occur both sequentially and simultaneously. Thus, the computational processing of FSL is too complex than the spoken languages. We present a novel approach based on HMM to reduce the recognition complexity. version:1
arxiv-1304-3209 | Improvement studies on neutron-gamma separation in HPGe detectors by using neural networks | http://arxiv.org/abs/1304.3209 | id:1304.3209 author:Serkan Akkoyun, Tuncay Bayram, S. Okan Kara category:physics.ins-det cs.NE nucl-ex  published:2013-04-11 summary:The neutrons emitted in heavy-ion fusion-evaporation (HIFE) reactions together with the gamma-rays cause unwanted backgrounds in gamma-ray spectra. Especially in the nuclear reactions, where relativistic ion beams (RIBs) are used, these neutrons are serious problem. They have to be rejected in order to obtain clearer gamma-ray peaks. In this study, the radiation energy and three criteria which were previously determined for separation between neutron and gamma-rays in the HPGe detectors have been used in artificial neural network (ANN) for improving of the decomposition power. According to the preliminary results obtained from ANN method, the ratio of neutron rejection has been improved by a factor of 1.27 and the ratio of the lost in gamma-rays has been decreased by a factor of 0.50. version:1
arxiv-1209-4850 | The Pascal Triangle of a Discrete Image: Definition, Properties and Application to Shape Analysis | http://arxiv.org/abs/1209.4850 | id:1209.4850 author:Mireille Boutin, Shanshan Huang category:math-ph cs.CV math.MP  published:2012-09-21 summary:We define the Pascal triangle of a discrete (gray scale) image as a pyramidal arrangement of complex-valued moments and we explore its geometric significance. In particular, we show that the entries of row k of this triangle correspond to the Fourier series coefficients of the moment of order k of the Radon transform of the image. Group actions on the plane can be naturally prolonged onto the entries of the Pascal triangle. We study the prolongation of some common group actions, such as rotations and reflections, and we propose simple tests for detecting equivalences and self-equivalences under these group actions. The motivating application of this work is the problem of characterizing the geometry of objects on images, for example by detecting approximate symmetries. version:2
arxiv-1304-3200 | An Approach to Solve Linear Equations Using a Time-Variant Adaptation Based Hybrid Evolutionary Algorithm | http://arxiv.org/abs/1304.3200 | id:1304.3200 author:A. R. M. Jalal Uddin Jamali, M. M. A. Hashem, Md. Bazlar Rahman category:cs.NE cs.NA  published:2013-04-11 summary:For small number of equations, systems of linear (and sometimes nonlinear) equations can be solved by simple classical techniques. However, for large number of systems of linear (or nonlinear) equations, solutions using classical method become arduous. On the other hand evolutionary algorithms have mostly been used to solve various optimization and learning problems. Recently, hybridization of evolutionary algorithm with classical Gauss-Seidel based Successive Over Relaxation (SOR) method has successfully been used to solve large number of linear equations; where a uniform adaptation (UA) technique of relaxation factor is used. In this paper, a new hybrid algorithm is proposed in which a time-variant adaptation (TVA) technique of relaxation factor is used instead of uniform adaptation technique to solve large number of linear equations. The convergence theorems of the proposed algorithms are proved theoretically. And the performance of the proposed TVA-based algorithm is compared with the UA-based hybrid algorithm in the experimental domain. The proposed algorithm outperforms the hybrid one in terms of efficiency. version:1
arxiv-1304-3192 | Rotational Projection Statistics for 3D Local Surface Description and Object Recognition | http://arxiv.org/abs/1304.3192 | id:1304.3192 author:Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, Jianwei Wan category:cs.CV I.4; I.5.4  published:2013-04-11 summary:Recognizing 3D objects in the presence of noise, varying mesh resolution, occlusion and clutter is a very challenging task. This paper presents a novel method named Rotational Projection Statistics (RoPS). It has three major modules: Local Reference Frame (LRF) definition, RoPS feature description and 3D object recognition. We propose a novel technique to define the LRF by calculating the scatter matrix of all points lying on the local surface. RoPS feature descriptors are obtained by rotationally projecting the neighboring points of a feature point onto 2D planes and calculating a set of statistics (including low-order central moments and entropy) of the distribution of these projected points. Using the proposed LRF and RoPS descriptor, we present a hierarchical 3D object recognition algorithm. The performance of the proposed LRF, RoPS descriptor and object recognition algorithm was rigorously tested on a number of popular and publicly available datasets. Our proposed techniques exhibited superior performance compared to existing techniques. We also showed that our method is robust with respect to noise and varying mesh resolution. Our RoPS based algorithm achieved recognition rates of 100%, 98.9%, 95.4% and 96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' Foscari Venezia Datasets. version:1
arxiv-1304-3138 | Sustainable Cooperative Coevolution with a Multi-Armed Bandit | http://arxiv.org/abs/1304.3138 | id:1304.3138 author:François-Michel De Rainville, Michèle Sebag, Christian Gagné, Marc Schoenauer, Denis Laurendeau category:cs.NE I.2.8  published:2013-04-10 summary:This paper proposes a self-adaptation mechanism to manage the resources allocated to the different species comprising a cooperative coevolutionary algorithm. The proposed approach relies on a dynamic extension to the well-known multi-armed bandit framework. At each iteration, the dynamic multi-armed bandit makes a decision on which species to evolve for a generation, using the history of progress made by the different species to guide the decisions. We show experimentally, on a benchmark and a real-world problem, that evolving the different populations at different paces allows not only to identify solutions more rapidly, but also improves the capacity of cooperative coevolution to solve more complex problems. version:1
arxiv-1301-3457 | A Geometric Descriptor for Cell-Division Detection | http://arxiv.org/abs/1301.3457 | id:1301.3457 author:Marcelo Cicconet, Italo Lima, Davi Geiger, Kris Gunsalus category:cs.CV  published:2013-01-15 summary:We describe a method for cell-division detection based on a geometric-driven descriptor that can be represented as a 5-layers processing network, based mainly on wavelet filtering and a test for mirror symmetry between pairs of pixels. After the centroids of the descriptors are computed for a sequence of frames, the two-steps piecewise constant function that best fits the sequence of centroids determines the frame where the division occurs. version:2
arxiv-1211-7045 | Orientation Determination from Cryo-EM images Using Least Unsquared Deviation | http://arxiv.org/abs/1211.7045 | id:1211.7045 author:Lanhui Wang, Amit Singer, Zaiwen Wen category:cs.LG math.NA math.OC q-bio.BM  published:2012-11-29 summary:A major challenge in single particle reconstruction from cryo-electron microscopy is to establish a reliable ab-initio three-dimensional model using two-dimensional projection images with unknown orientations. Common-lines based methods estimate the orientations without additional geometric information. However, such methods fail when the detection rate of common-lines is too low due to the high level of noise in the images. An approximation to the least squares global self consistency error was obtained using convex relaxation by semidefinite programming. In this paper we introduce a more robust global self consistency error and show that the corresponding optimization problem can be solved via semidefinite relaxation. In order to prevent artificial clustering of the estimated viewing directions, we further introduce a spectral norm term that is added as a constraint or as a regularization term to the relaxed minimization problem. The resulted problems are solved by using either the alternating direction method of multipliers or an iteratively reweighted least squares procedure. Numerical experiments with both simulated and real images demonstrate that the proposed methods significantly reduce the orientation estimation error when the detection rate of common-lines is low. version:2
arxiv-1304-2888 | Roborobo! a Fast Robot Simulator for Swarm and Collective Robotics | http://arxiv.org/abs/1304.2888 | id:1304.2888 author:Nicolas Bredeche, Jean-Marc Montanier, Berend Weel, Evert Haasdijk category:cs.RO cs.AI cs.NE  published:2013-04-10 summary:Roborobo! is a multi-platform, highly portable, robot simulator for large-scale collective robotics experiments. Roborobo! is coded in C++, and follows the KISS guideline ("Keep it simple"). Therefore, its external dependency is solely limited to the widely available SDL library for fast 2D Graphics. Roborobo! is based on a Khepera/ePuck model. It is targeted for fast single and multi-robots simulation, and has already been used in more than a dozen published research mainly concerned with evolutionary swarm robotics, including environment-driven self-adaptation and distributed evolutionary optimization, as well as online onboard embodied evolution and embodied morphogenesis. version:1
arxiv-1206-1529 | Sparse projections onto the simplex | http://arxiv.org/abs/1206.1529 | id:1206.1529 author:Anastasios Kyrillidis, Stephen Becker, Volkan Cevher and, Christoph Koch category:cs.LG stat.ML  published:2012-06-07 summary:Most learning methods with rank or sparsity constraints use convex relaxations, which lead to optimization with the nuclear norm or the $\ell_1$-norm. However, several important learning applications cannot benefit from this approach as they feature these convex norms as constraints in addition to the non-convex rank and sparsity constraints. In this setting, we derive efficient sparse projections onto the simplex and its extension, and illustrate how to use them to solve high-dimensional learning problems in quantum tomography, sparse density estimation and portfolio selection with non-convex constraints. version:5
arxiv-1304-2865 | The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New DCF | http://arxiv.org/abs/1304.2865 | id:1304.2865 author:Niko Brümmer, Edward de Villiers category:stat.AP cs.LG stat.ML  published:2013-04-10 summary:The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10, relative to the 'old DCF' evaluation criterion, posed a difficult challenge for participants and evaluator alike. Initially, participants were at a loss as to how to calibrate their systems, while the evaluator underestimated the required number of evaluation trials. After the fact, it is now obvious that both calibration and evaluation require very large sets of trials. This poses the challenges of (i) how to decide what number of trials is enough, and (ii) how to process such large data sets with reasonable memory and CPU requirements. After SRE'10, at the BOSARIS Workshop, we built solutions to these problems into the freely available BOSARIS Toolkit. This paper explains the principles and algorithms behind this toolkit. The main contributions of the toolkit are: 1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratio calibration over a wide range of DCF operating points. These plots also help in judging the adequacy of the sizes of calibration and evaluation databases. 2. Efficient algorithms to compute DCF and minDCF for large score files, over the range of operating points required by these plots. 3. A new score file format, which facilitates working with very large trial lists. 4. A faster logistic regression optimizer for fusion and calibration. 5. A principled way to define EER (equal error rate), which is of practical interest when the absolute error count is small. version:1
arxiv-1303-2378 | Predictive Correlation Screening: Application to Two-stage Predictor Design in High Dimension | http://arxiv.org/abs/1303.2378 | id:1303.2378 author:Hamed Firouzi, Bala Rajaratnam, Alfred Hero category:stat.ML  published:2013-03-10 summary:We introduce a new approach to variable selection, called Predictive Correlation Screening, for predictor design. Predictive Correlation Screening (PCS) implements false positive control on the selected variables, is well suited to small sample sizes, and is scalable to high dimensions. We establish asymptotic bounds for Familywise Error Rate (FWER), and resultant mean square error of a linear predictor on the selected variables. We apply Predictive Correlation Screening to the following two-stage predictor design problem. An experimenter wants to learn a multivariate predictor of gene expressions based on successive biological samples assayed on mRNA arrays. She assays the whole genome on a few samples and from these assays she selects a small number of variables using Predictive Correlation Screening. To reduce assay cost, she subsequently assays only the selected variables on the remaining samples, to learn the predictor coefficients. We show superiority of Predictive Correlation Screening relative to LASSO and correlation learning (sometimes popularly referred to in the literature as marginal regression or simple thresholding) in terms of performance and computational complexity. version:2
arxiv-1304-2683 | Image Classification by Feature Dimension Reduction and Graph based Ranking | http://arxiv.org/abs/1304.2683 | id:1304.2683 author:Yao Nan, Qian Feng, Sun Zuolei category:cs.CV  published:2013-04-09 summary:Dimensionality reduction (DR) of image features plays an important role in image retrieval and classification tasks. Recently, two types of methods have been proposed to improve the both the accuracy and efficiency for the dimensionality reduction problem. One uses Non-negative matrix factorization (NMF) to describe the image distribution on the space of base matrix. Another one for dimension reduction trains a subspace projection matrix to project original data space into some low-dimensional subspaces which have deep architecture, so that the low-dimensional codes would be learned. At the same time, the graph based similarity learning algorithm which tries to exploit contextual information for improving the effectiveness of image rankings is also proposed for image class and retrieval problem. In this paper, after above two methods mentioned are utilized to reduce the high-dimensional features of images respectively, we learn the graph based similarity for the image classification problem. This paper compares the proposed approach with other approaches on an image database. version:1
arxiv-1304-1995 | Image Retrieval using Histogram Factorization and Contextual Similarity Learning | http://arxiv.org/abs/1304.1995 | id:1304.1995 author:Liu Liang category:cs.CV cs.DB cs.LG  published:2013-04-07 summary:Image retrieval has been a top topic in the field of both computer vision and machine learning for a long time. Content based image retrieval, which tries to retrieve images from a database visually similar to a query image, has attracted much attention. Two most important issues of image retrieval are the representation and ranking of the images. Recently, bag-of-words based method has shown its power as a representation method. Moreover, nonnegative matrix factorization is also a popular way to represent the data samples. In addition, contextual similarity learning has also been studied and proven to be an effective method for the ranking problem. However, these technologies have never been used together. In this paper, we developed an effective image retrieval system by representing each image using the bag-of-words method as histograms, and then apply the nonnegative matrix factorization to factorize the histograms, and finally learn the ranking score using the contextual similarity learning method. The proposed novel system is evaluated on a large scale image database and the effectiveness is shown. version:2
arxiv-1304-2545 | For Solving Linear Equations Recombination is a Needless Operation in Time-Variant Adaptive Hybrid Algorithms | http://arxiv.org/abs/1304.2545 | id:1304.2545 author:A. R. M. Jalal Uddin Jamali, Mohammad Arif Hossain, G. M. Moniruzzaman, M. M. A. Hashem category:cs.NE cs.NA  published:2013-04-09 summary:Recently hybrid evolutionary computation (EC) techniques are successfully implemented for solving large sets of linear equations. All the recently developed hybrid evolutionary algorithms, for solving linear equations, contain both the recombination and the mutation operations. In this paper, two modified hybrid evolutionary algorithms contained time-variant adaptive evolutionary technique are proposed for solving linear equations in which recombination operation is absent. The effectiveness of the recombination operator has been studied for the time-variant adaptive hybrid algorithms for solving large set of linear equations. Several experiments have been carried out using both the proposed modified hybrid evolutionary algorithms (in which the recombination operation is absent) and corresponding existing hybrid algorithms (in which the recombination operation is present) to solve large set of linear equations. It is found that the number of generations required by the existing hybrid algorithms (i.e. the Gauss-Seidel-SR based time variant adaptive (GSBTVA) hybrid algorithm and the Jacobi-SR based time variant adaptive (JBTVA) hybrid algorithm) and modified hybrid algorithms (i.e. the modified Gauss-Seidel-SR based time variant adaptive (MGSBTVA) hybrid algorithm and the modified Jacobi-SR based time variant adaptive (MJBTVA) hybrid algorithm) are comparable. Also the proposed modified algorithms require less amount of computational time in comparison to the corresponding existing hybrid algorithms. As the proposed modified hybrid algorithms do not contain recombination operation, so they require less computational effort, and also they are more efficient, effective and easy to implement. version:1
arxiv-1304-2543 | A New Distributed Evolutionary Computation Technique for Multi-Objective Optimization | http://arxiv.org/abs/1304.2543 | id:1304.2543 author:Md. Asadul Islam, G. M. Mashrur-E-Elahi, M. M. A. Hashem category:cs.NE  published:2013-04-09 summary:Now-a-days, it is important to find out solutions of Multi-Objective Optimization Problems (MOPs). Evolutionary Strategy helps to solve such real world problems efficiently and quickly. But sequential Evolutionary Algorithms (EAs) require an enormous computation power to solve such problems and it takes much time to solve large problems. To enhance the performance for solving this type of problems, this paper presents a new Distributed Novel Evolutionary Strategy Algorithm (DNESA) for Multi-Objective Optimization. The proposed DNESA applies the divide-and-conquer approach to decompose population into smaller sub-population and involves multiple solutions in the form of cooperative sub-populations. In DNESA, the server distributes the total computation load to all associate clients and simulation results show that the time for solving large problems is much less than sequential EAs. Also DNESA shows better performance in convergence test when compared with other three well-known EAs. version:1
arxiv-1304-2490 | Kernel Reconstruction ICA for Sparse Representation | http://arxiv.org/abs/1304.2490 | id:1304.2490 author:Yanhui Xiao, Zhenfeng Zhu, Yao Zhao category:cs.CV cs.LG  published:2013-04-09 summary:Independent Component Analysis (ICA) is an effective unsupervised tool to learn statistically independent representation. However, ICA is not only sensitive to whitening but also difficult to learn an over-complete basis. Consequently, ICA with soft Reconstruction cost(RICA) was presented to learn sparse representations with over-complete basis even on unwhitened data. Whereas RICA is infeasible to represent the data with nonlinear structure due to its intrinsic linearity. In addition, RICA is essentially an unsupervised method and can not utilize the class information. In this paper, we propose a kernel ICA model with reconstruction constraint (kRICA) to capture the nonlinear features. To bring in the class information, we further extend the unsupervised kRICA to a supervised one by introducing a discrimination constraint, namely d-kRICA. This constraint leads to learn a structured basis consisted of basis vectors from different basis subsets corresponding to different class labels. Then each subset will sparsely represent well for its own class but not for the others. Furthermore, data samples belonging to the same class will have similar representations, and thereby the learned sparse representations can take more discriminative power. Experimental results validate the effectiveness of kRICA and d-kRICA for image classification. version:1
arxiv-1304-2476 | Corpus-based Web Document Summarization using Statistical and Linguistic Approach | http://arxiv.org/abs/1304.2476 | id:1304.2476 author:Rushdi Shams, M. M. A. Hashem, Afrina Hossain, Suraiya Rumana Akter, Monika Gope category:cs.IR cs.CL  published:2013-04-09 summary:Single document summarization generates summary by extracting the representative sentences from the document. In this paper, we presented a novel technique for summarization of domain-specific text from a single web document that uses statistical and linguistic analysis on the text in a reference corpus and the web document. The proposed summarizer uses the combinational function of Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of a sentence, where SW is the function of number of terms (t_n) and number of words (w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is the function of t_n and w_n in a subject, and t_f in the corpus. 30 percent of the ranked sentences are considered to be the summary of the web document. We generated three web document summaries using our technique and compared each of them with the summaries developed manually from 16 different human subjects. Results showed that 68 percent of the summaries produced by our approach satisfy the manual summaries. version:1
arxiv-1304-2467 | Evolutionary Design of Digital Circuits Using Genetic Programming | http://arxiv.org/abs/1304.2467 | id:1304.2467 author:S. M. Ashik Eftekhar, Sk. Mahbub Habib, M. M. A. Hashem category:cs.NE  published:2013-04-09 summary:For simple digital circuits, conventional method of designing circuits can easily be applied. But for complex digital circuits, the conventional method of designing circuits is not fruitfully applicable because it is time-consuming. On the contrary, Genetic Programming is used mostly for automatic program generation. The modern approach for designing Arithmetic circuits, commonly digital circuits, is based on Graphs. This graph-based evolutionary design of arithmetic circuits is a method of optimized designing of arithmetic circuits. In this paper, a new technique for evolutionary design of digital circuits is proposed using Genetic Programming (GP) with Subtree Mutation in place of Graph-based design. The results obtained using this technique demonstrates the potential capability of genetic programming in digital circuit design with limited computer algorithms. The proposed technique, helps to simplify and speed up the process of designing digital circuits, discovers a variation in the field of digital circuit design where optimized digital circuits can be successfully and effectively designed. version:1
arxiv-1109-3250 | Convergence of latent mixing measures in finite and infinite mixture models | http://arxiv.org/abs/1109.3250 | id:1109.3250 author:XuanLong Nguyen category:math.ST stat.ML stat.TH  published:2011-09-15 summary:This paper studies convergence behavior of latent mixing measures that arise in finite and infinite mixture models, using transportation distances (i.e., Wasserstein metrics). The relationship between Wasserstein distances on the space of mixing measures and f-divergence functionals such as Hellinger and Kullback-Leibler distances on the space of mixture distributions is investigated in detail using various identifiability conditions. Convergence in Wasserstein metrics for discrete measures implies convergence of individual atoms that provide support for the measures, thereby providing a natural interpretation of convergence of clusters in clustering applications where mixture models are typically employed. Convergence rates of posterior distributions for latent mixing measures are established, for both finite mixtures of multivariate distributions and infinite mixtures based on the Dirichlet process. version:5
arxiv-1301-3224 | Efficient Learning of Domain-invariant Image Representations | http://arxiv.org/abs/1301.3224 | id:1301.3224 author:Judy Hoffman, Erik Rodner, Jeff Donahue, Trevor Darrell, Kate Saenko category:cs.LG  published:2013-01-15 summary:We present an algorithm that learns representations which explicitly compensate for domain mismatch and which can be efficiently realized as linear classifiers. Specifically, we form a linear transformation that maps features from the target (test) domain to the source (training) domain as part of training the classifier. We optimize both the transformation and classifier parameters jointly, and introduce an efficient cost function based on misclassification loss. Our method combines several features previously unavailable in a single algorithm: multi-class adaptation through representation learning, ability to map across heterogeneous feature spaces, and scalability to large datasets. We present experiments on several image datasets that demonstrate improved accuracy and computational advantages compared to previous approaches. version:5
arxiv-1304-2331 | The PAV algorithm optimizes binary proper scoring rules | http://arxiv.org/abs/1304.2331 | id:1304.2331 author:Niko Brummer, Johan du Preez category:stat.AP cs.LG stat.ML  published:2013-04-08 summary:There has been much recent interest in application of the pool-adjacent-violators (PAV) algorithm for the purpose of calibrating the probabilistic outputs of automatic pattern recognition and machine learning algorithms. Special cost functions, known as proper scoring rules form natural objective functions to judge the goodness of such calibration. We show that for binary pattern classifiers, the non-parametric optimization of calibration, subject to a monotonicity constraint, can be solved by PAV and that this solution is optimal for all regular binary proper scoring rules. This extends previous results which were limited to convex binary proper scoring rules. We further show that this result holds not only for calibration of probabilities, but also for calibration of log-likelihood-ratios, in which case optimality holds independently of the prior probabilities of the pattern classes. version:1
arxiv-1304-2302 | ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process Mixtures | http://arxiv.org/abs/1304.2302 | id:1304.2302 author:Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka category:stat.ML cs.DC cs.LG  published:2013-04-08 summary:The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian nonparametric modeling, and is widely used in tasks such as density estimation, natural language processing, and time series modeling. Although MCMC inference methods for the DP often provide a gold standard in terms asymptotic accuracy, they can be computationally expensive and are not obviously parallelizable. We propose a reparameterization of the Dirichlet process that induces conditional independencies between the atoms that form the random measure. This conditional independence enables many of the Markov chain transition operators for DP inference to be simulated in parallel across multiple cores. Applied to mixture modeling, our approach enables the Dirichlet process to simultaneously learn clusters that describe the data and superclusters that define the granularity of parallelization. Unlike previous approaches, our technique does not require alteration of the model and leaves the true posterior distribution invariant. It also naturally lends itself to a distributed software implementation in terms of Map-Reduce, which we test in cluster configurations of over 50 machines and 100 cores. We present experiments exploring the parallel efficiency and convergence properties of our approach on both synthetic and real-world data, including runs on 1MM data vectors in 256 dimensions. version:1
