arxiv-1212-1918 | Condensés de textes par des méthodes numériques | http://arxiv.org/abs/1212.1918 | id:1212.1918 author:Juan-Manuel Torres-Moreno, Patricia Velázquez-Morales, Jean-Guy Meunier category:cs.IR cs.CL  published:2012-12-09 summary:Since information in electronic form is already a standard, and that the variety and the quantity of information become increasingly large, the methods of summarizing or automatic condensation of texts is a critical phase of the analysis of texts. This article describes CORTEX a system based on numerical methods, which allows obtaining a condensation of a text, which is independent of the topic and of the length of the text. The structure of the system enables it to find the abstracts in French or Spanish in very short times. version:1
arxiv-1212-1863 | Self Authentication of image through Daubechies Transform technique (SADT) | http://arxiv.org/abs/1212.1863 | id:1212.1863 author:Madhumita Sengupta, J. K. Mandal category:cs.CR cs.CV  published:2012-12-09 summary:In this paper a 4 x 4 Daubechies transform based authentication technique termed as SADT has been proposed to authenticate gray scale images. The cover image is transformed into the frequency domain using 4 x 4 mask in a row major order using Daubechies transform technique, resulting four frequency subbands AF, HF, VF and DF. One byte of every band in a mask is embedding with two or four bits of secret information. Experimental results are computed and compared with the existing authentication techniques like Li s method [5], SCDFT [6], Region-Based method [7] and other similar techniques based on Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR) and Image Fidelity (IF), which shows better performance in SADT. version:1
arxiv-1212-1780 | An Empirical Comparison of V-fold Penalisation and Cross Validation for Model Selection in Distribution-Free Regression | http://arxiv.org/abs/1212.1780 | id:1212.1780 author:Charanpal Dhanjal, Nicolas Baskiotis, Stéphan Clémençon, Nicolas Usunier category:stat.ML  published:2012-12-08 summary:Model selection is a crucial issue in machine-learning and a wide variety of penalisation methods (with possibly data dependent complexity penalties) have recently been introduced for this purpose. However their empirical performance is generally not well documented in the literature. It is the goal of this paper to investigate to which extent such recent techniques can be successfully used for the tuning of both the regularisation and kernel parameters in support vector regression (SVR) and the complexity measure in regression trees (CART). This task is traditionally solved via V-fold cross-validation (VFCV), which gives efficient results for a reasonable computational cost. A disadvantage however of VFCV is that the procedure is known to provide an asymptotically suboptimal risk estimate as the number of examples tends to infinity. Recently, a penalisation procedure called V-fold penalisation has been proposed to improve on VFCV, supported by theoretical arguments. Here we report on an extensive set of experiments comparing V-fold penalisation and VFCV for SVR/CART calibration on several benchmark datasets. We highlight cases in which VFCV and V-fold penalisation provide poor estimates of the risk respectively and introduce a modified penalisation technique to reduce the estimation error. version:1
arxiv-1212-1752 | Hybrid Optimized Back propagation Learning Algorithm For Multi-layer Perceptron | http://arxiv.org/abs/1212.1752 | id:1212.1752 author:Mriganka Chakraborty, Arka Ghosh category:cs.NE  published:2012-12-08 summary:Standard neural network based on general back propagation learning using delta method or gradient descent method has some great faults like poor optimization of error-weight objective function, low learning rate, instability .This paper introduces a hybrid supervised back propagation learning algorithm which uses trust-region method of unconstrained optimization of the error objective function by using quasi-newton method .This optimization leads to more accurate weight update system for minimizing the learning error during learning phase of multi-layer perceptron.[13][14][15] In this paper augmented line search is used for finding points which satisfies Wolfe condition. In this paper, This hybrid back propagation algorithm has strong global convergence properties & is robust & efficient in practice. version:1
arxiv-1212-1709 | Evolution of the most common English words and phrases over the centuries | http://arxiv.org/abs/1212.1709 | id:1212.1709 author:Matjaz Perc category:physics.soc-ph cs.CL cs.DL  published:2012-12-07 summary:By determining which were the most common English words and phrases since the beginning of the 16th century, we obtain a unique large-scale view of the evolution of written text. We find that the most common words and phrases in any given year had a much shorter popularity lifespan in the 16th than they had in the 20th century. By measuring how their usage propagated across the years, we show that for the past two centuries the process has been governed by linear preferential attachment. Along with the steady growth of the English lexicon, this provides an empirical explanation for the ubiquity of the Zipf's law in language statistics and confirms that writing, although undoubtedly an expression of art and skill, is not immune to the same influences of self-organization that are known to regulate processes as diverse as the making of new friends and World Wide Web growth. version:1
arxiv-1106-3571 | ANOVA kernels and RKHS of zero mean functions for model-based sensitivity analysis | http://arxiv.org/abs/1106.3571 | id:1106.3571 author:Nicolas Durrande, David Ginsbourger, Olivier Roustant, Laurent Carraro category:stat.ML  published:2011-06-17 summary:Given a reproducing kernel Hilbert space H of real-valued functions and a suitable measure mu over the source space D (subset of R), we decompose H as the sum of a subspace of centered functions for mu and its orthogonal in H. This decomposition leads to a special case of ANOVA kernels, for which the functional ANOVA representation of the best predictor can be elegantly derived, either in an interpolation or regularization framework. The proposed kernels appear to be particularly convenient for analyzing the e ffect of each (group of) variable(s) and computing sensitivity indices without recursivity. version:2
arxiv-1208-5062 | Changepoint detection for high-dimensional time series with missing data | http://arxiv.org/abs/1208.5062 | id:1208.5062 author:Yao Xie, Jiaji Huang, Rebecca Willett category:stat.ML cs.LG  published:2012-08-24 summary:This paper describes a novel approach to change-point detection when the observed high-dimensional data may have missing elements. The performance of classical methods for change-point detection typically scales poorly with the dimensionality of the data, so that a large number of observations are collected after the true change-point before it can be reliably detected. Furthermore, missing components in the observed data handicap conventional approaches. The proposed method addresses these challenges by modeling the dynamic distribution underlying the data as lying close to a time-varying low-dimensional submanifold embedded within the ambient observation space. Specifically, streaming data is used to track a submanifold approximation, measure deviations from this approximation, and calculate a series of statistics of the deviations for detecting when the underlying manifold has changed in a sharp or unexpected manner. The approach described in this paper leverages several recent results in the field of high-dimensional data analysis, including subspace tracking with missing data, multiscale analysis techniques for point clouds, online optimization, and change-point detection performance analysis. Simulations and experiments highlight the robustness and efficacy of the proposed approach in detecting an abrupt change in an otherwise slowly varying low-dimensional manifold. version:3
arxiv-1212-1192 | Using external sources of bilingual information for on-the-fly word alignment | http://arxiv.org/abs/1212.1192 | id:1212.1192 author:Miquel Esplà-Gomis, Felipe Sánchez-Martínez, Mikel L. Forcada category:cs.CL I.2.7  published:2012-12-05 summary:In this paper we present a new and simple language-independent method for word-alignment based on the use of external sources of bilingual information such as machine translation systems. We show that the few parameters of the aligner can be trained on a very small corpus, which leads to results comparable to those obtained by the state-of-the-art tool GIZA++ in terms of precision. Regarding other metrics, such as alignment error rate or F-measure, the parametric aligner, when trained on a very small gold-standard (450 pairs of sentences), provides results comparable to those produced by GIZA++ when trained on an in-domain corpus of around 10,000 pairs of sentences. Furthermore, the results obtained indicate that the training is domain-independent, which enables the use of the trained aligner 'on the fly' on any new pair of sentences. version:2
arxiv-1212-1478 | The Clustering of Author's Texts of English Fiction in the Vector Space of Semantic Fields | http://arxiv.org/abs/1212.1478 | id:1212.1478 author:Bohdan Pavlyshenko category:cs.CL cs.DL cs.IR  published:2012-12-06 summary:The clustering of text documents in the vector space of semantic fields and in the semantic space with orthogonal basis has been analysed. It is shown that using the vector space model with the basis of semantic fields is effective in the cluster analysis algorithms of author's texts in English fiction. The analysis of the author's texts distribution in cluster structure showed the presence of the areas of semantic space that represent the author's ideolects of individual authors. SVD factorization of the semantic fields matrix makes it possible to reduce significantly the dimension of the semantic space in the cluster analysis of author's texts. version:1
arxiv-1212-1329 | Automatic Detection of Texture Defects Using Texture-Periodicity and Gabor Wavelets | http://arxiv.org/abs/1212.1329 | id:1212.1329 author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.10  published:2012-12-06 summary:In this paper, we propose a machine vision algorithm for automatically detecting defects in textures belonging to 16 out of 17 wallpaper groups using texture-periodicity and a family of Gabor wavelets. Input defective images are subjected to Gabor wavelet transformation in multi-scales and multi-orientations and a resultant image is obtained in L2 norm. The resultant image is split into several periodic blocks and energy of each block is used as a feature space to automatically identify defective and defect-free blocks using Ward's hierarchical clustering. Experiments on defective fabric images of three major wallpaper groups, namely, pmm, p2 and p4m, show that the proposed method is robust in finding fabric defects without human intervention and can be used for automatic defect detection in fabric industries. version:1
arxiv-1212-1313 | Autonomous Navigation by Robust Scan Matching Technique | http://arxiv.org/abs/1212.1313 | id:1212.1313 author:Debajyoti Banerji, Ranjit Ray, Jhankar Basu, Indrajit Basak category:cs.CV cs.AI  published:2012-12-06 summary:For effective autonomous navigation,estimation of the pose of the robot is essential at every sampling time. For computing an accurate estimation,odometric error needs to be reduced with the help of data from external sensor. In this work, a technique has been developed for accurate pose estimation of mobile robot by using Laser Range data. The technique is robust to noisy data, which may contain considerable amount of outliers. A grey image is formed from laser range data and the key points from this image are extracted by Harris corner detector. The matching of the key points from consecutive data sets have been done while outliers have been rejected by RANSAC method. Robot state is measured by the correspondence between the two sets of keypoints. Finally, optimal robot state is estimated by Extended Kalman Filter. The technique has been applied to an operational robot in the laboratory environment to show the robustness of the technique in presence of noisy sensor data. The performance of this new technique has been compared with that of conventional ICP method. Through this method, effective and accurate navigation has been achieved even in presence of substantial noise in the sensor data at the cost of a small amount of additional computational complexity. version:1
arxiv-1208-4290 | A Learning Theoretic Approach to Energy Harvesting Communication System Optimization | http://arxiv.org/abs/1208.4290 | id:1208.4290 author:Pol Blasco, Deniz Gündüz, Mischa Dohler category:cs.LG cs.NI  published:2012-08-21 summary:A point-to-point wireless communication system in which the transmitter is equipped with an energy harvesting device and a rechargeable battery, is studied. Both the energy and the data arrivals at the transmitter are modeled as Markov processes. Delay-limited communication is considered assuming that the underlying channel is block fading with memory, and the instantaneous channel state information is available at both the transmitter and the receiver. The expected total transmitted data during the transmitter's activation time is maximized under three different sets of assumptions regarding the information available at the transmitter about the underlying stochastic processes. A learning theoretic approach is introduced, which does not assume any a priori information on the Markov processes governing the communication system. In addition, online and offline optimization problems are studied for the same setting. Full statistical knowledge and causal information on the realizations of the underlying stochastic processes are assumed in the online optimization problem, while the offline optimization problem assumes non-causal knowledge of the realizations in advance. Comparing the optimal solutions in all three frameworks, the performance loss due to the lack of the transmitter's information regarding the behaviors of the underlying Markov processes is quantified. version:2
arxiv-1212-1263 | On the probabilistic continuous complexity conjecture | http://arxiv.org/abs/1212.1263 | id:1212.1263 author:Mark A. Kon category:stat.ML  published:2012-12-06 summary:In this paper we prove the probabilistic continuous complexity conjecture. In continuous complexity theory, this states that the complexity of solving a continuous problem with probability approaching 1 converges (in this limit) to the complexity of solving the same problem in its worst case. We prove the conjecture holds if and only if space of problem elements is uniformly convex. The non-uniformly convex case has a striking counterexample in the problem of identifying a Brownian path in Wiener space, where it is shown that probabilistic complexity converges to only half of the worst case complexity in this limit. version:1
arxiv-1212-1180 | On Some Integrated Approaches to Inference | http://arxiv.org/abs/1212.1180 | id:1212.1180 author:Mark A. Kon, Leszek Plaskota category:stat.ML cs.LG  published:2012-12-05 summary:We present arguments for the formulation of unified approach to different standard continuous inference methods from partial information. It is claimed that an explicit partition of information into a priori (prior knowledge) and a posteriori information (data) is an important way of standardizing inference approaches so that they can be compared on a normative scale, and so that notions of optimal algorithms become farther-reaching. The inference methods considered include neural network approaches, information-based complexity, and Monte Carlo, spline, and regularization methods. The model is an extension of currently used continuous complexity models, with a class of algorithms in the form of optimization methods, in which an optimization functional (involving the data) is minimized. This extends the family of current approaches in continuous complexity theory, which include the use of interpolatory algorithms in worst and average case settings. version:1
arxiv-1212-1143 | Multiscale Markov Decision Problems: Compression, Solution, and Transfer Learning | http://arxiv.org/abs/1212.1143 | id:1212.1143 author:Jake Bouvrie, Mauro Maggioni category:cs.AI cs.SY math.OC stat.ML  published:2012-12-05 summary:Many problems in sequential decision making and stochastic control often have natural multiscale structure: sub-tasks are assembled together to accomplish complex goals. Systematically inferring and leveraging hierarchical structure, particularly beyond a single level of abstraction, has remained a longstanding challenge. We describe a fast multiscale procedure for repeatedly compressing, or homogenizing, Markov decision processes (MDPs), wherein a hierarchy of sub-problems at different scales is automatically determined. Coarsened MDPs are themselves independent, deterministic MDPs, and may be solved using existing algorithms. The multiscale representation delivered by this procedure decouples sub-tasks from each other and can lead to substantial improvements in convergence rates both locally within sub-problems and globally across sub-problems, yielding significant computational savings. A second fundamental aspect of this work is that these multiscale decompositions yield new transfer opportunities across different problems, where solutions of sub-tasks at different levels of the hierarchy may be amenable to transfer to new problems. Localized transfer of policies and potential operators at arbitrary scales is emphasized. Finally, we demonstrate compression and transfer in a collection of illustrative domains, including examples involving discrete and continuous statespaces. version:1
arxiv-1212-1131 | Using Wikipedia to Boost SVD Recommender Systems | http://arxiv.org/abs/1212.1131 | id:1212.1131 author:Gilad Katz, Guy Shani, Bracha Shapira, Lior Rokach category:cs.LG cs.IR stat.ML  published:2012-12-05 summary:Singular Value Decomposition (SVD) has been used successfully in recent years in the area of recommender systems. In this paper we present how this model can be extended to consider both user ratings and information from Wikipedia. By mapping items to Wikipedia pages and quantifying their similarity, we are able to use this information in order to improve recommendation accuracy, especially when the sparsity is high. Another advantage of the proposed approach is the fact that it can be easily integrated into any other SVD implementation, regardless of additional parameters that may have been added to it. Preliminary experimental results on the MovieLens dataset are encouraging. version:1
arxiv-1212-1100 | Making Early Predictions of the Accuracy of Machine Learning Applications | http://arxiv.org/abs/1212.1100 | id:1212.1100 author:J. E. Smith, P. Caleb-Solly, M. A. Tahir, D. Sannen, H. van-Brussel category:cs.LG cs.AI stat.ML I.2.6; I.5.2  published:2012-12-05 summary:The accuracy of machine learning systems is a widely studied research topic. Established techniques such as cross-validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set. However, they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy. In this paper we investigate techniques for making such early predictions. We note that when a machine learning algorithm is presented with a training set the classifier produced, and hence its error, will depend on the characteristics of the algorithm, on training set's size, and also on its specific composition. In particular we hypothesise that if a number of classifiers are produced, and their observed error is decomposed into bias and variance terms, then although these components may behave differently, their behaviour may be predictable. We test our hypothesis by building models that, given a measurement taken from the classifier created from a limited number of samples, predict the values that would be measured from the classifier produced when the full data set is presented. We create separate models for bias, variance and total error. Our models are built from the results of applying ten different machine learning algorithms to a range of data sets, and tested with "unseen" algorithms and datasets. We analyse the results for various numbers of initial training samples, and total dataset sizes. Results show that our predictions are very highly correlated with the values observed after undertaking the extra training. Finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained, and show how we can accurately estimate an upper bound on the accuracy achievable after further training. version:1
arxiv-1202-0302 | Kernels on Sample Sets via Nonparametric Divergence Estimates | http://arxiv.org/abs/1202.0302 | id:1202.0302 author:Dougal J. Sutherland, Liang Xiong, Barnabás Póczos, Jeff Schneider category:cs.LG stat.ML  published:2012-02-01 summary:Most machine learning algorithms, such as classification or regression, treat the individual data point as the object of interest. Here we consider extending machine learning algorithms to operate on groups of data points. We suggest treating a group of data points as an i.i.d. sample set from an underlying feature distribution for that group. Our approach employs kernel machines with a kernel on i.i.d. sample sets of vectors. We define certain kernel functions on pairs of distributions, and then use a nonparametric estimator to consistently estimate those functions based on sample sets. The projection of the estimated Gram matrix to the cone of symmetric positive semi-definite matrices enables us to use kernel machines for classification, regression, anomaly detection, and low-dimensional embedding in the space of distributions. We present several numerical experiments both on real and simulated datasets to demonstrate the advantages of our new approach. version:2
arxiv-1212-0967 | Compiling Relational Database Schemata into Probabilistic Graphical Models | http://arxiv.org/abs/1212.0967 | id:1212.0967 author:Sameer Singh, Thore Graepel category:cs.AI cs.DB cs.LG stat.ML  published:2012-12-05 summary:Instead of requiring a domain expert to specify the probabilistic dependencies of the data, in this work we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for columns, latent variables that cluster the data, and factors that reflect and represent the foreign key links. Experiments demonstrate the accuracy of the model and the scalability of inference on synthetic and real-world data. version:1
arxiv-1206-4822 | Feature extraction in protein sequences classification : a new stability measure | http://arxiv.org/abs/1206.4822 | id:1206.4822 author:Rabie Saidi, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu Nguifo category:cs.LG cs.CE q-bio.QM  published:2012-06-21 summary:Feature extraction is an unavoidable task, especially in the critical step of preprocessing biological sequences. This step consists for example in transforming the biological sequences into vectors of motifs where each motif is a subsequence that can be seen as a property (or attribute) characterizing the sequence. Hence, we obtain an object-property table where objects are sequences and properties are motifs extracted from sequences. This output can be used to apply standard machine learning tools to perform data mining tasks such as classification. Several previous works have described feature extraction methods for bio-sequence classification, but none of them discussed the robustness of these methods when perturbing the input data. In this work, we introduce the notion of stability of the generated motifs in order to study the robustness of motif extraction methods. We express this robustness in terms of the ability of the method to reveal any change occurring in the input data and also its ability to target the interesting motifs. We use these criteria to evaluate and experimentally compare four existing extraction methods for biological sequences. version:3
arxiv-1212-0960 | Evaluating Classifiers Without Expert Labels | http://arxiv.org/abs/1212.0960 | id:1212.0960 author:Hyun Joon Jung, Matthew Lease category:cs.LG cs.IR stat.ML  published:2012-12-05 summary:This paper considers the challenge of evaluating a set of classifiers, as done in shared task evaluations like the KDD Cup or NIST TREC, without expert labels. While expert labels provide the traditional cornerstone for evaluating statistical learners, limited or expensive access to experts represents a practical bottleneck. Instead, we seek methodology for estimating performance of the classifiers which is more scalable than expert labeling yet preserves high correlation with evaluation based on expert labels. We consider both: 1) using only labels automatically generated by the classifiers (blind evaluation); and 2) using labels obtained via crowdsourcing. While crowdsourcing methods are lauded for scalability, using such data for evaluation raises serious concerns given the prevalence of label noise. In regard to blind evaluation, two broad strategies are investigated: combine & score and score & combine methods infer a single pseudo-gold label set by aggregating classifier labels; classifiers are then evaluated based on this single pseudo-gold label set. On the other hand, score & combine methods: 1) sample multiple label sets from classifier outputs, 2) evaluate classifiers on each label set, and 3) average classifier performance across label sets. When additional crowd labels are also collected, we investigate two alternative avenues for exploiting them: 1) direct evaluation of classifiers; or 2) supervision of combine & score methods. To assess generality of our techniques, classifier performance is measured using four common classification metrics, with statistical significance tests. Finally, we measure both score and rank correlations between estimated classifier performance vs. actual performance according to expert judgments. Rigorous evaluation of classifiers from the TREC 2011 Crowdsourcing Track shows reliable evaluation can be achieved without reliance on expert labels. version:1
arxiv-1212-0945 | Multiclass Diffuse Interface Models for Semi-Supervised Learning on Graphs | http://arxiv.org/abs/1212.0945 | id:1212.0945 author:Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus category:stat.ML cs.LG math.ST physics.data-an stat.TH I.5.3  published:2012-12-05 summary:We present a graph-based variational algorithm for multiclass classification of high-dimensional data, motivated by total variation techniques. The energy functional is based on a diffuse interface model with a periodic potential. We augment the model by introducing an alternative measure of smoothness that preserves symmetry among the class labels. Through this modification of the standard Laplacian, we construct an efficient multiclass method that allows for sharp transitions between classes. The experimental results demonstrate that our approach is competitive with the state of the art among other graph-based algorithms. version:1
arxiv-1212-0912 | Sparse seismic imaging using variable projection | http://arxiv.org/abs/1212.0912 | id:1212.0912 author:Aleksandr Y. Aravkin, Tristan van Leeuwen, Ning Tu category:math.OC stat.ML 65K05  65K10  86-08  published:2012-12-05 summary:We consider an important class of signal processing problems where the signal of interest is known to be sparse, and can be recovered from data given auxiliary information about how the data was generated. For example, a sparse Green's function may be recovered from seismic experimental data using sparsity optimization when the source signature is known. Unfortunately, in practice this information is often missing, and must be recovered from data along with the signal using deconvolution techniques. In this paper, we present a novel methodology to simultaneously solve for the sparse signal and auxiliary parameters using a recently proposed variable projection technique. Our main contribution is to combine variable projection with sparsity promoting optimization, obtaining an efficient algorithm for large-scale sparse deconvolution problems. We demonstrate the algorithm on a seismic imaging example. version:1
arxiv-1109-3843 | Fast approximation of matrix coherence and statistical leverage | http://arxiv.org/abs/1109.3843 | id:1109.3843 author:Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, David P. Woodruff category:cs.DS cs.DM cs.LG  published:2011-09-18 summary:The statistical leverage scores of a matrix $A$ are the squared row-norms of the matrix containing its (top) left singular vectors and the coherence is the largest leverage score. These quantities are of interest in recently-popular problems such as matrix completion and Nystr\"{o}m-based low-rank matrix approximation as well as in large-scale statistical data analysis applications more generally; moreover, they are of interest since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms. Our main result is a randomized algorithm that takes as input an arbitrary $n \times d$ matrix $A$, with $n \gg d$, and that returns as output relative-error approximations to all $n$ of the statistical leverage scores. The proposed algorithm runs (under assumptions on the precise values of $n$ and $d$) in $O(n d \log n)$ time, as opposed to the $O(nd^2)$ time required by the na\"{i}ve algorithm that involves computing an orthogonal basis for the range of $A$. Our analysis may be viewed in terms of computing a relative-error approximation to an underconstrained least-squares approximation problem, or, relatedly, it may be viewed as an application of Johnson-Lindenstrauss type ideas. Several practically-important extensions of our basic result are also described, including the approximation of so-called cross-leverage scores, the extension of these ideas to matrices with $n \approx d$, and the extension to streaming environments. version:2
arxiv-1212-0888 | Unmixing of Hyperspectral Data Using Robust Statistics-based NMF | http://arxiv.org/abs/1212.0888 | id:1212.0888 author:Roozbeh Rajabi, Hassan Ghassemian category:cs.CV  published:2012-12-04 summary:Mixed pixels are presented in hyperspectral images due to low spatial resolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixels spectra into endmembers spectra and abundance fractions. In this paper using of robust statistics-based nonnegative matrix factorization (RNMF) for spectral unmixing of hyperspectral data is investigated. RNMF uses a robust cost function and iterative updating procedure, so is not sensitive to outliers. This method has been applied to simulated data using USGS spectral library, AVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMF method based on SAD and AAD measures. Results demonstrate that this method can be used efficiently for hyperspectral unmixing purposes. version:1
arxiv-1206-5162 | Fast Variational Inference in the Conjugate Exponential Family | http://arxiv.org/abs/1206.5162 | id:1206.5162 author:James Hensman, Magnus Rattray, Neil D. Lawrence category:cs.LG stat.ML  published:2012-06-22 summary:We present a general method for deriving collapsed variational inference algo- rithms for probabilistic models in the conjugate exponential family. Our method unifies many existing approaches to collapsed variational inference. Our collapsed variational inference leads to a new lower bound on the marginal likelihood. We exploit the information geometry of the bound to derive much faster optimization methods based on conjugate gradients for these models. Our approach is very general and is easily applied to any model where the mean field update equations have been derived. Empirically we show significant speed-ups for probabilistic models optimized using our bound. version:2
arxiv-1212-0819 | A Topological Code for Plane Images | http://arxiv.org/abs/1212.0819 | id:1212.0819 author:Evgeny Shchepin category:cs.CV math.GT  published:2012-12-04 summary:It is proposed a new code for contours of plane images. This code was applied for optical character recognition of printed and handwritten characters. One can apply it to recognition of any visual images. version:1
arxiv-1211-6248 | A simple non-parametric Topic Mixture for Authors and Documents | http://arxiv.org/abs/1211.6248 | id:1211.6248 author:Arnim Bleier category:cs.LG stat.ML  published:2012-11-27 summary:This article reviews the Author-Topic Model and presents a new non-parametric extension based on the Hierarchical Dirichlet Process. The extension is especially suitable when no prior information about the number of components necessary is available. A blocked Gibbs sampler is described and focus put on staying as close as possible to the original model with only the minimum of theoretical and implementation overhead necessary. version:2
arxiv-1212-0695 | Training Support Vector Machines Using Frank-Wolfe Optimization Methods | http://arxiv.org/abs/1212.0695 | id:1212.0695 author:Emanuele Frandi, Ricardo Nanculef, Maria Grazia Gasparo, Stefano Lodi, Claudio Sartori category:cs.LG cs.CV math.OC stat.ML  published:2012-12-04 summary:Training a Support Vector Machine (SVM) requires the solution of a quadratic programming problem (QP) whose computational complexity becomes prohibitively expensive for large scale datasets. Traditional optimization methods cannot be directly applied in these cases, mainly due to memory restrictions. By adopting a slightly different objective function and under mild conditions on the kernel used within the model, efficient algorithms to train SVMs have been devised under the name of Core Vector Machines (CVMs). This framework exploits the equivalence of the resulting learning problem with the task of building a Minimal Enclosing Ball (MEB) problem in a feature space, where data is implicitly embedded by a kernel function. In this paper, we improve on the CVM approach by proposing two novel methods to build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast method to approximate the solution of a MEB problem. In contrast to CVMs, our algorithms do not require to compute the solutions of a sequence of increasingly complex QPs and are defined by using only analytic optimization steps. Experiments on a large collection of datasets show that our methods scale better than CVMs in most cases, sometimes at the price of a slightly lower accuracy. As CVMs, the proposed methods can be easily extended to machine learning problems other than binary classification. However, effective classifiers are also obtained using kernels which do not satisfy the condition required by CVMs and can thus be used for a wider set of problems. version:1
arxiv-1008-1566 | Separate Training for Conditional Random Fields Using Co-occurrence Rate Factorization | http://arxiv.org/abs/1008.1566 | id:1008.1566 author:Zhemin Zhu, Djoerd Hiemstra, Peter Apers, Andreas Wombacher category:cs.LG cs.AI  published:2010-08-09 summary:The standard training method of Conditional Random Fields (CRFs) is very slow for large-scale applications. As an alternative, piecewise training divides the full graph into pieces, trains them independently, and combines the learned weights at test time. In this paper, we present \emph{separate} training for undirected models based on the novel Co-occurrence Rate Factorization (CR-F). Separate training is a local training method. In contrast to MEMMs, separate training is unaffected by the label bias problem. Experiments show that separate training (i) is unaffected by the label bias problem; (ii) reduces the training time from weeks to seconds; and (iii) obtains competitive results to the standard and piecewise training on linear-chain CRFs. version:5
arxiv-1212-0639 | Evaluation of Particle Swarm Optimization Algorithms for Weighted Max-Sat Problem: Technical Report | http://arxiv.org/abs/1212.0639 | id:1212.0639 author:Osama Khalil category:cs.NE  published:2012-12-04 summary:An experimental evaluation is conducted to asses the performance of 4 different Particle Swarm Optimization neighborhood structures in solving Max-Sat problem. The experiment has shown that none of the algorithms achieves statistically significant performance over the others under confidence level of 0.05. version:1
arxiv-1112-0918 | On best subset regression | http://arxiv.org/abs/1112.0918 | id:1112.0918 author:Shifeng Xiong category:stat.ME stat.CO stat.ML 62J07  62F12 G.3  published:2011-12-05 summary:In this paper we discuss the variable selection method from \ell0-norm constrained regression, which is equivalent to the problem of finding the best subset of a fixed size. Our study focuses on two aspects, consistency and computation. We prove that the sparse estimator from such a method can retain all of the important variables asymptotically for even exponentially growing dimensionality under regularity conditions. This indicates that the best subset regression method can efficiently shrink the full model down to a submodel of a size less than the sample size, which can be analyzed by well-developed regression techniques for such cases in a follow-up study. We provide an iterative algorithm, called orthogonalizing subset selection (OSS), to address computational issues in best subset regression. OSS is an EM algorithm, and thus possesses the monotonicity property. For any sparse estimator, OSS can improve its fit of the model by putting it as an initial point. After this improvement, the sparsity of the estimator is kept. Another appealing feature of OSS is that, similarly to an effective algorithm for a continuous optimization problem, OSS can converge to the global solution to the \ell0-norm constrained regression problem if the initial point lies in a neighborhood of the global solution. An accelerating algorithm of OSS and its combination with forward stepwise selection are also investigated. Simulations and a real example are presented to evaluate the performances of the proposed methods. version:2
arxiv-1212-0467 | Low-rank Matrix Completion using Alternating Minimization | http://arxiv.org/abs/1212.0467 | id:1212.0467 author:Prateek Jain, Praneeth Netrapalli, Sujay Sanghavi category:stat.ML cs.LG math.OC  published:2012-12-03 summary:Alternating minimization represents a widely applicable and empirically successful approach for finding low-rank matrices that best fit the given data. For example, for the problem of low-rank matrix completion, this method is believed to be one of the most accurate and efficient, and formed a major component of the winning entry in the Netflix Challenge. In the alternating minimization approach, the low-rank target matrix is written in a bi-linear form, i.e. $X = UV^\dag$; the algorithm then alternates between finding the best $U$ and the best $V$. Typically, each alternating step in isolation is convex and tractable. However the overall problem becomes non-convex and there has been almost no theoretical understanding of when this approach yields a good result. In this paper we present first theoretical analysis of the performance of alternating minimization for matrix completion, and the related problem of matrix sensing. For both these problems, celebrated recent results have shown that they become well-posed and tractable once certain (now standard) conditions are imposed on the problem. We show that alternating minimization also succeeds under similar conditions. Moreover, compared to existing results, our paper shows that alternating minimization guarantees faster (in particular, geometric) convergence to the true matrix, while allowing a simpler analysis. version:1
arxiv-1212-0463 | Time series forecasting: model evaluation and selection using nonparametric risk bounds | http://arxiv.org/abs/1212.0463 | id:1212.0463 author:Daniel J. McDonald, Cosma Rohilla Shalizi, Mark Schervish category:math.ST cs.LG stat.ML stat.TH  published:2012-12-03 summary:We derive generalization error bounds --- bounds on the expected inaccuracy of the predictions --- for traditional time series forecasting models. Our results hold for many standard forecasting tools including autoregressive models, moving average models, and, more generally, linear state-space models. These bounds allow forecasters to select among competing models and to guarantee that with high probability, their chosen model will perform well without making strong assumptions about the data generating process or appealing to asymptotic theory. We motivate our techniques with and apply them to standard economic and financial forecasting tools --- a GARCH model for predicting equity volatility and a dynamic stochastic general equilibrium model (DSGE), the standard tool in macroeconomic forecasting. We demonstrate in particular how our techniques can aid forecasters and policy makers in choosing models which behave well under uncertainty and mis-specification. version:1
arxiv-1212-0433 | Compressive Schlieren Deflectometry | http://arxiv.org/abs/1212.0433 | id:1212.0433 author:Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc Joannes category:cs.CV  published:2012-12-03 summary:Schlieren deflectometry aims at characterizing the deflections undergone by refracted incident light rays at any surface point of a transparent object. For smooth surfaces, each surface location is actually associated with a sparse deflection map (or spectrum). This paper presents a novel method to compressively acquire and reconstruct such spectra. This is achieved by altering the way deflection information is captured in a common Schlieren Deflectometer, i.e., the deflection spectra are indirectly observed by the principle of spread spectrum compressed sensing. These observations are realized optically using a 2-D Spatial Light Modulator (SLM) adjusted to the corresponding sensing basis and whose modulations encode the light deviation subsequently recorded by a CCD camera. The efficiency of this approach is demonstrated experimentally on the observation of few test objects. Further, using a simple parametrization of the deflection spectra we show that relevant key parameters can be directly computed using the measurements, avoiding full reconstruction. version:1
arxiv-1212-0402 | UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild | http://arxiv.org/abs/1212.0402 | id:1212.0402 author:Khurram Soomro, Amir Roshan Zamir, Mubarak Shah category:cs.CV  published:2012-12-03 summary:We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips. version:1
arxiv-1212-0388 | Hypergraph and protein function prediction with gene expression data | http://arxiv.org/abs/1212.0388 | id:1212.0388 author:Loc Tran category:stat.ML cs.LG q-bio.QM G.2.2  published:2012-12-03 summary:Most network-based protein (or gene) function prediction methods are based on the assumption that the labels of two adjacent proteins in the network are likely to be the same. However, assuming the pairwise relationship between proteins or genes is not complete, the information a group of genes that show very similar patterns of expression and tend to have similar functions (i.e. the functional modules) is missed. The natural way overcoming the information loss of the above assumption is to represent the gene expression data as the hypergraph. Thus, in this paper, the three un-normalized, random walk, and symmetric normalized hypergraph Laplacian based semi-supervised learning methods applied to hypergraph constructed from the gene expression data in order to predict the functions of yeast proteins are introduced. Experiment results show that the average accuracy performance measures of these three hypergraph Laplacian based semi-supervised learning methods are the same. However, their average accuracy performance measures of these three methods are much greater than the average accuracy performance measures of un-normalized graph Laplacian based semi-supervised learning method (i.e. the baseline method of this paper) applied to gene co-expression network created from the gene expression data. version:1
arxiv-1212-0383 | GLCM-based chi-square histogram distance for automatic detection of defects on patterned textures | http://arxiv.org/abs/1212.0383 | id:1212.0383 author:V. Asha, N. U. Bhajantri, P. Nagabhushan category:cs.CV I.0; I.2.10  published:2012-12-03 summary:Chi-square histogram distance is one of the distance measures that can be used to find dissimilarity between two histograms. Motivated by the fact that texture discrimination by human vision system is based on second-order statistics, we make use of histogram of gray-level co-occurrence matrix (GLCM) that is based on second-order statistics and propose a new machine vision algorithm for automatic defect detection on patterned textures. Input defective images are split into several periodic blocks and GLCMs are computed after quantizing the gray levels from 0-255 to 0-63 to keep the size of GLCM compact and to reduce computation time. Dissimilarity matrix derived from chi-square distances of the GLCMs is subjected to hierarchical clustering to automatically identify defective and defect-free blocks. Effectiveness of the proposed method is demonstrated through experiments on defective real-fabric images of 2 major wallpaper groups (pmm and p4m groups). version:1
arxiv-1212-0763 | Dynamic recommender system : using cluster-based biases to improve the accuracy of the predictions | http://arxiv.org/abs/1212.0763 | id:1212.0763 author:Modou Gueye, Talel Abdessalem, Hubert Naacke category:cs.LG cs.DB cs.IR H.2.8; H.3.3  published:2012-12-03 summary:It is today accepted that matrix factorization models allow a high quality of rating prediction in recommender systems. However, a major drawback of matrix factorization is its static nature that results in a progressive declining of the accuracy of the predictions after each factorization. This is due to the fact that the new obtained ratings are not taken into account until a new factorization is computed, which can not be done very often because of the high cost of matrix factorization. In this paper, aiming at improving the accuracy of recommender systems, we propose a cluster-based matrix factorization technique that enables online integration of new ratings. Thus, we significantly enhance the obtained predictions between two matrix factorizations. We use finer-grained user biases by clustering similar items into groups, and allocating in these groups a bias to each user. The experiments we did on large datasets demonstrated the efficiency of our approach. version:1
arxiv-1212-0318 | Comparison of Fuzzy and Neuro Fuzzy Image Fusion Techniques and its Applications | http://arxiv.org/abs/1212.0318 | id:1212.0318 author:D. Srinivasa Rao, M. Seetha, M. H. M. Krishna Prasad category:cs.CV  published:2012-12-03 summary:Image fusion is the process of integrating multiple images of the same scene into a single fused image to reduce uncertainty and minimizing redundancy while extracting all the useful information from the source images. Image fusion process is required for different applications like medical imaging, remote sensing, medical imaging, machine vision, biometrics and military applications where quality and critical information is required. In this paper, image fusion using fuzzy and neuro fuzzy logic approaches utilized to fuse images from different sensors, in order to enhance visualization. The proposed work further explores comparison between fuzzy based image fusion and neuro fuzzy fusion technique along with quality evaluation indices for image fusion like image quality index, mutual information measure, fusion factor, fusion symmetry, fusion index, root mean square error, peak signal to noise ratio, entropy, correlation coefficient and spatial frequency. Experimental results obtained from fusion process prove that the use of the neuro fuzzy based image fusion approach shows better performance in first two test cases while in the third test case fuzzy based image fusion technique gives better results. version:1
arxiv-1212-0291 | An Image Based Technique for Enhancement of Underwater Images | http://arxiv.org/abs/1212.0291 | id:1212.0291 author:C. J. Prabhakar, P. U. Praveen Kumar category:cs.CV  published:2012-12-03 summary:The underwater images usually suffers from non-uniform lighting, low contrast, blur and diminished colors. In this paper, we proposed an image based preprocessing technique to enhance the quality of the underwater images. The proposed technique comprises a combination of four filters such as homomorphic filtering, wavelet denoising, bilateral filter and contrast equalization. These filters are applied sequentially on degraded underwater images. The literature survey reveals that image based preprocessing algorithms uses standard filter techniques with various combinations. For smoothing the image, the image based preprocessing algorithms uses the anisotropic filter. The main drawback of the anisotropic filter is that iterative in nature and computation time is high compared to bilateral filter. In the proposed technique, in addition to other three filters, we employ a bilateral filter for smoothing the image. The experimentation is carried out in two stages. In the first stage, we have conducted various experiments on captured images and estimated optimal parameters for bilateral filter. Similarly, optimal filter bank and optimal wavelet shrinkage function are estimated for wavelet denoising. In the second stage, we conducted the experiments using estimated optimal parameters, optimal filter bank and optimal wavelet shrinkage function for evaluating the proposed technique. We evaluated the technique using quantitative based criteria such as a gradient magnitude histogram and Peak Signal to Noise Ratio (PSNR). Further, the results are qualitatively evaluated based on edge detection results. The proposed technique enhances the quality of the underwater images and can be employed prior to apply computer vision techniques. version:1
arxiv-1212-0229 | Simplification and integration in computing and cognition: the SP theory and the multiple alignment concept | http://arxiv.org/abs/1212.0229 | id:1212.0229 author:James Gerard Wolff category:cs.AI cs.CL  published:2012-12-02 summary:The main purpose of this article is to describe potential benefits and applications of the SP theory, a unique attempt to simplify and integrate ideas across artificial intelligence, mainstream computing and human cognition, with information compression as a unifying theme. The theory, including a concept of multiple alignment, combines conceptual simplicity with descriptive and explanatory power in several areas including representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, the storage and retrieval of information, planning and problem solving, unsupervised learning, information compression, and human perception and cognition. In the SP machine -- an expression of the SP theory which is currently realised in the form of computer models -- there is potential for an overall simplification of computing systems, including software. As a theory with a broad base of support, the SP theory promises useful insights in many areas and the integration of structures and functions, both within a given area and amongst different areas. There are potential benefits in natural language processing (with potential for the understanding and translation of natural languages), the need for a versatile intelligence in autonomous robots, computer vision, intelligent databases, maintaining multiple versions of documents or web pages, software engineering, criminal investigations, the management of big data and gaining benefits from it, the semantic web, medical diagnosis, the detection of computer viruses, the economical transmission of data, and data fusion. Further development of these ideas would be facilitated by the creation of a high-parallel, web-based, open-source version of the SP machine, with a good user interface. This would provide a means for researchers to explore what can be done with the system and to refine it. version:1
arxiv-1212-0220 | Metaheuristic Optimization: Algorithm Analysis and Open Problems | http://arxiv.org/abs/1212.0220 | id:1212.0220 author:Xin-She Yang category:math.OC cs.NE 90C26  published:2012-12-02 summary:Metaheuristic algorithms are becoming an important part of modern optimization. A wide range of metaheuristic algorithms have emerged over the last two decades, and many metaheuristics such as particle swarm optimization are becoming increasingly popular. Despite their popularity, mathematical analysis of these algorithms lacks behind. Convergence analysis still remains unsolved for the majority of metaheuristic algorithms, while efficiency analysis is equally challenging. In this paper, we intend to provide an overview of convergence and efficiency studies of metaheuristics, and try to provide a framework for analyzing metaheuristics in terms of convergence and efficiency. This can form a basis for analyzing other algorithms. We also outline some open questions as further research topics. version:1
arxiv-1212-0215 | Artificial Neural Network for Performance Modeling and Optimization of CMOS Analog Circuits | http://arxiv.org/abs/1212.0215 | id:1212.0215 author:Mriganka Chakraborty category:cs.NE  published:2012-12-02 summary:This paper presents an implementation of multilayer feed forward neural networks (NN) to optimize CMOS analog circuits. For modeling and design recently neural network computational modules have got acceptance as an unorthodox and useful tool. To achieve high performance of active or passive circuit component neural network can be trained accordingly. A well trained neural network can produce more accurate outcome depending on its learning capability. Neural network model can replace empirical modeling solutions limited by range and accuracy.[2] Neural network models are easy to obtain for new circuits or devices which can replace analytical methods. Numerical modeling methods can also be replaced by neural network model due to their computationally expansive behavior.[2][10][20]. The pro- posed implementation is aimed at reducing resource requirement, without much compromise on the speed. The NN ensures proper functioning by assigning the appropriate inputs, weights, biases, and excitation function of the layer that is currently being computed. The concept used is shown to be very effective in reducing resource requirements and enhancing speed. version:1
arxiv-1212-0171 | Message-Passing Algorithms for Quadratic Minimization | http://arxiv.org/abs/1212.0171 | id:1212.0171 author:Nicholas Ruozzi, Sekhar Tatikonda category:cs.IT cs.LG math.IT stat.ML  published:2012-12-02 summary:Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive definite quadratic function. Sufficient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive definite quadratic function. As was observed in previous work, the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive definite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive definite whenever the input matrix is positive definite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges. version:1
arxiv-1212-0170 | An Evolution Strategy Approach toward Rule-set Generation for Network Intrusion Detection Systems (IDS) | http://arxiv.org/abs/1212.0170 | id:1212.0170 author:Herve Kabamba Mbikayi category:cs.CR cs.NE  published:2012-12-01 summary:With the increasing number of intrusions in system and network infrastructures, Intrusion Detection Systems (IDS) have become an active area of research to develop reliable and effective solutions to detect and counter them. The use of Evolutionary Algorithms in IDS has proved its maturity over the times. Although most of the research works have been based on the use of genetic algorithms in IDS, this paper presents an approach toward the generation of rules for the identification of anomalous connections using evolution strategies . The emphasis is given on how the problem can be modeled into ES primitives and how the fitness of the population can be evaluated in order to find the local optima, therefore resulting in optimal rules that can be used for detecting intrusions in intrusion detection systems. version:1
arxiv-1207-3859 | Approximate Message Passing with Consistent Parameter Estimation and Applications to Sparse Learning | http://arxiv.org/abs/1207.3859 | id:1207.3859 author:Ulugbek S. Kamilov, Sundeep Rangan, Alyson K. Fletcher, Michael Unser category:cs.IT cs.LG math.IT  published:2012-07-17 summary:We consider the estimation of an i.i.d. (possibly non-Gaussian) vector $\xbf \in \R^n$ from measurements $\ybf \in \R^m$ obtained by a general cascade model consisting of a known linear transform followed by a probabilistic componentwise (possibly nonlinear) measurement channel. A novel method, called adaptive generalized approximate message passing (Adaptive GAMP), that enables joint learning of the statistics of the prior and measurement channel along with estimation of the unknown vector $\xbf$ is presented. The proposed algorithm is a generalization of a recently-developed EM-GAMP that uses expectation-maximization (EM) iterations where the posteriors in the E-steps are computed via approximate message passing. The methodology can be applied to a large class of learning problems including the learning of sparse priors in compressed sensing or identification of linear-nonlinear cascade models in dynamical systems and neural spiking processes. We prove that for large i.i.d. Gaussian transform matrices the asymptotic componentwise behavior of the adaptive GAMP algorithm is predicted by a simple set of scalar state evolution equations. In addition, we show that when a certain maximum-likelihood estimation can be performed in each step, the adaptive GAMP method can yield asymptotically consistent parameter estimates, which implies that the algorithm achieves a reconstruction quality equivalent to the oracle algorithm that knows the correct parameter values. Remarkably, this result applies to essentially arbitrary parametrizations of the unknown distributions, including ones that are nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides a systematic, general and computationally efficient method applicable to a large range of complex linear-nonlinear models with provable guarantees. version:3
arxiv-1212-0139 | Cumulative Step-size Adaptation on Linear Functions | http://arxiv.org/abs/1212.0139 | id:1212.0139 author:Alexandre Chotard, Anne Auger, Nikolaus Hansen category:cs.LG stat.ML  published:2012-12-01 summary:The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing functions with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied. version:1
arxiv-1212-0134 | Fingertip Detection: A Fast Method with Natural Hand | http://arxiv.org/abs/1212.0134 | id:1212.0134 author:J. L. Raheja, Karen Das, Ankit Chaudhary category:cs.CV  published:2012-12-01 summary:Many vision based applications have used fingertips to track or manipulate gestures in their applications. Gesture identification is a natural way to pass the signals to the machine, as the human express its feelings most of the time with hand expressions. Here a novel time efficient algorithm has been described for fingertip detection. This method is invariant to hand direction and in preprocessing it cuts only hand part from the full image, hence further computation would be much faster than processing full image. Binary silhouette of the input image is generated using HSV color space based skin filter and hand cropping done based on intensity histogram of the hand image version:1
arxiv-1212-0083 | From the decoding of cortical activities to the control of a JACO robotic arm: a whole processing chain | http://arxiv.org/abs/1212.0083 | id:1212.0083 author:Laurent Bougrain, Olivier Rochel, Octave Boussaton, Lionel Havet category:cs.NE cs.HC cs.RO q-bio.NC  published:2012-12-01 summary:This paper presents a complete processing chain for decoding intracranial data recorded in the cortex of a monkey and replicates the associated movements on a JACO robotic arm by Kinova. We developed specific modules inside the OpenViBE platform in order to build a Brain-Machine Interface able to read the data, compute the position of the robotic finger and send this position to the robotic arm. More pre- cisely, two client/server protocols have been tested to transfer the finger positions: VRPN and a light protocol based on TCP/IP sockets. According to the requested finger position, the server calls the associ- ated functions of an API by Kinova to move the fin- gers properly. Finally, we monitor the gap between the requested and actual fingers positions. This chain can be generalized to any movement of the arm or wrist. version:1
arxiv-1212-0074 | Challenges in Kurdish Text Processing | http://arxiv.org/abs/1212.0074 | id:1212.0074 author:Kyumars Sheykh Esmaili category:cs.IR cs.CL  published:2012-12-01 summary:Despite having a large number of speakers, the Kurdish language is among the less-resourced languages. In this work we highlight the challenges and problems in providing the required tools and techniques for processing texts written in Kurdish. From a high-level perspective, the main challenges are: the inherent diversity of the language, standardization and segmentation issues, and the lack of language resources. version:1
arxiv-1212-0059 | Artificial Neural Network Fuzzy Inference System (ANFIS) For Brain Tumor Detection | http://arxiv.org/abs/1212.0059 | id:1212.0059 author:Minakshi Sharma category:cs.CV cs.AI  published:2012-12-01 summary:Detection and segmentation of Brain tumor is very important because it provides anatomical information of normal and abnormal tissues which helps in treatment planning and patient follow-up. There are number of techniques for image segmentation. Proposed research work uses ANFIS (Artificial Neural Network Fuzzy Inference System) for image classification and then compares the results with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includes benefits of both ANN and the fuzzy logic systems. A comprehensive feature set and fuzzy rules are selected to classify an abnormal image to the corresponding tumor type. Experimental results illustrate promising results in terms of classification accuracy. A comparative analysis is performed with the FCM and K-NN to show the superior nature of ANFIS systems. version:1
arxiv-1108-4146 | Simulation-based optimal Bayesian experimental design for nonlinear systems | http://arxiv.org/abs/1108.4146 | id:1108.4146 author:Xun Huan, Youssef M. Marzouk category:stat.ML stat.CO stat.ME  published:2011-08-20 summary:The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters. Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter estimation problems arising in detailed combustion kinetics. version:3
arxiv-1212-0042 | Secure voice based authentication for mobile devices: Vaulted Voice Verification | http://arxiv.org/abs/1212.0042 | id:1212.0042 author:R. C. Johnson, Walter J. Scheirer, Terrance E. Boult category:cs.CR cs.CV  published:2012-11-30 summary:As the use of biometrics becomes more wide-spread, the privacy concerns that stem from the use of biometrics are becoming more apparent. As the usage of mobile devices grows, so does the desire to implement biometric identification into such devices. A large majority of mobile devices being used are mobile phones. While work is being done to implement different types of biometrics into mobile phones, such as photo based biometrics, voice is a more natural choice. The idea of voice as a biometric identifier has been around a long time. One of the major concerns with using voice as an identifier is the instability of voice. We have developed a protocol that addresses those instabilities and preserves privacy. This paper describes a novel protocol that allows a user to authenticate using voice on a mobile/remote device without compromising their privacy. We first discuss the \vv protocol, which has recently been introduced in research literature, and then describe its limitations. We then introduce a novel adaptation and extension of the vaulted verification protocol to voice, dubbed $V^3$. Following that we show a performance evaluation and then conclude with a discussion of security and future work. version:1
arxiv-1212-0030 | Viewpoint Invariant Object Detector | http://arxiv.org/abs/1212.0030 | id:1212.0030 author:Osama Khalil, Andrew Habib category:cs.CV  published:2012-11-30 summary:Object Detection is the task of identifying the existence of an object class instance and locating it within an image. Difficulties in handling high intra-class variations constitute major obstacles to achieving high performance on standard benchmark datasets (scale, viewpoint, lighting conditions and orientation variations provide good examples). Suggested model aims at providing more robustness to detecting objects suffering severe distortion due to < 60{\deg} viewpoint changes. In addition, several model computational bottlenecks have been resolved leading to a significant increase in the model performance (speed and space) without compromising the resulting accuracy. Finally, we produced two illustrative applications showing the potential of the object detection technology being deployed in real life applications; namely content-based image search and content-based video search. version:1
arxiv-1211-7369 | Approximate Rank-Detecting Factorization of Low-Rank Tensors | http://arxiv.org/abs/1211.7369 | id:1211.7369 author:Franz J. Király, Andreas Ziehe category:stat.ML cs.LG math.NA  published:2012-11-30 summary:We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3 tensor and calculates its factorization into rank-one components. We provide generative conditions for the algorithm to work and demonstrate on both synthetic and real world data that AROFAC2 is a potentially outperforming alternative to the gold standard PARAFAC over which it has the advantages that it can intrinsically detect the true rank, avoids spurious components, and is stable with respect to outliers and non-Gaussian noise. version:1
arxiv-1211-7219 | A recursive divide-and-conquer approach for sparse principal component analysis | http://arxiv.org/abs/1211.7219 | id:1211.7219 author:Qian Zhao, Deyu Meng, Zongben Xu category:cs.CV cs.LG stat.ML 62H25  68T10 I.5.0; I.5.1  published:2012-11-30 summary:In this paper, a new method is proposed for sparse PCA based on the recursive divide-and-conquer methodology. The main idea is to separate the original sparse PCA problem into a series of much simpler sub-problems, each having a closed-form solution. By recursively solving these sub-problems in an analytical way, an efficient algorithm is constructed to solve the sparse PCA problem. The algorithm only involves simple computations and is thus easy to implement. The proposed method can also be very easily extended to other sparse PCA problems with certain constraints, such as the nonnegative sparse PCA problem. Furthermore, we have shown that the proposed algorithm converges to a stationary point of the problem, and its computational complexity is approximately linear in both data size and dimensionality. The effectiveness of the proposed method is substantiated by extensive experiments implemented on a series of synthetic and real data in both reconstruction-error-minimization and data-variance-maximization viewpoints. version:1
arxiv-1211-7200 | Using Differential Evolution for the Graph Coloring | http://arxiv.org/abs/1211.7200 | id:1211.7200 author:Iztok Fister, Janez Brest category:math.CO cs.NE  published:2012-11-30 summary:Differential evolution was developed for reliable and versatile function optimization. It has also become interesting for other domains because of its ease to use. In this paper, we posed the question of whether differential evolution can also be used by solving of the combinatorial optimization problems, and in particular, for the graph coloring problem. Therefore, a hybrid self-adaptive differential evolution algorithm for graph coloring was proposed that is comparable with the best heuristics for graph coloring today, i.e. Tabucol of Hertz and de Werra and the hybrid evolutionary algorithm of Galinier and Hao. We have focused on the graph 3-coloring. Therefore, the evolutionary algorithm with method SAW of Eiben et al., which achieved excellent results for this kind of graphs, was also incorporated into this study. The extensive experiments show that the differential evolution could become a competitive tool for the solving of graph coloring problem in the future. version:1
arxiv-1211-7184 | Erratum: Simplified Drift Analysis for Proving Lower Bounds in Evolutionary Computation | http://arxiv.org/abs/1211.7184 | id:1211.7184 author:Pietro S. Oliveto, Carsten Witt category:cs.NE F.2.0  published:2012-11-30 summary:This erratum points out an error in the simplified drift theorem (SDT) [Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modification of one of its conditions is sufficient to establish a valid result. In many respects, the new theorem is more general than before. We no longer assume a Markov process nor a finite search space. Furthermore, the proof of the theorem is more compact than the previous ones. Finally, previous applications of the SDT are revisited. It turns out that all of these either meet the modified condition directly or by means of few additional arguments. version:1
arxiv-1211-7180 | Multislice Modularity Optimization in Community Detection and Image Segmentation | http://arxiv.org/abs/1211.7180 | id:1211.7180 author:Huiyi Hu, Yves van Gennip, Blake Hunter, Mason A. Porter, Andrea L. Bertozzi category:cs.SI cs.CV physics.data-an physics.soc-ph  published:2012-11-30 summary:Because networks can be used to represent many complex systems, they have attracted considerable attention in physics, computer science, sociology, and many other disciplines. One of the most important areas of network science is the algorithmic detection of cohesive groups (i.e., "communities") of nodes. In this paper, we algorithmically detect communities in social networks and image data by optimizing multislice modularity. A key advantage of modularity optimization is that it does not require prior knowledge of the number or sizes of communities, and it is capable of finding network partitions that are composed of communities of different sizes. By optimizing multislice modularity and subsequently calculating diagnostics on the resulting network partitions, it is thereby possible to obtain information about network structure across multiple system scales. We illustrate this method on data from both social networks and images, and we find that optimization of multislice modularity performs well on these two tasks without the need for extensive problem-specific adaptation. However, improving the computational speed of this method remains a challenging open problem. version:1
arxiv-1112-1450 | A recursive procedure for density estimation on the binary hypercube | http://arxiv.org/abs/1112.1450 | id:1112.1450 author:Maxim Raginsky, Jorge Silva, Svetlana Lazebnik, Rebecca Willett category:math.ST stat.ML stat.TH  published:2011-12-07 summary:This paper describes a recursive estimation procedure for multivariate binary densities (probability distributions of vectors of Bernoulli random variables) using orthogonal expansions. For $d$ covariates, there are $2^d$ basis coefficients to estimate, which renders conventional approaches computationally prohibitive when $d$ is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error for moderate sample sizes, and (2) the computational complexity is lower for sparser densities. Our method also allows for flexible control of the trade-off between mean-squared error and computational complexity. version:2
arxiv-1211-7120 | Exact and Efficient Parallel Inference for Nonparametric Mixture Models | http://arxiv.org/abs/1211.7120 | id:1211.7120 author:Sinead A. Williamson, Avinava Dubey, Eric P. Xing category:stat.ML  published:2012-11-29 summary:Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to sample from the true posterior in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods. version:1
arxiv-1211-7102 | SVD Based Image Processing Applications: State of The Art, Contributions and Research Challenges | http://arxiv.org/abs/1211.7102 | id:1211.7102 author:Rowayda A. Sadek category:cs.CV cs.MM  published:2012-11-29 summary:Singular Value Decomposition (SVD) has recently emerged as a new paradigm for processing different types of images. SVD is an attractive algebraic transform for image processing applications. The paper proposes an experimental survey for the SVD as an efficient transform in image processing applications. Despite the well-known fact that SVD offers attractive properties in imaging, the exploring of using its properties in various image applications is currently at its infancy. Since the SVD has many attractive properties have not been utilized, this paper contributes in using these generous properties in newly image applications and gives a highly recommendation for more research challenges. In this paper, the SVD properties for images are experimentally presented to be utilized in developing new SVD-based image processing applications. The paper offers survey on the developed SVD based image applications. The paper also proposes some new contributions that were originated from SVD properties analysis in different image processing. The aim of this paper is to provide a better understanding of the SVD in image processing and identify important various applications and open research directions in this increasingly important area; SVD based image processing in the future research. version:1
arxiv-1203-2990 | Evolving Culture vs Local Minima | http://arxiv.org/abs/1203.2990 | id:1203.2990 author:Yoshua Bengio category:cs.LG cs.AI I.2.6  published:2012-03-14 summary:We propose a theory that relates difficulty of learning in deep architectures to culture and language. It is articulated around the following hypotheses: (1) learning in an individual human brain is hampered by the presence of effective local minima; (2) this optimization difficulty is particularly important when it comes to learning higher-level abstractions, i.e., concepts that cover a vast and highly-nonlinear span of sensory configurations; (3) such high-level abstractions are best represented in brains by the composition of many levels of representation, i.e., by deep architectures; (4) a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints or indirect supervision for these high-level abstractions; and (5), language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator, and this gives rise to rapid search in the space of communicable ideas that help humans build up better high-level internal representations of their world. These hypotheses put together imply that human culture and the evolution of ideas have been crucial to counter an optimization difficulty: this optimization difficulty would otherwise make it very difficult for human brains to capture high-level knowledge of the world. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks. Plausible consequences of this theory for the efficiency of cultural evolutions are sketched. version:2
arxiv-1207-4598 | Quick HyperVolume | http://arxiv.org/abs/1207.4598 | id:1207.4598 author:Luís M. S. Russo, Alexandre P. Francisco category:cs.DS cs.DM cs.NE  published:2012-07-19 summary:We present a new algorithm to calculate exact hypervolumes. Given a set of $d$-dimensional points, it computes the hypervolume of the dominated space. Determining this value is an important subroutine of Multiobjective Evolutionary Algorithms (MOEAs). We analyze the "Quick Hypervolume" (QHV) algorithm theoretically and experimentally. The theoretical results are a significant contribution to the current state of the art. Moreover the experimental performance is also very competitive, compared with existing exact hypervolume algorithms. A full description of the algorithm is currently submitted to IEEE Transactions on Evolutionary Computation. version:2
arxiv-1211-6971 | A New Automatic Method to Adjust Parameters for Object Recognition | http://arxiv.org/abs/1211.6971 | id:1211.6971 author:Issam Qaffou, Mohamed Sadgal, Aziz Elfazziki category:cs.CV cs.AI  published:2012-11-29 summary:To recognize an object in an image, the user must apply a combination of operators, where each operator has a set of parameters. These parameters must be well adjusted in order to reach good results. Usually, this adjustment is made manually by the user. In this paper we propose a new method to automate the process of parameter adjustment for an object recognition task. Our method is based on reinforcement learning, we use two types of agents: User Agent that gives the necessary information and Parameter Agent that adjusts the parameters of each operator. Due to the nature of reinforcement learning the results do not depend only on the system characteristics but also on the user favorite choices. version:1
arxiv-1211-6950 | Dynamic Network Cartography | http://arxiv.org/abs/1211.6950 | id:1211.6950 author:Gonzalo Mateos, Ketan Rajawat category:cs.NI cs.IT cs.MA math.IT stat.ML  published:2012-11-29 summary:Communication networks have evolved from specialized, research and tactical transmission systems to large-scale and highly complex interconnections of intelligent devices, increasingly becoming more commercial, consumer-oriented, and heterogeneous. Propelled by emergent social networking services and high-definition streaming platforms, network traffic has grown explosively thanks to the advances in processing speed and storage capacity of state-of-the-art communication technologies. As "netizens" demand a seamless networking experience that entails not only higher speeds, but also resilience and robustness to failures and malicious cyber-attacks, ample opportunities for signal processing (SP) research arise. The vision is for ubiquitous smart network devices to enable data-driven statistical learning algorithms for distributed, robust, and online network operation and management, adaptable to the dynamically-evolving network landscape with minimal need for human intervention. The present paper aims at delineating the analytical background and the relevance of SP tools to dynamic network monitoring, introducing the SP readership to the concept of dynamic network cartography -- a framework to construct maps of the dynamic network state in an efficient and scalable manner tailored to large-scale heterogeneous networks. version:1
arxiv-1211-6898 | On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes | http://arxiv.org/abs/1211.6898 | id:1211.6898 author:Bruno Scherrer, Boris Lesner category:cs.LG cs.AI  published:2012-11-29 summary:We consider infinite-horizon stationary $\gamma$-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error $\epsilon$ at each iteration, it is well-known that one can compute stationary policies that are $\frac{2\gamma}{(1-\gamma)^2}\epsilon$-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to $\frac{2\gamma}{1-\gamma}\epsilon$-optimal, which constitutes a significant improvement in the usual situation when $\gamma$ is close to 1. Surprisingly, this shows that the problem of "computing near-optimal non-stationary policies" is much simpler than that of "computing near-optimal stationary policies". version:1
arxiv-1211-6887 | Automating rule generation for grammar checkers | http://arxiv.org/abs/1211.6887 | id:1211.6887 author:Marcin Miłkowski category:cs.CL cs.LG  published:2012-11-29 summary:In this paper, I describe several approaches to automatic or semi-automatic development of symbolic rules for grammar checkers from the information contained in corpora. The rules obtained this way are an important addition to manually-created rules that seem to dominate in rule-based checkers. However, the manual process of creation of rules is costly, time-consuming and error-prone. It seems therefore advisable to use machine-learning algorithms to create the rules automatically or semi-automatically. The results obtained seem to corroborate my initial hypothesis that symbolic machine learning algorithms can be useful for acquiring new rules for grammar checking. It turns out, however, that for practical uses, error corpora cannot be the sole source of information used in grammar checking. I suggest therefore that only by using different approaches, grammar-checkers, or more generally, computer-aided proofreading tools, will be able to cover most frequent and severe mistakes and avoid false alarms that seem to distract users. version:1
arxiv-1211-6859 | Overlapping clustering based on kernel similarity metric | http://arxiv.org/abs/1211.6859 | id:1211.6859 author:Chiheb-Eddine Ben N'Cir, Nadia Essoussi, Patrice Bertrand category:stat.ML cs.LG stat.ME  published:2012-11-29 summary:Producing overlapping schemes is a major issue in clustering. Recent proposed overlapping methods relies on the search of an optimal covering and are based on different metrics, such as Euclidean distance and I-Divergence, used to measure closeness between observations. In this paper, we propose the use of another measure for overlapping clustering based on a kernel similarity metric .We also estimate the number of overlapped clusters using the Gram matrix. Experiments on both Iris and EachMovie datasets show the correctness of the estimation of number of clusters and show that measure based on kernel similarity metric improves the precision, recall and f-measure in overlapping clustering. version:1
arxiv-1211-6851 | Classification Recouvrante Basée sur les Méthodes à Noyau | http://arxiv.org/abs/1211.6851 | id:1211.6851 author:Chiheb-Eddine Ben N'Cir, Nadia Essoussi category:cs.LG stat.CO stat.ME stat.ML  published:2012-11-29 summary:Overlapping clustering problem is an important learning issue in which clusters are not mutually exclusive and each object may belongs simultaneously to several clusters. This paper presents a kernel based method that produces overlapping clusters on a high feature space using mercer kernel techniques to improve separability of input patterns. The proposed method, called OKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping $k$-means) method to produce overlapping schemes. Experiments are performed on overlapping dataset and empirical results obtained with OKM-K outperform results obtained with OKM. version:1
arxiv-1112-3166 | Higher-Order Momentum Distributions and Locally Affine LDDMM Registration | http://arxiv.org/abs/1112.3166 | id:1112.3166 author:Stefan Sommer, Mads Nielsen, Sune Darkner, Xavier Pennec category:cs.CV cs.NA  published:2011-12-14 summary:To achieve sparse parametrizations that allows intuitive analysis, we aim to represent deformation with a basis containing interpretable elements, and we wish to use elements that have the description capacity to represent the deformation compactly. To accomplish this, we introduce in this paper higher-order momentum distributions in the LDDMM registration framework. While the zeroth order moments previously used in LDDMM only describe local displacement, the first-order momenta that are proposed here represent a basis that allows local description of affine transformations and subsequent compact description of non-translational movement in a globally non-rigid deformation. The resulting representation contains directly interpretable information from both mathematical and modeling perspectives. We develop the mathematical construction of the registration framework with higher-order momenta, we show the implications for sparse image registration and deformation description, and we provide examples of how the parametrization enables registration with a very low number of parameters. The capacity and interpretability of the parametrization using higher-order momenta lead to natural modeling of articulated movement, and the method promises to be useful for quantifying ventricle expansion and progressing atrophy during Alzheimer's disease. version:2
arxiv-1211-6847 | Letter counting: a stem cell for Cryptology, Quantitative Linguistics, and Statistics | http://arxiv.org/abs/1211.6847 | id:1211.6847 author:Bernard Ycart category:math.HO cs.CL cs.CR  published:2012-11-29 summary:Counting letters in written texts is a very ancient practice. It has accompanied the development of Cryptology, Quantitative Linguistics, and Statistics. In Cryptology, counting frequencies of the different characters in an encrypted message is the basis of the so called frequency analysis method. In Quantitative Linguistics, the proportion of vowels to consonants in different languages was studied long before authorship attribution. In Statistics, the alternation vowel-consonants was the only example that Markov ever gave of his theory of chained events. A short history of letter counting is presented. The three domains, Cryptology, Quantitative Linguistics, and Statistics, are then examined, focusing on the interactions with the other two fields through letter counting. As a conclusion, the eclectism of past centuries scholars, their background in humanities, and their familiarity with cryptograms, are identified as contributing factors to the mutual enrichment process which is described here. version:1
arxiv-1211-6834 | On unbiased performance evaluation for protein inference | http://arxiv.org/abs/1211.6834 | id:1211.6834 author:Zengyou He, Ting Huang, Peijun Zhu category:stat.AP cs.LG q-bio.QM  published:2012-11-29 summary:This letter is a response to the comments of Serang (2012) on Huang and He (2012) in Bioinformatics. Serang (2012) claimed that the parameters for the Fido algorithm should be specified using the grid search method in Serang et al. (2010) so as to generate a deserved accuracy in performance comparison. It seems that it is an argument on parameter tuning. However, it is indeed the issue of how to conduct an unbiased performance evaluation for comparing different protein inference algorithms. In this letter, we would explain why we don't use the grid search for parameter selection in Huang and He (2012) and show that this procedure may result in an over-estimated performance that is unfair to competing algorithms. In fact, this issue has also been pointed out by Li and Radivojac (2012). version:1
arxiv-1211-6727 | Graph Laplacians on Singular Manifolds: Toward understanding complex spaces: graph Laplacians on manifolds with singularities and boundaries | http://arxiv.org/abs/1211.6727 | id:1211.6727 author:Mikhail Belkin, Qichao Que, Yusu Wang, Xueyuan Zhou category:cs.AI cs.CG cs.LG  published:2012-11-28 summary:Recently, much of the existing work in manifold learning has been done under the assumption that the data is sampled from a manifold without boundaries and singularities or that the functions of interest are evaluated away from such points. At the same time, it can be argued that singularities and boundaries are an important aspect of the geometry of realistic data. In this paper we consider the behavior of graph Laplacians at points at or near boundaries and two main types of other singularities: intersections, where different manifolds come together and sharp "edges", where a manifold sharply changes direction. We show that the behavior of graph Laplacian near these singularities is quite different from that in the interior of the manifolds. In fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of Fourier series, can be observed in the behavior of graph Laplacian near such points. Unlike in the interior of the domain, where graph Laplacian converges to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a first-order differential operator, which exhibits different scaling behavior as a function of the kernel width. One important implication is that while points near the singularities occupy only a small part of the total volume, the difference in scaling results in a disproportionately large contribution to the total behavior. Another significant finding is that while the scaling behavior of the operator is the same near different types of singularities, they are very distinct at a more refined level of analysis. We believe that a comprehensive understanding of these structures in addition to the standard case of a smooth manifold can take us a long way toward better methods for analysis of complex non-linear data and can lead to significant progress in algorithm design. version:1
arxiv-1211-6675 | Nonlinear Dynamic Field Embedding: On Hyperspectral Scene Visualization | http://arxiv.org/abs/1211.6675 | id:1211.6675 author:Dalton Lunga 'and' Okan Ersoy category:cs.CV cs.CE stat.ML  published:2012-11-28 summary:Graph embedding techniques are useful to characterize spectral signature relations for hyperspectral images. However, such images consists of disjoint classes due to spatial details that are often ignored by existing graph computing tools. Robust parameter estimation is a challenge for kernel functions that compute such graphs. Finding a corresponding high quality coordinate system to map signature relations remains an open research question. We answer positively on these challenges by first proposing a kernel function of spatial and spectral information in computing neighborhood graphs. Secondly, the study exploits the force field interpretation from mechanics and devise a unifying nonlinear graph embedding framework. The generalized framework leads to novel unsupervised multidimensional artificial field embedding techniques that rely on the simple additive assumption of pair-dependent attraction and repulsion functions. The formulations capture long range and short range distance related effects often associated with living organisms and help to establish algorithmic properties that mimic mutual behavior for the purpose of dimensionality reduction. The main benefits from the proposed models includes the ability to preserve the local topology of data and produce quality visualizations i.e. maintaining disjoint meaningful neighborhoods. As part of evaluation, visualization, gradient field trajectories, and semisupervised classification experiments are conducted for image scenes acquired by multiple sensors at various spatial resolutions over different types of objects. The results demonstrate the superiority of the proposed embedding framework over various widely used methods. version:1
arxiv-1211-6658 | Nature-Inspired Mateheuristic Algorithms: Success and New Challenges | http://arxiv.org/abs/1211.6658 | id:1211.6658 author:Xin-She Yang category:math.OC cs.NE 90C26  published:2012-11-28 summary:Despite the increasing popularity of metaheuristics, many crucially important questions remain unanswered. There are two important issues: theoretical framework and the gap between theory and applications. At the moment, the practice of metaheuristics is like heuristic itself, to some extent, by trial and error. Mathematical analysis lags far behind, apart from a few, limited, studies on convergence analysis and stability, there is no theoretical framework for analyzing metaheuristic algorithms. I believe mathematical and statistical methods using Markov chains and dynamical systems can be very useful in the future work. There is no doubt that any theoretical progress will provide potentially huge insightful into meteheuristic algorithms. version:1
arxiv-1211-6653 | Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process Approach | http://arxiv.org/abs/1211.6653 | id:1211.6653 author:Yuyang Wang, Roni Khardon category:cs.LG stat.ML  published:2012-11-28 summary:Multi-task learning models using Gaussian processes (GP) have been developed and successfully applied in various applications. The main difficulty with this approach is the computational cost of inference using the union of examples from all tasks. Therefore sparse solutions, that avoid using the entire data directly and instead use a set of informative "representatives" are desirable. The paper investigates this problem for the grouped mixed-effect GP model where each individual response is given by a fixed-effect, taken from one of a set of unknown groups, plus a random individual effect function that captures variations among individuals. Such models have been widely used in previous work but no sparse solutions have been developed. The paper presents the first sparse solution for such problems, showing how the sparse approximation can be obtained by maximizing a variational lower bound on the marginal likelihood, generalizing ideas from single-task Gaussian processes to handle the mixed-effect model as well as grouping. Experiments using artificial and real data validate the approach showing that it can recover the performance of inference with the full sample, that it outperforms baseline methods, and that it outperforms state of the art sparse solutions for other multi-task GP formulations. version:1
arxiv-1211-2881 | Deep Attribute Networks | http://arxiv.org/abs/1211.2881 | id:1211.2881 author:Junyoung Chung, Donghoon Lee, Youngjoo Seo, Chang D. Yoo category:cs.CV cs.LG stat.ML  published:2012-11-13 summary:Obtaining compact and discriminative features is one of the major challenges in many of the real-world image classification tasks such as face verification and object recognition. One possible approach is to represent input image on the basis of high-level features that carry semantic meaning which humans can understand. In this paper, a model coined deep attribute network (DAN) is proposed to address this issue. For an input image, the model outputs the attributes of the input image without performing any classification. The efficacy of the proposed model is evaluated on unconstrained face verification and real-world object recognition tasks using the LFW and the a-PASCAL datasets. We demonstrate the potential of deep learning for attribute-based classification by showing comparable results with existing state-of-the-art results. Once properly trained, the DAN is fast and does away with calculating low-level features which are maybe unreliable and computationally expensive. version:3
arxiv-1211-6451 | A LASSO-Penalized BIC for Mixture Model Selection | http://arxiv.org/abs/1211.6451 | id:1211.6451 author:Sakyajit Bhattacharya, Paul D. McNicholas category:stat.ME math.ST stat.CO stat.ML stat.TH  published:2012-11-27 summary:The efficacy of family-based approaches to mixture model-based clustering and classification depends on the selection of parsimonious models. Current wisdom suggests the Bayesian information criterion (BIC) for mixture model selection. However, the BIC has well-known limitations, including a tendency to overestimate the number of components as well as a proclivity for, often drastically, underestimating the number of components in higher dimensions. While the former problem might be soluble through merging components, the latter is impossible to mitigate in clustering and classification applications. In this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome this problem. This approach is illustrated based on applications of extensions of mixtures of factor analyzers, where the LPBIC is used to select both the number of components and the number of latent factors. The LPBIC is shown to match or outperform the BIC in several situations. version:1
arxiv-1108-4559 | Optimal Algorithms for Ridge and Lasso Regression with Partially Observed Attributes | http://arxiv.org/abs/1108.4559 | id:1108.4559 author:Elad Hazan, Tomer Koren category:cs.LG  published:2011-08-23 summary:We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art. version:2
arxiv-0712-4159 | Creating a Digital Ecosystem: Service-Oriented Architectures with Distributed Evolutionary Computing | http://arxiv.org/abs/0712.4159 | id:0712.4159 author:G Briscoe category:cs.NE  published:2007-12-26 summary:We start with a discussion of the relevant literature, including Nature Inspired Computing as a framework in which to understand this work, and the process of biomimicry to be used in mimicking the necessary biological processes to create Digital Ecosystems. We then consider the relevant theoretical ecology in creating the digital counterpart of a biological ecosystem, including the topological structure of ecosystems, and evolutionary processes within distributed environments. This leads to a discussion of the relevant fields from computer science for the creation of Digital Ecosystems, including evolutionary computing, Multi-Agent Systems, and Service-Oriented Architectures. We then define Ecosystem-Oriented Architectures for the creation of Digital Ecosystems, imbibed with the properties of self-organisation and scalability from biological ecosystems, including a novel form of distributed evolutionary computing. version:5
arxiv-1211-2532 | Iterative Thresholding Algorithm for Sparse Inverse Covariance Estimation | http://arxiv.org/abs/1211.2532 | id:1211.2532 author:Dominique Guillot, Bala Rajaratnam, Benjamin T. Rolfs, Arian Maleki, Ian Wong category:stat.CO cs.LG stat.ML  published:2012-11-12 summary:The L1-regularized maximum likelihood estimation problem has recently become a topic of great interest within the machine learning, statistics, and optimization communities as a method for producing sparse inverse covariance estimators. In this paper, a proximal gradient method (G-ISTA) for performing L1-regularized covariance matrix estimation is presented. Although numerous algorithms have been proposed for solving this problem, this simple proximal gradient method is found to have attractive theoretical and numerical properties. G-ISTA has a linear rate of convergence, resulting in an O(log e) iteration complexity to reach a tolerance of e. This paper gives eigenvalue bounds for the G-ISTA iterates, providing a closed-form linear convergence rate. The rate is shown to be closely related to the condition number of the optimal point. Numerical convergence results and timing comparisons for the proposed method are presented. G-ISTA is shown to perform very well, especially when the optimal point is well-conditioned. version:3
arxiv-1211-6205 | Neuro-Fuzzy Computing System with the Capacity of Implementation on Memristor-Crossbar and Optimization-Free Hardware Training | http://arxiv.org/abs/1211.6205 | id:1211.6205 author:Farnood Merrikh-Bayat, Farshad Merrikh-Bayat, Saeed Bagheri Shouraki category:cs.NE cs.AI  published:2012-11-27 summary:In this paper, first we present a new explanation for the relation between logical circuits and artificial neural networks, logical circuits and fuzzy logic, and artificial neural networks and fuzzy inference systems. Then, based on these results, we propose a new neuro-fuzzy computing system which can effectively be implemented on the memristor-crossbar structure. One important feature of the proposed system is that its hardware can directly be trained using the Hebbian learning rule and without the need to any optimization. The system also has a very good capability to deal with huge number of input-out training data without facing problems like overtraining. version:1
arxiv-1211-6158 | The Interplay Between Stability and Regret in Online Learning | http://arxiv.org/abs/1211.6158 | id:1211.6158 author:Ankan Saha, Prateek Jain, Ambuj Tewari category:cs.LG stat.ML  published:2012-11-26 summary:This paper considers the stability of online learning algorithms and its implications for learnability (bounded regret). We introduce a novel quantity called {\em forward regret} that intuitively measures how good an online learning algorithm is if it is allowed a one-step look-ahead into the future. We show that given stability, bounded forward regret is equivalent to bounded regret. We also show that the existence of an algorithm with bounded regret implies the existence of a stable algorithm with bounded regret and bounded forward regret. The equivalence results apply to general, possibly non-convex problems. To the best of our knowledge, our analysis provides the first general connection between stability and regret in the online setting that is not restricted to a particular class of algorithms. Our stability-regret connection provides a simple recipe for analyzing regret incurred by any online learning algorithm. Using our framework, we analyze several existing online learning algorithms as well as the "approximate" versions of algorithms like RDA that solve an optimization problem at each iteration. Our proofs are simpler than existing analysis for the respective algorithms, show a clear trade-off between stability and forward regret, and provide tighter regret bounds in some cases. Furthermore, using our recipe, we analyze "approximate" versions of several algorithms such as follow-the-regularized-leader (FTRL) that requires solving an optimization problem at each step. version:1
arxiv-1208-3665 | An Evaluation of Popular Copy-Move Forgery Detection Approaches | http://arxiv.org/abs/1208.3665 | id:1208.3665 author:Vincent Christlein, Christian Riess, Johannes Jordan, Corinna Riess, Elli Angelopoulou category:cs.CV I.4.9  published:2012-08-17 summary:A copy-move forgery is created by copying and pasting content within the same image, and potentially post-processing it. In recent years, the detection of copy-move forgeries has become one of the most actively researched topics in blind image forensics. A considerable number of different algorithms have been proposed focusing on different types of postprocessed copies. In this paper, we aim to answer which copy-move forgery detection algorithms and processing steps (e.g., matching, filtering, outlier detection, affine transformation estimation) perform best in various postprocessing scenarios. The focus of our analysis is to evaluate the performance of previously proposed feature sets. We achieve this by casting existing algorithms in a common pipeline. In this paper, we examined the 15 most prominent feature sets. We analyzed the detection performance on a per-image basis and on a per-pixel basis. We created a challenging real-world copy-move dataset, and a software framework for systematic image manipulation. Experiments show, that the keypoint-based features SIFT and SURF, as well as the block-based DCT, DWT, KPCA, PCA and Zernike features perform very well. These feature sets exhibit the best robustness against various noise sources and downsampling, while reliably identifying the copied regions. version:2
arxiv-1208-3901 | Trace transform based method for color image domain identification | http://arxiv.org/abs/1208.3901 | id:1208.3901 author:Igor G. Olaizola, Marco Quartulli, Julian Florez, Basilio Sierra category:cs.CV  published:2012-08-19 summary:Context categorization is a fundamental pre-requisite for multi-domain multimedia content analysis applications in order to manage contextual information in an efficient manner. In this paper, we introduce a new color image context categorization method (DITEC) based on the trace transform. The problem of dimensionality reduction of the obtained trace transform signal is addressed through statistical descriptors that keep the underlying information. These extracted features offer a highly discriminant behavior for content categorization. The theoretical properties of the method are analyzed and validated experimentally through two different datasets. version:2
arxiv-1211-7276 | Efficient algorithms for robust recovery of images from compressed data | http://arxiv.org/abs/1211.7276 | id:1211.7276 author:Duc Son Pham, Svetha Venkatesh category:cs.IT cs.LG math.IT stat.ML  published:2012-11-26 summary:Compressed sensing (CS) is an important theory for sub-Nyquist sampling and recovery of compressible data. Recently, it has been extended by Pham and Venkatesh to cope with the case where corruption to the CS data is modeled as impulsive noise. The new formulation, termed as robust CS, combines robust statistics and CS into a single framework to suppress outliers in the CS recovery. To solve the newly formulated robust CS problem, Pham and Venkatesh suggested a scheme that iteratively solves a number of CS problems, the solutions from which converge to the true robust compressed sensing solution. However, this scheme is rather inefficient as it has to use existing CS solvers as a proxy. To overcome limitation with the original robust CS algorithm, we propose to solve the robust CS problem directly in this paper and drive more computationally efficient algorithms by following latest advances in large-scale convex optimization for non-smooth regularization. Furthermore, we also extend the robust CS formulation to various settings, including additional affine constraints, $\ell_1$-norm loss function, mixed-norm regularization, and multi-tasking, so as to further improve robust CS. We also derive simple but effective algorithms to solve these extensions. We demonstrate that the new algorithms provide much better computational advantage over the original robust CS formulation, and effectively solve more sophisticated extensions where the original methods simply cannot. We demonstrate the usefulness of the extensions on several CS imaging tasks. version:1
arxiv-1210-5502 | OpenCFU, a New Free and Open-Source Software to Count Cell Colonies and Other Circular Objects | http://arxiv.org/abs/1210.5502 | id:1210.5502 author:Quentin Geissmann category:q-bio.QM cs.CV  published:2012-10-18 summary:Counting circular objects such as cell colonies is an important source of information for biologists. Although this task is often time-consuming and subjective, it is still predominantly performed manually. The aim of the present work is to provide a new tool to enumerate circular objects from digital pictures and video streams. Here, I demonstrate that the created program, OpenCFU, is very robust, accurate and fast. In addition, it provides control over the processing parameters and is implemented in an in- tuitive and modern interface. OpenCFU is a cross-platform and open-source software freely available at http://opencfu.sourceforge.net. version:3
arxiv-1211-5901 | Bayesian learning of noisy Markov decision processes | http://arxiv.org/abs/1211.5901 | id:1211.5901 author:Sumeetpal S. Singh, Nicolas Chopin, Nick Whiteley category:stat.ML cs.LG stat.CO  published:2012-11-26 summary:We consider the inverse reinforcement learning problem, that is, the problem of learning from, and then predicting or mimicking a controller based on state/action data. We propose a statistical model for such data, derived from the structure of a Markov decision process. Adopting a Bayesian approach to inference, we show how latent variables of the model can be estimated, and how predictions about actions can be made, in a unified framework. A new Markov chain Monte Carlo (MCMC) sampler is devised for simulation from the posterior distribution. This step includes a parameter expansion step, which is shown to be essential for good convergence properties of the MCMC sampler. As an illustration, the method is applied to learning a human controller. version:1
arxiv-1009-3896 | Optimistic Rates for Learning with a Smooth Loss | http://arxiv.org/abs/1009.3896 | id:1009.3896 author:Nathan Srebro, Karthik Sridharan, Ambuj Tewari category:cs.LG  published:2010-09-20 summary:We establish an excess risk bound of O(H R_n^2 + R_n \sqrt{H L*}) for empirical risk minimization with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L* is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = \sqrt{R/n}, this translates to a learning rate of O(RH/n) in the separable (L*=0) case and O(RH/n + \sqrt{L^* RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization with a smooth non-negative objective. version:2
arxiv-1211-5829 | An Automatic Algorithm for Object Recognition and Detection Based on ASIFT Keypoints | http://arxiv.org/abs/1211.5829 | id:1211.5829 author:Reza Oji category:cs.AI cs.CV  published:2012-11-26 summary:Object recognition is an important task in image processing and computer vision. This paper presents a perfect method for object recognition with full boundary detection by combining affine scale invariant feature transform (ASIFT) and a region merging algorithm. ASIFT is a fully affine invariant algorithm that means features are invariant to six affine parameters namely translation (2 parameters), zoom, rotation and two camera axis orientations. The features are very reliable and give us strong keypoints that can be used for matching between different images of an object. We trained an object in several images with different aspects for finding best keypoints of it. Then, a robust region merging algorithm is used to recognize and detect the object with full boundary in the other images based on ASIFT keypoints and a similarity measure for merging regions in the image. Experimental results show that the presented method is very efficient and powerful to recognize the object and detect it with high accuracy. version:1
arxiv-1211-5712 | Detection of elliptical shapes via cross-entropy clustering | http://arxiv.org/abs/1211.5712 | id:1211.5712 author:Jacek Tabor, Krzysztof Misztal category:cs.CV  published:2012-11-24 summary:The problem of finding elliptical shapes in an image will be considered. We discuss the solution which uses cross-entropy clustering. The proposed method allows the search for ellipses with predefined sizes and position in the space. Moreover, it works well for search of ellipsoids in higher dimensions. version:1
arxiv-1211-5687 | Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep Extensions | http://arxiv.org/abs/1211.5687 | id:1211.5687 author:Heng Luo, Pierre Luc Carrier, Aaron Courville, Yoshua Bengio category:cs.LG stat.ML  published:2012-11-24 summary:We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture modeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or surpasses the state-of-the-art on texture synthesis and inpainting by parametric models. We also develop a novel RBM model with a spike-and-slab visible layer and binary variables in the hidden layer. This model is designed to be stacked on top of the TssRBM. We show the resulting deep belief network (DBN) is a powerful generative model that improves on single-layer models and is capable of modeling not only single high-resolution and challenging textures but also multiple textures. version:1
arxiv-1211-6410 | New Hoopoe Heuristic Optimization | http://arxiv.org/abs/1211.6410 | id:1211.6410 author:Mohammed El-Dosuky, Ahmed EL-Bassiouny, Taher Hamza, Magdy Rashad category:cs.NE cs.AI  published:2012-11-24 summary:Most optimization problems in real life applications are often highly nonlinear. Local optimization algorithms do not give the desired performance. So, only global optimization algorithms should be used to obtain optimal solutions. This paper introduces a new nature-inspired metaheuristic optimization algorithm, called Hoopoe Heuristic (HH). In this paper, we will study HH and validate it against some test functions. Investigations show that it is very promising and could be seen as an optimization of the powerful algorithm of cuckoo search. Finally, we discuss the features of Hoopoe Heuristic and propose topics for further studies. version:1
arxiv-1211-5590 | Theano: new features and speed improvements | http://arxiv.org/abs/1211.5590 | id:1211.5590 author:Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio category:cs.SC cs.LG  published:2012-11-23 summary:Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks. version:1
arxiv-1211-5556 | Improving Perceptual Color Difference using Basic Color Terms | http://arxiv.org/abs/1211.5556 | id:1211.5556 author:Ofir Pele, Michael Werman category:cs.CV cs.GR  published:2012-11-23 summary:We suggest a new color distance based on two observations. First, perceptual color differences were designed to be used to compare very similar colors. They do not capture human perception for medium and large color differences well. Thresholding was proposed to solve the problem for large color differences, i.e. two totally different colors are always the same distance apart. We show that thresholding alone cannot improve medium color differences. We suggest to alleviate this problem using basic color terms. Second, when a color distance is used for edge detection, many small distances around the just noticeable difference may account for false edges. We suggest to reduce the effect of small distances. version:1
arxiv-1211-5481 | Genetic Algorithm Modeling with GPU Parallel Computing Technology | http://arxiv.org/abs/1211.5481 | id:1211.5481 author:Stefano Cavuoti, Mauro Garofalo, Massimo Brescia, Antonio Pescapé, Giuseppe Longo, Giorgio Ventre category:astro-ph.IM cs.DC cs.NE  published:2012-11-23 summary:We present a multi-purpose genetic algorithm, designed and implemented with GPGPU / CUDA parallel computing technology. The model was derived from a multi-core CPU serial implementation, named GAME, already scientifically successfully tested and validated on astrophysical massive data classification problems, through a web application resource (DAMEWARE), specialized in data mining based on Machine Learning paradigms. Since genetic algorithms are inherently parallel, the GPGPU computing paradigm has provided an exploit of the internal training features of the model, permitting a strong optimization in terms of processing performances and scalability. version:1
arxiv-1211-5414 | Analysis of a randomized approximation scheme for matrix multiplication | http://arxiv.org/abs/1211.5414 | id:1211.5414 author:Daniel Hsu, Sham M. Kakade, Tong Zhang category:cs.DS cs.LG cs.NA stat.ML  published:2012-11-23 summary:This note gives a simple analysis of a randomized approximation scheme for matrix multiplication proposed by Sarlos (2006) based on a random rotation followed by uniform column sampling. The result follows from a matrix version of Bernstein's inequality and a tail inequality for quadratic forms in subgaussian random vectors. version:1
arxiv-1211-5400 | Ecosystem-Oriented Distributed Evolutionary Computing | http://arxiv.org/abs/1211.5400 | id:1211.5400 author:Gerard Briscoe, Philippe De Wilde category:cs.NE  published:2012-11-23 summary:We create a novel optimisation technique inspired by natural ecosystems, where the optimisation works at two levels: a first optimisation, migration of genes which are distributed in a peer-to-peer network, operating continuously in time; this process feeds a second optimisation based on evolutionary computing that operates locally on single peers and is aimed at finding solutions to satisfy locally relevant constraints. We consider from the domain of computer science distributed evolutionary computing, with the relevant theory from the domain of theoretical biology, including the fields of evolutionary and ecological theory, the topological structure of ecosystems, and evolutionary processes within distributed environments. We then define ecosystem- oriented distributed evolutionary computing, imbibed with the properties of self-organisation, scalability and sustainability from natural ecosystems, including a novel form of distributed evolu- tionary computing. Finally, we conclude with a discussion of the apparent compromises resulting from the hybrid model created, such as the network topology. version:1
arxiv-1008-3450 | Bottleneck of using single memristor as a synapse and its solution | http://arxiv.org/abs/1008.3450 | id:1008.3450 author:Farnood Merrikh-Bayat, Saeed Bagheri Shouraki, Iman Esmaili Paeen Afrakoti category:cs.NE  published:2010-08-20 summary:It is now widely accepted that memristive devices are perfect candidates for the emulation of biological synapses in neuromorphic systems. This is mainly because of the fact that like the strength of synapse, memristance of the memristive device can be tuned actively (e.g., by the application of volt- age or current). In addition, it is also possible to fabricate very high density of memristive devices (comparable to the number of synapses in real biological system) through the nano-crossbar structures. However, in this paper we will show that there are some problems associated with memristive synapses (memristive devices which are playing the role of biological synapses). For example, we show that the variation rate of the memristance of memristive device depends completely on the current memristance of the device and therefore it can change significantly with time during the learning phase. This phenomenon can degrade the performance of learning methods like Spike Timing-Dependent Plasticity (STDP) and cause the corresponding neuromorphic systems to become unstable. Finally, at the end of this paper, we illustrate that using two serially connected memristive devices with different polarities as a synapse can somewhat fix the aforementioned problem. version:3
arxiv-1211-5355 | Cobb Angle Measurement of Scoliosis with Reduced Variability | http://arxiv.org/abs/1211.5355 | id:1211.5355 author:Raka Kundu, Amlan Chakrabarti, Prasanna K. Lenka category:cs.CV  published:2012-11-22 summary:Cobb angle, which is a measure of spinal curvature is the standard method for quantifying the magnitude of Scoliosis related to spinal deformity in orthopedics. Determining the Cobb angle through manual process is subject to human errors. In this work, we propose a methodology to measure the magnitude of Cobb angle, which appreciably reduces the variability related to its measurement compared to the related works. The proposed methodology is facilitated by using a suitable new improved version of Non-Local Means for image denoisation and Otsus automatic threshold selection for Canny edge detection. We have selected NLM for preprocessing of the image as it is one of the fine states of art for image denoisation and helps in retaining the image quality. Trimmedmean, median are more robust to outliners than mean and following this concept we observed that NLM denoising quality performance can be enhanced by using Euclidean trimmed-mean replacing the mean. To prove the better performance of the Non-Local Euclidean Trimmed-mean denoising filter, we have provided some comparative study results of the proposed denoising technique with traditional NLM and NonLocal Euclidean Medians. The experimental results for Cobb angle measurement over intra observer and inter observer experimental data reveals the better performance and superiority of the proposed approach compared to the related works. MATLAB2009b image processing toolbox was used for the purpose of simulation and verification of the proposed methodology. version:1
arxiv-1211-5227 | Service Composition Design Pattern for Autonomic Computing Systems using Association Rule based Learning and Service-Oriented Architecture | http://arxiv.org/abs/1211.5227 | id:1211.5227 author:Vishnuvardhan Mannava, T. Ramesh category:cs.SE cs.DC cs.LG  published:2012-11-22 summary:In this paper we present a Service Injection and composition Design Pattern for Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented design patterns, and amalgamation of the Strategy, Worker Object, and Check-List Design Patterns used to design the Self-Adaptive Systems. It will apply self reconfiguration planes dynamically without the interruption or intervention of the administrator for handling service failures at the servers. When a client requests for a complex service, Service Composition should be done to fulfil the request. If a service is not available in the memory, it will be injected as Aspectual Feature Module code. We used Service Oriented Architecture (SOA) with Web Services in Java to Implement the composite Design Pattern. As far as we know, there are no studies on composition of design patterns for Peer-to-peer computing domain. The pattern is described using a java-like notation for the classes and interfaces. A simple UML class and Sequence diagrams are depicted. version:1
arxiv-1211-5194 | On pattern recovery of the fused Lasso | http://arxiv.org/abs/1211.5194 | id:1211.5194 author:Junyang Qian, Jinzhu Jia category:stat.ML math.ST stat.TH  published:2012-11-22 summary:We study the property of the Fused Lasso Signal Approximator (FLSA) for estimating a blocky signal sequence with additive noise. We transform the FLSA to an ordinary Lasso problem. By studying the property of the design matrix in the transformed Lasso problem, we find that the irrepresentable condition might not hold, in which case we show that the FLSA might not be able to recover the signal pattern. We then apply the newly developed preconditioning method -- Puffer Transformation [Jia and Rohe, 2012] on the transformed Lasso problem. We call the new method the preconditioned fused Lasso and we give non-asymptotic results for this method. Results show that when the signal jump strength (signal difference between two neighboring groups) is big and the noise level is small, our preconditioned fused Lasso estimator gives the correct pattern with high probability. Theoretical results give insight on what controls the signal pattern recovery ability -- it is the noise level {instead of} the length of the sequence. Simulations confirm our theorems and show significant improvement of the preconditioned fused Lasso estimator over the vanilla FLSA. version:1
arxiv-1206-0652 | Learning in Hierarchical Social Networks | http://arxiv.org/abs/1206.0652 | id:1206.0652 author:Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, William Moran, Stephen D. Howard category:cs.SI cs.IT cs.LG math.IT  published:2012-05-30 summary:We study a social network consisting of agents organized as a hierarchical M-ary rooted tree, common in enterprise and military organizational structures. The goal is to aggregate information to solve a binary hypothesis testing problem. Each agent at a leaf of the tree, and only such an agent, makes a direct measurement of the underlying true hypothesis. The leaf agent then makes a decision and sends it to its supervising agent, at the next level of the tree. Each supervising agent aggregates the decisions from the M members of its group, produces a summary message, and sends it to its supervisor at the next level, and so on. Ultimately, the agent at the root of the tree makes an overall decision. We derive upper and lower bounds for the Type I and II error probabilities associated with this decision with respect to the number of leaf agents, which in turn characterize the converge rates of the Type I, Type II, and total error probabilities. We also provide a message-passing scheme involving non-binary message alphabets and characterize the exponent of the error probability with respect to the message alphabet size. version:4
arxiv-1211-5098 | Scaling Genetic Programming for Source Code Modification | http://arxiv.org/abs/1211.5098 | id:1211.5098 author:Brendan Cody-Kenny, Stephen Barrett category:cs.NE cs.SE  published:2012-11-21 summary:In Search Based Software Engineering, Genetic Programming has been used for bug fixing, performance improvement and parallelisation of programs through the modification of source code. Where an evolutionary computation algorithm, such as Genetic Programming, is to be applied to similar code manipulation tasks, the complexity and size of source code for real-world software poses a scalability problem. To address this, we intend to inspect how the Software Engineering concepts of modularity, granularity and localisation of change can be reformulated as additional mechanisms within a Genetic Programming algorithm. version:1
arxiv-1210-2748 | Quantifying Causal Coupling Strength: A Lag-specific Measure For Multivariate Time Series Related To Transfer Entropy | http://arxiv.org/abs/1210.2748 | id:1210.2748 author:Jakob Runge, Jobst Heitzig, Norbert Marwan, Jürgen Kurths category:physics.data-an cs.IT math.IT stat.ML  published:2012-10-09 summary:While it is an important problem to identify the existence of causal associations between two components of a multivariate time series, a topic addressed in Runge et al. (2012), it is even more important to assess the strength of their association in a meaningful way. In the present article we focus on the problem of defining a meaningful coupling strength using information theoretic measures and demonstrate the short-comings of the well-known mutual information and transfer entropy. Instead, we propose a certain time-delayed conditional mutual information, the momentary information transfer (MIT), as a measure of association that is general, causal and lag-specific, reflects a well interpretable notion of coupling strength and is practically computable. MIT is based on the fundamental concept of source entropy, which we utilize to yield a notion of coupling strength that is, compared to mutual information and transfer entropy, well interpretable, in that for many cases it solely depends on the interaction of the two components at a certain lag. In particular, MIT is thus in many cases able to exclude the misleading influence of autodependency within a process in an information-theoretic way. We formalize and prove this idea analytically and numerically for a general class of nonlinear stochastic processes and illustrate the potential of MIT on climatological data. version:2
arxiv-1211-0587 | Partition Tree Weighting | http://arxiv.org/abs/1211.0587 | id:1211.0587 author:Joel Veness, Martha White, Michael Bowling, András György category:cs.IT cs.LG math.IT stat.ML  published:2012-11-03 summary:This paper introduces the Partition Tree Weighting technique, an efficient meta-algorithm for piecewise stationary sources. The technique works by performing Bayesian model averaging over a large class of possible partitions of the data into locally stationary segments. It uses a prior, closely related to the Context Tree Weighting technique of Willems, that is well suited to data compression applications. Our technique can be applied to any coding distribution at an additional time and space cost only logarithmic in the sequence length. We provide a competitive analysis of the redundancy of our method, and explore its application in a variety of settings. The order of the redundancy and the complexity of our algorithm matches those of the best competitors available in the literature, and the new algorithm exhibits a superior complexity-performance trade-off in our experiments. version:2
arxiv-1211-4971 | A Hybrid Bacterial Foraging Algorithm For Solving Job Shop Scheduling Problems | http://arxiv.org/abs/1211.4971 | id:1211.4971 author:S. Narendhar, T. Amudha category:cs.NE  published:2012-11-21 summary:Bio-Inspired computing is the subset of Nature-Inspired computing. Job Shop Scheduling Problem is categorized under popular scheduling problems. In this research work, Bacterial Foraging Optimization was hybridized with Ant Colony Optimization and a new technique Hybrid Bacterial Foraging Optimization for solving Job Shop Scheduling Problem was proposed. The optimal solutions obtained by proposed Hybrid Bacterial Foraging Optimization algorithms are much better when compared with the solutions obtained by Bacterial Foraging Optimization algorithm for well-known test problems of different sizes. From the implementation of this research work, it could be observed that the proposed Hybrid Bacterial Foraging Optimization was effective than Bacterial Foraging Optimization algorithm in solving Job Shop Scheduling Problems. Hybrid Bacterial Foraging Optimization is used to implement real world Job Shop Scheduling Problems. version:1
arxiv-1211-4929 | Summarizing Reviews with Variable-length Syntactic Patterns and Topic Models | http://arxiv.org/abs/1211.4929 | id:1211.4929 author:Trung V. Nguyen, Alice H. Oh category:cs.IR cs.CL  published:2012-11-21 summary:We present a novel summarization framework for reviews of products and services by selecting informative and concise text segments from the reviews. Our method consists of two major steps. First, we identify five frequently occurring variable-length syntactic patterns and use them to extract candidate segments. Then we use the output of a joint generative sentiment topic model to filter out the non-informative segments. We verify the proposed method with quantitative and qualitative experiments. In a quantitative study, our approach outperforms previous methods in producing informative segments and summaries that capture aspects of products and services as expressed in the user-generated pros and cons lists. Our user study with ninety users resonates with this result: individual segments extracted and filtered by our method are rated as more useful by users compared to previous approaches by users. version:1
arxiv-1103-1417 | Localization from Incomplete Noisy Distance Measurements | http://arxiv.org/abs/1103.1417 | id:1103.1417 author:Adel Javanmard, Andrea Montanari category:math.ST cs.LG cs.SY math.OC math.PR stat.TH  published:2011-03-08 summary:We consider the problem of positioning a cloud of points in the Euclidean space $\mathbb{R}^d$, using noisy measurements of a subset of pairwise distances. This task has applications in various areas, such as sensor network localization and reconstruction of protein conformations from NMR measurements. Also, it is closely related to dimensionality reduction problems and manifold learning, where the goal is to learn the underlying global geometry of a data set using local (or partial) metric information. Here we propose a reconstruction algorithm based on semidefinite programming. For a random geometric graph model and uniformly bounded noise, we provide a precise characterization of the algorithm's performance: In the noiseless case, we find a radius $r_0$ beyond which the algorithm reconstructs the exact positions (up to rigid transformations). In the presence of noise, we obtain upper and lower bounds on the reconstruction error that match up to a factor that depends only on the dimension $d$, and the average degree of the nodes in the graph. version:4
arxiv-1211-4888 | A Traveling Salesman Learns Bayesian Networks | http://arxiv.org/abs/1211.4888 | id:1211.4888 author:Tuhin Sahai, Stefan Klus, Michael Dellnitz category:cs.LG stat.ML  published:2012-11-20 summary:Structure learning of Bayesian networks is an important problem that arises in numerous machine learning applications. In this work, we present a novel approach for learning the structure of Bayesian networks using the solution of an appropriately constructed traveling salesman problem. In our approach, one computes an optimal ordering (partially ordered set) of random variables using methods for the traveling salesman problem. This ordering significantly reduces the search space for the subsequent greedy optimization that computes the final structure of the Bayesian network. We demonstrate our approach of learning Bayesian networks on real world census and weather datasets. In both cases, we demonstrate that the approach very accurately captures dependencies between random variables. We check the accuracy of the predictions based on independent studies in both application domains. version:1
arxiv-1211-4860 | Domain Adaptations for Computer Vision Applications | http://arxiv.org/abs/1211.4860 | id:1211.4860 author:Oscar Beijbom category:cs.CV cs.LG stat.ML  published:2012-11-20 summary:A basic assumption of statistical learning theory is that train and test data are drawn from the same underlying distribution. Unfortunately, this assumption doesn't hold in many applications. Instead, ample labeled data might exist in a particular `source' domain while inference is needed in another, `target' domain. Domain adaptation methods leverage labeled data from both domains to improve classification on unseen data in the target domain. In this work we survey domain transfer learning methods for various application domains with focus on recent work in Computer Vision. version:1
arxiv-1211-4798 | A survey of non-exchangeable priors for Bayesian nonparametric models | http://arxiv.org/abs/1211.4798 | id:1211.4798 author:Nicholas J. Foti, Sinead Williamson category:stat.ML cs.LG  published:2012-11-20 summary:Dependent nonparametric processes extend distributions over measures, such as the Dirichlet process and the beta process, to give distributions over collections of measures, typically indexed by values in some covariate space. Such models are appropriate priors when exchangeability assumptions do not hold, and instead we want our model to vary fluidly with some set of covariates. Since the concept of dependent nonparametric processes was formalized by MacEachern [1], there have been a number of models proposed and used in the statistics and machine learning literatures. Many of these models exhibit underlying similarities, an understanding of which, we hope, will help in selecting an appropriate prior, developing new models, and leveraging inference techniques. version:1
arxiv-1211-4771 | Matching Through Features and Features Through Matching | http://arxiv.org/abs/1211.4771 | id:1211.4771 author:Ganesh Sundaramoorthi, Yanchao Yang category:cs.CV  published:2012-11-20 summary:This paper addresses how to construct features for the problem of image correspondence, in particular, the paper addresses how to construct features so as to maintain the right level of invariance versus discriminability. We show that without additional prior knowledge of the 3D scene, the right tradeoff cannot be established in a pre-processing step of the images as is typically done in most feature-based matching methods. However, given knowledge of the second image to match, the tradeoff between invariance and discriminability of features in the first image is less ambiguous. This suggests to setup the problem of feature extraction and matching as a joint estimation problem. We develop a possible mathematical framework, a possible computational algorithm, and we give example demonstration on finding correspondence on images related by a scene that undergoes large 3D deformation of non-planar objects and camera viewpoint change. version:1
arxiv-1211-4753 | A unifying representation for a class of dependent random measures | http://arxiv.org/abs/1211.4753 | id:1211.4753 author:Nicholas J. Foti, Joseph D. Futoma, Daniel N. Rockmore, Sinead Williamson category:stat.ML cs.LG  published:2012-11-20 summary:We present a general construction for dependent random measures based on thinning Poisson processes on an augmented space. The framework is not restricted to dependent versions of a specific nonparametric model, but can be applied to all models that can be represented using completely random measures. Several existing dependent random measures can be seen as specific cases of this framework. Interesting properties of the resulting measures are derived and the efficacy of the framework is demonstrated by constructing a covariate-dependent latent feature model and topic model that obtain superior predictive performance. version:1
arxiv-1211-4706 | Random Input Sampling for Complex Models Using Markov Chain Monte Carlo | http://arxiv.org/abs/1211.4706 | id:1211.4706 author:A. Gokcen Mahmutoglu, Alper T. Erdogan, Alper Demir category:stat.ML  published:2012-11-20 summary:Many random processes can be simulated as the output of a deterministic model accepting random inputs. Such a model usually describes a complex mathematical or physical stochastic system and the randomness is introduced in the input variables of the model. When the statistics of the output event are known, these input variables have to be chosen in a specific way for the output to have the prescribed statistics. Because the probability distribution of the input random variables is not directly known but dictated implicitly by the statistics of the output random variables, this problem is usually intractable for classical sampling methods. Based on Markov Chain Monte Carlo we propose a novel method to sample random inputs to such models by introducing a modification to the standard Metropolis-Hastings algorithm. As an example we consider a system described by a stochastic differential equation (sde) and demonstrate how sample paths of a random process satisfying this sde can be generated with our technique. version:1
arxiv-1211-4683 | Content based video retrieval | http://arxiv.org/abs/1211.4683 | id:1211.4683 author:B. V. Patel, B. B. Meshram category:cs.MM cs.CV  published:2012-11-20 summary:Content based video retrieval is an approach for facilitating the searching and browsing of large image collections over World Wide Web. In this approach, video analysis is conducted on low level visual properties extracted from video frame. We believed that in order to create an effective video retrieval system, visual perception must be taken into account. We conjectured that a technique which employs multiple features for indexing and retrieval would be more effective in the discrimination and search tasks of videos. In order to validate this claim, content based indexing and retrieval systems were implemented using color histogram, various texture features and other approaches. Videos were stored in Oracle 9i Database and a user study measured correctness of response. version:1
arxiv-1211-4658 | An Effective Method for Fingerprint Classification | http://arxiv.org/abs/1211.4658 | id:1211.4658 author:Monowar H. Bhuyan, Sarat Saharia, Dhruba Kr Bhattacharyya category:cs.CV cs.CR 68U35 I.5.3  published:2012-11-20 summary:This paper presents an effective method for fingerprint classification using data mining approach. Initially, it generates a numeric code sequence for each fingerprint image based on the ridge flow patterns. Then for each class, a seed is selected by using a frequent itemsets generation technique. These seeds are subsequently used for clustering the fingerprint images. The proposed method was tested and evaluated in terms of several real-life datasets and a significant improvement in reducing the misclassification errors has been noticed in comparison to its other counterparts. version:1
arxiv-1211-4866 | A Brief Review of Data Mining Application Involving Protein Sequence Classification | http://arxiv.org/abs/1211.4866 | id:1211.4866 author:Suprativ Saha, Rituparna Chaki category:cs.DB cs.NE  published:2012-11-20 summary:Data mining techniques have been used by researchers for analyzing protein sequences. In protein analysis, especially in protein sequence classification, selection of feature is most important. Popular protein sequence classification techniques involve extraction of specific features from the sequences. Researchers apply some well-known classification techniques like neural networks, Genetic algorithm, Fuzzy ARTMAP, Rough Set Classifier etc for accurate classification. This paper presents a review is with three different classification models such as neural network model, fuzzy ARTMAP model and Rough set classifier model. A new technique for classifying protein sequences have been proposed in the end. The proposed technique tries to reduce the computational overheads encountered by earlier approaches and increase the accuracy of classification. version:1
arxiv-1211-4591 | Five Modulus Method For Image Compression | http://arxiv.org/abs/1211.4591 | id:1211.4591 author:Firas A. Jassim, Hind E. Qassim category:cs.CV cs.MM  published:2012-11-19 summary:Data is compressed by reducing its redundancy, but this also makes the data less reliable, more prone to errors. In this paper a novel approach of image compression based on a new method that has been created for image compression which is called Five Modulus Method (FMM). The new method consists of converting each pixel value in an 8-by-8 block into a multiple of 5 for each of the R, G and B arrays. After that, the new values could be divided by 5 to get new values which are 6-bit length for each pixel and it is less in storage space than the original value which is 8-bits. Also, a new protocol for compression of the new values as a stream of bits has been presented that gives the opportunity to store and transfer the new compressed image easily. version:1
arxiv-1211-4524 | Applying Dynamic Model for Multiple Manoeuvring Target Tracking Using Particle Filtering | http://arxiv.org/abs/1211.4524 | id:1211.4524 author:Mohammad Javad Parseh, Saeid Pashazadeh category:cs.CV cs.AI  published:2012-11-19 summary:In this paper, we applied a dynamic model for manoeuvring targets in SIR particle filter algorithm for improving tracking accuracy of multiple manoeuvring targets. In our proposed approach, a color distribution model is used to detect changes of target's model . Our proposed approach controls deformation of target's model. If deformation of target's model is larger than a predetermined threshold, then the model will be updated. Global Nearest Neighbor (GNN) algorithm is used as data association algorithm. We named our proposed method as Deformation Detection Particle Filter (DDPF) . DDPF approach is compared with basic SIR-PF algorithm on real airshow videos. Comparisons results show that, the basic SIR-PF algorithm is not able to track the manoeuvring targets when the rotation or scaling is occurred in target' s model. However, DDPF approach updates target's model when the rotation or scaling is occurred. Thus, the proposed approach is able to track the manoeuvring targets more efficiently and accurately. version:1
arxiv-1211-4503 | An Effective Fingerprint Classification and Search Method | http://arxiv.org/abs/1211.4503 | id:1211.4503 author:Monowar H. Bhuyan, D. K. Bhattacharyya category:cs.CV cs.CR 68U35 I.5.3  published:2012-11-19 summary:This paper presents an effective fingerprint classification method designed based on a hierarchical agglomerative clustering technique. The performance of the technique was evaluated in terms of several real-life datasets and a significant improvement in reducing the misclassification error has been noticed. This paper also presents a query based faster fingerprint search method over the clustered fingerprint databases. The retrieval accuracy of the search method has been found effective in light of several real-life databases. version:1
arxiv-1211-4499 | Rate-Distortion Analysis of Multiview Coding in a DIBR Framework | http://arxiv.org/abs/1211.4499 | id:1211.4499 author:Boshra Rajaei, Thomas Maugey, Hamid-Reza Pourreza, Pascal Frossard category:cs.CV  published:2012-11-19 summary:Depth image based rendering techniques for multiview applications have been recently introduced for efficient view generation at arbitrary camera positions. Encoding rate control has thus to consider both texture and depth data. Due to different structures of depth and texture images and their different roles on the rendered views, distributing the available bit budget between them however requires a careful analysis. Information loss due to texture coding affects the value of pixels in synthesized views while errors in depth information lead to shift in objects or unexpected patterns at their boundaries. In this paper, we address the problem of efficient bit allocation between textures and depth data of multiview video sequences. We adopt a rate-distortion framework based on a simplified model of depth and texture images. Our model preserves the main features of depth and texture images. Unlike most recent solutions, our method permits to avoid rendering at encoding time for distortion estimation so that the encoding complexity is not augmented. In addition to this, our model is independent of the underlying inpainting method that is used at decoder. Experiments confirm our theoretical results and the efficiency of our rate allocation strategy. version:1
arxiv-1211-4488 | A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A Comparable Corpora | http://arxiv.org/abs/1211.4488 | id:1211.4488 author:Jessica C. Ramírez, Yuji Matsumoto category:cs.CL cs.AI  published:2012-11-19 summary:The performance of a Statistical Machine Translation System (SMT) system is proportionally directed to the quality and length of the parallel corpus it uses. However for some pair of languages there is a considerable lack of them. The long term goal is to construct a Japanese-Spanish parallel corpus to be used for SMT, whereas, there are a lack of useful Japanese-Spanish parallel Corpus. To address this problem, In this study we proposed a method for extracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging and Rule-Based approach. The main focus of this approach is the syntactic features of both languages. Human evaluation was performed over a sample and shows promising results, in comparison with the baseline. version:1
arxiv-1211-4385 | Artificial Neural Network Based Optical Character Recognition | http://arxiv.org/abs/1211.4385 | id:1211.4385 author:Vivek Shrivastava, Navdeep Sharma category:cs.CV cs.NE  published:2012-11-19 summary:Optical Character Recognition deals in recognition and classification of characters from an image. For the recognition to be accurate, certain topological and geometrical properties are calculated, based on which a character is classified and recognized. Also, the Human psychology perceives characters by its overall shape and features such as strokes, curves, protrusions, enclosures etc. These properties, also called Features are extracted from the image by means of spatial pixel-based calculation. A collection of such features, called Vectors, help in defining a character uniquely, by means of an Artificial Neural Network that uses these Feature Vectors. version:1
arxiv-1211-4384 | A Sensing Policy Based on Confidence Bounds and a Restless Multi-Armed Bandit Model | http://arxiv.org/abs/1211.4384 | id:1211.4384 author:Jan Oksanen, Visa Koivunen, H. Vincent Poor category:cs.IT cs.LG math.IT  published:2012-11-19 summary:A sensing policy for the restless multi-armed bandit problem with stationary but unknown reward distributions is proposed. The work is presented in the context of cognitive radios in which the bandit problem arises when deciding which parts of the spectrum to sense and exploit. It is shown that the proposed policy attains asymptotically logarithmic weak regret rate when the rewards are bounded independent and identically distributed or finite state Markovian. Simulation results verifying uniformly logarithmic weak regret are also presented. The proposed policy is a centrally coordinated index policy, in which the index of a frequency band is comprised of a sample mean term and a confidence term. The sample mean term promotes spectrum exploitation whereas the confidence term encourages exploration. The confidence term is designed such that the time interval between consecutive sensing instances of any suboptimal band grows exponentially. This exponential growth between suboptimal sensing time instances leads to logarithmically growing weak regret. Simulation results demonstrate that the proposed policy performs better than other similar methods in the literature. version:1
arxiv-1211-4321 | Bayesian nonparametric models for ranked data | http://arxiv.org/abs/1211.4321 | id:1211.4321 author:Francois Caron, Yee Whye Teh category:stat.ML cs.LG stat.ME  published:2012-11-19 summary:We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process. We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation. We develop a time-varying extension of our model, and apply it to the New York Times lists of weekly bestselling books. version:1
arxiv-1211-4307 | Efficient Superimposition Recovering Algorithm | http://arxiv.org/abs/1211.4307 | id:1211.4307 author:Han Li, Kun Gai, Pinghua Gong, Changshui Zhang category:cs.CV  published:2012-11-19 summary:In this article, we address the issue of recovering latent transparent layers from superimposition images. Here, we assume we have the estimated transformations and extracted gradients of latent layers. To rapidly recover high-quality image layers, we propose an Efficient Superimposition Recovering Algorithm (ESRA) by extending the framework of accelerated gradient method. In addition, a key building block (in each iteration) in our proposed method is the proximal operator calculating. Here we propose to employ a dual approach and present our Parallel Algorithm with Constrained Total Variation (PACTV) method. Our recovering method not only reconstructs high-quality layers without color-bias problem, but also theoretically guarantees good convergence performance. version:1
arxiv-1211-4264 | Non-Local Patch Regression: Robust Image Denoising in Patch Space | http://arxiv.org/abs/1211.4264 | id:1211.4264 author:Kunal N. Chaudhury, Amit Singer category:cs.CV  published:2012-11-18 summary:It was recently demonstrated in [Chaudhury et al.,Non-Local Euclidean Medians,2012] that the denoising performance of Non-Local Means (NLM) can be improved at large noise levels by replacing the mean by the robust Euclidean median. Numerical experiments on synthetic and natural images showed that the latter consistently performed better than NLM beyond a certain noise level, and significantly so for images with sharp edges. The Euclidean mean and median can be put into a common regression (on the patch space) framework, in which the l_2 norm of the residuals is considered in the former, while the l_1 norm is considered in the latter. The natural question then is what happens if we consider l_p (0<p<1) regression? We investigate this possibility in this paper. version:1
arxiv-1206-2372 | PRISMA: PRoximal Iterative SMoothing Algorithm | http://arxiv.org/abs/1206.2372 | id:1206.2372 author:Francesco Orabona, Andreas Argyriou, Nathan Srebro category:math.OC cs.LG  published:2012-06-11 summary:Motivated by learning problems including max-norm regularized matrix completion and clustering, robust PCA and sparse inverse covariance selection, we propose a novel optimization algorithm for minimizing a convex objective which decomposes into three parts: a smooth part, a simple non-smooth Lipschitz part, and a simple non-smooth non-Lipschitz part. We use a time variant smoothing strategy that allows us to obtain a guarantee that does not depend on knowing in advance the total number of iterations nor a bound on the domain. version:2
arxiv-1211-4161 | Semantic Polarity of Adjectival Predicates in Online Reviews | http://arxiv.org/abs/1211.4161 | id:1211.4161 author:Ae-Lim Ahn, Éric Laporte, Jee-Sun Nam category:cs.CL  published:2012-11-17 summary:Web users produce more and more documents expressing opinions. Because these have become important resources for customers and manufacturers, many have focused on them. Opinions are often expressed through adjectives with positive or negative semantic values. In extracting information from users' opinion in online reviews, exact recognition of the semantic polarity of adjectives is one of the most important requirements. Since adjectives have different semantic orientations according to contexts, it is not satisfying to extract opinion information without considering the semantic and lexical relations between the adjectives and the feature nouns appropriate to a given domain. In this paper, we present a classification of adjectives by polarity, and we analyze adjectives that are undetermined in the absence of contexts. Our research should be useful for accurately predicting semantic orientations of opinion sentences, and should be taken into account before relying on an automatic methods. version:1
arxiv-1211-4150 | Efficiently Learning from Revealed Preference | http://arxiv.org/abs/1211.4150 | id:1211.4150 author:Morteza Zadimoghaddam, Aaron Roth category:cs.GT cs.DS cs.LG  published:2012-11-17 summary:In this paper, we consider the revealed preferences problem from a learning perspective. Every day, a price vector and a budget is drawn from an unknown distribution, and a rational agent buys his most preferred bundle according to some unknown utility function, subject to the given prices and budget constraint. We wish not only to find a utility function which rationalizes a finite set of observations, but to produce a hypothesis valuation function which accurately predicts the behavior of the agent in the future. We give efficient algorithms with polynomial sample-complexity for agents with linear valuation functions, as well as for agents with linearly separable, concave valuation functions with bounded second derivative. version:1
arxiv-1211-4142 | Data Clustering via Principal Direction Gap Partitioning | http://arxiv.org/abs/1211.4142 | id:1211.4142 author:Ralph Abbey, Jeremy Diepenbrock, Amy Langville, Carl Meyer, Shaina Race, Dexin Zhou category:stat.ML cs.LG  published:2012-11-17 summary:We explore the geometrical interpretation of the PCA based clustering algorithm Principal Direction Divisive Partitioning (PDDP). We give several examples where this algorithm breaks down, and suggest a new method, gap partitioning, which takes into account natural gaps in the data between clusters. Geometric features of the PCA space are derived and illustrated and experimental results are given which show our method is comparable on the datasets used in the original paper on PDDP. version:1
arxiv-1211-3955 | On Calibrated Predictions for Auction Selection Mechanisms | http://arxiv.org/abs/1211.3955 | id:1211.3955 author:H. Brendan McMahan, Omkar Muralidharan category:cs.GT cs.LG  published:2012-11-16 summary:Calibration is a basic property for prediction systems, and algorithms for achieving it are well-studied in both statistics and machine learning. In many applications, however, the predictions are used to make decisions that select which observations are made. This makes calibration difficult, as adjusting predictions to achieve calibration changes future data. We focus on click-through-rate (CTR) prediction for search ad auctions. Here, CTR predictions are used by an auction that determines which ads are shown, and we want to maximize the value generated by the auction. We show that certain natural notions of calibration can be impossible to achieve, depending on the details of the auction. We also show that it can be impossible to maximize auction efficiency while using calibrated predictions. Finally, we give conditions under which calibration is achievable and simultaneously maximizes auction efficiency: roughly speaking, bids and queries must not contain information about CTRs that is not already captured by the predictions. version:1
arxiv-1211-3901 | Visual Recognition of Isolated Swedish Sign Language Signs | http://arxiv.org/abs/1211.3901 | id:1211.3901 author:Saad Akram, Jonas Beskow, Hedvig Kjellstrom category:cs.CV  published:2012-11-16 summary:We present a method for recognition of isolated Swedish Sign Language signs. The method will be used in a game intended to help children training signing at home, as a complement to training with a teacher. The target group is not primarily deaf children, but children with language disorders. Using sign language as a support in conversation has been shown to greatly stimulate the speech development of such children. The signer is captured with an RGB-D (Kinect) sensor, which has three advantages over a regular RGB camera. Firstly, it allows complex backgrounds to be removed easily. We segment the hands and face based on skin color and depth information. Secondly, it helps with the resolution of hand over face occlusion. Thirdly, signs take place in 3D; some aspects of the signs are defined by hand motion vertically to the image plane. This motion can be estimated if the depth is observable. The 3D motion of the hands relative to the torso are used as a cue together with the hand shape, and HMMs trained with this input are used for classification. To obtain higher robustness towards differences across signers, Fisher Linear Discriminant Analysis is used to find the combinations of features that are most descriptive for each sign, regardless of signer. Experiments show that the system can distinguish signs from a challenging 94 word vocabulary with a precision of up to 94% in the signer dependent case and up to 47% in the signer independent case. version:1
arxiv-1211-3845 | A Bayesian Interpretation of the Particle Swarm Optimization and Its Kernel Extension | http://arxiv.org/abs/1211.3845 | id:1211.3845 author:Peter Andras category:cs.NE  published:2012-11-16 summary:Particle swarm optimization is a popular method for solving difficult optimization problems. There have been attempts to formulate the method in formal probabilistic or stochastic terms (e.g. bare bones particle swarm) with the aim to achieve more generality and explain the practical behavior of the method. Here we present a Bayesian interpretation of the particle swarm optimization. This interpretation provides a formal framework for incorporation of prior knowledge about the problem that is being solved. Furthermore, it also allows to extend the particle optimization method through the use of kernel functions that represent the intermediary transformation of the data into a different space where the optimization problem is expected to be easier to be resolved, such transformation can be seen as a form of prior knowledge about the nature of the optimization problem. We derive from the general Bayesian formulation the commonly used particle swarm methods as particular cases. version:1
arxiv-1011-4104 | Clustering and Latent Semantic Indexing Aspects of the Singular Value Decomposition | http://arxiv.org/abs/1011.4104 | id:1011.4104 author:Andri Mirzal category:cs.LG cs.NA math.SP 15A18  65F15  published:2010-11-17 summary:This paper discusses clustering and latent semantic indexing (LSI) aspects of the singular value decomposition (SVD). The purpose of this paper is twofold. The first is to give an explanation on how and why the singular vectors can be used in clustering. And the second is to show that the two seemingly unrelated SVD aspects actually originate from the same source: related vertices tend to be more clustered in the graph representation of lower rank approximate matrix using the SVD than in the original semantic graph. Accordingly, the SVD can improve retrieval performance of an information retrieval system since queries made to the approximate matrix can retrieve more relevant documents and filter out more irrelevant documents than the same queries made to the original matrix. By utilizing this fact, we will devise an LSI algorithm that mimicks SVD capability in clustering related vertices. Convergence analysis shows that the algorithm is convergent and produces a unique solution for each input. Experimental results using some standard datasets in LSI research show that retrieval performances of the algorithm are comparable to the SVD's. In addition, the algorithm is more practical and easier to use because there is no need to determine decomposition rank which is crucial in driving retrieval performance of the SVD. version:4
arxiv-1211-3643 | A Principled Approach to Grammars for Controlled Natural Languages and Predictive Editors | http://arxiv.org/abs/1211.3643 | id:1211.3643 author:Tobias Kuhn category:cs.CL  published:2012-11-15 summary:Controlled natural languages (CNL) with a direct mapping to formal logic have been proposed to improve the usability of knowledge representation systems, query interfaces, and formal specifications. Predictive editors are a popular approach to solve the problem that CNLs are easy to read but hard to write. Such predictive editors need to be able to "look ahead" in order to show all possible continuations of a given unfinished sentence. Such lookahead features, however, are difficult to implement in a satisfying way with existing grammar frameworks, especially if the CNL supports complex nonlocal structures such as anaphoric references. Here, methods and algorithms are presented for a new grammar notation called Codeco, which is specifically designed for controlled natural languages and predictive editors. A parsing approach for Codeco based on an extended chart parsing algorithm is presented. A large subset of Attempto Controlled English (ACE) has been represented in Codeco. Evaluation of this grammar and the parser implementation shows that the approach is practical, adequate and efficient. version:1
arxiv-1105-0167 | SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity | http://arxiv.org/abs/1105.0167 | id:1105.0167 author:Gang Niu, Bo Dai, Makoto Yamada, Masashi Sugiyama category:stat.ML cs.AI  published:2011-05-01 summary:We propose a general information-theoretic approach called Seraph (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, Seraph is regularized by encouraging a low-rank projection induced from the metric. The optimization of Seraph is solved efficiently and stably by an EM-like scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that Seraph compares favorably with many well-known global and local metric learning methods. version:3
arxiv-1207-3961 | Ensemble Clustering with Logic Rules | http://arxiv.org/abs/1207.3961 | id:1207.3961 author:Deniz Akdemir category:stat.ML cs.LG  published:2012-07-17 summary:In this article, the logic rule ensembles approach to supervised learning is applied to the unsupervised or semi-supervised clustering. Logic rules which were obtained by combining simple conjunctive rules are used to partition the input space and an ensemble of these rules is used to define a similarity matrix. Similarity partitioning is used to partition the data in an hierarchical manner. We have used internal and external measures of cluster validity to evaluate the quality of clusterings or to identify the number of clusters. version:3
arxiv-1211-3451 | Memory Capacity of a Random Neural Network | http://arxiv.org/abs/1211.3451 | id:1211.3451 author:Matt Stowe category:cs.NE  published:2012-11-14 summary:This paper considers the problem of information capacity of a random neural network. The network is represented by matrices that are square and symmetrical. The matrices have a weight which determines the highest and lowest possible value found in the matrix. The examined matrices are randomly generated and analyzed by a computer program. We find the surprising result that the capacity of the network is a maximum for the binary random neural network and it does not change as the number of quantization levels associated with the weights increases. version:1
arxiv-1211-3444 | Spectral Clustering: An empirical study of Approximation Algorithms and its Application to the Attrition Problem | http://arxiv.org/abs/1211.3444 | id:1211.3444 author:B. Cung, T. Jin, J. Ramirez, A. Thompson, C. Boutsidis, D. Needell category:cs.LG math.NA stat.ML  published:2012-11-14 summary:Clustering is the problem of separating a set of objects into groups (called clusters) so that objects within the same cluster are more similar to each other than to those in different clusters. Spectral clustering is a now well-known method for clustering which utilizes the spectrum of the data similarity matrix to perform this separation. Since the method relies on solving an eigenvector problem, it is computationally expensive for large datasets. To overcome this constraint, approximation methods have been developed which aim to reduce running time while maintaining accurate classification. In this article, we summarize and experimentally evaluate several approximation methods for spectral clustering. From an applications standpoint, we employ spectral clustering to solve the so-called attrition problem, where one aims to identify from a set of employees those who are likely to voluntarily leave the company from those who are not. Our study sheds light on the empirical performance of existing approximate spectral clustering methods and shows the applicability of these methods in an important business optimization related problem. version:1
arxiv-1211-3402 | Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship | http://arxiv.org/abs/1211.3402 | id:1211.3402 author:Bohdan Pavlyshenko category:cs.IR cs.CL  published:2012-11-14 summary:The genetic selection of keywords set, the text frequencies of which are considered as attributes in text classification analysis, has been analyzed. The genetic optimization was performed on a set of words, which is the fraction of the frequency dictionary with given frequency limits. The frequency dictionary was formed on the basis of analyzed text array of texts of English fiction. As the fitness function which is minimized by the genetic algorithm, the error of nearest k neighbors classifier was used. The obtained results show high precision and recall of texts classification by authorship categories on the basis of attributes of keywords set which were selected by the genetic algorithm from the frequency dictionary. version:1
arxiv-1211-3711 | Sequence Transduction with Recurrent Neural Networks | http://arxiv.org/abs/1211.3711 | id:1211.3711 author:Alex Graves category:cs.NE cs.LG stat.ML  published:2012-11-14 summary:Many machine learning tasks can be expressed as the transformation---or \emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus. version:1
arxiv-1211-3371 | A Comparison of Meta-heuristic Search for Interactive Software Design | http://arxiv.org/abs/1211.3371 | id:1211.3371 author:C. L. Simons, J. E. Smith category:cs.AI cs.NE  published:2012-11-14 summary:Advances in processing capacity, coupled with the desire to tackle problems where a human subjective judgment plays an important role in determining the value of a proposed solution, has led to a dramatic rise in the number of applications of Interactive Artificial Intelligence. Of particular note is the coupling of meta-heuristic search engines with user-provided evaluation and rating of solutions, usually in the form of Interactive Evolutionary Algorithms (IEAs). These have a well-documented history of successes, but arguably the preponderance of IEAs stems from this history, rather than as a conscious design choice of meta-heuristic based on the characteristics of the problem at hand. This paper sets out to examine the basis for that assumption, taking as a case study the domain of interactive software design. We consider a range of factors that should affect the design choice including ease of use, scalability, and of course, performance, i.e. that ability to generate good solutions within the limited number of evaluations available in interactive work before humans lose focus. We then evaluate three methods, namely greedy local search, an evolutionary algorithm and ant colony optimization, with a variety of representations for candidate solutions. Results show that after suitable parameter tuning, ant colony optimization is highly effective within interactive search and out-performs evolutionary algorithms with respect to increasing numbers of attributes and methods in the software design problem. However, when larger numbers of classes are present in the software design, an evolutionary algorithm using a naive grouping integer-based representation appears more scalable. version:1
arxiv-1211-3212 | Distributed Non-Stochastic Experts | http://arxiv.org/abs/1211.3212 | id:1211.3212 author:Varun Kanade, Zhenming Liu, Bozidar Radunovic category:cs.LG cs.AI  published:2012-11-14 summary:We consider the online distributed non-stochastic experts problem, where the distributed system consists of one coordinator node that is connected to $k$ sites, and the sites are required to communicate with each other via the coordinator. At each time-step $t$, one of the $k$ site nodes has to pick an expert from the set ${1, ..., n}$, and the same site receives information about payoffs of all experts for that round. The goal of the distributed system is to minimize regret at time horizon $T$, while simultaneously keeping communication to a minimum. The two extreme solutions to this problem are: (i) Full communication: This essentially simulates the non-distributed setting to obtain the optimal $O(\sqrt{\log(n)T})$ regret bound at the cost of $T$ communication. (ii) No communication: Each site runs an independent copy : the regret is $O(\sqrt{log(n)kT})$ and the communication is 0. This paper shows the difficulty of simultaneously achieving regret asymptotically better than $\sqrt{kT}$ and communication better than $T$. We give a novel algorithm that for an oblivious adversary achieves a non-trivial trade-off: regret $O(\sqrt{k^{5(1+\epsilon)/6} T})$ and communication $O(T/k^{\epsilon})$, for any value of $\epsilon \in (0, 1/5)$. We also consider a variant of the model, where the coordinator picks the expert. In this model, we show that the label-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us strategy that is near optimal in regret vs communication trade-off. version:1
arxiv-1211-3412 | Network Sampling: From Static to Streaming Graphs | http://arxiv.org/abs/1211.3412 | id:1211.3412 author:Nesreen K. Ahmed, Jennifer Neville, Ramana Kompella category:cs.SI cs.DS cs.LG physics.soc-ph stat.ML  published:2012-11-14 summary:Network sampling is integral to the analysis of social, information, and biological networks. Since many real-world networks are massive in size, continuously evolving, and/or distributed in nature, the network structure is often sampled in order to facilitate study. For these reasons, a more thorough and complete understanding of network sampling is critical to support the field of network science. In this paper, we outline a framework for the general problem of network sampling, by highlighting the different objectives, population and units of interest, and classes of network sampling methods. In addition, we propose a spectrum of computational models for network sampling methods, ranging from the traditionally studied model based on the assumption of a static domain to a more challenging model that is appropriate for streaming domains. We design a family of sampling methods based on the concept of graph induction that generalize across the full spectrum of computational models (from static to streaming) while efficiently preserving many of the topological properties of the input graphs. Furthermore, we demonstrate how traditional static sampling algorithms can be modified for graph streams for each of the three main classes of sampling methods: node, edge, and topology-based sampling. Our experimental results indicate that our proposed family of sampling methods more accurately preserves the underlying properties of the graph for both static and streaming graphs. Finally, we study the impact of network sampling algorithms on the parameter estimation and performance evaluation of relational classification algorithms. version:1
arxiv-1201-4058 | On the Prior and Posterior Distributions Used in Graphical Modelling | http://arxiv.org/abs/1201.4058 | id:1201.4058 author:Marco Scutari category:math.ST stat.ML stat.TH  published:2012-01-19 summary:Graphical model learning and inference are often performed using Bayesian techniques. In particular, learning is usually performed in two separate steps. First, the graph structure is learned from the data; then the parameters of the model are estimated conditional on that graph structure. While the probability distributions involved in this second step have been studied in depth, the ones used in the first step have not been explored in as much detail. In this paper, we will study the prior and posterior distributions defined over the space of the graph structures for the purpose of learning the structure of a graphical model. In particular, we will provide a characterisation of the behaviour of those distributions as a function of the possible edges of the graph. We will then use the properties resulting from this characterisation to define measures of structural variability for both Bayesian and Markov networks, and we will point out some of their possible applications. version:2
arxiv-1211-3010 | Time-series Scenario Forecasting | http://arxiv.org/abs/1211.3010 | id:1211.3010 author:Sriharsha Veeramachaneni category:stat.ML cs.LG stat.AP  published:2012-11-13 summary:Many applications require the ability to judge uncertainty of time-series forecasts. Uncertainty is often specified as point-wise error bars around a mean or median forecast. Due to temporal dependencies, such a method obscures some information. We would ideally have a way to query the posterior probability of the entire time-series given the predictive variables, or at a minimum, be able to draw samples from this distribution. We use a Bayesian dictionary learning algorithm to statistically generate an ensemble of forecasts. We show that the algorithm performs as well as a physics-based ensemble method for temperature forecasts for Houston. We conclude that the method shows promise for scenario forecasting where physics-based methods are absent. version:1
arxiv-1211-2980 | Shattering-Extremal Systems | http://arxiv.org/abs/1211.2980 | id:1211.2980 author:Shay Moran category:math.CO cs.CG cs.DM cs.LG  published:2012-11-13 summary:The Shatters relation and the VC dimension have been investigated since the early seventies. These concepts have found numerous applications in statistics, combinatorics, learning theory and computational geometry. Shattering extremal systems are set-systems with a very rich structure and many different characterizations. The goal of this thesis is to elaborate on the structure of these systems. version:1
arxiv-1211-2945 | The application of a perceptron model to classify an individual's response to a proposed loading dose regimen of Warfarin | http://arxiv.org/abs/1211.2945 | id:1211.2945 author:Cen Wan, Irina V. Biktasheva, Steven Lane category:stat.AP cs.NE 68T05  92C50  published:2012-11-13 summary:The dose regimen of Warfarin is separated into two phases. Firstly a loading dose is given, which is designed to bring the International Normalisation Ratio (INR) to within therapeutic range. Then a stable maintenance dose is given to maintain the INR within therapeutic range. In the United Kingdom (UK) the loading dose is usually given as three individual daily doses, the standard loading dose being 10mg on days one and two and 5mgs on day three, which can be varied at the discretion of the clinician. However, due to the large inter-individual variation in the response to Warfarin therapy, it is difficult to identify which patients will reach the narrow therapeutic window for target INR, and which will be above or below the therapeutic window. The aim of this research was to develop a methodology using a neural networks classification algorithm and data mining techniques to predict for a given loading dose and patient characteristics if the patient is more likely to achieve target INR or more likely to be above or below therapeutic range. Multilayer perceptron (MLP) and 10-fold stratified cross validation algorithms were used to determine an artificial neural network to classify patients' response to their initial Warfarin loading dose. The resulting neural network model correctly classifies an individual's response to their Warfarin loading dose over 80% of the time. As well as taking into account the initial loading dose, the final model also includes demographic, genetic and a number of other potential confounding factors. With this model clinicians can predetermine whether a given loading regimen, along with specific patient characteristics will achieve a therapeutic response for a particular patient. Thus tailoring the loading dose regimen to meet the individual needs of the patient and reducing the risk of adverse drug reactions associated with Warfarin. version:1
arxiv-0903-5255 | Sure independence screening in generalized linear models with NP-dimensionality | http://arxiv.org/abs/0903.5255 | id:0903.5255 author:Jianqing Fan, Rui Song category:stat.ME math.ST stat.ML stat.TH  published:2009-03-30 summary:Ultrahigh-dimensional variable selection plays an increasingly important role in contemporary scientific discoveries and statistical research. Among others, Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] propose an independent screening framework by ranking the marginal correlations. They showed that the correlation ranking procedure possesses a sure independence screening property within the context of the linear model with Gaussian covariates and responses. In this paper, we propose a more general version of the independent learning with ranking the maximum marginal likelihood estimates or the maximum marginal likelihood itself in generalized linear models. We show that the proposed methods, with Fan and Lv [J. R. Stat. Soc. Ser. B Stat. Methodol. 70 (2008) 849-911] as a very special case, also possess the sure screening property with vanishing false selection rate. The conditions under which the independence learning possesses a sure screening is surprisingly simple. This justifies the applicability of such a simple method in a wide spectrum. We quantify explicitly the extent to which the dimensionality can be reduced by independence screening, which depends on the interactions of the covariance matrix of covariates and true parameters. Simulation studies are used to illustrate the utility of the proposed approaches. In addition, we establish an exponential inequality for the quasi-maximum likelihood estimator which is useful for high-dimensional statistical learning. version:5
arxiv-1211-2891 | Boosting Simple Collaborative Filtering Models Using Ensemble Methods | http://arxiv.org/abs/1211.2891 | id:1211.2891 author:Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, Alon Schclar category:cs.IR cs.LG stat.ML  published:2012-11-13 summary:In this paper we examine the effect of applying ensemble learning to the performance of collaborative filtering methods. We present several systematic approaches for generating an ensemble of collaborative filtering models based on a single collaborative filtering algorithm (single-model or homogeneous ensemble). We present an adaptation of several popular ensemble techniques in machine learning for the collaborative filtering domain, including bagging, boosting, fusion and randomness injection. We evaluate the proposed approach on several types of collaborative filtering base models: k- NN, matrix factorization and a neighborhood matrix factorization model. Empirical evaluation shows a prediction improvement compared to all base CF algorithms. In particular, we show that the performance of an ensemble of simple (weak) CF models such as k-NN is competitive compared with a single strong CF model (such as matrix factorization) while requiring an order of magnitude less computational cost. version:1
arxiv-1211-2863 | Multi-Sensor Fusion via Reduction of Dimensionality | http://arxiv.org/abs/1211.2863 | id:1211.2863 author:Alon Schclar category:cs.CV  published:2012-11-13 summary:Large high-dimensional datasets are becoming more and more popular in an increasing number of research areas. Processing the high dimensional data incurs a high computational cost and is inherently inefficient since many of the values that describe a data object are redundant due to noise and inner correlations. Consequently, the dimensionality, i.e. the number of values that are used to describe a data object, needs to be reduced prior to any other processing of the data. The dimensionality reduction removes, in most cases, noise from the data and reduces substantially the computational cost of algorithms that are applied to the data. In this thesis, a novel coherent integrated methodology is introduced (theory, algorithm and applications) to reduce the dimensionality of high-dimensional datasets. The method constructs a diffusion process among the data coordinates via a random walk. The dimensionality reduction is obtained based on the eigen-decomposition of the Markov matrix that is associated with the random walk. The proposed method is utilized for: (a) segmentation and detection of anomalies in hyper-spectral images; (b) segmentation of multi-contrast MRI images; and (c) segmentation of video sequences. We also present algorithms for: (a) the characterization of materials using their spectral signatures to enable their identification; (b) detection of vehicles according to their acoustic signatures; and (c) classification of vascular vessels recordings to detect hyper-tension and cardio-vascular diseases. The proposed methodology and algorithms produce excellent results that successfully compete with current state-of-the-art algorithms. version:1
arxiv-1206-3099 | Sparse Distributed Learning Based on Diffusion Adaptation | http://arxiv.org/abs/1206.3099 | id:1206.3099 author:Paolo Di Lorenzo, Ali H. Sayed category:cs.LG cs.DC  published:2012-06-14 summary:This article proposes diffusion LMS strategies for distributed estimation over adaptive networks that are able to exploit sparsity in the underlying system model. The approach relies on convex regularization, common in compressive sensing, to enhance the detection of sparsity via a diffusive process over the network. The resulting algorithms endow networks with learning abilities and allow them to learn the sparse structure from the incoming data in real-time, and also to track variations in the sparsity of the model. We provide convergence and mean-square performance analysis of the proposed method and show under what conditions it outperforms the unregularized diffusion version. We also show how to adaptively select the regularization parameter. Simulation results illustrate the advantage of the proposed filters for sparse data recovery. version:2
arxiv-1211-2742 | Sketch Recognition using Domain Classification | http://arxiv.org/abs/1211.2742 | id:1211.2742 author:Vasudha Vashisht, Tanupriya Choudhury, T. V. Prasad category:cs.CV cs.HC  published:2012-11-12 summary:Conceptualizing away the sketch processing details in a user interface will enable general users and domain experts to create more complex sketches. There are many domains for which sketch recognition systems are being developed. But they entail image-processing skill if they are to handle the details of each domain, and also they are lengthy to build. The implemented system goal is to enable user interface designers and domain experts who may not have proficiency in sketch recognition to be able to construct these sketch systems. This sketch recognition system takes in rough sketches from user drawn with the help of mouse as its input. It then recognizes the sketch using segmentation and domain classification, the properties of the user drawn sketch and segments are searched heuristically in the domains and each figures of each domain, and finally it shows its domain, the figure name and properties. It also draws the sketch smoothly. The work is resulted through extensive research and study of many existing image processing and pattern matching algorithms. version:1
arxiv-1211-2741 | A Hindi Speech Actuated Computer Interface for Web Search | http://arxiv.org/abs/1211.2741 | id:1211.2741 author:Kamlesh Sharma, S. V. A. V. Prasad, T. V. Prasad category:cs.CL cs.HC cs.IR  published:2012-11-12 summary:Aiming at increasing system simplicity and flexibility, an audio evoked based system was developed by integrating simplified headphone and user-friendly software design. This paper describes a Hindi Speech Actuated Computer Interface for Web search (HSACIWS), which accepts spoken queries in Hindi language and provides the search result on the screen. This system recognizes spoken queries by large vocabulary continuous speech recognition (LVCSR), retrieves relevant document by text retrieval, and provides the search result on the Web by the integration of the Web and the voice systems. The LVCSR in this system showed enough performance levels for speech with acoustic and language models derived from a query corpus with target contents. version:1
arxiv-1211-2717 | Proximal Stochastic Dual Coordinate Ascent | http://arxiv.org/abs/1211.2717 | id:1211.2717 author:Shai Shalev-Shwartz, Tong Zhang category:stat.ML cs.LG math.OC  published:2012-11-12 summary:We introduce a proximal version of dual coordinate ascent method. We demonstrate how the derived algorithmic framework can be used for numerous regularized loss minimization problems, including $\ell_1$ regularization and structured output SVM. The convergence rates we obtain match, and sometimes improve, state-of-the-art results. version:1
arxiv-1211-1550 | A Riemannian geometry for low-rank matrix completion | http://arxiv.org/abs/1211.1550 | id:1211.1550 author:B. Mishra, K. Adithya Apuroop, R. Sepulchre category:cs.LG cs.NA math.OC  published:2012-11-07 summary:We propose a new Riemannian geometry for fixed-rank matrices that is specifically tailored to the low-rank matrix completion problem. Exploiting the degree of freedom of a quotient space, we tune the metric on our search space to the particular least square cost function. At one level, it illustrates in a novel way how to exploit the versatile framework of optimization on quotient manifold. At another level, our algorithm can be considered as an improved version of LMaFit, the state-of-the-art Gauss-Seidel algorithm. We develop necessary tools needed to perform both first-order and second-order optimization. In particular, we propose gradient descent schemes (steepest descent and conjugate gradient) and trust-region algorithms. We also show that, thanks to the simplicity of the cost function, it is numerically cheap to perform an exact linesearch given a search direction, which makes our algorithms competitive with the state-of-the-art on standard low-rank matrix completion instances. version:2
arxiv-1211-2699 | A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete Wavelet Transform Domain using Two Subbands | http://arxiv.org/abs/1211.2699 | id:1211.2699 author:Abdur Shahid, Shahriar Badsha, Md. Rethwan Kabeer, Junaid Ahsan, Mufti Mahmud category:cs.MM cs.CV  published:2012-11-12 summary:Digital watermarking is the process to hide digital pattern directly into a digital content. Digital watermarking techniques are used to address digital rights management, protect information and conceal secrets. An invisible non-blind watermarking approach for gray scale images is proposed in this paper. The host image is decomposed into 3-levels using Discrete Wavelet Transform. Based on the parent-child relationship between the wavelet coefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression algorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the significant coefficients. The most significant coefficients of LH2 and HL2 bands are selected to embed a binary watermark image. The selected significant coefficients are modulated using Noise Visibility Function, which is considered as the best strength to ensure better imperceptibility. The approach is tested against various image processing attacks such as addition of noise, filtering, cropping, JPEG compression, histogram equalization and contrast adjustment. The experimental results reveal the high effectiveness of the method. version:1
arxiv-1109-0486 | The Variational Garrote | http://arxiv.org/abs/1109.0486 | id:1109.0486 author:Hilbert J. Kappen, Vicenç Gómez category:stat.ME cs.LG  published:2011-09-02 summary:In this paper, we present a new variational method for sparse regression using $L_0$ regularization. The variational parameters appear in the approximate model in a way that is similar to Breiman's Garrote model. We refer to this method as the variational Garrote (VG). We show that the combination of the variational approximation and $L_0$ regularization has the effect of making the problem effectively of maximal rank even when the number of samples is small compared to the number of variables. The VG is compared numerically with the Lasso method, ridge regression and the recently introduced paired mean field method (PMF) (M. Titsias & M. L\'azaro-Gredilla., NIPS 2012). Numerical results show that the VG and PMF yield more accurate predictions and more accurately reconstruct the true model than the other methods. It is shown that the VG finds correct solutions when the Lasso solution is inconsistent due to large input correlations. Globally, VG is significantly faster than PMF and tends to perform better as the problems become denser and in problems with strongly correlated inputs. The naive implementation of the VG scales cubic with the number of features. By introducing Lagrange multipliers we obtain a dual formulation of the problem that scales cubic in the number of samples, but close to linear in the number of features. version:3
arxiv-1211-2556 | A Comparative Study of Gaussian Mixture Model and Radial Basis Function for Voice Recognition | http://arxiv.org/abs/1211.2556 | id:1211.2556 author:Fatai Adesina Anifowose category:cs.LG cs.CV stat.ML  published:2012-11-12 summary:A comparative study of the application of Gaussian Mixture Model (GMM) and Radial Basis Function (RBF) in biometric recognition of voice has been carried out and presented. The application of machine learning techniques to biometric authentication and recognition problems has gained a widespread acceptance. In this research, a GMM model was trained, using Expectation Maximization (EM) algorithm, on a dataset containing 10 classes of vowels and the model was used to predict the appropriate classes using a validation dataset. For experimental validity, the model was compared to the performance of two different versions of RBF model using the same learning and validation datasets. The results showed very close recognition accuracy between the GMM and the standard RBF model, but with GMM performing better than the standard RBF by less than 1% and the two models outperformed similar models reported in literature. The DTREG version of RBF outperformed the other two models by producing 94.8% recognition accuracy. In terms of recognition time, the standard RBF was found to be the fastest among the three models. version:1
arxiv-1211-2502 | New Edge Detection Technique based on the Shannon Entropy in Gray Level Images | http://arxiv.org/abs/1211.2502 | id:1211.2502 author:Mohamed A. El-Sayed, Tarek Abd-El Hafeez category:cs.CV 94A17  68U10  94A08  published:2012-11-12 summary:Edge detection is an important field in image processing. Edges characterize object boundaries and are therefore useful for segmentation, registration, feature extraction, and identification of objects in a scene. In this paper, an approach utilizing an improvement of Baljit and Amar method which uses Shannon entropy other than the evaluation of derivatives of the image in detecting edges in gray level images has been proposed. The proposed method can reduce the CPU time required for the edge detection process and the quality of the edge detector of the output images is robust. A standard test images, the real-world and synthetic images are used to compare the results of the proposed edge detector with the Baljit and Amar edge detector method. In order to validate the results, the run time of the proposed method and the pervious method are presented. It has been observed that the proposed edge detector works effectively for different gray scale digital images. The performance evaluation of the proposed technique in terms of the measured CPU time and the quality of edge detector method are presented. Experimental results demonstrate that the proposed method achieve better result than the relevant classic method. version:1
arxiv-1211-2500 | A New Algorithm Based Entropic Threshold for Edge Detection in Images | http://arxiv.org/abs/1211.2500 | id:1211.2500 author:Mohamed A. El-Sayed category:cs.CV 94A17  68U10  94A08  published:2012-11-12 summary:Edge detection is one of the most critical tasks in automatic image analysis. There exists no universal edge detection method which works well under all conditions. This paper shows the new approach based on the one of the most efficient techniques for edge detection, which is entropy-based thresholding. The main advantages of the proposed method are its robustness and its flexibility. We present experimental results for this method, and compare results of the algorithm against several leading edge detection methods, such as Canny, LOG, and Sobel. Experimental results demonstrate that the proposed method achieves better result than some classic methods and the quality of the edge detector of the output images is robust and decrease the computation time. version:1
arxiv-1206-5349 | Provable ICA with Unknown Gaussian Noise, and Implications for Gaussian Mixtures and Autoencoders | http://arxiv.org/abs/1206.5349 | id:1206.5349 author:Sanjeev Arora, Rong Ge, Ankur Moitra, Sushant Sachdeva category:cs.LG cs.DS  published:2012-06-23 summary:We present a new algorithm for Independent Component Analysis (ICA) which has provable performance guarantees. In particular, suppose we are given samples of the form $y = Ax + \eta$ where $A$ is an unknown $n \times n$ matrix and $x$ is a random variable whose components are independent and have a fourth moment strictly less than that of a standard Gaussian random variable and $\eta$ is an $n$-dimensional Gaussian random variable with unknown covariance $\Sigma$: We give an algorithm that provable recovers $A$ and $\Sigma$ up to an additive $\epsilon$ and whose running time and sample complexity are polynomial in $n$ and $1 / \epsilon$. To accomplish this, we introduce a novel "quasi-whitening" step that may be useful in other contexts in which the covariance of Gaussian noise is not known in advance. We also give a general framework for finding all local optima of a function (given an oracle for approximately finding just one) and this is a crucial step in our algorithm, one that has been overlooked in previous attempts, and allows us to control the accumulation of error when we find the columns of $A$ one by one via local search. version:2
arxiv-1211-2476 | Random Utility Theory for Social Choice | http://arxiv.org/abs/1211.2476 | id:1211.2476 author:Hossein Azari Soufiani, David C. Parkes, Lirong Xia category:cs.MA cs.LG stat.ML  published:2012-11-11 summary:Random utility theory models an agent's preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received significant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce. version:1
arxiv-1210-5486 | A Lightweight Stemmer for Gujarati | http://arxiv.org/abs/1210.5486 | id:1210.5486 author:Juhi Ameta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2012-10-19 summary:Gujarati is a resource poor language with almost no language processing tools being available. In this paper we have shown an implementation of a rule based stemmer of Gujarati. We have shown the creation of rules for stemming and the richness in morphology that Gujarati possesses. We have also evaluated our results by verifying it with a human expert. version:2
arxiv-1211-2378 | Hybrid methodology for hourly global radiation forecasting in Mediterranean area | http://arxiv.org/abs/1211.2378 | id:1211.2378 author:Cyril Voyant, Marc Muselli, Christophe Paoli, Marie Laure Nivet category:cs.NE cs.LG physics.ao-ph stat.AP  published:2012-11-11 summary:The renewable energies prediction and particularly global radiation forecasting is a challenge studied by a growing number of research teams. This paper proposes an original technique to model the insolation time series based on combining Artificial Neural Network (ANN) and Auto-Regressive and Moving Average (ARMA) model. While ANN by its non-linear nature is effective to predict cloudy days, ARMA techniques are more dedicated to sunny days without cloud occurrences. Thus, three hybrids models are suggested: the first proposes simply to use ARMA for 6 months in spring and summer and to use an optimized ANN for the other part of the year; the second model is equivalent to the first but with a seasonal learning; the last model depends on the error occurred the previous hour. These models were used to forecast the hourly global radiation for five places in Mediterranean area. The forecasting performance was compared among several models: the 3 above mentioned models, the best ANN and ARMA for each location. In the best configuration, the coupling of ANN and ARMA allows an improvement of more than 1%, with a maximum in autumn (3.4%) and a minimum in winter (0.9%) where ANN alone is the best. version:1
arxiv-1206-1898 | A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function | http://arxiv.org/abs/1206.1898 | id:1206.1898 author:Pedro A. Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, Daniel A. Braun category:stat.ML cs.AI math.ST stat.TH  published:2012-06-09 summary:We propose a novel Bayesian approach to solve stochastic optimization problems that involve finding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of first, doing inference over the function space and second, finding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. We illustrate the effectiveness of our model by optimizing a noisy, high-dimensional, non-convex objective function. version:2
arxiv-1211-2304 | Probabilistic Combination of Classifier and Cluster Ensembles for Non-transductive Learning | http://arxiv.org/abs/1211.2304 | id:1211.2304 author:Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Badrul Sarwar, Jean-David Ruvini category:cs.LG stat.ML  published:2012-11-10 summary:Unsupervised models can provide supplementary soft constraints to help classify new target data under the assumption that similar objects in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place. This paper describes a Bayesian framework that takes as input class labels from existing classifiers (designed based on labeled data from the source domain), as well as cluster labels from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework is particularly useful when the statistics of the target data drift or change from those of the training data. We also show that the proposed framework is privacy-aware and allows performing distributed learning when data/models have sharing restrictions. Experiments show that our framework can yield superior results to those provided by applying classifier ensembles only. version:1
arxiv-1211-2290 | Dating Texts without Explicit Temporal Cues | http://arxiv.org/abs/1211.2290 | id:1211.2290 author:Abhimanu Kumar, Jason Baldridge, Matthew Lease, Joydeep Ghosh category:cs.CL cs.AI  published:2012-11-10 summary:This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely {\it solely} on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals' lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for non-biographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts. version:1
arxiv-1211-2264 | Calibrated Elastic Regularization in Matrix Completion | http://arxiv.org/abs/1211.2264 | id:1211.2264 author:Tingni Sun, Cun-Hui Zhang category:math.ST stat.ML stat.TH  published:2012-11-09 summary:This paper concerns the problem of matrix completion, which is to estimate a matrix from observations in a small subset of indices. We propose a calibrated spectrum elastic net method with a sum of the nuclear and Frobenius penalties and develop an iterative algorithm to solve the convex minimization problem. The iterative algorithm alternates between imputing the missing entries in the incomplete matrix by the current guess and estimating the matrix by a scaled soft-thresholding singular value decomposition of the imputed matrix until the resulting matrix converges. A calibration step follows to correct the bias caused by the Frobenius penalty. Under proper coherence conditions and for suitable penalties levels, we prove that the proposed estimator achieves an error bound of nearly optimal order and in proportion to the noise level. This provides a unified analysis of the noisy and noiseless matrix completion problems. Simulation results are presented to compare our proposal with previous ones. version:1
arxiv-1211-2260 | No-Regret Algorithms for Unconstrained Online Convex Optimization | http://arxiv.org/abs/1211.2260 | id:1211.2260 author:Matthew Streeter, H. Brendan McMahan category:cs.LG  published:2012-11-09 summary:Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x^* are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of x^*. In particular, regret with respect to x^* = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting. version:1
arxiv-1211-2150 | NF-SAVO: Neuro-Fuzzy system for Arabic Video OCR | http://arxiv.org/abs/1211.2150 | id:1211.2150 author:Mohamed Ben Halima, Hichem karray, Adel. M. Alimi, Ana Fernández Vila category:cs.CV  published:2012-11-09 summary:In this paper we propose a robust approach for text extraction and recognition from video clips which is called Neuro-Fuzzy system for Arabic Video OCR. In Arabic video text recognition, a number of noise components provide the text relatively more complicated to separate from the background. Further, the characters can be moving or presented in a diversity of colors, sizes and fonts that are not uniform. Added to this, is the fact that the background is usually moving making text extraction a more intricate process. Video include two kinds of text, scene text and artificial text. Scene text is usually text that becomes part of the scene itself as it is recorded at the time of filming the scene. But artificial text is produced separately and away from the scene and is laid over it at a later stage or during the post processing time. The emergence of artificial text is consequently vigilantly directed. This type of text carries with it important information that helps in video referencing, indexing and retrieval. version:1
arxiv-1211-2116 | Localisation of Numerical Date Field in an Indian Handwritten Document | http://arxiv.org/abs/1211.2116 | id:1211.2116 author:S Arunkumar, Pallab Kumar Sahu, Sudeep Gorai, Kalyan Ghosh category:cs.CV  published:2012-11-09 summary:This paper describes a method to localise all those areas which may constitute the date field in an Indian handwritten document. Spatial patterns of the date field are studied from various handwritten documents and an algorithm is developed through statistical analysis to identify those sets of connected components which may constitute the date. Common date patterns followed in India are considered to classify the date formats in different classes. Reported results demonstrate promising performance of the proposed approach version:1
arxiv-1211-1544 | Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds | http://arxiv.org/abs/1211.1544 | id:1211.1544 author:Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling category:cs.CV cs.LG  published:2012-11-07 summary:Image denoising can be described as the problem of mapping from a noisy image to a noise-free image. The best currently available denoising methods approximate this mapping with cleverly engineered algorithms. In this work we attempt to learn this mapping directly with plain multi layer perceptrons (MLP) applied to image patches. We will show that by training on large image databases we are able to outperform the current state-of-the-art image denoising methods. In addition, our method achieves results that are superior to one type of theoretical bound and goes a large way toward closing the gap with a second type of theoretical bound. Our approach is easily adapted to less extensively studied types of noise, such as mixed Poisson-Gaussian noise, JPEG artifacts, salt-and-pepper noise and noise resembling stripes, for which we achieve excellent results as well. We will show that combining a block-matching procedure with MLPs can further improve the results on certain images. In a second paper, we detail the training trade-offs and the inner mechanisms of our MLPs. version:3
arxiv-1211-6340 | An Approach of Improving Students Academic Performance by using k means clustering algorithm and Decision tree | http://arxiv.org/abs/1211.6340 | id:1211.6340 author:Md. Hedayetul Islam Shovon, Mahfuza Haque category:cs.LG  published:2012-11-09 summary:Improving students academic performance is not an easy task for the academic community of higher learning. The academic performance of engineering and science students during their first year at university is a turning point in their educational path and usually encroaches on their General Point Average,GPA in a decisive manner. The students evaluation factors like class quizzes mid and final exam assignment lab work are studied. It is recommended that all these correlated information should be conveyed to the class teacher before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students. In this paper, we present a hybrid procedure based on Decision Tree of Data mining method and Data Clustering that enables academicians to predict students GPA and based on that instructor can take necessary step to improve student academic performance. version:1
arxiv-1211-2082 | 3D Surface Reconstruction of Underwater Objects | http://arxiv.org/abs/1211.2082 | id:1211.2082 author:C. J. Prabhakar, P. U. Praveen Kumar category:cs.CV  published:2012-11-09 summary:In this paper, we propose a novel technique to reconstruct 3D surface of an underwater object using stereo images. Reconstructing the 3D surface of an underwater object is really a challenging task due to degraded quality of underwater images. There are various reason of quality degradation of underwater images i.e., non-uniform illumination of light on the surface of objects, scattering and absorption effects. Floating particles present in underwater produces Gaussian noise on the captured underwater images which degrades the quality of images. The degraded underwater images are preprocessed by applying homomorphic, wavelet denoising and anisotropic filtering sequentially. The uncalibrated rectification technique is applied to preprocessed images to rectify the left and right images. The rectified left and right image lies on a common plane. To find the correspondence points in a left and right images, we have applied dense stereo matching technique i.e., graph cut method. Finally, we estimate the depth of images using triangulation technique. The experimental result shows that the proposed method reconstruct 3D surface of underwater objects accurately using captured underwater stereo images. version:1
arxiv-1211-2073 | LAGE: A Java Framework to reconstruct Gene Regulatory Networks from Large-Scale Continues Expression Data | http://arxiv.org/abs/1211.2073 | id:1211.2073 author:Yang Lu, Mengying Wang, Kenny Q. Zhu, Bo Yuan category:cs.LG cs.CE q-bio.QM stat.ML  published:2012-11-09 summary:LAGE is a systematic framework developed in Java. The motivation of LAGE is to provide a scalable and parallel solution to reconstruct Gene Regulatory Networks (GRNs) from continuous gene expression data for very large amount of genes. The basic idea of our framework is motivated by the philosophy of divideand-conquer. Specifically, LAGE recursively partitions genes into multiple overlapping communities with much smaller sizes, learns intra-community GRNs respectively before merge them altogether. Besides, the complete information of overlapping communities serves as the byproduct, which could be used to mine meaningful functional modules in biological networks. version:1
arxiv-1211-2037 | Time Complexity Analysis of Binary Space Partitioning Scheme for Image Compression | http://arxiv.org/abs/1211.2037 | id:1211.2037 author:Rehna V. J., M. K. Jeyakumar category:cs.CV  published:2012-11-09 summary:Segmentation-based image coding methods provide high compression ratios when compared with traditional image coding approaches like the transform and sub band coding for low bit-rate compression applications. In this paper, a segmentation-based image coding method, namely the Binary Space Partition scheme, that divides the desired image using a recursive procedure for coding is presented. The BSP approach partitions the desired image recursively by using bisecting lines, selected from a collection of discrete optional lines, in a hierarchical manner. This partitioning procedure generates a binary tree, which is referred to as the BSP-tree representation of the desired image. The algorithm is extremely complex in computation and has high execution time. The time complexity of the BSP scheme is explored in this work. version:1
arxiv-1211-2007 | Multi-input Multi-output Beta Wavelet Network: Modeling of Acoustic Units for Speech Recognition | http://arxiv.org/abs/1211.2007 | id:1211.2007 author:Ridha Ejbali, Mourad Zaied, Chokri Ben Amar category:cs.CV  published:2012-11-08 summary:In this paper, we propose a novel architecture of wavelet network called Multi-input Multi-output Wavelet Network MIMOWN as a generalization of the old architecture of wavelet network. This newel prototype was applied to speech recognition application especially to model acoustic unit of speech. The originality of our work is the proposal of MIMOWN to model acoustic unit of speech. This approach was proposed to overcome limitation of old wavelet network model. The use of the multi-input multi-output architecture will allows training wavelet network on various examples of acoustic units. version:1
arxiv-1211-1526 | Explosion prediction of oil gas using SVM and Logistic Regression | http://arxiv.org/abs/1211.1526 | id:1211.1526 author:Xiaofei Wang, Mingming Zhang, Liyong Shen, Suixiang Gao category:cs.CE cs.LG 62P30  68T05  published:2012-11-07 summary:The prevention of dangerous chemical accidents is a primary problem of industrial manufacturing. In the accidents of dangerous chemicals, the oil gas explosion plays an important role. The essential task of the explosion prevention is to estimate the better explosion limit of a given oil gas. In this paper, Support Vector Machines (SVM) and Logistic Regression (LR) are used to predict the explosion of oil gas. LR can get the explicit probability formula of explosion, and the explosive range of the concentrations of oil gas according to the concentration of oxygen. Meanwhile, SVM gives higher accuracy of prediction. Furthermore, considering the practical requirements, the effects of penalty parameter on the distribution of two types of errors are discussed. version:2
arxiv-1211-1800 | A Comparative study of Arabic handwritten characters invariant feature | http://arxiv.org/abs/1211.1800 | id:1211.1800 author:Hamdi Hassen, Maher khemakhem category:cs.CV  published:2012-11-08 summary:This paper is practically interested in the unchangeable feature of Arabic handwritten character. It presents results of comparative study achieved on certain features extraction techniques of handwritten character, based on Hough transform, Fourier transform, Wavelet transform and Gabor Filter. Obtained results show that Hough Transform and Gabor filter are insensible to the rotation and translation, Fourier Transform is sensible to the rotation but insensible to the translation, in contrast to Hough Transform and Gabor filter, Wavelets Transform is sensitive to the rotation as well as to the translation. version:1
arxiv-1211-1799 | Algorithm for Missing Values Imputation in Categorical Data with Use of Association Rules | http://arxiv.org/abs/1211.1799 | id:1211.1799 author:Jiří Kaiser category:cs.LG  published:2012-11-08 summary:This paper presents algorithm for missing values imputation in categorical data. The algorithm is based on using association rules and is presented in three variants. Experimental shows better accuracy of missing values imputation using the algorithm then using most common attribute value. version:1
arxiv-1207-3142 | Color Constancy based on Image Similarity via Bilayer Sparse Coding | http://arxiv.org/abs/1207.3142 | id:1207.3142 author:Bing Li, Weihua Xiong, Weiming Hu category:cs.CV  published:2012-07-13 summary:Computational color constancy is a very important topic in computer vision and has attracted many researchers' attention. Recently, lots of research has shown the effects of high level visual content information for illumination estimation. However, all of these existing methods are essentially combinational strategies in which image's content analysis is only used to guide the combination or selection from a variety of individual illumination estimation methods. In this paper, we propose a novel bilayer sparse coding model for illumination estimation that considers image similarity in terms of both low level color distribution and high level image scene content simultaneously. For the purpose, the image's scene content information is integrated with its color distribution to obtain optimal illumination estimation model. The experimental results on two real-world image sets show that our algorithm is superior to other prevailing illumination estimation methods, even better than combinational methods. version:2
arxiv-1211-1752 | 3D Scene Grammar for Parsing RGB-D Pointclouds | http://arxiv.org/abs/1211.1752 | id:1211.1752 author:Abhishek Anand, Sherwin Li category:cs.CV  published:2012-11-08 summary:We pose 3D scene-understanding as a problem of parsing in a grammar. A grammar helps us capture the compositional structure of real-word objects, e.g., a chair is composed of a seat, a back-rest and some legs. Having multiple rules for an object helps us capture structural variations in objects, e.g., a chair can optionally also have arm-rests. Finally, having rules to capture composition at different levels helps us formulate the entire scene-processing pipeline as a single problem of finding most likely parse-tree---small segments combine to form parts of objects, parts to objects and objects to a scene. We attach a generative probability model to our grammar by having a feature-dependent probability function for every rule. We evaluated it by extracting labels for every segment and comparing the results with the state-of-the-art segment-labeling algorithm. Our algorithm was outperformed by the state-or-the-art method. But, Our model can be trained very efficiently (within seconds), and it scales only linearly in with the number of rules in the grammar. Also, we think that this is an important problem for the 3D vision community. So, we are releasing our dataset and related code. version:1
arxiv-1210-7014 | Computer vision tools for the non-invasive assessment of autism-related behavioral markers | http://arxiv.org/abs/1210.7014 | id:1210.7014 author:Jordan Hashemi, Thiago Vallin Spina, Mariano Tepper, Amy Esler, Vassilios Morellas, Nikolaos Papanikolopoulos, Guillermo Sapiro category:cs.CV  published:2012-10-25 summary:The early detection of developmental disorders is key to child outcome, allowing interventions to be initiated that promote development and improve prognosis. Research on autism spectrum disorder (ASD) suggests behavioral markers can be observed late in the first year of life. Many of these studies involved extensive frame-by-frame video observation and analysis of a child's natural behavior. Although non-intrusive, these methods are extremely time-intensive and require a high level of observer training; thus, they are impractical for clinical and large population research purposes. Diagnostic measures for ASD are available for infants but are only accurate when used by specialists experienced in early diagnosis. This work is a first milestone in a long-term multidisciplinary project that aims at helping clinicians and general practitioners accomplish this early detection/measurement task automatically. We focus on providing computer vision tools to measure and identify ASD behavioral markers based on components of the Autism Observation Scale for Infants (AOSI). In particular, we develop algorithms to measure three critical AOSI activities that assess visual attention. We augment these AOSI activities with an additional test that analyzes asymmetrical patterns in unsupported gait. The first set of algorithms involves assessing head motion by tracking facial features, while the gait analysis relies on joint foreground segmentation and 2D body pose estimation in video. We show results that provide insightful knowledge to augment the clinician's behavioral observations obtained from real in-clinic assessments. version:2
arxiv-1211-1733 | Linear Antenna Array with Suppressed Sidelobe and Sideband Levels using Time Modulation | http://arxiv.org/abs/1211.1733 | id:1211.1733 author:Swaprava Nath, Subrata Mitra category:cs.NE  published:2012-11-08 summary:In this paper, the goal is to achieve an ultra low sidelobe level (SLL) and sideband levels (SBL) of a time modulated linear antenna array. The approach followed here is not to give fixed level of excitation to the elements of an array, but to change it dynamically with time. The excitation levels of the different array elements over time are varied to get the low sidelobe and sideband levels. The mathematics of getting the SLL and SBL furnished in detail and simulation is done using the mathematical results. The excitation pattern over time is optimized using Genetic Algorithm (GA). Since, the amplitudes of the excitations of the elements are varied within a finite limit, results show it gives better sidelobe and sideband suppression compared to previous time modulated arrays with uniform amplitude excitations. version:1
arxiv-1211-1722 | Inverse problems in approximate uniform generation | http://arxiv.org/abs/1211.1722 | id:1211.1722 author:Anindya De, Ilias Diakonikolas, Rocco A. Servedio category:cs.CC cs.DS cs.LG  published:2012-11-07 summary:We initiate the study of \emph{inverse} problems in approximate uniform generation, focusing on uniform generation of satisfying assignments of various types of Boolean functions. In such an inverse problem, the algorithm is given uniform random satisfying assignments of an unknown function $f$ belonging to a class $\C$ of Boolean functions, and the goal is to output a probability distribution $D$ which is $\epsilon$-close, in total variation distance, to the uniform distribution over $f^{-1}(1)$. Positive results: We prove a general positive result establishing sufficient conditions for efficient inverse approximate uniform generation for a class $\C$. We define a new type of algorithm called a \emph{densifier} for $\C$, and show (roughly speaking) how to combine (i) a densifier, (ii) an approximate counting / uniform generation algorithm, and (iii) a Statistical Query learning algorithm, to obtain an inverse approximate uniform generation algorithm. We apply this general result to obtain a poly$(n,1/\eps)$-time algorithm for the class of halfspaces; and a quasipoly$(n,1/\eps)$-time algorithm for the class of $\poly(n)$-size DNF formulas. Negative results: We prove a general negative result establishing that the existence of certain types of signature schemes in cryptography implies the hardness of certain inverse approximate uniform generation problems. This implies that there are no {subexponential}-time inverse approximate uniform generation algorithms for 3-CNF formulas; for intersections of two halfspaces; for degree-2 polynomial threshold functions; and for monotone 2-CNF formulas. Finally, we show that there is no general relationship between the complexity of the "forward" approximate uniform generation problem and the complexity of the inverse problem for a class $\C$ -- it is possible for either one to be easy while the other is hard. version:1
arxiv-1211-1690 | Learning Monocular Reactive UAV Control in Cluttered Natural Environments | http://arxiv.org/abs/1211.1690 | id:1211.1690 author:Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert category:cs.RO cs.CV cs.LG cs.SY  published:2012-11-07 summary:Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly straight-forward, as expensive sensors and monitoring devices can be employed. In contrast, obstacle avoidance remains a challenging task for Micro Aerial Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike large vehicles, MAVs can only carry very light sensors, such as cameras, making autonomous navigation through obstacles much more challenging. In this paper, we describe a system that navigates a small quadrotor helicopter autonomously at low altitude through natural forest environments. Using only a single cheap camera to perceive the environment, we are able to maintain a constant velocity of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent state-of-the-art imitation learning techniques to train a controller that can avoid trees by adapting the MAVs heading. We demonstrate the performance of our system in a more controlled environment indoors, and in real natural forest environments outdoors. version:1
arxiv-1210-7362 | Discrete Energy Minimization, beyond Submodularity: Applications and Approximations | http://arxiv.org/abs/1210.7362 | id:1210.7362 author:Shai Bagon category:cs.CV cs.LG math.OC stat.ML  published:2012-10-27 summary:In this thesis I explore challenging discrete energy minimization problems that arise mainly in the context of computer vision tasks. This work motivates the use of such "hard-to-optimize" non-submodular functionals, and proposes methods and algorithms to cope with the NP-hardness of their optimization. Consequently, this thesis revolves around two axes: applications and approximations. The applications axis motivates the use of such "hard-to-optimize" energies by introducing new tasks. As the energies become less constrained and structured one gains more expressive power for the objective function achieving more accurate models. Results show how challenging, hard-to-optimize, energies are more adequate for certain computer vision applications. To overcome the resulting challenging optimization tasks the second axis of this thesis proposes approximation algorithms to cope with the NP-hardness of the optimization. Experiments show that these new methods yield good results for representative challenging problems. version:2
arxiv-1211-1656 | James-Stein Type Center Pixel Weights for Non-Local Means Image Denoising | http://arxiv.org/abs/1211.1656 | id:1211.1656 author:Yue Wu, Brian Tracey, Joseph P. Noonan category:cs.CV  published:2012-11-07 summary:Non-Local Means (NLM) and variants have been proven to be effective and robust in many image denoising tasks. In this letter, we study the parameter selection problem of center pixel weights (CPW) in NLM. Our key contributions are: 1) we give a novel formulation of the CPW problem from the statistical shrinkage perspective; 2) we introduce the James-Stein type CPWs for NLM; and 3) we propose a new adaptive CPW that is locally tuned for each image pixel. Our experimental results showed that compared to existing CPW solutions, the new proposed CPWs are more robust and effective under various noise levels. In particular, the NLM with the James-Stein type CPWs attain higher means with smaller variances in terms of the peak signal and noise ratio, implying they improve the NLM robustness and make it less sensitive to parameter selection. version:1
arxiv-1211-1654 | A New Randomness Evaluation Method with Applications to Image Shuffling and Encryption | http://arxiv.org/abs/1211.1654 | id:1211.1654 author:Yue Wu, Sos Agaian, Joseph P. Noonan category:cs.CR cs.CV stat.AP  published:2012-11-07 summary:This letter discusses the problem of testing the degree of randomness within an image, particularly for a shuffled or encrypted image. Its key contributions are: 1) a mathematical model of perfectly shuffled images; 2) the derivation of the theoretical distribution of pixel differences; 3) a new $Z$-test based approach to differentiate whether or not a test image is perfectly shuffled; and 4) a randomized algorithm to unbiasedly evaluate the degree of randomness within a given image. Simulation results show that the proposed method is robust and effective in evaluating the degree of randomness within an image, and may often be more suitable for image applications than commonly used testing schemes designed for binary data like NIST 800-22. The developed method may be also useful as a first step in determining whether or not a shuffling or encryption scheme is suitable for a particular cryptographic application. version:1
arxiv-1211-1650 | Different Operating Systems Compatible for Image Prepress Process in Color Management: Analysis and Performance Testing | http://arxiv.org/abs/1211.1650 | id:1211.1650 author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV  published:2012-11-07 summary:Image computing has become a real catchphrase over the past few years and the interpretations of the meaning of the term vary greatly. The Imagecomputing market is currently rapidly evolving with high growth prospects and almost daily announcements of new devices and application platforms, which results in an increasing diversification of devices, operating system and development platforms. Compared to more traditional information technology markets like the one of desktop computing, mobile computing is much less consolidated and neither standards nor even industry standards have yet been established. There are various platforms and interfaces which may be used to perform the desired tasks through the device. We have tried to compare the various mobile operating systems and their trade-offs. version:1
arxiv-1211-1552 | Image denoising with multi-layer perceptrons, part 2: training trade-offs and analysis of their mechanisms | http://arxiv.org/abs/1211.1552 | id:1211.1552 author:Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling category:cs.CV cs.LG  published:2012-11-07 summary:Image denoising can be described as the problem of mapping from a noisy image to a noise-free image. In another paper, we show that multi-layer perceptrons can achieve outstanding image denoising performance for various types of noise (additive white Gaussian noise, mixed Poisson-Gaussian noise, JPEG artifacts, salt-and-pepper noise and noise resembling stripes). In this work we discuss in detail which trade-offs have to be considered during the training procedure. We will show how to achieve good results and which pitfalls to avoid. By analysing the activation patterns of the hidden units we are able to make observations regarding the functioning principle of multi-layer perceptrons trained for image denoising. version:1
arxiv-1209-0167 | Automatic ECG Beat Arrhythmia Detection | http://arxiv.org/abs/1209.0167 | id:1209.0167 author:M. Bazarghan, Y. Jaberi, R. Amandi, M. Abedi category:cs.NE  published:2012-09-02 summary:Background: In recent years automated data analysis techniques have drawn great attention and are used in almost every field of research including biomedical. Artificial Neural Networks (ANNs) are one of the Computer- Aided- Diagnosis tools which are used extensively by advances in computer hardware technology. The application of these techniques for disease diagnosis has made great progress and is widely used by physicians. An Electrocardiogram carries vital information about heart activity and physicians use this signal for cardiac disease diagnosis which was the great motivation towards our study. Methods: In this study we are using Probabilistic Neural Networks (PNN) as an automatic technique for ECG signal analysis along with a Genetic Algorithm (GA). As every real signal recorded by the equipment can have different artifacts, we need to do some preprocessing steps before feeding it to the ANN. Wavelet transform is used for extracting the morphological parameters and median filter for data reduction of the ECG signal. The subset of morphological parameters are chosen and optimized using GA. We had two approaches in our investigation, the first one uses the whole signal with 289 normalized and de-noised data points as input to the ANN. In the second approach after applying all the preprocessing steps the signal is reduced to 29 data points and also their important parameters extracted to form the ANN input with 35 data points. Results: The outcome of the two approaches for 8 types of arrhythmia shows that the second approach is superior than the first one with an average accuracy of %99.42. version:3
arxiv-1211-1893 | Tangent-based manifold approximation with locally linear models | http://arxiv.org/abs/1211.1893 | id:1211.1893 author:Sofia Karygianni, Pascal Frossard category:cs.LG cs.CV  published:2012-11-06 summary:In this paper, we consider the problem of manifold approximation with affine subspaces. Our objective is to discover a set of low dimensional affine subspaces that represents manifold data accurately while preserving the manifold's structure. For this purpose, we employ a greedy technique that partitions manifold samples into groups that can be each approximated by a low dimensional subspace. We start by considering each manifold sample as a different group and we use the difference of tangents to determine appropriate group mergings. We repeat this procedure until we reach the desired number of sample groups. The best low dimensional affine subspaces corresponding to the final groups constitute our approximate manifold representation. Our experiments verify the effectiveness of the proposed scheme and show its superior performance compared to state-of-the-art methods for manifold approximation. version:1
arxiv-1202-5918 | Replica theory for learning curves for Gaussian processes on random graphs | http://arxiv.org/abs/1202.5918 | id:1202.5918 author:Matthew J. Urry, Peter Sollich category:stat.ML cond-mat.dis-nn cond-mat.stat-mech  published:2012-02-27 summary:Statistical physics approaches can be used to derive accurate predictions for the performance of inference methods learning from potentially noisy data, as quantified by the learning curve defined as the average error versus number of training examples. We analyse a challenging problem in the area of non-parametric inference where an effectively infinite number of parameters has to be learned, specifically Gaussian process regression. When the inputs are vertices on a random graph and the outputs noisy function values, we show that replica techniques can be used to obtain exact performance predictions in the limit of large graphs. The covariance of the Gaussian process prior is defined by a random walk kernel, the discrete analogue of squared exponential kernels on continuous spaces. Conventionally this kernel is normalised only globally, so that the prior variance can differ between vertices; as a more principled alternative we consider local normalisation, where the prior variance is uniform. version:3
arxiv-1211-1265 | From Bits to Images: Inversion of Local Binary Descriptors | http://arxiv.org/abs/1211.1265 | id:1211.1265 author:Emmanuel d'Angelo, Laurent jacques, Alexandre Alahi, Pierre Vandergheynst category:cs.CV cs.IT math.IT  published:2012-11-06 summary:Local Binary Descriptors are becoming more and more popular for image matching tasks, especially when going mobile. While they are extensively studied in this context, their ability to carry enough information in order to infer the original image is seldom addressed. In this work, we leverage an inverse problem approach to show that it is possible to directly reconstruct the image content from Local Binary Descriptors. This process relies on very broad assumptions besides the knowledge of the pattern of the descriptor at hand. This generalizes previous results that required either a prior learning database or non-binarized features. Furthermore, our reconstruction scheme reveals differences in the way different Local Binary Descriptors capture and encode image information. Hence, the potential applications of our work are multiple, ranging from privacy issues caused by eavesdropping image keypoints streamed by mobile devices to the design of better descriptors through the visualization and the analysis of their geometric content. version:1
arxiv-1211-1255 | Handwritten digit recognition by bio-inspired hierarchical networks | http://arxiv.org/abs/1211.1255 | id:1211.1255 author:Antonio G. Zippo, Giuliana Gelsomino, Sara Nencini, Gabriele E. M. Biella category:cs.LG cs.CV q-bio.NC  published:2012-11-06 summary:The human brain processes information showing learning and prediction abilities but the underlying neuronal mechanisms still remain unknown. Recently, many studies prove that neuronal networks are able of both generalizations and associations of sensory inputs. In this paper, following a set of neurophysiological evidences, we propose a learning framework with a strong biological plausibility that mimics prominent functions of cortical circuitries. We developed the Inductive Conceptual Network (ICN), that is a hierarchical bio-inspired network, able to learn invariant patterns by Variable-order Markov Models implemented in its nodes. The outputs of the top-most node of ICN hierarchy, representing the highest input generalization, allow for automatic classification of inputs. We found that the ICN clusterized MNIST images with an error of 5.73% and USPS images with an error of 12.56%. version:1
arxiv-1211-1127 | Visual Transfer Learning: Informal Introduction and Literature Overview | http://arxiv.org/abs/1211.1127 | id:1211.1127 author:Erik Rodner category:cs.CV cs.LG  published:2012-11-06 summary:Transfer learning techniques are important to handle small training sets and to allow for quick generalization even from only a few examples. The following paper is the introduction as well as the literature overview part of my thesis related to the topic of transfer learning for visual recognition problems. version:1
arxiv-1211-1119 | A Survey on Techniques of Improving Generalization Ability of Genetic Programming Solutions | http://arxiv.org/abs/1211.1119 | id:1211.1119 author:Vipul K. Dabhi, Sanjay Chaudhary category:cs.NE  published:2012-11-06 summary:In the field of empirical modeling using Genetic Programming (GP), it is important to evolve solution with good generalization ability. Generalization ability of GP solutions get affected by two important issues: bloat and over-fitting. We surveyed and classified existing literature related to different techniques used by GP research community to deal with these issues. We also point out limitation of these techniques, if any. Moreover, the classification of different bloat control approaches and measures for bloat and over-fitting are also discussed. We believe that this work will be useful to GP practitioners in following ways: (i) to better understand concepts of generalization in GP (ii) comparing existing bloat and over-fitting control techniques and (iii) selecting appropriate approach to improve generalization ability of GP evolved solutions. version:1
arxiv-1211-1043 | Soft (Gaussian CDE) regression models and loss functions | http://arxiv.org/abs/1211.1043 | id:1211.1043 author:Jose Hernandez-Orallo category:cs.LG stat.ML  published:2012-11-05 summary:Regression, unlike classification, has lacked a comprehensive and effective approach to deal with cost-sensitive problems by the reuse (and not a re-training) of general regression models. In this paper, a wide variety of cost-sensitive problems in regression (such as bids, asymmetric losses and rejection rules) can be solved effectively by a lightweight but powerful approach, consisting of: (1) the conversion of any traditional one-parameter crisp regression model into a two-parameter soft regression model, seen as a normal conditional density estimator, by the use of newly-introduced enrichment methods; and (2) the reframing of an enriched soft regression model to new contexts by an instance-dependent optimisation of the expected loss derived from the conditional normal distribution. version:1
arxiv-1205-6326 | A Framework for Evaluating Approximation Methods for Gaussian Process Regression | http://arxiv.org/abs/1205.6326 | id:1205.6326 author:Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray category:stat.ML cs.LG stat.CO  published:2012-05-29 summary:Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n^2) space and O(n^3) time for a dataset of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. version:2
arxiv-1211-0932 | Kernels and Submodels of Deep Belief Networks | http://arxiv.org/abs/1211.0932 | id:1211.0932 author:Guido F. Montufar, Jason Morton category:stat.ML  published:2012-11-05 summary:We study the mixtures of factorizing probability distributions represented as visible marginal distributions in stochastic layered networks. We take the perspective of kernel transitions of distributions, which gives a unified picture of distributed representations arising from Deep Belief Networks (DBN) and other networks without lateral connections. We describe combinatorial and geometric properties of the set of kernels and products of kernels realizable by DBNs as the network parameters vary. We describe explicit classes of probability distributions, including exponential families, that can be learned by DBNs. We use these submodels to bound the maximal and the expected Kullback-Leibler approximation errors of DBNs from above depending on the number of hidden layers and units that they contain. version:1
arxiv-1101-0434 | Sparse recovery with unknown variance: a LASSO-type approach | http://arxiv.org/abs/1101.0434 | id:1101.0434 author:Stéphane Chrétien, Sébastien Darses category:math.ST stat.ML stat.TH  published:2011-01-02 summary:We address the issue of estimating the regression vector $\beta$ in the generic $s$-sparse linear model $y = X\beta+z$, with $\beta\in\R^{p}$, $y\in\R^{n}$, $z\sim\mathcal N(0,\sg^2 I)$ and $p> n$ when the variance $\sg^{2}$ is unknown. We study two LASSO-type methods that jointly estimate $\beta$ and the variance. These estimators are minimizers of the $\ell_1$ penalized least-squares functional, where the relaxation parameter is tuned according to two different strategies. In the first strategy, the relaxation parameter is of the order $\ch{\sigma} \sqrt{\log p}$, where $\ch{\sigma}^2$ is the empirical variance. %The resulting optimization problem can be solved by running only a few successive LASSO instances with %recursive updating of the relaxation parameter. In the second strategy, the relaxation parameter is chosen so as to enforce a trade-off between the fidelity and the penalty terms at optimality. For both estimators, our assumptions are similar to the ones proposed by Cand\`es and Plan in {\it Ann. Stat. (2009)}, for the case where $\sg^{2}$ is known. We prove that our estimators ensure exact recovery of the support and sign pattern of $\beta$ with high probability. We present simulations results showing that the first estimator enjoys nearly the same performances in practice as the standard LASSO (known variance case) for a wide range of the signal to noise ratio. Our second estimator is shown to outperform both in terms of false detection, when the signal to noise ratio is low. version:5
arxiv-1203-6329 | Analysis of Magnification in Depth from Defocus | http://arxiv.org/abs/1203.6329 | id:1203.6329 author:Arnav Bhavsar category:cs.CV  published:2012-03-28 summary:In depth from defocus (DFD), when images are captured with different camera parameters, a relative magnification is induced between them. Image warping is a simpler solution to account for magnification than seemingly more accurate optical approaches. This work is an investigation into the effects of magnification on the accuracy of DFD. We comment on issues regarding scaling effect on relative blur computation. We statistically analyze accountability of scale factor, commenting on the bias and efficiency of the estimator that does not consider scale. We also discuss the effect of interpolation errors on blur estimation in a warping based solution to handle magnification and carry out experimental analysis to comment on the blur estimation accuracy. version:2
arxiv-1211-0879 | Comparing K-Nearest Neighbors and Potential Energy Method in classification problem. A case study using KNN applet by E.M. Mirkes and real life benchmark data sets | http://arxiv.org/abs/1211.0879 | id:1211.0879 author:Yanshan Shi category:stat.ML cs.LG  published:2012-11-05 summary:K-nearest neighbors (KNN) method is used in many supervised learning classification problems. Potential Energy (PE) method is also developed for classification problems based on its physical metaphor. The energy potential used in the experiments are Yukawa potential and Gaussian Potential. In this paper, I use both applet and MATLAB program with real life benchmark data to analyze the performances of KNN and PE method in classification problems. The results show that in general, KNN and PE methods have similar performance. In particular, PE with Yukawa potential has worse performance than KNN when the density of the data is higher in the distribution of the database. When the Gaussian potential is applied, the results from PE and KNN have similar behavior. The indicators used are correlation coefficients and information gain. version:1
arxiv-1211-0835 | Rejoinder: Latent variable graphical model selection via convex optimization | http://arxiv.org/abs/1211.0835 | id:1211.0835 author:Venkat Chandrasekaran, Pablo A. Parrilo, Alan S. Willsky category:math.ST cs.LG stat.ML stat.TH  published:2012-11-05 summary:Rejoinder to "Latent variable graphical model selection via convex optimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290]. version:1
arxiv-1211-0817 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/abs/1211.0817 | id:1211.0817 author:Emmanuel J. Candés, Mahdi Soltanolkotabi category:math.ST cs.LG stat.ML stat.TH  published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convex optimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290]. version:1
arxiv-1211-0808 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/abs/1211.0808 | id:1211.0808 author:Martin J. Wainwright category:math.ST cs.LG stat.ML stat.TH  published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convex optimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290]. version:1
arxiv-1211-0806 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/abs/1211.0806 | id:1211.0806 author:Steffen Lauritzen, Nicolai Meinshausen category:math.ST cs.LG stat.ML stat.TH  published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convex optimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290]. version:1
arxiv-1211-0801 | Discussion: Latent variable graphical model selection via convex optimization | http://arxiv.org/abs/1211.0801 | id:1211.0801 author:Ming Yuan category:math.ST cs.LG stat.ML stat.TH  published:2012-11-05 summary:Discussion of "Latent variable graphical model selection via convex optimization" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky [arXiv:1008.1290]. version:1
arxiv-1211-0757 | Efficient Point-to-Subspace Query in $\ell^1$: Theory and Applications in Computer Vision | http://arxiv.org/abs/1211.0757 | id:1211.0757 author:Ju Sun, Yuqian Zhang, John Wright category:stat.ML cs.CV stat.AP  published:2012-11-05 summary:Motivated by vision tasks such as robust face and object recognition, we consider the following general problem: given a collection of low-dimensional linear subspaces in a high-dimensional ambient (image) space and a query point (image), efficiently determine the nearest subspace to the query in $\ell^1$ distance. We show in theory that Cauchy random embedding of the objects into significantly-lower-dimensional spaces helps preserve the identity of the nearest subspace with constant probability. This offers the possibility of efficiently selecting several candidates for accurate search. We sketch preliminary experiments on robust face and digit recognition to corroborate our theory. version:1
arxiv-1211-0730 | Intelligent Algorithm for Optimum Solutions Based on the Principles of Bat Sonar | http://arxiv.org/abs/1211.0730 | id:1211.0730 author:Mohammed Ali Tawfeeq category:cs.NE  published:2012-11-04 summary:This paper presents a new intelligent algorithm that can solve the problems of finding the optimum solution in the state space among which the desired solution resides. The algorithm mimics the principles of bat sonar in finding its targets. The algorithm introduces three search approaches. The first search approach considers a single sonar unit (SSU) with a fixed beam length and a single starting point. In this approach, although the results converge toward the optimum fitness, it is not guaranteed to find the global optimum solution especially for complex problems; it is satisfied with finding 'acceptably good' solutions to these problems. The second approach considers multisonar units (MSU) working in parallel in the same state space. Each unit has its own starting point and tries to find the optimum solution. In this approach the probability that the algorithm converges toward the optimum solution is significantly increased. It is found that this approach is suitable for complex functions and for problems of wide state space. In the third approach, a single sonar unit with a moment (SSM) is used in order to handle the problem of convergence toward a local optimum rather than a global optimum. The momentum term is added to the length of the transmitted beams. This will give the chance to find the best fitness in a wider range within the state space. In this paper a comparison between the proposed algorithm and genetic algorithm (GA) has been made. It showed that both of the algorithms can catch approximately the optimum solutions for all of the testbed functions except for the function that has a local minimum, in which the proposed algorithm's result is much better than that of the GA algorithm. On the other hand, the comparison showed that the required execution time to obtain the optimum solution using the proposed algorithm is much less than that of the GA algorithm. version:1
arxiv-1112-5309 | POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem | http://arxiv.org/abs/1112.5309 | id:1112.5309 author:Jürgen Schmidhuber category:cs.AI cs.LG  published:2011-12-22 summary:Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. Consider the infinite set of all computable descriptions of tasks with possibly computable solutions. The novel algorithmic framework POWERPLAY (2011) continually searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. Wow-effects are achieved by continually making previously learned skills more efficient such that they require less time and space. New skills may (partially) re-use previously learned skills. POWERPLAY's search orders candidate pairs of tasks and solver modifications by their conditional computational (time & space) complexity, given the stored experience so far. The new task and its corresponding task-solving skill are those first found and validated. The computational costs of validating new tasks need not grow with task repertoire size. POWERPLAY's ongoing search for novelty keeps breaking the generalization abilities of its present solver. This is related to Goedel's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems. The continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks. POWERPLAY may be viewed as a greedy but practical implementation of basic principles of creativity. A first experimental analysis can be found in separate papers [53,54]. version:2
arxiv-1211-0660 | Generation of Two-Layer Monotonic Functions | http://arxiv.org/abs/1211.0660 | id:1211.0660 author:Yukihiro Kamada, Kiyonori Miyasaki category:cs.NE  published:2012-11-04 summary:The problem of implementing a class of functions with particular conditions by using monotonic multilayer functions is considered. A genetic algorithm is used to create monotonic functions of a certain class, and these are implemented with two-layer monotonic functions. The existence of a solution to the given problem suggests that from two monotone functions, a monotonic function with the same dimensions can be created. A new algorithm based on the genetic algorithm is proposed, which easily implemented two-layer monotonic functions of a specific class for up to six variables. version:1
arxiv-1204-5721 | Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems | http://arxiv.org/abs/1204.5721 | id:1204.5721 author:Sébastien Bubeck, Nicolò Cesa-Bianchi category:cs.LG stat.ML  published:2012-04-25 summary:Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the Thirties, exploration-exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this survey, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model. version:2
arxiv-1211-0602 | Segmentation of ultrasound images of thyroid nodule for assisting fine needle aspiration cytology | http://arxiv.org/abs/1211.0602 | id:1211.0602 author:Jie Zhao, Wei Zheng, Li Zhang, Hua Tian category:cs.CV  published:2012-11-03 summary:The incidence of thyroid nodule is very high and generally increases with the age. Thyroid nodule may presage the emergence of thyroid cancer. The thyroid nodule can be completely cured if detected early. Fine needle aspiration cytology is a recognized early diagnosis method of thyroid nodule. There are still some limitations in the fine needle aspiration cytology, and the ultrasound diagnosis of thyroid nodule has become the first choice for auxiliary examination of thyroid nodular disease. If we could combine medical imaging technology and fine needle aspiration cytology, the diagnostic rate of thyroid nodule would be improved significantly. The properties of ultrasound will degrade the image quality, which makes it difficult to recognize the edges for physicians. Image segmentation technique based on graph theory has become a research hotspot at present. Normalized cut (Ncut) is a representative one, which is suitable for segmentation of feature parts of medical image. However, how to solve the normalized cut has become a problem, which needs large memory capacity and heavy calculation of weight matrix. It always generates over segmentation or less segmentation which leads to inaccurate in the segmentation. The speckle noise in B ultrasound image of thyroid tumor makes the quality of the image deteriorate. In the light of this characteristic, we combine the anisotropic diffusion model with the normalized cut in this paper. After the enhancement of anisotropic diffusion model, it removes the noise in the B ultrasound image while preserves the important edges and local details. This reduces the amount of computation in constructing the weight matrix of the improved normalized cut and improves the accuracy of the final segmentation results. The feasibility of the method is proved by the experimental results. version:1
arxiv-1211-0498 | Detecting English Writing Styles For Non-native Speakers | http://arxiv.org/abs/1211.0498 | id:1211.0498 author:Rami Al-Rfou' category:cs.CL  published:2012-11-02 summary:Analyzing writing styles of non-native speakers is a challenging task. In this paper, we analyze the comments written in the discussion pages of the English Wikipedia. Using learning algorithms, we are able to detect native speakers' writing style with an accuracy of 74%. Given the diversity of the English Wikipedia users and the large number of languages they speak, we measure the similarities among their native languages by comparing the influence they have on their English writing style. Our results show that languages known to have the same origin and development path have similar footprint on their speakers' English writing style. To enable further studies, the dataset we extracted from Wikipedia will be made available publicly. version:1
arxiv-1211-0447 | Ordinal Rating of Network Performance and Inference by Matrix Completion | http://arxiv.org/abs/1211.0447 | id:1211.0447 author:Wei Du, Yongjun Liao, and Pierre Geurts, Guy Leduc category:cs.NI cs.LG  published:2012-11-02 summary:This paper addresses the large-scale acquisition of end-to-end network performance. We made two distinct contributions: ordinal rating of network performance and inference by matrix completion. The former reduces measurement costs and unifies various metrics which eases their processing in applications. The latter enables scalable and accurate inference with no requirement of structural information of the network nor geometric constraints. By combining both, the acquisition problem bears strong similarities to recommender systems. This paper investigates the applicability of various matrix factorization models used in recommender systems. We found that the simple regularized matrix factorization is not only practical but also produces accurate results that are beneficial for peer selection. version:1
arxiv-1211-0439 | Learning curves for multi-task Gaussian process regression | http://arxiv.org/abs/1211.0439 | id:1211.0439 author:Simon R. F. Ashton, Peter Sollich category:cs.LG cond-mat.dis-nn stat.ML  published:2012-11-02 summary:We study the average case performance of multi-task Gaussian process (GP) regression as captured in the learning curve, i.e. the average Bayes error for a chosen task versus the total number of examples $n$ for all tasks. For GP covariances that are the product of an input-dependent covariance function and a free-form inter-task covariance matrix, we show that accurate approximations for the learning curve can be obtained for an arbitrary number of tasks $T$. We use these to study the asymptotic learning behaviour for large $n$. Surprisingly, multi-task learning can be asymptotically essentially useless, in the sense that examples from other tasks help only when the degree of inter-task correlation, $\rho$, is near its maximal value $\rho=1$. This effect is most extreme for learning of smooth target functions as described by e.g. squared exponential kernels. We also demonstrate that when learning many tasks, the learning curves separate into an initial phase, where the Bayes error on each task is reduced down to a plateau value by "collective learning" even though most tasks have not seen examples, and a final decay that occurs once the number of examples is proportional to the number of tasks. version:1
arxiv-1211-0424 | Learning classifier systems with memory condition to solve non-Markov problems | http://arxiv.org/abs/1211.0424 | id:1211.0424 author:Zhaoxiang Zang, Dehua Li, Junying Wang category:cs.NE cs.AI  published:2012-11-02 summary:In the family of Learning Classifier Systems, the classifier system XCS has been successfully used for many applications. However, the standard XCS has no memory mechanism and can only learn optimal policy in Markov environments, where the optimal action is determined solely by the state of current sensory input. In practice, most environments are partially observable environments on agent's sensation, which are also known as non-Markov environments. Within these environments, XCS either fails, or only develops a suboptimal policy, since it has no memory. In this work, we develop a new classifier system based on XCS to tackle this problem. It adds an internal message list to XCS as the memory list to record input sensation history, and extends a small number of classifiers with memory conditions. The classifier's memory condition, as a foothold to disambiguate non-Markov states, is used to sense a specified element in the memory list. Besides, a detection method is employed to recognize non-Markov states in environments, to avoid these states controlling over classifiers' memory conditions. Furthermore, four sets of different complex maze environments have been tested by the proposed method. Experimental results show that our system is one of the best techniques to solve partially observable environments, compared with some well-known classifier systems proposed for these environments. version:1
arxiv-1211-0418 | Verbalizing Ontologies in Controlled Baltic Languages | http://arxiv.org/abs/1211.0418 | id:1211.0418 author:Normunds Grūzītis, Gunta Nešpore, Baiba Saulīte category:cs.CL cs.AI  published:2012-11-02 summary:Controlled natural languages (mostly English-based) recently have emerged as seemingly informal supplementary means for OWL ontology authoring, if compared to the formal notations that are used by professional knowledge engineers. In this paper we present by examples controlled Latvian language that has been designed to be compliant with the state of the art Attempto Controlled English. We also discuss relation with controlled Lithuanian language that is being designed in parallel. version:1
arxiv-1210-7070 | A Multiscale Framework for Challenging Discrete Optimization | http://arxiv.org/abs/1210.7070 | id:1210.7070 author:Shai Bagon, Meirav Galun category:cs.CV cs.LG math.OC stat.ML  published:2012-10-26 summary:Current state-of-the-art discrete optimization methods struggle behind when it comes to challenging contrast-enhancing discrete energies (i.e., favoring different labels for neighboring variables). This work suggests a multiscale approach for these challenging problems. Deriving an algebraic representation allows us to coarsen any pair-wise energy using any interpolation in a principled algebraic manner. Furthermore, we propose an energy-aware interpolation operator that efficiently exposes the multiscale landscape of the energy yielding an effective coarse-to-fine optimization scheme. Results on challenging contrast-enhancing energies show significant improvement over state-of-the-art methods. version:3
arxiv-1211-0056 | Iterative Hard Thresholding Methods for $l_0$ Regularized Convex Cone Programming | http://arxiv.org/abs/1211.0056 | id:1211.0056 author:Zhaosong Lu category:math.OC cs.LG math.NA stat.CO stat.ML  published:2012-10-31 summary:In this paper we consider $l_0$ regularized convex cone programming problems. In particular, we first propose an iterative hard thresholding (IHT) method and its variant for solving $l_0$ regularized box constrained convex programming. We show that the sequence generated by these methods converges to a local minimizer. Also, we establish the iteration complexity of the IHT method for finding an $\epsilon$-local-optimal solution. We then propose a method for solving $l_0$ regularized convex cone programming by applying the IHT method to its quadratic penalty relaxation and establish its iteration complexity for finding an $\epsilon$-approximate local minimizer. Finally, we propose a variant of this method in which the associated penalty parameter is dynamically updated, and show that every accumulation point is a local minimizer of the problem. version:2
arxiv-1205-1482 | Risk estimation for matrix recovery with spectral regularization | http://arxiv.org/abs/1205.1482 | id:1205.1482 author:Charles-Alban Deledalle, Samuel Vaiter, Gabriel Peyré, Jalal Fadili, Charles Dossal category:math.OC cs.IT cs.LG math.IT math.ST stat.ML stat.TH  published:2012-05-07 summary:In this paper, we develop an approach to recursively estimate the quadratic risk for matrix recovery problems regularized with spectral functions. Toward this end, in the spirit of the SURE theory, a key step is to compute the (weak) derivative and divergence of a solution with respect to the observations. As such a solution is not available in closed form, but rather through a proximal splitting algorithm, we propose to recursively compute the divergence from the sequence of iterates. A second challenge that we unlocked is the computation of the (weak) derivative of the proximity operator of a spectral function. To show the potential applicability of our approach, we exemplify it on a matrix completion problem to objectively and automatically select the regularization parameter. version:3
arxiv-1211-0210 | Extension of TSVM to Multi-Class and Hierarchical Text Classification Problems With General Losses | http://arxiv.org/abs/1211.0210 | id:1211.0210 author:Sathiya Keerthi Selvaraj, Sundararajan Sellamanickam, Shirish Shevade category:cs.LG  published:2012-11-01 summary:Transductive SVM (TSVM) is a well known semi-supervised large margin learning method for binary text classification. In this paper we extend this method to multi-class and hierarchical classification problems. We point out that the determination of labels of unlabeled examples with fixed classifier weights is a linear programming problem. We devise an efficient technique for solving it. The method is applicable to general loss functions. We demonstrate the value of the new method using large margin loss on a number of multi-class and hierarchical classification datasets. For maxent loss we show empirically that our method is better than expectation regularization/constraint and posterior regularization methods, and competitive with the version of entropy regularization method which uses label constraints. version:1
arxiv-1211-0135 | Sampling and Reconstruction of Spatial Fields using Mobile Sensors | http://arxiv.org/abs/1211.0135 | id:1211.0135 author:Jayakrishnan Unnikrishnan, Martin Vetterli category:cs.MM cs.CV cs.IT math.IT  published:2012-11-01 summary:Spatial sampling is traditionally studied in a static setting where static sensors scattered around space take measurements of the spatial field at their locations. In this paper we study the emerging paradigm of sampling and reconstructing spatial fields using sensors that move through space. We show that mobile sensing offers some unique advantages over static sensing in sensing time-invariant bandlimited spatial fields. Since a moving sensor encounters such a spatial field along its path as a time-domain signal, a time-domain anti-aliasing filter can be employed prior to sampling the signal received at the sensor. Such a filtering procedure, when used by a configuration of sensors moving at constant speeds along equispaced parallel lines, leads to a complete suppression of spatial aliasing in the direction of motion of the sensors. We analytically quantify the advantage of using such a sampling scheme over a static sampling scheme by computing the reduction in sampling noise due to the filter. We also analyze the effects of non-uniform sensor speeds on the reconstruction accuracy. Using simulation examples we demonstrate the advantages of mobile sampling over static sampling in practical problems. We extend our analysis to sampling and reconstruction schemes for monitoring time-varying bandlimited fields using mobile sensors. We demonstrate that in some situations we require a lower density of sensors when using a mobile sensing scheme instead of the conventional static sensing scheme. The exact advantage is quantified for a problem of sampling and reconstructing an audio field. version:1
arxiv-1211-0074 | Transition-Based Dependency Parsing With Pluggable Classifiers | http://arxiv.org/abs/1211.0074 | id:1211.0074 author:Alex Rudnick category:cs.CL  published:2012-11-01 summary:In principle, the design of transition-based dependency parsers makes it possible to experiment with any general-purpose classifier without other changes to the parsing algorithm. In practice, however, it often takes substantial software engineering to bridge between the different representations used by two software packages. Here we present extensions to MaltParser that allow the drop-in use of any classifier conforming to the interface of the Weka machine learning package, a wrapper for the TiMBL memory-based learner to this interface, and experiments on multilingual dependency parsing with a variety of classifiers. While earlier work had suggested that memory-based learners might be a good choice for low-resource parsing scenarios, we cannot support that hypothesis in this work. We observed that support-vector machines give better parsing performance than the memory-based learner, regardless of the size of the training set. version:1
arxiv-1211-0055 | Dimensionality Reduction and Classification Feature Using Mutual Information Applied to Hyperspectral Images: A Wrapper Strategy Algorithm Based on Minimizing the Error Probability Using the Inequality of Fano | http://arxiv.org/abs/1211.0055 | id:1211.0055 author:Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine category:cs.CV  published:2012-10-31 summary:In the feature classification domain, the choice of data affects widely the results. For the Hyperspectral image, the bands dont all contain the information; some bands are irrelevant like those affected by various atmospheric effects, see Figure.4, and decrease the classification accuracy. And there exist redundant bands to complicate the learning system and product incorrect prediction [14]. Even the bands contain enough information about the scene they may can't predict the classes correctly if the dimension of space images, see Figure.3, is so large that needs many cases to detect the relationship between the bands and the scene (Hughes phenomenon) [10]. We can reduce the dimensionality of hyperspectral images by selecting only the relevant bands (feature selection or subset selection methodology), or extracting, from the original bands, new bands containing the maximal information about the classes, using any functions, logical or numerical (feature extraction methodology) [11][9]. Here we focus on the feature selection using mutual information. Hyperspectral images have three advantages regarding the multispectral images [6], version:1
arxiv-1211-0028 | Understanding the Interaction between Interests, Conversations and Friendships in Facebook | http://arxiv.org/abs/1211.0028 | id:1211.0028 author:Qirong Ho, Rong Yan, Rajat Raina, Eric P. Xing category:cs.SI cs.LG stat.ML  published:2012-10-31 summary:In this paper, we explore salient questions about user interests, conversations and friendships in the Facebook social network, using a novel latent space model that integrates several data types. A key challenge of studying Facebook's data is the wide range of data modalities such as text, network links, and categorical labels. Our latent space model seamlessly combines all three data modalities over millions of users, allowing us to study the interplay between user friendships, interests, and higher-order network-wide social trends on Facebook. The recovered insights not only answer our initial questions, but also reveal surprising facts about user interests in the context of Facebook's ecosystem. We also confirm that our results are significant with respect to evidential information from the study subjects. version:1
arxiv-1210-8440 | Large Scale Language Modeling in Automatic Speech Recognition | http://arxiv.org/abs/1210.8440 | id:1210.8440 author:Ciprian Chelba, Dan Bikel, Maria Shugrina, Patrick Nguyen, Shankar Kumar category:cs.CL  published:2012-10-31 summary:Large language models have been proven quite beneficial for a variety of automatic speech recognition tasks in Google. We summarize results on Voice Search and a few YouTube speech transcription tasks to highlight the impact that one can expect from increasing both the amount of training data, and the size of the language model estimated from such data. Depending on the task, availability and amount of training data used, language model size and amount of work and care put into integrating them in the lattice rescoring step we observe reductions in word error rate between 6% and 10% relative, for systems on a wide range of operating points between 17% and 52% word error rate. version:1
arxiv-1210-8436 | Optimal size, freshness and time-frame for voice search vocabulary | http://arxiv.org/abs/1210.8436 | id:1210.8436 author:Maryam Kamvar, Ciprian Chelba category:cs.CL cs.IR  published:2012-10-31 summary:In this paper, we investigate how to optimize the vocabulary for a voice search language model. The metric we optimize over is the out-of-vocabulary (OoV) rate since it is a strong indicator of user experience. In a departure from the usual way of measuring OoV rates, web search logs allow us to compute the per-session OoV rate and thus estimate the percentage of users that experience a given OoV rate. Under very conservative text normalization, we find that a voice search vocabulary consisting of 2 to 2.5 million words extracted from 1 week of search query data will result in an aggregate OoV rate of 1%; at that size, the same OoV rate will also be experienced by 90% of users. The number of words included in the vocabulary is a stable indicator of the OoV rate. Altering the freshness of the vocabulary or the duration of the time window over which the training data is gathered does not significantly change the OoV rate. Surprisingly, a significantly larger vocabulary (approximately 10 million words) is required to guarantee OoV rates below 1% for 95% of the users. version:1
arxiv-1210-8429 | Anomaly Detection in Time Series of Graphs using Fusion of Graph Invariants | http://arxiv.org/abs/1210.8429 | id:1210.8429 author:Youngser Park, Carey E. Priebe, Abdou Youssef category:stat.ML  published:2012-10-31 summary:Given a time series of graphs G(t) = (V, E(t)), t = 1, 2, ..., where the fixed vertex set V represents "actors" and an edge between vertex u and vertex v at time t (uv \in E(t)) represents the existence of a communications event between actors u and v during the tth time period, we wish to detect anomalies and/or change points. We consider a collection of graph features, or invariants, and demonstrate that adaptive fusion provides superior inferential efficacy compared to naive equal weighting for a certain class of anomaly detection problems. Simulation results using a latent process model for time series of graphs, as well as illustrative experimental results for a time series of graphs derived from the Enron email data, show that a fusion statistic can provide superior inference compared to individual invariants alone. These results also demonstrate that an adaptive weighting scheme for fusion of invariants performs better than naive equal weighting. version:1
arxiv-1210-8385 | First Experiments with PowerPlay | http://arxiv.org/abs/1210.8385 | id:1210.8385 author:Rupesh Kumar Srivastava, Bas R. Steunebrink, Jürgen Schmidhuber category:cs.AI cs.LG  published:2012-10-31 summary:Like a scientist or a playing child, PowerPlay not only learns new skills to solve given problems, but also invents new interesting problems by itself. By design, it continually comes up with the fastest to find, initially novel, but eventually solvable tasks. It also continually simplifies or compresses or speeds up solutions to previous tasks. Here we describe first experiments with PowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as a general computational problem solving architecture. Its connection weights can encode arbitrary, self-delimiting, halting or non-halting programs affecting both environment (through effectors) and internal states encoding abstractions of event sequences. Our PowerPlay-driven SLIM RNN learns to become an increasingly general solver of self-invented problems, continually adding new problem solving procedures to its growing skill repertoire. Extending a recent conference paper, we identify interesting, emerging, developmental stages of our open-ended system. We also show how it automatically self-modularizes, frequently re-using code for previously invented skills, always trying to invent novel tasks that can be quickly validated because they do not require too many weight changes affecting too many previous tasks. version:1
arxiv-1210-3926 | Learning Attitudes and Attributes from Multi-Aspect Reviews | http://arxiv.org/abs/1210.3926 | id:1210.3926 author:Julian McAuley, Jure Leskovec, Dan Jurafsky category:cs.CL cs.IR cs.LG  published:2012-10-15 summary:The majority of online reviews consist of plain-text feedback together with a single numeric score. However, there are multiple dimensions to products and opinions, and understanding the `aspects' that contribute to users' ratings may help us to better understand their individual preferences. For example, a user's impression of an audiobook presumably depends on aspects such as the story and the narrator, and knowing their opinions on these aspects may help us to recommend better products. In this paper, we build models for rating systems in which such dimensions are explicit, in the sense that users leave separate ratings for each aspect of a product. By introducing new corpora consisting of five million reviews, rated with between three and six aspects, we evaluate our models on three prediction tasks: First, we use our model to uncover which parts of a review discuss which of the rated aspects. Second, we use our model to summarize reviews, which for us means finding the sentences that best explain a user's rating. Finally, since aspect ratings are optional in many of the datasets we consider, we use our model to recover those ratings that are missing from a user's evaluation. Our model matches state-of-the-art approaches on existing small-scale datasets, while scaling to the real-world datasets we introduce. Moreover, our model is able to `disentangle' content and sentiment words: we automatically learn content words that are indicative of a particular aspect as well as the aspect-specific sentiment words that are indicative of a particular rating. version:2
arxiv-1210-8353 | Temporal Autoencoding Restricted Boltzmann Machine | http://arxiv.org/abs/1210.8353 | id:1210.8353 author:Chris Häusler, Alex Susemihl category:stat.ML cs.AI cs.LG  published:2012-10-31 summary:Much work has been done refining and characterizing the receptive fields learned by deep learning algorithms. A lot of this work has focused on the development of Gabor-like filters learned when enforcing sparsity constraints on a natural image dataset. Little work however has investigated how these filters might expand to the temporal domain, namely through training on natural movies. Here we investigate exactly this problem in established temporal deep learning algorithms as well as a new learning paradigm suggested here, the Temporal Autoencoding Restricted Boltzmann Machine (TARBM). version:1
arxiv-1210-8318 | Mugshot Identification from Manipulated Facial Images | http://arxiv.org/abs/1210.8318 | id:1210.8318 author:H. R. Chennamma, Lalitha Rangarajan category:cs.CV cs.MM  published:2012-10-31 summary:Editing on digital images is ubiquitous. Identification of deliberately modified facial images is a new challenge for face identification system. In this paper, we address the problem of identification of a face or person from heavily altered facial images. In this face identification problem, the input to the system is a manipulated or transformed face image and the system reports back the determined identity from a database of known individuals. Such a system can be useful in mugshot identification in which mugshot database contains two views (frontal and profile) of each criminal. We considered only frontal view from the available database for face identification and the query image is a manipulated face generated by face transformation software tool available online. We propose SIFT features for efficient face identification in this scenario. Further comparative analysis has been given with well known eigenface approach. Experiments have been conducted with real case images to evaluate the performance of both methods. version:1
arxiv-1210-8291 | Learning in the Model Space for Fault Diagnosis | http://arxiv.org/abs/1210.8291 | id:1210.8291 author:Huanhuan Chen, Peter Tino, Xin Yao, Ali Rodan category:cs.LG cs.AI  published:2012-10-31 summary:The emergence of large scaled sensor networks facilitates the collection of large amounts of real-time data to monitor and control complex engineering systems. However, in many cases the collected data may be incomplete or inconsistent, while the underlying environment may be time-varying or un-formulated. In this paper, we have developed an innovative cognitive fault diagnosis framework that tackles the above challenges. This framework investigates fault diagnosis in the model space instead of in the signal space. Learning in the model space is implemented by fitting a series of models using a series of signal segments selected with a rolling window. By investigating the learning techniques in the fitted model space, faulty models can be discriminated from healthy models using one-class learning algorithm. The framework enables us to construct fault library when unknown faults occur, which can be regarded as cognitive fault isolation. This paper also theoretically investigates how to measure the pairwise distance between two models in the model space and incorporates the model distance into the learning algorithm in the model space. The results on three benchmark applications and one simulated model for the Barcelona water distribution network have confirmed the effectiveness of the proposed framework. version:1
arxiv-1210-8262 | On the Relation Between the Common Labelling and the Median Graph | http://arxiv.org/abs/1210.8262 | id:1210.8262 author:Nicola Rebagliati, Albert Solé-Ribalta, Marcello Pelillo, Francesc Serratosa category:cs.CV  published:2012-10-31 summary:In structural pattern recognition, given a set of graphs, the computation of a Generalized Median Graph is a well known problem. Some methods approach the problem by assuming a relation between the Generalized Median Graph and the Common Labelling problem. However, this relation has still not been formally proved. In this paper, we analyse such relation between both problems. The main result proves that the cost of the common labelling upper-bounds the cost of the median with respect to the given set. In addition, we show that the two problems are equivalent in some cases. version:1
arxiv-1210-8124 | Hierarchical Learning Algorithm for the Beta Basis Function Neural Network | http://arxiv.org/abs/1210.8124 | id:1210.8124 author:Habib Dhahri, Mohamed Adel Alimi category:cs.NE cs.AI  published:2012-10-30 summary:The paper presents a two-level learning method for the design of the Beta Basis Function Neural Network BBFNN. A Genetic Algorithm is employed at the upper level to construct BBFNN, while the key learning parameters :the width, the centers and the Beta form are optimised using the gradient algorithm at the lower level. In order to demonstrate the effectiveness of this hierarchical learning algorithm HLABBFNN, we need to validate our algorithm for the approximation of non-linear function. version:1
arxiv-1202-5298 | Min Max Generalization for Two-stage Deterministic Batch Mode Reinforcement Learning: Relaxation Schemes | http://arxiv.org/abs/1202.5298 | id:1202.5298 author:Raphael Fonteneau, Damien Ernst, Bernard Boigelot, Quentin Louveaux category:cs.SY cs.LG  published:2012-02-23 summary:We study the minmax optimization problem introduced in [22] for computing policies for batch mode reinforcement learning in a deterministic setting. First, we show that this problem is NP-hard. In the two-stage case, we provide two relaxation schemes. The first relaxation scheme works by dropping some constraints in order to obtain a problem that is solvable in polynomial time. The second relaxation scheme, based on a Lagrangian relaxation where all constraints are dualized, leads to a conic quadratic programming problem. We also theoretically prove and empirically illustrate that both relaxation schemes provide better results than those given in [22]. version:2
arxiv-1112-5640 | Learning Smooth Pattern Transformation Manifolds | http://arxiv.org/abs/1112.5640 | id:1112.5640 author:Elif Vural, Pascal Frossard category:cs.CV  published:2011-12-23 summary:Manifold models provide low-dimensional representations that are useful for processing and analyzing data in a transformation-invariant way. In this paper, we study the problem of learning smooth pattern transformation manifolds from image sets that represent observations of geometrically transformed signals. In order to construct a manifold, we build a representative pattern whose transformations accurately fit various input images. We examine two objectives of the manifold building problem, namely, approximation and classification. For the approximation problem, we propose a greedy method that constructs a representative pattern by selecting analytic atoms from a continuous dictionary manifold. We present a DC (Difference-of-Convex) optimization scheme that is applicable to a wide range of transformation and dictionary models, and demonstrate its application to transformation manifolds generated by rotation, translation and anisotropic scaling of a reference pattern. Then, we generalize this approach to a setting with multiple transformation manifolds, where each manifold represents a different class of signals. We present an iterative multiple manifold building algorithm such that the classification accuracy is promoted in the learning of the representative patterns. Experimental results suggest that the proposed methods yield high accuracy in the approximation and classification of data compared to some reference methods, while the invariance to geometric transformations is achieved due to the transformation manifold model. version:5
arxiv-1210-7956 | Implementation of a Vision System for a Landmine Detecting Robot Using Artificial Neural Network | http://arxiv.org/abs/1210.7956 | id:1210.7956 author:Roger Achkar, Michel Owayjan category:cs.NE cs.CV 68T45 I.2.6; I.5.1  published:2012-10-30 summary:Landmines, specifically anti-tank mines, cluster bombs, and unexploded ordnance form a serious problem in many countries. Several landmine sweeping techniques are used for minesweeping. This paper presents the design and the implementation of the vision system of an autonomous robot for landmines localization. The proposed work develops state-of-the-art techniques in digital image processing for pre-processing captured images of the contaminated area. After enhancement, Artificial Neural Network (ANN) is used in order to identify, recognize and classify the landmines' make and model. The Back-Propagation algorithm is used for training the network. The proposed work proved to be able to identify and classify different types of landmines under various conditions (rotated landmine, partially covered landmine) with a success rate of up to 90%. version:1
arxiv-1210-7917 | The Model of Semantic Concepts Lattice For Data Mining Of Microblogs | http://arxiv.org/abs/1210.7917 | id:1210.7917 author:Bohdan Pavlyshenko category:cs.CL cs.IR  published:2012-10-30 summary:The model of semantic concept lattice for data mining of microblogs has been proposed in this work. It is shown that the use of this model is effective for the semantic relations analysis and for the detection of associative rules of key words. version:1
arxiv-1210-2984 | Learning Onto-Relational Rules with Inductive Logic Programming | http://arxiv.org/abs/1210.2984 | id:1210.2984 author:Francesca A. Lisi category:cs.AI cs.DB cs.LG cs.LO  published:2012-10-10 summary:Rules complement and extend ontologies on the Semantic Web. We refer to these rules as onto-relational since they combine DL-based ontology languages and Knowledge Representation formalisms supporting the relational data model within the tradition of Logic Programming and Deductive Databases. Rule authoring is a very demanding Knowledge Engineering task which can be automated though partially by applying Machine Learning algorithms. In this chapter we show how Inductive Logic Programming (ILP), born at the intersection of Machine Learning and Logic Programming and considered as a major approach to Relational Learning, can be adapted to Onto-Relational Learning. For the sake of illustration, we provide details of a specific Onto-Relational Learning solution to the problem of learning rule-based definitions of DL concepts and roles with ILP. version:2
arxiv-1210-7669 | Performance Evaluation of Different Techniques for texture Classification | http://arxiv.org/abs/1210.7669 | id:1210.7669 author:Pooja Maknikar category:cs.CV  published:2012-10-29 summary:Texture is the term used to characterize the surface of a given object or phenomenon and is an important feature used in image processing and pattern recognition. Our aim is to compare various Texture analyzing methods and compare the results based on time complexity and accuracy of classification. The project describes texture classification using Wavelet Transform and Co occurrence Matrix. Comparison of features of a sample texture with database of different textures is performed. In wavelet transform we use the Haar, Symlets and Daubechies wavelets. We find that, thee Haar wavelet proves to be the most efficient method in terms of performance assessment parameters mentioned above. Comparison of Haar wavelet and Co-occurrence matrix method of classification also goes in the favor of Haar. Though the time requirement is high in the later method, it gives excellent results for classification accuracy except if the image is rotated. version:1
arxiv-1210-7657 | Text Classification with Compression Algorithms | http://arxiv.org/abs/1210.7657 | id:1210.7657 author:Antonio Giuliano Zippo category:cs.LG  published:2012-10-29 summary:This work concerns a comparison of SVM kernel methods in text categorization tasks. In particular I define a kernel function that estimates the similarity between two objects computing by their compressed lengths. In fact, compression algorithms can detect arbitrarily long dependencies within the text strings. Data text vectorization looses information in feature extractions and is highly sensitive by textual language. Furthermore, these methods are language independent and require no text preprocessing. Moreover, the accuracy computed on the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater than Gaussian, linear and polynomial kernels. The method limits are represented by computational time complexity of the Gram matrix and by very poor performance on non-textual datasets. version:1
arxiv-1210-7631 | The fortresses of Ejin: an example of outlining a site from satellite images | http://arxiv.org/abs/1210.7631 | id:1210.7631 author:Amelia Carolina Sparavigna category:cs.CV  published:2012-10-29 summary:From 1960's to 1970's, the Chinese Army built some fortified artificial hills. Some of them are located in the Inner Mongolia, Western China. These large fortresses are surrounded by moats. For some of them it is still possible to see earthworks, trenches and ditches, the planning of which could have a symbolic meaning. We can argue this result form their digital outlining, obtained after an image processing of satellite images, based on edge detection. version:1
arxiv-1203-6345 | Empirical Normalization for Quadratic Discriminant Analysis and Classifying Cancer Subtypes | http://arxiv.org/abs/1203.6345 | id:1203.6345 author:Mark A. Kon, Nikolay Nikolaev category:stat.ML  published:2012-03-28 summary:We introduce a new discriminant analysis method (Empirical Discriminant Analysis or EDA) for binary classification in machine learning. Given a dataset of feature vectors, this method defines an empirical feature map transforming the training and test data into new data with components having Gaussian empirical distributions. This map is an empirical version of the Gaussian copula used in probability and mathematical finance. The purpose is to form a feature mapped dataset as close as possible to Gaussian, after which standard quadratic discriminants can be used for classification. We discuss this method in general, and apply it to some datasets in computational biology. version:2
arxiv-1210-7461 | Recognizing Static Signs from the Brazilian Sign Language: Comparing Large-Margin Decision Directed Acyclic Graphs, Voting Support Vector Machines and Artificial Neural Networks | http://arxiv.org/abs/1210.7461 | id:1210.7461 author:César Roberto de Souza, Ednaldo Brigante Pizzolato, Mauro dos Santos Anjo category:cs.CV cs.LG stat.ML  published:2012-10-28 summary:In this paper, we explore and detail our experiments in a high-dimensionality, multi-class image classification problem often found in the automatic recognition of Sign Languages. Here, our efforts are directed towards comparing the characteristics, advantages and drawbacks of creating and training Support Vector Machines disposed in a Directed Acyclic Graph and Artificial Neural Networks to classify signs from the Brazilian Sign Language (LIBRAS). We explore how the different heuristics, hyperparameters and multi-class decision schemes affect the performance, efficiency and ease of use for each classifier. We provide hyperparameter surface maps capturing accuracy and efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects of heuristics when training ANNs with Resilient Backpropagation. We report statistically significant results using Cohen's Kappa statistic for contingency tables. version:1
arxiv-1206-5766 | Learning mixtures of spherical Gaussians: moment methods and spectral decompositions | http://arxiv.org/abs/1206.5766 | id:1206.5766 author:Daniel Hsu, Sham M. Kakade category:cs.LG stat.ML  published:2012-06-25 summary:This work provides a computationally efficient and statistically consistent moment-based estimator for mixtures of spherical Gaussians. Under the condition that component means are in general position, a simple spectral decomposition technique yields consistent parameter estimates from low-order observable moments, without additional minimum separation assumptions needed by previous computationally efficient estimation procedures. Thus computational and information-theoretic barriers to efficient estimation in mixture models are precluded when the mixture components have means in general position and spherical covariances. Some connections are made to estimation problems related to independent component analysis. version:4
arxiv-1210-7403 | Resolution Enhancement of Range Images via Color-Image Segmentation | http://arxiv.org/abs/1210.7403 | id:1210.7403 author:Arnav Bhavsar category:cs.CV  published:2012-10-28 summary:We report a method for super-resolution of range images. Our approach leverages the interpretation of LR image as sparse samples on the HR grid. Based on this interpretation, we demonstrate that our recently reported approach, which reconstructs dense range images from sparse range data by exploiting a registered colour image, can be applied for the task of resolution enhancement of range images. Our method only uses a single colour image in addition to the range observation in the super-resolution process. Using the proposed approach, we demonstrate super-resolution results for large factors (e.g. 4) with good localization accuracy. version:1
arxiv-1109-2279 | The Bayesian Bridge | http://arxiv.org/abs/1109.2279 | id:1109.2279 author:Nicholas G. Polson, James G. Scott, Jesse Windle category:stat.ME stat.CO stat.ML  published:2011-09-11 summary:We propose the Bayesian bridge estimator for regularized regression and classification. Two key mixture representations for the Bayesian bridge model are developed: (1) a scale mixture of normals with respect to an alpha-stable random variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle densities) with respect to a two-component mixture of gamma random variables. Both lead to MCMC methods for posterior simulation, and these methods turn out to have complementary domains of maximum efficiency. The first representation is a well known result due to West (1987), and is the better choice for collinear design matrices. The second representation is new, and is more efficient for orthogonal problems, largely because it avoids the need to deal with exponentially tilted stable random variables. It also provides insight into the multimodality of the joint posterior distribution, a feature of the bridge model that is notably absent under ridge or lasso-type priors. We prove a theorem that extends this representation to a wider class of densities representable as scale mixtures of betas, and provide an explicit inversion formula for the mixing distribution. The connections with slice sampling and scale mixtures of normals are explored. On the practical side, we find that the Bayesian bridge model outperforms its classical cousin in estimation and prediction across a variety of data sets, both simulated and real. We also show that the MCMC for fitting the bridge model exhibits excellent mixing properties, particularly for the global scale parameter. This makes for a favorable contrast with analogous MCMC algorithms for other sparse Bayesian models. All methods described in this paper are implemented in the R package BayesBridge. An extensive set of simulation results are provided in two supplemental files. version:2
arxiv-1210-7282 | The Hangulphabet: A Descriptive Alphabet | http://arxiv.org/abs/1210.7282 | id:1210.7282 author:Robert Bishop, Ruggero Micheletto category:cs.CL  published:2012-10-27 summary:This paper describes the Hangulphabet, a new writing system that should prove useful in a number of contexts. Using the Hangulphabet, a user can instantly see voicing, manner and place of articulation of any phoneme found in human language. The Hangulphabet places consonant graphemes on a grid with the x-axis representing the place of articulation and the y-axis representing manner of articulation. Each individual grapheme contains radicals from both axes where the points intersect. The top radical represents manner of articulation where the bottom represents place of articulation. A horizontal line running through the middle of the bottom radical represents voicing. For vowels, place of articulation is located on a grid that represents the position of the tongue in the mouth. This grid is similar to that of the IPA vowel chart (International Phonetic Association, 1999). The difference with the Hangulphabet being the trapezoid representing the vocal apparatus is on a slight tilt. Place of articulation for a vowel is represented by a breakout figure from the grid. This system can be used as an alternative to the International Phonetic Alphabet (IPA) or as a complement to it. Beginning students of linguistics may find it particularly useful. A Hangulphabet font has been created to facilitate switching between the Hangulphabet and the IPA. version:1
arxiv-1206-5345 | Dynamic Pricing under Finite Space Demand Uncertainty: A Multi-Armed Bandit with Dependent Arms | http://arxiv.org/abs/1206.5345 | id:1206.5345 author:Pouya Tehrani, Yixuan Zhai, Qing Zhao category:cs.LG  published:2012-06-23 summary:We consider a dynamic pricing problem under unknown demand models. In this problem a seller offers prices to a stream of customers and observes either success or failure in each sale attempt. The underlying demand model is unknown to the seller and can take one of N possible forms. In this paper, we show that this problem can be formulated as a multi-armed bandit with dependent arms. We propose a dynamic pricing policy based on the likelihood ratio test. We show that the proposed policy achieves complete learning, i.e., it offers a bounded regret where regret is defined as the revenue loss with respect to the case with a known demand model. This is in sharp contrast with the logarithmic growing regret in multi-armed bandit with independent arms. version:4
arxiv-1210-6287 | Fast Exact Max-Kernel Search | http://arxiv.org/abs/1210.6287 | id:1210.6287 author:Ryan R. Curtin, Parikshit Ram, Alexander G. Gray category:cs.DS cs.IR cs.LG  published:2012-10-23 summary:The wide applicability of kernels makes the problem of max-kernel search ubiquitous and more general than the usual similarity search in metric spaces. We focus on solving this problem efficiently. We begin by characterizing the inherent hardness of the max-kernel search problem with a novel notion of directional concentration. Following that, we present a method to use an $O(n \log n)$ algorithm to index any set of objects (points in $\Real^\dims$ or abstract objects) directly in the Hilbert space without any explicit feature representations of the objects in this space. We present the first provably $O(\log n)$ algorithm for exact max-kernel search using this index. Empirical results for a variety of data sets as well as abstract objects demonstrate up to 4 orders of magnitude speedup in some cases. Extensions for approximate max-kernel search are also presented. version:2
arxiv-1210-7137 | Alberti's letter counts | http://arxiv.org/abs/1210.7137 | id:1210.7137 author:Bernard Ycart category:math.HO cs.CL  published:2012-10-26 summary:Four centuries before modern statistical linguistics was born, Leon Battista Alberti (1404--1472) compared the frequency of vowels in Latin poems and orations, making the first quantified observation of a stylistic difference ever. Using a corpus of 20 Latin texts (over 5 million letters), Alberti's observations are statistically assessed. Letter counts prove that poets used significantly more a's, e's, and y's, whereas orators used more of the other vowels. The sample sizes needed to justify the assertions are studied, and proved to be within reach for Alberti's scholarship. version:1
arxiv-1210-7102 | 3D Face Recognition using Significant Point based SULD Descriptor | http://arxiv.org/abs/1210.7102 | id:1210.7102 author:B. H. Shekar, N. Harivinod, M. Sharmila Kumari, K. Raghurama Holla category:cs.CV  published:2012-10-26 summary:In this work, we present a new 3D face recognition method based on Speeded-Up Local Descriptor (SULD) of significant points extracted from the range images of faces. The proposed model consists of a method for extracting distinctive invariant features from range images of faces that can be used to perform reliable matching between different poses of range images of faces. For a given 3D face scan, range images are computed and the potential interest points are identified by searching at all scales. Based on the stability of the interest point, significant points are extracted. For each significant point we compute the SULD descriptor which consists of vector made of values from the convolved Haar wavelet responses located on concentric circles centred on the significant point, and where the amount of Gaussian smoothing is proportional to the radii of the circles. Experimental results show that the newly proposed method provides higher recognition rate compared to other existing contemporary models developed for 3D face recognition. version:1
arxiv-1210-7056 | Selective Transfer Learning for Cross Domain Recommendation | http://arxiv.org/abs/1210.7056 | id:1210.7056 author:Zhongqi Lu, Erheng Zhong, Lili Zhao, Wei Xiang, Weike Pan, Qiang Yang category:cs.LG cs.IR stat.ML  published:2012-10-26 summary:Collaborative filtering (CF) aims to predict users' ratings on items according to historical user-item preference data. In many real-world applications, preference data are usually sparse, which would make models overfit and fail to give accurate predictions. Recently, several research works show that by transferring knowledge from some manually selected source domains, the data sparseness problem could be mitigated. However for most cases, parts of source domain data are not consistent with the observations in the target domain, which may misguide the target domain model building. In this paper, we propose a novel criterion based on empirical prediction error and its variance to better capture the consistency across domains in CF settings. Consequently, we embed this criterion into a boosting framework to perform selective knowledge transfer. Comparing to several state-of-the-art methods, we show that our proposed selective transfer learning framework can significantly improve the accuracy of rating prediction tasks on several real-world recommendation tasks. version:1
arxiv-1210-7054 | Large-Scale Sparse Principal Component Analysis with Application to Text Data | http://arxiv.org/abs/1210.7054 | id:1210.7054 author:Youwei Zhang, Laurent El Ghaoui category:stat.ML cs.LG math.OC  published:2012-10-26 summary:Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models. version:1
arxiv-1210-7047 | User-level Weibo Recommendation incorporating Social Influence based on Semi-Supervised Algorithm | http://arxiv.org/abs/1210.7047 | id:1210.7047 author:Daifeng Li, Zhipeng Luo, Golden Guo-zheng Sun, Jie Tang, Jingwei Zhang category:cs.SI cs.CY cs.LG  published:2012-10-26 summary:Tencent Weibo, as one of the most popular micro-blogging services in China, has attracted millions of users, producing 30-60 millions of weibo (similar as tweet in Twitter) daily. With the overload problem of user generate content, Tencent users find it is more and more hard to browse and find valuable information at the first time. In this paper, we propose a Factor Graph based weibo recommendation algorithm TSI-WR (Topic-Level Social Influence based Weibo Recommendation), which could help Tencent users to find most suitable information. The main innovation is that we consider both direct and indirect social influence from topic level based on social balance theory. The main advantages of adopting this strategy are that it could first build a more accurate description of latent relationship between two users with weak connections, which could help to solve the data sparsity problem; second provide a more accurate recommendation for a certain user from a wider range. Other meaningful contextual information is also combined into our model, which include: Users profile, Users influence, Content of weibos, Topic information of weibos and etc. We also design a semi-supervised algorithm to further reduce the influence of data sparisty. The experiments show that all the selected variables are important and the proposed model outperforms several baseline methods. version:1
arxiv-1210-7038 | Full Object Boundary Detection by Applying Scale Invariant Features in a Region Merging Segmentation Algorithm | http://arxiv.org/abs/1210.7038 | id:1210.7038 author:Reza Oji, Farshad Tajeripour category:cs.CV cs.AI  published:2012-10-26 summary:Object detection is a fundamental task in computer vision and has many applications in image processing. This paper proposes a new approach for object detection by applying scale invariant feature transform (SIFT) in an automatic segmentation algorithm. SIFT is an invariant algorithm respect to scale, translation and rotation. The features are very distinct and provide stable keypoints that can be used for matching an object in different images. At first, an object is trained with different aspects for finding best keypoints. The object can be recognized in the other images by using achieved keypoints. Then, a robust segmentation algorithm is used to detect the object with full boundary based on SIFT keypoints. In segmentation algorithm, a merging role is defined to merge the regions in image with the assistance of keypoints. The results show that the proposed approach is reliable for object detection and can extract object boundary well. version:1
arxiv-0903-4817 | An Exponential Lower Bound on the Complexity of Regularization Paths | http://arxiv.org/abs/0903.4817 | id:0903.4817 author:Bernd Gärtner, Martin Jaggi, Clément Maria category:cs.LG cs.CG cs.CV math.OC stat.ML 90C20 F.2.2; I.5.1  published:2009-03-27 summary:For a variety of regularized optimization problems in machine learning, algorithms computing the entire solution path have been developed recently. Most of these methods are quadratic programs that are parameterized by a single parameter, as for example the Support Vector Machine (SVM). Solution path algorithms do not only compute the solution for one particular value of the regularization parameter but the entire path of solutions, making the selection of an optimal parameter much easier. It has been assumed that these piecewise linear solution paths have only linear complexity, i.e. linearly many bends. We prove that for the support vector machine this complexity can be exponential in the number of training points in the worst case. More strongly, we construct a single instance of n input points in d dimensions for an SVM such that at least \Theta(2^{n/2}) = \Theta(2^d) many distinct subsets of support vectors occur as the regularization parameter changes. version:3
arxiv-1211-0191 | Performance Evaluation of Random Set Based Pedestrian Tracking Algorithms | http://arxiv.org/abs/1211.0191 | id:1211.0191 author:Branko Ristic, Jamie Sherrah, Ángel F. García-Fernández category:cs.CV  published:2012-10-25 summary:The paper evaluates the error performance of three random finite set based multi-object trackers in the context of pedestrian video tracking. The evaluation is carried out using a publicly available video dataset of 4500 frames (town centre street) for which the ground truth is available. The input to all pedestrian tracking algorithms is an identical set of head and body detections, obtained using the Histogram of Oriented Gradients (HOG) detector. The tracking error is measured using the recently proposed OSPA metric for tracks, adopted as the only known mathematically rigorous metric for measuring the distance between two sets of tracks. A comparative analysis is presented under various conditions. version:1
arxiv-1210-6912 | Enhancing the functional content of protein interaction networks | http://arxiv.org/abs/1210.6912 | id:1210.6912 author:Gaurav Pandey, Sahil Manocha, Gowtham Atluri, Vipin Kumar category:q-bio.MN cs.CE cs.LG q-bio.GN stat.ML  published:2012-10-25 summary:Protein interaction networks are a promising type of data for studying complex biological systems. However, despite the rich information embedded in these networks, they face important data quality challenges of noise and incompleteness that adversely affect the results obtained from their analysis. Here, we explore the use of the concept of common neighborhood similarity (CNS), which is a form of local structure in networks, to address these issues. Although several CNS measures have been proposed in the literature, an understanding of their relative efficacies for the analysis of interaction networks has been lacking. We follow the framework of graph transformation to convert the given interaction network into a transformed network corresponding to a variety of CNS measures evaluated. The effectiveness of each measure is then estimated by comparing the quality of protein function predictions obtained from its corresponding transformed network with those from the original network. Using a large set of S. cerevisiae interactions, and a set of 136 GO terms, we find that several of the transformed networks produce more accurate predictions than those obtained from the original network. In particular, the $HC.cont$ measure proposed here performs particularly well for this task. Further investigation reveals that the two major factors contributing to this improvement are the abilities of CNS measures, especially $HC.cont$, to prune out noisy edges and introduce new links between functionally related proteins. version:1
arxiv-1210-6911 | Ancestor Sampling for Particle Gibbs | http://arxiv.org/abs/1210.6911 | id:1210.6911 author:Fredrik Lindsten, Michael I. Jordan, Thomas B. Schön category:stat.CO stat.ML  published:2012-10-25 summary:We present a novel method in the family of particle MCMC methods that we refer to as particle Gibbs with ancestor sampling (PG-AS). Similarly to the existing PG with backward simulation (PG-BS) procedure, we use backward sampling to (considerably) improve the mixing of the PG kernel. Instead of using separate forward and backward sweeps as in PG-BS, however, we achieve the same effect in a single forward sweep. We apply the PG-AS framework to the challenging class of non-Markovian state-space models. We develop a truncation strategy of these models that is applicable in principle to any backward-simulation-based method, but which is particularly well suited to the PG-AS framework. In particular, as we show in a simulation study, PG-AS can yield an order-of-magnitude improved accuracy relative to PG-BS due to its robustness to the truncation error. Several application examples are discussed, including Rao-Blackwellized particle smoothing and inference in degenerate state-space models. version:1
arxiv-1210-6230 | A Self-Organized Neural Comparator | http://arxiv.org/abs/1210.6230 | id:1210.6230 author:Guillermo A. Ludueña, Claudius Gros category:q-bio.NC cond-mat.dis-nn cs.NE  published:2012-10-23 summary:Learning algorithms need generally the possibility to compare several streams of information. Neural learning architectures hence need a unit, a comparator, able to compare several inputs encoding either internal or external information, like for instance predictions and sensory readings. Without the possibility of comparing the values of prediction to actual sensory inputs, reward evaluation and supervised learning would not be possible. Comparators are usually not implemented explicitly, necessary comparisons are commonly performed by directly comparing one-to-one the respective activities. This implies that the characteristics of the two input streams (like size and encoding) must be provided at the time of designing the system. It is however plausible that biological comparators emerge from self-organizing, genetically encoded principles, which allow the system to adapt to the changes in the input and in the organism. We propose an unsupervised neural circuitry, where the function of input comparison emerges via self-organization only from the interaction of the system with the respective inputs, without external influence or supervision. The proposed neural comparator adapts, unsupervised, according to the correlations present in the input streams. The system consists of a multilayer feed-forward neural network which follows a local output minimization (anti-Hebbian) rule for adaptation of the synaptic weights. The local output minimization allows the circuit to autonomously acquire the capability of comparing the neural activities received from different neural populations, which may differ in the size of the population and in the neural encoding used. The comparator is able to compare objects never encountered before in the sensory input streams and to evaluate a measure of their similarity, even when differently encoded. version:2
arxiv-1210-6649 | Extended object reconstruction in adaptive-optics imaging: the multiresolution approach | http://arxiv.org/abs/1210.6649 | id:1210.6649 author:Roberto Baena Gallé, Jorge Núñez, Szymon Gladysz category:astro-ph.IM cs.CV math.NA  published:2012-10-25 summary:We propose the application of multiresolution transforms, such as wavelets (WT) and curvelets (CT), to the reconstruction of images of extended objects that have been acquired with adaptive optics (AO) systems. Such multichannel approaches normally make use of probabilistic tools in order to distinguish significant structures from noise and reconstruction residuals. Furthermore, we aim to check the historical assumption that image-reconstruction algorithms using static PSFs are not suitable for AO imaging. We convolve an image of Saturn taken with the Hubble Space Telescope (HST) with AO PSFs from the 5-m Hale telescope at the Palomar Observatory and add both shot and readout noise. Subsequently, we apply different approaches to the blurred and noisy data in order to recover the original object. The approaches include multi-frame blind deconvolution (with the algorithm IDAC), myopic deconvolution with regularization (with MISTRAL) and wavelets- or curvelets-based static PSF deconvolution (AWMLE and ACMLE algorithms). We used the mean squared error (MSE) and the structural similarity index (SSIM) to compare the results. We discuss the strengths and weaknesses of the two metrics. We found that CT produces better results than WT, as measured in terms of MSE and SSIM. Multichannel deconvolution with a static PSF produces results which are generally better than the results obtained with the myopic/blind approaches (for the images we tested) thus showing that the ability of a method to suppress the noise and to track the underlying iterative process is just as critical as the capability of the myopic/blind approaches to update the PSF. version:1
arxiv-1210-6766 | Structured Sparsity Models for Multiparty Speech Recovery from Reverberant Recordings | http://arxiv.org/abs/1210.6766 | id:1210.6766 author:Afsaneh Asaei, Mohammad Golbabaee, Hervé Bourlard, Volkan Cevher category:cs.LG cs.SD  published:2012-10-25 summary:We tackle the multi-party speech recovery problem through modeling the acoustic of the reverberant chambers. Our approach exploits structured sparsity models to perform room modeling and speech recovery. We propose a scheme for characterizing the room acoustic from the unknown competing speech sources relying on localization of the early images of the speakers by sparse approximation of the spatial spectra of the virtual sources in a free-space model. The images are then clustered exploiting the low-rank structure of the spectro-temporal components belonging to each source. This enables us to identify the early support of the room impulse response function and its unique map to the room geometry. To further tackle the ambiguity of the reflection ratios, we propose a novel formulation of the reverberation model and estimate the absorption coefficients through a convex optimization exploiting joint sparsity model formulated upon spatio-spectral sparsity of concurrent speech representation. The acoustic parameters are then incorporated for separating individual speech signals through either structured sparse recovery or inverse filtering the acoustic channels. The experiments conducted on real data recordings demonstrate the effectiveness of the proposed approach for multi-party speech recovery and recognition. version:1
arxiv-1205-0047 | $QD$-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through Consensus + Innovations | http://arxiv.org/abs/1205.0047 | id:1205.0047 author:Soummya Kar, Jose' M. F. Moura, H. Vincent Poor category:stat.ML cs.LG cs.MA math.OC math.PR  published:2012-04-30 summary:The paper considers a class of multi-agent Markov decision processes (MDPs), in which the network agents respond differently (as manifested by the instantaneous one-stage random costs) to a global controlled state and the control actions of a remote controller. The paper investigates a distributed reinforcement learning setup with no prior information on the global state transition and local agent cost statistics. Specifically, with the agents' objective consisting of minimizing a network-averaged infinite horizon discounted cost, the paper proposes a distributed version of $Q$-learning, $\mathcal{QD}$-learning, in which the network agents collaborate by means of local processing and mutual information exchange over a sparse (possibly stochastic) communication network to achieve the network goal. Under the assumption that each agent is only aware of its local online cost data and the inter-agent communication network is \emph{weakly} connected, the proposed distributed scheme is almost surely (a.s.) shown to yield asymptotically the desired value function and the optimal stationary control policy at each network agent. The analytical techniques developed in the paper to address the mixed time-scale stochastic dynamics of the \emph{consensus + innovations} form, which arise as a result of the proposed interactive distributed scheme, are of independent interest. version:2
arxiv-1210-6707 | Clustering hidden Markov models with variational HEM | http://arxiv.org/abs/1210.6707 | id:1210.6707 author:Emanuele Coviello, Antoni B. Chan, Gert R. G. Lanckriet category:cs.LG cs.CV stat.ML  published:2012-10-24 summary:The hidden Markov model (HMM) is a widely-used generative model that copes with sequential data, assuming that each observation is conditioned on the state of a hidden Markov chain. In this paper, we derive a novel algorithm to cluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed algorithm i) clusters a given collection of HMMs into groups of HMMs that are similar, in terms of the distributions they represent, and ii) characterizes each group by a "cluster center", i.e., a novel HMM that is representative for the group, in a manner that is consistent with the underlying generative model of the HMM. To cope with intractable inference in the E-step, the HEM algorithm is formulated as a variational optimization problem, and efficiently solved for the HMM case by leveraging an appropriate variational approximation. The benefits of the proposed algorithm, which we call variational HEM (VHEM), are demonstrated on several tasks involving time-series data, such as hierarchical clustering of motion capture sequences, and automatic annotation and retrieval of music and of online hand-writing data, showing improvements over current methods. In particular, our variational HEM algorithm effectively leverages large amounts of data when learning annotation models by using an efficient hierarchical estimation procedure, which reduces learning times and memory requirements, while improving model robustness through better regularization. version:1
arxiv-1210-6511 | Neural Networks for Complex Data | http://arxiv.org/abs/1210.6511 | id:1210.6511 author:Marie Cottrell, Madalina Olteanu, Fabrice Rossi, Joseph Rynkiewicz, Nathalie Villa-Vialaneix category:cs.NE cs.LG stat.ML  published:2012-10-24 summary:Artificial neural networks are simple and efficient machine learning tools. Defined originally in the traditional setting of simple vector data, neural network models have evolved to address more and more difficulties of complex real world problems, ranging from time evolving data to sophisticated data structures such as graphs and functions. This paper summarizes advances on those themes from the last decade, with a focus on results obtained by members of the SAMM team of Universit\'e Paris 1 version:1
arxiv-1210-6497 | Topic-Level Opinion Influence Model(TOIM): An Investigation Using Tencent Micro-Blogging | http://arxiv.org/abs/1210.6497 | id:1210.6497 author:Daifeng Li, Ying Ding, Xin Shuai, Golden Guo-zheng Sun, Jie Tang, Zhipeng Luo, Jingwei Zhang, Guo Zhang category:cs.SI cs.CY cs.LG  published:2012-10-24 summary:Mining user opinion from Micro-Blogging has been extensively studied on the most popular social networking sites such as Twitter and Facebook in the U.S., but few studies have been done on Micro-Blogging websites in other countries (e.g. China). In this paper, we analyze the social opinion influence on Tencent, one of the largest Micro-Blogging websites in China, endeavoring to unveil the behavior patterns of Chinese Micro-Blogging users. This paper proposes a Topic-Level Opinion Influence Model (TOIM) that simultaneously incorporates topic factor and social direct influence in a unified probabilistic framework. Based on TOIM, two topic level opinion influence propagation and aggregation algorithms are developed to consider the indirect influence: CP (Conservative Propagation) and NCP (None Conservative Propagation). Users' historical social interaction records are leveraged by TOIM to construct their progressive opinions and neighbors' opinion influence through a statistical learning process, which can be further utilized to predict users' future opinions on some specific topics. To evaluate and test this proposed model, an experiment was designed and a sub-dataset from Tencent Micro-Blogging was used. The experimental results show that TOIM outperforms baseline methods on predicting users' opinion. The applications of CP and NCP have no significant differences and could significantly improve recall and F1-measure of TOIM. version:1
arxiv-1210-6465 | Black-Box Complexity: Breaking the $O(n \log n)$ Barrier of LeadingOnes | http://arxiv.org/abs/1210.6465 | id:1210.6465 author:Benjamin Doerr, Carola Winzen category:cs.DS cs.NE  published:2012-10-24 summary:We show that the unrestricted black-box complexity of the $n$-dimensional XOR- and permutation-invariant LeadingOnes function class is $O(n \log (n) / \log \log n)$. This shows that the recent natural looking $O(n\log n)$ bound is not tight. The black-box optimization algorithm leading to this bound can be implemented in a way that only 3-ary unbiased variation operators are used. Hence our bound is also valid for the unbiased black-box complexity recently introduced by Lehre and Witt (GECCO 2010). The bound also remains valid if we impose the additional restriction that the black-box algorithm does not have access to the objective values but only to their relative order (ranking-based black-box complexity). version:1
arxiv-1210-6891 | Predicting Near-Future Churners and Win-Backs in the Telecommunications Industry | http://arxiv.org/abs/1210.6891 | id:1210.6891 author:Clifton Phua, Hong Cao, João Bártolo Gomes, Minh Nhut Nguyen category:cs.CE cs.LG  published:2012-10-24 summary:In this work, we presented the strategies and techniques that we have developed for predicting the near-future churners and win-backs for a telecom company. On a large-scale and real-world database containing customer profiles and some transaction data from a telecom company, we first analyzed the data schema, developed feature computation strategies and then extracted a large set of relevant features that can be associated with the customer churning and returning behaviors. Our features include both the original driver factors as well as some derived features. We evaluated our features on the imbalance corrected dataset, i.e. under-sampled dataset and compare a large number of existing machine learning tools, especially decision tree-based classifiers, for predicting the churners and win-backs. In general, we find RandomForest and SimpleCart learning algorithms generally perform well and tend to provide us with highly competitive prediction performance. Among the top-15 driver factors that signal the churn behavior, we find that the service utilization, e.g. last two months' download and upload volume, last three months' average upload and download, and the payment related factors are the most indicative features for predicting if churn will happen soon. Such features can collectively tell discrepancies between the service plans, payments and the dynamically changing utilization needs of the customers. Our proposed features and their computational strategy exhibit reasonable precision performance to predict churn behavior in near future. version:1
arxiv-1210-6317 | On the geometric structure of fMRI searchlight-based information maps | http://arxiv.org/abs/1210.6317 | id:1210.6317 author:Shivakumar Viswanathan, Matthew Cieslak, Scott T. Grafton category:q-bio.NC q-bio.QM stat.AP stat.ML  published:2012-10-23 summary:Information mapping is a popular application of Multivoxel Pattern Analysis (MVPA) to fMRI. Information maps are constructed using the so called searchlight method, where the spherical multivoxel neighborhood of every voxel (i.e., a searchlight) in the brain is evaluated for the presence of task-relevant response patterns. Despite their widespread use, information maps present several challenges for interpretation. One such challenge has to do with inferring the size and shape of a multivoxel pattern from its signature on the information map. To address this issue, we formally examined the geometric basis of this mapping relationship. Based on geometric considerations, we show how and why small patterns (i.e., having smaller spatial extents) can produce a larger signature on the information map as compared to large patterns, independent of the size of the searchlight radius. Furthermore, we show that the number of informative searchlights over the brain increase as a function of searchlight radius, even in the complete absence of any multivariate response patterns. These properties are unrelated to the statistical capabilities of the pattern-analysis algorithms used but are obligatory geometric properties arising from using the searchlight procedure. version:1
arxiv-1210-6293 | MLPACK: A Scalable C++ Machine Learning Library | http://arxiv.org/abs/1210.6293 | id:1210.6293 author:Ryan R. Curtin, James R. Cline, N. P. Slagle, William B. March, Parikshit Ram, Nishant A. Mehta, Alexander G. Gray category:cs.MS cs.CV cs.LG  published:2012-10-23 summary:MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org. version:1
arxiv-1210-6192 | Textural Approach to Palmprint Identification | http://arxiv.org/abs/1210.6192 | id:1210.6192 author:Rachita Misra, Kasturika B ray category:cs.CV cs.CR cs.GR  published:2012-10-23 summary:Biometrics which use of human physiological characteristics for identifying an individual is now a widespread method of identification and authentication. Biometric identification is a technology which uses several image processing techniques and describes the general procedure for identification and verification using feature extraction, storage and matching from the digitized image of biometric characters such as Finger Print, Face, Iris or Palm Print. The current paper uses palm print biometrics. Here we have presented an identification approach using textural properties of palm print images. The elegance of the method is that the conventional edge detection technique is extended to suitably describe the texture features. In this technique all the characteristics of the palm such as principal lines, edges and wrinkles are considered with equal importance. version:1
arxiv-1210-6170 | Further properties of Gaussian Reproducing Kernel Hilbert Spaces | http://arxiv.org/abs/1210.6170 | id:1210.6170 author:Minh Ha Quang category:stat.ML math.FA 68T05  68P30  published:2012-10-23 summary:We generalize the orthonormal basis for the Gaussian RKHS described in \cite{MinhGaussian2010} to an infinite, continuously parametrized, family of orthonormal bases, along with some implications. The proofs are direct generalizations of those in \cite{MinhGaussian2010}. version:1
arxiv-1210-6157 | Novel Architecture for 3D model in virtual communities from detected face | http://arxiv.org/abs/1210.6157 | id:1210.6157 author:Vibekananda Dutta, Dr Nishtha Kesswani, Deepti Gahalot category:cs.CV  published:2012-10-23 summary:In this research paper we suggest how to extract a face from an image, modify it, characterize it in terms of high-level properties, and apply it to the creation of a personalized avatar. In this research work we tested, we implemented the algorithm on several hundred facial images, including many taken under uncontrolled acquisition conditions, and found to exhibit satisfactory performance for immediate practical use. version:1
arxiv-1209-3730 | A Bayesian method for the analysis of deterministic and stochastic time series | http://arxiv.org/abs/1209.3730 | id:1209.3730 author:C. A. L. Bailer-Jones category:astro-ph.IM astro-ph.SR physics.data-an stat.ML  published:2012-09-17 summary:I introduce a general, Bayesian method for modelling univariate time series data assumed to be drawn from a continuous, stochastic process. The method accommodates arbitrary temporal sampling, and takes into account measurement uncertainties for arbitrary error models (not just Gaussian) on both the time and signal variables. Any model for the deterministic component of the variation of the signal with time is supported, as is any model of the stochastic component on the signal and time variables. Models illustrated here are constant and sinusoidal models for the signal mean combined with a Gaussian stochastic component, as well as a purely stochastic model, the Ornstein-Uhlenbeck process. The posterior probability distribution over model parameters is determined via Monte Carlo sampling. Models are compared using the "cross-validation likelihood", in which the posterior-averaged likelihood for different partitions of the data are combined. In principle this is more robust to changes in the prior than is the evidence (the prior-averaged likelihood). The method is demonstrated by applying it to the light curves of 11 ultra cool dwarf stars, claimed by a previous study to show statistically significant variability. This is reassessed here by calculating the cross-validation likelihood for various time series models, including a null hypothesis of no variability beyond the error bars. 10 of 11 light curves are confirmed as being significantly variable, and one of these seems to be periodic, with two plausible periods identified. Another object is best described by the Ornstein-Uhlenbeck process, a conclusion which is obviously limited to the set of models actually tested. version:2
arxiv-1210-6119 | Time After Time: Notes on Delays In Spiking Neural P Systems | http://arxiv.org/abs/1210.6119 | id:1210.6119 author:Francis George C. Cabarle, Kelvin C. Buño, Henry N. Adorna category:cs.NE cs.DC cs.ET 97P20 F.1  published:2012-10-23 summary:Spiking Neural P systems, SNP systems for short, are biologically inspired computing devices based on how neurons perform computations. SNP systems use only one type of symbol, the spike, in the computations. Information is encoded in the time differences of spikes or the multiplicity of spikes produced at certain times. SNP systems with delays (associated with rules) and those without delays are two of several Turing complete SNP system variants in literature. In this work we investigate how restricted forms of SNP systems with delays can be simulated by SNP systems without delays. We show the simulations for the following spike routing constructs: sequential, iteration, join, and split. version:1
arxiv-1210-6082 | Interplay: Dispersed Activation in Neural Networks | http://arxiv.org/abs/1210.6082 | id:1210.6082 author:Richard L. Churchill category:cs.NE q-bio.NC  published:2012-10-22 summary:This paper presents a multi-point stimulation of a Hebbian neural network with investigation of the interplay between the stimulus waves through the neurons of the network. Equilibrium of the resulting memory is achieved for recall of specific memory data at a rate faster than single point stimulus. The interplay of the intersecting stimuli appears to parallel the clarification process of recall in biological systems. version:1
arxiv-1210-5965 | Classification Analysis Of Authorship Fiction Texts in The Space Of Semantic Fields | http://arxiv.org/abs/1210.5965 | id:1210.5965 author:Bohdan Pavlyshenko category:cs.CL  published:2012-10-22 summary:The use of naive Bayesian classifier (NB) and the classifier by the k nearest neighbors (kNN) in classification semantic analysis of authors' texts of English fiction has been analysed. The authors' works are considered in the vector space the basis of which is formed by the frequency characteristics of semantic fields of nouns and verbs. Highly precise classification of authors' texts in the vector space of semantic fields indicates about the presence of particular spheres of author's idiolect in this space which characterizes the individual author's style. version:1
arxiv-1210-5898 | Some Chances and Challenges in Applying Language Technologies to Historical Studies in Chinese | http://arxiv.org/abs/1210.5898 | id:1210.5898 author:Chao-Lin Liu, Guantao Jin, Qingfeng Liu, Wei-Yun Chiu, Yih-Soong Yu category:cs.CL cs.DL cs.IR  published:2012-10-22 summary:We report applications of language technology to analyzing historical documents in the Database for the Study of Modern Chinese Thoughts and Literature (DSMCTL). We studied two historical issues with the reported techniques: the conceptualization of "huaren" (Chinese people) and the attempt to institute constitutional monarchy in the late Qing dynasty. We also discuss research challenges for supporting sophisticated issues using our experience with DSMCTL, the Database of Government Officials of the Republic of China, and the Dream of the Red Chamber. Advanced techniques and tools for lexical, syntactic, semantic, and pragmatic processing of language information, along with more thorough data collection, are needed to strengthen the collaboration between historians and computer scientists. version:1
arxiv-1210-5873 | Initialization of Self-Organizing Maps: Principal Components Versus Random Initialization. A Case Study | http://arxiv.org/abs/1210.5873 | id:1210.5873 author:A. A. Akinduko, E. M. Mirkes category:stat.ML cs.LG  published:2012-10-22 summary:The performance of the Self-Organizing Map (SOM) algorithm is dependent on the initial weights of the map. The different initialization methods can broadly be classified into random and data analysis based initialization approach. In this paper, the performance of random initialization (RI) approach is compared to that of principal component initialization (PCI) in which the initial map weights are chosen from the space of the principal component. Performance is evaluated by the fraction of variance unexplained (FVU). Datasets were classified into quasi-linear and non-linear and it was observed that RI performed better for non-linear datasets; however the performance of PCI approach remains inconclusive for quasi-linear datasets. version:1
arxiv-1210-5840 | Supervised Learning with Similarity Functions | http://arxiv.org/abs/1210.5840 | id:1210.5840 author:Purushottam Kar, Prateek Jain category:cs.LG stat.ML  published:2012-10-22 summary:We address the problem of general supervised learning when data can only be accessed through an (indefinite) similarity function between data points. Existing work on learning with indefinite kernels has concentrated solely on binary/multi-class classification problems. We propose a model that is generic enough to handle any supervised learning task and also subsumes the model previously proposed for classification. We give a "goodness" criterion for similarity functions w.r.t. a given supervised learning task and then adapt a well-known landmarking technique to provide efficient algorithms for supervised learning using "good" similarity functions. We demonstrate the effectiveness of our model on three important super-vised learning problems: a) real-valued regression, b) ordinal regression and c) ranking where we show that our method guarantees bounded generalization error. Furthermore, for the case of real-valued regression, we give a natural goodness definition that, when used in conjunction with a recent result in sparse vector recovery, guarantees a sparse predictor with bounded generalization error. Finally, we report results of our learning algorithms on regression and ordinal regression tasks using non-PSD similarity functions and demonstrate the effectiveness of our algorithms, especially that of the sparse landmark selection algorithm that achieves significantly higher accuracies than the baseline methods while offering reduced computational costs. version:1
arxiv-1210-5806 | Multi-Stage Multi-Task Feature Learning | http://arxiv.org/abs/1210.5806 | id:1210.5806 author:Pinghua Gong, Jieping Ye, Changshui Zhang category:stat.ML  published:2012-10-22 summary:Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an $\ell_0$-type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. version:1
arxiv-1210-5751 | Extraction of domain-specific bilingual lexicon from comparable corpora: compositional translation and ranking | http://arxiv.org/abs/1210.5751 | id:1210.5751 author:Estelle Delpech, Béatrice Daille, Emmanuel Morin, Claire Lemaire category:cs.CL  published:2012-10-21 summary:This paper proposes a method for extracting translations of morphologically constructed terms from comparable corpora. The method is based on compositional translation and exploits translation equivalences at the morpheme-level, which allows for the generation of "fertile" translations (translation pairs in which the target term has more words than the source term). Ranking methods relying on corpus-based and translation-based features are used to select the best candidate translation. We obtain an average precision of 91% on the Top1 candidate translation. The method was tested on two language pairs (English-French and English-German) and with a small specialized comparable corpora (400k words per language). version:1
arxiv-1210-5732 | Developing ICC Profile Using Gray Level Control In Offset Printing Process | http://arxiv.org/abs/1210.5732 | id:1210.5732 author:Jaswinder Singh Dilawari, Ravinder Khanna category:cs.CV  published:2012-10-21 summary:In prepress department RGB image has to be converted to CMYK image. To control that amount of black, cyan, magenta and yellow has to be controlled by using color separation method. Graycolor separation method is selected to control the amounts of these colors because it increase the quality of printing also. A single printer used for printing the same image on different paper also results in different printed images. To remove this problem a different ICC profile based on gray level control is developedand a sheet offset printer is calibrated using that profile and a subjective evaluation shows satisfactory results for different quality papers. version:1
arxiv-1210-5034 | Optimal Computational Trade-Off of Inexact Proximal Methods | http://arxiv.org/abs/1210.5034 | id:1210.5034 author:Pierre Machart, Sandrine Anthoine, Luca Baldassarre category:cs.LG cs.CV cs.NA  published:2012-10-18 summary:In this paper, we investigate the trade-off between convergence rate and computational cost when minimizing a composite functional with proximal-gradient methods, which are popular optimisation tools in machine learning. We consider the case when the proximity operator is computed via an iterative procedure, which provides an approximation of the exact proximity operator. In that case, we obtain algorithms with two nested loops. We show that the strategy that minimizes the computational cost to reach a solution with a desired accuracy in finite time is to set the number of inner iterations to a constant, which differs from the strategy indicated by a convergence rate analysis. In the process, we also present a new procedure called SIP (that is Speedy Inexact Proximal-gradient algorithm) that is both computationally efficient and easy to implement. Our numerical experiments confirm the theoretical findings and suggest that SIP can be a very competitive alternative to the standard procedure. version:2
arxiv-1210-5653 | Identifications of concealed weapon in a Human Body | http://arxiv.org/abs/1210.5653 | id:1210.5653 author:Prof. Samir K. Bandyopadhyay, Biswajita Datta, Sudipta Roy category:cs.CV  published:2012-10-20 summary:The detection of weapons concealed underneath a person cloths is very much important to the improvement of the security of the public as well as the safety of public assets like airports, buildings and railway stations etc. version:1
arxiv-1210-5644 | Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials | http://arxiv.org/abs/1210.5644 | id:1210.5644 author:Philipp Krähenbühl, Vladlen Koltun category:cs.CV cs.AI cs.LG  published:2012-10-20 summary:Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While region-level models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy. version:1
arxiv-1210-5581 | Hidden Trends in 90 Years of Harvard Business Review | http://arxiv.org/abs/1210.5581 | id:1210.5581 author:Chia-Chi Tsai, Chao-Lin Liu, Wei-Jie Huang, Man-Kwan Shan category:cs.CL cs.DL cs.IR  published:2012-10-20 summary:In this paper, we demonstrate and discuss results of our mining the abstracts of the publications in Harvard Business Review between 1922 and 2012. Techniques for computing n-grams, collocations, basic sentiment analysis, and named-entity recognition were employed to uncover trends hidden in the abstracts. We present findings about international relationships, sentiment in HBR's abstracts, important international companies, influential technological inventions, renown researchers in management theories, US presidents via chronological analyses. version:1
arxiv-1210-5544 | Online Learning in Decentralized Multiuser Resource Sharing Problems | http://arxiv.org/abs/1210.5544 | id:1210.5544 author:Cem Tekin, Mingyan Liu category:cs.LG  published:2012-10-19 summary:In this paper, we consider the general scenario of resource sharing in a decentralized system when the resource rewards/qualities are time-varying and unknown to the users, and using the same resource by multiple users leads to reduced quality due to resource sharing. Firstly, we consider a user-independent reward model with no communication between the users, where a user gets feedback about the congestion level in the resource it uses. Secondly, we consider user-specific rewards and allow costly communication between the users. The users have a cooperative goal of achieving the highest system utility. There are multiple obstacles in achieving this goal such as the decentralized nature of the system, unknown resource qualities, communication, computation and switching costs. We propose distributed learning algorithms with logarithmic regret with respect to the optimal allocation. Our logarithmic regret result holds under both i.i.d. and Markovian reward models, as well as under communication, computation and switching costs. version:1
arxiv-1210-5500 | Modeling with Copulas and Vines in Estimation of Distribution Algorithms | http://arxiv.org/abs/1210.5500 | id:1210.5500 author:Marta Soto, Yasser González-Fernández, Alberto Ochoa category:cs.NE stat.ME  published:2012-10-19 summary:The aim of this work is studying the use of copulas and vines in the optimization with Estimation of Distribution Algorithms (EDAs). Two EDAs are built around the multivariate product and normal copulas, and other two are based on pair-copula decomposition of vine models. Empirically we study the effect of both marginal distributions and dependence structure separately, and show that both aspects play a crucial role in the success of the optimization. The results show that the use of copulas and vines opens new opportunities to a more appropriate modeling of search distributions in EDAs. version:1
arxiv-1210-5517 | Design of English-Hindi Translation Memory for Efficient Translation | http://arxiv.org/abs/1210.5517 | id:1210.5517 author:Nisheeth Joshi, Iti Mathur category:cs.CL  published:2012-10-19 summary:Developing parallel corpora is an important and a difficult activity for Machine Translation. This requires manual annotation by Human Translators. Translating same text again is a useless activity. There are tools available to implement this for European Languages, but no such tool is available for Indian Languages. In this paper we present a tool for Indian Languages which not only provides automatic translations of the previously available translation but also provides multiple translations, in cases where a sentence has multiple translations, in ranked list of suggestive translations for a sentence. Moreover this tool also lets translators have global and local saving options of their work, so that they may share it with others, which further lightens the task. version:1
