arxiv-1401-2184 | Variations on Memetic Algorithms for Graph Coloring Problems | http://arxiv.org/abs/1401.2184 | id:1401.2184 author:Laurent Moalic, Alexandre Gondran category:cs.AI cs.NE math.OC  published:2014-01-08 summary:Graph vertices coloring with a given number of colors is a famous and much-studied NP-complete problem. The best methods to solve this problem are hybrid algorithms such as memetic algorithms [Galinier99, Lu10, Wu12] or quantum annealing [Titiloye11a, Titiloye11b, Titiloye12]. Those hybrid algorithms use a powerful local search inside a population-based algorithm. The balance between intensification and diversification is essential for those metaheuristics but difficult to archieve. Customizing metaheuristics takes long time and is one of the main weak points of these approaches. This paper studies the impact of the increase and the decrease of diversification in one of the most effective algorithms known: the Hybrid Evolutionary Algorithm (HEA) from Galinier and Hao [Galinier99]. We then propose a modification of this memetic algorithm in order to work with a population of only two individuals. This new algorithm more effectively manages the correct 'dose' of diversification to add into the included local search - TabuCol [Hertz87] in the case of the HEA. It has produced several good results for the well-known DIMACS benchmark graphs, such as 47-colorings for DSJC500.5, 82-colorings for DSJC1000.5, 222-colorings for DSJC1000.9 and 81-colorings for flat1000\_76\_0, which have so far only been produced by quantum annealing [Titiloye12] in 2012 with massive multi-CPUs. version:1
arxiv-1401-1778 | Large Scale Visual Recommendations From Street Fashion Images | http://arxiv.org/abs/1401.1778 | id:1401.1778 author:Vignesh Jagadeesh, Robinson Piramuthu, Anurag Bhardwaj, Wei Di, Neel Sundaresan category:cs.CV  published:2014-01-08 summary:We describe a completely automated large scale visual recommendation system for fashion. Our focus is to efficiently harness the availability of large quantities of online fashion images and their rich meta-data. Specifically, we propose four data driven models in the form of Complementary Nearest Neighbor Consensus, Gaussian Mixture Models, Texture Agnostic Retrieval and Markov Chain LDA for solving this problem. We analyze relative merits and pitfalls of these algorithms through extensive experimentation on a large-scale data set and baseline them against existing ideas from color science. We also illustrate key fashion insights learned through these experiments and show how they can be employed to design better recommendation systems. Finally, we also outline a large-scale annotated data set of fashion images (Fashion-136K) that can be exploited for future vision research. version:1
arxiv-1401-0569 | Natural Language Processing in Biomedicine: A Unified System Architecture Overview | http://arxiv.org/abs/1401.0569 | id:1401.0569 author:Son Doan, Mike Conway, Tu Minh Phuong, Lucila Ohno-Machado category:cs.CL  published:2014-01-03 summary:In modern electronic medical records (EMR) much of the clinically important data - signs and symptoms, symptom severity, disease status, etc. - are not provided in structured data fields, but rather are encoded in clinician generated narrative text. Natural language processing (NLP) provides a means of "unlocking" this important data source for applications in clinical decision support, quality assurance, and public health. This chapter provides an overview of representative NLP systems in biomedicine based on a unified architectural view. A general architecture in an NLP system consists of two main components: background knowledge that includes biomedical knowledge resources and a framework that integrates NLP tools to process text. Systems differ in both components, which we will review briefly. Additionally, challenges facing current research efforts in biomedical NLP include the paucity of large, publicly available annotated corpora, although initiatives that facilitate data sharing, system evaluation, and collaborative work between researchers in clinical NLP are starting to emerge. version:2
arxiv-1401-1742 | Content Based Image Indexing and Retrieval | http://arxiv.org/abs/1401.1742 | id:1401.1742 author:Avinash N Bhute, B. B. Meshram category:cs.CV cs.GR cs.IR cs.MM  published:2014-01-08 summary:In this paper, we present the efficient content based image retrieval systems which employ the color, texture and shape information of images to facilitate the retrieval process. For efficient feature extraction, we extract the color, texture and shape feature of images automatically using edge detection which is widely used in signal processing and image compression. For facilitated the speedy retrieval we are implements the antipole-tree algorithm for indexing the images. version:1
arxiv-1401-2899 | Application of the Modified Fractal Signature Method for Terrain Classification from Synthetic Aperture Radar Images | http://arxiv.org/abs/1401.2899 | id:1401.2899 author:A. Malamou, C. Pandis, P. Frangos, P. Stefaneas, A. Karakasiliotis, D. Kodokostas category:cs.CV  published:2014-01-08 summary:In this paper the Modified Fractal Signature method is applied to real Synthetic Aperture Radar images provided to our research group by SET 163 Working Group on SAR radar techniques. This method uses the blanket technique to provide useful information for SAR image classification. It is based on the calculation of the volume of a blanket, corresponding to the image to be classified, and then on the calculation of the corresponding Fractal Area curve and Fractal Dimension curve of the image. The main idea concerning this proposed technique is the fact that different terrain types encountered in SAR images yield different values of Fractal Area curves and Fractal Dimension curves, upon which classification of different types of terrain is possible. As a result, a classification technique for five different terrain types, i.e. urban, suburban, rural, mountain and sea, is presented in this paper. version:1
arxiv-1209-0089 | Estimating the historical and future probabilities of large terrorist events | http://arxiv.org/abs/1209.0089 | id:1209.0089 author:Aaron Clauset, Ryan Woodard category:physics.data-an cs.LG physics.soc-ph stat.AP stat.ME  published:2012-09-01 summary:Quantities with right-skewed distributions are ubiquitous in complex social systems, including political conflict, economics and social networks, and these systems sometimes produce extremely large events. For instance, the 9/11 terrorist events produced nearly 3000 fatalities, nearly six times more than the next largest event. But, was this enormous loss of life statistically unlikely given modern terrorism's historical record? Accurately estimating the probability of such an event is complicated by the large fluctuations in the empirical distribution's upper tail. We present a generic statistical algorithm for making such estimates, which combines semi-parametric models of tail behavior and a nonparametric bootstrap. Applied to a global database of terrorist events, we estimate the worldwide historical probability of observing at least one 9/11-sized or larger event since 1968 to be 11-35%. These results are robust to conditioning on global variations in economic development, domestic versus international events, the type of weapon used and a truncated history that stops at 1998. We then use this procedure to make a data-driven statistical forecast of at least one similar event over the next decade. version:3
arxiv-1209-5218 | A New Continuous-Time Equality-Constrained Optimization Method to Avoid Singularity | http://arxiv.org/abs/1209.5218 | id:1209.5218 author:Quan Quan, Kai-Yuan Cai category:cs.NE  published:2012-09-24 summary:In equality-constrained optimization, a standard regularity assumption is often associated with feasible point methods, namely the gradients of constraints are linearly independent. In practice, the regularity assumption may be violated. To avoid such a singularity, we propose a new projection matrix, based on which a feasible point method for the continuous-time, equality-constrained optimization problem is developed. First, the equality constraint is transformed into a continuous-time dynamical system with solutions that always satisfy the equality constraint. Then, the singularity is explained in detail and a new projection matrix is proposed to avoid singularity. An update (or say a controller) is subsequently designed to decrease the objective function along the solutions of the transformed system. The invariance principle is applied to analyze the behavior of the solution. We also propose a modified approach for addressing cases in which solutions do not satisfy the equality constraint. Finally, the proposed optimization approaches are applied to two examples to demonstrate its effectiveness. version:2
arxiv-1401-1560 | Beyond One-Step-Ahead Forecasting: Evaluation of Alternative Multi-Step-Ahead Forecasting Models for Crude Oil Prices | http://arxiv.org/abs/1401.1560 | id:1401.1560 author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.LG cs.AI  published:2014-01-08 summary:An accurate prediction of crude oil prices over long future horizons is challenging and of great interest to governments, enterprises, and investors. This paper proposes a revised hybrid model built upon empirical mode decomposition (EMD) based on the feed-forward neural network (FNN) modeling framework incorporating the slope-based method (SBM), which is capable of capturing the complex dynamic of crude oil prices. Three commonly used multi-step-ahead prediction strategies proposed in the literature, including iterated strategy, direct strategy, and MIMO (multiple-input multiple-output) strategy, are examined and compared, and practical considerations for the selection of a prediction strategy for multi-step-ahead forecasting relating to crude oil prices are identified. The weekly data from the WTI (West Texas Intermediate) crude oil spot price are used to compare the performance of the alternative models under the EMD-SBM-FNN modeling framework with selected counterparts. The quantitative and comprehensive assessments are performed on the basis of prediction accuracy and computational cost. The results obtained in this study indicate that the proposed EMD-SBM-FNN model using the MIMO strategy is the best in terms of prediction accuracy with accredited computational load. version:1
arxiv-1401-1558 | The Continuity of Images by Transmission Imaging Revisited | http://arxiv.org/abs/1401.1558 | id:1401.1558 author:Zhitao Fan, Feng Guan, Chunlin Wu, Ming Yan category:math.DG cs.CV math.NA 92C55  90C90  68U10  published:2014-01-08 summary:Transmission imaging, as an important imaging technique widely used in astronomy, medical diagnosis, and biology science, has been shown in [49] quite different from reflection imaging used in our everyday life. Understanding the structures of images (the prior information) is important for designing, testing, and choosing image processing methods, and good image processing methods are helpful for further uses of the image data, e.g., increasing the accuracy of the object reconstruction methods in transmission imaging applications. In reflection imaging, the images are usually modeled as discontinuous functions and even piecewise constant functions. In transmission imaging, it was shown very recently in [49] that almost all images are continuous functions. However, the author in [49] considered only the case of parallel beam geometry and used some too strong assumptions in the proof, which exclude some common cases such as cylindrical objects. In this paper, we consider more general beam geometries and simplify the assumptions by using totally different techniques. In particular, we will prove that almost all images in transmission imaging with both parallel and divergent beam geometries (two most typical beam geometries) are continuous functions, under much weaker assumptions than those in [49], which admit almost all practical cases. Besides, taking into accounts our analysis, we compare two image processing methods for Poisson noise (which is the most significant noise in transmission imaging) removal. Numerical experiments will be provided to demonstrate our analysis. version:1
arxiv-1401-0942 | Factorized Point Process Intensities: A Spatial Analysis of Professional Basketball | http://arxiv.org/abs/1401.0942 | id:1401.0942 author:Andrew Miller, Luke Bornn, Ryan Adams, Kirk Goldsberry category:stat.ML stat.AP  published:2014-01-05 summary:We develop a machine learning approach to represent and analyze the underlying spatial structure that governs shot selection among professional basketball players in the NBA. Typically, NBA players are discussed and compared in an heuristic, imprecise manner that relies on unmeasured intuitions about player behavior. This makes it difficult to draw comparisons between players and make accurate player specific predictions. Modeling shot attempt data as a point process, we create a low dimensional representation of offensive player types in the NBA. Using non-negative matrix factorization (NMF), an unsupervised dimensionality reduction technique, we show that a low-rank spatial decomposition summarizes the shooting habits of NBA players. The spatial representations discovered by the algorithm correspond to intuitive descriptions of NBA player types, and can be used to model other spatial effects, such as shooting accuracy. version:2
arxiv-1401-1489 | Key point selection and clustering of swimmer coordination through Sparse Fisher-EM | http://arxiv.org/abs/1401.1489 | id:1401.1489 author:John Komar, Romain Hérault, Ludovic Seifert category:stat.ML cs.CV cs.LG physics.data-an stat.AP  published:2014-01-07 summary:To answer the existence of optimal swimmer learning/teaching strategies, this work introduces a two-level clustering in order to analyze temporal dynamics of motor learning in breaststroke swimming. Each level have been performed through Sparse Fisher-EM, a unsupervised framework which can be applied efficiently on large and correlated datasets. The induced sparsity selects key points of the coordination phase without any prior knowledge. version:1
arxiv-1401-1486 | Design & Development of the Graphical User Interface for Sindhi Language | http://arxiv.org/abs/1401.1486 | id:1401.1486 author:Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah category:cs.HC cs.CL  published:2014-01-07 summary:This paper describes the design and implementation of a Unicode-based GUISL (Graphical User Interface for Sindhi Language). The idea is to provide a software platform to the people of Sindh as well as Sindhi diasporas living across the globe to make use of computing for basic tasks such as editing, composition, formatting, and printing of documents in Sindhi by using GUISL. The implementation of the GUISL has been done in the Java technology to make the system platform independent. The paper describes several design issues of Sindhi GUI in the context of existing software tools and technologies and explains how mapping and concatenation techniques have been employed to achieve the cursive shape of Sindhi script. version:1
arxiv-1401-1465 | Cortical prediction markets | http://arxiv.org/abs/1401.1465 | id:1401.1465 author:David Balduzzi category:cs.AI cs.GT cs.LG cs.MA q-bio.NC  published:2014-01-07 summary:We investigate cortical learning from the perspective of mechanism design. First, we show that discretizing standard models of neurons and synaptic plasticity leads to rational agents maximizing simple scoring rules. Second, our main result is that the scoring rules are proper, implying that neurons faithfully encode expected utilities in their synaptic weights and encode high-scoring outcomes in their spikes. Third, with this foundation in hand, we propose a biologically plausible mechanism whereby neurons backpropagate incentives which allows them to optimize their usefulness to the rest of cortex. Finally, experiments show that networks that backpropagate incentives can learn simple tasks. version:1
arxiv-1401-0086 | Forward-Backward Greedy Algorithms for General Convex Smooth Functions over A Cardinality Constraint | http://arxiv.org/abs/1401.0086 | id:1401.0086 author:Ji Liu, Ryohei Fujimaki, Jieping Ye category:stat.ML  published:2013-12-31 summary:We consider forward-backward greedy algorithms for solving sparse feature selection problems with general convex smooth functions. A state-of-the-art greedy method, the Forward-Backward greedy algorithm (FoBa-obj) requires to solve a large number of optimization problems, thus it is not scalable for large-size problems. The FoBa-gdt algorithm, which uses the gradient information for feature selection at each forward iteration, significantly improves the efficiency of FoBa-obj. In this paper, we systematically analyze the theoretical properties of both forward-backward greedy algorithms. Our main contributions are: 1) We derive better theoretical bounds than existing analyses regarding FoBa-obj for general smooth convex functions; 2) We show that FoBa-gdt achieves the same theoretical performance as FoBa-obj under the same condition: restricted strong convexity condition. Our new bounds are consistent with the bounds of a special case (least squares) and fills a previously existing theoretical gap for general convex smooth functions; 3) We show that the restricted strong convexity condition is satisfied if the number of independent samples is more than $\bar{k}\log d$ where $\bar{k}$ is the sparsity number and $d$ is the dimension of the variable; 4) We apply FoBa-gdt (with the conditional random field objective) to the sensor selection problem for human indoor activity recognition and our results show that FoBa-gdt outperforms other methods (including the ones based on forward greedy selection and L1-regularization). version:2
arxiv-1405-7393 | Insights from the Wikipedia Contest (IEEE Contest for Data Mining 2011) | http://arxiv.org/abs/1405.7393 | id:1405.7393 author:Kalpit V Desai, Roopesh Ranjan category:cs.CY physics.soc-ph stat.ML  published:2014-01-07 summary:The Wikimedia Foundation has recently observed that newly joining editors on Wikipedia are increasingly failing to integrate into the Wikipedia editors' community, i.e. the community is becoming increasingly harder to penetrate. To sustain healthy growth of the community, the Wikimedia Foundation aims to quantitatively understand the factors that determine the editing behavior, and explain why most new editors become inactive soon after joining. As a step towards this broader goal, the Wikimedia foundation sponsored the ICDM (IEEE International Conference for Data Mining) contest for the year 2011. The objective for the participants was to develop models to predict the number of edits that an editor will make in future five months based on the editing history of the editor. Here we describe the approach we followed for developing predictive models towards this goal, the results that we obtained and the modeling insights that we gained from this exercise. In addition, towards the broader goal of Wikimedia Foundation, we also summarize the factors that emerged during our model building exercise as powerful predictors of future editing activity. version:1
arxiv-1401-1333 | Time series forecasting using neural networks | http://arxiv.org/abs/1401.1333 | id:1401.1333 author:Bogdan Oancea, ŞTefan Cristian Ciucu category:cs.NE  published:2014-01-07 summary:Recent studies have shown the classification and prediction power of the Neural Networks. It has been demonstrated that a NN can approximate any continuous function. Neural networks have been successfully used for forecasting of financial data series. The classical methods used for time series prediction like Box-Jenkins or ARIMA assumes that there is a linear relationship between inputs and outputs. Neural Networks have the advantage that can approximate nonlinear functions. In this paper we compared the performances of different feed forward and recurrent neural networks and training algorithms for predicting the exchange rate EUR/RON and USD/RON. We used data series with daily exchange rates starting from 2005 until 2013. version:1
arxiv-1304-2999 | A New Approach To Two-View Motion Segmentation Using Global Dimension Minimization | http://arxiv.org/abs/1304.2999 | id:1304.2999 author:Bryan Poling, Gilad Lerman category:cs.CV  published:2013-04-10 summary:We present a new approach to rigid-body motion segmentation from two views. We use a previously developed nonlinear embedding of two-view point correspondences into a 9-dimensional space and identify the different motions by segmenting lower-dimensional subspaces. In order to overcome nonuniform distributions along the subspaces, whose dimensions are unknown, we suggest the novel concept of global dimension and its minimization for clustering subspaces with some theoretical motivation. We propose a fast projected gradient algorithm for minimizing global dimension and thus segmenting motions from 2-views. We develop an outlier detection framework around the proposed method, and we present state-of-the-art results on outlier-free and outlier-corrupted two-view data for segmenting motion. version:2
arxiv-1305-0087 | Quantile Regression for Large-scale Applications | http://arxiv.org/abs/1305.0087 | id:1305.0087 author:Jiyan Yang, Xiangrui Meng, Michael W. Mahoney category:cs.DS cs.DC cs.NA stat.ML  published:2013-05-01 summary:Quantile regression is a method to estimate the quantiles of the conditional distribution of a response variable, and as such it permits a much more accurate portrayal of the relationship between the response variable and observed covariates than methods such as Least-squares or Least Absolute Deviations regression. It can be expressed as a linear program, and, with appropriate preprocessing, interior-point methods can be used to find a solution for moderately large problems. Dealing with very large problems, \emph(e.g.), involving data up to and beyond the terabyte regime, remains a challenge. Here, we present a randomized algorithm that runs in nearly linear time in the size of the input and that, with constant probability, computes a $(1+\epsilon)$ approximate solution to an arbitrary quantile regression problem. As a key step, our algorithm computes a low-distortion subspace-preserving embedding with respect to the loss function of quantile regression. Our empirical evaluation illustrates that our algorithm is competitive with the best previous work on small to medium-sized problems, and that in addition it can be implemented in MapReduce-like environments and applied to terabyte-sized problems. version:3
arxiv-1401-1190 | Bangla Text Recognition from Video Sequence: A New Focus | http://arxiv.org/abs/1401.1190 | id:1401.1190 author:Souvik Bhowmick, Purnendu Banerjee category:cs.CV  published:2014-01-06 summary:Extraction and recognition of Bangla text from video frame images is challenging due to complex color background, low-resolution etc. In this paper, we propose an algorithm for extraction and recognition of Bangla text form such video frames with complex background. Here, a two-step approach has been proposed. First, the text line is segmented into words using information based on line contours. First order gradient value of the text blocks are used to find the word gap. Next, a local binarization technique is applied on each word and text line is reconstructed using those words. Secondly, this binarized text block is sent to OCR for recognition purpose. version:1
arxiv-1312-5412 | Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural Images | http://arxiv.org/abs/1312.5412 | id:1312.5412 author:Taichi Kiwaki, Takaki Makino, Kazuyuki Aihara category:stat.ML cs.LG  published:2013-12-19 summary:We pursue an early stopping technique that helps Gaussian Restricted Boltzmann Machines (GRBMs) to gain good natural image representations in terms of overcompleteness and data fitting. GRBMs are widely considered as an unsuitable model for natural images because they gain non-overcomplete representations which include uniform filters that do not represent useful image features. We have recently found that GRBMs once gain and subsequently lose useful filters during their training, contrary to this common perspective. We attribute this phenomenon to a tradeoff between overcompleteness of GRBM representations and data fitting. To gain GRBM representations that are overcomplete and fit data well, we propose a measure for GRBM representation quality, approximated mutual information, and an early stopping technique based on this measure. The proposed method boosts performance of classifiers trained on GRBM representations. version:3
arxiv-1401-1158 | Effective Slot Filling Based on Shallow Distant Supervision Methods | http://arxiv.org/abs/1401.1158 | id:1401.1158 author:Benjamin Roth, Tassilo Barth, Michael Wiegand, Mittul Singh, Dietrich Klakow category:cs.CL  published:2014-01-06 summary:Spoken Language Systems at Saarland University (LSV) participated this year with 5 runs at the TAC KBP English slot filling track. Effective algorithms for all parts of the pipeline, from document retrieval to relation prediction and response post-processing, are bundled in a modular end-to-end relation extraction system called RelationFactory. The main run solely focuses on shallow techniques and achieved significant improvements over LSV's last year's system, while using the same training data and patterns. Improvements mainly have been obtained by a feature representation focusing on surface skip n-grams and improved scoring for extracted distant supervision patterns. Important factors for effective extraction are the training and tuning scheme for distant supervision classifiers, and the query expansion by a translation model based on Wikipedia links. In the TAC KBP 2013 English Slotfilling evaluation, the submitted main run of the LSV RelationFactory system achieved the top-ranked F1-score of 37.3%. version:1
arxiv-1401-1123 | Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits | http://arxiv.org/abs/1401.1123 | id:1401.1123 author:Nicolas Galichet, Michèle Sebag, Olivier Teytaud category:cs.LG  published:2014-01-06 summary:Motivated by applications in energy management, this paper presents the Multi-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting the exploration of risky arms, MARAB takes as arm quality its conditional value at risk. When the user-supplied risk level goes to 0, the arm quality tends toward the essential infimum of the arm distribution density, and MARAB tends toward the MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal value. As a first contribution, this paper presents a theoretical analysis of the MIN algorithm under mild assumptions, establishing its robustness comparatively to UCB. The analysis is supported by extensive experimental validation of MIN and MARAB compared to UCB and state-of-art risk-aware MAB algorithms on artificial and real-world problems. version:1
arxiv-1212-0478 | Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses | http://arxiv.org/abs/1212.0478 | id:1212.0478 author:Po-Ling Loh, Martin J. Wainwright category:stat.ML math.ST stat.TH  published:2012-12-03 summary:We investigate the relationship between the structure of a discrete graphical model and the support of the inverse of a generalized covariance matrix. We show that for certain graph structures, the support of the inverse covariance matrix of indicator variables on the vertices of a graph reflects the conditional independence structure of the graph. Our work extends results that have previously been established only in the context of multivariate Gaussian graphical models, thereby addressing an open question about the significance of the inverse covariance matrix of a non-Gaussian distribution. The proof exploits a combination of ideas from the geometry of exponential families, junction tree theory and convex analysis. These population-level results have various consequences for graph selection methods, both known and novel, including a novel method for structure estimation for missing or corrupted observations. We provide nonasymptotic guarantees for such methods and illustrate the sharpness of these predictions via simulations. version:2
arxiv-1401-0987 | Differentially Private Data Releasing for Smooth Queries with Synthetic Database Output | http://arxiv.org/abs/1401.0987 | id:1401.0987 author:Chi Jin, Ziteng Wang, Junliang Huang, Yiqiao Zhong, Liwei Wang category:cs.DB stat.ML  published:2014-01-06 summary:We consider accurately answering smooth queries while preserving differential privacy. A query is said to be $K$-smooth if it is specified by a function defined on $[-1,1]^d$ whose partial derivatives up to order $K$ are all bounded. We develop an $\epsilon$-differentially private mechanism for the class of $K$-smooth queries. The major advantage of the algorithm is that it outputs a synthetic database. In real applications, a synthetic database output is appealing. Our mechanism achieves an accuracy of $O (n^{-\frac{K}{2d+K}}/\epsilon )$, and runs in polynomial time. We also generalize the mechanism to preserve $(\epsilon, \delta)$-differential privacy with slightly improved accuracy. Extensive experiments on benchmark datasets demonstrate that the mechanisms have good accuracy and are efficient. version:1
arxiv-1401-0898 | Feature Selection Using Classifier in High Dimensional Data | http://arxiv.org/abs/1401.0898 | id:1401.0898 author:Vijendra Singh, Shivani Pathak category:cs.CV cs.LG stat.ML  published:2014-01-05 summary:Feature selection is frequently used as a pre-processing step to machine learning. It is a process of choosing a subset of original features so that the feature space is optimally reduced according to a certain evaluation criterion. The central objective of this paper is to reduce the dimension of the data by finding a small set of important features which can give good classification performance. We have applied filter and wrapper approach with different classifiers QDA and LDA respectively. A widely-used filter method is used for bioinformatics data i.e. a univariate criterion separately on each feature, assuming that there is no interaction between features and then applied Sequential Feature Selection method. Experimental results show that filter approach gives better performance in respect of Misclassification Error Rate. version:1
arxiv-1401-0887 | Learning parametric dictionaries for graph signals | http://arxiv.org/abs/1401.0887 | id:1401.0887 author:Dorina Thanou, David I Shuman, Pascal Frossard category:cs.LG cs.SI stat.ML  published:2014-01-05 summary:In sparse signal representation, the choice of a dictionary often involves a tradeoff between two desirable properties -- the ability to adapt to specific signal data and a fast implementation of the dictionary. To sparsely represent signals residing on weighted graphs, an additional design challenge is to incorporate the intrinsic geometric structure of the irregular data domain into the atoms of the dictionary. In this work, we propose a parametric dictionary learning algorithm to design data-adapted, structured dictionaries that sparsely represent graph signals. In particular, we model graph signals as combinations of overlapping local patterns. We impose the constraint that each dictionary is a concatenation of subdictionaries, with each subdictionary being a polynomial of the graph Laplacian matrix, representing a single pattern translated to different areas of the graph. The learning algorithm adapts the patterns to a training set of graph signals. Experimental results on both synthetic and real datasets demonstrate that the dictionaries learned by the proposed algorithm are competitive with and often better than unstructured dictionaries learned by state-of-the-art numerical learning algorithms in terms of sparse approximation of graph signals. In contrast to the unstructured dictionaries, however, the dictionaries learned by the proposed algorithm feature localized atoms and can be implemented in a computationally efficient manner in signal processing tasks such as compression, denoising, and classification. version:1
arxiv-1401-0886 | Spectrum Hole Prediction Based On Historical Data: A Neural Network Approach | http://arxiv.org/abs/1401.0886 | id:1401.0886 author:Barau Gafai Najashi, Feng Wenjiang, Mohammed Dikko Almustapha category:cs.NE  published:2014-01-05 summary:The concept of cognitive radio pioneered by Mitola promises to change the future of wireless communication especially in the area of spectrum management. Currently, the command and control strategy employed in spectrum assignment is too rigid and needs to be reviewed. Recent studies have shown that assigned spectrum is underutilized spectrally and temporally. Cognitive radio provides a viable solution whereby licensed users can share the spectrum with unlicensed users opportunistically without causing interference. Unlicensed users must be able to sense weather the channel is busy or idle, failure to do so will lead to interference to the licensed user. In this paper, a neural network based prediction model for predicting the channel status using historical data obtained during a spectrum occupancy measurement is presented. Genetic algorithm is combined with LM BP for increasing the probability of obtaining the best weights thus optimizing the network. The results obtained indicate high prediction accuracy over all bands considered version:1
arxiv-1401-0871 | Stylistic Clusters and the Syrian/South Syrian Tradition of First-Millennium BCE Levantine Ivory Carving: A Machine Learning Approach | http://arxiv.org/abs/1401.0871 | id:1401.0871 author:Amy Rebecca Gansell, Jan-Willem van de Meent, Sakellarios Zairis, Chris H. Wiggins category:stat.ML stat.AP  published:2014-01-05 summary:Thousands of first-millennium BCE ivory carvings have been excavated from Neo-Assyrian sites in Mesopotamia (primarily Nimrud, Khorsabad, and Arslan Tash) hundreds of miles from their Levantine production contexts. At present, their specific manufacture dates and workshop localities are unknown. Relying on subjective, visual methods, scholars have grappled with their classification and regional attribution for over a century. This study combines visual approaches with machine-learning techniques to offer data-driven perspectives on the classification and attribution of this early Iron Age corpus. The study sample consisted of 162 sculptures of female figures. We have developed an algorithm that clusters the ivories based on a combination of descriptive and anthropometric data. The resulting categories, which are based on purely statistical criteria, show good agreement with conventional art historical classifications, while revealing new perspectives, especially with regard to the contested Syrian/South Syrian/Intermediate tradition. Specifically, we have identified that objects of the Syrian/South Syrian/Intermediate tradition may be more closely related to Phoenician objects than to North Syrian objects; we offer a reconsideration of a subset of Phoenician objects, and we confirm Syrian/South Syrian/Intermediate stylistic subgroups that might distinguish networks of acquisition among the sites of Nimrud, Khorsabad, Arslan Tash and the Levant. We have also identified which features are most significant in our cluster assignments and might thereby be most diagnostic of regional carving traditions. In short, our study both corroborates traditional visual classification methods and demonstrates how machine-learning techniques may be employed to reveal complementary information not accessible through the exclusively visual analysis of an archaeological corpus. version:1
arxiv-1401-0870 | Pectoral Muscles Suppression in Digital Mammograms using Hybridization of Soft Computing Methods | http://arxiv.org/abs/1401.0870 | id:1401.0870 author:I. Laurence Aroquiaraj, K. Thangavel category:cs.CV cs.CE  published:2014-01-05 summary:Breast region segmentation is an essential prerequisite in computerized analysis of mammograms. It aims at separating the breast tissue from the background of the mammogram and it includes two independent segmentations. The first segments the background region which usually contains annotations, labels and frames from the whole breast region, while the second removes the pectoral muscle portion (present in Medio Lateral Oblique (MLO) views) from the rest of the breast tissue. In this paper we propose hybridization of Connected Component Labeling (CCL), Fuzzy, and Straight line methods. Our proposed methods worked good for separating pectoral region. After removal pectoral muscle from the mammogram, further processing is confined to the breast region alone. To demonstrate the validity of our segmentation algorithm, it is extensively tested using over 322 mammographic images from the Mammographic Image Analysis Society (MIAS) database. The segmentation results were evaluated using a Mean Absolute Error (MAE), Hausdroff Distance (HD), Probabilistic Rand Index (PRI), Local Consistency Error (LCE) and Tanimoto Coefficient (TC). The hybridization of fuzzy with straight line method is given more than 96% of the curve segmentations to be adequate or better. In addition a comparison with similar approaches from the state of the art has been given, obtaining slightly improved results. Experimental results demonstrate the effectiveness of the proposed approach. version:1
arxiv-1212-0692 | An Empirical Evaluation of Portfolios Approaches for solving CSPs | http://arxiv.org/abs/1212.0692 | id:1212.0692 author:Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro category:cs.AI cs.LG  published:2012-12-04 summary:Recent research in areas such as SAT solving and Integer Linear Programming has shown that the performances of a single arbitrarily efficient solver can be significantly outperformed by a portfolio of possibly slower on-average solvers. We report an empirical evaluation and comparison of portfolio approaches applied to Constraint Satisfaction Problems (CSPs). We compared models developed on top of off-the-shelf machine learning algorithms with respect to approaches used in the SAT field and adapted for CSPs, considering different portfolio sizes and using as evaluation metrics the number of solved problems and the time taken to solve them. Results indicate that the best SAT approaches have top performances also in the CSP field and are slightly more competitive than simple models built on top of classification algorithms. version:2
arxiv-1401-0858 | Multimodal Optimization by Sparkling Squid Populations | http://arxiv.org/abs/1401.0858 | id:1401.0858 author:Videh Seksaria category:cs.NE  published:2014-01-05 summary:The swarm intelligence of animals is a natural paradigm to apply to optimization problems. Ant colony, bee colony, firefly and bat algorithms are amongst those that have been demonstrated to efficiently to optimize complex constraints. This paper proposes the new Sparkling Squid Algorithm (SSA) for multimodal optimization, inspired by the intelligent swarm behavior of its namesake. After an introduction, formulation and discussion of its implementation, it will be compared to other popular metaheuristics. Finally, applications to well - known problems such as image registration and the traveling salesperson problem will be discussed. version:1
arxiv-1401-0843 | Least Squares Policy Iteration with Instrumental Variables vs. Direct Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage | http://arxiv.org/abs/1401.0843 | id:1401.0843 author:Warren R. Scott, Warren B. Powell, Somayeh Moazehi category:math.OC cs.LG  published:2014-01-04 summary:This paper studies approximate policy iteration (API) methods which use least-squares Bellman error minimization for policy evaluation. We address several of its enhancements, namely, Bellman error minimization using instrumental variables, least-squares projected Bellman error minimization, and projected Bellman error minimization using instrumental variables. We prove that for a general discrete-time stochastic control problem, Bellman error minimization using instrumental variables is equivalent to both variants of projected Bellman error minimization. An alternative to these API methods is direct policy search based on knowledge gradient. The practical performance of these three approximate dynamic programming methods are then investigated in the context of an application in energy storage, integrated with an intermittent wind energy supply to fully serve a stochastic time-varying electricity demand. We create a library of test problems using real-world data and apply value iteration to find their optimal policies. These benchmarks are then used to compare the developed policies. Our analysis indicates that API with instrumental variables Bellman error minimization prominently outperforms API with least-squares Bellman error minimization. However, these approaches underperform our direct policy search implementation. version:1
arxiv-1401-0794 | Properties of phoneme N -grams across the world's language families | http://arxiv.org/abs/1401.0794 | id:1401.0794 author:Taraka Rama, Lars Borin category:cs.CL stat.CO  published:2014-01-04 summary:In this article, we investigate the properties of phoneme N-grams across half of the world's languages. We investigate if the sizes of three different N-gram distributions of the world's language families obey a power law. Further, the N-gram distributions of language families parallel the sizes of the families, which seem to obey a power law distribution. The correlation between N-gram distributions and language family sizes improves with increasing values of N. We applied statistical tests, originally given by physicists, to test the hypothesis of power law fit to twelve different datasets. The study also raises some new questions about the use of N-gram distributions in linguistic research, which we answer by running a statistical test. version:1
arxiv-1401-0767 | From Kernel Machines to Ensemble Learning | http://arxiv.org/abs/1401.0767 | id:1401.0767 author:Chunhua Shen, Fayao Liu category:cs.LG cs.CV  published:2014-01-04 summary:Ensemble methods such as boosting combine multiple learners to obtain better prediction than could be obtained from any individual learner. Here we propose a principled framework for directly constructing ensemble learning methods from kernel methods. Unlike previous studies showing the equivalence between boosting and support vector machines (SVMs), which needs a translation procedure, we show that it is possible to design boosting-like procedure to solve the SVM optimization problems. In other words, it is possible to design ensemble methods directly from SVM without any middle procedure. This finding not only enables us to design new ensemble learning methods directly from kernel methods, but also makes it possible to take advantage of those highly-optimized fast linear SVM solvers for ensemble learning. We exemplify this framework for designing binary ensemble learning as well as a new multi-class ensemble learning methods. Experimental results demonstrate the flexibility and usefulness of the proposed framework. version:1
arxiv-1401-0764 | Context-Aware Hypergraph Construction for Robust Spectral Clustering | http://arxiv.org/abs/1401.0764 | id:1401.0764 author:Xi Li, Weiming Hu, Chunhua Shen, Anthony Dick, Zhongfei Zhang category:cs.CV cs.LG  published:2014-01-04 summary:Spectral clustering is a powerful tool for unsupervised data analysis. In this paper, we propose a context-aware hypergraph similarity measure (CAHSM), which leads to robust spectral clustering in the case of noisy data. We construct three types of hypergraph---the pairwise hypergraph, the k-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering hypergraph. The pairwise hypergraph captures the pairwise similarity of data points; the kNN hypergraph captures the neighborhood of each point; and the clustering hypergraph encodes high-order contexts within the dataset. By combining the affinity information from these three hypergraphs, the CAHSM algorithm is able to explore the intrinsic topological information of the dataset. Therefore, data clustering using CAHSM tends to be more robust. Considering the intra-cluster compactness and the inter-cluster separability of vertices, we further design a discriminative hypergraph partitioning criterion (DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is developed. Theoretical analysis and experimental evaluation demonstrate the effectiveness and robustness of the proposed algorithm. version:1
arxiv-1401-0742 | Data Smashing | http://arxiv.org/abs/1401.0742 | id:1401.0742 author:Ishanu Chattopadhyay, Hod Lipson category:cs.LG cs.AI cs.CE cs.IT math.IT stat.ML  published:2014-01-03 summary:Investigation of the underlying physics or biology from empirical data requires a quantifiable notion of similarity - when do two observed data sets indicate nearly identical generating processes, and when they do not. The discriminating characteristics to look for in data is often determined by heuristics designed by experts, $e.g.$, distinct shapes of "folded" lightcurves may be used as "features" to classify variable stars, while determination of pathological brain states might require a Fourier analysis of brainwave activity. Finding good features is non-trivial. Here, we propose a universal solution to this problem: we delineate a principle for quantifying similarity between sources of arbitrary data streams, without a priori knowledge, features or training. We uncover an algebraic structure on a space of symbolic models for quantized data, and show that such stochastic generators may be added and uniquely inverted; and that a model and its inverse always sum to the generator of flat white noise. Therefore, every data stream has an anti-stream: data generated by the inverse model. Similarity between two streams, then, is the degree to which one, when summed to the other's anti-stream, mutually annihilates all statistical structure to noise. We call this data smashing. We present diverse applications, including disambiguation of brainwaves pertaining to epileptic seizures, detection of anomalous cardiac rhythms, and classification of astronomical objects from raw photometry. In our examples, the data smashing principle, without access to any domain knowledge, meets or exceeds the performance of specialized algorithms tuned by domain experts. version:1
arxiv-1401-0708 | Quantitative methods for Phylogenetic Inference in Historical Linguistics: An experimental case study of South Central Dravidian | http://arxiv.org/abs/1401.0708 | id:1401.0708 author:Taraka Rama, Sudheer Kolachina, Lakshmi Bai B category:cs.CL cs.AI  published:2014-01-03 summary:In this paper we examine the usefulness of two classes of algorithms Distance Methods, Discrete Character Methods (Felsenstein and Felsenstein 2003) widely used in genetics, for predicting the family relationships among a set of related languages and therefore, diachronic language change. Applying these algorithms to the data on the numbers of shared cognates- with-change and changed as well as unchanged cognates for a group of six languages belonging to a Dravidian language sub-family given in Krishnamurti et al. (1983), we observed that the resultant phylogenetic trees are largely in agreement with the linguistic family tree constructed using the comparative method of reconstruction with only a few minor differences. Furthermore, we studied these minor differences and found that they were cases of genuine ambiguity even for a well-trained historical linguist. We evaluated the trees obtained through our experiments using a well-defined criterion and report the results here. We finally conclude that quantitative methods like the ones we examined are quite useful in predicting family relationships among languages. In addition, we conclude that a modest degree of confidence attached to the intuition that there could indeed exist a parallelism between the processes of linguistic and genetic change is not totally misplaced. version:1
arxiv-1302-5729 | Sparse Signal Estimation by Maximally Sparse Convex Optimization | http://arxiv.org/abs/1302.5729 | id:1302.5729 author:Ivan W. Selesnick, Ilker Bayram category:cs.LG stat.ML  published:2013-02-22 summary:This paper addresses the problem of sparsity penalized least squares for applications in sparse signal processing, e.g. sparse deconvolution. This paper aims to induce sparsity more strongly than L1 norm regularization, while avoiding non-convex optimization. For this purpose, this paper describes the design and use of non-convex penalty functions (regularizers) constrained so as to ensure the convexity of the total cost function, F, to be minimized. The method is based on parametric penalty functions, the parameters of which are constrained to ensure convexity of F. It is shown that optimal parameters can be obtained by semidefinite programming (SDP). This maximally sparse convex (MSC) approach yields maximally non-convex sparsity-inducing penalty functions constrained such that the total cost function, F, is convex. It is demonstrated that iterative MSC (IMSC) can yield solutions substantially more sparse than the standard convex sparsity-inducing approach, i.e., L1 norm minimization. version:3
arxiv-1301-4938 | A type theoretical framework for natural language semantics: the Montagovian generative lexicon | http://arxiv.org/abs/1301.4938 | id:1301.4938 author:Christian Retoré category:cs.LO cs.CL  published:2013-01-21 summary:We present a framework, named the Montagovian generative lexicon, for computing the semantics of natural language sentences, expressed in many sorted higher order logic. Word meaning is depicted by lambda terms of second order lambda calculus (Girard's system F) with base types including a type for propositions and many types for sorts of a many sorted logic. This framework is able to integrate a proper treatment of lexical phenomena into a Montagovian compositional semantics, including the restriction of selection which imposes the nature of the arguments of a predicate, and the possible adaptation of a word meaning to some contexts. Among these adaptations of a word's sense to the context, ontological inclusions are handled by an extension of system F with coercive subtyping that is introduced in the present paper. The benefits of this framework for lexical pragmatics are illustrated on meaning transfers and coercions, on possible and impossible copredication over different senses, on deverbal ambiguities, and on "fictive motion". Next we show that the compositional treatment of determiners, quantifiers, plurals,... are finer grained in our framework. We then conclude with the linguistic, logical and computational perspectives opened by the Montagovian generative lexicon. version:3
arxiv-1401-0660 | Plurals: individuals and sets in a richly typed semantics | http://arxiv.org/abs/1401.0660 | id:1401.0660 author:Bruno Mery, Richard Moot, Christian Retoré category:cs.CL  published:2014-01-03 summary:We developed a type-theoretical framework for natural lan- guage semantics that, in addition to the usual Montagovian treatment of compositional semantics, includes a treatment of some phenomena of lex- ical semantic: coercions, meaning, transfers, (in)felicitous co-predication. In this setting we see how the various readings of plurals (collective, dis- tributive, coverings,...) can be modelled. version:1
arxiv-1401-0640 | Multi-Topic Multi-Document Summarizer | http://arxiv.org/abs/1401.0640 | id:1401.0640 author:Fatma El-Ghannam, Tarek El-Shishtawy category:cs.CL  published:2014-01-03 summary:Current multi-document summarization systems can successfully extract summary sentences, however with many limitations including: low coverage, inaccurate extraction to important sentences, redundancy and poor coherence among the selected sentences. The present study introduces a new concept of centroid approach and reports new techniques for extracting summary sentences for multi-document. In both techniques keyphrases are used to weigh sentences and documents. The first summarization technique (Sen-Rich) prefers maximum richness sentences. While the second (Doc-Rich), prefers sentences from centroid document. To demonstrate the new summarization system application to extract summaries of Arabic documents we performed two experiments. First, we applied Rouge measure to compare the new techniques among systems presented at TAC2011. The results show that Sen-Rich outperformed all systems in ROUGE-S. Second, the system was applied to summarize multi-topic documents. Using human evaluators, the results show that Doc-Rich is the superior, where summary sentences characterized by extra coverage and more cohesion. version:1
arxiv-1401-0604 | Particle Gibbs with Ancestor Sampling | http://arxiv.org/abs/1401.0604 | id:1401.0604 author:Fredrik Lindsten, Michael I. Jordan, Thomas B. Schön category:stat.CO stat.ML  published:2014-01-03 summary:Particle Markov chain Monte Carlo (PMCMC) is a systematic way of combining the two main tools used for Monte Carlo statistical inference: sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC). We present a novel PMCMC algorithm that we refer to as particle Gibbs with ancestor sampling (PGAS). PGAS provides the data analyst with an off-the-shelf class of Markov kernels that can be used to simulate the typically high-dimensional and highly autocorrelated state trajectory in a state-space model. The ancestor sampling procedure enables fast mixing of the PGAS kernel even when using seemingly few particles in the underlying SMC sampler. This is important as it can significantly reduce the computational burden that is typically associated with using SMC. PGAS is conceptually similar to the existing PG with backward simulation (PGBS) procedure. Instead of using separate forward and backward sweeps as in PGBS, however, we achieve the same effect in a single forward sweep. This makes PGAS well suited for addressing inference problems not only in state-space models, but also in models with more complex dependencies, such as non-Markovian, Bayesian nonparametric, and general probabilistic graphical models. version:1
arxiv-1211-0373 | Minimax sparse principal subspace estimation in high dimensions | http://arxiv.org/abs/1211.0373 | id:1211.0373 author:Vincent Q. Vu, Jing Lei category:math.ST stat.ML stat.TH  published:2012-11-02 summary:We study sparse principal components analysis in high dimensions, where $p$ (the number of variables) can be much larger than $n$ (the number of observations), and analyze the problem of estimating the subspace spanned by the principal eigenvectors of the population covariance matrix. We introduce two complementary notions of $\ell_q$ subspace sparsity: row sparsity and column sparsity. We prove nonasymptotic lower and upper bounds on the minimax subspace estimation error for $0\leq q\leq1$. The bounds are optimal for row sparse subspaces and nearly optimal for column sparse subspaces, they apply to general classes of covariance matrices, and they show that $\ell_q$ constrained estimates can achieve optimal minimax rates without restrictive spiked covariance conditions. Interestingly, the form of the rates matches known results for sparse regression when the effective noise variance is defined appropriately. Our proof employs a novel variational $\sin\Theta$ theorem that may be useful in other regularized spectral estimation problems. version:4
arxiv-1401-0583 | Adaptive-Rate Compressive Sensing Using Side Information | http://arxiv.org/abs/1401.0583 | id:1401.0583 author:Garrett Warnell, Sourabh Bhattacharya, Rama Chellappa, Tamer Basar category:cs.CV  published:2014-01-03 summary:We provide two novel adaptive-rate compressive sensing (CS) strategies for sparse, time-varying signals using side information. Our first method utilizes extra cross-validation measurements, and the second one exploits extra low-resolution measurements. Unlike the majority of current CS techniques, we do not assume that we know an upper bound on the number of significant coefficients that comprise the images in the video sequence. Instead, we use the side information to predict the number of significant coefficients in the signal at the next time instant. For each image in the video sequence, our techniques specify a fixed number of spatially-multiplexed CS measurements to acquire, and adjust this quantity from image to image. Our strategies are developed in the specific context of background subtraction for surveillance video, and we experimentally validate the proposed methods on real video sequences. version:1
arxiv-1401-0579 | More Algorithms for Provable Dictionary Learning | http://arxiv.org/abs/1401.0579 | id:1401.0579 author:Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma category:cs.DS cs.LG stat.ML  published:2014-01-03 summary:In dictionary learning, also known as sparse coding, the algorithm is given samples of the form $y = Ax$ where $x\in \mathbb{R}^m$ is an unknown random sparse vector and $A$ is an unknown dictionary matrix in $\mathbb{R}^{n\times m}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$ and $x$. This problem has been studied in neuroscience, machine learning, visions, and image processing. In practice it is solved by heuristic algorithms and provable algorithms seemed hard to find. Recently, provable algorithms were found that work if the unknown feature vector $x$ is $\sqrt{n}$-sparse or even sparser. Spielman et al. \cite{DBLP:journals/jmlr/SpielmanWW12} did this for dictionaries where $m=n$; Arora et al. \cite{AGM} gave an algorithm for overcomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al. \cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker guarantees. This raised the problem of designing provable algorithms that allow sparsity $\gg \sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms that allow sparsity up to $n/poly(\log n)$. It works for a class of matrices where features are individually recoverable, a new notion identified in this paper that may motivate further work. The algorithm runs in quasipolynomial time because they use limited enumeration. version:1
arxiv-1305-1019 | Simple Deep Random Model Ensemble | http://arxiv.org/abs/1305.1019 | id:1305.1019 author:Xiao-Lei Zhang, Ji Wu category:cs.LG  published:2013-05-05 summary:Representation learning and unsupervised learning are two central topics of machine learning and signal processing. Deep learning is one of the most effective unsupervised representation learning approach. The main contributions of this paper to the topics are as follows. (i) We propose to view the representative deep learning approaches as special cases of the knowledge reuse framework of clustering ensemble. (ii) We propose to view sparse coding when used as a feature encoder as the consensus function of clustering ensemble, and view dictionary learning as the training process of the base clusterings of clustering ensemble. (ii) Based on the above two views, we propose a very simple deep learning algorithm, named deep random model ensemble (DRME). It is a stack of random model ensembles. Each random model ensemble is a special k-means ensemble that discards the expectation-maximization optimization of each base k-means but only preserves the default initialization method of the base k-means. (iv) We propose to select the most powerful representation among the layers by applying DRME to clustering where the single-linkage is used as the clustering algorithm. Moreover, the DRME based clustering can also detect the number of the natural clusters accurately. Extensive experimental comparisons with 5 representation learning methods on 19 benchmark data sets demonstrate the effectiveness of DRME. version:2
arxiv-1308-4922 | Learning Deep Representation Without Parameter Inference for Nonlinear Dimensionality Reduction | http://arxiv.org/abs/1308.4922 | id:1308.4922 author:Xiao-Lei Zhang category:cs.LG stat.ML  published:2013-08-22 summary:Unsupervised deep learning is one of the most powerful representation learning techniques. Restricted Boltzman machine, sparse coding, regularized auto-encoders, and convolutional neural networks are pioneering building blocks of deep learning. In this paper, we propose a new building block -- distributed random models. The proposed method is a special full implementation of the product of experts: (i) each expert owns multiple hidden units and different experts have different numbers of hidden units; (ii) the model of each expert is a k-center clustering, whose k-centers are only uniformly sampled examples, and whose output (i.e. the hidden units) is a sparse code that only the similarity values from a few nearest neighbors are reserved. The relationship between the pioneering building blocks, several notable research branches and the proposed method is analyzed. Experimental results show that the proposed deep model can learn better representations than deep belief networks and meanwhile can train a much larger network with much less time than deep belief networks. version:2
arxiv-1401-0546 | Low-Complexity Particle Swarm Optimization for Time-Critical Applications | http://arxiv.org/abs/1401.0546 | id:1401.0546 author:Muhammad Saqib Sohail, Muhammad Omer Bin Saeed, Syed Zeeshan Rizvi, Mobien Shoaib, Asrar Ul Haq Sheikh category:cs.NE  published:2014-01-02 summary:Particle swam optimization (PSO) is a popular stochastic optimization method that has found wide applications in diverse fields. However, PSO suffers from high computational complexity and slow convergence speed. High computational complexity hinders its use in applications that have limited power resources while slow convergence speed makes it unsuitable for time critical applications. In this paper, we propose two techniques to overcome these limitations. The first technique reduces the computational complexity of PSO while the second technique speeds up its convergence. These techniques can be applied, either separately or in conjunction, to any existing PSO variant. The proposed techniques are robust to the number of dimensions of the optimization problem. Simulation results are presented for the proposed techniques applied to the standard PSO as well as to several PSO variants. The results show that the use of both these techniques in conjunction results in a reduction in the number of computations required as well as faster convergence speed while maintaining an acceptable error performance for time-critical applications. version:1
arxiv-1401-0523 | Solving Poisson Equation by Genetic Algorithms | http://arxiv.org/abs/1401.0523 | id:1401.0523 author:Khalid Jebari, Mohammed Madiafi, Abdelaziz El Moujahid category:cs.NE  published:2014-01-02 summary:This paper deals with a method for solving Poisson Equation (PE) based on genetic algorithms and grammatical evolution. The method forms generations of solutions expressed in an analytical form. Several examples of PE are tested and in most cases the exact solution is recovered. But, when the solution cannot be expressed in an analytical form, our method produces a satisfactory solution with a good level of accuracy version:1
arxiv-1401-0486 | A Hybrid NN/HMM Modeling Technique for Online Arabic Handwriting Recognition | http://arxiv.org/abs/1401.0486 | id:1401.0486 author:Najiba Tagougui, Houcine Boubaker, Monji Kherallah, Adel M. ALIMI category:cs.CV  published:2014-01-02 summary:In this work we propose a hybrid NN/HMM model for online Arabic handwriting recognition. The proposed system is based on Hidden Markov Models (HMMs) and Multi Layer Perceptron Neural Networks (MLPNNs). The input signal is segmented to continuous strokes called segments based on the Beta-Elliptical strategy by inspecting the extremum points of the curvilinear velocity profile. A neural network trained with segment level contextual information is used to extract class character probabilities. The output of this network is decoded by HMMs to provide character level recognition. In evaluations on the ADAB database, we achieved 96.4% character recognition accuracy that is statistically significantly important in comparison with character recognition accuracies obtained from state-of-the-art online Arabic systems.8 version:1
arxiv-1312-6157 | Distinction between features extracted using deep belief networks | http://arxiv.org/abs/1312.6157 | id:1312.6157 author:Mohammad Pezeshki, Sajjad Gholami, Ahmad Nickabadi category:cs.LG cs.NE  published:2013-12-20 summary:Data representation is an important pre-processing step in many machine learning algorithms. There are a number of methods used for this task such as Deep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some of the features extracted using automated feature extraction methods may not always be related to a specific machine learning task, in this paper we propose two methods in order to make a distinction between extracted features based on their relevancy to the task. We applied these two methods to a Deep Belief Network trained for a face recognition task. version:2
arxiv-1312-6158 | Deep Belief Networks for Image Denoising | http://arxiv.org/abs/1312.6158 | id:1312.6158 author:Mohammad Ali Keyvanrad, Mohammad Pezeshki, Mohammad Ali Homayounpour category:cs.LG cs.CV cs.NE  published:2013-12-20 summary:Deep Belief Networks which are hierarchical generative models are effective tools for feature representation and extraction. Furthermore, DBNs can be used in numerous aspects of Machine Learning such as image denoising. In this paper, we propose a novel method for image denoising which relies on the DBNs' ability in feature representation. This work is based upon learning of the noise behavior. Generally, features which are extracted using DBNs are presented as the values of the last layer nodes. We train a DBN a way that the network totally distinguishes between nodes presenting noise and nodes presenting image content in the last later of DBN, i.e. the nodes in the last layer of trained DBN are divided into two distinct groups of nodes. After detecting the nodes which are presenting the noise, we are able to make the noise nodes inactive and reconstruct a noiseless image. In section 4 we explore the results of applying this method on the MNIST dataset of handwritten digits which is corrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in average mean square error (MSE) was achieved when the proposed method was used for the reconstruction of the noisy images. version:2
arxiv-1312-6197 | An empirical analysis of dropout in piecewise linear networks | http://arxiv.org/abs/1312.6197 | id:1312.6197 author:David Warde-Farley, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio category:stat.ML cs.LG cs.NE  published:2013-12-21 summary:The recently introduced dropout training criterion for neural networks has been the subject of much attention due to its simplicity and remarkable effectiveness as a regularizer, as well as its interpretation as a training procedure for an exponentially large ensemble of networks that share parameters. In this work we empirically investigate several questions related to the efficacy of dropout, specifically as it concerns networks employing the popular rectified linear activation function. We investigate the quality of the test time weight-scaling inference procedure by evaluating the geometric average exactly in small models, as well as compare the performance of the geometric mean to the arithmetic mean more commonly employed by ensemble techniques. We explore the effect of tied weights on the ensemble interpretation by training ensembles of masked networks without tied weights. Finally, we investigate an alternative criterion based on a biased estimator of the maximum likelihood ensemble gradient. version:2
arxiv-1401-5808 | Reducing the Computational Cost in Multi-objective Evolutionary Algorithms by Filtering Worthless Individuals | http://arxiv.org/abs/1401.5808 | id:1401.5808 author:Zahra Pourbahman, Ali Hamzeh category:cs.NE  published:2014-01-02 summary:The large number of exact fitness function evaluations makes evolutionary algorithms to have computational cost. In some real-world problems, reducing number of these evaluations is much more valuable even by increasing computational complexity and spending more time. To fulfill this target, we introduce an effective factor, in spite of applied factor in Adaptive Fuzzy Fitness Granulation with Non-dominated Sorting Genetic Algorithm-II, to filter out worthless individuals more precisely. Our proposed approach is compared with respect to Adaptive Fuzzy Fitness Granulation with Non-dominated Sorting Genetic Algorithm-II, using the Hyper volume and the Inverted Generational Distance performance measures. The proposed method is applied to 1 traditional and 1 state-of-the-art benchmarks with considering 3 different dimensions. From an average performance view, the results indicate that although decreasing the number of fitness evaluations leads to have performance reduction but it is not tangible compared to what we gain. version:1
arxiv-1401-0395 | Hybrid Approach to Face Recognition System using Principle component and Independent component with score based fusion process | http://arxiv.org/abs/1401.0395 | id:1401.0395 author:Trupti M. Kodinariya category:cs.CV  published:2014-01-02 summary:Hybrid approach has a special status among Face Recognition Systems as they combine different recognition approaches in an either serial or parallel to overcome the shortcomings of individual methods. This paper explores the area of Hybrid Face Recognition using score based strategy as a combiner/fusion process. In proposed approach, the recognition system operates in two modes: training and classification. Training mode involves normalization of the face images (training set), extracting appropriate features using Principle Component Analysis (PCA) and Independent Component Analysis (ICA). The extracted features are then trained in parallel using Back-propagation neural networks (BPNNs) to partition the feature space in to different face classes. In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face image(s). The score based strategy which works as a combiner is applied to the results of both PCA BPNN and ICA BPNN to classify given new face image(s) according to face classes obtained during the training mode. The proposed approach has been tested on ORL and other face databases; the experimented results show that the proposed system has higher accuracy than face recognition systems using single feature extractor. version:1
arxiv-1312-6205 | Relaxations for inference in restricted Boltzmann machines | http://arxiv.org/abs/1312.6205 | id:1312.6205 author:Sida I. Wang, Roy Frostig, Percy Liang, Christopher D. Manning category:stat.ML cs.LG  published:2013-12-21 summary:We propose a relaxation-based approximate inference algorithm that samples near-MAP configurations of a binary pairwise Markov random field. We experiment on MAP inference tasks in several restricted Boltzmann machines. We also use our underlying sampler to estimate the log-partition function of restricted Boltzmann machines and compare against other sampling-based methods. version:2
arxiv-1401-0376 | Generalization Bounds for Representative Domain Adaptation | http://arxiv.org/abs/1401.0376 | id:1401.0376 author:Chao Zhang, Lei Zhang, Wei Fan, Jieping Ye category:cs.LG stat.ML  published:2014-01-02 summary:In this paper, we propose a novel framework to analyze the theoretical properties of the learning process for a representative type of domain adaptation, which combines data from multiple sources and one target (or briefly called representative domain adaptation). In particular, we use the integral probability metric to measure the difference between the distributions of two domains and meanwhile compare it with the H-divergence and the discrepancy distance. We develop the Hoeffding-type, the Bennett-type and the McDiarmid-type deviation inequalities for multiple domains respectively, and then present the symmetrization inequality for representative domain adaptation. Next, we use the derived inequalities to obtain the Hoeffding-type and the Bennett-type generalization bounds respectively, both of which are based on the uniform entropy number. Moreover, we present the generalization bounds based on the Rademacher complexity. Finally, we analyze the asymptotic convergence and the rate of convergence of the learning process for representative domain adaptation. We discuss the factors that affect the asymptotic behavior of the learning process and the numerical experiments support our theoretical findings as well. Meanwhile, we give a comparison with the existing results of domain adaptation and the classical results under the same-distribution assumption. version:1
arxiv-1401-0334 | Convex optimization on Banach Spaces | http://arxiv.org/abs/1401.0334 | id:1401.0334 author:R. A. DeVore, V. N. Temlyakov category:stat.ML math.OC  published:2014-01-01 summary:Greedy algorithms which use only function evaluations are applied to convex optimization in a general Banach space $X$. Along with algorithms that use exact evaluations, algorithms with approximate evaluations are treated. A priori upper bounds for the convergence rate of the proposed algorithms are given. These bounds depend on the smoothness of the objective function and the sparsity or compressibility (with respect to a given dictionary) of a point in $X$ where the minimum is attained. version:1
arxiv-1306-5884 | Design of an Agent for Answering Back in Smart Phones | http://arxiv.org/abs/1306.5884 | id:1306.5884 author:Sandeep Venkatesh, Meera V Patil, Nanditha Swamy category:cs.AI cs.HC cs.LG  published:2013-06-25 summary:The objective of the paper is to design an agent which provides efficient response to the caller when a call goes unanswered in smartphones. The agent provides responses through text messages, email etc stating the most likely reason as to why the callee is unable to answer a call. Responses are composed taking into consideration the importance of the present call and the situation the callee is in at the moment like driving, sleeping, at work etc. The agent makes decisons in the compostion of response messages based on the patterns it has come across in the learning environment. Initially the user helps the agent to compose response messages. The agent associates this message to the percept it recieves with respect to the environment the callee is in. The user may thereafter either choose to make to response system automatic or choose to recieve suggestions from the agent for responses messages and confirm what is to be sent to the caller. version:2
arxiv-1304-6803 | Direct Learning of Sparse Changes in Markov Networks by Density Ratio Estimation | http://arxiv.org/abs/1304.6803 | id:1304.6803 author:Song Liu, John A. Quinn, Michael U. Gutmann, Taiji Suzuki, Masashi Sugiyama category:stat.ML  published:2013-04-25 summary:We propose a new method for detecting changes in Markov network structure between two sets of samples. Instead of naively fitting two Markov network models separately to the two data sets and figuring out their difference, we \emph{directly} learn the network structure change by estimating the ratio of Markov network models. This density-ratio formulation naturally allows us to introduce sparsity in the network structure change, which highly contributes to enhancing interpretability. Furthermore, computation of the normalization term, which is a critical bottleneck of the naive approach, can be remarkably mitigated. We also give the dual formulation of the optimization problem, which further reduces the computation cost for large-scale Markov networks. Through experiments, we demonstrate the usefulness of our method. version:5
arxiv-1401-0255 | Modeling Attractiveness and Multiple Clicks in Sponsored Search Results | http://arxiv.org/abs/1401.0255 | id:1401.0255 author:Dinesh Govindaraj, Tao Wang, S. V. N. Vishwanathan category:cs.IR cs.LG  published:2014-01-01 summary:Click models are an important tool for leveraging user feedback, and are used by commercial search engines for surfacing relevant search results. However, existing click models are lacking in two aspects. First, they do not share information across search results when computing attractiveness. Second, they assume that users interact with the search results sequentially. Based on our analysis of the click logs of a commercial search engine, we observe that the sequential scan assumption does not always hold, especially for sponsored search results. To overcome the above two limitations, we propose a new click model. Our key insight is that sharing information across search results helps in identifying important words or key-phrases which can then be used to accurately compute attractiveness of a search result. Furthermore, we argue that the click probability of a position as well as its attractiveness changes during a user session and depends on the user's past click experience. Our model seamlessly incorporates the effect of externalities (quality of other search results displayed in response to a user query), user fatigue, as well as pre and post-click relevance of a sponsored search result. We propose an efficient one-pass inference scheme and empirically evaluate the performance of our model via extensive experiments using the click logs of a large commercial search engine. version:1
arxiv-1312-7869 | Consistent Bounded-Asynchronous Parameter Servers for Distributed ML | http://arxiv.org/abs/1312.7869 | id:1312.7869 author:Jinliang Wei, Wei Dai, Abhimanu Kumar, Xun Zheng, Qirong Ho, Eric P. Xing category:stat.ML cs.DC cs.LG  published:2013-12-30 summary:In distributed ML applications, shared parameters are usually replicated among computing nodes to minimize network overhead. Therefore, proper consistency model must be carefully chosen to ensure algorithm's correctness and provide high throughput. Existing consistency models used in general-purpose databases and modern distributed ML systems are either too loose to guarantee correctness of the ML algorithms or too strict and thus fail to fully exploit the computing power of the underlying distributed system. Many ML algorithms fall into the category of \emph{iterative convergent algorithms} which start from a randomly chosen initial point and converge to optima by repeating iteratively a set of procedures. We've found that many such algorithms are to a bounded amount of inconsistency and still converge correctly. This property allows distributed ML to relax strict consistency models to improve system performance while theoretically guarantees algorithmic correctness. In this paper, we present several relaxed consistency models for asynchronous parallel computation and theoretically prove their algorithmic correctness. The proposed consistency models are implemented in a distributed parameter server and evaluated in the context of a popular ML application: topic modeling. version:2
arxiv-1401-0201 | Sparse Recovery with Very Sparse Compressed Counting | http://arxiv.org/abs/1401.0201 | id:1401.0201 author:Ping Li, Cun-Hui Zhang, Tong Zhang category:stat.ME cs.DS cs.IT cs.LG math.IT  published:2013-12-31 summary:Compressed sensing (sparse signal recovery) often encounters nonnegative data (e.g., images). Recently we developed the methodology of using (dense) Compressed Counting for recovering nonnegative K-sparse signals. In this paper, we adopt very sparse Compressed Counting for nonnegative signal recovery. Our design matrix is sampled from a maximally-skewed p-stable distribution (0<p<1), and we sparsify the design matrix so that on average (1-g)-fraction of the entries become zero. The idea is related to very sparse stable random projections (Li et al 2006 and Li 2007), the prior work for estimating summary statistics of the data. In our theoretical analysis, we show that, when p->0, it suffices to use M= K/(1-exp(-gK) log N measurements, so that all coordinates can be recovered in one scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N. If g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16K log N. This means the design matrix can be indeed very sparse at only a minor inflation of the sample complexity. Interestingly, as p->1, the required number of measurements is essentially M = 2.7K log N, provided g= 1/K. It turns out that this result is a general worst-case bound. version:1
arxiv-1212-2036 | Query-focused Multi-document Summarization: Combining a Novel Topic Model with Graph-based Semi-supervised Learning | http://arxiv.org/abs/1212.2036 | id:1212.2036 author:Jiwei Li, Sujian Li category:cs.CL cs.IR  published:2012-12-10 summary:Graph-based semi-supervised learning has proven to be an effective approach for query-focused multi-document summarization. The problem of previous semi-supervised learning is that sentences are ranked without considering the higher level information beyond sentence level. Researches on general summarization illustrated that the addition of topic level can effectively improve the summary quality. Inspired by previous researches, we propose a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach. At the same time, we propose a novel topic model which makes full use of the dependence between sentences and words. Experimental results on DUC and TAC data sets demonstrate the effectiveness of our proposed approach. version:3
arxiv-1302-4245 | Gaussian Process Kernels for Pattern Discovery and Extrapolation | http://arxiv.org/abs/1302.4245 | id:1302.4245 author:Andrew Gordon Wilson, Ryan Prescott Adams category:stat.ML cs.AI stat.ME  published:2013-02-18 summary:Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework. version:3
arxiv-1401-0166 | Medical Image Fusion: A survey of the state of the art | http://arxiv.org/abs/1401.0166 | id:1401.0166 author:A. P. James, B. V. Dasarathy category:cs.CV cs.AI physics.med-ph  published:2013-12-31 summary:Medical image fusion is the process of registering and combining multiple images from single or multiple imaging modalities to improve the imaging quality and reduce randomness and redundancy in order to increase the clinical applicability of medical images for diagnosis and assessment of medical problems. Multi-modal medical image fusion algorithms and devices have shown notable achievements in improving clinical accuracy of decisions based on medical images. This review article provides a factual listing of methods and summarizes the broad scientific challenges faced in the field of medical image fusion. We characterize the medical image fusion research based on (1) the widely used image fusion methods, (2) imaging modalities, and (3) imaging of organs that are under study. This review concludes that even though there exists several open ended technological and scientific challenges, the fusion of medical images has proved to be useful for advancing the clinical reliability of using medical imaging for medical diagnostics and analysis, and is a scientific discipline that has the potential to significantly grow in the coming years. version:1
arxiv-1401-0159 | Speeding-Up Convergence via Sequential Subspace Optimization: Current State and Future Directions | http://arxiv.org/abs/1401.0159 | id:1401.0159 author:Michael Zibulevsky category:cs.NA cs.LG  published:2013-12-31 summary:This is an overview paper written in style of research proposal. In recent years we introduced a general framework for large-scale unconstrained optimization -- Sequential Subspace Optimization (SESOP) and demonstrated its usefulness for sparsity-based signal/image denoising, deconvolution, compressive sensing, computed tomography, diffraction imaging, support vector machines. We explored its combination with Parallel Coordinate Descent and Separable Surrogate Function methods, obtaining state of the art results in above-mentioned areas. There are several methods, that are faster than plain SESOP under specific conditions: Trust region Newton method - for problems with easily invertible Hessian matrix; Truncated Newton method - when fast multiplication by Hessian is available; Stochastic optimization methods - for problems with large stochastic-type data; Multigrid methods - for problems with nested multilevel structure. Each of these methods can be further improved by merge with SESOP. One can also accelerate Augmented Lagrangian method for constrained optimization problems and Alternating Direction Method of Multipliers for problems with separable objective function and non-separable constraints. version:1
arxiv-1310-5288 | GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian Processes | http://arxiv.org/abs/1310.5288 | id:1310.5288 author:Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, John P. Cunningham category:stat.ML cs.AI cs.LG stat.ME  published:2013-10-20 summary:Gaussian processes are typically used for smoothing and interpolation on small datasets. We introduce a new Bayesian nonparametric framework -- GPatt -- enabling automatic pattern extrapolation with Gaussian processes on large multidimensional datasets. GPatt unifies and extends highly expressive kernels and fast exact inference techniques. Without human intervention -- no hand crafting of kernel features, and no sophisticated initialisation procedures -- we show that GPatt can solve large scale pattern extrapolation, inpainting, and kernel discovery problems, including a problem with 383400 training points. We find that GPatt significantly outperforms popular alternative scalable Gaussian process methods in speed and accuracy. Moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact inference are useful for modelling large scale multidimensional patterns. version:3
arxiv-1310-0423 | Inference of Network Summary Statistics Through Network Denoising | http://arxiv.org/abs/1310.0423 | id:1310.0423 author:Prakash Balachandran, Edoardo Airoldi, Eric Kolaczyk category:stat.ML math.SP  published:2013-10-01 summary:Consider observing an undirected network that is `noisy' in the sense that there are Type I and Type II errors in the observation of edges. Such errors can arise, for example, in the context of inferring gene regulatory networks in genomics or functional connectivity networks in neuroscience. Given a single observed network then, to what extent are summary statistics for that network representative of their analogues for the true underlying network? Can we infer such statistics more accurately by taking into account the noise in the observed network edges? In this paper, we answer both of these questions. In particular, we develop a spectral-based methodology using the adjacency matrix to `denoise' the observed network data and produce more accurate inference of the summary statistics of the true network. We characterize performance of our methodology through bounds on appropriate notions of risk in the $L^2$ sense, and conclude by illustrating the practical impact of this work on synthetic and real-world data. version:3
arxiv-1401-0131 | System Analysis And Design For Multimedia Retrieval Systems | http://arxiv.org/abs/1401.0131 | id:1401.0131 author:Avinash N Bhute, B B Meshram category:cs.IR cs.CV cs.MM  published:2013-12-31 summary:Due to the extensive use of information technology and the recent developments in multimedia systems, the amount of multimedia data available to users has increased exponentially. Video is an example of multimedia data as it contains several kinds of data such as text, image, meta-data, visual and audio. Content based video retrieval is an approach for facilitating the searching and browsing of large multimedia collections over WWW. In order to create an effective video retrieval system, visual perception must be taken into account. We conjectured that a technique which employs multiple features for indexing and retrieval would be more effective in the discrimination and search tasks of videos. In order to validate this, content based indexing and retrieval systems were implemented using color histogram, Texture feature (GLCM), edge density and motion.. version:1
arxiv-1401-0118 | Black Box Variational Inference | http://arxiv.org/abs/1401.0118 | id:1401.0118 author:Rajesh Ranganath, Sean Gerrish, David M. Blei category:stat.ML cs.LG stat.CO stat.ME  published:2013-12-31 summary:Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data. version:1
arxiv-1401-0116 | Controlled Sparsity Kernel Learning | http://arxiv.org/abs/1401.0116 | id:1401.0116 author:Dinesh Govindaraj, Raman Sankaran, Sreedal Menon, Chiranjib Bhattacharyya category:cs.LG  published:2013-12-31 summary:Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a popular front of research in recent times due to its success in application problems like Object Categorization. This success is due to the fact that MKL has the ability to choose from a variety of feature kernels to identify the optimal kernel combination. But the initial formulation of MKL was only able to select the best of the features and misses out many other informative kernels presented. To overcome this, the Lp norm based formulation was proposed by Kloft et. al. This formulation is capable of choosing a non-sparse set of kernels through a control parameter p. Unfortunately, the parameter p does not have a direct meaning to the number of kernels selected. We have observed that stricter control over the number of kernels selected gives us an edge over these techniques in terms of accuracy of classification and also helps us to fine tune the algorithms to the time requirements at hand. In this work, we propose a Controlled Sparsity Kernel Learning (CSKL) formulation that can strictly control the number of kernels which we wish to select. The CSKL formulation introduces a parameter t which directly corresponds to the number of kernels selected. It is important to note that a search in t space is finite and fast as compared to p. We have also provided an efficient Reduced Gradient Descent based algorithm to solve the CSKL formulation, which is proven to converge. Through our experiments on the Caltech101 Object Categorization dataset, we have also shown that one can achieve better accuracies than the previous formulations through the right choice of t. version:1
arxiv-1401-0104 | PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction | http://arxiv.org/abs/1401.0104 | id:1401.0104 author:Yukun Bao, Tao Xiong, Zhongyi Hu category:cs.AI cs.LG cs.NE stat.ML  published:2013-12-31 summary:Multi-step-ahead time series prediction is one of the most challenging research topics in the field of time series modeling and prediction, and is continually under research. Recently, the multiple-input several multiple-outputs (MISMO) modeling strategy has been proposed as a promising alternative for multi-step-ahead time series prediction, exhibiting advantages compared with the two currently dominating strategies, the iterated and the direct strategies. Built on the established MISMO strategy, this study proposes a particle swarm optimization (PSO)-based MISMO modeling strategy, which is capable of determining the number of sub-models in a self-adaptive mode, with varying prediction horizons. Rather than deriving crisp divides with equal-size s prediction horizons from the established MISMO, the proposed PSO-MISMO strategy, implemented with neural networks, employs a heuristic to create flexible divides with varying sizes of prediction horizons and to generate corresponding sub-models, providing considerable flexibility in model construction, which has been validated with simulated and real datasets. version:1
arxiv-1401-0092 | A Novel Approach For Generating Face Template Using Bda | http://arxiv.org/abs/1401.0092 | id:1401.0092 author:Shraddha S. Shinde, Prof. Anagha P. Khedkar category:cs.CV  published:2013-12-31 summary:In identity management system, commonly used biometric recognition system needs attention towards issue of biometric template protection as far as more reliable solution is concerned. In view of this biometric template protection algorithm should satisfy security, discriminability and cancelability. As no single template protection method is capable of satisfying the basic requirements, a novel technique for face template generation and protection is proposed. The novel approach is proposed to provide security and accuracy in new user enrollment as well as authentication process. This novel technique takes advantage of both the hybrid approach and the binary discriminant analysis algorithm. This algorithm is designed on the basis of random projection, binary discriminant analysis and fuzzy commitment scheme. Three publicly available benchmark face databases are used for evaluation. The proposed novel technique enhances the discriminability and recognition accuracy by 80% in terms of matching score of the face images and provides high security. version:1
arxiv-1209-2139 | Fused Multiple Graphical Lasso | http://arxiv.org/abs/1209.2139 | id:1209.2139 author:Sen Yang, Zhaosong Lu, Xiaotong Shen, Peter Wonka, Jieping Ye category:cs.LG stat.ML  published:2012-09-10 summary:In this paper, we consider the problem of estimating multiple graphical models simultaneously using the fused lasso penalty, which encourages adjacent graphs to share similar structures. A motivating example is the analysis of brain networks of Alzheimer's disease using neuroimaging data. Specifically, we may wish to estimate a brain network for the normal controls (NC), a brain network for the patients with mild cognitive impairment (MCI), and a brain network for Alzheimer's patients (AD). We expect the two brain networks for NC and MCI to share common structures but not to be identical to each other; similarly for the two brain networks for MCI and AD. The proposed formulation can be solved using a second-order method. Our key technical contribution is to establish the necessary and sufficient condition for the graphs to be decomposable. Based on this key property, a simple screening rule is presented, which decomposes the large graphs into small subgraphs and allows an efficient estimation of multiple independent (small) subgraphs, dramatically reducing the computational cost. We perform experiments on both synthetic and real data; our results demonstrate the effectiveness and efficiency of the proposed approach. version:2
arxiv-1401-0044 | Approximating the Bethe partition function | http://arxiv.org/abs/1401.0044 | id:1401.0044 author:Adrian Weller, Tony Jebara category:cs.LG  published:2013-12-30 summary:When belief propagation (BP) converges, it does so to a stationary point of the Bethe free energy $F$, and is often strikingly accurate. However, it may converge only to a local optimum or may not converge at all. An algorithm was recently introduced for attractive binary pairwise MRFs which is guaranteed to return an $\epsilon$-approximation to the global minimum of $F$ in polynomial time provided the maximum degree $\Delta=O(\log n)$, where $n$ is the number of variables. Here we significantly improve this algorithm and derive several results including a new approach based on analyzing first derivatives of $F$, which leads to performance that is typically far superior and yields a fully polynomial-time approximation scheme (FPTAS) for attractive models without any degree restriction. Further, the method applies to general (non-attractive) models, though with no polynomial time guarantee in this case, leading to the important result that approximating $\log$ of the Bethe partition function, $\log Z_B=-\min F$, for a general model to additive $\epsilon$-accuracy may be reduced to a discrete MAP inference problem. We explore an application to predicting equipment failure on an urban power network and demonstrate that the Bethe approximation can perform well even when BP fails to converge. version:1
arxiv-1312-7852 | Evolutionary Design of Numerical Methods: Generating Finite Difference and Integration Schemes by Differential Evolution | http://arxiv.org/abs/1312.7852 | id:1312.7852 author:C. D. Erdbrink, V. V. Krzhizhanovskaya, P. M. A. Sloot category:cs.NE cs.NA  published:2013-12-30 summary:Classical and new numerical schemes are generated using evolutionary computing. Differential Evolution is used to find the coefficients of finite difference approximations of function derivatives, and of single and multi-step integration methods. The coefficients are reverse engineered based on samples from a target function and its derivative used for training. The Runge-Kutta schemes are trained using the order condition equations. An appealing feature of the evolutionary method is the low number of model parameters. The population size, termination criterion and number of training points are determined in a sensitivity analysis. Computational results show good agreement between evolved and analytical coefficients. In particular, a new fifth-order Runge-Kutta scheme is computed which adheres to the order conditions with a sum of absolute errors of order 10^-14. Execution of the evolved schemes proved the intended orders of accuracy. The outcome of this study is valuable for future developments in the design of complex numerical methods that are out of reach by conventional means. version:1
arxiv-1312-7750 | A Fused Elastic Net Logistic Regression Model for Multi-Task Binary Classification | http://arxiv.org/abs/1312.7750 | id:1312.7750 author:Venelin Mitov, Manfred Claassen category:stat.ML  published:2013-12-30 summary:Multi-task learning has shown to significantly enhance the performance of multiple related learning tasks in a variety of situations. We present the fused logistic regression, a sparse multi-task learning approach for binary classification. Specifically, we introduce sparsity inducing penalties over parameter differences of related logistic regression models to encode similarity across related tasks. The resulting joint learning task is cast into a form that lends itself to be efficiently optimized with a recursive variant of the alternating direction method of multipliers. We show results on synthetic data and describe the regime of settings where our multi-task approach achieves significant improvements over the single task learning approach and discuss the implications on applying the fused logistic regression in different real world settings. version:1
arxiv-1312-7710 | Total variation regularization for manifold-valued data | http://arxiv.org/abs/1312.7710 | id:1312.7710 author:Andreas Weinmann, Laurent Demaret, Martin Storath category:math.OC cs.CV physics.med-ph  published:2013-12-30 summary:We consider total variation minimization for manifold valued data. We propose a cyclic proximal point algorithm and a parallel proximal point algorithm to minimize TV functionals with $\ell^p$-type data terms in the manifold case. These algorithms are based on iterative geodesic averaging which makes them easily applicable to a large class of data manifolds. As an application, we consider denoising images which take their values in a manifold. We apply our algorithms to diffusion tensor images, interferometric SAR images as well as sphere and cylinder valued images. For the class of Cartan-Hadamard manifolds (which includes the data space in diffusion tensor imaging) we show the convergence of the proposed TV minimizing algorithms to a global minimizer. version:1
arxiv-1212-3530 | A Multi-Orientation Analysis Approach to Retinal Vessel Tracking | http://arxiv.org/abs/1212.3530 | id:1212.3530 author:Erik Bekkers, Remco Duits, Tos Berendschot, Bart ter Haar Romeny category:cs.CV  published:2012-12-14 summary:This paper presents a method for retinal vasculature extraction based on biologically inspired multi-orientation analysis. We apply multi-orientation analysis via so-called invertible orientation scores, modeling the cortical columns in the visual system of higher mammals. This allows us to generically deal with many hitherto complex problems inherent to vessel tracking, such as crossings, bifurcations, parallel vessels, vessels of varying widths and vessels with high curvature. Our approach applies tracking in invertible orientation scores via a novel geometrical principle for curve optimization in the Euclidean motion group SE(2). The method runs fully automatically and provides a detailed model of the retinal vasculature, which is crucial as a sound basis for further quantitative analysis of the retina, especially in screening applications. version:5
arxiv-1212-4569 | Feature vector regularization in machine learning | http://arxiv.org/abs/1212.4569 | id:1212.4569 author:Yue Fan, Louise Raphael, Mark Kon category:stat.ML  published:2012-12-19 summary:Problems in machine learning (ML) can involve noisy input data, and ML classification methods have reached limiting accuracies when based on standard ML data sets consisting of feature vectors and their classes. Greater accuracy will require incorporation of prior structural information on data into learning. We study methods to regularize feature vectors (unsupervised regularization methods), analogous to supervised regularization for estimating functions in ML. We study regularization (denoising) of ML feature vectors using Tikhonov and other regularization methods for functions on ${\bf R}^n$. A feature vector ${\bf x}=(x_1,\ldots,x_n)=\{x_q\}_{q=1}^n$ is viewed as a function of its index $q$, and smoothed using prior information on its structure. This can involve a penalty functional on feature vectors analogous to those in statistical learning, or use of proximity (e.g. graph) structure on the set of indices. Such feature vector regularization inherits a property from function denoising on ${\bf R}^n$, in that accuracy is non-monotonic in the denoising (regularization) parameter $\alpha$. Under some assumptions about the noise level and the data structure, we show that the best reconstruction accuracy also occurs at a finite positive $\alpha$ in index spaces with graph structures. We adapt two standard function denoising methods used on ${\bf R}^n$, local averaging and kernel regression. In general the index space can be any discrete set with a notion of proximity, e.g. a metric space, a subset of ${\bf R}^n$, or a graph/network, with feature vectors as functions with some notion of continuity. We show this improves feature vector recovery, and thus the subsequent classification or regression done on them. We give an example in gene expression analysis for cancer classification with the genome as an index space and network structure based protein-protein interactions. version:2
arxiv-1312-7658 | Response-Based Approachability and its Application to Generalized No-Regret Algorithms | http://arxiv.org/abs/1312.7658 | id:1312.7658 author:Andrey Bernstein, Nahum Shimkin category:cs.LG cs.GT  published:2013-12-30 summary:Approachability theory, introduced by Blackwell (1956), provides fundamental results on repeated games with vector-valued payoffs, and has been usefully applied since in the theory of learning in games and to learning algorithms in the online adversarial setup. Given a repeated game with vector payoffs, a target set $S$ is approachable by a certain player (the agent) if he can ensure that the average payoff vector converges to that set no matter what his adversary opponent does. Blackwell provided two equivalent sets of conditions for a convex set to be approachable. The first (primary) condition is a geometric separation condition, while the second (dual) condition requires that the set be {\em non-excludable}, namely that for every mixed action of the opponent there exists a mixed action of the agent (a {\em response}) such that the resulting payoff vector belongs to $S$. Existing approachability algorithms rely on the primal condition and essentially require to compute at each stage a projection direction from a given point to $S$. In this paper, we introduce an approachability algorithm that relies on Blackwell's {\em dual} condition. Thus, rather than projection, the algorithm relies on computation of the response to a certain action of the opponent at each stage. The utility of the proposed algorithm is demonstrated by applying it to certain generalizations of the classical regret minimization problem, which include regret minimization with side constraints and regret minimization for global cost functions. In these problems, computation of the required projections is generally complex but a response is readily obtainable. version:1
arxiv-1205-0651 | Generative Maximum Entropy Learning for Multiclass Classification | http://arxiv.org/abs/1205.0651 | id:1205.0651 author:Ambedkar Dukkipati, Gaurav Pandey, Debarghya Ghoshdastidar, Paramita Koley, D. M. V. Satya Sriram category:cs.IT cs.LG math.IT  published:2012-05-03 summary:Maximum entropy approach to classification is very well studied in applied statistics and machine learning and almost all the methods that exists in literature are discriminative in nature. In this paper, we introduce a maximum entropy classification method with feature selection for large dimensional data such as text datasets that is generative in nature. To tackle the curse of dimensionality of large data sets, we employ conditional independence assumption (Naive Bayes) and we perform feature selection simultaneously, by enforcing a `maximum discrimination' between estimated class conditional densities. For two class problems, in the proposed method, we use Jeffreys ($J$) divergence to discriminate the class conditional densities. To extend our method to the multi-class case, we propose a completely new approach by considering a multi-distribution divergence: we replace Jeffreys divergence by Jensen-Shannon ($JS$) divergence to discriminate conditional densities of multiple classes. In order to reduce computational complexity, we employ a modified Jensen-Shannon divergence ($JS_{GM}$), based on AM-GM inequality. We show that the resulting divergence is a natural generalization of Jeffreys divergence to a multiple distributions case. As far as the theoretical justifications are concerned we show that when one intends to select the best features in a generative maximum entropy approach, maximum discrimination using $J-$divergence emerges naturally in binary classification. Performance and comparative study of the proposed algorithms have been demonstrated on large dimensional text and gene expression datasets that show our methods scale up very well with large dimensional datasets. version:3
arxiv-1312-5766 | Structure-Aware Dynamic Scheduler for Parallel Machine Learning | http://arxiv.org/abs/1312.5766 | id:1312.5766 author:Seunghak Lee, Jin Kyu Kim, Qirong Ho, Garth A. Gibson, Eric P. Xing category:stat.ML cs.LG  published:2013-12-19 summary:Training large machine learning (ML) models with many variables or parameters can take a long time if one employs sequential procedures even with stochastic updates. A natural solution is to turn to distributed computing on a cluster; however, naive, unstructured parallelization of ML algorithms does not usually lead to a proportional speedup and can even result in divergence, because dependencies between model elements can attenuate the computational gains from parallelization and compromise correctness of inference. Recent efforts toward this issue have benefited from exploiting the static, a priori block structures residing in ML algorithms. In this paper, we take this path further by exploring the dynamic block structures and workloads therein present during ML program execution, which offers new opportunities for improving convergence, correctness, and load balancing in distributed ML. We propose and showcase a general-purpose scheduler, STRADS, for coordinating distributed updates in ML algorithms, which harnesses the aforementioned opportunities in a systematic way. We provide theoretical guarantees for our scheduler, and demonstrate its efficacy versus static block structures on Lasso and Matrix Factorization. version:2
arxiv-1308-3839 | Consensus Sequence Segmentation | http://arxiv.org/abs/1308.3839 | id:1308.3839 author:Tamal Chowdhury, Rabindra Rakshit, Arko Banerjee category:cs.CL 68T10  published:2013-08-18 summary:In this paper we introduce a method to detect words or phrases in a given sequence of alphabets without knowing the lexicon. Our linear time unsupervised algorithm relies entirely on statistical relationships among alphabets in the input sequence to detect location of word boundaries. We compare our algorithm to previous approaches from unsupervised sequence segmentation literature and provide superior segmentation over number of benchmarks. version:2
arxiv-1105-2894 | Ant Colony Optimization and Hypergraph Covering Problems | http://arxiv.org/abs/1105.2894 | id:1105.2894 author:Ankit Pat, Ashish Ranjan Hota category:cs.NE  published:2011-05-14 summary:Ant Colony Optimization (ACO) is a very popular metaheuristic for solving computationally hard combinatorial optimization problems. Runtime analysis of ACO with respect to various pseudo-boolean functions and different graph based combinatorial optimization problems has been taken up in recent years. In this paper, we investigate the runtime behavior of an MMAS*(Max-Min Ant System) ACO algorithm on some well known hypergraph covering problems that are NP-Hard. In particular, we have addressed the Minimum Edge Cover problem, the Minimum Vertex Cover problem and the Maximum Weak- Independent Set problem. The influence of pheromone values and heuristic information on the running time is analysed. The results indicate that the heuristic information has greater impact towards improving the expected optimization time as compared to pheromone values. For certain instances of hypergraphs, we show that the MMAS* algorithm gives a constant order expected optimization time when the dominance of heuristic information is suitably increased. version:2
arxiv-1312-7573 | A Novel Method for Automatic Segmentation of Brain Tumors in MRI Images | http://arxiv.org/abs/1312.7573 | id:1312.7573 author:Saeid Fazli, Parisa Nadirkhanlou category:cs.CV  published:2013-12-29 summary:The brain tumor segmentation on MRI images is a very difficult and important task which is used in surgical and medical planning and assessments. If experts do the segmentation manually with their own medical knowledge, it will be time-consuming. Therefore, researchers propose methods and systems which can do the segmentation automatically and without any interference. In this article, an unsupervised automatic method for brain tumor segmentation on MRI images is presented. In this method, at first in the pre-processing level, the extra parts which are outside the skull and don't have any helpful information are removed and then anisotropic diffusion filter with 8-connected neighborhood is applied to the MRI images to remove noise. By applying the fast bounding box(FBB) algorithm, the tumor area is displayed on the MRI image with a bounding box and the central part is selected as sample points for training of a One Class SVM classifier. A database is also provided by the Zanjan MRI Center. The MRI images are related to 10 patients who have brain tumor. 100 T2-weighted MRI images are used in this study. Experimental results show the high precision and dependability of the proposed algorithm. The results are also highly helpful for specialists and radiologists to easily estimate the size and position of a tumor. version:1
arxiv-1312-7570 | Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition | http://arxiv.org/abs/1312.7570 | id:1312.7570 author:Stefan Mathe, Cristian Sminchisescu category:cs.CV  published:2013-12-29 summary:Systems based on bag-of-words models from image features collected at maxima of sparse interest point operators have been used successfully for both computer visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in `saccade and fixate' regimes, the methodology and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large scale dynamic computer vision annotated datasets like Hollywood-2 and UCF Sports with human eye movements collected under the ecological constraints of the visual action recognition task. To our knowledge these are the first large human eye tracking datasets to be collected and made publicly available for video, vision.imar.ro/eyetracking (497,107 frames, each viewed by 16 subjects), unique in terms of their (a) large scale and computer vision relevance, (b) dynamic, video stimuli, (c) task control, as opposed to free-viewing. Second, we introduce novel sequential consistency and alignment measures, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the significant amount of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and the human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the advanced computer vision practice, can lead to state of the art results. version:1
arxiv-1312-7567 | Nonparametric Inference For Density Modes | http://arxiv.org/abs/1312.7567 | id:1312.7567 author:Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli, Larry Wasserman category:stat.ME cs.LG 62G07  published:2013-12-29 summary:We derive nonparametric confidence intervals for the eigenvalues of the Hessian at modes of a density estimate. This provides information about the strength and shape of modes and can also be used as a significance test. We use a data-splitting approach in which potential modes are identified using the first half of the data and inference is done with the second half of the data. To get valid confidence sets for the eigenvalues, we use a bootstrap based on an elementary-symmetric-polynomial (ESP) transformation. This leads to valid bootstrap confidence sets regardless of any multiplicities in the eigenvalues. We also suggest a new method for bandwidth selection, namely, choosing the bandwidth to maximize the number of significant modes. We show by example that this method works well. Even when the true distribution is singular, and hence does not have a density, (in which case cross validation chooses a zero bandwidth), our method chooses a reasonable bandwidth. version:1
arxiv-1312-7560 | Implementation of Hand Detection based Techniques for Human Computer Interaction | http://arxiv.org/abs/1312.7560 | id:1312.7560 author:Amiraj Dhawan, Vipul Honrao category:cs.CV cs.HC  published:2013-12-29 summary:The computer industry is developing at a fast pace. With this development almost all of the fields under computers have advanced in the past couple of decades. But the same technology is being used for human computer interaction that was used in 1970s. Even today the same type of keyboard and mouse is used for interacting with computer systems. With the recent boom in the mobile segment touchscreens have become popular for interaction with cell phones. But these touchscreens are rarely used on traditional systems. This paper tries to introduce methods for human computer interaction using the users hand which can be used both on traditional computer platforms as well as cell phones. The methods explain how the users detected hand can be used as input for applications and also explain applications that can take advantage of this type of interaction mechanism. version:1
arxiv-1312-7557 | A Novel Retinal Vessel Segmentation Based On Histogram Transformation Using 2-D Morlet Wavelet and Supervised Classification | http://arxiv.org/abs/1312.7557 | id:1312.7557 author:Saeid Fazli, Sevin Samadi category:cs.CV  published:2013-12-29 summary:The appearance and structure of blood vessels in retinal images have an important role in diagnosis of diseases. This paper proposes a method for automatic retinal vessel segmentation. In this work, a novel preprocessing based on local histogram equalization is used to enhance the original image then pixels are classified as vessel and non-vessel using a classifier. For this classification, special feature vectors are organized based on responses to Morlet wavelet. Morlet wavelet is a continues transform which has the ability to filter existing noises after preprocessing. Bayesian classifier is used and Gaussian mixture model (GMM) is its likelihood function. The probability distributions are approximated according to training set of manual that has been segmented by a specialist. After this, morphological transforms are used in different directions to make the existing discontinuities uniform on the DRIVE database, it achieves the accuracy about 0.9571 which shows that it is an accurate method among the available ones for retinal vessel segmentation. version:1
arxiv-1310-1803 | A Fast Hadamard Transform for Signals with Sub-linear Sparsity in the Transform Domain | http://arxiv.org/abs/1310.1803 | id:1310.1803 author:Robin Scheibler, Saeid Haghighatshoar, Martin Vetterli category:cs.IT math.IT stat.ML  published:2013-10-07 summary:A new iterative low complexity algorithm has been presented for computing the Walsh-Hadamard transform (WHT) of an $N$ dimensional signal with a $K$-sparse WHT, where $N$ is a power of two and $K = O(N^\alpha)$, scales sub-linearly in $N$ for some $0 < \alpha < 1$. Assuming a random support model for the non-zero transform domain components, the algorithm reconstructs the WHT of the signal with a sample complexity $O(K \log_2(\frac{N}{K}))$, a computational complexity $O(K\log_2(K)\log_2(\frac{N}{K}))$ and with a very high probability asymptotically tending to 1. The approach is based on the subsampling (aliasing) property of the WHT, where by a carefully designed subsampling of the time domain signal, one can induce a suitable aliasing pattern in the transform domain. By treating the aliasing patterns as parity-check constraints and borrowing ideas from erasure correcting sparse-graph codes, the recovery of the non-zero spectral values has been formulated as a belief propagation (BP) algorithm (peeling decoding) over a sparse-graph code for the binary erasure channel (BEC). Tools from coding theory are used to analyze the asymptotic performance of the algorithm in the very sparse ($\alpha\in(0,\frac{1}{3}]$) and the less sparse ($\alpha\in(\frac{1}{3},1)$) regime. version:2
arxiv-1312-7523 | Learning Temporal Logical Properties Discriminating ECG models of Cardiac Arrhytmias | http://arxiv.org/abs/1312.7523 | id:1312.7523 author:Ezio Bartocci, Luca Bortolussi, Guido Sanguinetti category:cs.LO cs.CV q-bio.QM  published:2013-12-29 summary:We present a novel approach to learn the formulae characterising the emergent behaviour of a dynamical system from system observations. At a high level, the approach starts by devising a statistical dynamical model of the system which optimally fits the observations. We then propose general optimisation strategies for selecting high support formulae (under the learnt model of the system) either within a discrete set of formulae of bounded complexity, or a parametric family of formulae. We illustrate and apply the methodology on an in-depth case study of characterising cardiac malfunction from electro-cardiogram data, where our approach enables us to quantitatively determine the diagnostic power of a formula in discriminating between different cardiac conditions. version:1
arxiv-1312-7511 | A Novel Scheme for Generating Secure Face Templates Using BDA | http://arxiv.org/abs/1312.7511 | id:1312.7511 author:Shraddha S. Shinde, Prof. Anagha P. Khedkar category:cs.CV cs.CR  published:2013-12-29 summary:In identity management system, frequently used biometric recognition system needs awareness towards issue of protecting biometric template as far as more reliable solution is apprehensive. In sight of this biometric template protection algorithm should gratify the basic requirements viz. security, discriminability and cancelability. As no single template protection method is capable of satisfying these requirements, a novel scheme for face template generation and protection is proposed. The novel scheme is proposed to provide security and accuracy in new user enrolment and authentication process. This novel scheme takes advantage of both the hybrid approach and the binary discriminant analysis algorithm. This algorithm is designed on the basis of random projection, binary discriminant analysis and fuzzy commitment scheme. Publicly available benchmark face databases (FERET, FRGC, CMU-PIE) and other datasets are used for evaluation. The proposed novel scheme enhances the discriminability and recognition accuracy in terms of matching score of the face images for each stage and provides high security against potential attacks namely brute force and smart attacks. In this paper, we discuss results viz. averages matching score, computation time and security for hybrid approach and novel approach. version:1
arxiv-1312-7485 | A General Algorithm for Deciding Transportability of Experimental Results | http://arxiv.org/abs/1312.7485 | id:1312.7485 author:Elias Bareinboim, Judea Pearl category:cs.AI stat.ME stat.ML  published:2013-12-29 summary:Generalizing empirical findings to new environments, settings, or populations is essential in most scientific explorations. This article treats a particular problem of generalizability, called "transportability", defined as a license to transfer information learned in experimental studies to a different population, on which only observational studies can be conducted. Given a set of assumptions concerning commonalities and differences between the two populations, Pearl and Bareinboim (2011) derived sufficient conditions that permit such transfer to take place. This article summarizes their findings and supplements them with an effective procedure for deciding when and how transportability is feasible. It establishes a necessary and sufficient condition for deciding when causal effects in the target population are estimable from both the statistical information available and the causal information transferred from the experiments. The article further provides a complete algorithm for computing the transport formula, that is, a way of combining observational and experimental information to synthesize bias-free estimate of the desired causal relation. Finally, the article examines the differences between transportability and other variants of generalizability. version:1
arxiv-1312-7463 | Generalized Ambiguity Decomposition for Understanding Ensemble Diversity | http://arxiv.org/abs/1312.7463 | id:1312.7463 author:Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran, Shrikanth S. Narayanan category:stat.ML cs.CV cs.LG I.5  published:2013-12-28 summary:Diversity or complementarity of experts in ensemble pattern recognition and information processing systems is widely-observed by researchers to be crucial for achieving performance improvement upon fusion. Understanding this link between ensemble diversity and fusion performance is thus an important research question. However, prior works have theoretically characterized ensemble diversity and have linked it with ensemble performance in very restricted settings. We present a generalized ambiguity decomposition (GAD) theorem as a broad framework for answering these questions. The GAD theorem applies to a generic convex ensemble of experts for any arbitrary twice-differentiable loss function. It shows that the ensemble performance approximately decomposes into a difference of the average expert performance and the diversity of the ensemble. It thus provides a theoretical explanation for the empirically-observed benefit of fusing outputs from diverse classifiers and regressors. It also provides a loss function-dependent, ensemble-dependent, and data-dependent definition of diversity. We present extensions of this decomposition to common regression and classification loss functions, and report a simulation-based analysis of the diversity term and the accuracy of the decomposition. We finally present experiments on standard pattern recognition data sets which indicate the accuracy of the decomposition for real-world classification and regression problems. version:1
arxiv-1312-7414 | Stopping Rules for Bag-of-Words Image Search and Its Application in Appearance-Based Localization | http://arxiv.org/abs/1312.7414 | id:1312.7414 author:Kiana Hajebi, Hong Zhang category:cs.CV cs.RO  published:2013-12-28 summary:We propose a technique to improve the search efficiency of the bag-of-words (BoW) method for image retrieval. We introduce a notion of difficulty for the image matching problems and propose methods that reduce the amount of computations required for the feature vector-quantization task in BoW by exploiting the fact that easier queries need less computational resources. Measuring the difficulty of a query and stopping the search accordingly is formulated as a stopping problem. We introduce stopping rules that terminate the image search depending on the difficulty of each query, thereby significantly reducing the computational cost. Our experimental results show the effectiveness of our approach when it is applied to appearance-based localization problem. version:1
arxiv-1110-1773 | Positive definite matrices and the S-divergence | http://arxiv.org/abs/1110.1773 | id:1110.1773 author:Suvrit Sra category:math.FA stat.ML  published:2011-10-08 summary:Positive definite matrices abound in a dazzling variety of applications. This ubiquity can be in part attributed to their rich geometric structure: positive definite matrices form a self-dual convex cone whose strict interior is a Riemannian manifold. The manifold view is endowed with a "natural" distance function while the conic view is not. Nevertheless, drawing motivation from the conic view, we introduce the S-Divergence as a "natural" distance-like function on the open cone of positive definite matrices. We motivate the S-divergence via a sequence of results that connect it to the Riemannian distance. In particular, we show that (a) this divergence is the square of a distance; and (b) that it has several geometric properties similar to those of the Riemannian distance, though without being computationally as demanding. The S-divergence is even more intriguing: although nonconvex, we can still compute matrix means and medians using it to global optimality. We complement our results with some numerical experiments illustrating our theorems and our optimization algorithm for computing matrix medians. version:4
arxiv-1312-7308 | lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits | http://arxiv.org/abs/1312.7308 | id:1312.7308 author:Kevin Jamieson, Matthew Malloy, Robert Nowak, Sébastien Bubeck category:stat.ML cs.LG  published:2013-12-27 summary:The paper proposes a novel upper confidence bound (UCB) procedure for identifying the arm with the largest mean in a multi-armed bandit game in the fixed confidence setting using a small number of total samples. The procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of a lower bound based on the law of the iterated logarithm (LIL). Inspired by the LIL, we construct our confidence bounds to explicitly account for the infinite time horizon of the algorithm. In addition, by using a novel stopping time for the algorithm we avoid a union bound over the arms that has been observed in other UCB-type algorithms. We prove that the algorithm is optimal up to constants and also show through simulations that it provides superior performance with respect to the state-of-the-art. version:1
arxiv-1212-2006 | A Novel Feature-based Bayesian Model for Query Focused Multi-document Summarization | http://arxiv.org/abs/1212.2006 | id:1212.2006 author:Jiwei Li, Sujian Li category:cs.CL cs.IR  published:2012-12-10 summary:Both supervised learning methods and LDA based topic model have been successfully applied in the field of query focused multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experiments on TAC2008 and TAC2009 demonstrate the effectiveness of our approach. version:2
arxiv-1311-6392 | A Comprehensive Approach to Universal Piecewise Nonlinear Regression Based on Trees | http://arxiv.org/abs/1311.6392 | id:1311.6392 author:N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG stat.ML  published:2013-11-25 summary:In this paper, we investigate adaptive nonlinear regression and introduce tree based piecewise linear regression algorithms that are highly efficient and provide significantly improved performance with guaranteed upper bounds in an individual sequence manner. We use a tree notion in order to partition the space of regressors in a nested structure. The introduced algorithms adapt not only their regression functions but also the complete tree structure while achieving the performance of the "best" linear mixture of a doubly exponential number of partitions, with a computational complexity only polynomial in the number of nodes of the tree. While constructing these algorithms, we also avoid using any artificial "weighting" of models (with highly data dependent parameters) and, instead, directly minimize the final regression error, which is the ultimate performance goal. The introduced methods are generic such that they can readily incorporate different tree construction methods such as random trees in their framework and can use different regressor or partitioning functions as demonstrated in the paper. version:2
arxiv-1312-7223 | Quality Estimation of English-Hindi Outputs using Naive Bayes Classifier | http://arxiv.org/abs/1312.7223 | id:1312.7223 author:Rashmi Gupta, Nisheeth Joshi, Iti Mathur category:cs.CL  published:2013-12-27 summary:In this paper we present an approach for estimating the quality of machine translation system. There are various methods for estimating the quality of output sentences, but in this paper we focus on Na\"ive Bayes classifier to build model using features which are extracted from the input sentences. These features are used for finding the likelihood of each of the sentences of the training data which are then further used for determining the scores of the test data. On the basis of these scores we determine the class labels of the test data. version:1
arxiv-1312-7179 | Sub-Classifier Construction for Error Correcting Output Code Using Minimum Weight Perfect Matching | http://arxiv.org/abs/1312.7179 | id:1312.7179 author:Patoomsiri Songsiri, Thimaporn Phetkaew, Ryutaro Ichise, Boonserm Kijsirikul category:cs.LG cs.IT math.IT  published:2013-12-27 summary:Multi-class classification is mandatory for real world problems and one of promising techniques for multi-class classification is Error Correcting Output Code. We propose a method for constructing the Error Correcting Output Code to obtain the suitable combination of positive and negative classes encoded to represent binary classifiers. The minimum weight perfect matching algorithm is applied to find the optimal pairs of subset of classes by using the generalization performance as a weighting criterion. Based on our method, each subset of classes with positive and negative labels is appropriately combined for learning the binary classifiers. Experimental results show that our technique gives significantly higher performance compared to traditional methods including the dense random code and the sparse random code both in terms of accuracy and classification times. Moreover, our method requires significantly smaller number of binary classifiers while maintaining accuracy compared to the One-Versus-One. version:1
arxiv-1312-7167 | Near-separable Non-negative Matrix Factorization with $\ell_1$- and Bregman Loss Functions | http://arxiv.org/abs/1312.7167 | id:1312.7167 author:Abhishek Kumar, Vikas Sindhwani category:stat.ML cs.CV cs.LG  published:2013-12-27 summary:Recently, a family of tractable NMF algorithms have been proposed under the assumption that the data matrix satisfies a separability condition Donoho & Stodden (2003); Arora et al. (2012). Geometrically, this condition reformulates the NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. In this paper, we develop several extensions of the conical hull procedures of Kumar et al. (2013) for robust ($\ell_1$) approximations and Bregman divergences. Our methods inherit all the advantages of Kumar et al. (2013) including scalability and noise-tolerance. We show that on foreground-background separation problems in computer vision, robust near-separable NMFs match the performance of Robust PCA, considered state of the art on these problems, with an order of magnitude faster training time. We also demonstrate applications in exemplar selection settings. version:1
arxiv-1312-7345 | Lesion Border Detection in Dermoscopy Images Using Ensembles of Thresholding Methods | http://arxiv.org/abs/1312.7345 | id:1312.7345 author:M. Emre Celebi, Quan Wen, Sae Hwang, Hitoshi Iyatomi, Gerald Schaefer category:cs.CV I.4.6  published:2013-12-26 summary:Dermoscopy is one of the major imaging modalities used in the diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty and subjectivity of human interpretation, automated analysis of dermoscopy images has become an important research area. Border detection is often the first step in this analysis. In many cases, the lesion can be roughly separated from the background skin using a thresholding method applied to the blue channel. However, no single thresholding method appears to be robust enough to successfully handle the wide variety of dermoscopy images encountered in clinical practice. In this paper, we present an automated method for detecting lesion borders in dermoscopy images using ensembles of thresholding methods. Experiments on a difficult set of 90 images demonstrate that the proposed method is robust, fast, and accurate when compared to nine state-of-the-art methods. version:1
arxiv-1312-7085 | Finding More Relevance: Propagating Similarity on Markov Random Field for Image Retrieval | http://arxiv.org/abs/1312.7085 | id:1312.7085 author:Peng Lu, Xujun Peng, Xinshan Zhu, Xiaojie Wang category:cs.CV  published:2013-12-26 summary:To effectively retrieve objects from large corpus with high accuracy is a challenge task. In this paper, we propose a method that propagates visual feature level similarities on a Markov random field (MRF) to obtain a high level correspondence in image space for image pairs. The proposed correspondence between image pair reflects not only the similarity of low-level visual features but also the relations built through other images in the database and it can be easily integrated into the existing bag-of-visual-words(BoW) based systems to reduce the missing rate. We evaluate our method on the standard Oxford-5K, Oxford-105K and Paris-6K dataset. The experiment results show that the proposed method significantly improves the retrieval accuracy on three datasets and exceeds the current state-of-the-art retrieval performance. version:1
arxiv-1306-0940 | (More) Efficient Reinforcement Learning via Posterior Sampling | http://arxiv.org/abs/1306.0940 | id:1306.0940 author:Ian Osband, Daniel Russo, Benjamin Van Roy category:stat.ML cs.LG  published:2013-06-04 summary:Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S \sqrt{AT})$ bound on the expected regret, where $T$ is time, $\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds. version:5
arxiv-1312-6506 | Top Down Approach to Multiple Plane Detection | http://arxiv.org/abs/1312.6506 | id:1312.6506 author:Prateek Singhal, Aditya Deshpande, N Dinesh Reddy, K Madhava Krishna category:cs.CV  published:2013-12-23 summary:Detecting multiple planes in images is a challenging problem, but one with many applications. Recent work such as J-Linkage and Ordered Residual Kernels have focussed on developing a domain independent approach to detect multiple structures. These multiple structure detection methods are then used for estimating multiple homographies given feature matches between two images. Features participating in the multiple homographies detected, provide us the multiple scene planes. We show that these methods provide locally optimal results and fail to merge detected planar patches to the true scene planes. These methods use only residues obtained on applying homography of one plane to another as cue for merging. In this paper, we develop additional cues such as local consistency of planes, local normals, texture etc. to perform better classification and merging . We formulate the classification as an MRF problem and use TRWS message passing algorithm to solve non metric energy terms and complex sparse graph structure. We show results on challenging dataset common in robotics navigation scenarios where our method shows accuracy of more than 85 percent on average while being close or same as the actual number of scene planes. version:2
arxiv-1312-7035 | Shape-constrained Estimation of Value Functions | http://arxiv.org/abs/1312.7035 | id:1312.7035 author:Mohammad Mousavi, Peter W. Glynn category:math.PR cs.CE math.OC stat.ML  published:2013-12-26 summary:We present a fully nonparametric method to estimate the value function, via simulation, in the context of expected infinite-horizon discounted rewards for Markov chains. Estimating such value functions plays an important role in approximate dynamic programming and applied probability in general. We incorporate "soft information" into the estimation algorithm, such as knowledge of convexity, monotonicity, or Lipchitz constants. In the presence of such information, a nonparametric estimator for the value function can be computed that is provably consistent as the simulated time horizon tends to infinity. As an application, we implement our method on price tolling agreement contracts in energy markets. version:1
arxiv-1312-7024 | Model-based clustering with Hidden Markov Model regression for time series with regime changes | http://arxiv.org/abs/1312.7024 | id:1312.7024 author:Faicel Chamroukhi, Allou Samé, Patrice Aknin, Gérard Govaert category:stat.ML cs.LG stat.ME  published:2013-12-25 summary:This paper introduces a novel model-based clustering approach for clustering time series which present changes in regime. It consists of a mixture of polynomial regressions governed by hidden Markov chains. The underlying hidden process for each cluster activates successively several polynomial regimes during time. The parameter estimation is performed by the maximum likelihood method through a dedicated Expectation-Maximization (EM) algorithm. The proposed approach is evaluated using simulated time series and real-world time series issued from a railway diagnosis application. Comparisons with existing approaches for time series clustering, including the stand EM for Gaussian mixtures, $K$-means clustering, the standard mixture of regression models and mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed approach. version:1
arxiv-1312-7022 | Robust EM algorithm for model-based curve clustering | http://arxiv.org/abs/1312.7022 | id:1312.7022 author:Faicel Chamroukhi category:stat.ME cs.LG stat.ML  published:2013-12-25 summary:Model-based clustering approaches concern the paradigm of exploratory data analysis relying on the finite mixture model to automatically find a latent structure governing observed data. They are one of the most popular and successful approaches in cluster analysis. The mixture density estimation is generally performed by maximizing the observed-data log-likelihood by using the expectation-maximization (EM) algorithm. However, it is well-known that the EM algorithm initialization is crucial. In addition, the standard EM algorithm requires the number of clusters to be known a priori. Some solutions have been provided in [31, 12] for model-based clustering with Gaussian mixture models for multivariate data. In this paper we focus on model-based curve clustering approaches, when the data are curves rather than vectorial data, based on regression mixtures. We propose a new robust EM algorithm for clustering curves. We extend the model-based clustering approach presented in [31] for Gaussian mixture models, to the case of curve clustering by regression mixtures, including polynomial regression mixtures as well as spline or B-spline regressions mixtures. Our approach both handles the problem of initialization and the one of choosing the optimal number of clusters as the EM learning proceeds, rather than in a two-fold scheme. This is achieved by optimizing a penalized log-likelihood criterion. A simulation study confirms the potential benefit of the proposed algorithm in terms of robustness regarding initialization and funding the actual number of clusters. version:1
arxiv-1312-7018 | Mixture model-based functional discriminant analysis for curve classification | http://arxiv.org/abs/1312.7018 | id:1312.7018 author:Faicel Chamroukhi, Hervé Glotin category:stat.ME cs.LG stat.ML  published:2013-12-25 summary:Statistical approaches for Functional Data Analysis concern the paradigm for which the individuals are functions or curves rather than finite dimensional vectors. In this paper, we particularly focus on the modeling and the classification of functional data which are temporal curves presenting regime changes over time. More specifically, we propose a new mixture model-based discriminant analysis approach for functional data using a specific hidden process regression model. Our approach is particularly adapted to both handle the problem of complex-shaped classes of curves, where each class is composed of several sub-classes, and to deal with the regime changes within each homogeneous sub-class. The model explicitly integrates the heterogeneity of each class of curves via a mixture model formulation, and the regime changes within each sub-class through a hidden logistic process. The approach allows therefore for fitting flexible curve-models to each class of complex-shaped curves presenting regime changes through an unsupervised learning scheme, to automatically summarize it into a finite number of homogeneous clusters, each of them is decomposed into several regimes. The model parameters are learned by maximizing the observed-data log-likelihood for each class by using a dedicated expectation-maximization (EM) algorithm. Comparisons on simulated data and real data with alternative approaches, including functional linear discriminant analysis and functional mixture discriminant analysis with polynomial regression mixtures and spline regression mixtures, show that the proposed approach provides better results regarding the discrimination results and significantly improves the curves approximation. version:1
arxiv-1312-7011 | Classification automatique de données temporelles en classes ordonnées | http://arxiv.org/abs/1312.7011 | id:1312.7011 author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ML stat.ME  published:2013-12-25 summary:This paper proposes a method of segmenting temporal data into ordered classes. It is based on mixture models and a discrete latent process, which enables to successively activates the classes. The classification can be performed by maximizing the likelihood via the EM algorithm or by simultaneously optimizing the model parameters and the partition by the CEM algorithm. These two algorithms can be seen as alternatives to Fisher's algorithm, which improve its computing time. version:1
arxiv-1312-7007 | Functional Mixture Discriminant Analysis with hidden process regression for curve classification | http://arxiv.org/abs/1312.7007 | id:1312.7007 author:Faicel Chamroukhi, Heré Glotin, Céline Rabouy category:stat.ME cs.LG stat.ML  published:2013-12-25 summary:We present a new mixture model-based discriminant analysis approach for functional data using a specific hidden process regression model. The approach allows for fitting flexible curve-models to each class of complex-shaped curves presenting regime changes. The model parameters are learned by maximizing the observed-data log-likelihood for each class by using a dedicated expectation-maximization (EM) algorithm. Comparisons on simulated data with alternative approaches show that the proposed approach provides better results. version:1
arxiv-1312-7003 | Supervised learning of a regression model based on latent process. Application to the estimation of fuel cell life time | http://arxiv.org/abs/1312.7003 | id:1312.7003 author:Raïssa Onanena, Faicel Chamroukhi, Latifa Oukhellou, Denis Candusso, Patrice Aknin, Daniel Hissel category:stat.ML cs.LG stat.AP  published:2013-12-25 summary:This paper describes a pattern recognition approach aiming to estimate fuel cell duration time from electrochemical impedance spectroscopy measurements. It consists in first extracting features from both real and imaginary parts of the impedance spectrum. A parametric model is considered in the case of the real part, whereas regression model with latent variables is used in the latter case. Then, a linear regression model using different subsets of extracted features is used fo r the estimation of fuel cell time duration. The performances of the proposed approach are evaluated on experimental data set to show its feasibility. This could lead to interesting perspectives for predictive maintenance policy of fuel cell. version:1
arxiv-1312-7001 | A regression model with a hidden logistic process for feature extraction from time series | http://arxiv.org/abs/1312.7001 | id:1312.7001 author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2013-12-25 summary:A new approach for feature extraction from time series is proposed in this paper. This approach consists of a specific regression model incorporating a discrete hidden logistic process. The model parameters are estimated by the maximum likelihood method performed by a dedicated Expectation Maximization (EM) algorithm. The parameters of the hidden logistic process, in the inner loop of the EM algorithm, are estimated using a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm and its iterative variant have also been considered for comparisons. An experimental study using simulated and real data reveals good performances of the proposed approach. version:1
arxiv-1312-6994 | A regression model with a hidden logistic process for signal parametrization | http://arxiv.org/abs/1312.6994 | id:1312.6994 author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG stat.ML  published:2013-12-25 summary:A new approach for signal parametrization, which consists of a specific regression model incorporating a discrete hidden logistic process, is proposed. The model parameters are estimated by the maximum likelihood method performed by a dedicated Expectation Maximization (EM) algorithm. The parameters of the hidden logistic process, in the inner loop of the EM algorithm, are estimated using a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. An experimental study using simulated and real data reveals good performances of the proposed approach. version:1
arxiv-1304-0160 | Parallel Computation Is ESS | http://arxiv.org/abs/1304.0160 | id:1304.0160 author:Nabarun Mondal, Partha P. Ghosh category:cs.LG cs.AI cs.GT  published:2013-03-31 summary:There are enormous amount of examples of Computation in nature, exemplified across multiple species in biology. One crucial aim for these computations across all life forms their ability to learn and thereby increase the chance of their survival. In the current paper a formal definition of autonomous learning is proposed. From that definition we establish a Turing Machine model for learning, where rule tables can be added or deleted, but can not be modified. Sequential and parallel implementations of this model are discussed. It is found that for general purpose learning based on this model, the implementations capable of parallel execution would be evolutionarily stable. This is proposed to be of the reasons why in Nature parallelism in computation is found in abundance. version:8
arxiv-1312-6978 | Modèle à processus latent et algorithme EM pour la régression non linéaire | http://arxiv.org/abs/1312.6978 | id:1312.6978 author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:math.ST cs.LG stat.ME stat.ML stat.TH  published:2013-12-25 summary:A non linear regression approach which consists of a specific regression model incorporating a latent process, allowing various polynomial regression models to be activated preferentially and smoothly, is introduced in this paper. The model parameters are estimated by maximum likelihood performed via a dedicated expecation-maximization (EM) algorithm. An experimental study using simulated and real data sets reveals good performances of the proposed approach. version:1
arxiv-1312-6969 | Time series modeling by a regression approach based on a latent process | http://arxiv.org/abs/1312.6969 | id:1312.6969 author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2013-12-25 summary:Time series are used in many domains including finance, engineering, economics and bioinformatics generally to represent the change of a measurement over time. Modeling techniques may then be used to give a synthetic representation of such data. A new approach for time series modeling is proposed in this paper. It consists of a regression model incorporating a discrete hidden logistic process allowing for activating smoothly or abruptly different polynomial regression models. The model parameters are estimated by the maximum likelihood method performed by a dedicated Expectation Maximization (EM) algorithm. The M step of the EM algorithm uses a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm to estimate the hidden process parameters. To evaluate the proposed approach, an experimental study on simulated data and real world data was performed using two alternative approaches: a heteroskedastic piecewise regression model using a global optimization algorithm based on dynamic programming, and a Hidden Markov Regression Model whose parameters are estimated by the Baum-Welch algorithm. Finally, in the context of the remote monitoring of components of the French railway infrastructure, and more particularly the switch mechanism, the proposed approach has been applied to modeling and classifying time series representing the condition measurements acquired during switch operations. version:1
arxiv-1312-6968 | A hidden process regression model for functional data description. Application to curve discrimination | http://arxiv.org/abs/1312.6968 | id:1312.6968 author:Faicel Chamroukhi, Allou Samé, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG stat.ML  published:2013-12-25 summary:A new approach for functional data description is proposed in this paper. It consists of a regression model with a discrete hidden logistic process which is adapted for modeling curves with abrupt or smooth regime changes. The model parameters are estimated in a maximum likelihood framework through a dedicated Expectation Maximization (EM) algorithm. From the proposed generative model, a curve discrimination rule is derived using the Maximum A Posteriori rule. The proposed model is evaluated using simulated curves and real world curves acquired during railway switch operations, by performing comparisons with the piecewise regression approach in terms of curve modeling and classification. version:1
arxiv-1312-6967 | Model-based clustering and segmentation of time series with changes in regime | http://arxiv.org/abs/1312.6967 | id:1312.6967 author:Allou Samé, Faicel Chamroukhi, Gérard Govaert, Patrice Aknin category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2013-12-25 summary:Mixture model-based clustering, usually applied to multidimensional data, has become a popular approach in many data analysis problems, both for its good statistical properties and for the simplicity of implementation of the Expectation-Maximization (EM) algorithm. Within the context of a railway application, this paper introduces a novel mixture model for dealing with time series that are subject to changes in regime. The proposed approach consists in modeling each cluster by a regression model in which the polynomial coefficients vary according to a discrete hidden process. In particular, this approach makes use of logistic functions to model the (smooth or abrupt) transitions between regimes. The model parameters are estimated by the maximum likelihood method solved by an Expectation-Maximization algorithm. The proposed approach can also be regarded as a clustering approach which operates by finding groups of time series having common changes in regime. In addition to providing a time series partition, it therefore provides a time series segmentation. The problem of selecting the optimal numbers of clusters and segments is solved by means of the Bayesian Information Criterion (BIC). The proposed approach is shown to be efficient using a variety of simulated time series and real-world time series of electrical power consumption from rail switching operations. version:1
arxiv-1312-3386 | Clustering for high-dimension, low-sample size data using distance vectors | http://arxiv.org/abs/1312.3386 | id:1312.3386 author:Yoshikazu Terada category:stat.ML cs.LG  published:2013-12-12 summary:In high-dimension, low-sample size (HDLSS) data, it is not always true that closeness of two objects reflects a hidden cluster structure. We point out the important fact that it is not the closeness, but the "values" of distance that contain information of the cluster structure in high-dimensional space. Based on this fact, we propose an efficient and simple clustering approach, called distance vector clustering, for HDLSS data. Under the assumptions given in the work of Hall et al. (2005), we show the proposed approach provides a true cluster label under milder conditions when the dimension tends to infinity with the sample size fixed. The effectiveness of the distance vector clustering approach is illustrated through a numerical experiment and real data analysis. version:2
arxiv-1312-6966 | Model-based functional mixture discriminant analysis with hidden process regression for curve classification | http://arxiv.org/abs/1312.6966 | id:1312.6966 author:Faicel Chamroukhi, Hervé Glotin, Allou Samé category:stat.ME cs.LG math.ST stat.ML stat.TH  published:2013-12-25 summary:In this paper, we study the modeling and the classification of functional data presenting regime changes over time. We propose a new model-based functional mixture discriminant analysis approach based on a specific hidden process regression model that governs the regime changes over time. Our approach is particularly adapted to handle the problem of complex-shaped classes of curves, where each class is potentially composed of several sub-classes, and to deal with the regime changes within each homogeneous sub-class. The proposed model explicitly integrates the heterogeneity of each class of curves via a mixture model formulation, and the regime changes within each sub-class through a hidden logistic process. Each class of complex-shaped curves is modeled by a finite number of homogeneous clusters, each of them being decomposed into several regimes. The model parameters of each class are learned by maximizing the observed-data log-likelihood by using a dedicated expectation-maximization (EM) algorithm. Comparisons are performed with alternative curve classification approaches, including functional linear discriminant analysis and functional mixture discriminant analysis with polynomial regression mixtures and spline regression mixtures. Results obtained on simulated data and real data show that the proposed approach outperforms the alternative approaches in terms of discrimination, and significantly improves the curves approximation. version:1
arxiv-1312-6965 | An Unsupervised Approach for Automatic Activity Recognition based on Hidden Markov Model Regression | http://arxiv.org/abs/1312.6965 | id:1312.6965 author:Dorra Trabelsi, Samer Mohammed, Faicel Chamroukhi, Latifa Oukhellou, Yacine Amirat category:stat.ML cs.CV cs.LG  published:2013-12-25 summary:Using supervised machine learning approaches to recognize human activities from on-body wearable accelerometers generally requires a large amount of labelled data. When ground truth information is not available, too expensive, time consuming or difficult to collect, one has to rely on unsupervised approaches. This paper presents a new unsupervised approach for human activity recognition from raw acceleration data measured using inertial wearable sensors. The proposed method is based upon joint segmentation of multidimensional time series using a Hidden Markov Model (HMM) in a multiple regression context. The model is learned in an unsupervised framework using the Expectation-Maximization (EM) algorithm where no activity labels are needed. The proposed method takes into account the sequential appearance of the data. It is therefore adapted for the temporal acceleration data to accurately detect the activities. It allows both segmentation and classification of the human activities. Experimental results are provided to demonstrate the efficiency of the proposed approach with respect to standard supervised and unsupervised classification approaches version:1
arxiv-1312-6962 | Subjectivity Classification using Machine Learning Techniques for Mining Feature-Opinion Pairs from Web Opinion Sources | http://arxiv.org/abs/1312.6962 | id:1312.6962 author:Ahmad Kamal category:cs.IR cs.CL cs.LG  published:2013-12-25 summary:Due to flourish of the Web 2.0, web opinion sources are rapidly emerging containing precious information useful for both customers and manufactures. Recently, feature based opinion mining techniques are gaining momentum in which customer reviews are processed automatically for mining product features and user opinions expressed over them. However, customer reviews may contain both opinionated and factual sentences. Distillations of factual contents improve mining performance by preventing noisy and irrelevant extraction. In this paper, combination of both supervised machine learning and rule-based approaches are proposed for mining feasible feature-opinion pairs from subjective review sentences. In the first phase of the proposed approach, a supervised machine learning technique is applied for classifying subjective and objective sentences from customer reviews. In the next phase, a rule based method is implemented which applies linguistic and semantic analysis of texts to mine feasible feature-opinion pairs from subjective sentences retained after the first phase. The effectiveness of the proposed methods is established through experimentation over customer reviews on different electronic products. version:1
arxiv-1312-6956 | Joint segmentation of multivariate time series with hidden process regression for human activity recognition | http://arxiv.org/abs/1312.6956 | id:1312.6956 author:Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou, Yacine Amirat category:stat.ML cs.LG  published:2013-12-25 summary:The problem of human activity recognition is central for understanding and predicting the human behavior, in particular in a prospective of assistive services to humans, such as health monitoring, well being, security, etc. There is therefore a growing need to build accurate models which can take into account the variability of the human activities over time (dynamic models) rather than static ones which can have some limitations in such a dynamic context. In this paper, the problem of activity recognition is analyzed through the segmentation of the multidimensional time series of the acceleration data measured in the 3-d space using body-worn accelerometers. The proposed model for automatic temporal segmentation is a specific statistical latent process model which assumes that the observed acceleration sequence is governed by sequence of hidden (unobserved) activities. More specifically, the proposed approach is based on a specific multiple regression model incorporating a hidden discrete logistic process which governs the switching from one activity to another over time. The model is learned in an unsupervised context by maximizing the observed-data log-likelihood via a dedicated expectation-maximization (EM) algorithm. We applied it on a real-world automatic human activity recognition problem and its performance was assessed by performing comparisons with alternative approaches, including well-known supervised static classifiers and the standard hidden Markov model (HMM). The obtained results are very encouraging and show that the proposed approach is quite competitive even it works in an entirely unsupervised way and does not requires a feature extraction preprocessing step. version:1
arxiv-1312-6948 | Description Logics based Formalization of Wh-Queries | http://arxiv.org/abs/1312.6948 | id:1312.6948 author:Sourish Dasgupta, Rupali KaPatel, Ankur Padia, Kushal Shah category:cs.CL cs.AI  published:2013-12-25 summary:The problem of Natural Language Query Formalization (NLQF) is to translate a given user query in natural language (NL) into a formal language so that the semantic interpretation has equivalence with the NL interpretation. Formalization of NL queries enables logic based reasoning during information retrieval, database query, question-answering, etc. Formalization also helps in Web query normalization and indexing, query intent analysis, etc. In this paper we are proposing a Description Logics based formal methodology for wh-query intent (also called desire) identification and corresponding formal translation. We evaluated the scalability of our proposed formalism using Microsoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset. version:1
arxiv-1312-6317 | Outlier robust system identification: a Bayesian kernel-based approach | http://arxiv.org/abs/1312.6317 | id:1312.6317 author:Giulio Bottegal, Aleksandr Y. Aravkin, Hakan Hjalmarsson, Gianluigi Pillonetto category:stat.ML 47N30  65K10  published:2013-12-21 summary:In this paper, we propose an outlier-robust regularized kernel-based method for linear system identification. The unknown impulse response is modeled as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. To build robustness to outliers, we model the measurement noise as realizations of independent Laplacian random variables. The identification problem is cast in a Bayesian framework, and solved by a new Markov Chain Monte Carlo (MCMC) scheme. In particular, exploiting the representation of the Laplacian random variables as scale mixtures of Gaussians, we design a Gibbs sampler which quickly converges to the target distribution. Numerical simulations show a substantial improvement in the accuracy of the estimates over state-of-the-art kernel-based methods. version:2
arxiv-1312-6885 | Deep learning for class-generic object detection | http://arxiv.org/abs/1312.6885 | id:1312.6885 author:Brody Huval, Adam Coates, Andrew Ng category:cs.CV cs.LG cs.NE  published:2013-12-24 summary:We investigate the use of deep neural networks for the novel task of class generic object detection. We show that neural networks originally designed for image recognition can be trained to detect objects within images, regardless of their class, including objects for which no bounding box labels have been provided. In addition, we show that bounding box labels yield a 1% performance increase on the ImageNet recognition challenge. version:1
arxiv-1312-6838 | Greedy Column Subset Selection for Large-scale Data Sets | http://arxiv.org/abs/1312.6838 | id:1312.6838 author:Ahmed K. Farahat, Ahmed Elgohary, Ali Ghodsi, Mohamed S. Kamel category:cs.DS cs.LG  published:2013-12-24 summary:In today's information systems, the availability of massive amounts of data necessitates the development of fast and accurate algorithms to summarize these data and represent them in a succinct format. One crucial problem in big data analytics is the selection of representative instances from large and massively-distributed data, which is formally known as the Column Subset Selection (CSS) problem. The solution to this problem enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix. This paper presents a fast and accurate greedy algorithm for large-scale column subset selection. The algorithm minimizes an objective function which measures the reconstruction error of the data matrix based on the subset of selected columns. The paper first presents a centralized greedy algorithm for column subset selection which depends on a novel recursive formula for calculating the reconstruction error of the data matrix. The paper then presents a MapReduce algorithm which selects a few representative columns from a matrix whose columns are massively distributed across several commodity machines. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets. version:1
arxiv-1312-6826 | 3D Interest Point Detection via Discriminative Learning | http://arxiv.org/abs/1312.6826 | id:1312.6826 author:Leizer Teran, Philippos Mordohai category:cs.CV  published:2013-12-24 summary:The task of detecting the interest points in 3D meshes has typically been handled by geometric methods. These methods, while greatly describing human preference, can be ill-equipped for handling the variety and subjectivity in human responses. Different tasks have different requirements for interest point detection; some tasks may necessitate high precision while other tasks may require high recall. Sometimes points with high curvature may be desirable, while in other cases high curvature may be an indication of noise. Geometric methods lack the required flexibility to adapt to such changes. As a consequence, interest point detection seems to be well suited for machine learning methods that can be trained to match the criteria applied on the annotated training data. In this paper, we formulate interest point detection as a supervised binary classification problem using a random forest as our classifier. Among other challenges, we are faced with an imbalanced learning problem due to the substantial difference in the priors between interest and non-interest points. We address this by re-sampling the training set. We validate the accuracy of our method and compare our results to those of five state of the art methods on a new, standard benchmark. version:1
arxiv-1312-6820 | A Fast Greedy Algorithm for Generalized Column Subset Selection | http://arxiv.org/abs/1312.6820 | id:1312.6820 author:Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel category:cs.DS cs.LG stat.ML  published:2013-12-24 summary:This paper defines a generalized column subset selection problem which is concerned with the selection of a few columns from a source matrix A that best approximate the span of a target matrix B. The paper then proposes a fast greedy algorithm for solving this problem and draws connections to different problems that can be efficiently solved using the proposed algorithm. version:1
arxiv-1312-6807 | Iterative Nearest Neighborhood Oversampling in Semisupervised Learning from Imbalanced Data | http://arxiv.org/abs/1312.6807 | id:1312.6807 author:Fengqi Li, Chuang Yu, Nanhai Yang, Feng Xia, Guangming Li, Fatemeh Kaveh-Yazdy category:cs.LG 68P20 H.3.3  published:2013-12-24 summary:Transductive graph-based semi-supervised learning methods usually build an undirected graph utilizing both labeled and unlabeled samples as vertices. Those methods propagate label information of labeled samples to neighbors through their edges in order to get the predicted labels of unlabeled samples. Most popular semi-supervised learning approaches are sensitive to initial label distribution happened in imbalanced labeled datasets. The class boundary will be severely skewed by the majority classes in an imbalanced classification. In this paper, we proposed a simple and effective approach to alleviate the unfavorable influence of imbalance problem by iteratively selecting a few unlabeled samples and adding them into the minority classes to form a balanced labeled dataset for the learning methods afterwards. The experiments on UCI datasets and MNIST handwritten digits dataset showed that the proposed approach outperforms other existing state-of-art methods. version:1
arxiv-1312-6802 | Suffix Stripping Problem as an Optimization Problem | http://arxiv.org/abs/1312.6802 | id:1312.6802 author:B. P. Pande, Pawan Tamta, H. S. Dhami category:cs.IR cs.CL  published:2013-12-24 summary:Stemming or suffix stripping, an important part of the modern Information Retrieval systems, is to find the root word (stem) out of a given cluster of words. Existing algorithms targeting this problem have been developed in a haphazard manner. In this work, we model this problem as an optimization problem. An Integer Program is being developed to overcome the shortcomings of the existing approaches. The sample results of the proposed method are also being compared with an established technique in the field for English language. An AMPL code for the same IP has also been given. version:1
arxiv-1401-6118 | Comparative study of Authorship Identification Techniques for Cyber Forensics Analysis | http://arxiv.org/abs/1401.6118 | id:1401.6118 author:Smita Nirkhi, R. V. Dharaskar category:cs.CY cs.CR cs.IR cs.LG  published:2013-12-24 summary:Authorship Identification techniques are used to identify the most appropriate author from group of potential suspects of online messages and find evidences to support the conclusion. Cybercriminals make misuse of online communication for sending blackmail or a spam email and then attempt to hide their true identities to void detection.Authorship Identification of online messages is the contemporary research issue for identity tracing in cyber forensics. This is highly interdisciplinary area as it takes advantage of machine learning, information retrieval, and natural language processing. In this paper, a study of recent techniques and automated approaches to attributing authorship of online messages is presented. The focus of this review study is to summarize all existing authorship identification techniques used in literature to identify authors of online messages. Also it discusses evaluation criteria and parameters for authorship attribution studies and list open questions that will attract future work in this area. version:1
arxiv-1312-6782 | IVSS Integration of Color Feature Extraction Techniques for Intelligent Video Search Systems | http://arxiv.org/abs/1312.6782 | id:1312.6782 author:Avinash N Bhute, B. B. Meshram category:cs.CV cs.IR cs.MM  published:2013-12-24 summary:As large amount of visual Information is available on web in form of images, graphics, animations and videos, so it is important in internet era to have an effective video search system. As there are number of video search engine (blinkx, Videosurf, Google, YouTube, etc.) which search for relevant videos based on user keyword or term, But very less commercial video search engine are available which search videos based on visual image/clip/video. In this paper we are recommending a system that will search for relevant video using color feature of video in response of user Query. version:1
arxiv-1401-3322 | A Subband-Based SVM Front-End for Robust ASR | http://arxiv.org/abs/1401.3322 | id:1401.3322 author:Jibran Yousafzai, Zoran Cvetkovic, Peter Sollich, Matthew Ager category:cs.CL cs.LG cs.SD  published:2013-12-24 summary:This work proposes a novel support vector machine (SVM) based robust automatic speech recognition (ASR) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms. The key issues of selecting the appropriate SVM kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed. The proposed front-end is compared with state-of-the-art ASR front-ends in terms of robustness to additive noise and linear filtering. Experiments performed on the TIMIT phoneme classification task demonstrate the benefits of the proposed subband based SVM front-end: it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed front-end with a conventional front-end such as MFCC yields further improvements over the individual front ends across the full range of noise levels. version:1
arxiv-1307-6365 | Time-Series Classification Through Histograms of Symbolic Polynomials | http://arxiv.org/abs/1307.6365 | id:1307.6365 author:Josif Grabocka, Martin Wistuba, Lars Schmidt-Thieme category:cs.AI cs.DB cs.LG  published:2013-07-24 summary:Time-series classification has attracted considerable research attention due to the various domains where time-series data are observed, ranging from medicine to econometrics. Traditionally, the focus of time-series classification has been on short time-series data composed of a unique pattern with intraclass pattern distortions and variations, while recently there have been attempts to focus on longer series composed of various local patterns. This study presents a novel method which can detect local patterns in long time-series via fitting local polynomial functions of arbitrary degrees. The coefficients of the polynomial functions are converted to symbolic words via equivolume discretizations of the coefficients' distributions. The symbolic polynomial words enable the detection of similar local patterns by assigning the same words to similar polynomials. Moreover, a histogram of the frequencies of the words is constructed from each time-series' bag of words. Each row of the histogram enables a new representation for the series and symbolize the existence of local patterns and their frequencies. Experimental evidence demonstrates outstanding results of our method compared to the state-of-art baselines, by exhibiting the best classification accuracies in all the datasets and having statistically significant improvements in the absolute majority of experiments. version:4
arxiv-1312-6712 | Invariant Factorization Of Time-Series | http://arxiv.org/abs/1312.6712 | id:1312.6712 author:Josif Grabocka, Lars Schmidt-Thieme category:cs.LG  published:2013-12-23 summary:Time-series classification is an important domain of machine learning and a plethora of methods have been developed for the task. In comparison to existing approaches, this study presents a novel method which decomposes a time-series dataset into latent patterns and membership weights of local segments to those patterns. The process is formalized as a constrained objective function and a tailored stochastic coordinate descent optimization is applied. The time-series are projected to a new feature representation consisting of the sums of the membership weights, which captures frequencies of local patterns. Features from various sliding window sizes are concatenated in order to encapsulate the interaction of patterns from different sizes. Finally, a large-scale experimental comparison against 6 state of the art baselines and 43 real life datasets is conducted. The proposed method outperforms all the baselines with statistically significant margins in terms of prediction accuracy. version:1
arxiv-1305-7388 | A central limit theorem for scaled eigenvectors of random dot product graphs | http://arxiv.org/abs/1305.7388 | id:1305.7388 author:Avanti Athreya, Vince Lyzinski, David J. Marchette, Carey E. Priebe, Daniel L. Sussman, Minh Tang category:math.ST stat.ML stat.TH  published:2013-05-31 summary:We prove a central limit theorem for the components of the largest eigenvectors of the adjacency matrix of a finite-dimensional random dot product graph whose true latent positions are unknown. In particular, we follow the methodology outlined in \citet{sussman2012universally} to construct consistent estimates for the latent positions, and we show that the appropriately scaled differences between the estimated and true latent positions converge to a mixture of Gaussian random variables. As a corollary, we obtain a central limit theorem for the first eigenvector of the adjacency matrix of an Erd\"os-Renyi random graph. version:2
arxiv-1312-6652 | Rounding Sum-of-Squares Relaxations | http://arxiv.org/abs/1312.6652 | id:1312.6652 author:Boaz Barak, Jonathan Kelner, David Steurer category:cs.DS cs.LG quant-ph  published:2013-12-23 summary:We present a general approach to rounding semidefinite programming relaxations obtained by the Sum-of-Squares method (Lasserre hierarchy). Our approach is based on using the connection between these relaxations and the Sum-of-Squares proof system to transform a *combining algorithm* -- an algorithm that maps a distribution over solutions into a (possibly weaker) solution -- into a *rounding algorithm* that maps a solution of the relaxation to a solution of the original problem. Using this approach, we obtain algorithms that yield improved results for natural variants of three well-known problems: 1) We give a quasipolynomial-time algorithm that approximates the maximum of a low degree multivariate polynomial with non-negative coefficients over the Euclidean unit sphere. Beyond being of interest in its own right, this is related to an open question in quantum information theory, and our techniques have already led to improved results in this area (Brand\~{a}o and Harrow, STOC '13). 2) We give a polynomial-time algorithm that, given a d dimensional subspace of R^n that (almost) contains the characteristic function of a set of size n/k, finds a vector $v$ in the subspace satisfying $ v _4^4 > c(k/d^{1/3}) v _2^2$, where $ v _p = (E_i v_i^p)^{1/p}$. Aside from being a natural relaxation, this is also motivated by a connection to the Small Set Expansion problem shown by Barak et al. (STOC 2012) and our results yield a certain improvement for that problem. 3) We use this notion of L_4 vs. L_2 sparsity to obtain a polynomial-time algorithm with substantially improved guarantees for recovering a planted $\mu$-sparse vector v in a random d-dimensional subspace of R^n. If v has mu n nonzero coordinates, we can recover it with high probability whenever $\mu < O(\min(1,n/d^2))$, improving for $d < n^{2/3}$ prior methods which intrinsically required $\mu < O(1/\sqrt(d))$. version:1
arxiv-1312-6615 | Automated Coin Recognition System using ANN | http://arxiv.org/abs/1312.6615 | id:1312.6615 author:Shatrughan Modi, Dr. Seema Bawa category:cs.CV cs.AI  published:2013-12-23 summary:Coins are integral part of our day to day life. We use coins everywhere like grocery store, banks, buses, trains etc. So it becomes a basic need that coins can be sorted and counted automatically. For this it is necessary that coins can be recognized automatically. In this paper we have developed an ANN (Artificial Neural Network) based Automated Coin Recognition System for the recognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotation invariance. We have taken images from both sides of coin. So this system is capable of recognizing coins from both sides. Features are extracted from images using techniques of Hough Transformation, Pattern Averaging etc. Then, the extracted features are passed as input to a trained Neural Network. 97.74% recognition rate has been achieved during the experiments i.e. only 2.26% miss recognition, which is quite encouraging. version:1
arxiv-1312-6609 | A comprehensive review of firefly algorithms | http://arxiv.org/abs/1312.6609 | id:1312.6609 author:Iztok Fister, Iztok Fister Jr., Xin-She Yang, Janez Brest category:cs.NE  published:2013-12-23 summary:The firefly algorithm has become an increasingly important tool of Swarm Intelligence that has been applied in almost all areas of optimization, as well as engineering practice. Many problems from various areas have been successfully solved using the firefly algorithm and its variants. In order to use the algorithm to solve diverse problems, the original firefly algorithm needs to be modified or hybridized. This paper carries out a comprehensive review of this living and evolving discipline of Swarm Intelligence, in order to show that the firefly algorithm could be applied to every problem arising in practice. On the other hand, it encourages new researchers and algorithm developers to use this simple and yet very efficient algorithm for problem solving. It often guarantees that the obtained results will meet the expectations. version:1
arxiv-1312-6607 | Using Latent Binary Variables for Online Reconstruction of Large Scale Systems | http://arxiv.org/abs/1312.6607 | id:1312.6607 author:Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner category:math.PR cs.LG stat.ML  published:2013-12-23 summary:We propose a probabilistic graphical model realizing a minimal encoding of real variables dependencies based on possibly incomplete observation and an empirical cumulative distribution function per variable. The target application is a large scale partially observed system, like e.g. a traffic network, where a small proportion of real valued variables are observed, and the other variables have to be predicted. Our design objective is therefore to have good scalability in a real-time setting. Instead of attempting to encode the dependencies of the system directly in the description space, we propose a way to encode them in a latent space of binary variables, reflecting a rough perception of the observable (congested/non-congested for a traffic road). The method relies in part on message passing algorithms, i.e. belief propagation, but the core of the work concerns the definition of meaningful latent variables associated to the variables of interest and their pairwise dependencies. Numerical experiments demonstrate the applicability of the method in practice. version:1
arxiv-1312-6599 | Image Processing based Systems and Techniques for the Recognition of Ancient and Modern Coins | http://arxiv.org/abs/1312.6599 | id:1312.6599 author:Shatrughan Modi, Dr. Seema Bawa category:cs.CV cs.AI  published:2013-12-23 summary:Coins are frequently used in everyday life at various places like in banks, grocery stores, supermarkets, automated weighing machines, vending machines etc. So, there is a basic need to automate the counting and sorting of coins. For this machines need to recognize the coins very fast and accurately, as further transaction processing depends on this recognition. Three types of systems are available in the market: Mechanical method based systems, Electromagnetic method based systems and Image processing based systems. This paper presents an overview of available systems and techniques based on image processing to recognize ancient and modern coins. version:1
arxiv-1312-6410 | A Survey on Eye-Gaze Tracking Techniques | http://arxiv.org/abs/1312.6410 | id:1312.6410 author:H. R. Chennamma, Xiaohui Yuan category:cs.CV  published:2013-12-22 summary:Study of eye-movement is being employed in Human Computer Interaction (HCI) research. Eye - gaze tracking is one of the most challenging problems in the area of computer vision. The goal of this paper is to present a review of latest research in this continued growth of remote eye-gaze tracking. This overview includes the basic definitions and terminologies, recent advances in the field and finally the need of future development in the field. version:1
arxiv-1312-6370 | An Efficient Edge Detection Technique by Two Dimensional Rectangular Cellular Automata | http://arxiv.org/abs/1312.6370 | id:1312.6370 author:Jahangir Mohammed, Deepak Ranjan Nayak category:cs.CV  published:2013-12-22 summary:This paper proposes a new pattern of two dimensional cellular automata linear rules that are used for efficient edge detection of an image. Since cellular automata is inherently parallel in nature, it has produced desired output within a unit time interval. We have observed four linear rules among 512 total linear rules of a rectangular cellular automata in adiabatic or reflexive boundary condition that produces an optimal result. These four rules are directly applied once to the images and produced edge detected output. We compare our results with the existing edge detection algorithms and found that our results shows better edge detection with an enhancement of edges. version:1
arxiv-1312-6282 | Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning | http://arxiv.org/abs/1312.6282 | id:1312.6282 author:François Denis, Mattias Gybels, Amaury Habrard category:cs.LG  published:2013-12-21 summary:Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix $H_S$, called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in $H_S$ and on the distance between $H_S$ and its mean $H_r$. Existing concentration bounds seem to indicate that the concentration over $H_r$ gets looser with the size of $H_r$, suggesting to make a trade-off between the quantity of used information and the size of $H_r$. We propose new dimension-free concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size. version:1
arxiv-1312-6273 | Parallel architectures for fuzzy triadic similarity learning | http://arxiv.org/abs/1312.6273 | id:1312.6273 author:Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui category:cs.DC cs.LG stat.ML  published:2013-12-21 summary:In a context of document co-clustering, we define a new similarity measure which iteratively computes similarity while combining fuzzy sets in a three-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal with uncertainty offers by the fuzzy sets. Moreover, with the development of the Web and the high availability of storage spaces, more and more documents become accessible. Documents can be provided from multiple sites and make similarity computation an expensive processing. This problem motivated us to use parallel computing. In this paper, we introduce parallel architectures which are able to treat large and multi-source data sets by a sequential, a merging or a splitting-based process. Then, we proceed to a local and a central (or global) computing using the basic FT-Sim measure. The idea behind these architectures is to reduce both time and space complexities thanks to parallel computation. version:1
arxiv-1212-0655 | G-invariant Persistent Homology | http://arxiv.org/abs/1212.0655 | id:1212.0655 author:Patrizio Frosini category:math.AT cs.CG cs.CV I.4.7; I.5.1  published:2012-12-04 summary:Classical persistent homology is a powerful mathematical tool for shape comparison. Unfortunately, it is not tailored to study the action of transformation groups that are different from the group Homeo(X) of all self-homeomorphisms of a topological space X. This fact restricts its use in applications. In order to obtain better lower bounds for the natural pseudo-distance d_G associated with a subgroup G of Homeo(X), we need to adapt persistent homology and consider G-invariant persistent homology. Roughly speaking, the main idea consists in defining persistent homology by means of a set of chains that is invariant under the action of G. In this paper we formalize this idea, and prove the stability of the persistent Betti number functions in G-invariant persistent homology with respect to the natural pseudo-distance d_G. We also show how G-invariant persistent homology could be used in applications concerning shape comparison, when the invariance group is a proper subgroup of the group of all self-homeomorphisms of a topological space. In this paper we will assume that the space X is triangulable, in order to guarantee that the persistent Betti number functions are finite without using any tameness assumption. version:5
arxiv-1312-6219 | Extracting Region of Interest for Palm Print Authentication | http://arxiv.org/abs/1312.6219 | id:1312.6219 author:Kasturika B. Ray category:cs.CV  published:2013-12-21 summary:Biometrics authentication is an effective method for automatically recognizing individuals. The authentication consists of an enrollment phase and an identification or verification phase. In the stages of enrollment known (training) samples after the pre-processing stage are used for suitable feature extraction to generate the template database. In the verification stage, the test sample is similarly pre processed and subjected to feature extraction modules, and then it is matched with the training feature templates to decide whether it is a genuine or not. This paper presents use of a region of interest (ROI) for palm print technology. First some of the existing methods for palm print identification have been introduced. Then focus has been given on extraction of a suitable smaller region from the acquired palm print to improve the identification method accuracy. Several existing work in the topic of region extraction have been examined. Subsequently, a simple and original method has then proposed for locating the ROI that can be effectively used for palm print analysis. The ROI extracted using this new technique is suitable for different types of processing as it creates a rectangular or square area around the center of activity represented by the lines, wrinkles and ridges of the palm print. The effectiveness of the ROI approach has been tested by integrating it with a texture based identification / authentication system proposed earlier. The improvement has been shown by comparing the identification accuracy rate before and after the ROI pre-processing. version:1
arxiv-1312-6208 | Total variation with overlapping group sparsity for image deblurring under impulse noise | http://arxiv.org/abs/1312.6208 | id:1312.6208 author:Gang Liu, Ting-Zhu Huang, Jun Liu, Xiao-Guang Lv category:math.NA cs.CV  published:2013-12-21 summary:The total variation (TV) regularization method is an effective method for image deblurring in preserving edges. However, the TV based solutions usually have some staircase effects. In this paper, in order to alleviate the staircase effect, we propose a new model for restoring blurred images with impulse noise. The model consists of an $\ell_1$-fidelity term and a TV with overlapping group sparsity (OGS) regularization term. Moreover, we impose a box constraint to the proposed model for getting more accurate solutions. An efficient and effective algorithm is proposed to solve the model under the framework of the alternating direction method of multipliers (ADMM). We use an inner loop which is nested inside the majorization minimization (MM) iteration for the subproblem of the proposed method. Compared with other methods, numerical results illustrate that the proposed method, can significantly improve the restoration quality, both in avoiding staircase effects and in terms of peak signal-to-noise ratio (PSNR) and relative error (ReE). version:1
arxiv-1312-6186 | GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training | http://arxiv.org/abs/1312.6186 | id:1312.6186 author:Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang category:cs.CV cs.DC cs.LG cs.NE  published:2013-12-21 summary:The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time. version:1
arxiv-1312-4740 | Learning High-level Image Representation for Image Retrieval via Multi-Task DNN using Clickthrough Data | http://arxiv.org/abs/1312.4740 | id:1312.4740 author:Yalong Bai, Kuiyuan Yang, Wei Yu, Wei-Ying Ma, Tiejun Zhao category:cs.CV  published:2013-12-17 summary:Image retrieval refers to finding relevant images from an image database for a query, which is considered difficult for the gap between low-level representation of images and high-level representation of queries. Recently further developed Deep Neural Network sheds light on automatically learning high-level image representation from raw pixels. In this paper, we proposed a multi-task DNN learned for image retrieval, which contains two parts, i.e., query-sharing layers for image representation computation and query-specific layers for relevance estimation. The weights of multi-task DNN are learned on clickthrough data by Ring Training. Experimental results on both simulated and real dataset show the effectiveness of the proposed method. version:2
arxiv-1312-6182 | Large-Scale Paralleled Sparse Principal Component Analysis | http://arxiv.org/abs/1312.6182 | id:1312.6182 author:W. Liu, H. Zhang, D. Tao, Y. Wang, K. Lu category:cs.MS cs.LG cs.NA stat.ML  published:2013-12-21 summary:Principal component analysis (PCA) is a statistical technique commonly used in multivariate data analysis. However, PCA can be difficult to interpret and explain since the principal components (PCs) are linear combinations of the original variables. Sparse PCA (SPCA) aims to balance statistical fidelity and interpretability by approximating sparse PCs whose projections capture the maximal variance of original data. In this paper we present an efficient and paralleled method of SPCA using graphics processing units (GPUs), which can process large blocks of data in parallel. Specifically, we construct parallel implementations of the four optimization formulations of the generalized power method of SPCA (GP-SPCA), one of the most efficient and effective SPCA approaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS) is up to eleven times faster than the corresponding CPU implementation (using CBLAS), and up to 107 times faster than a MatLab implementation. Extensive comparative experiments in several real-world datasets confirm that SPCA offers a practical advantage. version:1
arxiv-1312-6180 | Manifold regularized kernel logistic regression for web image annotation | http://arxiv.org/abs/1312.6180 | id:1312.6180 author:W. Liu, H. Liu, D. Tao, Y. Wang, K. Lu category:cs.LG cs.MM  published:2013-12-21 summary:With the rapid advance of Internet technology and smart devices, users often need to manage large amounts of multimedia information using smart devices, such as personal image and video accessing and browsing. These requirements heavily rely on the success of image (video) annotation, and thus large scale image annotation through innovative machine learning methods has attracted intensive attention in recent years. One representative work is support vector machine (SVM). Although it works well in binary classification, SVM has a non-smooth loss function and can not naturally cover multi-class case. In this paper, we propose manifold regularized kernel logistic regression (KLR) for web image annotation. Compared to SVM, KLR has the following advantages: (1) the KLR has a smooth loss function; (2) the KLR produces an explicit estimate of the probability instead of class label; and (3) the KLR can naturally be generalized to the multi-class case. We carefully conduct experiments on MIR FLICKR dataset and demonstrate the effectiveness of manifold regularized kernel logistic regression for image annotation. version:1
arxiv-1312-6159 | Learned versus Hand-Designed Feature Representations for 3d Agglomeration | http://arxiv.org/abs/1312.6159 | id:1312.6159 author:John A. Bogovic, Gary B. Huang, Viren Jain category:cs.CV  published:2013-12-20 summary:For image recognition and labeling tasks, recent results suggest that machine learning methods that rely on manually specified feature representations may be outperformed by methods that automatically derive feature representations based on the data. Yet for problems that involve analysis of 3d objects, such as mesh segmentation, shape retrieval, or neuron fragment agglomeration, there remains a strong reliance on hand-designed feature descriptors. In this paper, we evaluate a large set of hand-designed 3d feature descriptors alongside features learned from the raw data using both end-to-end and unsupervised learning techniques, in the context of agglomeration of 3d neuron fragments. By combining unsupervised learning techniques with a novel dynamic pooling scheme, we show how pure learning-based methods are for the first time competitive with hand-designed 3d shape descriptors. We investigate data augmentation strategies for dramatically increasing the size of the training set, and show how combining both learned and hand-designed features leads to the highest accuracy. version:1
arxiv-1312-6086 | The return of AdaBoost.MH: multi-class Hamming trees | http://arxiv.org/abs/1312.6086 | id:1312.6086 author:Balázs Kégl category:cs.LG  published:2013-12-20 summary:Within the framework of AdaBoost.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem to $K$ binary one-against-all classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary AdaBoost. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLogitBoost, and it is significantly better than other known implementations of AdaBoost.MH. version:1
arxiv-1312-5697 | Using Web Co-occurrence Statistics for Improving Image Categorization | http://arxiv.org/abs/1312.5697 | id:1312.5697 author:Samy Bengio, Jeff Dean, Dumitru Erhan, Eugene Ie, Quoc Le, Andrew Rabinovich, Jonathon Shlens, Yoram Singer category:cs.CV cs.LG  published:2013-12-19 summary:Object recognition and localization are important tasks in computer vision. The focus of this work is the incorporation of contextual information in order to improve object recognition and localization. For instance, it is natural to expect not to see an elephant to appear in the middle of an ocean. We consider a simple approach to encapsulate such common sense knowledge using co-occurrence statistics from web documents. By merely counting the number of times nouns (such as elephants, sharks, oceans, etc.) co-occur in web documents, we obtain a good estimate of expected co-occurrences in visual data. We then cast the problem of combining textual co-occurrence statistics with the predictions of image-based classifiers as an optimization problem. The resulting optimization problem serves as a surrogate for our inference procedure. Albeit the simplicity of the resulting optimization problem, it is effective in improving both recognition and localization accuracy. Concretely, we observe significant improvements in recognition and localization rates for both ImageNet Detection 2012 and Sun 2012 datasets. version:2
arxiv-1312-6024 | Occupancy Detection in Vehicles Using Fisher Vector Image Representation | http://arxiv.org/abs/1312.6024 | id:1312.6024 author:Yusuf Artan, Peter Paul category:cs.CV  published:2013-12-20 summary:Due to the high volume of traffic on modern roadways, transportation agencies have proposed High Occupancy Vehicle (HOV) lanes and High Occupancy Tolling (HOT) lanes to promote car pooling. However, enforcement of the rules of these lanes is currently performed by roadside enforcement officers using visual observation. Manual roadside enforcement is known to be inefficient, costly, potentially dangerous, and ultimately ineffective. Violation rates up to 50%-80% have been reported, while manual enforcement rates of less than 10% are typical. Therefore, there is a need for automated vehicle occupancy detection to support HOV/HOT lane enforcement. A key component of determining vehicle occupancy is to determine whether or not the vehicle's front passenger seat is occupied. In this paper, we examine two methods of determining vehicle front seat occupancy using a near infrared (NIR) camera system pointed at the vehicle's front windshield. The first method examines a state-of-the-art deformable part model (DPM) based face detection system that is robust to facial pose. The second method examines state-of- the-art local aggregation based image classification using bag-of-visual-words (BOW) and Fisher vectors (FV). A dataset of 3000 images was collected on a public roadway and is used to perform the comparison. From these experiments it is clear that the image classification approach is superior for this problem. version:1
arxiv-1308-2302 | High-Dimensional Regression with Gaussian Mixtures and Partially-Latent Response Variables | http://arxiv.org/abs/1308.2302 | id:1308.2302 author:Antoine Deleforge, Florence Forbes, Radu Horaud category:cs.LG stat.ML  published:2013-08-10 summary:In this work we address the problem of approximating high-dimensional data with a low-dimensional representation. We make the following contributions. We propose an inverse regression method which exchanges the roles of input and response, such that the low-dimensional variable becomes the regressor, and which is tractable. We introduce a mixture of locally-linear probabilistic mapping model that starts with estimating the parameters of inverse regression, and follows with inferring closed-form solutions for the forward parameters of the high-dimensional regression problem of interest. Moreover, we introduce a partially-latent paradigm, such that the vector-valued response variable is composed of both observed and latent entries, thus being able to deal with data contaminated by experimental artifacts that cannot be explained with noise models. The proposed probabilistic formulation could be viewed as a latent-variable augmentation of regression. We devise expectation-maximization (EM) procedures based on a data augmentation strategy which facilitates the maximum-likelihood search over the model parameters. We propose two augmentation schemes and we describe in detail the associated EM inference procedures that may well be viewed as generalizations of a number of EM regression, dimension reduction, and factor analysis algorithms. The proposed framework is validated with both synthetic and real data. We provide experimental evidence that our method outperforms several existing regression techniques. version:3
arxiv-1312-5946 | Simple Methods for Initializing the EM Algorithm for Gaussian Mixture Models | http://arxiv.org/abs/1312.5946 | id:1312.5946 author:Johannes Blömer, Kathrin Bujna category:cs.LG  published:2013-12-20 summary:In this paper, we consider simple and fast approaches to initialize the Expectation-Maximization algorithm (EM) for multivariate Gaussian mixture models. We present new initialization methods based on the well-known $K$-means++ algorithm and the Gonzalez algorithm. These methods close the gap between simple uniform initialization techniques and complex methods, that have been specifically designed for Gaussian mixture models and depend on the right choice of hyperparameters. In our evaluation we compare our methods with a commonly used random initialization method, an approach based on agglomerative hierarchical clustering, and a known, plain adaption of the Gonzalez algorithm. Our results indicate that algorithms based on $K$-means++ outperform the other methods. version:1
arxiv-1312-5891 | The Sparse Principal Component of a Constant-rank Matrix | http://arxiv.org/abs/1312.5891 | id:1312.5891 author:Megasthenis Asteris, Dimitris S. Papailiopoulos, George N. Karystinos category:cs.IT math.IT stat.ML  published:2013-12-20 summary:The computation of the sparse principal component of a matrix is equivalent to the identification of its principal submatrix with the largest maximum eigenvalue. Finding this optimal submatrix is what renders the problem ${\mathcal{NP}}$-hard. In this work, we prove that, if the matrix is positive semidefinite and its rank is constant, then its sparse principal component is polynomially computable. Our proof utilizes the auxiliary unit vector technique that has been recently developed to identify problems that are polynomially solvable. Moreover, we use this technique to design an algorithm which, for any sparsity value, computes the sparse principal component with complexity ${\mathcal O}\left(N^{D+1}\right)$, where $N$ and $D$ are the matrix size and rank, respectively. Our algorithm is fully parallelizable and memory efficient. version:1
arxiv-1312-5889 | Non-parametric Bayesian modeling of complex networks | http://arxiv.org/abs/1312.5889 | id:1312.5889 author:Mikkel N. Schmidt, Morten Mørup category:stat.ML  published:2013-12-20 summary:Modeling structure in complex networks using Bayesian non-parametrics makes it possible to specify flexible model structures and infer the adequate model complexity from the observed data. This paper provides a gentle introduction to non-parametric Bayesian modeling of complex networks: Using an infinite mixture model as running example we go through the steps of deriving the model as an infinite limit of a finite parametric model, inferring the model parameters by Markov chain Monte Carlo, and checking the model's fit and predictive performance. We explain how advanced non-parametric models for complex networks can be derived and point out relevant literature. version:1
arxiv-1312-5814 | Optimal parameter selection for unsupervised neural network using genetic algorithm | http://arxiv.org/abs/1312.5814 | id:1312.5814 author:suneetha chittineni, Raveendra Babu Bhogapathi category:cs.NE  published:2013-12-20 summary:K-means Fast Learning Artificial Neural Network (K-FLANN) is an unsupervised neural network requires two parameters: tolerance and vigilance. Best Clustering results are feasible only by finest parameters specified to the neural network. Selecting optimal values for these parameters is a major problem. To solve this issue, Genetic Algorithm (GA) is used to determine optimal parameters of K-FLANN for finding groups in multidimensional data. K-FLANN is a simple topological network, in which output nodes grows dynamically during the clustering process on receiving input patterns. Original K-FLANN is enhanced to select winner unit out of the matched nodes so that stable clusters are formed with in a less number of epochs. The experimental results show that the GA is efficient in finding optimal values of parameters from the large search space and is tested using artificial and synthetic data sets. version:1
arxiv-1312-5783 | Unsupervised Feature Learning by Deep Sparse Coding | http://arxiv.org/abs/1312.5783 | id:1312.5783 author:Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi category:cs.LG cs.CV cs.NE  published:2013-12-20 summary:In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks. version:1
arxiv-1312-5734 | Time-varying Learning and Content Analytics via Sparse Factor Analysis | http://arxiv.org/abs/1312.5734 | id:1312.5734 author:Andrew S. Lan, Christoph Studer, Richard G. Baraniuk category:stat.ML cs.LG math.OC stat.AP  published:2013-12-19 summary:We propose SPARFA-Trace, a new machine learning-based framework for time-varying learning and content analytics for education applications. We develop a novel message passing-based, blind, approximate Kalman filter for sparse factor analysis (SPARFA), that jointly (i) traces learner concept knowledge over time, (ii) analyzes learner concept knowledge state transitions (induced by interacting with learning resources, such as textbook sections, lecture videos, etc, or the forgetting effect), and (iii) estimates the content organization and intrinsic difficulty of the assessment questions. These quantities are estimated solely from binary-valued (correct/incorrect) graded learner response data and a summary of the specific actions each learner performs (e.g., answering a question or studying a learning resource) at each time instance. Experimental results on two online course datasets demonstrate that SPARFA-Trace is capable of tracing each learner's concept knowledge evolution over time, as well as analyzing the quality and content organization of learning resources, the question-concept associations, and the question intrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable or better performance in predicting unobserved learner responses than existing collaborative filtering and knowledge tracing approaches for personalized education. version:1
arxiv-1312-5673 | Flower Pollination Algorithm for Global Optimization | http://arxiv.org/abs/1312.5673 | id:1312.5673 author:Xin-She Yang category:math.OC cs.NE nlin.AO  published:2013-12-19 summary:Flower pollination is an intriguing process in the natural world. Its evolutionary characteristics can be used to design new optimization algorithms. In this paper, we propose a new algorithm, namely, flower pollination algorithm, inspired by the pollination process of flowers. We first use ten test functions to validate the new algorithm, and compare its performance with genetic algorithms and particle swarm optimization. Our simulation results show the flower algorithm is more efficient than both GA and PSO. We also use the flower algorithm to solve a nonlinear design benchmark, which shows the convergence rate is almost exponential. version:1
arxiv-1312-4353 | Abstraction in decision-makers with limited information processing capabilities | http://arxiv.org/abs/1312.4353 | id:1312.4353 author:Tim Genewein, Daniel A. Braun category:cs.AI cs.IT math.IT stat.ML  published:2013-12-16 summary:A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems abstractions are often implemented through computationally costly formations of groups or clusters. In this work we establish the relation between the free-energy framework for decision making and rate-distortion theory and demonstrate how the application of rate-distortion for decision-making leads to the emergence of abstractions. We argue that abstractions are induced due to a limit in information processing capacity. version:2
arxiv-1312-5602 | Playing Atari with Deep Reinforcement Learning | http://arxiv.org/abs/1312.5602 | id:1312.5602 author:Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller category:cs.LG  published:2013-12-19 summary:We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them. version:1
arxiv-1312-5568 | An Adaptive Dictionary Learning Approach for Modeling Dynamical Textures | http://arxiv.org/abs/1312.5568 | id:1312.5568 author:Xian Wei, Hao Shen, Martin Kleinsteuber category:cs.CV  published:2013-12-19 summary:Video representation is an important and challenging task in the computer vision community. In this paper, we assume that image frames of a moving scene can be modeled as a Linear Dynamical System. We propose a sparse coding framework, named adaptive video dictionary learning (AVDL), to model a video adaptively. The developed framework is able to capture the dynamics of a moving scene by exploring both sparse properties and the temporal correlations of consecutive video frames. The proposed method is compared with state of the art video processing methods on several benchmark data sequences, which exhibit appearance changes and heavy occlusions. version:1
arxiv-1312-5548 | My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013 | http://arxiv.org/abs/1312.5548 | id:1312.5548 author:Jürgen Schmidhuber category:cs.NE  published:2013-12-19 summary:Deep Learning has attracted significant attention in recent years. Here I present a brief overview of my first Deep Learner of 1991, and its historic context, with a timeline of Deep Learning highlights. version:1
arxiv-1312-5129 | Deep Learning Embeddings for Discontinuous Linguistic Units | http://arxiv.org/abs/1312.5129 | id:1312.5129 author:Wenpeng Yin, Hinrich Schütze category:cs.CL  published:2013-12-18 summary:Deep learning embeddings have been successfully used for many natural language processing problems. Embeddings are mostly computed for word forms although a number of recent papers have extended this to other linguistic units like morphemes and phrases. In this paper, we argue that learning embeddings for discontinuous linguistic units should also be considered. In an experimental evaluation on coreference resolution, we show that such embeddings perform better than word form embeddings. version:2
arxiv-1312-5457 | Codebook based Audio Feature Representation for Music Information Retrieval | http://arxiv.org/abs/1312.5457 | id:1312.5457 author:Yonatan Vaizman, Brian McFee, Gert Lanckriet category:cs.IR cs.LG cs.MM  published:2013-12-19 summary:Digital music has become prolific in the web in recent decades. Automated recommendation systems are essential for users to discover music they love and for artists to reach appropriate audience. When manual annotations and user preference data is lacking (e.g. for new artists) these systems must rely on \emph{content based} methods. Besides powerful machine learning tools for classification and retrieval, a key component for successful recommendation is the \emph{audio content representation}. Good representations should capture informative musical patterns in the audio signal of songs. These representations should be concise, to enable efficient (low storage, easy indexing, fast search) management of huge music repositories, and should also be easy and fast to compute, to enable real-time interaction with a user supplying new songs to the system. Before designing new audio features, we explore the usage of traditional local features, while adding a stage of encoding with a pre-computed \emph{codebook} and a stage of pooling to get compact vectorial representations. We experiment with different encoding methods, namely \emph{the LASSO}, \emph{vector quantization (VQ)} and \emph{cosine similarity (CS)}. We evaluate the representations' quality in two music information retrieval applications: query-by-tag and query-by-example. Our results show that concise representations can be used for successful performance in both applications. We recommend using top-$\tau$ VQ encoding, which consistently performs well in both applications, and requires much less computation time than the LASSO. version:1
arxiv-1401-6126 | Delegating Custom Object Detection Tasks to a Universal Classification System | http://arxiv.org/abs/1401.6126 | id:1401.6126 author:Andrew Gleibman category:cs.CV 68T10  published:2013-12-19 summary:In this paper, a concept of multipurpose object detection system, recently introduced in our previous work, is clarified. The business aspect of this method is transformation of a classifier into an object detector/locator via an image grid. This is a universal framework for locating objects of interest through classification. The framework standardizes and simplifies implementation of custom systems by doing only a custom analysis of the classification results on the image grid. version:1
arxiv-1312-5402 | Some Improvements on Deep Convolutional Neural Network Based Image Classification | http://arxiv.org/abs/1312.5402 | id:1312.5402 author:Andrew G. Howard category:cs.CV  published:2013-12-19 summary:We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner. version:1
arxiv-1312-5394 | Missing Value Imputation With Unsupervised Backpropagation | http://arxiv.org/abs/1312.5394 | id:1312.5394 author:Michael S. Gashler, Michael R. Smith, Richard Morris, Tony Martinez category:cs.NE cs.LG stat.ML  published:2013-12-19 summary:Many data mining and data analysis techniques operate on dense matrices or complete tables of data. Real-world data sets, however, often contain unknown values. Even many classification algorithms that are designed to operate with missing values still exhibit deteriorated accuracy. One approach to handling missing values is to fill in (impute) the missing values. In this paper, we present a technique for unsupervised learning called Unsupervised Backpropagation (UBP), which trains a multi-layer perceptron to fit to the manifold sampled by a set of observed point-vectors. We evaluate UBP with the task of imputing missing values in datasets, and show that UBP is able to predict missing values with significantly lower sum-squared error than other collaborative filtering and imputation techniques. We also demonstrate with 24 datasets and 9 supervised learning algorithms that classification accuracy is usually higher when randomly-withheld values are imputed using UBP, rather than with other methods. version:1
arxiv-1312-5386 | Detecting Parameter Symmetries in Probabilistic Models | http://arxiv.org/abs/1312.5386 | id:1312.5386 author:Robert Nishihara, Thomas Minka, Daniel Tarlow category:stat.ML  published:2013-12-19 summary:Probabilistic models often have parameters that can be translated, scaled, permuted, or otherwise transformed without changing the model. These symmetries can lead to strong correlation and multimodality in the posterior distribution over the model's parameters, which can pose challenges both for performing inference and interpreting the results. In this work, we address the automatic detection of common problematic model symmetries. To do so, we introduce local symmetries, which cover many common cases and are amenable to automatic detection. We show how to derive algorithms to detect several broad classes of local symmetries. Our algorithms are compatible with probabilistic programming constructs such as arrays, for loops, and if statements, and they scale to models with many variables. version:1
arxiv-1312-5370 | Perturbed Gibbs Samplers for Synthetic Data Release | http://arxiv.org/abs/1312.5370 | id:1312.5370 author:Yubin Park, Joydeep Ghosh category:stat.ML stat.AP  published:2013-12-18 summary:We propose a categorical data synthesizer with a quantifiable disclosure risk. Our algorithm, named Perturbed Gibbs Sampler, can handle high-dimensional categorical data that are often intractable to represent as contingency tables. The algorithm extends a multiple imputation strategy for fully synthetic data by utilizing feature hashing and non-parametric distribution approximations. California Patient Discharge data are used to demonstrate statistical properties of the proposed synthesizing methodology. Marginal and conditional distributions, as well as the coefficients of regression models built on the synthesized data are compared to those obtained from the original data. Intruder scenarios are simulated to evaluate disclosure risks of the synthesized data from multiple angles. Limitations and extensions of the proposed algorithm are also discussed. version:1
arxiv-1312-5355 | Generative NeuroEvolution for Deep Learning | http://arxiv.org/abs/1312.5355 | id:1312.5355 author:Phillip Verbancsics, Josh Harguess category:cs.NE cs.CV  published:2013-12-18 summary:An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature. version:1
arxiv-1312-5753 | SOMz: photometric redshift PDFs with self organizing maps and random atlas | http://arxiv.org/abs/1312.5753 | id:1312.5753 author:M. Carrasco Kind, R. J. Brunner category:astro-ph.IM astro-ph.CO cs.LG stat.ML  published:2013-12-18 summary:In this paper we explore the applicability of the unsupervised machine learning technique of Self Organizing Maps (SOM) to estimate galaxy photometric redshift probability density functions (PDFs). This technique takes a spectroscopic training set, and maps the photometric attributes, but not the redshifts, to a two dimensional surface by using a process of competitive learning where neurons compete to more closely resemble the training data multidimensional space. The key feature of a SOM is that it retains the topology of the input set, revealing correlations between the attributes that are not easily identified. We test three different 2D topological mapping: rectangular, hexagonal, and spherical, by using data from the DEEP2 survey. We also explore different implementations and boundary conditions on the map and also introduce the idea of a random atlas where a large number of different maps are created and their individual predictions are aggregated to produce a more robust photometric redshift PDF. We also introduced a new metric, the $I$-score, which efficiently incorporates different metrics, making it easier to compare different results (from different parameters or different photometric redshift codes). We find that by using a spherical topology mapping we obtain a better representation of the underlying multidimensional topology, which provides more accurate results that are comparable to other, state-of-the-art machine learning algorithms. Our results illustrate that unsupervised approaches have great potential for many astronomical problems, and in particular for the computation of photometric redshifts. version:1
arxiv-1312-5271 | Systematic and multifactor risk models revisited | http://arxiv.org/abs/1312.5271 | id:1312.5271 author:Michel Fliess, Cédric Join category:q-fin.RM cs.CE math.LO q-fin.CP stat.ML  published:2013-12-18 summary:Systematic and multifactor risk models are revisited via methods which were already successfully developed in signal processing and in automatic control. The results, which bypass the usual criticisms on those risk modeling, are illustrated by several successful computer experiments. version:1
arxiv-1208-4391 | Shape Tracking With Occlusions via Coarse-To-Fine Region-Based Sobolev Descent | http://arxiv.org/abs/1208.4391 | id:1208.4391 author:Yanchao Yang, Ganesh Sundaramoorthi category:cs.CV cs.SY  published:2012-08-21 summary:We present a method to track the precise shape of an object in video based on new modeling and optimization on a new Riemannian manifold of parameterized regions. Joint dynamic shape and appearance models, in which a template of the object is propagated to match the object shape and radiance in the next frame, are advantageous over methods employing global image statistics in cases of complex object radiance and cluttered background. In cases of 3D object motion and viewpoint change, self-occlusions and dis-occlusions of the object are prominent, and current methods employing joint shape and appearance models are unable to adapt to new shape and appearance information, leading to inaccurate shape detection. In this work, we model self-occlusions and dis-occlusions in a joint shape and appearance tracking framework. Self-occlusions and the warp to propagate the template are coupled, thus a joint problem is formulated. We derive a coarse-to-fine optimization scheme, advantageous in object tracking, that initially perturbs the template by coarse perturbations before transitioning to finer-scale perturbations, traversing all scales, seamlessly and automatically. The scheme is a gradient descent on a novel infinite-dimensional Riemannian manifold that we introduce. The manifold consists of planar parameterized regions, and the metric that we introduce is a novel Sobolev-type metric defined on infinitesimal vector fields on regions. The metric has the property of resulting in a gradient descent that automatically favors coarse-scale deformations (when they reduce the energy) before moving to finer-scale deformations. Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and background show that occlusion/dis-occlusion modeling leads to superior shape accuracy compared to recent methods employing joint shape/appearance models or employing global statistics. version:2
arxiv-1310-8203 | A U-statistic estimator for the variance of resampling-based error estimators | http://arxiv.org/abs/1310.8203 | id:1310.8203 author:Mathias Fuchs, Roman Hornung, Riccardo De Bin, Anne-Laure Boulesteix category:math.ST stat.ML stat.TH  published:2013-10-30 summary:We revisit resampling procedures for error estimation in binary classification in terms of U-statistics. In particular, we exploit the fact that the error rate estimator involving all learning-testing splits is a U-statistic. Thus, it has minimal variance among all unbiased estimators and is asymptotically normally distributed. Moreover, there is an unbiased estimator for this minimal variance if the total sample size is at least the double learning set size plus two. In this case, we exhibit such an estimator which is another U-statistic. It enjoys, again, various optimality properties and yields an asymptotically exact hypothesis test of the equality of error rates when two learning algorithms are compared. Our statements apply to any deterministic learning algorithms under weak non-degeneracy assumptions. version:2
arxiv-1312-5179 | The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited | http://arxiv.org/abs/1312.5179 | id:1312.5179 author:Matthias Hein, Simon Setzer, Leonardo Jost, Syama Sundar Rangapuram category:stat.ML cs.LG math.OC  published:2013-12-18 summary:Hypergraphs allow one to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or on tensor methods which are only applicable under special conditions. In this paper, we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element is a family of regularization functionals based on the total variation on hypergraphs. version:1
arxiv-1312-5124 | Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the Score Matrix | http://arxiv.org/abs/1312.5124 | id:1312.5124 author:Paul Fogel category:stat.AP cs.LG stat.ML  published:2013-12-18 summary:Non-Negative Matrix Factorization, NMF, attempts to find a number of archetypal response profiles, or parts, such that any sample profile in the dataset can be approximated by a close profile among these archetypes or a linear combination of these profiles. The non-negativity constraint is imposed while estimating archetypal profiles, due to the non-negative nature of the observed signal. Apart from non negativity, a volume constraint can be applied on the Score matrix W to enhance the ability of learning parts of NMF. In this report, we describe a very simple algorithm, which in effect achieves volume minimization, although indirectly. version:1
arxiv-1205-3109 | Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search | http://arxiv.org/abs/1205.3109 | id:1205.3109 author:Arthur Guez, David Silver, Peter Dayan category:cs.LG cs.AI stat.ML  published:2012-05-14 summary:Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration. version:4
arxiv-1312-5066 | Functional Bipartite Ranking: a Wavelet-Based Filtering Approach | http://arxiv.org/abs/1312.5066 | id:1312.5066 author:Stéphan Clémençon, Marine Depecker category:stat.ML  published:2013-12-18 summary:It is the main goal of this article to address the bipartite ranking issue from the perspective of functional data analysis (FDA). Given a training set of independent realizations of a (possibly sampled) second-order random function with a (locally) smooth autocorrelation structure and to which a binary label is randomly assigned, the objective is to learn a scoring function s with optimal ROC curve. Based on linear/nonlinear wavelet-based approximations, it is shown how to select compact finite dimensional representations of the input curves adaptively, in order to build accurate ranking rules, using recent advances in the ranking problem for multivariate data with binary feedback. Beyond theoretical considerations, the performance of the learning methods for functional bipartite ranking proposed in this paper are illustrated by numerical experiments. version:1
arxiv-1312-5045 | Comparative analysis of evolutionary algorithms for image enhancement | http://arxiv.org/abs/1312.5045 | id:1312.5045 author:Anupriya Gogna, Akash Tayal category:cs.CV cs.NE  published:2013-12-18 summary:Evolutionary algorithms are metaheuristic techniques that derive inspiration from the natural process of evolution. They can efficiently solve (generate acceptable quality of solution in reasonable time) complex optimization (NP-Hard) problems. In this paper, automatic image enhancement is considered as an optimization problem and three evolutionary algorithms (Genetic Algorithm, Differential Evolution and Self Organizing Migration Algorithm) are employed to search for an optimum solution. They are used to find an optimum parameter set for an image enhancement transfer function. The aim is to maximize a fitness criterion which is a measure of image contrast and the visibility of details in the enhanced image. The enhancement results obtained using all three evolutionary algorithms are compared amongst themselves and also with the output of histogram equalization method. version:1
arxiv-1312-5033 | Evaluation of Plane Detection with RANSAC According to Density of 3D Point Clouds | http://arxiv.org/abs/1312.5033 | id:1312.5033 author:Tomofumi Fujiwara, Tetsushi Kamegawa, Akio Gofuku category:cs.RO cs.CV  published:2013-12-18 summary:We have implemented a method that detects planar regions from 3D scan data using Random Sample Consensus (RANSAC) algorithm to address the issue of a trade-off between the scanning speed and the point density of 3D scanning. However, the limitation of the implemented method has not been clear yet. In this paper, we conducted an additional experiment to evaluate the implemented method by changing its parameter and environments in both high and low point density data. As a result, the number of detected planes in high point density data was different from that in low point density data with the same parameter value. version:1
arxiv-1312-5023 | Contextually Supervised Source Separation with Application to Energy Disaggregation | http://arxiv.org/abs/1312.5023 | id:1312.5023 author:Matt Wytock, J. Zico Kolter category:stat.ML cs.LG math.OC  published:2013-12-18 summary:We propose a new framework for single-channel source separation that lies between the fully supervised and unsupervised setting. Instead of supervision, we provide input features for each source signal and use convex methods to estimate the correlations between these features and the unobserved signal decomposition. We analyze the case of $\ell_2$ loss theoretically and show that recovery of the signal components depends only on cross-correlation between features for different signals, not on correlations between features for the same signal. Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision; our motivating application is energy disaggregation of hourly smart meter data (the separation of whole-home power signals into different energy uses). Here we apply contextual supervision to disaggregate the energy usage of thousands homes over four years, a significantly larger scale than previously published efforts, and demonstrate on synthetic data that our method outperforms the unsupervised approach. version:1
arxiv-1312-5021 | Efficient Online Bootstrapping for Large Scale Learning | http://arxiv.org/abs/1312.5021 | id:1312.5021 author:Zhen Qin, Vaclav Petricek, Nikos Karampatziakis, Lihong Li, John Langford category:cs.LG  published:2013-12-18 summary:Bootstrapping is a useful technique for estimating the uncertainty of a predictor, for example, confidence intervals for prediction. It is typically used on small to moderate sized datasets, due to its high computation cost. This work describes a highly scalable online bootstrapping strategy, implemented inside Vowpal Wabbit, that is several times faster than traditional strategies. Our experiments indicate that, in addition to providing a black box-like method for estimating uncertainty, our implementation of online bootstrapping may also help to train models with better prediction performance due to model averaging. version:1
arxiv-1312-4986 | A Comparative Evaluation of Curriculum Learning with Filtering and Boosting | http://arxiv.org/abs/1312.4986 | id:1312.4986 author:Michael R. Smith, Tony Martinez category:cs.LG  published:2013-12-17 summary:Not all instances in a data set are equally beneficial for inferring a model of the data. Some instances (such as outliers) are detrimental to inferring a model of the data. Several machine learning techniques treat instances in a data set differently during training such as curriculum learning, filtering, and boosting. However, an automated method for determining how beneficial an instance is for inferring a model of the data does not exist. In this paper, we present an automated method that orders the instances in a data set by complexity based on the their likelihood of being misclassified (instance hardness). The underlying assumption of this method is that instances with a high likelihood of being misclassified represent more complex concepts in a data set. Ordering the instances in a data set allows a learning algorithm to focus on the most beneficial instances and ignore the detrimental ones. We compare ordering the instances in a data set in curriculum learning, filtering and boosting. We find that ordering the instances significantly increases classification accuracy and that filtering has the largest impact on classification accuracy. On a set of 52 data sets, ordering the instances increases the average accuracy from 81% to 84%. version:1
arxiv-1312-4895 | Recursive Compressed Sensing | http://arxiv.org/abs/1312.4895 | id:1312.4895 author:Nikolaos M. Freris, Orhan Öçal, Martin Vetterli category:stat.ML cs.IT math.IT 94  published:2013-12-17 summary:We introduce a recursive algorithm for performing compressed sensing on streaming data. The approach consists of a) recursive encoding, where we sample the input stream via overlapping windowing and make use of the previous measurement in obtaining the next one, and b) recursive decoding, where the signal estimate from the previous window is utilized in order to achieve faster convergence in an iterative optimization scheme applied to decode the new one. To remove estimation bias, a two-step estimation procedure is proposed comprising support set detection and signal amplitude estimation. Estimation accuracy is enhanced by a non-linear voting method and averaging estimates over multiple windows. We analyze the computational complexity and estimation error, and show that the normalized error variance asymptotically goes to zero for sublinear sparsity. Our simulation results show speed up of an order of magnitude over traditional CS, while obtaining significantly lower reconstruction error under mild conditions on the signal magnitudes and the noise level. version:1
arxiv-1312-4852 | Identification of Gaussian Process State-Space Models with Particle Stochastic Approximation EM | http://arxiv.org/abs/1312.4852 | id:1312.4852 author:Roger Frigola, Fredrik Lindsten, Thomas B. Schön, Carl E. Rasmussen category:stat.ML cs.SY  published:2013-12-17 summary:Gaussian process state-space models (GP-SSMs) are a very flexible family of models of nonlinear dynamical systems. They comprise a Bayesian nonparametric representation of the dynamics of the system and additional (hyper-)parameters governing the properties of this nonparametric representation. The Bayesian formalism enables systematic reasoning about the uncertainty in the system dynamics. We present an approach to maximum likelihood identification of the parameters in GP-SSMs, while retaining the full nonparametric description of the dynamics. The method is based on a stochastic approximation version of the EM algorithm that employs recent developments in particle Markov chain Monte Carlo for efficient identification. version:1
arxiv-1306-2861 | Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC | http://arxiv.org/abs/1306.2861 | id:1306.2861 author:Roger Frigola, Fredrik Lindsten, Thomas B. Schön, Carl E. Rasmussen category:stat.ML cs.LG cs.SY  published:2013-06-12 summary:State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference \emph{and learning} (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. version:2
arxiv-1312-6872 | Matrix recovery using Split Bregman | http://arxiv.org/abs/1312.6872 | id:1312.6872 author:Anupriya Gogna, Ankita Shukla, Angshul Majumdar category:cs.NA cs.LG  published:2013-12-17 summary:In this paper we address the problem of recovering a matrix, with inherent low rank structure, from its lower dimensional projections. This problem is frequently encountered in wide range of areas including pattern recognition, wireless sensor networks, control systems, recommender systems, image/video reconstruction etc. Both in theory and practice, the most optimal way to solve the low rank matrix recovery problem is via nuclear norm minimization. In this paper, we propose a Split Bregman algorithm for nuclear norm minimization. The use of Bregman technique improves the convergence speed of our algorithm and gives a higher success rate. Also, the accuracy of reconstruction is much better even for cases where small number of linear measurements are available. Our claim is supported by empirical results obtained using our algorithm and its comparison to other existing methods for matrix recovery. The algorithms are compared on the basis of NMSE, execution time and success rate for varying ranks and sampling ratios. version:1
arxiv-1401-3615 | Performance Engineering for a Medical Imaging Application on the Intel Xeon Phi Accelerator | http://arxiv.org/abs/1401.3615 | id:1401.3615 author:Johannes Hofmann, Jan Treibig, Georg Hager, Gerhard Wellein category:cs.DC cs.CV cs.PF  published:2013-12-17 summary:We examine the Xeon Phi, which is based on Intel's Many Integrated Cores architecture, for its suitability to run the FDK algorithm--the most commonly used algorithm to perform the 3D image reconstruction in cone-beam computed tomography. We study the challenges of efficiently parallelizing the application and means to enable sensible data sharing between threads despite the lack of a shared last level cache. Apart from parallelization, SIMD vectorization is critical for good performance on the Xeon Phi; we perform various micro-benchmarks to investigate the platform's new set of vector instructions and put a special emphasis on the newly introduced vector gather capability. We refine a previous performance model for the application and adapt it for the Xeon Phi to validate the performance of our optimized hand-written assembly implementation, as well as the performance of several different auto-vectorization approaches. version:1
arxiv-1312-4752 | BW - Eye Ophthalmologic decision support system based on clinical workflow and data mining techniques-image registration algorithm | http://arxiv.org/abs/1312.4752 | id:1312.4752 author:Ricardo Martins category:cs.CV  published:2013-12-17 summary:Blueworks - Medical Expert Diagnosis is developing an application, BWEye, to be used as an ophthalmology consultation decision support system. The implementation of this application involves several different tasks and one of them is the implementation of an ophthalmology images registration algorithm. The work reported in this document is related with the implementation of an algorithm to register images of angiography, colour retinography and redfree retinography. The implementations described were developed in the software MATLAB. The implemented algorithm is based in the detection of the bifurcation points (y-features) of the vascular structures of the retina that usually are visible in the referred type of images. There are proposed two approaches to establish an initial set of features correspondences. The first approach is based in the maximization of the mutual information of the bifurcation regions of the features of images. The second approach is based in the characterization of each bifurcation point and in the minimization of the Euclidean distance between the descriptions of the features of the images in the descriptors space. The final set of the matching features for a pair of images is defined through the application of the RANSAC algorithm. Although, it was not achieved the implementation of a full functional algorithm, there were made several analysis that can be important to future improvement of the current implementation. version:1
arxiv-1312-4746 | Co-Sparse Textural Similarity for Image Segmentation | http://arxiv.org/abs/1312.4746 | id:1312.4746 author:Claudia Nieuwenhuis, Daniel Cremers, Simon Hawe, Martin Kleinsteuber category:cs.CV  published:2013-12-17 summary:We propose an algorithm for segmenting natural images based on texture and color information, which leverages the co-sparse analysis model for image segmentation within a convex multilabel optimization framework. As a key ingredient of this method, we introduce a novel textural similarity measure, which builds upon the co-sparse representation of image patches. We propose a Bayesian approach to merge textural similarity with information about color and location. Combined with recently developed convex multilabel optimization methods this leads to an efficient algorithm for both supervised and unsupervised segmentation, which is easily parallelized on graphics hardware. The approach provides competitive results in unsupervised segmentation and outperforms state-of-the-art interactive segmentation methods on the Graz Benchmark. version:1
arxiv-1312-4719 | The Bernstein Function: A Unifying Framework of Nonconvex Penalization in Sparse Estimation | http://arxiv.org/abs/1312.4719 | id:1312.4719 author:Zhihua Zhang category:stat.ML  published:2013-12-17 summary:In this paper we study nonconvex penalization using Bernstein functions. Since the Bernstein function is concave and nonsmooth at the origin, it can induce a class of nonconvex functions for high-dimensional sparse estimation problems. We derive a threshold function based on the Bernstein penalty and give its mathematical properties in sparsity modeling. We show that a coordinate descent algorithm is especially appropriate for penalized regression problems with the Bernstein penalty. Additionally, we prove that the Bernstein function can be defined as the concave conjugate of a $\varphi$-divergence and develop a conjugate maximization algorithm for finding the sparse solution. Finally, we particularly exemplify a family of Bernstein nonconvex penalties based on a generalized Gamma measure and conduct empirical analysis for this family. version:1
arxiv-1312-4717 | The Matrix Ridge Approximation: Algorithms and Applications | http://arxiv.org/abs/1312.4717 | id:1312.4717 author:Zhihua Zhang category:stat.ML  published:2013-12-17 summary:We are concerned with an approximation problem for a symmetric positive semidefinite matrix due to motivation from a class of nonlinear machine learning methods. We discuss an approximation approach that we call {matrix ridge approximation}. In particular, we define the matrix ridge approximation as an incomplete matrix factorization plus a ridge term. Moreover, we present probabilistic interpretations using a normal latent variable model and a Wishart model for this approximation approach. The idea behind the latent variable model in turn leads us to an efficient EM iterative method for handling the matrix ridge approximation problem. Finally, we illustrate the applications of the approximation approach in multivariate data analysis. Empirical studies in spectral clustering and Gaussian process regression show that the matrix ridge approximation with the EM iteration is potentially useful. version:1
arxiv-1312-3393 | Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem | http://arxiv.org/abs/1312.3393 | id:1312.3393 author:Masrour Zoghi, Shimon Whiteson, Remi Munos, Maarten de Rijke category:cs.LG  published:2013-12-12 summary:This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a finite-time regret bound of order O(log t). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art. version:2
arxiv-1312-4710 | Markov Network Structure Learning via Ensemble-of-Forests Models | http://arxiv.org/abs/1312.4710 | id:1312.4710 author:Eirini Arvaniti, Manfred Claassen category:stat.ML  published:2013-12-17 summary:Real world systems typically feature a variety of different dependency types and topologies that complicate model selection for probabilistic graphical models. We introduce the ensemble-of-forests model, a generalization of the ensemble-of-trees model. Our model enables structure learning of Markov random fields (MRF) with multiple connected components and arbitrary potentials. We present two approximate inference techniques for this model and demonstrate their performance on synthetic data. Our results suggest that the ensemble-of-forests approach can accurately recover sparse, possibly disconnected MRF topologies, even in presence of non-Gaussian dependencies and/or low sample size. We applied the ensemble-of-forests model to learn the structure of perturbed signaling networks of immune cells and found that these frequently exhibit non-Gaussian dependencies with disconnected MRF topologies. In summary, we expect that the ensemble-of-forests model will enable MRF structure learning in other high dimensional real world settings that are governed by non-trivial dependencies. version:1
arxiv-1312-4706 | Designing Spontaneous Speech Search Interface for Historical Archives | http://arxiv.org/abs/1312.4706 | id:1312.4706 author:Donna Vakharia, Rachel Gibbs category:cs.HC cs.CL  published:2013-12-17 summary:Spontaneous speech in the form of conversations, meetings, voice-mail, interviews, oral history, etc. is one of the most ubiquitous forms of human communication. Search engines providing access to such speech collections have the potential to better inform intelligence and make relevant data over vast audio/video archives available to users. This project presents a search user interface design supporting search tasks over a speech collection consisting of an historical archive with nearly 52,000 audiovisual testimonies of survivors and witnesses of the Holocaust and other genocides. The design incorporates faceted search, along with other UI elements like highlighted search items, tags, snippets, etc., to promote discovery and exploratory search. Two different designs have been created to support both manual and automated transcripts. Evaluation was performed using human subjects to measure accuracy in retrieving results, understanding user-perspective on the design elements, and ease of parsing information. version:1
arxiv-1110-4168 | Stable mixed graphs | http://arxiv.org/abs/1110.4168 | id:1110.4168 author:Kayvan Sadeghi category:stat.OT math.ST stat.ML stat.TH  published:2011-10-19 summary:In this paper, we study classes of graphs with three types of edges that capture the modified independence structure of a directed acyclic graph (DAG) after marginalisation over unobserved variables and conditioning on selection variables using the $m$-separation criterion. These include MC, summary, and ancestral graphs. As a modification of MC graphs, we define the class of ribbonless graphs (RGs) that permits the use of the $m$-separation criterion. RGs contain summary and ancestral graphs as subclasses, and each RG can be generated by a DAG after marginalisation and conditioning. We derive simple algorithms to generate RGs, from given DAGs or RGs, and also to generate summary and ancestral graphs in a simple way by further extension of the RG-generating algorithm. This enables us to develop a parallel theory on these three classes and to study the relationships between them as well as the use of each class. version:3
arxiv-1312-4626 | Compact Random Feature Maps | http://arxiv.org/abs/1312.4626 | id:1312.4626 author:Raffay Hamid, Ying Xiao, Alex Gittens, Dennis DeCoste category:stat.ML cs.LG  published:2013-12-17 summary:Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results. version:1
arxiv-1312-4599 | Evolution and Computational Learning Theory: A survey on Valiant's paper | http://arxiv.org/abs/1312.4599 | id:1312.4599 author:Arka Bhattacharya category:cs.LG 68Q32  published:2013-12-17 summary:Darwin's theory of evolution is considered to be one of the greatest scientific gems in modern science. It not only gives us a description of how living things evolve, but also shows how a population evolves through time and also, why only the fittest individuals continue the generation forward. The paper basically gives a high level analysis of the works of Valiant[1]. Though, we know the mechanisms of evolution, but it seems that there does not exist any strong quantitative and mathematical theory of the evolution of certain mechanisms. What is defined exactly as the fitness of an individual, why is that only certain individuals in a population tend to mutate, how computation is done in finite time when we have exponentially many examples: there seems to be a lot of questions which need to be answered. [1] basically treats Darwinian theory as a form of computational learning theory, which calculates the net fitness of the hypotheses and thus distinguishes functions and their classes which could be evolvable using polynomial amount of resources. Evolution is considered as a function of the environment and the previous evolutionary stages that chooses the best hypothesis using learning techniques that makes mutation possible and hence, gives a quantitative idea that why only the fittest individuals tend to survive and have the power to mutate. version:1
arxiv-1312-4551 | Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs | http://arxiv.org/abs/1312.4551 | id:1312.4551 author:Armen E. Allahverdyan, Aram Galstyan category:stat.ML cs.LG  published:2013-12-16 summary:We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only finite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam's razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters. version:1
arxiv-1312-4479 | Parametric Modelling of Multivariate Count Data Using Probabilistic Graphical Models | http://arxiv.org/abs/1312.4479 | id:1312.4479 author:Pierre Fernique, Jean-Baptiste Durand, Yann Guédon category:stat.ML cs.LG stat.ME  published:2013-12-16 summary:Multivariate count data are defined as the number of items of different categories issued from sampling within a population, which individuals are grouped into categories. The analysis of multivariate count data is a recurrent and crucial issue in numerous modelling problems, particularly in the fields of biology and ecology (where the data can represent, for example, children counts associated with multitype branching processes), sociology and econometrics. We focus on I) Identifying categories that appear simultaneously, or on the contrary that are mutually exclusive. This is achieved by identifying conditional independence relationships between the variables; II)Building parsimonious parametric models consistent with these relationships; III) Characterising and testing the effects of covariates on the joint distribution of the counts. To achieve these goals, we propose an approach based on graphical probabilistic models, and more specifically partially directed acyclic graphs. version:1
arxiv-1312-6150 | A Review on Automated Brain Tumor Detection and Segmentation from MRI of Brain | http://arxiv.org/abs/1312.6150 | id:1312.6150 author:Sudipta Roy, Sanjay Nag, Indra Kanta Maitra, Samir Kumar Bandyopadhyay category:cs.CV  published:2013-12-16 summary:Tumor segmentation from magnetic resonance imaging (MRI) data is an important but time consuming manual task performed by medical experts. Automating this process is a challenging task because of the high diversity in the appearance of tumor tissues among different patients and in many cases similarity with the normal tissues. MRI is an advanced medical imaging technique providing rich information about the human soft-tissue anatomy. There are different brain tumor detection and segmentation methods to detect and segment a brain tumor from MRI images. These detection and segmentation approaches are reviewed with an importance placed on enlightening the advantages and drawbacks of these methods for brain tumor detection and segmentation. The use of MRI image detection and segmentation in different procedures are also described. Here a brief review of different segmentation for detection of brain tumor from MRI of brain has been discussed. version:1
arxiv-1312-4426 | Optimization for Compressed Sensing: the Simplex Method and Kronecker Sparsification | http://arxiv.org/abs/1312.4426 | id:1312.4426 author:Robert Vanderbei, Han Liu, Lie Wang, Kevin Lin category:stat.ML cs.LG  published:2013-12-16 summary:In this paper we present two new approaches to efficiently solve large-scale compressed sensing problems. These two ideas are independent of each other and can therefore be used either separately or together. We consider all possibilities. For the first approach, we note that the zero vector can be taken as the initial basic (infeasible) solution for the linear programming problem and therefore, if the true signal is very sparse, some variants of the simplex method can be expected to take only a small number of pivots to arrive at a solution. We implemented one such variant and demonstrate a dramatic improvement in computation time on very sparse signals. The second approach requires a redesigned sensing mechanism in which the vector signal is stacked into a matrix. This allows us to exploit the Kronecker compressed sensing (KCS) mechanism. We show that the Kronecker sensing requires stronger conditions for perfect recovery compared to the original vector problem. However, the Kronecker sensing, modeled correctly, is a much sparser linear optimization problem. Hence, algorithms that benefit from sparse problem representation, such as interior-point methods, can solve the Kronecker sensing problems much faster than the corresponding vector problem. In our numerical studies, we demonstrate a ten-fold improvement in the computation time. version:1
arxiv-1312-3429 | Unsupervised learning of depth and motion | http://arxiv.org/abs/1312.3429 | id:1312.3429 author:Kishore Konda, Roland Memisevic category:cs.CV cs.LG stat.ML  published:2013-12-12 summary:We present a model for the joint estimation of disparity and motion. The model is based on learning about the interrelations between images from multiple cameras, multiple frames in a video, or the combination of both. We show that learning depth and motion cues, as well as their combinations, from data is possible within a single type of architecture and a single type of learning algorithm, by using biologically inspired "complex cell" like units, which encode correlations between the pixels across image pairs. Our experimental results show that the learning of depth and motion makes it possible to achieve state-of-the-art performance in 3-D activity analysis, and to outperform existing hand-engineered 3-D motion features by a very large margin. version:2
arxiv-1312-4405 | Learning Deep Representations By Distributed Random Samplings | http://arxiv.org/abs/1312.4405 | id:1312.4405 author:Xiao-Lei Zhang category:cs.LG  published:2013-12-16 summary:In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction -- deep distributed random samplings, which performs like a stack of unsupervised bootstrap aggregating. First, its network structure is novel: each layer of the network is a group of mutually independent $k$-centers clusterings. Second, its learning method is extremely simple: the $k$ centers of each clustering are only $k$ randomly selected examples from the training data; for small-scale data sets, the $k$ centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems. version:1
arxiv-1312-4384 | Rectifying Self Organizing Maps for Automatic Concept Learning from Web Images | http://arxiv.org/abs/1312.4384 | id:1312.4384 author:Eren Golge, Pinar Duygulu category:cs.CV cs.LG cs.NE  published:2013-12-16 summary:We attack the problem of learning concepts automatically from noisy web image search results. Going beyond low level attributes, such as colour and texture, we explore weakly-labelled datasets for the learning of higher level concepts, such as scene categories. The idea is based on discovering common characteristics shared among subsets of images by posing a method that is able to organise the data while eliminating irrelevant instances. We propose a novel clustering and outlier detection method, namely Rectifying Self Organizing Maps (RSOM). Given an image collection returned for a concept query, RSOM provides clusters pruned from outliers. Each cluster is used to train a model representing a different characteristics of the concept. The proposed method outperforms the state-of-the-art studies on the task of learning low-level concepts, and it is competitive in learning higher level concepts as well. It is capable to work at large scale with no supervision through exploiting the available sources. version:1
arxiv-1312-4382 | Single-trial estimation of stimulus and spike-history effects on time-varying ensemble spiking activity of multiple neurons: a simulation study | http://arxiv.org/abs/1312.4382 | id:1312.4382 author:Hideaki Shimazaki category:q-bio.NC cond-mat.stat-mech stat.ML  published:2013-12-16 summary:Neurons in cortical circuits exhibit coordinated spiking activity, and can produce correlated synchronous spikes during behavior and cognition. We recently developed a method for estimating the dynamics of correlated ensemble activity by combining a model of simultaneous neuronal interactions (e.g., a spin-glass model) with a state-space method (Shimazaki et al. 2012 PLoS Comput Biol 8 e1002385). This method allows us to estimate stimulus-evoked dynamics of neuronal interactions which is reproducible in repeated trials under identical experimental conditions. However, the method may not be suitable for detecting stimulus responses if the neuronal dynamics exhibits significant variability across trials. In addition, the previous model does not include effects of past spiking activity of the neurons on the current state of ensemble activity. In this study, we develop a parametric method for simultaneously estimating the stimulus and spike-history effects on the ensemble activity from single-trial data even if the neurons exhibit dynamics that is largely unrelated to these effects. For this goal, we model ensemble neuronal activity as a latent process and include the stimulus and spike-history effects as exogenous inputs to the latent process. We develop an expectation-maximization algorithm that simultaneously achieves estimation of the latent process, stimulus responses, and spike-history effects. The proposed method is useful to analyze an interaction of internal cortical states and sensory evoked activity. version:1
arxiv-1312-4346 | Teleoperation System Using Past Image Records Considering Narrow Communication Band | http://arxiv.org/abs/1312.4346 | id:1312.4346 author:Noritaka Sato, Masataka Ito, Yoshifumi Morita, Fumitoshi Matsuno category:cs.RO cs.CV  published:2013-12-16 summary:Teleoperation is necessary when the robot is applied to real missions, for example surveillance, search and rescue. We proposed teleoperation system using past image records (SPIR). SPIR virtually generates the bird's-eye view image by overlaying the CG model of the robot at the corresponding current position on the background image which is captured from the camera mounted on the robot at a past time. The problem for SPIR is that the communication bandwidth is often narrow in some teleoperation tasks. In this case, the candidates of background image of SPIR are few and the position of the robot is often delayed. In this study, we propose zoom function for insufficiency of candidates of the background image and additional interpolation lines for the delay of the position data of the robot. To evaluate proposed system, an outdoor experiments are carried out. The outdoor experiment is conducted on a training course of a driving school. version:1
arxiv-1312-4527 | Probable convexity and its application to Correlated Topic Models | http://arxiv.org/abs/1312.4527 | id:1312.4527 author:Khoat Than, Tu Bao Ho category:cs.LG stat.ML  published:2013-12-16 summary:Non-convex optimization problems often arise from probabilistic modeling, such as estimation of posterior distributions. Non-convexity makes the problems intractable, and poses various obstacles for us to design efficient algorithms. In this work, we attack non-convexity by first introducing the concept of \emph{probable convexity} for analyzing convexity of real functions in practice. We then use the new concept to analyze an inference problem in the \emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contrary to the existing belief of intractability, we show that this inference problem is concave under certain conditions. One consequence of our analyses is a novel algorithm for learning CTM which is significantly more scalable and qualitative than existing methods. Finally, we highlight that stochastic gradient algorithms might be a practical choice to resolve efficiently non-convex problems. This finding might find beneficial in many contexts which are beyond probabilistic modeling. version:1
arxiv-1311-2460 | Vision-Guided Robot Hearing | http://arxiv.org/abs/1311.2460 | id:1311.2460 author:Xavier Alameda-Pineda, Radu Horaud category:cs.RO cs.CV  published:2013-11-06 summary:Natural human-robot interaction in complex and unpredictable environments is one of the main research lines in robotics. In typical real-world scenarios, humans are at some distance from the robot and the acquired signals are strongly impaired by noise, reverberations and other interfering sources. In this context, the detection and localisation of speakers plays a key role since it is the pillar on which several tasks (e.g.: speech recognition and speaker tracking) rely. We address the problem of how to detect and localize people that are both seen and heard by a humanoid robot. We introduce a hybrid deterministic/probabilistic model. Indeed, the deterministic component allows us to map the visual information into the auditory space. By means of the probabilistic component, the visual features guide the grouping of the auditory features in order to form AV objects. The proposed model and the associated algorithm are implemented in real-time (17 FPS) using a stereoscopic camera pair and two microphones embedded into the head of the humanoid robot NAO. We performed experiments on (i) synthetic data, (ii) a publicly available data set and (iii) data acquired using the robot. The results we obtained validate the approach and encourage us to further investigate how vision can help robot hearing. version:2
arxiv-1010-3091 | Near-Optimal Bayesian Active Learning with Noisy Observations | http://arxiv.org/abs/1010.3091 | id:1010.3091 author:Daniel Golovin, Andreas Krause, Debajyoti Ray category:cs.LG cs.AI cs.DS I.2.6; G.3; F.2.2  published:2010-10-15 summary:We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non-uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty. version:2
arxiv-1312-4209 | Feature Graph Architectures | http://arxiv.org/abs/1312.4209 | id:1312.4209 author:Richard Davis, Sanjay Chawla, Philip Leong category:cs.LG  published:2013-12-15 summary:In this article we propose feature graph architectures (FGA), which are deep learning systems employing a structured initialisation and training method based on a feature graph which facilitates improved generalisation performance compared with a standard shallow architecture. The goal is to explore alternative perspectives on the problem of deep network training. We evaluate FGA performance for deep SVMs on some experimental datasets, and show how generalisation and stability results may be derived for these models. We describe the effect of permutations on the model accuracy, and give a criterion for the optimal permutation in terms of feature correlations. The experimental results show that the algorithm produces robust and significant test set improvements over a standard shallow SVM training method for a range of datasets. These gains are achieved with a moderate increase in time complexity. version:1
arxiv-1312-0086 | A Framework for Genetic Algorithms Based on Hadoop | http://arxiv.org/abs/1312.0086 | id:1312.0086 author:Filomena Ferrucci, M-Tahar Kechadi, Pasquale Salza, Federica Sarro category:cs.NE cs.DC  published:2013-11-30 summary:Genetic Algorithms (GAs) are powerful metaheuristic techniques mostly used in many real-world applications. The sequential execution of GAs requires considerable computational power both in time and resources. Nevertheless, GAs are naturally parallel and accessing a parallel platform such as Cloud is easy and cheap. Apache Hadoop is one of the common services that can be used for parallel applications. However, using Hadoop to develop a parallel version of GAs is not simple without facing its inner workings. Even though some sequential frameworks for GAs already exist, there is no framework supporting the development of GA applications that can be executed in parallel. In this paper is described a framework for parallel GAs on the Hadoop platform, following the paradigm of MapReduce. The main purpose of this framework is to allow the user to focus on the aspects of GA that are specific to the problem to be addressed, being sure that this task is going to be correctly executed on the Cloud with a good performance. The framework has been also exploited to develop an application for Feature Subset Selection problem. A preliminary analysis of the performance of the developed GA application has been performed using three datasets and shown very promising performance. version:2
arxiv-1312-6834 | Face Detection from still and Video Images using Unsupervised Cellular Automata with K means clustering algorithm | http://arxiv.org/abs/1312.6834 | id:1312.6834 author:P. Kiran Sree, I. Ramesh Babu category:cs.CV  published:2013-12-15 summary:Pattern recognition problem rely upon the features inherent in the pattern of images. Face detection and recognition is one of the challenging research areas in the field of computer vision. In this paper, we present a method to identify skin pixels from still and video images using skin color. Face regions are identified from this skin pixel region. Facial features such as eyes, nose and mouth are then located. Faces are recognized from color images using an RBF based neural network. Unsupervised Cellular Automata with K means clustering algorithm is used to locate different facial elements. Orientation is corrected by using eyes. Parameters like inter eye distance, nose length, mouth position, Discrete Cosine Transform (DCT) coefficients etc. are computed and used for a Radial Basis Function (RBF) based neural network. This approach reliably works for face sequence with orientation in head, expressions etc. version:1
arxiv-1312-4149 | Autonomous Quantum Perceptron Neural Network | http://arxiv.org/abs/1312.4149 | id:1312.4149 author:Alaa Sagheer, Mohammed Zidan category:cs.NE  published:2013-12-15 summary:Recently, with the rapid development of technology, there are a lot of applications require to achieve low-cost learning. However the computational power of classical artificial neural networks, they are not capable to provide low-cost learning. In contrast, quantum neural networks may be representing a good computational alternate to classical neural network approaches, based on the computational power of quantum bit (qubit) over the classical bit. In this paper we present a new computational approach to the quantum perceptron neural network can achieve learning in low-cost computation. The proposed approach has only one neuron can construct self-adaptive activation operators capable to accomplish the learning process in a limited number of iterations and, thereby, reduce the overall computational cost. The proposed approach is capable to construct its own set of activation operators to be applied widely in both quantum and classical applications to overcome the linearity limitation of classical perceptron. The computational power of the proposed approach is illustrated via solving variety of problems where promising and comparable results are given. version:1
arxiv-1312-4132 | An introduction to synchronous self-learning Pareto strategy | http://arxiv.org/abs/1312.4132 | id:1312.4132 author:Ahmad Mozaffari, Alireza Fathi category:cs.NE  published:2013-12-15 summary:In last decades optimization and control of complex systems that possessed various conflicted objectives simultaneously attracted an incremental interest of scientists. This is because of the vast applications of these systems in various fields of real life engineering phenomena that are generally multi modal, non convex and multi criterion. Hence, many researchers utilized versatile intelligent models such as Pareto based techniques, game theory (cooperative and non cooperative games), neuro evolutionary systems, fuzzy logic and advanced neural networks for handling these types of problems. In this paper a novel method called Synchronous Self Learning Pareto Strategy Algorithm (SSLPSA) is presented which utilizes Evolutionary Computing (EC), Swarm Intelligence (SI) techniques and adaptive Classical Self Organizing Map (CSOM) simultaneously incorporating with a data shuffling behavior. Evolutionary Algorithms (EA) which attempt to simulate the phenomenon of natural evolution are powerful numerical optimization algorithms that reach an approximate global maximum of a complex multi variable function over a wide search space and swarm base technique can improved the intensity and the robustness in EA. CSOM is a neural network capable of learning and can improve the quality of obtained optimal Pareto front. To prove the efficient performance of proposed algorithm, authors utilized some well known benchmark test functions. Obtained results indicate that the cited method is best suit in the case of vector optimization. version:1
arxiv-1312-4124 | A robust Iris recognition method on adverse conditions | http://arxiv.org/abs/1312.4124 | id:1312.4124 author:Maryam Soltanali Khalili, Hamed Sadjedi category:cs.CV  published:2013-12-15 summary:As a stable biometric system, iris has recently attracted great attention among the researchers. However, research is still needed to provide appropriate solutions to ensure the resistance of the system against error factors. The present study has tried to apply a mask to the image so that the unexpected factors affecting the location of the iris can be removed. So, pupil localization will be faster and robust. Then to locate the exact location of the iris, a simple stage of boundary displacement due to the Canny edge detector has been applied. Then, with searching left and right IRIS edge point, outer radios of IRIS will be detect. Through the process of extracting the iris features, it has been sought to obtain the distinctive iris texture features by using a discrete stationary wavelets transform 2-D (DSWT2). Using DSWT2 tool and symlet 4 wavelet, distinctive features are extracted. To reduce the computational cost, the features obtained from the application of the wavelet have been investigated and a feature selection procedure, using similarity criteria, has been implemented. Finally, the iris matching has been performed using a semi-correlation criterion. The accuracy of the proposed method for localization on CASIA-v1, CASIA-v3 is 99.73%, 98.24% and 97.04%, respectively. The accuracy of the feature extraction proposed method for CASIA3 iris images database is 97.82%, which confirms the efficiency of the proposed method. version:1
arxiv-1312-4108 | A MapReduce based distributed SVM algorithm for binary classification | http://arxiv.org/abs/1312.4108 | id:1312.4108 author:Ferhat Özgür Çatak, Mehmet Erdal Balaban category:cs.LG cs.DC  published:2013-12-15 summary:Although Support Vector Machine (SVM) algorithm has a high generalization property to classify for unseen examples after training phase and it has small loss value, the algorithm is not suitable for real-life classification and regression problems. SVMs cannot solve hundreds of thousands examples in training dataset. In previous studies on distributed machine learning algorithms, SVM is trained over a costly and preconfigured computer environment. In this research, we present a MapReduce based distributed parallel SVM training algorithm for binary classification problems. This work shows how to distribute optimization problem over cloud computing systems with MapReduce technique. In the second step of this work, we used statistical learning theory to find the predictive hypothesis that minimize our empirical risks from hypothesis spaces that created with reduce function of MapReduce. The results of this research are important for training of big datasets for SVM algorithm based classification problems. We provided that iterative training of split dataset with MapReduce technique; accuracy of the classifier function will converge to global optimal classifier function's accuracy in finite iteration size. The algorithm performance was measured on samples from letter recognition and pen-based recognition of handwritten digits dataset. version:1
arxiv-1312-4092 | Domain adaptation for sequence labeling using hidden Markov models | http://arxiv.org/abs/1312.4092 | id:1312.4092 author:Edouard Grave, Guillaume Obozinski, Francis Bach category:cs.CL cs.LG  published:2013-12-14 summary:Most natural language processing systems based on machine learning are not robust to domain shift. For example, a state-of-the-art syntactic dependency parser trained on Wall Street Journal sentences has an absolute drop in performance of more than ten points when tested on textual data from the Web. An efficient solution to make these methods more robust to domain shift is to first learn a word representation using large amounts of unlabeled data from both domains, and then use this representation as features in a supervised learning algorithm. In this paper, we propose to use hidden Markov models to learn word representations for part-of-speech tagging. In particular, we study the influence of using data from the source, the target or both domains to learn the representation and the different ways to represent words using an HMM. version:1
arxiv-1312-4078 | A natural-inspired optimization machine based on the annual migration of salmons in nature | http://arxiv.org/abs/1312.4078 | id:1312.4078 author:Ahmad Mozaffari, Alireza Fathi category:cs.NE  published:2013-12-14 summary:Bio inspiration is a branch of artificial simulation science that shows pervasive contributions to variety of engineering fields such as automate pattern recognition, systematic fault detection and applied optimization. In this paper, a new metaheuristic optimizing algorithm that is the simulation of The Great Salmon Run(TGSR) is developed. The obtained results imply on the acceptable performance of implemented method in optimization of complex non convex, multi dimensional and multi-modal problems. To prove the superiority of TGSR in both robustness and quality, it is also compared with most of the well known proposed optimizing techniques such as Simulated Annealing (SA), Parallel Migrating Genetic Algorithm (PMGA), Differential Evolutionary Algorithm (DEA), Particle Swarm Optimization (PSO), Bee Algorithm (BA), Artificial Bee Colony (ABC), Firefly Algorithm (FA) and Cuckoo Search (CS). The obtained results confirm the acceptable performance of the proposed method in both robustness and quality for different bench-mark optimizing problems and also prove the authors claim. version:1
arxiv-1312-4074 | Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm | http://arxiv.org/abs/1312.4074 | id:1312.4074 author:Srinjoy Ganguly, Digbalay Bose, Amit Konar category:cs.CV  published:2013-12-14 summary:Clustering is an important facet of explorative data mining and finds extensive use in several fields. In this paper, we propose an extension of the classical Fuzzy C-Means clustering algorithm. The proposed algorithm, abbreviated as VFC, adopts a multi-dimensional membership vector for each data point instead of the traditional, scalar membership value defined in the original algorithm. The membership vector for each point is obtained by considering each feature of that point separately and obtaining individual membership values for the same. We also propose an algorithm to efficiently allocate the initial cluster centers close to the actual centers, so as to facilitate rapid convergence. Further, we propose a scheme to achieve crisp clustering using the VFC algorithm. The proposed, novel clustering scheme has been tested on two standard data sets in order to analyze its performance. We also examine the efficacy of the proposed scheme by analyzing its performance on image segmentation examples and comparing it with the classical Fuzzy C-means clustering algorithm. version:1
arxiv-1312-4044 | CACO : Competitive Ant Colony Optimization, A Nature-Inspired Metaheuristic For Large-Scale Global Optimization | http://arxiv.org/abs/1312.4044 | id:1312.4044 author:M. A. El-Dosuky category:cs.NE  published:2013-12-14 summary:Large-scale problems are nonlinear problems that need metaheuristics, or global optimization algorithms. This paper reviews nature-inspired metaheuristics, then it introduces a framework named Competitive Ant Colony Optimization inspired by the chemical communications among insects. Then a case study is presented to investigate the proposed framework for large-scale global optimization. version:1
arxiv-1310-3556 | Identifying Influential Entries in a Matrix | http://arxiv.org/abs/1310.3556 | id:1310.3556 author:Abhisek Kundu, Srinivas Nambirajan, Petros Drineas category:cs.NA cs.LG stat.ML  published:2013-10-14 summary:For any matrix A in R^(m x n) of rank \rho, we present a probability distribution over the entries of A (the element-wise leverage scores of equation (2)) that reveals the most influential entries in the matrix. From a theoretical perspective, we prove that sampling at most s = O ((m + n) \rho^2 ln (m + n)) entries of the matrix (see eqn. (3) for the precise value of s) with respect to these scores and solving the nuclear norm minimization problem on the sampled entries, reconstructs A exactly. To the best of our knowledge, these are the strongest theoretical guarantees on matrix completion without any incoherence assumptions on the matrix A. From an experimental perspective, we show that entries corresponding to high element-wise leverage scores reveal structural properties of the data matrix that are of interest to domain scientists. version:2
arxiv-1211-0919 | High-Dimensional Covariance Decomposition into Sparse Markov and Independence Models | http://arxiv.org/abs/1211.0919 | id:1211.0919 author:Majid Janzamin, Animashree Anandkumar category:stat.ML math.ST stat.TH 62H12  published:2012-11-05 summary:Fitting high-dimensional data involves a delicate tradeoff between faithful representation and the use of sparse models. Too often, sparsity assumptions on the fitted model are too restrictive to provide a faithful representation of the observed data. In this paper, we present a novel framework incorporating sparsity in different domains.We decompose the observed covariance matrix into a sparse Gaussian Markov model (with a sparse precision matrix) and a sparse independence model (with a sparse covariance matrix). Our framework incorporates sparse covariance and sparse precision estimation as special cases and thus introduces a richer class of high-dimensional models. We characterize sufficient conditions for identifiability of the two models, \viz Markov and independence models. We propose an efficient decomposition method based on a modification of the popular $\ell_1$-penalized maximum-likelihood estimator ($\ell_1$-MLE). We establish that our estimator is consistent in both the domains, i.e., it successfully recovers the supports of both Markov and independence models, when the number of samples $n$ scales as $n = \Omega(d^2 \log p)$, where $p$ is the number of variables and $d$ is the maximum node degree in the Markov model. Our experiments validate these results and also demonstrate that our models have better inference accuracy under simple algorithms such as loopy belief propagation. version:2
arxiv-1312-3990 | ECOC-Based Training of Neural Networks for Face Recognition | http://arxiv.org/abs/1312.3990 | id:1312.3990 author:Nima Hatami, Reza Ebrahimpour, Reza Ghaderi category:cs.CV cs.LG  published:2013-12-14 summary:Error Correcting Output Codes, ECOC, is an output representation method capable of discovering some of the errors produced in classification tasks. This paper describes the application of ECOC to the training of feed forward neural networks, FFNN, for improving the overall accuracy of classification systems. Indeed, to improve the generalization of FFNN classifiers, this paper proposes an ECOC-Based training method for Neural Networks that use ECOC as the output representation, and adopts the traditional Back-Propagation algorithm, BP, to adjust weights of the network. Experimental results for face recognition problem on Yale database demonstrate the effectiveness of our method. With a rejection scheme defined by a simple robustness rate, high reliability is achieved in this application. version:1
arxiv-1312-3989 | Classifiers With a Reject Option for Early Time-Series Classification | http://arxiv.org/abs/1312.3989 | id:1312.3989 author:Nima Hatami, Camelia Chira category:cs.CV cs.LG  published:2013-12-14 summary:Early classification of time-series data in a dynamic environment is a challenging problem of great importance in signal processing. This paper proposes a classifier architecture with a reject option capable of online decision making without the need to wait for the entire time series signal to be present. The main idea is to classify an odor/gas signal with an acceptable accuracy as early as possible. Instead of using posterior probability of a classifier, the proposed method uses the "agreement" of an ensemble to decide whether to accept or reject the candidate label. The introduced algorithm is applied to the bio-chemistry problem of odor classification to build a novel Electronic-Nose called Forefront-Nose. Experimental results on wind tunnel test-bed facility confirms the robustness of the forefront-nose compared to the standard classifiers from both earliness and recognition perspectives. version:1
arxiv-1312-3970 | An Extensive Evaluation of Filtering Misclassified Instances in Supervised Classification Tasks | http://arxiv.org/abs/1312.3970 | id:1312.3970 author:Michael R. Smith, Tony Martinez category:cs.LG stat.ML  published:2013-12-13 summary:Removing or filtering outliers and mislabeled instances prior to training a learning algorithm has been shown to increase classification accuracy. A popular approach for handling outliers and mislabeled instances is to remove any instance that is misclassified by a learning algorithm. However, an examination of which learning algorithms to use for filtering as well as their effects on multiple learning algorithms over a large set of data sets has not been done. Previous work has generally been limited due to the large computational requirements to run such an experiment, and, thus, the examination has generally been limited to learning algorithms that are computationally inexpensive and using a small number of data sets. In this paper, we examine 9 learning algorithms as filtering algorithms as well as examining the effects of filtering in the 9 chosen learning algorithms on a set of 54 data sets. In addition to using each learning algorithm individually as a filter, we also use the set of learning algorithms as an ensemble filter and use an adaptive algorithm that selects a subset of the learning algorithms for filtering for a specific task and learning algorithm. We find that for most cases, using an ensemble of learning algorithms for filtering produces the greatest increase in classification accuracy. We also compare filtering with a majority voting ensemble. The voting ensemble significantly outperforms filtering unless there are high amounts of noise present in the data set. Additionally, we find that a majority voting ensemble is robust to noise as filtering with a voting ensemble does not increase the classification accuracy of the voting ensemble. version:1
arxiv-1312-0624 | Efficient coordinate-descent for orthogonal matrices through Givens rotations | http://arxiv.org/abs/1312.0624 | id:1312.0624 author:Uri Shalit, Gal Chechik category:cs.LG stat.ML  published:2013-12-02 summary:Optimizing over the set of orthogonal matrices is a central component in problems like sparse-PCA or tensor decomposition. Unfortunately, such optimization is hard since simple operations on orthogonal matrices easily break orthogonality, and correcting orthogonality usually costs a large amount of computation. Here we propose a framework for optimizing orthogonal matrices, that is the parallel of coordinate-descent in Euclidean spaces. It is based on {\em Givens-rotations}, a fast-to-compute operation that affects a small number of entries in the learned matrix, and preserves orthogonality. We show two applications of this approach: an algorithm for tensor decomposition that is used in learning mixture models, and an algorithm for sparse-PCA. We study the parameter regime where a Givens rotation approach converges faster and achieves a superior model on a genome-wide brain-wide mRNA expression dataset. version:2
arxiv-1312-3903 | A Methodology for Player Modeling based on Machine Learning | http://arxiv.org/abs/1312.3903 | id:1312.3903 author:Marlos C. Machado category:cs.AI cs.LG  published:2013-12-13 summary:AI is gradually receiving more attention as a fundamental feature to increase the immersion in digital games. Among the several AI approaches, player modeling is becoming an important one. The main idea is to understand and model the player characteristics and behaviors in order to develop a better AI. In this work, we discuss several aspects of this new field. We proposed a taxonomy to organize the area, discussing several facets of this topic, ranging from implementation decisions up to what a model attempts to describe. We then classify, in our taxonomy, some of the most important works in this field. We also presented a generic approach to deal with player modeling using ML, and we instantiated this approach to model players' preferences in the game Civilization IV. The instantiation of this approach has several steps. We first discuss a generic representation, regardless of what is being modeled, and evaluate it performing experiments with the strategy game Civilization IV. Continuing the instantiation of the proposed approach we evaluated the applicability of using game score information to distinguish different preferences. We presented a characterization of virtual agents in the game, comparing their behavior with their stated preferences. Once we have characterized these agents, we were able to observe that different preferences generate different behaviors, measured by several game indicators. We then tackled the preference modeling problem as a binary classification task, with a supervised learning approach. We compared four different methods, based on different paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on a set of matches played by different virtual agents. We conclude our work using the learned models to infer human players' preferences. Using some of the evaluated classifiers we obtained accuracies over 60% for most of the inferred preferences. version:1
arxiv-1312-3811 | Efficient Baseline-free Sampling in Parameter Exploring Policy Gradients: Super Symmetric PGPE | http://arxiv.org/abs/1312.3811 | id:1312.3811 author:Frank Sehnke category:cs.LG  published:2013-12-13 summary:Policy Gradient methods that explore directly in parameter space are among the most effective and robust direct policy search methods and have drawn a lot of attention lately. The basic method from this field, Policy Gradients with Parameter-based Exploration, uses two samples that are symmetric around the current hypothesis to circumvent misleading reward in \emph{asymmetrical} reward distributed problems gathered with the usual baseline approach. The exploration parameters are still updated by a baseline approach - leaving the exploration prone to asymmetric reward distributions. In this paper we will show how the exploration parameters can be sampled quasi symmetric despite having limited instead of free parameters for exploration. We give a transformation approximation to get quasi symmetric samples with respect to the exploration without changing the overall sampling distribution. Finally we will demonstrate that sampling symmetrically also for the exploration parameters is superior in needs of samples and robustness than the original sampling approach. version:1
arxiv-1312-3724 | ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented perception | http://arxiv.org/abs/1312.3724 | id:1312.3724 author:Pierluigi Gallo, Ilenia Tinnirello, Laura Giarré, Domenico Garlisi, Daniele Croce, Adriano Fagiolini category:cs.CV cs.HC  published:2013-12-13 summary:ARIANNA stands for pAth Recognition for Indoor Assisted Navigation with Augmented perception. It is a flexible and low cost navigation system for vi- sually impaired people. Arianna permits to navigate colored paths painted or sticked on the floor revealing their directions through vibrational feedback on commercial smartphones. version:1
arxiv-1208-3380 | Consistent selection of tuning parameters via variable selection stability | http://arxiv.org/abs/1208.3380 | id:1208.3380 author:Wei Sun, Junhui Wang, Yixin Fang category:stat.ML stat.ME  published:2012-08-16 summary:Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model fitting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model fitting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross-validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on a novel concept of variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both fixed and diverging dimensions. The effectiveness of the proposed criterion is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. version:2
arxiv-1302-3639 | A Latent Source Model for Nonparametric Time Series Classification | http://arxiv.org/abs/1302.3639 | id:1302.3639 author:George H. Chen, Stanislav Nikolov, Devavrat Shah category:stat.ML cs.LG cs.SI  published:2013-02-14 summary:For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren't actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a "weighted majority voting" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such "trending topics" in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%. version:5
arxiv-1312-3525 | Oracle Inequalities for Convex Loss Functions with Non-Linear Targets | http://arxiv.org/abs/1312.3525 | id:1312.3525 author:Mehmet Caner, Anders Bredahl Kock category:math.ST stat.ML stat.TH  published:2013-12-12 summary:This paper consider penalized empirical loss minimization of convex loss functions with unknown non-linear target functions. Using the elastic net penalty we establish a finite sample oracle inequality which bounds the loss of our estimator from above with high probability. If the unknown target is linear this inequality also provides an upper bound of the estimation error of the estimated parameter vector. These are new results and they generalize the econometrics and statistics literature. Next, we use the non-asymptotic results to show that the excess loss of our estimator is asymptotically of the same order as that of the oracle. If the target is linear we give sufficient conditions for consistency of the estimated parameter vector. Next, we briefly discuss how a thresholded version of our estimator can be used to perform consistent variable selection. We give two examples of loss functions covered by our framework and show how penalized nonparametric series estimation is contained as a special case and provide a finite sample upper bound on the mean square error of the elastic net series estimator. version:1
arxiv-1312-3388 | Online Bayesian Passive-Aggressive Learning | http://arxiv.org/abs/1312.3388 | id:1312.3388 author:Tianlin Shi, Jun Zhu category:cs.LG  published:2013-12-12 summary:Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This pa- per presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts. version:1
arxiv-1312-3291 | Near-optimal Anomaly Detection in Graphs using Lovasz Extended Scan Statistic | http://arxiv.org/abs/1312.3291 | id:1312.3291 author:James Sharpnack, Akshay Krishnamurthy, Aarti Singh category:stat.ML 62G10  published:2013-12-11 summary:The detection of anomalous activity in graphs is a statistical problem that arises in many applications, such as network surveillance, disease outbreak detection, and activity monitoring in social networks. Beyond its wide applicability, graph structured anomaly detection serves as a case study in the difficulty of balancing computational complexity with statistical power. In this work, we develop from first principles the generalized likelihood ratio test for determining if there is a well connected region of activation over the vertices in the graph in Gaussian noise. Because this test is computationally infeasible, we provide a relaxation, called the Lovasz extended scan statistic (LESS) that uses submodularity to approximate the intractable generalized likelihood ratio. We demonstrate a connection between LESS and maximum a-posteriori inference in Markov random fields, which provides us with a poly-time algorithm for LESS. Using electrical network theory, we are able to control type 1 error for LESS and prove conditions under which LESS is risk consistent. Finally, we consider specific graph models, the torus, k-nearest neighbor graphs, and epsilon-random graphs. We show that on these graphs our results provide near-optimal performance by matching our results to known lower bounds. version:1
arxiv-1312-3258 | Implicit Sensitive Text Summarization based on Data Conveyed by Connectives | http://arxiv.org/abs/1312.3258 | id:1312.3258 author:Henda Chorfi Ouertani category:cs.CL  published:2013-12-11 summary:So far and trying to reach human capabilities, research in automatic summarization has been based on hypothesis that are both enabling and limiting. Some of these limitations are: how to take into account and reflect (in the generated summary) the implicit information conveyed in the text, the author intention, the reader intention, the context influence, the general world knowledge. Thus, if we want machines to mimic human abilities, then they will need access to this same large variety of knowledge. The implicit is affecting the orientation and the argumentation of the text and consequently its summary. Most of Text Summarizers (TS) are processing as compressing the initial data and they necessarily suffer from information loss. TS are focusing on features of the text only, not on what the author intended or why the reader is reading the text. In this paper, we address this problem and we present a system focusing on acquiring knowledge that is implicit. We principally spotlight the implicit information conveyed by the argumentative connectives such as: but, even, yet and their effect on the summary. version:1
arxiv-1312-3251 | Towards The Development of a Bishnupriya Manipuri Corpus | http://arxiv.org/abs/1312.3251 | id:1312.3251 author:Nayan Jyoti Kalita, Navanath Saharia, Smriti Kumar Sinha category:cs.CL  published:2013-12-11 summary:For any deep computational processing of language we need evidences, and one such set of evidences is corpus. This paper describes the development of a text-based corpus for the Bishnupriya Manipuri language. A Corpus is considered as a building block for any language processing tasks. Due to the lack of awareness like other Indian languages, it is also studied less frequently. As a result the language still lacks a good corpus and basic language processing tools. As per our knowledge this is the first effort to develop a corpus for Bishnupriya Manipuri language. version:1
arxiv-1312-3199 | Thickness Mapping of Eleven Retinal Layers in Normal Eyes Using Spectral Domain Optical Coherence Tomography | http://arxiv.org/abs/1312.3199 | id:1312.3199 author:Raheleh Kafieh, Hossein Rabbani, Fedra Hajizadeh, Michael D. Abramoff, Milan Sonka category:cs.CV  published:2013-12-11 summary:Purpose. This study was conducted to determine the thickness map of eleven retinal layers in normal subjects by spectral domain optical coherence tomography (SD-OCT) and evaluate their association with sex and age. Methods. Mean regional retinal thickness of 11 retinal layers were obtained by automatic three-dimensional diffusion-map-based method in 112 normal eyes of 76 Iranian subjects. Results. The thickness map of central foveal area in layer 1, 3, and 4 displayed the minimum thickness (P<0.005 for all). Maximum thickness was observed in nasal to the fovea of layer 1 (P<0.001) and in a circular pattern in the parafoveal retinal area of layers 2, 3 and 4 and in central foveal area of layer 6 (P<0.001). Temporal and inferior quadrants of the total retinal thickness and most of other quadrants of layer 1 were significantly greater in the men than in the women. Surrounding eight sectors of total retinal thickness and a limited number of sectors in layer 1 and 4 significantly correlated with age. Conclusion. SD-OCT demonstrated the three-dimensional thickness distribution of retinal layers in normal eyes. Thickness of layers varied with sex and age and in different sectors. These variables should be considered while evaluating macular thickness. version:1
arxiv-1312-3168 | Semantic Types, Lexical Sorts and Classifiers | http://arxiv.org/abs/1312.3168 | id:1312.3168 author:Bruno Mery, Christian Retoré category:cs.CL  published:2013-12-11 summary:We propose a cognitively and linguistically motivated set of sorts for lexical semantics in a compositional setting: the classifiers in languages that do have such pronouns. These sorts are needed to include lexical considerations in a semantical analyser such as Boxer or Grail. Indeed, all proposed lexical extensions of usual Montague semantics to model restriction of selection, felicitous and infelicitous copredication require a rich and refined type system whose base types are the lexical sorts, the basis of the many-sorted logic in which semantical representations of sentences are stated. However, none of those approaches define precisely the actual base types or sorts to be used in the lexicon. In this article, we shall discuss some of the options commonly adopted by researchers in formal lexical semantics, and defend the view that classifiers in the languages which have such pronouns are an appealing solution, both linguistically and cognitively motivated. version:1
arxiv-1312-3062 | Fast Neighborhood Graph Search using Cartesian Concatenation | http://arxiv.org/abs/1312.3062 | id:1312.3062 author:Jingdong Wang, Jing Wang, Gang Zeng, Rui Gan, Shipeng Li, Baining Guo category:cs.CV  published:2013-12-11 summary:In this paper, we propose a new data structure for approximate nearest neighbor search. This structure augments the neighborhood graph with a bridge graph. We propose to exploit Cartesian concatenation to produce a large set of vectors, called bridge vectors, from several small sets of subvectors. Each bridge vector is connected with a few reference vectors near to it, forming a bridge graph. Our approach finds nearest neighbors by simultaneously traversing the neighborhood graph and the bridge graph in the best-first strategy. The success of our approach stems from two factors: the exact nearest neighbor search over a large number of bridge vectors can be done quickly, and the reference vectors connected to a bridge (reference) vector near the query are also likely to be near the query. Experimental results on searching over large scale datasets (SIFT, GIST and HOG) show that our approach outperforms state-of-the-art ANN search algorithms in terms of efficiency and accuracy. The combination of our approach with the IVFADC system also shows superior performance over the BIGANN dataset of $1$ billion SIFT features compared with the best previously published result. version:1
arxiv-1312-3061 | Fast Approximate $K$-Means via Cluster Closures | http://arxiv.org/abs/1312.3061 | id:1312.3061 author:Jingdong Wang, Jing Wang, Qifa Ke, Gang Zeng, Shipeng Li category:cs.CV  published:2013-12-11 summary:$K$-means, a simple and effective clustering algorithm, is one of the most widely used algorithms in multimedia and computer vision community. Traditional $k$-means is an iterative algorithm---in each iteration new cluster centers are computed and each data point is re-assigned to its nearest center. The cluster re-assignment step becomes prohibitively expensive when the number of data points and cluster centers are large. In this paper, we propose a novel approximate $k$-means algorithm to greatly reduce the computational complexity in the assignment step. Our approach is motivated by the observation that most active points changing their cluster assignments at each iteration are located on or near cluster boundaries. The idea is to efficiently identify those active points by pre-assembling the data into groups of neighboring points using multiple random spatial partition trees, and to use the neighborhood information to construct a closure for each cluster, in such a way only a small number of cluster candidates need to be considered when assigning a data point to its nearest cluster. Using complexity analysis, image data clustering, and applications to image retrieval, we show that our approach out-performs state-of-the-art approximate $k$-means algorithms in terms of clustering quality and efficiency. version:1
arxiv-1312-3035 | Heat kernel coupling for multiple graph analysis | http://arxiv.org/abs/1312.3035 | id:1312.3035 author:Michael M. Bronstein, Klaus Glashoff category:cs.CV  published:2013-12-11 summary:In this paper, we introduce heat kernel coupling (HKC) as a method of constructing multimodal spectral geometry on weighted graphs of different size without vertex-wise bijective correspondence. We show that Laplacian averaging can be derived as a limit case of HKC, and demonstrate its applications on several problems from the manifold learning and pattern recognition domain. version:1
arxiv-1312-2936 | Active Player Modelling | http://arxiv.org/abs/1312.2936 | id:1312.2936 author:Julian Togelius, Noor Shaker, Georgios N. Yannakakis category:cs.LG  published:2013-12-10 summary:We argue for the use of active learning methods for player modelling. In active learning, the learning algorithm chooses where to sample the search space so as to optimise learning progress. We hypothesise that player modelling based on active learning could result in vastly more efficient learning, but will require big changes in how data is collected. Some example active player modelling scenarios are described. A particular form of active learning is also equivalent to an influential formalisation of (human and machine) curiosity, and games with active learning could therefore be seen as being curious about the player. We further hypothesise that this form of curiosity is symmetric, and therefore that games that explore their players based on the principles of active learning will turn out to select game configurations that are interesting to the player that is being explored. version:1
arxiv-1312-2877 | Automated Classification of L/R Hand Movement EEG Signals using Advanced Feature Extraction and Machine Learning | http://arxiv.org/abs/1312.2877 | id:1312.2877 author:Mohammad H. Alomari, Aya Samaha, Khaled AlKamha category:cs.NE cs.CV cs.HC  published:2013-12-10 summary:In this paper, we propose an automated computer platform for the purpose of classifying Electroencephalography (EEG) signals associated with left and right hand movements using a hybrid system that uses advanced feature extraction techniques and machine learning algorithms. It is known that EEG represents the brain activity by the electrical voltage fluctuations along the scalp, and Brain-Computer Interface (BCI) is a device that enables the use of the brain neural activity to communicate with others or to control machines, artificial limbs, or robots without direct physical movements. In our research work, we aspired to find the best feature extraction method that enables the differentiation between left and right executed fist movements through various classification algorithms. The EEG dataset used in this research was created and contributed to PhysioNet by the developers of the BCI2000 instrumentation system. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts removal was done using AAR. Data was epoched on the basis of Event-Related (De) Synchronization (ERD/ERS) and movement-related cortical potentials (MRCP) features. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta rhythms were isolated for the MRCP analysis. The Independent Component Analysis (ICA) spatial filter was applied on related channels for noise reduction and isolation of both artifactually and neutrally generated EEG sources. The final feature vector included the ERD, ERS, and MRCP features in addition to the mean, power and energy of the activations of the resulting independent components of the epoched feature datasets. The datasets were inputted into two machine-learning algorithms: Neural Networks (NNs) and Support Vector Machines (SVMs). Intensive experiments were carried out and optimum classification performances of 89.8 and 97.1 were obtained using NN and SVM, respectively. version:1
arxiv-1311-6799 | Wavelet and Fast Fourier Transform based analysis of Solar Image | http://arxiv.org/abs/1311.6799 | id:1311.6799 author:Sabyasachi Mukhopadhyay, Debadatta Dash, Swapnil Barmase, Prasanta K Panigrahi category:cs.CV cs.CE  published:2013-10-30 summary:Both of Wavelet and Fast Fourier Transform are strong signal processing tools in the field of Data Analysis. In this paper fast fourier transform (FFT) and Wavelet Transform are employed to observe some important features of Solar image (December, 2004). We have tried to find out the periodicity and coherence of different sections of the solar image. We plotted the distribution of energy in solar surface by analyzing the solar image with scalograms and 3D-coefficient plots. version:2
arxiv-1312-2853 | Performance Analysis Of Neural Network Models For Oxazolines And Oxazoles Derivatives Descriptor Dataset | http://arxiv.org/abs/1312.2853 | id:1312.2853 author:Doreswamy, Chanabasayya . M. Vastrad category:cs.CE cs.NE  published:2013-12-10 summary:Neural networks have been used successfully to a broad range of areas such as business, data mining, drug discovery and biology. In medicine, neural networks have been applied widely in medical diagnosis, detection and evaluation of new drugs and treatment cost estimation. In addition, neural networks have begin practice in data mining strategies for the aim of prediction, knowledge discovery. This paper will present the application of neural networks for the prediction and analysis of antitubercular activity of Oxazolines and Oxazoles derivatives. This study presents techniques based on the development of Single hidden layer neural network (SHLFFNN), Gradient Descent Back propagation neural network (GDBPNN), Gradient Descent Back propagation with momentum neural network (GDBPMNN), Back propagation with Weight decay neural network (BPWDNN) and Quantile regression neural network (QRNN) of artificial neural network (ANN) models Here, we comparatively evaluate the performance of five neural network techniques. The evaluation of the efficiency of each model by ways of benchmark experiments is an accepted application. Cross-validation and resampling techniques are commonly used to derive point estimates of the performances which are compared to identify methods with good properties. Predictive accuracy was evaluated using the root mean squared error (RMSE), Coefficient determination(???), mean absolute error(MAE), mean percentage error(MPE) and relative square error(RSE). We found that all five neural network models were able to produce feasible models. QRNN model is outperforms with all statistical tests amongst other four models. version:1
arxiv-1312-2844 | mARC: Memory by Association and Reinforcement of Contexts | http://arxiv.org/abs/1312.2844 | id:1312.2844 author:Norbert Rimoux, Patrice Descourt category:cs.IR cs.CL nlin.AO nlin.CD  published:2013-12-10 summary:This paper introduces the memory by Association and Reinforcement of Contexts (mARC). mARC is a novel data modeling technology rooted in the second quantization formulation of quantum mechanics. It is an all-purpose incremental and unsupervised data storage and retrieval system which can be applied to all types of signal or data, structured or unstructured, textual or not. mARC can be applied to a wide range of information clas-sification and retrieval problems like e-Discovery or contextual navigation. It can also for-mulated in the artificial life framework a.k.a Conway "Game Of Life" Theory. In contrast to Conway approach, the objects evolve in a massively multidimensional space. In order to start evaluating the potential of mARC we have built a mARC-based Internet search en-gine demonstrator with contextual functionality. We compare the behavior of the mARC demonstrator with Google search both in terms of performance and relevance. In the study we find that the mARC search engine demonstrator outperforms Google search by an order of magnitude in response time while providing more relevant results for some classes of queries. version:1
arxiv-1310-5114 | Explore or exploit? A generic model and an exactly solvable case | http://arxiv.org/abs/1310.5114 | id:1310.5114 author:Thomas Gueudré, Alexander Dobrinevski, Jean-Philippe Bouchaud category:cond-mat.dis-nn cs.LG physics.soc-ph q-fin.GN  published:2013-10-18 summary:Finding a good compromise between the exploitation of known resources and the exploration of unknown, but potentially more profitable choices, is a general problem, which arises in many different scientific disciplines. We propose a stylized model for these exploration-exploitation situations, including population or economic growth, portfolio optimisation, evolutionary dynamics, or the problem of optimal pinning of vortices or dislocations in disordered materials. We find the exact growth rate of this model for tree-like geometries and prove the existence of an optimal migration rate in this case. Numerical simulations in the one-dimensional case confirm the generic existence of an optimum. version:3
arxiv-1312-2789 | Performance Analysis Of Regularized Linear Regression Models For Oxazolines And Oxazoles Derivitive Descriptor Dataset | http://arxiv.org/abs/1312.2789 | id:1312.2789 author:Doreswamy, Chanabasayya . M. Vastrad category:cs.LG  published:2013-12-10 summary:Regularized regression techniques for linear regression have been created the last few ten years to reduce the flaws of ordinary least squares regression with regard to prediction accuracy. In this paper, new methods for using regularized regression in model choice are introduced, and we distinguish the conditions in which regularized regression develops our ability to discriminate models. We applied all the five methods that use penalty-based (regularization) shrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset with far more predictors than observations. The lasso, ridge, elasticnet, lars and relaxed lasso further possess the desirable property that they simultaneously select relevant predictive descriptors and optimally estimate their effects. Here, we comparatively evaluate the performance of five regularized linear regression methods The assessment of the performance of each model by means of benchmark experiments is an established exercise. Cross-validation and resampling methods are generally used to arrive point evaluates the efficiencies which are compared to recognize methods with acceptable features. Predictive accuracy was evaluated using the root mean squared error (RMSE) and Square of usual correlation between predictors and observed mean inhibitory concentration of antitubercular activity (R square). We found that all five regularized regression models were able to produce feasible models and efficient capturing the linearity in the data. The elastic net and lars had similar accuracies as well as lasso and relaxed lasso had similar accuracies but outperformed ridge regression in terms of the RMSE and R square metrics. version:1
arxiv-1309-1508 | Accelerating Hessian-free optimization for deep neural networks by implicit preconditioning and sampling | http://arxiv.org/abs/1309.1508 | id:1309.1508 author:Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin, Bhuvana Ramabhadran category:cs.LG cs.CL cs.NE math.OC stat.ML 65K05  90C15  90C90  published:2013-09-05 summary:Hessian-free training has become a popular parallel second or- der optimization technique for Deep Neural Network training. This study aims at speeding up Hessian-free training, both by means of decreasing the amount of data used for training, as well as through reduction of the number of Krylov subspace solver iterations used for implicit estimation of the Hessian. In this paper, we develop an L-BFGS based preconditioning scheme that avoids the need to access the Hessian explicitly. Since L-BFGS cannot be regarded as a fixed-point iteration, we further propose the employment of flexible Krylov subspace solvers that retain the desired theoretical convergence guarantees of their conventional counterparts. Second, we propose a new sampling algorithm, which geometrically increases the amount of data utilized for gradient and Krylov subspace iteration calculations. On a 50-hr English Broadcast News task, we find that these methodologies provide roughly a 1.5x speed-up, whereas, on a 300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no loss in WER. These results suggest that even further speed-up is expected, as problems scale and complexity grows. version:3
arxiv-1309-1501 | Improvements to deep convolutional neural networks for LVCSR | http://arxiv.org/abs/1309.1501 | id:1309.1501 author:Tara N. Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E. Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y. Aravkin, Bhuvana Ramabhadran category:cs.LG cs.CL cs.NE math.OC stat.ML 65K05  90C15  90C90  published:2013-09-05 summary:Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural Networks (DNN), as they are able to better reduce spectral variation in the input signal. This has also been confirmed experimentally, with CNNs showing improvements in word error rate (WER) between 4-12% relative compared to DNNs across a variety of LVCSR tasks. In this paper, we describe different methods to further improve CNN performance. First, we conduct a deep analysis comparing limited weight sharing and full weight sharing with state-of-the-art features. Second, we apply various pooling strategies that have shown improvements in computer vision to an LVCSR speech task. Third, we introduce a method to effectively incorporate speaker adaptation, namely fMLLR, into log-mel features. Fourth, we introduce an effective strategy to use dropout during Hessian-free sequence training. We find that with these improvements, particularly with fMLLR and dropout, we are able to achieve an additional 2-3% relative improvement in WER on a 50-hour Broadcast News task over our previous best CNN baseline. On a larger 400-hour BN task, we find an additional 4-5% relative improvement over our previous best CNN baseline. version:3
arxiv-1312-2710 | Improving circuit miniaturization and its efficiency using Rough Set Theory | http://arxiv.org/abs/1312.2710 | id:1312.2710 author:Sarvesh SS Rawat, Dheeraj Dilip Mor, Anugrah Kumar, Sanjiban Shekar Roy, Rohit kumar category:cs.LG cs.AI  published:2013-12-10 summary:High-speed, accuracy, meticulousness and quick response are notion of the vital necessities for modern digital world. An efficient electronic circuit unswervingly affects the maneuver of the whole system. Different tools are required to unravel different types of engineering tribulations. Improving the efficiency, accuracy and low power consumption in an electronic circuit is always been a bottle neck problem. So the need of circuit miniaturization is always there. It saves a lot of time and power that is wasted in switching of gates, the wiring-crises is reduced, cross-sectional area of chip is reduced, the number of transistors that can implemented in chip is multiplied many folds. Therefore to trounce with this problem we have proposed an Artificial intelligence (AI) based approach that make use of Rough Set Theory for its implementation. Theory of rough set has been proposed by Z Pawlak in the year 1982. Rough set theory is a new mathematical tool which deals with uncertainty and vagueness. Decisions can be generated using rough set theory by reducing the unwanted and superfluous data. We have condensed the number of gates without upsetting the productivity of the given circuit. This paper proposes an approach with the help of rough set theory which basically lessens the number of gates in the circuit, based on decision rules. version:1
arxiv-1312-2606 | Multi-Task Classification Hypothesis Space with Improved Generalization Bounds | http://arxiv.org/abs/1312.2606 | id:1312.2606 author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2013-12-09 summary:This paper presents a RKHS, in general, of vector-valued functions intended to be used as hypothesis space for multi-task classification. It extends similar hypothesis spaces that have previously considered in the literature. Assuming this space, an improved Empirical Rademacher Complexity-based generalization bound is derived. The analysis is itself extended to an MKL setting. The connection between the proposed hypothesis space and a Group-Lasso type regularizer is discussed. Finally, experimental results, with some SVM-based Multi-Task Learning problems, underline the quality of the derived bounds and validate the paper's analysis. version:1
arxiv-1310-7170 | Object Recognition System Design in Computer Vision: a Universal Approach | http://arxiv.org/abs/1310.7170 | id:1310.7170 author:Andrew Gleibman category:cs.CV  published:2013-10-27 summary:The first contribution of this paper is architecture of a multipurpose system, which delegates a range of object detection tasks to a classifier, applied in special grid positions of the tested image. The second contribution is Gray Level-Radius Co-occurrence Matrix, which describes local image texture and topology and, unlike common second order statistics methods, is robust to image resolution. The third contribution is a parametrically controlled automatic synthesis of unlimited number of numerical features for classification. The fourth contribution is a method of optimizing parameters C and gamma in LibSVM-based classifier, which is 20-100 times faster than the commonly applied method. The work is essentially experimental, with demonstration of various methods for definition of objects of interest in images and video sequences. version:2
arxiv-1312-2383 | On the Performance of Filters for Reduction of Speckle Noise in SAR Images off the Coast of the Gulf of Guinea | http://arxiv.org/abs/1312.2383 | id:1312.2383 author:Griffith S. Klogo, Akpeko Gasonoo, Isaac K. E. Ampomah category:cs.CV  published:2013-12-09 summary:Synthetic Aperture Radar (SAR) imagery to monitor oil spills are some methods that have been proposed for the West African sub-region. With the increase in the number of oil exploration companies in Ghana (and her neighbors) and the rise in the coastal activities in the sub-region, there is the need for proper monitoring of the environmental impact of these socio-economic activities on the environment. Detection and near real-time information about oil spills are fundamental in reducing oil spill environmental impact. SAR images are prone to some noise, which is predominantly speckle noise around the coastal areas. This paper evaluates the performance of the mean and median filters used in the preprocessing filtering to reduce speckle noise in SAR images for most image processing algorithms. version:1
arxiv-1312-2368 | A Unified Markov Chain Approach to Analysing Randomised Search Heuristics | http://arxiv.org/abs/1312.2368 | id:1312.2368 author:Jun He, Feidun He, Xin Yao category:math.OC cs.NE  published:2013-12-09 summary:The convergence, convergence rate and expected hitting time play fundamental roles in the analysis of randomised search heuristics. This paper presents a unified Markov chain approach to studying them. Using the approach, the sufficient and necessary conditions of convergence in distribution are established. Then the average convergence rate is introduced to randomised search heuristics and its lower and upper bounds are derived. Finally, novel average drift analysis and backward drift analysis are proposed for bounding the expected hitting time. A computational study is also conducted to investigate the convergence, convergence rate and expected hitting time. The theoretical study belongs to a prior and general study while the computational study belongs to a posterior and case study. version:1
arxiv-1312-2366 | A preliminary survey on optimized multiobjective metaheuristic methods for data clustering using evolutionary approaches | http://arxiv.org/abs/1312.2366 | id:1312.2366 author:Ramachandra Rao Kurada, Dr. K Karteeka Pavan, Dr. AV Dattareya Rao category:cs.NE  published:2013-12-09 summary:The present survey provides the state-of-the-art of research, copiously devoted to Evolutionary Approach (EAs) for clustering exemplified with a diversity of evolutionary computations. The Survey provides a nomenclature that highlights some aspects that are very important in the context of evolutionary data clustering. The paper missions the clustering trade-offs branched out with wide-ranging Multi Objective Evolutionary Approaches (MOEAs) methods. Finally, this study addresses the potential challenges of MOEA design and data clustering, along with conclusions and recommendations for novice and researchers by positioning most promising paths of future research. MOEAs have substantial success across a variety of MOP applications, from pedagogical multifunction optimization to real-world engineering design. The survey paper noticeably organizes the developments witnessed in the past three decades for EAs based metaheuristics to solve multiobjective optimization problems (MOP) and to derive significant progression in ruling high quality elucidations in a single run. Data clustering is an exigent task, whose intricacy is caused by a lack of unique and precise definition of a cluster. The discrete optimization problem uses the cluster space to derive a solution for Multiobjective data clustering. Discovery of a majority or all of the clusters (of illogical shapes) present in the data is a long-standing goal of unsupervised predictive learning problems or exploratory pattern analysis. version:1
arxiv-1311-0966 | Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems | http://arxiv.org/abs/1311.0966 | id:1311.0966 author:Emre Neftci, Srinjoy Das, Bruno Pedroni, Kenneth Kreutz-Delgado, Gert Cauwenberghs category:cs.NE q-bio.NC  published:2013-11-05 summary:Restricted Boltzmann Machines (RBMs) and Deep Belief Networks have been demonstrated to perform efficiently in a variety of applications, such as dimensionality reduction, feature learning, and classification. Their implementation on neuromorphic hardware platforms emulating large-scale networks of spiking neurons can have significant advantages from the perspectives of scalability, power dissipation and real-time interfacing with the environment. However the traditional RBM architecture and the commonly used training algorithm known as Contrastive Divergence (CD) are based on discrete updates and exact arithmetics which do not directly map onto a dynamical neural substrate. Here, we present an event-driven variation of CD to train a RBM constructed with Integrate & Fire (I&F) neurons, that is constrained by the limitations of existing and near future neuromorphic hardware platforms. Our strategy is based on neural sampling, which allows us to synthesize a spiking neural network that samples from a target Boltzmann distribution. The recurrent activity of the network replaces the discrete steps of the CD algorithm, while Spike Time Dependent Plasticity (STDP) carries out the weight updates in an online, asynchronous fashion. We demonstrate our approach by training an RBM composed of leaky I&F neurons with STDP synapses to learn a generative model of the MNIST hand-written digit dataset, and by testing it in recognition, generation and cue integration tasks. Our results contribute to a machine learning-driven approach for synthesizing networks of spiking neurons capable of carrying out practical, high-level functionality. version:3
arxiv-1309-1392 | Bayesian Structural Inference for Hidden Processes | http://arxiv.org/abs/1309.1392 | id:1309.1392 author:Christopher C. Strelioff, James P. Crutchfield category:stat.ML cs.LG math.ST nlin.CD physics.data-an stat.TH  published:2013-09-05 summary:We introduce a Bayesian approach to discovering patterns in structurally complex processes. The proposed method of Bayesian Structural Inference (BSI) relies on a set of candidate unifilar HMM (uHMM) topologies for inference of process structure from a data series. We employ a recently developed exact enumeration of topological epsilon-machines. (A sequel then removes the topological restriction.) This subset of the uHMM topologies has the added benefit that inferred models are guaranteed to be epsilon-machines, irrespective of estimated transition probabilities. Properties of epsilon-machines and uHMMs allow for the derivation of analytic expressions for estimating transition probabilities, inferring start states, and comparing the posterior probability of candidate model topologies, despite process internal structure being only indirectly present in data. We demonstrate BSI's effectiveness in estimating a process's randomness, as reflected by the Shannon entropy rate, and its structure, as quantified by the statistical complexity. We also compare using the posterior distribution over candidate models and the single, maximum a posteriori model for point estimation and show that the former more accurately reflects uncertainty in estimated values. We apply BSI to in-class examples of finite- and infinite-order Markov processes, as well to an out-of-class, infinite-state hidden process. version:2
arxiv-1205-6548 | State Transition Algorithm | http://arxiv.org/abs/1205.6548 | id:1205.6548 author:Xiaojun Zhou, Chunhua Yang, Weihua Gui category:math.OC cs.NE 90C26  90C30  90C59  published:2012-05-30 summary:In terms of the concepts of state and state transition, a new heuristic random search algorithm named state transition algorithm is proposed. For continuous function optimization problems, four special transformation operators called rotation, translation, expansion and axesion are designed. Adjusting measures of the transformations are mainly studied to keep the balance of exploration and exploitation. Convergence analysis is also discussed about the algorithm based on random search theory. In the meanwhile, to strengthen the search ability in high dimensional space, communication strategy is introduced into the basic algorithm and intermittent exchange is presented to prevent premature convergence. Finally, experiments are carried out for the algorithms. With 10 common benchmark unconstrained continuous functions used to test the performance, the results show that state transition algorithms are promising algorithms due to their good global search capability and convergence property when compared with some popular algorithms. version:4
arxiv-1312-2249 | Scalable Object Detection using Deep Neural Networks | http://arxiv.org/abs/1312.2249 | id:1312.2249 author:Dumitru Erhan, Christian Szegedy, Alexander Toshev, Dragomir Anguelov category:cs.CV stat.ML  published:2013-12-08 summary:Deep convolutional neural networks have recently achieved state-of-the-art performance on a number of image recognition benchmarks, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC-2012). The winning model on the localization sub-task was a network that predicts a single bounding box and a confidence score for each object category in the image. Such a model captures the whole-image context around the objects but cannot handle multiple instances of the same object in the image without naively replicating the number of outputs for each instance. In this work, we propose a saliency-inspired neural network model for detection, which predicts a set of class-agnostic bounding boxes along with a single score for each box, corresponding to its likelihood of containing any object of interest. The model naturally handles a variable number of instances for each class and allows for cross-class generalization at the highest levels of the network. We are able to obtain competitive recognition performance on VOC2007 and ILSVRC2012, while using only the top few predicted locations in each image and a small number of neural network evaluations. version:1
arxiv-1311-3287 | Nonparametric Estimation of Multi-View Latent Variable Models | http://arxiv.org/abs/1311.3287 | id:1311.3287 author:Le Song, Animashree Anandkumar, Bo Dai, Bo Xie category:cs.LG stat.ML  published:2013-11-13 summary:Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments. version:2
arxiv-1312-2154 | Sequential Monte Carlo Inference of Mixed Membership Stochastic Blockmodels for Dynamic Social Networks | http://arxiv.org/abs/1312.2154 | id:1312.2154 author:Tomoki Kobayashi, Koji Eguchi category:cs.SI cs.LG stat.ML  published:2013-12-07 summary:Many kinds of data can be represented as a network or graph. It is crucial to infer the latent structure underlying such a network and to predict unobserved links in the network. Mixed Membership Stochastic Blockmodel (MMSB) is a promising model for network data. Latent variables and unknown parameters in MMSB have been estimated through Bayesian inference with the entire network; however, it is important to estimate them online for evolving networks. In this paper, we first develop online inference methods for MMSB through sequential Monte Carlo methods, also known as particle filters. We then extend them for time-evolving networks, taking into account the temporal dependency of the network structure. We demonstrate through experiments that the time-dependent particle filter outperformed several baselines in terms of prediction performance in an online condition. version:1
arxiv-1312-2137 | End-to-end Phoneme Sequence Recognition using Convolutional Neural Networks | http://arxiv.org/abs/1312.2137 | id:1312.2137 author:Dimitri Palaz, Ronan Collobert, Mathew Magimai. -Doss category:cs.LG cs.CL cs.NE  published:2013-12-07 summary:Most phoneme recognition state-of-the-art systems rely on a classical neural network classifiers, fed with highly tuned features, such as MFCC or PLP features. Recent advances in ``deep learning'' approaches questioned such systems, but while some attempts were made with simpler features such as spectrograms, state-of-the-art systems still rely on MFCCs. This might be viewed as a kind of failure from deep learning approaches, which are often claimed to have the ability to train with raw signals, alleviating the need of hand-crafted features. In this paper, we investigate a convolutional neural network approach for raw speech signals. While convolutional architectures got tremendous success in computer vision or text processing, they seem to have been let down in the past recent years in the speech processing field. We show that it is possible to learn an end-to-end phoneme sequence classifier system directly from raw signal, with similar performance on the TIMIT and WSJ datasets than existing systems based on MFCC, questioning the need of complex hand-crafted features on large datasets. version:1
arxiv-1312-2132 | Robust Subspace System Identification via Weighted Nuclear Norm Optimization | http://arxiv.org/abs/1312.2132 | id:1312.2132 author:Dorsa Sadigh, Henrik Ohlsson, S. Shankar Sastry, Sanjit A. Seshia category:cs.SY cs.LG stat.ML  published:2013-12-07 summary:Subspace identification is a classical and very well studied problem in system identification. The problem was recently posed as a convex optimization problem via the nuclear norm relaxation. Inspired by robust PCA, we extend this framework to handle outliers. The proposed framework takes the form of a convex optimization problem with an objective that trades off fit, rank and sparsity. As in robust PCA, it can be problematic to find a suitable regularization parameter. We show how the space in which a suitable parameter should be sought can be limited to a bounded open set of the two dimensional parameter space. In practice, this is very useful since it restricts the parameter space that is needed to be surveyed. version:1
arxiv-1312-2087 | Towards Structural Natural Language Formalization: Mapping Discourse to Controlled Natural Language | http://arxiv.org/abs/1312.2087 | id:1312.2087 author:Nicholas H. Kirk category:cs.CL  published:2013-12-07 summary:The author describes a conceptual study towards mapping grounded natural language discourse representation structures to instances of controlled language statements. This can be achieved via a pipeline of preexisting state of the art technologies, namely natural language syntax to semantic discourse mapping, and a reduction of the latter to controlled language discourse, given a set of previously learnt reduction rules. Concludingly a description on evaluation, potential and limitations for ontology-based reasoning is presented. version:1
arxiv-1312-2061 | Region and Location Based Indexing and Retrieval of MR-T2 Brain Tumor Images | http://arxiv.org/abs/1312.2061 | id:1312.2061 author:Krishna A N, B G Prasad category:cs.CV cs.IR  published:2013-12-07 summary:In this paper, region based and location based retrieval systems have been implemented for retrieval of MR-T2 axial 2-D brain images. This is done by extracting and characterizing the tumor portion of 2-D brain slices by use of a suitable threshold computed over the entire image. Indexing and retrieval is then performed by computing texture features based on gray-tone spatial-dependence matrix of segmented regions. A Hash structure is used to index all images. A combined index is adopted to point to all similar images in terms of the texture features. At query time, only those images that are in the same hash bucket as those of the queried image are compared for similarity, thus reducing the search space and time. version:1
arxiv-1308-2930 | Semistability-Based Convergence Analysis for Paracontracting Multiagent Coordination Optimization | http://arxiv.org/abs/1308.2930 | id:1308.2930 author:Qing Hui, Haopeng Zhang category:cs.SY cs.NE math.OC 90C59  93D99 G.1.0; G.1.6; I.2.8  published:2013-08-13 summary:This sequential technical report extends some of the previous results we posted at arXiv:1306.0225. version:6
arxiv-1311-4472 | A Component Lasso | http://arxiv.org/abs/1311.4472 | id:1311.4472 author:Nadine Hussami, Robert Tibshirani category:stat.ML cs.LG 62J07  published:2013-11-18 summary:We propose a new sparse regression method called the component lasso, based on a simple idea. The method uses the connected-components structure of the sample covariance matrix to split the problem into smaller ones. It then solves the subproblems separately, obtaining a coefficient vector for each one. Then, it uses non-negative least squares to recombine the different vectors into a single solution. This step is useful in selecting and reweighting components that are correlated with the response. Simulated and real data examples show that the component lasso can outperform standard regression methods such as the lasso and elastic net, achieving a lower mean squared error as well as better support recovery. version:2
arxiv-1312-1970 | An Algorithmic Theory of Dependent Regularizers, Part 1: Submodular Structure | http://arxiv.org/abs/1312.1970 | id:1312.1970 author:Hoyt Koepke, Marina Meila category:stat.ML  published:2013-12-06 summary:We present an exploration of the rich theoretical connections between several classes of regularized models, network flows, and recent results in submodular function theory. This work unifies key aspects of these problems under a common theory, leading to novel methods for working with several important models of interest in statistics, machine learning and computer vision. In Part 1, we review the concepts of network flows and submodular function optimization theory foundational to our results. We then examine the connections between network flows and the minimum-norm algorithm from submodular optimization, extending and improving several current results. This leads to a concise representation of the structure of a large class of pairwise regularized models important in machine learning, statistics and computer vision. In Part 2, we describe the full regularization path of a class of penalized regression problems with dependent variables that includes the graph-guided LASSO and total variation constrained models. This description also motivates a practical algorithm. This allows us to efficiently find the regularization path of the discretized version of TV penalized models. Ultimately, our new algorithms scale up to high-dimensional problems with millions of variables. version:1
arxiv-1309-3223 | Partitioning into Expanders | http://arxiv.org/abs/1309.3223 | id:1309.3223 author:Shayan Oveis Gharan, Luca Trevisan category:cs.DS math.SP stat.ML  published:2013-09-12 summary:Let G=(V,E) be an undirected graph, lambda_k be the k-th smallest eigenvalue of the normalized laplacian matrix of G. There is a basic fact in algebraic graph theory that lambda_k > 0 if and only if G has at most k-1 connected components. We prove a robust version of this fact. If lambda_k>0, then for some 1\leq \ell\leq k-1, V can be {\em partitioned} into l sets P_1,\ldots,P_l such that each P_i is a low-conductance set in G and induces a high conductance induced subgraph. In particular, \phi(P_i)=O(l^3\sqrt{\lambda_l}) and \phi(G[P_i]) >= \lambda_k/k^2). We make our results algorithmic by designing a simple polynomial time spectral algorithm to find such partitioning of G with a quadratic loss in the inside conductance of P_i's. Unlike the recent results on higher order Cheeger's inequality [LOT12,LRTV12], our algorithmic results do not use higher order eigenfunctions of G. If there is a sufficiently large gap between lambda_k and lambda_{k+1}, more precisely, if \lambda_{k+1} >= \poly(k) lambda_{k}^{1/4} then our algorithm finds a k partitioning of V into sets P_1,...,P_k such that the induced subgraph G[P_i] has a significantly larger conductance than the conductance of P_i in G. Such a partitioning may represent the best k clustering of G. Our algorithm is a simple local search that only uses the Spectral Partitioning algorithm as a subroutine. We expect to see further applications of this simple algorithm in clustering applications. version:3
arxiv-1312-2451 | CEAI: CCM based Email Authorship Identification Model | http://arxiv.org/abs/1312.2451 | id:1312.2451 author:Sarwat Nizamani, Nasrullah Memon category:cs.LG  published:2013-12-06 summary:In this paper we present a model for email authorship identification (EAI) by employing a Cluster-based Classification (CCM) technique. Traditionally, stylometric features have been successfully employed in various authorship analysis tasks; we extend the traditional feature-set to include some more interesting and effective features for email authorship identification (e.g. the last punctuation mark used in an email, the tendency of an author to use capitalization at the start of an email, or the punctuation after a greeting or farewell). We also included Info Gain feature selection based content features. It is observed that the use of such features in the authorship identification process has a positive impact on the accuracy of the authorship identification task. We performed experiments to justify our arguments and compared the results with other base line models. Experimental results reveal that the proposed CCM-based email authorship identification model, along with the proposed feature set, outperforms the state-of-the-art support vector machine (SVM)-based models, as well as the models proposed by Iqbal et al. [1, 2]. The proposed model attains an accuracy rate of 94% for 10 authors, 89% for 25 authors, and 81% for 50 authors, respectively on Enron dataset, while 89.5% accuracy has been achieved on authors' constructed real email dataset. The results on Enron dataset have been achieved on quite a large number of authors as compared to the models proposed by Iqbal et al. [1, 2]. version:1
arxiv-1310-1285 | Semantic Measures for the Comparison of Units of Language, Concepts or Instances from Text and Knowledge Base Analysis | http://arxiv.org/abs/1310.1285 | id:1310.1285 author:Sébastien Harispe, Sylvie Ranwez, Stefan Janaqi, Jacky Montmain category:cs.CL  published:2013-10-04 summary:Semantic measures are widely used today to estimate the strength of the semantic relationship between elements of various types: units of language (e.g., words, sentences, documents), concepts or even instances semantically characterized (e.g., diseases, genes, geographical locations). Semantic measures play an important role to compare such elements according to semantic proxies: texts and knowledge representations, which support their meaning or describe their nature. Semantic measures are therefore essential for designing intelligent agents which will for example take advantage of semantic analysis to mimic human ability to compare abstract or concrete objects. This paper proposes a comprehensive survey of the broad notion of semantic measure for the comparison of units of language, concepts or instances based on semantic proxy analyses. Semantic measures generalize the well-known notions of semantic similarity, semantic relatedness and semantic distance, which have been extensively studied by various communities over the last decades (e.g., Cognitive Sciences, Linguistics, and Artificial Intelligence to mention a few). version:2
arxiv-1310-0354 | Deep and Wide Multiscale Recursive Networks for Robust Image Labeling | http://arxiv.org/abs/1310.0354 | id:1310.0354 author:Gary B. Huang, Viren Jain category:cs.CV cs.LG  published:2013-10-01 summary:Feedforward multilayer networks trained by supervised learning have recently demonstrated state of the art performance on image labeling problems such as boundary prediction and scene parsing. As even very low error rates can limit practical usage of such systems, methods that perform closer to human accuracy remain desirable. In this work, we propose a new type of network with the following properties that address what we hypothesize to be limiting aspects of existing methods: (1) a `wide' structure with thousands of features, (2) a large field of view, (3) recursive iterations that exploit statistical dependencies in label space, and (4) a parallelizable architecture that can be trained in a fraction of the time compared to benchmark multilayer convolutional networks. For the specific image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. Experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these "Deep And Wide Multiscale Recursive" (DAWMR) networks lead to new levels of image labeling performance. The highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input field of view of 157,464 voxels ($54^3$) to make a prediction at each image location. We present an associated open source software package that enables the simple and flexible creation of DAWMR networks. version:3
arxiv-1312-1760 | Towards Normalizing the Edit Distance Using a Genetic Algorithms Based Scheme | http://arxiv.org/abs/1312.1760 | id:1312.1760 author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI  published:2013-12-06 summary:The normalized edit distance is one of the distances derived from the edit distance. It is useful in some applications because it takes into account the lengths of the two strings compared. The normalized edit distance is not defined in terms of edit operations but rather in terms of the edit path. In this paper we propose a new derivative of the edit distance that also takes into consideration the lengths of the two strings, but the new distance is related directly to the edit distance. The particularity of the new distance is that it uses the genetic algorithms to set the values of the parameters it uses. We conduct experiments to test the new distance and we obtain promising results. version:1
arxiv-1312-1752 | Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation | http://arxiv.org/abs/1312.1752 | id:1312.1752 author:Muhammad Marwan Muhammad Fuad category:cs.NE cs.AI  published:2013-12-06 summary:Bio-inspired optimization algorithms have been gaining more popularity recently. One of the most important of these algorithms is particle swarm optimization (PSO). PSO is based on the collective intelligence of a swam of particles. Each particle explores a part of the search space looking for the optimal position and adjusts its position according to two factors; the first is its own experience and the second is the collective experience of the whole swarm. PSO has been successfully used to solve many optimization problems. In this work we use PSO to improve the performance of a well-known representation method of time series data which is the symbolic aggregate approximation (SAX). As with other time series representation methods, SAX results in loss of information when applied to represent time series. In this paper we use PSO to propose a new minimum distance WMD for SAX to remedy this problem. Unlike the original minimum distance, the new distance sets different weights to different segments of the time series according to their information content. This weighted minimum distance enhances the performance of SAX as we show through experiments using different time series datasets. version:1
arxiv-1111-4601 | Non-Asymptotic Analysis of Tangent Space Perturbation | http://arxiv.org/abs/1111.4601 | id:1111.4601 author:Daniel N. Kaslovsky, Francois G. Meyer category:physics.data-an cs.NA stat.ML 62H25  15A42  60B20  published:2011-11-20 summary:Constructing an efficient parameterization of a large, noisy data set of points lying close to a smooth manifold in high dimension remains a fundamental problem. One approach consists in recovering a local parameterization using the local tangent plane. Principal component analysis (PCA) is often the tool of choice, as it returns an optimal basis in the case of noise-free samples from a linear subspace. To process noisy data samples from a nonlinear manifold, PCA must be applied locally, at a scale small enough such that the manifold is approximately linear, but at a scale large enough such that structure may be discerned from noise. Using eigenspace perturbation theory and non-asymptotic random matrix theory, we study the stability of the subspace estimated by PCA as a function of scale, and bound (with high probability) the angle it forms with the true tangent space. By adaptively selecting the scale that minimizes this bound, our analysis reveals an appropriate scale for local tangent plane recovery. We also introduce a geometric uncertainty principle quantifying the limits of noise-curvature perturbation for stable recovery. With the purpose of providing perturbation bounds that can be used in practice, we propose plug-in estimates that make it possible to directly apply the theoretical results to real data sets. version:4
arxiv-1312-1737 | Curriculum Learning for Handwritten Text Line Recognition | http://arxiv.org/abs/1312.1737 | id:1312.1737 author:Jérôme Louradour, Christopher Kermorvant category:cs.LG  published:2013-12-05 summary:Recurrent Neural Networks (RNN) have recently achieved the best performance in off-line Handwriting Text Recognition. At the same time, learning RNN by gradient descent leads to slow convergence, and training times are particularly long when the training database consists of full lines of text. In this paper, we propose an easy way to accelerate stochastic gradient descent in this set-up, and in the general context of learning to recognize sequences. The principle is called Curriculum Learning, or shaping. The idea is to first learn to recognize short sequences before training on all available training sequences. Experiments on three different handwritten text databases (Rimes, IAM, OpenHaRT) show that a simple implementation of this strategy can significantly speed up the training of RNN for Text Recognition, and even significantly improve performance in some cases. version:1
arxiv-1312-1725 | Book embeddings of Reeb graphs | http://arxiv.org/abs/1312.1725 | id:1312.1725 author:Vitaliy Kurlin category:cs.CG cs.CV math.GT  published:2013-12-05 summary:Let $X$ be a simplicial complex with a piecewise linear function $f:X\to\mathbb{R}$. The Reeb graph $Reeb(f,X)$ is the quotient of $X$, where we collapse each connected component of $f^{-1}(t)$ to a single point. Let the nodes of $Reeb(f,X)$ be all homologically critical points where any homology of the corresponding component of the level set $f^{-1}(t)$ changes. Then we can label every arc of $Reeb(f,X)$ with the Betti numbers $(\beta_1,\beta_2,\dots,\beta_d)$ of the corresponding $d$-dimensional component of a level set. The homology labels give more information about the original complex $X$ than the classical Reeb graph. We describe a canonical embedding of a Reeb graph into a multi-page book (a star cross a line) and give a unique linear code of this book embedding. version:1
arxiv-1312-1613 | Max-Min Distance Nonnegative Matrix Factorization | http://arxiv.org/abs/1312.1613 | id:1312.1613 author:Jim Jing-Yan Wang category:stat.ML cs.LG cs.NA  published:2013-12-05 summary:Nonnegative Matrix Factorization (NMF) has been a popular representation method for pattern classification problem. It tries to decompose a nonnegative matrix of data samples as the product of a nonnegative basic matrix and a nonnegative coefficient matrix, and the coefficient matrix is used as the new representation. However, traditional NMF methods ignore the class labels of the data samples. In this paper, we proposed a supervised novel NMF algorithm to improve the discriminative ability of the new representation. Using the class labels, we separate all the data sample pairs into within-class pairs and between-class pairs. To improve the discriminate ability of the new NMF representations, we hope that the maximum distance of the within-class pairs in the new NMF space could be minimized, while the minimum distance of the between-class pairs pairs could be maximized. With this criterion, we construct an objective function and optimize it with regard to basic and coefficient matrices and slack variables alternatively, resulting in a iterative algorithm. version:1
arxiv-1310-5884 | The optimality of attaching unlinked labels to unlinked meanings | http://arxiv.org/abs/1310.5884 | id:1310.5884 author:Ramon Ferrer-i-Cancho category:cs.CL physics.data-an physics.soc-ph  published:2013-10-22 summary:Vocabulary learning by children can be characterized by many biases. When encountering a new word, children as well as adults, are biased towards assuming that it means something totally different from the words that they already know. To the best of our knowledge, the 1st mathematical proof of the optimality of this bias is presented here. First, it is shown that this bias is a particular case of the maximization of mutual information between words and meanings. Second, the optimality is proven within a more general information theoretic framework where mutual information maximization competes with other information theoretic principles. The bias is a prediction from modern information theory. The relationship between information theoretic principles and the principles of contrast and mutual exclusivity is also shown. version:2
arxiv-1312-1522 | Iterative Log Thresholding | http://arxiv.org/abs/1312.1522 | id:1312.1522 author:Dmitry Malioutov, Aleksandr Aravkin category:stat.ML math.OC  published:2013-12-05 summary:Sparse reconstruction approaches using the re-weighted l1-penalty have been shown, both empirically and theoretically, to provide a significant improvement in recovering sparse signals in comparison to the l1-relaxation. However, numerical optimization of such penalties involves solving problems with l1-norms in the objective many times. Using the direct link of reweighted l1-penalties to the concave log-regularizer for sparsity, we derive a simple prox-like algorithm for the log-regularized formulation. The proximal splitting step of the algorithm has a closed form solution, and we call the algorithm 'log-thresholding' in analogy to soft thresholding for the l1-penalty. We establish convergence results, and demonstrate that log-thresholding provides more accurate sparse reconstructions compared to both soft and hard thresholding. Furthermore, the approach can be directly extended to optimization over matrices with penalty for rank (i.e. the nuclear norm penalty and its re-weigthed version), where we suggest a singular-value log-thresholding approach. version:1
arxiv-1312-1685 | Human Face Recognition using Gabor based Kernel Entropy Component Analysis | http://arxiv.org/abs/1312.1685 | id:1312.1685 author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV  published:2013-12-05 summary:In this paper, we present a novel Gabor wavelet based Kernel Entropy Component Analysis (KECA) method by integrating the Gabor wavelet transformation (GWT) of facial images with the KECA method for enhanced face recognition performance. Firstly, from the Gabor wavelet transformed images the most important discriminative desirable facial features characterized by spatial frequency, spatial locality and orientation selectivity to cope with the variations due to illumination and facial expression changes were derived. After that KECA, relating to the Renyi entropy is extended to include cosine kernel function. The KECA with the cosine kernels is then applied on the extracted most important discriminating feature vectors of facial images to obtain only those real kernel ECA eigenvectors that are associated with eigenvalues having positive entropy contribution. Finally, these real KECA features are used for image classification using the L1, L2 distance measures; the Mahalanobis distance measure and the cosine similarity measure. The feasibility of the Gabor based KECA method with the cosine kernel has been successfully tested on both frontal and pose-angled face recognition, using datasets from the ORL, FRAV2D and the FERET database. version:1
arxiv-1312-1520 | A Face Recognition approach based on entropy estimate of the nonlinear DCT features in the Logarithm Domain together with Kernel Entropy Component Analysis | http://arxiv.org/abs/1312.1520 | id:1312.1520 author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV  published:2013-12-05 summary:This paper exploits the feature extraction capabilities of the discrete cosine transform (DCT) together with an illumination normalization approach in the logarithm domain that increase its robustness to variations in facial geometry and illumination. Secondly in the same domain the entropy measures are applied on the DCT coefficients so that maximum entropy preserving pixels can be extracted as the feature vector. Thus the informative features of a face can be extracted in a low dimensional space. Finally, the kernel entropy component analysis (KECA) with an extension of arc cosine kernels is applied on the extracted DCT coefficients that contribute most to the entropy estimate to obtain only those real kernel ECA eigenvectors that are associated with eigenvalues having high positive entropy contribution. The resulting system was successfully tested on real image sequences and is robust to significant partial occlusion and illumination changes, validated with the experiments on the FERET, AR, FRAV2D and ORL face databases. Experimental comparison is demonstrated to prove the superiority of the proposed approach in respect to recognition accuracy. Using specificity and sensitivity we find that the best is achieved when Renyi entropy is applied on the DCT coefficients. Extensive experimental comparison is demonstrated to prove the superiority of the proposed approach in respect to recognition accuracy. Moreover, the proposed approach is very simple, computationally fast and can be implemented in any real-time face recognition system. version:1
arxiv-1312-1684 | High Performance Human Face Recognition using Gabor based Pseudo Hidden Markov Model | http://arxiv.org/abs/1312.1684 | id:1312.1684 author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV  published:2013-12-05 summary:This paper introduces a novel methodology that combines the multi-resolution feature of the Gabor wavelet transformation (GWT) with the local interactions of the facial structures expressed through the Pseudo Hidden Markov model (PHMM). Unlike the traditional zigzag scanning method for feature extraction a continuous scanning method from top-left corner to right then top-down and right to left and so on until right-bottom of the image i.e. a spiral scanning technique has been proposed for better feature selection. Unlike traditional HMMs, the proposed PHMM does not perform the state conditional independence of the visible observation sequence assumption. This is achieved via the concept of local structures introduced by the PHMM used to extract facial bands and automatically select the most informative features of a face image. Thus, the long-range dependency problem inherent to traditional HMMs has been drastically reduced. Again with the use of most informative pixels rather than the whole image makes the proposed method reasonably faster for face recognition. This method has been successfully tested on frontal face images from the ORL, FRAV2D and FERET face databases where the images vary in pose, illumination, expression, and scale. The FERET data set contains 2200 frontal face images of 200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjects and the full ORL database is considered. The results reported in this application are far better than the recent and most referred systems. version:1
arxiv-1312-1517 | A Gabor block based Kernel Discriminative Common Vector (KDCV) approach using cosine kernels for Human Face Recognition | http://arxiv.org/abs/1312.1517 | id:1312.1517 author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV  published:2013-12-05 summary:In this paper a nonlinear Gabor Wavelet Transform (GWT) discriminant feature extraction approach for enhanced face recognition is proposed. Firstly, the low-energized blocks from Gabor wavelet transformed images are extracted. Secondly, the nonlinear discriminating features are analyzed and extracted from the selected low-energized blocks by the generalized Kernel Discriminative Common Vector (KDCV) method. The KDCV method is extended to include cosine kernel function in the discriminating method. The KDCV with the cosine kernels is then applied on the extracted low energized discriminating feature vectors to obtain the real component of a complex quantity for face recognition. In order to derive positive kernel discriminative vectors; we apply only those kernel discriminative eigenvectors that are associated with non-zero eigenvalues. The feasibility of the low energized Gabor block based generalized KDCV method with cosine kernel function models has been successfully tested for image classification using the L1, L2 distance measures; and the cosine similarity measure on both frontal and pose-angled face recognition. Experimental results on the FRAV2D and the FERET database demonstrate the effectiveness of this new approach. version:1
arxiv-1312-1683 | Face Recognition using Hough Peaks extracted from the significant blocks of the Gradient Image | http://arxiv.org/abs/1312.1683 | id:1312.1683 author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV  published:2013-12-05 summary:This paper proposes a new technique for automatic face recognition using integrated peaks of the Hough transformed significant blocks of the binary gradient image. In this approach firstly the gradient of an image is calculated and a threshold is set to obtain a binary gradient image, which is less sensitive to noise and illumination changes. Secondly, significant blocks are extracted from the absolute gradient image, to extract pertinent information with the idea of dimension reduction. Finally the best fitted Hough peaks are extracted from the Hough transformed significant blocks for efficient face recognition. Then these Hough peaks are concatenated together, which are used as feature in classification process. The efficiency of the proposed method is demonstrated by the experiment on 1100 images from the FRAV2D face database, 2200 images from the FERET database, where the images vary in pose, expression, illumination and scale and 400 images from the ORL face database, where the images slightly vary in pose. Our method has shown 93.3%, 88.5% and 99% recognition accuracy for the FRAV2D, FERET and the ORL database respectively. version:1
arxiv-1312-1512 | An adaptive block based integrated LDP,GLCM,and Morphological features for Face Recognition | http://arxiv.org/abs/1312.1512 | id:1312.1512 author:Arindam Kar, Debotosh Bhattacharjee, Dipak Kumar Basu, Mita Nasipuri, Mahantapas Kundu category:cs.CV  published:2013-12-05 summary:This paper proposes a technique for automatic face recognition using integrated multiple feature sets extracted from the significant blocks of a gradient image. We discuss about the use of novel morphological, local directional pattern (LDP) and gray-level co-occurrence matrix GLCM based feature extraction technique to recognize human faces. Firstly, the new morphological features i.e., features based on number of runs of pixels in four directions (N,NE,E,NW) are extracted, together with the GLCM based statistical features and LDP features that are less sensitive to the noise and non-monotonic illumination changes, are extracted from the significant blocks of the gradient image. Then these features are concatenated together. We integrate the above mentioned methods to take full advantage of the three approaches. Extraction of the significant blocks from the absolute gradient image and hence from the original image to extract pertinent information with the idea of dimension reduction forms the basis of the work. The efficiency of our method is demonstrated by the experiment on 1100 images from the FRAV2D face database, 2200 images from the FERET database, where the images vary in pose, expression, illumination and scale and 400 images from the ORL face database, where the images slightly vary in pose. Our method has shown 90.3%, 93% and 98.75% recognition accuracy for the FRAV2D, FERET and the ORL database respectively. version:1
arxiv-1312-1494 | Approximating persistent homology for a cloud of $n$ points in a subquadratic time | http://arxiv.org/abs/1312.1494 | id:1312.1494 author:Vitaliy Kurlin category:cs.CG cs.CV math.AT  published:2013-12-05 summary:The Vietoris-Rips filtration for an $n$-point metric space is a sequence of large simplicial complexes adding a topological structure to the otherwise disconnected space. The persistent homology is a key tool in topological data analysis and studies topological features of data that persist over many scales. The fastest algorithm for computing persistent homology of a filtration has time $O(M(u)+u^2\log^2 u)$, where $u$ is the number of updates (additions or deletions of simplices), $M(u)=O(u^{2.376})$ is the time for multiplication of $u\times u$ matrices. For a space of $n$ points given by their pairwise distances, we approximate the Vietoris-Rips filtration by a zigzag filtration consisting of $u=o(n)$ updates, which is sublinear in $n$. The constant depends on a given error of approximation and on the doubling dimension of the metric space. Then the persistent homology of this sublinear-size filtration can be computed in time $o(n^2)$, which is subquadratic in $n$. version:1
