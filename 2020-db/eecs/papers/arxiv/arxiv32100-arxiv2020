arxiv-1709-01560 | Ergodic Exploration using Binary Sensing for Non-Parametric Shape Estimation | http://arxiv.org/abs/1709.01560 | id:1709.01560 author:Ian Abraham, Ahalya Prabhakar, Mitra J. Z. Hartmann, Todd D. Murphey category:cs.RO  published:2017-09-05 summary:Current methods to estimate object shape---using either vision or touch---generally depend on high-resolution sensing. Here, we exploit ergodic exploration to demonstrate successful shape estimation when using a low-resolution binary contact sensor. The measurement model is posed as a collision-based tactile measurement, and classification methods are used to discriminate between shape boundary regions in the search space. Posterior likelihood estimates of the measurement model help the system actively seek out regions where the binary sensor is most likely to return informative measurements. Results show successful shape estimation of various objects as well as the ability to identify multiple objects in an environment. Interestingly, it is shown that ergodic exploration utilizes non-contact motion to gather significant information about shape. The algorithm is extended in three dimensions in simulation and we present two dimensional experimental results using the Rethink Baxter robot. version:1
arxiv-1709-01547 | Knowledge Transfer Between Artificial Intelligence Systems | http://arxiv.org/abs/1709.01547 | id:1709.01547 author:Ivan Y. Tyukin, Alexander N. Gorban, Konstantin Sofeikov, Ilya Romanenko category:cs.AI 68T05  68T30  published:2017-09-05 summary:We consider the fundamental question: how a legacy "student" Artificial Intelligent (AI) system could learn from a legacy "teacher" AI system or a human expert without complete re-training and, most importantly, without requiring significant computational resources. Here "learning" is understood as an ability of one system to mimic responses of the other and vice-versa. We call such learning an Artificial Intelligence knowledge transfer. We show that if internal variables of the "student" Artificial Intelligent system have the structure of an $n$-dimensional topological vector space and $n$ is sufficiently high then, with probability close to one, the required knowledge transfer can be implemented by simple cascades of linear functionals. In particular, for $n$ sufficiently large, with probability close to one, the "student" system can successfully and non-iteratively learn $k\ll n$ new examples from the "teacher" (or correct the same number of mistakes) at the cost of two additional inner products. The concept is illustrated with an example of knowledge transfer from a pre-trained convolutional neural network to a simple linear classifier with HOG features. version:1
arxiv-1709-01509 | Linking Generative Adversarial Learning and Binary Classification | http://arxiv.org/abs/1709.01509 | id:1709.01509 author:Akshay Balsubramani category:cs.LG cs.AI stat.ML  published:2017-09-05 summary:In this note, we point out a basic link between generative adversarial (GA) training and binary classification -- any powerful discriminator essentially computes an (f-)divergence between real and generated samples. The result, repeatedly re-derived in decision theory, has implications for GA Networks (GANs), providing an alternative perspective on training f-GANs by designing the discriminator loss function. version:1
arxiv-1709-01500 | SeDAR - Semantic Detection and Ranging: Humans can localise without LiDAR, can robots? | http://arxiv.org/abs/1709.01500 | id:1709.01500 author:Oscar Mendez, Simon Hadfield, Nicolas Pugeault, Richard Bowden category:cs.RO cs.CV  published:2017-09-05 summary:How does a person work out their location using a floorplan? It is probably safe to say that we do not explicitly measure depths to every visible surface and try to match them against different pose estimates in the floorplan. And yet, this is exactly how most robotic scan-matching algorithms operate. Similarly, we do not extrude the 2D geometry present in the floorplan into 3D and try to align it to the real-world. And yet, this is how most vision-based approaches localise. Humans do the exact opposite. Instead of depth, we use high level semantic cues. Instead of extruding the floorplan up into the third dimension, we collapse the 3D world into a 2D representation. Evidence of this is that many of the floorplans we use in everyday life are not accurate, opting instead for high levels of discriminative landmarks. In this work, we use this insight to present a global localisation approach that relies solely on the semantic labels present in the floorplan and extracted from RGB images. While our approach is able to use range measurements if available, we demonstrate that they are unnecessary as we can achieve results comparable to state-of-the-art without them. version:1
arxiv-1709-01490 | Active Exploration for Learning Symbolic Representations | http://arxiv.org/abs/1709.01490 | id:1709.01490 author:Garrett Andersen, George Konidaris category:cs.AI  published:2017-09-05 summary:We introduce an online active exploration algorithm for data-efficiently learning an abstract symbolic model of an environment. Our algorithm is divided into two parts: the first part quickly generates an intermediate Bayesian symbolic model from the data that the agent has collected so far, which the agent can then use along with the second part to guide its future exploration towards regions of the state space that the model is uncertain about. We show that our algorithm outperforms random and greedy exploration policies on two different computer game domains. The first domain is an Asteroids-inspired game with complex dynamics, but basic logical structure. The second is the Treasure Game, with simpler dynamics, but more complex logical structure. version:1
arxiv-1709-01476 | Fine-tuning deep CNN models on specific MS COCO categories | http://arxiv.org/abs/1709.01476 | id:1709.01476 author:Daniel Sonntag, Michael Barz, Jan Zacharias, Sven Stauden, Vahid Rahmani, Áron Fóthi, András Lőrincz category:cs.CV cs.AI cs.LG  published:2017-09-05 summary:Fine-tuning of a deep convolutional neural network (CNN) is often desired. This paper provides an overview of our publicly available py-faster-rcnn-ft software library that can be used to fine-tune the VGG_CNN_M_1024 model on custom subsets of the Microsoft Common Objects in Context (MS COCO) dataset. For example, we improved the procedure so that the user does not have to look for suitable image files in the dataset by hand which can then be used in the demo program. Our implementation randomly selects images that contain at least one object of the categories on which the model is fine-tuned. version:1
arxiv-1709-01438 | KUKA Sunrise Toolbox: Interfacing Collaborative Robots with MATLAB | http://arxiv.org/abs/1709.01438 | id:1709.01438 author:Mohammad Safeea, Pedro Neto category:cs.RO  published:2017-09-05 summary:Collaborative robots are increasingly present in our lives. The KUKA LBR iiwa equipped with the KUKA Sunrise.OS controller is a good example of a collaborative/sensitive robot. This paper presents a MATLAB Toolbox, the KUKA Sunrise Toolbox (KST), to interface KUKA Sunrise.OS using MATLAB. The KST contains functionalities for networking, real-time control, point-to-point motion, setters and getters of parameters and physical interaction. KST includes more than 50 functions and runs on a remote computer connected with the KUKA Sunrise controller via transmission control Protocol/Internet Protocol (TCP/IP). The KST potentialities are demonstrated in three use cases. version:1
arxiv-1709-01434 | A Generic Approach for Escaping Saddle points | http://arxiv.org/abs/1709.01434 | id:1709.01434 author:Sashank J Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov, Alexander J Smola category:cs.LG cs.AI  published:2017-09-05 summary:A central challenge to using first-order methods for optimizing nonconvex problems is the presence of saddle points. First-order methods often get stuck at saddle points, greatly deteriorating their performance. Typically, to escape from saddles one has to use second-order methods. However, most works on second-order methods rely extensively on expensive Hessian-based computations, making them impractical in large-scale settings. To tackle this challenge, we introduce a generic framework that minimizes Hessian based computations while at the same time provably converging to second-order critical points. Our framework carefully alternates between a first-order and a second-order subroutine, using the latter only close to saddle points, and yields convergence results competitive to the state-of-the-art. Empirical results suggest that our strategy also enjoys a good practical performance. version:1
arxiv-1709-01384 | SLO-aware Colocation of Data Center Tasks Based on Instantaneous Processor Requirements | http://arxiv.org/abs/1709.01384 | id:1709.01384 author:Pawel Janus, Krzysztof Rzadca category:cs.DC  published:2017-09-05 summary:In a cloud data center, a single physical machine simultaneously executes dozens of highly heterogeneous tasks. Such colocation results in more efficient utilization of machines, but, when tasks' requirements exceed available resources, some of the tasks might be throttled down or preempted. We analyze version 2.1 of the Google cluster trace that shows short-term (1 second) task CPU usage. Contrary to the assumptions taken by many theoretical studies, we demonstrate that the empirical distributions do not follow any single distribution. However, high percentiles of the total processor usage (summed over at least 10 tasks) can be reasonably estimated by the Gaussian distribution. We use this result for a probabilistic fit test, called the Gaussian Percentile Approximation (GPA), for standard bin-packing algorithms. To check whether a new task will fit into a machine, GPA checks whether the resulting distribution's percentile corresponding to the requested service level objective, SLO is still below the machine's capacity. In our simulation experiments, GPA resulted in colocations exceeding the machines' capacity with a frequency similar to the requested SLO. version:1
arxiv-1709-02256 | Rationally Biased Learning | http://arxiv.org/abs/1709.02256 | id:1709.02256 author:Michel De Lara category:cs.AI math.OC  published:2017-09-05 summary:Are human perception and decision biases grounded in a form of rationality? You return to your camp after hunting or gathering. You see the grass moving. You do not know the probability that a snake is in the grass. Should you cross the grass - at the risk of being bitten by a snake - or make a long, hence costly, detour? Based on this storyline, we consider a rational decision maker maximizing expected discounted utility with learning. We show that his optimal behavior displays three biases: status quo, salience, overestimation of small probabilities. Biases can be the product of rational behavior. version:1
arxiv-1709-01366 | Speeding-up the decision making of a learning agent using an ion trap quantum processor | http://arxiv.org/abs/1709.01366 | id:1709.01366 author:Theeraphot Sriarunothai, Sabine Wölk, Gouri Shankar Giri, Nicolai Fries, Vedran Dunjko, Hans J. Briegel, Christof Wunderlich category:quant-ph cs.AI  published:2017-09-05 summary:We report a proof-of-principle experimental demonstration of a quantum speed-up for learning agents utilizing a small-scale quantum information processor based on radiofrequency-driven trapped ions. The decision-making process of a quantum learning agent within the projective simulation paradigm for machine learning is modeled in a system of two qubits. The latter are realized in the hyperfine levels of two frequency-addressed ions exposed to a static magnetic field gradient. The deliberation algorithm is implemented using single-qubit rotations and two-qubit conditional quantum dynamics. We show that the deliberation time of this quantum learning agent is quadratically improved with respect to comparable classical learning agents. The performance of this quantum-enhanced learning agent highlights the potential of scalable ion trap quantum processors taking advantage of machine learning. version:1
arxiv-1709-01363 | Resource Elasticity for Distributed Data Stream Processing: A Survey and Future Directions | http://arxiv.org/abs/1709.01363 | id:1709.01363 author:Marcos Dias de Assuncao, Alexandre da Silva Veith, Rajkumar Buyya category:cs.DC C.2.4; H.3.4  published:2017-09-05 summary:Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructures, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. This work examines some of these challenges and discusses solutions proposed in the literature to address them. version:1
arxiv-1201-0856 | Complexity Classification in Infinite-Domain Constraint Satisfaction | http://arxiv.org/abs/1201.0856 | id:1201.0856 author:Manuel Bodirsky category:cs.CC cs.AI cs.DM cs.LO math.LO  published:2012-01-04 summary:A constraint satisfaction problem (CSP) is a computational problem where the input consists of a finite set of variables and a finite set of constraints, and where the task is to decide whether there exists a satisfying assignment of values to the variables. Depending on the type of constraints that we allow in the input, a CSP might be tractable, or computationally hard. In recent years, general criteria have been discovered that imply that a CSP is polynomial-time tractable, or that it is NP-hard. Finite-domain CSPs have become a major common research focus of graph theory, artificial intelligence, and finite model theory. It turned out that the key questions for complexity classification of CSPs are closely linked to central questions in universal algebra. This thesis studies CSPs where the variables can take values from an infinite domain. This generalization enhances dramatically the range of computational problems that can be modeled as a CSP. Many problems from areas that have so far seen no interaction with constraint satisfaction theory can be formulated using infinite domains, e.g. problems from temporal and spatial reasoning, phylogenetic reconstruction, and operations research. It turns out that the universal-algebraic approach can also be applied to study large classes of infinite-domain CSPs, yielding elegant complexity classification results. A new tool in this thesis that becomes relevant particularly for infinite domains is Ramsey theory. We demonstrate the feasibility of our approach with two complete complexity classification results: one on CSPs in temporal reasoning, the other on a generalization of Schaefer's theorem for propositional logic to logic over graphs. We also study the limits of complexity classification, and present classes of computational problems provably do not exhibit a complexity dichotomy into hard and easy problems. version:9
arxiv-1709-01308 | Knowledge Sharing for Reinforcement Learning: Writing a BOOK | http://arxiv.org/abs/1709.01308 | id:1709.01308 author:Simyung Chang, YoungJoon Yoo, Jaeseok Choi, Nojun Kwak category:cs.AI 68T05  published:2017-09-05 summary:This paper proposes a novel deep reinforcement learning (RL) method integrating the neural-network-based RL and the classical RL based on dynamic programming. In comparison to the conventional deep RL methods, our method enhances the convergence speed and the performance by delving into the following two characteristic features in the training of conventional RL: (1) Having many credible experiences is important in training RL algorithms, (2) Input states can be semantically clustered into a relatively small number of core clusters, and the states belonging to the same cluster tend to share similar Q-values given an action. By following the two observations, we propose a dictionary-type memory that accumulates the Q-value for each cluster of states as well as the corresponding action, in terms of priority. Then, we iteratively update each Q-value in the memory from the Q-value acquired from the network trained by the experiences stored in the memory. We demonstrate the effectiveness of our method through training RL algorithms on widely used game environments from OpenAI. version:1
arxiv-1604-01507 | Robotic manipulation of a rotating chain | http://arxiv.org/abs/1604.01507 | id:1604.01507 author:Hung Pham, Quang-Cuong Pham category:cs.RO I.2.9  published:2016-04-06 summary:This paper considers the problem of manipulating a uniformly rotating chain: the chain is rotated at a constant angular speed around a fixed axis using a robotic manipulator. Manipulation is quasi-static in the sense that transitions are slow enough for the chain to be always in "rotational" equilibrium. The curve traced by the chain in a rotating plane -- its shape function -- can be determined by a simple force analysis, yet it possesses complex multi-solutions behavior typical of non-linear systems. We prove that the configuration space of the uniformly rotating chain is homeomorphic to a two-dimensional surface embedded in $\mathbb{R}^3$. Using that representation, we devise a manipulation strategy for transiting between different rotation modes in a stable and controlled manner. We demonstrate the strategy on a physical robotic arm manipulating a rotating chain. Finally, we discuss how the ideas developed here might find fruitful applications in the study of other flexible objects, such as elastic rods or concentric tubes. version:2
arxiv-1709-01054 | Distributed Triangle Counting in the Graphulo Matrix Math Library | http://arxiv.org/abs/1709.01054 | id:1709.01054 author:Dylan Hutchison category:cs.DC cs.MS  published:2017-08-20 summary:Triangle counting is a key algorithm for large graph analysis. The Graphulo library provides a framework for implementing graph algorithms on the Apache Accumulo distributed database. In this work we adapt two algorithms for counting triangles, one that uses the adjacency matrix and another that also uses the incidence matrix, to the Graphulo library for server-side processing inside Accumulo. Cloud-based experiments show a similar performance profile for these different approaches on the family of power law Graph500 graphs, for which data skew increasingly bottlenecks. These results motivate the design of skew-aware hybrid algorithms that we propose for future work. version:2
arxiv-1709-01215 | Towards Understanding Adversarial Learning for Joint Distribution Matching | http://arxiv.org/abs/1709.01215 | id:1709.01215 author:Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo Henao, Lawrence Carin category:stat.ML cs.AI cs.CV cs.LG cs.NE  published:2017-09-05 summary:We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications. version:1
arxiv-1709-01190 | FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search | http://arxiv.org/abs/1709.01190 | id:1709.01190 author:Yiqiu Wang, Anshumali Shrivastava, Junghee Ryu category:cs.DS cs.DB cs.DC cs.IR cs.PF  published:2017-09-04 summary:We present FLASH ({\bf F}ast {\bf L}SH {\bf A}lgorithm for {\bf S}imilarity search accelerated with {\bf H}PC (High-Performance Computing)), a similarity search system for ultra-high dimensional datasets on a single machine, which does not require similarity computation. Our system is an auspicious illustration of the power of randomized algorithms carefully tailored for high-performance computing platforms. We leverage LSH style randomized indexing procedure and combine it with several principled techniques, such as reservoir sampling, recent advances in one-pass minwise hashing, and count based estimations. The combination, while retaining sound theoretical guarantees, reduces the computational as well as parallelization overhead of our proposal. We provide CPU and hybrid CPU-GPU implementations of FLASH for replicability of our results https://github.com/RUSH-LAB/Flash. We evaluate FLASH on several real high dimensional datasets coming from different domains including text, malicious URL, click-through prediction, social networks, etc. Our experiments shed new light on the difficulties associated with datasets having several millions of dimensions. Current state-of-the-art implementations either fail on the presented scale or are orders of magnitude slower than our system. FLASH is capable of computing an approximate k-NN graph, from scratch, over full webspam dataset (1.3 billion nonzeros) in less than 10 seconds. Computing full k-NN graph in less than 10 seconds on webspam dataset, using brute-force ($n^2D$), will require at least 20 TFLOPS. We hope that FLASH gets adopted in practice. version:1
arxiv-1709-01122 | Exact Inference for Relational Graphical Models with Interpreted Functions: Lifted Probabilistic Inference Modulo Theories | http://arxiv.org/abs/1709.01122 | id:1709.01122 author:Rodrigo de Salvo Braz, Ciaran O'Reilly category:cs.AI cs.SC  published:2017-09-04 summary:Probabilistic Inference Modulo Theories (PIMT) is a recent framework that expands exact inference on graphical models to use richer languages that include arithmetic, equalities, and inequalities on both integers and real numbers. In this paper, we expand PIMT to a lifted version that also processes random functions and relations. This enhancement is achieved by adapting Inversion, a method from Lifted First-Order Probabilistic Inference literature, to also be modulo theories. This results in the first algorithm for exact probabilistic inference that efficiently and simultaneously exploits random relations and functions, arithmetic, equalities and inequalities. version:1
arxiv-1709-01110 | Distributed circular formation flight of fixed-wing aircraft with Paparazzi autopilot | http://arxiv.org/abs/1709.01110 | id:1709.01110 author:Hector Garcia de Marina, Gautier Hattenberger category:cs.RO  published:2017-09-04 summary:In this paper we introduce the usage of guidance vector fields for the coordination and formation flight of fixed-wing aircraft. In particular, we describe in detail the technological implementation of the formation flight control for a fully distributed execution of the algorithm by employing the open-source project Paparazzi. In this context, distributed means that each aircraft executes the algorithm on board, each aircraft only needs information about its neighbors, and the implementation is straightforwardly scalable to an arbitrary number of vehicles, i.e., the needed resources such as memory or computational power not necessarily scale with the number of total aircraft. The coordination is based on commanding the aircraft to track circumferences with different radii but sharing the same center. Consequently, the vehicles will travel different distances but with the same speeds in order to control their relative angles in the circumference, i.e., their orbital velocities. We show the effectiveness of the proposed design with actual formation flights during the drone parade in IMAV2017. version:1
arxiv-1708-00495 | "I can assure you [$\ldots$] that it's going to be all right" -- A definition, case for, and survey of algorithmic assurances in human-autonomy trust relationships | http://arxiv.org/abs/1708.00495 | id:1708.00495 author:Brett W Israelsen category:cs.CY cs.AI cs.HC stat.ML  published:2017-08-01 summary:As technology become more advanced, those who design, use and are otherwise affected by it want to know that it will perform correctly, and understand why it does what it does, and how to use it appropriately. In essence they want to be able to trust the systems that are being designed. In this survey we present assurances that are the method by which users can understand how to trust this technology. Trust between humans and autonomy is reviewed, and the implications for the design of assurances are highlighted. A survey of research that has been performed with respect to assurances is presented, and several key ideas are extracted in order to refine the definition of assurances. Several directions for future research are identified and discussed. version:2
arxiv-1709-01033 | Starvation Freedom in Multi-Version Transactional Memory Systems | http://arxiv.org/abs/1709.01033 | id:1709.01033 author:Ved Prakash Chaudhary, Sandeep Kulkarni, Sweta Kumari, Sathya Peri category:cs.DC  published:2017-09-04 summary:Software Transactional Memory systems (STMs) have garnered significant interest as an elegant alternative for addressing synchronization and concurrency issues with multi-threaded programming in multi-core systems. In order for STMs to be efficient, they must guarantee some progress properties. This work explores the notion of starvation-freedom in Software Transactional Memory Systems (STMs). An STM system is said to be starvation-free if every thread invoking a transaction gets opportunity to take a step (due to the presence of a fair scheduler) then the transaction will eventually commit. A few starvation-free algorithms have been proposed in the literature in the context of single-version STM Systems. These algorithm work on the basis of priority. But the drawback with this approach is that if a set of high-priority transactions become slow then they can cause several other transactions to abort. Multi-version STMs maintain multiple-versions for each transactional object or t-object. By storing multiple versions, these systems can achieve greater concurrency. In this paper, we propose a multi-version starvation free STM, KSFTM, which as the name suggests achieves starvation freedom while storing K versions of each t-object. Here K is an input parameter fixed by the application programmer depending on the requirement. Our algorithm is dynamic which can support different values of K ranging from 1 to infinity . If K is infinity then there is no limit on the number of versions. But a separate garbage-collection mechanism is required to collect unwanted versions. On the other hand, when K is 1, it becomes same as a single-version STM system. We prove the correctness and starvation-freedom property of the proposed KSFTM algorithm. To the best of our knowledge, this is the first multi-version STM system that is correct and satisfies starvation-freedom as well. version:1
arxiv-1709-00993 | Grasp selection analysis for two-step manipulation tasks | http://arxiv.org/abs/1709.00993 | id:1709.00993 author:Ana C. Huamán Quispe category:cs.RO  published:2017-09-04 summary:Manipulation tasks are sequential in nature. Grasp selection approaches that take into account the con- straints at each task step are critical, since they allow to both (1) Identify grasps that likely require simple arm motions through the whole task and (2) To discard grasps that, although feasible to achieve at earlier steps, might not be executable at later stages due to goal task constraints. In this paper, we study how to use our previously proposed manipulation metric for tasks in which 2 steps are required (pick-and-place and pouring tasks). Even for such simple tasks, it was not clear how to use the results of applying our metric (or any metric for that matter) to rank all the candidate grasps: Should only the start state be considered, or only the goal, or a combination of both? In order to find an answer, we evaluated the (best) grasps selected by our metric under each of these 3 considerations. Our main conclusion was that for tasks in which the goal state is more constrained (pick-and-place), using a combination of the metric measured at the start and goal states renders better performance when compared with choosing any other candidate grasp, whereas in tasks in which the goal constraints are less rigidly defined, the metric measured at the start state should be mainly considered. We present quantitative results in simulation and validate our approach's practicality with experimental results in our physical robot manipulator, Crichton. version:1
arxiv-1709-00954 | Virtual Borders: Accurate Definition of a Mobile Robot's Workspace Using a RGB-D Google Tango Tablet | http://arxiv.org/abs/1709.00954 | id:1709.00954 author:Dennis Sprute, Klaus Tönnies, Matthias König category:cs.RO  published:2017-09-04 summary:We address the problem of interactively controlling the workspace of a mobile robot to ensure a human-aware navigation. This is especially of relevance for non-expert users living in human-robot shared spaces, e.g. home environments, since they want to keep the control of their mobile robots, such as vacuum cleaning or companion robots. Therefore, we introduce virtual borders that are respected by a robot while performing its tasks. For this purpose, we employ a RGB-D Google Tango tablet as human-robot interface to flexibly specify virtual borders in the environment. We evaluated our system concerning correctness, accuracy and teaching effort, and compared the results with other baseline methods. Our method features an equally-high accuracy while reducing the teaching effort by a factor of 3.1 compared to the baseline. version:1
arxiv-1709-00953 | Distributed Computation of Linear Inverse Problems with Application to Computed Tomography | http://arxiv.org/abs/1709.00953 | id:1709.00953 author:Yushan Gao, Thomas Blumensath category:cs.DC  published:2017-09-04 summary:The inversion of linear systems is a fundamental step in many inverse problems. Computational challenges exist when trying to invert large linear systems, where limited computing resources mean that only part of the system can be kept in computer memory at any one time. We are here motivated by tomographic inversion problems that often lead to linear inverse problems. In state of the art x-ray systems, even a standard scan can produce 4 million individual measurements and the reconstruction of x-ray attenuation profiles typically requires the estimation of a million attenuation coefficients. To deal with the large data sets encountered in real applications and to utilise modern graphics processing unit (GPU) based computing architectures, combinations of iterative reconstruction algorithms and parallel computing schemes are increasingly applied. Although both row and column action methods have been proposed to utilise parallel computing architectures, individual computations in current methods need to know either the entire set of observations or the entire set of estimated x-ray absorptions, which can be prohibitive in many realistic big data applications. We present a fully parallelizable computed tomography (CT) image reconstruction algorithm that works with arbitrary partial subsets of the data and the reconstructed volume. We further develop a non-homogeneously randomised selection criteria which guarantees that sub-matrices of the system matrix are selected more frequently if they are dense, thus maximising information flow through the algorithm. A grouped version of the algorithm is also proposed to further improve convergence speed and performance. Algorithm performance is verified experimentally. version:1
arxiv-1709-00931 | A Computer Composes A Fabled Problem: Four Knights vs. Queen | http://arxiv.org/abs/1709.00931 | id:1709.00931 author:Azlan Iqbal category:cs.AI  published:2017-09-04 summary:We explain how the prototype automatic chess problem composer, Chesthetica, successfully composed a rare and interesting chess problem using the new Digital Synaptic Neural Substrate (DSNS) computational creativity approach. This problem represents a greater challenge from a creative standpoint because the checkmate is not always clear and the method of winning even less so. Creating a decisive chess problem of this type without the aid of an omniscient 7-piece endgame tablebase (and one that also abides by several chess composition conventions) would therefore be a challenge for most human players and composers working on their own. The fact that a small computer with relatively low processing power and memory was sufficient to compose such a problem using the DSNS approach in just 10 days is therefore noteworthy. In this report we document the event and result in some detail. It lends additional credence to the DSNS as a viable new approach in the field of computational creativity. In particular, in areas where human-like creativity is required for targeted or specific problems with no clear path to the solution. version:1
arxiv-1709-00928 | Automation of Android Applications Testing Using Machine Learning Activities Classification | http://arxiv.org/abs/1709.00928 | id:1709.00928 author:Ariel Rosenfeld, Odaya Kardashov, Orel Zang category:cs.SE cs.AI  published:2017-09-04 summary:Mobile applications are being used every day by more than half of the world's population to perform a great variety of tasks. With the increasingly widespread usage of these applications, the need arises for efficient techniques to test them. Many frameworks allow automating the process of application testing, however existing frameworks mainly rely on the application developer for providing testing scripts for each developed application, thus preventing reuse of these tests for similar applications. In this paper, we present a novel approach for the automation of testing Android applications by leveraging machine learning techniques and reusing popular test scenarios. We discuss and demonstrate the potential benefits of our approach in an empirical study where we show that our developed testing tool, based on the proposed approach, outperforms standard methods in realistic settings. version:1
arxiv-1709-00901 | Distributed Colour Reduction Revisited | http://arxiv.org/abs/1709.00901 | id:1709.00901 author:Jukka Kohonen, Janne H. Korhonen, Christopher Purcell, Jukka Suomela, Przemysław Uznański category:cs.DC cs.DS 68W15 C.2.4  published:2017-09-04 summary:We give a new, simple distributed algorithm for graph colouring in paths and cycles. Our algorithm is fast and self-contained, it does not need any globally consistent orientation, and it reduces the number of colours from $10^{100}$ to $3$ in three iterations. version:1
arxiv-1709-00893 | Interactive Attention Networks for Aspect-Level Sentiment Classification | http://arxiv.org/abs/1709.00893 | id:1709.00893 author:Dehong Ma, Sujian Li, Xiaodong Zhang, Houfeng Wang category:cs.AI cs.CL  published:2017-09-04 summary:Aspect-level sentiment classification aims at identifying the sentiment polarity of specific target in its context. Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target-specific representations. However, these studies always ignore the separate modeling of targets. In this paper, we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning. Then, we propose the interactive attention networks (IAN) to interactively learn attentions in the contexts and targets, and generate the representations for targets and contexts separately. With this design, the IAN model can well represent a target and its collocative context, which is helpful to sentiment classification. Experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our model. version:1
arxiv-1709-00877 | Crash tolerant gathering on grid by asynchronous oblivious robots | http://arxiv.org/abs/1709.00877 | id:1709.00877 author:Kaustav Bose, Ranendu Adhikary, Sruti Gan Chaudhuri, Buddhadeb Sau category:cs.DC  published:2017-09-04 summary:Consider a system of autonomous mobile robots initially randomly deployed on the nodes of an anonymous finite grid. A gathering algorithm is a sequence of moves to be executed independently by each robot so that all robots meet at a single node after finite time. The robots operate in Look-Compute-Move cycles. In each cycle, a robot takes a snapshot of the current configuration of the grid in terms of occupied nodes (\emph{Look}), then based on the perceived configuration, decides whether to stay put or to move to an adjacent node (\emph{Compute}), and in the later case makes an instantaneous move accordingly (\emph{Move}). The robots have \emph{weak multiplicity detection} capability, which enables them to detect if a node is empty or occupied by a single robot or by multiple robots. The robots are \emph{asynchronous}, \emph{oblivious}, \emph{anonymous}, can not communicate with each other and execute the same distributed algorithm. In a faulty system, however, any robot can \emph{crash}, which means that it becomes completely inactive and does not take part in the process any further. In that case a fault-tolerant gathering algorithm is an algorithm that gathers all the non-faulty robots at a single node. This paper considers a faulty system that can have at most one crash fault. With these assumptions deterministic fault-tolerant gathering algorithms are presented that gather all initial configurations that are gatherable in a non-faulty system, except for one specific configuration called the \emph{2S2 configuration}. version:1
arxiv-1708-02747 | An automatic water detection approach based on Dempster-Shafer theory for multi spectral images | http://arxiv.org/abs/1708.02747 | id:1708.02747 author:Na Li, Arnaud Martin, Rémi Estival category:cs.AI cs.CV  published:2017-08-09 summary:Detection of surface water in natural environment via multi-spectral imagery has been widely utilized in many fields, such land cover identification. However, due to the similarity of the spectra of water bodies, built-up areas, approaches based on high-resolution satellites sometimes confuse these features. A popular direction to detect water is spectral index, often requiring the ground truth to find appropriate thresholds manually. As for traditional machine learning methods, they identify water merely via differences of spectra of various land covers, without taking specific properties of spectral reflection into account. In this paper, we propose an automatic approach to detect water bodies based on Dempster-Shafer theory, combining supervised learning with specific property of water in spectral band in a fully unsupervised context. The benefits of our approach are twofold. On the one hand, it performs well in mapping principle water bodies, including little streams and branches. On the other hand, it labels all objects usually confused with water as `ignorance', including half-dry watery areas, built-up areas and semi-transparent clouds and shadows. `Ignorance' indicates not only limitations of the spectral properties of water and supervised learning itself but insufficiency of information from multi-spectral bands as well, providing valuable information for further land cover classification. version:2
arxiv-1709-00846 | Extrinsic Parameter Calibration for Line Scanning Cameras on Ground Vehicles | http://arxiv.org/abs/1709.00846 | id:1709.00846 author:Alexander Wendel, James Underwood category:cs.RO cs.CV  published:2017-09-04 summary:Line scanning cameras, which capture only a single line of pixels, have been increasingly used in ground based mobile or robotic platforms. In applications where it is advantageous to directly georeference the camera data to world coordinates, an accurate estimate of the camera's 6D pose is required. This paper focuses on the common case where a mobile platform is equipped with a rigidly mounted line scanning camera, whose pose is unknown, and a navigation system providing vehicle body pose estimates. We propose a novel method that estimates the camera's pose relative to the navigation system. The approach has several advantages over previous methods, and involves imaging a calibration pattern with distinctly identifiable points, triangulating these points from camera and navigation system data and reprojecting them in order to compute a likelihood, which is maximised to estimate the 6D camera pose. Additionally, a Markov Chain Monte Carlo (MCMC) algorithm is used to estimate the uncertainty of the offset. Tested on two different platforms, the method was able to estimate the pose to within 0.06 m / 1.05$^{\circ}$ and 0.18 m / 2.39$^{\circ}$. We also propose several approaches to displaying and interpreting the 6D results in a human readable way. version:1
arxiv-1709-01070 | Maintaining Ad-Hoc Communication Network in Area Protection Scenarios with Adversarial Agents | http://arxiv.org/abs/1709.01070 | id:1709.01070 author:Marika Ivanová, Pavel Surynek, Diep Thi Ngoc Nguyen category:cs.MA cs.AI  published:2017-09-04 summary:We address a problem of area protection in graph-based scenarios with multiple mobile agents where connectivity is maintained among agents to ensure they can communicate. The problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams. Agents are placed in vertices of the graph; at most one agent can occupy a vertex; and they can move into adjacent vertices in a conflict free way. Teams have asymmetric goals: the aim of one team - attackers - is to invade into given area while the aim of the opponent team - defenders - is to protect the area from being entered by attackers by occupying selected vertices. The team of defenders need to maintain connectivity of vertices occupied by its own agents in a visibility graph. The visibility graph models possibility of communication between pairs of vertices. We study strategies for allocating vertices to be occupied by the team of defenders to block attacking agents where connectivity is maintained at the same time. To do this we reserve a subset of defending agents that do not try to block the attackers but instead are placed to support connectivity of the team. The performance of strategies is tested in multiple benchmarks. The success of a strategy is heavily dependent on the type of the instance, and so one of the contributions of this work is that we identify suitable strategies for diverse instance types. version:1
arxiv-1708-08611 | Safe Reinforcement Learning via Shielding | http://arxiv.org/abs/1708.08611 | id:1708.08611 author:Mohammed Alshiekh, Roderick Bloem, Ruediger Ehlers, Bettina Könighofer, Scott Niekum, Ufuk Topcu category:cs.LO cs.AI cs.LG  published:2017-08-29 summary:Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield is introduced in the traditional learning process in two alternative ways, depending on the location at which the shield is implemented. In the first one, the shield acts each time the learning agent is about to make a decision and provides a list of safe actions. In the second way, the shield is introduced after the learning agent. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios. version:2
arxiv-1308-4526 | Formalization, Mechanization and Automation of Gödel's Proof of God's Existence | http://arxiv.org/abs/1308.4526 | id:1308.4526 author:Christoph Benzmüller, Bruno Woltzenlogel Paleo category:cs.LO cs.AI math.LO F.4.1; I.2.3; I.2.4  published:2013-08-21 summary:G\"odel's ontological proof has been analysed for the first-time with an unprecedent degree of detail and formality with the help of higher-order theorem provers. The following has been done (and in this order): A detailed natural deduction proof. A formalization of the axioms, definitions and theorems in the TPTP THF syntax. Automatic verification of the consistency of the axioms and definitions with Nitpick. Automatic demonstration of the theorems with the provers LEO-II and Satallax. A step-by-step formalization using the Coq proof assistant. A formalization using the Isabelle proof assistant, where the theorems (and some additional lemmata) have been automated with Sledgehammer and Metis. version:5
arxiv-1709-00744 | An Improved Algorithm for E-Generalization | http://arxiv.org/abs/1709.00744 | id:1709.00744 author:Jochen Burghardt category:cs.LO cs.AI 68Q32  68Q45  68T15 I.2.3; F.4.1  published:2017-09-03 summary:E-generalization computes common generalizations of given ground terms w.r.t. a given equational background theory E. In 2005 [arXiv:1403.8118], we had presented a computation approach based on standard regular tree grammar algorithms, and a Prolog prototype implementation. In this report, we present algorithmic improvements, prove them correct and complete, and give some details of an efficiency-oriented implementation in C that allows us to handle problems larger by several orders of magnitude. version:1
arxiv-1611-09474 | Maximizing Non-Monotone DR-Submodular Functions with Cardinality Constraints | http://arxiv.org/abs/1611.09474 | id:1611.09474 author:Ali Khodabakhsh, Evdokia Nikolova category:cs.DS cs.AI  published:2016-11-29 summary:We consider the problem of maximizing a non-monotone DR-submodular function subject to a cardinality constraint. Diminishing returns (DR) submodularity is a generalization of the diminishing returns property for functions defined over the integer lattice. This generalization can be used to solve many machine learning or combinatorial optimization problems such as optimal budget allocation, revenue maximization, etc. In this work we propose the first polynomial-time approximation algorithms for non-monotone constrained maximization. We implement our algorithms for a revenue maximization problem with a real-world dataset to check their efficiency and performance. version:2
arxiv-1709-00722 | Faster Concurrent Range Queries with Contention Adapting Search Trees Using Immutable Data | http://arxiv.org/abs/1709.00722 | id:1709.00722 author:Kjell Winblad category:cs.DC cs.DS cs.PF D.2.8; E.1; H.2.4  published:2017-09-03 summary:The need for scalable concurrent ordered set data structures with linearizable range query support is increasing due to the rise of multicore computers, data processing platforms and in-memory databases. This paper presents a new concurrent ordered set with linearizable range query support. The new data structure is based on the contention adapting search tree and an immutable data structure. Experimental results show that the new data structure is as much as three times faster compared to related data structures. The data structure scales well due to its ability to adapt the sizes of its immutable parts to the contention level and the sizes of the range queries. version:1
arxiv-1709-00700 | Generating Custom Code for Efficient Query Execution on Heterogeneous Processors | http://arxiv.org/abs/1709.00700 | id:1709.00700 author:Sebastian Breß, Bastian Köcher, Henning Funke, Tilmann Rabl, Volker Markl category:cs.DB cs.DC  published:2017-09-03 summary:Processor manufacturers build increasingly specialized processors to mitigate the effects of the power wall to deliver improved performance. Currently, database engines are manually optimized for each processor: A costly and error prone process. In this paper, we propose concepts to enable the database engine to perform per-processor optimization automatically. Our core idea is to create variants of generated code and to learn a fast variant for each processor. We create variants by modifying parallelization strategies, specializing data structures, and applying different code transformations. Our experimental results show that the performance of variants may diverge up to two orders of magnitude. Therefore, we need to generate custom code for each processor to achieve peak performance. We show that our approach finds a fast custom variant for multi-core CPUs, GPUs, and MICs. version:1
arxiv-1709-00681 | Efficient means of Achieving Composability using Transactional Memory | http://arxiv.org/abs/1709.00681 | id:1709.00681 author:Sathya Peri, Ajay Singh, Archit Somani category:cs.DC  published:2017-09-03 summary:A major focus of software transaction memory systems (STMs) has been to felicitate the multiprocessor programming and provide parallel programmers an abstraction for speedy and efficient development of parallel applications. To this end, different models for incorporating object/higher level semantics into STM have recently been proposed in transactional boosting, transactional data structure library, open nested transactions and abstract nested transactions. We build an alternative object model STM (OSTM) by adopting the transactional tree model of Weikum et al. originally given for databases and extend the current work by proposing comprehensive legality definitions and conflict notion which allows efficient composability of \otm{}. We first time show the proposed OSTM to be co-opacity. We build OSTM using chained hash table data structure. Lazyskip-list is used to implement chaining using lazy approach. We notice that major concurrency hotspot is the chaining data structure within the hash table. Lazyskip-list is time efficient compared to lists in terms of traversal overhead by average case O(log(n)). We optimise lookups as they are validated at the instant they execute and they have not validated again in commit phase. This allows lookup dominated transactions to be more efficient and at the same time co-opacity. version:1
arxiv-1709-00670 | Difficulty-level Modeling of Ontology-based Factual Questions | http://arxiv.org/abs/1709.00670 | id:1709.00670 author:Vinu E. V, P Sreenivasa Kumar category:cs.AI  published:2017-09-03 summary:Semantics based knowledge representations such as ontologies are found to be very useful in automatically generating meaningful factual questions. Determining the difficulty level of these system generated questions is helpful to effectively utilize them in various educational and professional applications. The existing approaches for finding the difficulty level of factual questions are very simple and are limited to a few basic principles. We propose a new methodology for this problem by considering an educational theory called Item Response Theory (IRT). In the IRT, knowledge proficiency of end users (learners) are considered for assigning difficulty levels, because of the assumptions that a given question is perceived differently by learners of various proficiencies. We have done a detailed study on the features (factors) of a question statement which could possibly determine its difficulty level for three learner categories (experts, intermediates and beginners). We formulate ontology based metrics for the same. We then train three logistic regression models to predict the difficulty level corresponding to the three learner categories. version:1
arxiv-1709-00662 | Using Summarization to Discover Argument Facets in Online Ideological Dialog | http://arxiv.org/abs/1709.00662 | id:1709.00662 author:Amita Misra, Pranav Anand, Jean E Fox Tree, Marilyn Walker category:cs.AI cs.CL  published:2017-09-03 summary:More and more of the information available on the web is dialogic, and a significant portion of it takes place in online forum conversations about current social and political topics. We aim to develop tools to summarize what these conversations are about. What are the CENTRAL PROPOSITIONS associated with different stances on an issue, what are the abstract objects under discussion that are central to a speaker's argument? How can we recognize that two CENTRAL PROPOSITIONS realize the same FACET of the argument? We hypothesize that the CENTRAL PROPOSITIONS are exactly those arguments that people find most salient, and use human summarization as a probe for discovering them. We describe our corpus of human summaries of opinionated dialogs, then show how we can identify similar repeated arguments, and group them into FACETS across many discussions of a topic. We define a new task, ARGUMENT FACET SIMILARITY (AFS), and show that we can predict AFS with a .54 correlation score, versus an ngram system baseline of .39 and a semantic textual similarity system baseline of .45. version:1
arxiv-1709-00661 | Topic Independent Identification of Agreement and Disagreement in Social Media Dialogue | http://arxiv.org/abs/1709.00661 | id:1709.00661 author:Amita Misra, Marilyn Walker category:cs.AI cs.CL  published:2017-09-03 summary:Research on the structure of dialogue has been hampered for years because large dialogue corpora have not been available. This has impacted the dialogue research community's ability to develop better theories, as well as good off the shelf tools for dialogue processing. Happily, an increasing amount of information and opinion exchange occur in natural dialogue in online forums, where people share their opinions about a vast range of topics. In particular we are interested in rejection in dialogue, also called disagreement and denial, where the size of available dialogue corpora, for the first time, offers an opportunity to empirically test theoretical accounts of the expression and inference of rejection in dialogue. In this paper, we test whether topic-independent features motivated by theoretical predictions can be used to recognize rejection in online forums in a topic independent way. Our results show that our theoretically motivated features achieve 66% accuracy, an improvement over a unigram baseline of an absolute 6%. version:1
arxiv-1708-01967 | Fake News Detection on Social Media: A Data Mining Perspective | http://arxiv.org/abs/1708.01967 | id:1708.01967 author:Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, Huan Liu category:cs.SI cs.AI H.2.8  published:2017-08-07 summary:Social media for news consumption is a double-edged sword. On the one hand, its low cost, easy access, and rapid dissemination of information lead people to seek out and consume news from social media. On the other hand, it enables the wide spread of "fake news", i.e., low quality news with intentionally false information. The extensive spread of fake news has the potential for extremely negative impacts on individuals and society. Therefore, fake news detection on social media has recently become an emerging research that is attracting tremendous attention. Fake news detection on social media presents unique characteristics and challenges that make existing detection algorithms from traditional news media ineffective or not applicable. First, fake news is intentionally written to mislead readers to believe false information, which makes it difficult and nontrivial to detect based on news content; therefore, we need to include auxiliary information, such as user social engagements on social media, to help make a determination. Second, exploiting this auxiliary information is challenging in and of itself as users' social engagements with fake news produce data that is big, incomplete, unstructured, and noisy. Because the issue of fake news detection on social media is both challenging and relevant, we conducted this survey to further facilitate research on the problem. In this survey, we present a comprehensive review of detecting fake news on social media, including fake news characterizations on psychology and social theories, existing algorithms from a data mining perspective, evaluation metrics and representative datasets. We also discuss related research areas, open problems, and future research directions for fake news detection on social media. version:3
arxiv-1709-00653 | From Query-By-Keyword to Query-By-Example: LinkedIn Talent Search Approach | http://arxiv.org/abs/1709.00653 | id:1709.00653 author:Viet Ha-Thuc, Yan Yan, Xianren Wu, Vijay Dialani, Abhishek Gupta, Shakti Sinha category:cs.IR cs.AI cs.LG  published:2017-09-03 summary:One key challenge in talent search is to translate complex criteria of a hiring position into a search query, while it is relatively easy for a searcher to list examples of suitable candidates for a given position. To improve search efficiency, we propose the next generation of talent search at LinkedIn, also referred to as Search By Ideal Candidates. In this system, a searcher provides one or several ideal candidates as the input to hire for a given position. The system then generates a query based on the ideal candidates and uses it to retrieve and rank results. Shifting from the traditional Query-By-Keyword to this new Query-By-Example system poses a number of challenges: How to generate a query that best describes the candidates? When moving to a completely different paradigm, how does one leverage previous product logs to learn ranking models and/or evaluate the new system with no existing usage logs? Finally, given the different nature between the two search paradigms, the ranking features typically used for Query-By-Keyword systems might not be optimal for Query-By-Example. This paper describes our approach to solving these challenges. We present experimental results confirming the effectiveness of the proposed solution, particularly on query building and search ranking tasks. As of writing this paper, the new system has been available to all LinkedIn members. version:1
arxiv-1709-00627 | The Convex Feasible Set Algorithm for Real Time Optimization in Motion Planning | http://arxiv.org/abs/1709.00627 | id:1709.00627 author:Changliu Liu, Chung-Yen Lin, Masayoshi Tomizuka category:math.OC cs.RO  published:2017-09-02 summary:With the development of robotics, there are growing needs for real time motion planning. However, due to obstacles in the environment, the planning problem is highly non-convex, which makes it difficult to achieve real time computation using existing non-convex optimization algorithms. This paper introduces the convex feasible set algorithm (CFS) which is a fast algorithm for non-convex optimization problems that have convex costs and non-convex constraints. The idea is to find a convex feasible set for the original problem and iteratively solve a sequence of subproblems using the convex constraints. The feasibility and the convergence of the proposed algorithm are proved in the paper. The application of this method on motion planning for mobile robots is discussed. The simulations demonstrate the effectiveness of the proposed algorithm. version:1
arxiv-1612-03353 | FOCA: A Methodology for Ontology Evaluation | http://arxiv.org/abs/1612.03353 | id:1612.03353 author:Judson Bandeira, Ig Ibert Bittencourt, Patricia Espinheira, Seiji Isotani category:cs.AI  published:2016-12-10 summary:Modeling an ontology is a hard and time-consuming task. Although methodologies are useful for ontologists to create good ontologies, they do not help with the task of evaluating the quality of the ontology to be reused. For these reasons, it is imperative to evaluate the quality of the ontology after constructing it or before reusing it. Few studies usually present only a set of criteria and questions, but no guidelines to evaluate the ontology. The effort to evaluate an ontology is very high as there is a huge dependence on the evaluator's expertise to understand the criteria and questions in depth. Moreover, the evaluation is still very subjective. This study presents a novel methodology for ontology evaluation, taking into account three fundamental principles: i) it is based on the Goal, Question, Metric approach for empirical evaluation; ii) the goals of the methodologies are based on the roles of knowledge representations combined with specific evaluation criteria; iii) each ontology is evaluated according to the type of ontology. The methodology was empirically evaluated using different ontologists and ontologies of the same domain. The main contributions of this study are: i) defining a step-by-step approach to evaluate the quality of an ontology; ii) proposing an evaluation based on the roles of knowledge representations; iii) the explicit difference of the evaluation according to the type of the ontology iii) a questionnaire to evaluate the ontologies; iv) a statistical model that automatically calculates the quality of the ontologies. version:2
arxiv-1710-02553 | Artificial life, complex systems and cloud computing: a short review | http://arxiv.org/abs/1710.02553 | id:1710.02553 author:Juan-Julián Merelo-Guervós category:cs.DC cs.NE  published:2017-09-02 summary:Cloud computing is the prevailing mode of designing, creating and deploying complex applications nowadays. Its underlying assumptions include distributed computing, but also new concepts that need to be incorporated in the different fields. In this short paper we will make a review of how the world of cloud computing has intersected the complex systems and artificial life field, and how it has been used as inspiration for new models or implementation of new and powerful algorithms version:1
arxiv-1708-09419 | Proposal for a fully decentralized blockchain and proof-of-work algorithm for solving NP-complete problems | http://arxiv.org/abs/1708.09419 | id:1708.09419 author:Carlos G. Oliver, Alessandro Ricottone, Pericles Philippopoulos category:cs.DC cs.CR  published:2017-08-30 summary:We propose a proof-of-work algorithm that rewards blockchain miners for using computational resources to solve NP-complete puzzles. The resulting blockchain will publicly store and improve solutions to problems with real world applications while maintaining a secure and fully functional transaction ledger. version:2
arxiv-1708-07985 | GiViP: A Visual Profiler for Distributed Graph Processing Systems | http://arxiv.org/abs/1708.07985 | id:1708.07985 author:Alessio Arleo, Walter Didimo, Giuseppe Liotta, Fabrizio Montecchiani category:cs.DC  published:2017-08-26 summary:Analyzing large-scale graphs provides valuable insights in different application scenarios. While many graph processing systems working on top of distributed infrastructures have been proposed to deal with big graphs, the tasks of profiling and debugging their massive computations remain time consuming and error-prone. This paper presents GiViP, a visual profiler for distributed graph processing systems based on a Pregel-like computation model. GiViP captures the huge amount of messages exchanged throughout a computation and provides an interactive user interface for the visual analysis of the collected data. We show how to take advantage of GiViP to detect anomalies related to the computation and to the infrastructure, such as slow computing units and anomalous message patterns. version:2
arxiv-1709-00587 | 3D Registration of Aerial and Ground Robots for Disaster Response: An Evaluation of Features, Descriptors, and Transformation Estimation | http://arxiv.org/abs/1709.00587 | id:1709.00587 author:Abel Gawel, Renaud Dubé, Hartmut Surmann, Juan Nieto, Roland Siegwart, Cesar Cadena category:cs.RO  published:2017-09-02 summary:Global registration of heterogeneous ground and aerial mapping data is a challenging task. This is especially difficult in disaster response scenarios when we have no prior information on the environment and cannot assume the regular order of man-made environments or meaningful semantic cues. In this work we extensively evaluate different approaches to globally register UGV generated 3D point-cloud data from LiDAR sensors with UAV generated point-cloud maps from vision sensors. The approaches are realizations of different selections for: a) local features: key-points or segments; b) descriptors: FPFH, SHOT, or ESF; and c) transformation estimations: RANSAC or FGR. Additionally, we compare the results against standard approaches like applying ICP after a good prior transformation has been given. The evaluation criteria include the distance which a UGV needs to travel to successfully localize, the registration error, and the computational cost. In this context, we report our findings on effectively performing the task on two new Search and Rescue datasets. Our results have the potential to help the community take informed decisions when registering point-cloud maps from ground robots to those from aerial robots. version:1
arxiv-1709-00572 | XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification | http://arxiv.org/abs/1709.00572 | id:1709.00572 author:Cătălina Cangea, Petar Veličković, Pietro Liò category:stat.ML cs.AI cs.CV cs.LG  published:2017-09-02 summary:We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between the feature extractors, thereby extracting more interpretable features and obtaining a better representation than through unimodal learning, for the same amount of training data. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are therefore nontrivially exchangeable. Our work improves on existing multimodal deep learning metholodogies in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections, which only transfer information between streams that process compatible data. Both cross-modal architectures outperformed their baselines (by up to 7.5%) when evaluated on the AVletters dataset. version:1
arxiv-1709-00546 | Autonomous Waypoint Generation with Safety Guarantees: On-Line Motion Planning in Unknown Environments | http://arxiv.org/abs/1709.00546 | id:1709.00546 author:Sanjeev Sharma category:cs.RO  published:2017-09-02 summary:On-line motion planning in unknown environments is a challenging problem as it requires (i) ensuring collision avoidance and (ii) minimizing the motion time, while continuously predicting where to go next. Previous approaches to on-line motion planning assume that a rough map of the environment is available, thereby simplifying the problem. This paper presents a reactive on-line motion planner, Robust Autonomous Waypoint generation (RAW), for mobile robots navigating in unknown and unstructured environments. RAW generates a locally maximal ellipsoid around the robot, using semi-definite programming, such that the surrounding obstacles lie outside the ellipsoid. A reinforcement learning agent then generates a local waypoint in the robot's field of view, inside the ellipsoid. The robot navigates to the waypoint and the process iterates until it reaches the goal. By following the waypoints the robot navigates through a sequence of overlapping ellipsoids, and avoids collision. Robot's safety is guaranteed theoretically and the claims are validated through rigorous numerical experiments in four different experimental setups. Near-optimality is shown empirically by comparing RAW trajectories with the global optimal trajectories. version:1
arxiv-1709-00539 | An Automated Compatibility Prediction Engine using DISC Theory Based Classification and Neural Networks | http://arxiv.org/abs/1709.00539 | id:1709.00539 author:Chandrasekaran Anirudh Bhardwaj, Megha Mishra, Sweetlin Hemalatha category:cs.AI cs.NE  published:2017-09-02 summary:Traditionally psychometric tests were used for profiling incoming workers. These methods use DISC profiling method to classify people into distinct personality types, which are further used to predict if a person may be a possible fit to the organizational culture. This concept is taken further by introducing a novel technique to predict if a particular pair of an incoming worker and the manager being assigned are compatible at a psychological scale. This is done using multilayer perceptron neural network which can be adaptively trained to showcase the true nature of the compatibility index. The proposed prototype model is used to quantify the relevant attributes, use them to train the prediction engine, and to define the data pipeline required for it. version:1
arxiv-1709-00525 | Sensor Network Based Collision-Free Navigation and Map Building for Mobile Robots | http://arxiv.org/abs/1709.00525 | id:1709.00525 author:Hang Li category:cs.RO  published:2017-09-02 summary:Safe robot navigation is a fundamental research field for autonomous robots including ground mobile robots and flying robots. The primary objective of a safe robot navigation algorithm is to guide an autonomous robot from its initial position to a target or along a desired path with obstacle avoidance. With the development of information technology and sensor technology, the implementations combining robotics with sensor network are focused on in the recent researches. One of the relevant implementations is the sensor network based robot navigation. Moreover, another important navigation problem of robotics is safe area search and map building. In this report, a global collision-free path planning algorithm for ground mobile robots in dynamic environments is presented firstly. Considering the advantages of sensor network, the presented path planning algorithm is developed to a sensor network based navigation algorithm for ground mobile robots. The 2D range finder sensor network is used in the presented method to detect static and dynamic obstacles. The sensor network can guide each ground mobile robot in the detected safe area to the target. Furthermore, the presented navigation algorithm is extended into 3D environments. With the measurements of the sensor network, any flying robot in the workspace is navigated by the presented algorithm from the initial position to the target. Moreover, in this report, another navigation problem, safe area search and map building for ground mobile robot, is studied and two algorithms are presented. In the first presented method, we consider a ground mobile robot equipped with a 2D range finder sensor searching a bounded 2D area without any collision and building a complete 2D map of the area. Furthermore, the first presented map building algorithm is extended to another algorithm for 3D map building. version:1
arxiv-1709-00513 | Learning Loss for Knowledge Distillation with Conditional Adversarial Networks | http://arxiv.org/abs/1709.00513 | id:1709.00513 author:Zheng Xu, Yen-Chang Hsu, Jiawei Huang category:cs.LG cs.AI cs.CV  published:2017-09-02 summary:There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information provided by a large and accurate teacher network. We use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student. version:1
arxiv-1709-00503 | Mean Actor Critic | http://arxiv.org/abs/1709.00503 | id:1709.00503 author:Kavosh Asadi, Cameron Allen, Melrose Roderick, Abdel-rahman Mohamed, George Konidaris, Michael Littman category:stat.ML cs.AI cs.LG  published:2017-09-01 summary:We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action continuous-state reinforcement learning. MAC is a policy gradient algorithm that uses the agent's explicit representation of all action values to estimate the gradient of the policy, rather than using only the actions that were actually executed. This significantly reduces variance in the gradient updates and removes the need for a variance reduction baseline. We show empirical results on two control domains where MAC performs as well as or better than other policy gradient approaches, and on five Atari games, where MAC is competitive with state-of-the-art policy search algorithms. version:1
arxiv-1709-00488 | RMPD - A Recursive Mid-Point Displacement Algorithm for Path Planning | http://arxiv.org/abs/1709.00488 | id:1709.00488 author:Fangda Li, Avinash C. Kak category:cs.RO  published:2017-09-01 summary:Motivated by what is required for real-time path planning, the paper starts out by presenting sRMPD, a new recursive "local" planner founded on the key notion that, unless made necessary by an obstacle, there must be no deviation from the shortest path between any two points, which would normally be a straight line path in the configuration space. Subsequently, we increase the power of sRMPD by using it as a "connect" subroutine call in a higher-level sampling-based algorithm mRMPD that is inspired by multi-RRT. As a consequence, mRMPD spawns a larger number of space exploring trees in regions of the configuration space that are characterized by a higher density of obstacles. The overall effect is a hybrid tree growing strategy with a trade-off between random exploration as made possible by multi-RRT based logic and immediate exploitation of opportunities to connect two states as made possible by sRMPD. The mRMPD planner can be biased with regard to this trade-off for solving different kinds of planning problems efficiently. Based on the test cases we have run, our experiments show that mRMPD can reduce planning time by up to 80% compared to basic RRT. version:1
arxiv-1709-00462 | Mobile Edge Computing Empowers Internet of Things | http://arxiv.org/abs/1709.00462 | id:1709.00462 author:Xiang Sun, Nirwan Ansari category:cs.DC  published:2017-09-01 summary:In this paper, we propose a Mobile Edge Internet of Things (MEIoT) architecture by leveraging the fiber-wireless access technology, the cloudlet concept, and the software defined networking framework. The MEIoT architecture brings computing and storage resources close to Internet of Things (IoT) devices in order to speed up IoT data sharing and analytics. Specifically, the IoT devices (belonging to the same user) are associated to a specific proxy Virtual Machine (VM) in the nearby cloudlet. The proxy VM stores and analyzes the IoT data (generated by its IoT devices) in real-time. Moreover, we introduce the semantic and social IoT technology in the context of MEIoT to solve the interoperability and inefficient access control problem in the IoT system. In addition, we propose two dynamic proxy VM migration methods to minimize the end-to-end delay between proxy VMs and their IoT devices and to minimize the total on-grid energy consumption of the cloudlets, respectively. Performance of the proposed methods are validated via extensive simulations. version:1
arxiv-1709-00412 | My Home is My Post-Office: Evaluation of a decentralized email architecture on Internet-of-Things low-end device | http://arxiv.org/abs/1709.00412 | id:1709.00412 author:Gregory Tsipenyuk, Jon Crowcroft category:cs.DC  published:2017-09-01 summary:Users predominantly access their email via mobile devices. This presents a two-fold challenge to the email applications. First, email's update from multiple devices has to be eventually reconciled with the server. Prioritization of updates is difficult and maybe undesirable. Solving this problem requires a data store with the complete history of email changes. Second, legacy email protocols don't provide an optimal email synchronization and access in mobile environment. In this paper we are proposing to take advantage of the Internet of Things (IoT) phenomena. In IoT environment a user may have multiple interconnected in-home low-end devices with publicly accessible address. In this architecture we move the email application from the central service into user's in-home and mobile devices, store complete email history on each device, and replace legacy IMAP and SMTP protocols with a synchronization protocol found in Distributed Version Control Systems(DVCS). This addresses the email reconciliation issue, optimizes the bandwidth usage, and intrinsically puts the user in control of her data. We analyze a number of stores and synchronization implementations and compare them with the open source Dovecot email server. version:1
arxiv-1709-00359 | Convergence, Continuity and Recurrence in Dynamic Epistemic Logic | http://arxiv.org/abs/1709.00359 | id:1709.00359 author:Dominik Klein, Rasmus K. Rendsvig category:cs.LO cs.AI  published:2017-09-01 summary:The paper analyzes dynamic epistemic logic from a topological perspective. The main contribution consists of a framework in which dynamic epistemic logic satisfies the requirements for being a topological dynamical system thus interfacing discrete dynamic logics with continuous mappings of dynamical systems. The setting is based on a notion of logical convergence, demonstratively equivalent with convergence in Stone topology. Presented is a flexible, parametrized family of metrics inducing the latter, used as an analytical aid. We show maps induced by action model transformations continuous with respect to the Stone topology and present results on the recurrent behavior of said maps. version:1
arxiv-1709-00348 | Inferring Networked Device Categories from Low-Level Activity Indicators | http://arxiv.org/abs/1709.00348 | id:1709.00348 author:Kyumars Sheykh Esmaili, Jaideep Chandrashekar, Pascal Le Guyadec category:cs.NI cs.AI  published:2017-09-01 summary:We study the problem of inferring the type of a networked device in a home network by leveraging low level traffic activity indicators seen at commodity home gateways. We analyze a dataset of detailed device network activity obtained from 240 subscriber homes of a large European ISP and extract a number of traffic and spatial fingerprints for individual devices. We develop a two level taxonomy to describe devices onto which we map individual devices using a number of heuristics. We leverage the heuristically derived labels to train classifiers that distinguish device classes based on the traffic and spatial fingerprints of a device. Our results show an accuracy level up to 91% for the coarse level category and up to 84% for the fine grained category. By incorporating information from other sources (e.g., MAC OUI), we are able to further improve accuracy to above 97% and 92%, respectively. Finally, we also extract a set of simple and human-readable rules that concisely capture the behaviour of these distinct device categories. version:1
arxiv-1709-00333 | Kafka versus RabbitMQ | http://arxiv.org/abs/1709.00333 | id:1709.00333 author:Philippe Dobbelaere, Kyumars Sheykh Esmaili category:cs.DC cs.PF  published:2017-09-01 summary:Publish/subscribe is a distributed interaction paradigm well adapted to the deployment of scalable and loosely coupled systems. Apache Kafka and RabbitMQ are two popular open-source and commercially-supported pub/sub systems that have been around for almost a decade and have seen wide adoption. Given the popularity of these two systems and the fact that both are branded as pub/sub systems, two frequently asked questions in the relevant online forums are: how do they compare against each other and which one to use? In this paper, we frame the arguments in a holistic approach by establishing a common comparison framework based on the core functionalities of pub/sub systems. Using this framework, we then venture into a qualitative and quantitative (i.e. empirical) comparison of the common features of the two systems. Additionally, we also highlight the distinct features that each of these systems has. After enumerating a set of use cases that are best suited for RabbitMQ or Kafka, we try to guide the reader through a determination table to choose the best architecture given his/her particular set of requirements. version:1
arxiv-1708-05565 | LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online Auctions | http://arxiv.org/abs/1708.05565 | id:1708.05565 author:Yu Wang, Jiayi Liu, Yuxiang Liu, Jun Hao, Yang He, Jinghe Hu, Weipeng P. Yan, Mantian Li category:cs.LG cs.AI cs.CL cs.GT  published:2017-08-18 summary:We present LADDER, the first deep reinforcement learning agent that can successfully learn control policies for large-scale real-world problems directly from raw inputs composed of high-level semantic information. The agent is based on an asynchronous stochastic variant of DQN (Deep Q Network) named DASQN. The inputs of the agent are plain-text descriptions of states of a game of incomplete information, i.e. real-time large scale online auctions, and the rewards are auction profits of very large scale. We apply the agent to an essential portion of JD's online RTB (real-time bidding) advertising business and find that it easily beats the former state-of-the-art bidding policy that had been carefully engineered and calibrated by human experts: during JD.com's June 18th anniversary sale, the agent increased the company's ads revenue from the portion by more than 50%, while the advertisers' ROI (return on investment) also improved significantly. version:2
arxiv-1709-00309 | 2D Map Alignment With Region Decomposition | http://arxiv.org/abs/1709.00309 | id:1709.00309 author:Saeed Gholami Shahbandi, Martin Magnusson category:cs.RO  published:2017-09-01 summary:In many applications of autonomous mobile robots, the following problem is encountered. Two maps of the same environment are available, one a prior map and the other a sensor map built by the robot. To exploit the available information in both maps to the full capacity, the robot must find the correct association between the frames of the input maps. There exists many approaches to address this challenge, however, most of those methods rely on assumptions such as similar modalities of the maps, same scale, or existence of an initial guess for the alignment. In this work we proposed a decomposition based method for 2D spatial map alignment which do not rely on aforementioned assumptions. The proposed method is validated and compared with other approaches from generic data association approaches to map alignment specific algorithms, over real world examples of four different environment with thirty six sensor maps and four layout maps. version:1
arxiv-1709-00302 | Two-Sided Reduction to Compact Band Forms with Look-Ahead | http://arxiv.org/abs/1709.00302 | id:1709.00302 author:Rafael Rodríguez-Sánchez, Sandra Catalán, José R. Herrero, Enrique S. Quintana-Ortí, Andrés E. Tomás category:cs.MS cs.DC  published:2017-09-01 summary:We address the reduction to compact band forms, via unitary similarity transformations, for the solution of symmetric eigenvalue problems and singular value problems. Concretely, in the first case we revisit the reduction to symmetric band form while, for the second case, we propose a similar alternative, which transforms the original matrix to (unsymmetric) band form, replacing the conventional reduction method that produces a triangular-band output. In both cases, we describe algorithmic variants of the standard Level-3 BLAS-based procedures, enhanced with look-ahead, to overcome the performance bottleneck imposed by the panel factorization. Furthermore, our solutions detach the algorithmic block size from the target bandwidth, illustrating the important performance benefits of this decision. Finally, we show that our alternative compact band form for singular value problems is key to introduce look-ahead into the corresponding reduction procedure. version:1
arxiv-1709-00411 | On Reliability-Aware Server Consolidation in Cloud Datacenters | http://arxiv.org/abs/1709.00411 | id:1709.00411 author:Amir Varasteh, Farzad Tashtarian, Maziar Goudarzi category:cs.DC cs.NI  published:2017-09-01 summary:In the past few years, datacenter (DC) energy consumption has become an important issue in technology world. Server consolidation using virtualization and virtual machine (VM) live migration allows cloud DCs to improve resource utilization and hence energy efficiency. In order to save energy, consolidation techniques try to turn off the idle servers, while because of workload fluctuations, these offline servers should be turned on to support the increased resource demands. These repeated on-off cycles could affect the hardware reliability and wear-and-tear of servers and as a result, increase the maintenance and replacement costs. In this paper we propose a holistic mathematical model for reliability-aware server consolidation with the objective of minimizing total DC costs including energy and reliability costs. In fact, we try to minimize the number of active PMs and racks, in a reliability-aware manner. We formulate the problem as a Mixed Integer Linear Programming (MILP) model which is in form of NP-complete. Finally, we evaluate the performance of our approach in different scenarios using extensive numerical MATLAB simulations. version:1
arxiv-1708-09603 | PolarBear: A 28-nm FD-SOI ASIC for Decoding of Polar Codes | http://arxiv.org/abs/1708.09603 | id:1708.09603 author:Pascal Giard, Alexios Balatsoukas-Stimming, Thomas Christoph Müller, Andrea Bonetti, Claude Thibeault, Warren J. Gross, Philippe Flatresse, Andreas Burg category:cs.AR cs.IT math.IT  published:2017-08-31 summary:Polar codes are a recently proposed class of block codes that provably achieve the capacity of various communication channels. They received a lot of attention as they can do so with low-complexity encoding and decoding algorithms, and they have an explicit construction. Their recent inclusion in a 5G communication standard will only spur more research. However, only a couple of ASICs featuring decoders for polar codes were fabricated, and none of them implements a list-based decoding algorithm. In this paper, we present ASIC measurement results for a fabricated 28 nm CMOS chip that implements two different decoders: the first decoder is tailored toward error-correction performance and flexibility. It supports any code rate as well as three different decoding algorithms: successive cancellation (SC), SC flip and SC list (SCL). The flexible decoder can also decode both non-systematic and systematic polar codes. The second decoder targets speed and energy efficiency. We present measurement results for the first silicon-proven SCL decoder, where its coded throughput is shown to be of 306.8 Mbps with a latency of 3.34 us and an energy per bit of 418.3 pJ/bit at a clock frequency of 721 MHz for a supply of 1.3 V. The energy per bit drops down to 178.1 pJ/bit with a more modest clock frequency of 308 MHz, lower throughput of 130.9 Mbps and a reduced supply voltage of 0.9 V. For the other two operating modes, the energy per bit is shown to be of approximately 95 pJ/bit. The less flexible high-throughput unrolled decoder can achieve a coded throughput of 9.2 Gbps and a latency of 628 ns for a measured energy per bit of 1.15 pJ/bit at 451 MHz. version:2
arxiv-1709-00410 | Visual art inspired by the collective feeding behavior of sand-bubbler crabs | http://arxiv.org/abs/1709.00410 | id:1709.00410 author:Hendrik Richter category:cs.NE cs.AI q-bio.PE  published:2017-09-01 summary:Sand-bubbler crabs of the genera Dotilla and Scopimera are known to produce remarkable patterns and structures at tropical beaches. From these pattern-making abilities, we may draw inspiration for digital visual art. A simple mathematical model of sand--bubbler patterns is proposed and an algorithm is designed that may create such patterns artificially. In addition, design parameters to modify the patterns are identified and analyzed by computational aesthetic measures. Finally, an extension of the algorithm is discussed that may enable controlling and guiding generative evolution of the art-making process. version:1
arxiv-1709-00198 | Optimal epidemic dissemination | http://arxiv.org/abs/1709.00198 | id:1709.00198 author:Hugues Mercier, Laurent Hayez, Miguel Matos category:cs.DC C.2.4; F.2  published:2017-09-01 summary:We consider the problem of reliable epidemic dissemination of a rumor in a fully connected network of~$n$ processes using push and pull operations. We revisit the random phone call model and show that it is possible to disseminate a rumor to all processes with high probability using $\Theta(\ln n)$ rounds of communication and only $n+o(n)$ messages of size $b$, all of which are asymptotically optimal and achievable with pull and push-then-pull algorithms. This contradicts two highly-cited lower bounds of Karp et al. stating that any algorithm in the random phone call model running in $\mathcal{O}(\ln n)$ rounds with communication peers chosen uniformly at random requires at least $\omega(n)$ messages to disseminate a rumor with high probability, and that any address-oblivious algorithm needs $\Omega(n \ln \ln n)$ messages regardless of the number of communication rounds. The reason for this contradiction is that in the original work, processes do not have to share the rumor once the communication is established. However, it is implicitly assumed that they always do so in the proofs of their lower bounds, which, it turns out, is not optimal. Our algorithms are strikingly simple, address-oblivious, and robust against $\epsilon n$ adversarial failures and stochastic failures occurring with probability $\delta$ for any $0 \leq \{\epsilon,\delta\} < 1$. Furthermore, they can handle multiple rumors of size $b \in \omega(\ln n \ln \ln n)$ with $nb + o(nb)$ bits of communication per rumor. version:1
arxiv-1709-00196 | On Heterogeneous Coded Distributed Computing | http://arxiv.org/abs/1709.00196 | id:1709.00196 author:Mehrdad Kiamari, Chenwei Wang, A. Salman Avestimehr category:cs.DC cs.IT math.IT  published:2017-09-01 summary:We consider the recently proposed Coded Distributed Computing (CDC) framework that leverages carefully designed redundant computations to enable coding opportunities that substantially reduce the communication load of distributed computing. We generalize this framework to heterogeneous systems where different nodes in the computing cluster can have different storage (or processing) capabilities. We provide the information-theoretically optimal data set placement and coded data shuffling scheme that minimizes the communication load in a cluster with 3 nodes. For clusters with $K>3$ nodes, we provide an algorithm description to generalize our coding ideas to larger networks. version:1
arxiv-1709-00155 | Order-Planning Neural Text Generation From Structured Data | http://arxiv.org/abs/1709.00155 | id:1709.00155 author:Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li, Baobao Chang, Zhifang Sui category:cs.CL cs.AI cs.IR cs.LG  published:2017-09-01 summary:Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WikiBio dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores. version:1
arxiv-1709-00149 | Learning what to read: Focused machine reading | http://arxiv.org/abs/1709.00149 | id:1709.00149 author:Enrique Noriega-Atala, Marco A. Valenzuela-Escarcega, Clayton T. Morrison, Mihai Surdeanu category:cs.AI cs.CL cs.IR cs.LG H.3.3; I.2.6; I.2.7  published:2017-09-01 summary:Recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today's scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and processing overhead. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of algorithms for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the baseline, while being more efficient, i.e., reading fewer documents. version:1
arxiv-1709-00105 | A Novel Fog-Assisted Architecture for the Hospitality Industry | http://arxiv.org/abs/1709.00105 | id:1709.00105 author:Prasanna Kansakar, Arslan Munir, Neda Shabani category:cs.DC  published:2017-08-31 summary:The leisure and hospitality industry is one of the driving forces of the global economy. The widespread adoption of new technologies in this industry over recent years has fundamentally reshaped the way in which services are provided and received. However, there are still major challenges that need to be addressed to ensure that this industry maintains a steady pace in adoption of future technologies. In this paper, we identify these challenges and describe the problems they pose for guests and hospitality service providers (HSP). As a means to overcome these challenges, we propose a novel fog-assisted architecture which creates a sound technological framework with specialized guest-facing and back-of-house (BoH) management systems geared towards improving guest experience, enhancing business insight, and increasing revenue. The proposed architecture integrates cloud services platform with Internet of things (IoT) devices at the edge of the network through intermediary fog computing nodes to create an architectural paradigm with the benefits of both distributed computing and centralized analytics. The layered structure of this architecture facilitates interoperability between heterogeneous systems as well as resource sharing and cooperation between multiple deployments and implementations. Moreover, the adaptability of this architecture promotes scalability making it suitable for hotels of different caliber, and guarantees upgradability making it possible to add new and improved hospitality services in the future. version:1
arxiv-1709-00023 | R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering | http://arxiv.org/abs/1709.00023 | id:1709.00023 author:Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerald Tesauro, Bowen Zhou, Jing Jiang category:cs.CL cs.AI  published:2017-08-31 summary:In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that "reads" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader $(R^3)$, based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets. version:1
arxiv-1709-00341 | Structured Linearization of Discrete Mechanical Systems for Analysis and Optimal Control | http://arxiv.org/abs/1709.00341 | id:1709.00341 author:Elliot Johnson, Jarvis Schultz, Todd Murphey category:math.OC cs.RO  published:2017-08-31 summary:Variational integrators are well-suited for simulation of mechanical systems because they preserve mechanical quantities about a system such as momentum, or its change if external forcing is involved, and holonomic constraints. While they are not energy-preserving they do exhibit long-time stable energy behavior. However, variational integrators often simulate mechanical system dynamics by solving an implicit difference equation at each time step, one that is moreover expressed purely in terms of configurations at different time steps. This paper formulates the first- and second-order linearizations of a variational integrator in a manner that is amenable to control analysis and synthesis, creating a bridge between existing analysis and optimal control tools for discrete dynamic systems and variational integrators for mechanical systems in generalized coordinates with forcing and holonomic constraints. The forced pendulum is used to illustrate the technique. A second example solves the discrete LQR problem to find a locally stabilizing controller for a 40 DOF system with 6 constraints. version:1
arxiv-1709-00376 | Online Feedback Control for Input-Saturated Robotic Systems on Lie Groups | http://arxiv.org/abs/1709.00376 | id:1709.00376 author:Taosha Fan, Todd Murphey category:math.OC cs.RO  published:2017-08-31 summary:In this paper, we propose an approach to designing online feedback controllers for input-saturated robotic systems evolving on Lie groups by extending the recently developed Sequential Action Control (SAC). In contrast to existing feedback controllers, our approach poses the nonconvex constrained nonlinear optimization problem as the tracking of a desired negative mode insertion gradient on the configuration space of a Lie group. This results in a closed-form feedback control law even with input saturation and thus is well suited for online application. In extending SAC to Lie groups, the associated mode insertion gradient is derived and the switching time optimization on Lie groups is studied. We demonstrate the efficacy and scalability of our approach in the 2D kinematic car on SE(2) and the 3D quadrotor on SE(3). We also implement iLQG on a quadrator model and compare to SAC, demonstrating that SAC is both faster to compute and has a larger basin of attraction. version:1
arxiv-1709-00342 | Real-time Dynamic-Mode Scheduling Using Single-Integration Hybrid Optimization for Linear Time-Varying Systems | http://arxiv.org/abs/1709.00342 | id:1709.00342 author:Anastasia Mavrommati, Jarvis A. Schultz, Todd D. Murphey category:math.OC cs.RO  published:2017-08-31 summary:This paper considers the problem of real-time mode scheduling in linear time-varying switched systems subject to a quadratic cost functional. The execution time of hybrid control algorithms is often prohibitive for real-time applications and typically may only be reduced at the expense of approximation accuracy. We address this trade-off by taking advantage of system linearity to formulate a projection-based approach so that no simulation is required during open-loop optimization. A numerical example shows how the proposed open-loop algorithm outperforms methods employing common numerical integration techniques. Additionally, we follow a receding-horizon scheme to apply real-time closed-loop hybrid control to a customized experimental setup, using the Robot Operating System (ROS). In particular, we demonstrate---both in Monte-Carlo simulation and in experiment---that optimal hybrid control efficiently regulates a cart and suspended mass system in real time. version:1
arxiv-1708-08227 | ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity? | http://arxiv.org/abs/1708.08227 | id:1708.08227 author:Mostapha Benhenda category:stat.ML cs.AI cs.LG  published:2017-08-28 summary:Generating molecules with desired chemical properties is important for drug discovery. The use of generative neural networks is promising for this task. However, from visual inspection, it often appears that generated samples lack diversity. In this paper, we quantify this internal chemical diversity, and we raise the following challenge: can a nontrivial AI model reproduce natural chemical diversity for desired molecules? To illustrate this question, we consider two generative models: a Reinforcement Learning model and the recently introduced ORGAN. Both fail at this challenge. We hope this challenge will stimulate research in this direction. version:3
arxiv-1708-09707 | Algorithmic patterns for $\mathcal{H}$-matrices on many-core processors | http://arxiv.org/abs/1708.09707 | id:1708.09707 author:Peter Zaspel category:cs.DC cs.MS cs.NA math.NA  published:2017-08-31 summary:In this work, we consider the reformulation of hierarchical ($\mathcal{H}$) matrix algorithms for many-core processors with a model implementation on graphics processing units (GPUs). $\mathcal{H}$ matrices approximate specific dense matrices, e.g., from discretized integral equations or kernel ridge regression, leading to log-linear time complexity in dense matrix-vector products. The parallelization of $\mathcal{H}$ matrix operations on many-core processors is difficult due to the complex nature of the underlying algorithms. While previous algorithmic advances for many-core hardware focused on accelerating existing $\mathcal{H}$ matrix CPU implementations by many-core processors, we here aim at totally relying on that processor type. As main contribution, we introduce the necessary parallel algorithmic patterns allowing to map the full $\mathcal{H}$ matrix construction and the fast matrix-vector product to many-core hardware. Here, crucial ingredients are space filling curves, parallel tree traversal and batching of linear algebra operations. The resulting model GPU implementation hmglib is the, to the best of the authors knowledge, first entirely GPU-based Open Source $\mathcal{H}$ matrix library of this kind. We conclude this work by an in-depth performance analysis and a comparative performance study against a standard $\mathcal{H}$ matrix library, highlighting profound speedups of our many-core parallel approach. version:1
arxiv-1709-02287 | Gravitational Clustering: A Simple, Robust and Adaptive Approach for Distributed Networks | http://arxiv.org/abs/1709.02287 | id:1709.02287 author:Patricia Binder, Michael Muma, Abdelhak M. Zoubir category:cs.DC cs.CV  published:2017-08-31 summary:Distributed signal processing for wireless sensor networks enables that different devices cooperate to solve different signal processing tasks. A crucial first step is to answer the question: who observes what? Recently, several distributed algorithms have been proposed, which frame the signal/object labelling problem in terms of cluster analysis after extracting source-specific features, however, the number of clusters is assumed to be known. We propose a new method called Gravitational Clustering (GC) to adaptively estimate the time-varying number of clusters based on a set of feature vectors. The key idea is to exploit the physical principle of gravitational force between mass units: streaming-in feature vectors are considered as mass units of fixed position in the feature space, around which mobile mass units are injected at each time instant. The cluster enumeration exploits the fact that the highest attraction on the mobile mass units is exerted by regions with a high density of feature vectors, i.e., gravitational clusters. By sharing estimates among neighboring nodes via a diffusion-adaptation scheme, cooperative and distributed cluster enumeration is achieved. Numerical experiments concerning robustness against outliers, convergence and computational complexity are conducted. The application in a distributed cooperative multi-view camera network illustrates the applicability to real-world problems. version:1
arxiv-1708-09597 | Advanced Datapath Synthesis using Graph Isomorphism | http://arxiv.org/abs/1708.09597 | id:1708.09597 author:Cunxi Yu, Mihir Choudhury, Andrew Sullivan, Maciej Ciesielski category:cs.AR  published:2017-08-31 summary:This paper presents an advanced DAG-based algorithm for datapath synthesis that targets area minimization using logic-level resource sharing. The problem of identifying common specification logic is formulated using unweighted graph isomorphism problem, in contrast to a weighted graph isomorphism using AIGs. In the context of gate-level datapath circuits, our algorithm solves the un- weighted graph isomorphism problem in linear time. The experiments are conducted within an industrial synthesis flow that includes the complete high-level synthesis, logic synthesis and placement and route procedures. Experimental results show a significant runtime improvements compared to the existing datapath synthesis algorithms. version:1
arxiv-1708-09495 | Integer sorting on multicores: some (experiments and) observations | http://arxiv.org/abs/1708.09495 | id:1708.09495 author:Alexandros V. Gerbessiotis category:cs.DC  published:2017-08-30 summary:There have been many proposals for sorting integers on multicores/GPUs that include radix-sort and its variants or other approaches that exploit specialized hardware features of a particular multicore architecture. Comparison-based algorithms have also been used. Network-based algorithms have also been used with primary example Batcher's bitonic sorting algorithm. Although such a latter approach is theoretically "inefficient", if there are few keys to sort, it can lead to better running times as it has low overhead and is simple to implement. In this work we perform an experimental study of integer sorting on multicore processors using not only multithreading but also multiprocessing parallel programming approaches. Our implementations work under Open MPI, MulticoreBSP, and BSPlib. We have implemented serial and parallel radix-sort for various radixes and also some previously little explored or unexplored variants of bitonic-sort and odd-even transposition sort. We offer our observations on a performance evaluation using the MBSP model of such algorithm implementations on multiple platforms and architectures and multiple programming libraries. If we can conclude anything is that modeling their performance by taking into consideration architecture dependent features such as the structure and characteristics of multiple memory hierarchies is difficult and more often than not unsuccessful or unreliable. However we can still draw some very simple conclusions using traditional architecture independent parallel modeling. version:1
arxiv-1708-09453 | Inference of Fine-Grained Event Causality from Blogs and Films | http://arxiv.org/abs/1708.09453 | id:1708.09453 author:Zhichao Hu, Elahe Rahimtoroghi, Marilyn A Walker category:cs.CL cs.AI  published:2017-08-30 summary:Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire. version:1
arxiv-1708-09450 | Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events | http://arxiv.org/abs/1708.09450 | id:1708.09450 author:Elahe Rahimtoroghi, Ernesto Hernandez, Marilyn A Walker category:cs.CL cs.AI  published:2017-08-30 summary:Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-specific datasets enables learning finer-grained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent. version:1
arxiv-1708-09441 | Incorporating Feedback into Tree-based Anomaly Detection | http://arxiv.org/abs/1708.09441 | id:1708.09441 author:Shubhomoy Das, Weng-Keen Wong, Alan Fern, Thomas G. Dietterich, Md Amran Siddiqui category:cs.LG cs.AI stat.ML I.2.6; I.5.5  published:2017-08-30 summary:Anomaly detectors are often used to produce a ranked list of statistical anomalies, which are examined by human analysts in order to extract the actual anomalies of interest. Unfortunately, in realworld applications, this process can be exceedingly difficult for the analyst since a large fraction of high-ranking anomalies are false positives and not interesting from the application perspective. In this paper, we aim to make the analyst's job easier by allowing for analyst feedback during the investigation process. Ideally, the feedback influences the ranking of the anomaly detector in a way that reduces the number of false positives that must be examined before discovering the anomalies of interest. In particular, we introduce a novel technique for incorporating simple binary feedback into tree-based anomaly detectors. We focus on the Isolation Forest algorithm as a representative tree-based anomaly detector, and show that we can significantly improve its performance by incorporating feedback, when compared with the baseline algorithm that does not incorporate feedback. Our technique is simple and scales well as the size of the data increases, which makes it suitable for interactive discovery of anomalies in large datasets. version:1
arxiv-1708-06207 | Givers & Receivers perceive handover tasks differently: Implications for Human-Robot collaborative system design | http://arxiv.org/abs/1708.06207 | id:1708.06207 author:Roy Someshwar, Yael Edan category:cs.HC cs.CY cs.RO cs.SY  published:2017-08-10 summary:Human-human joint-action in short-cycle repetitive handover tasks was investigated for a bottle handover task using a three-fold approach: work-methods field studies in multiple supermarkets, simulation analysis using an ergonomics software package and by conducting an in-house lab experiment on human-human collaboration by re-creating the environment and conditions of a supermarket. Evaluation included both objective and subjective measures. Subjective evaluation was done taking a psychological perspective and showcases among other things, the differences in the way a common joint-action is being perceived by individual team partners depending upon their role (giver or receiver). The proposed approach can provide a systematic method to analyze similar tasks. Combining the results of all the three analyses, this research gives insight into the science of joint-action for short-cycle repetitive tasks and its implications for human-robot collaborative system design. version:2
arxiv-1708-09352 | Ergodic Exploration of Distributed Information | http://arxiv.org/abs/1708.09352 | id:1708.09352 author:Lauren M. Miller, Yonatan Silverman, Malcolm A. MacIver, Todd D. Murphey category:cs.RO  published:2017-08-30 summary:This paper presents an active search trajectory synthesis technique for autonomous mobile robots with nonlinear measurements and dynamics. The presented approach uses the ergodicity of a planned trajectory with respect to an expected information density map to close the loop during search. The ergodic control algorithm does not rely on discretization of the search or action spaces, and is well posed for coverage with respect to the expected information density whether the information is diffuse or localized, thus trading off between exploration and exploitation in a single objective function. As a demonstration, we use a robotic electrolocation platform to estimate location and size parameters describing static targets in an underwater environment. Our results demonstrate that the ergodic exploration of distributed information (EEDI) algorithm outperforms commonly used information-oriented controllers, particularly when distractions are present. version:1
arxiv-1708-09347 | Sequential Action Control: Closed-Form Optimal Control for Nonlinear and Nonsmooth Systems | http://arxiv.org/abs/1708.09347 | id:1708.09347 author:Alex Ansari, Todd Murphey category:cs.RO cs.SY  published:2017-08-30 summary:This paper presents a new model-based algorithm that computes predictive optimal controls on-line and in closed loop for traditionally challenging nonlinear systems. Examples demonstrate the same algorithm controlling hybrid impulsive, underactuated, and constrained systems using only high-level models and trajectory goals. Rather than iteratively optimize finite horizon control sequences to minimize an objective, this paper derives a closed-form expression for individual control actions, i.e., control values that can be applied for short duration, that optimally improve a tracking objective over a long time horizon. Under mild assumptions, actions become linear feedback laws near equilibria that permit stability analysis and performance-based parameter selection. Globally, optimal actions are guaranteed existence and uniqueness. By sequencing these actions on-line, in receding horizon fashion, the proposed controller provides a min-max constrained response to state that avoids the overhead typically required to impose control constraints. Benchmark examples show the approach can avoid local minima and outperform nonlinear optimal controllers and recent, case-specific methods in terms of tracking performance, and at speeds orders of magnitude faster than traditionally achievable. version:1
arxiv-1708-09342 | Optimal and Learning Control for Autonomous Robots | http://arxiv.org/abs/1708.09342 | id:1708.09342 author:Jonas Buchli, Farbod Farshidian, Alexander Winkler, Timothy Sandy, Markus Giftthaler category:cs.SY cs.LG cs.RO math.OC  published:2017-08-30 summary:Optimal and Learning Control for Autonomous Robots has been taught in the Robotics, Systems and Controls Masters at ETH Zurich with the aim to teach optimal control and reinforcement learning for closed loop control problems from a unified point of view. The starting point is the formulation of of an optimal control problem and deriving the different types of solutions and algorithms from there. These lecture notes aim at supporting this unified view with a unified notation wherever possible, and a bit of a translation help to compare the terminology and notation in the different fields. The course assumes basic knowledge of Control Theory, Linear Algebra and Stochastic Calculus. version:1
arxiv-1709-01494 | Latency Optimal Broadcasting in Noisy Wireless Mesh Networks | http://arxiv.org/abs/1709.01494 | id:1709.01494 author:Qin Xin, Yan Xia category:cs.NI cs.DC  published:2017-08-30 summary:In this paper, we adopt a new noisy wireless network model introduced very recently by Censor-Hillel et al. in [ACM PODC 2017, CHHZ17]. More specifically, for a given noise parameter $p\in [0,1],$ any sender has a probability of $p$ of transmitting noise or any receiver of a single transmission in its neighborhood has a probability $p$ of receiving noise. In this paper, we first propose a new asymptotically latency-optimal approximation algorithm (under faultless model) that can complete single-message broadcasting task in $D+O(\log^2 n)$ time units/rounds in any WMN of size $n,$ and diameter $D$. We then show this diameter-linear broadcasting algorithm remains robust under the noisy wireless network model and also improves the currently best known result in CHHZ17 by a $\Theta(\log\log n)$ factor. In this paper, we also further extend our robust single-message broadcasting algorithm to $k$ multi-message broadcasting scenario and show it can broadcast $k$ messages in $O(D+k\log n+\log^2 n)$ time rounds. This new robust multi-message broadcasting scheme is not only asymptotically optimal but also answers affirmatively the problem left open in CHHZ17 on the existence of an algorithm that is robust to sender and receiver faults and can broadcast $k$ messages in $O(D+k\log n + polylog(n))$ time rounds. version:1
arxiv-1708-09175 | Calibrating chemical multisensory devices for real world applications: An in-depth comparison of quantitative Machine Learning approaches | http://arxiv.org/abs/1708.09175 | id:1708.09175 author:S. De Vito, E. Esposito, M. Salvato, O. Popoola, F. Formisano, R. Jones, G. Di Francia category:cs.AI cs.NE  published:2017-08-30 summary:Chemical multisensor devices need calibration algorithms to estimate gas concentrations. Their possible adoption as indicative air quality measurements devices poses new challenges due to the need to operate in continuous monitoring modes in uncontrolled environments. Several issues, including slow dynamics, continue to affect their real world performances. At the same time, the need for estimating pollutant concentrations on board the devices, espe- cially for wearables and IoT deployments, is becoming highly desirable. In this framework, several calibration approaches have been proposed and tested on a variety of proprietary devices and datasets; still, no thorough comparison is available to researchers. This work attempts a benchmarking of the most promising calibration algorithms according to recent literature with a focus on machine learning approaches. We test the techniques against absolute and dynamic performances, generalization capabilities and computational/storage needs using three different datasets sharing continuous monitoring operation methodology. Our results can guide researchers and engineers in the choice of optimal strategy. They show that non-linear multivariate techniques yield reproducible results, outperforming lin- ear approaches. Specifically, the Support Vector Regression method consistently shows good performances in all the considered scenarios. We highlight the enhanced suitability of shallow neural networks in a trade-off between performance and computational/storage needs. We confirm, on a much wider basis, the advantages of dynamic approaches with respect to static ones that only rely on instantaneous sensor array response. The latter have been shown to be best choice whenever prompt and precise response is needed. version:1
arxiv-1708-09135 | Randomized Load-balanced Routing for Fat-tree Networks | http://arxiv.org/abs/1708.09135 | id:1708.09135 author:Suzhen Wang, Jingjing Luo, Bruce Kwong-Bun Tong, Wing S. Wong category:cs.DC  published:2017-08-30 summary:Fat-tree networks have been widely adopted to High Performance Computing (HPC) clusters and to Data Center Networks (DCN). These parallel systems usually have a large number of servers and hosts, which generate large volumes of highly-volatile traffic. Thus, distributed load-balancing routing design becomes critical to achieve high bandwidth utilization, and low-latency packet delivery. Existing distributed designs rely on remote congestion feedbacks to address congestion, which add overheads to collect and react to network-wide congestion information. In contrast, we propose a simple but effective load-balancing scheme, called Dynamic Randomized load-Balancing (DRB), to achieve network-wide low levels of path collisions through local-link adjustment which is free of communications and cooperations between switches. First, we use D-mod-k path selection scheme to allocate default paths to all source-destination (S-D) pairs in a fat-tree network, guaranteeing low levels of path collision over downlinks for any set of active S-D pairs. Then, we propose Threshold-based Two-Choice (TTC) randomized technique to balance uplink traffic through local uplink adjustment at each switch. We theoretically show that the proposed TTC for the uplink-load balancing in a fat-tree network have a similar performance as the two-choice technique in the area of randomized load balancing. Simulation results show that DRB with TTC technique achieves a significant improvement over many randomized routing schemes for fat-tree networks. version:1
arxiv-1708-08810 | Computation Rate Maximization for Wireless Powered Mobile-Edge Computing with Binary Computation Offloading | http://arxiv.org/abs/1708.08810 | id:1708.08810 author:Suzhi Bi, Ying Jun, Zhang category:cs.DC cs.IT math.IT  published:2017-08-29 summary:In this paper, we consider a multi-user mobile edge computing (MEC) network powered by wireless power transfer (WPT), where each energy-harvesting WD follows a binary computation offloading policy, i.e., data set of a task has to be executed as a whole either locally or remotely at the MEC server via task offloading. In particular, we are interested in maximizing the (weighted) sum computation rate of all the WDs in the network by jointly optimizing the individual computing mode selection (i.e., local computing or offloading) and the system transmission time allocation (on WPT and task offloading). The major difficulty lies in the combinatorial nature of multi-user computing mode selection and its strong coupling with transmission time allocation. To tackle this problem, we first consider a decoupled optimization, where we assume that the mode selection is given and propose a simple bi-section search algorithm to obtain the conditional optimal time allocation. On top of that, a coordinate descent method is devised to optimize the mode selection. The method is simple in implementation but may suffer from high computational complexity in a large-size network. To address this problem, we further propose a joint optimization method based on the ADMM (alternating direction method of multipliers) decomposition technique, which enjoys much slower increase of computational complexity as the networks size increases. Extensive simulations show that both the proposed methods can efficiently achieve near-optimal performance under various network setups, and significantly outperform the other representative benchmark methods considered. version:2
arxiv-1708-09086 | A Deep Learning Approach for Population Estimation from Satellite Imagery | http://arxiv.org/abs/1708.09086 | id:1708.09086 author:Caleb Robinson, Fred Hohman, Bistra Dilkina category:cs.AI cs.CV  published:2017-08-30 summary:Knowing where people live is a fundamental component of many decision making processes such as urban development, infectious disease containment, evacuation planning, risk management, conservation planning, and more. While bottom-up, survey driven censuses can provide a comprehensive view into the population landscape of a country, they are expensive to realize, are infrequently performed, and only provide population counts over broad areas. Population disaggregation techniques and population projection methods individually address these shortcomings, but also have shortcomings of their own. To jointly answer the questions of "where do people live" and "how many people live there," we propose a deep learning model for creating high-resolution population estimations from satellite imagery. Specifically, we train convolutional neural networks to predict population in the USA at a $0.01^{\circ} \times 0.01^{\circ}$ resolution grid from 1-year composite Landsat imagery. We validate these models in two ways: quantitatively, by comparing our model's grid cell estimates aggregated at a county-level to several US Census county-level population projections, and qualitatively, by directly interpreting the model's predictions in terms of the satellite image inputs. We find that aggregating our model's estimates gives comparable results to the Census county-level population projections and that the predictions made by our model can be directly interpreted, which give it advantages over traditional population disaggregation methods. In general, our model is an example of how machine learning techniques can be an effective tool for extracting information from inherently unstructured, remotely sensed data to provide effective solutions to social problems. version:1
arxiv-1708-09040 | Modelling Protagonist Goals and Desires in First-Person Narrative | http://arxiv.org/abs/1708.09040 | id:1708.09040 author:Elahe Rahimtoroghi, Jiaqi Wu, Ruimin Wang, Pranav Anand, Marilyn A Walker category:cs.AI cs.CL cs.NE  published:2017-08-29 summary:Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes gold-standard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus. version:1
arxiv-1708-09020 | Learning to Price with Reference Effects | http://arxiv.org/abs/1708.09020 | id:1708.09020 author:Abbas Kazerouni, Benjamin Van Roy category:cs.GT cs.AI  published:2017-08-29 summary:As a firm varies the price of a product, consumers exhibit reference effects, making purchase decisions based not only on the prevailing price but also the product's price history. We consider the problem of learning such behavioral patterns as a monopolist releases, markets, and prices products. This context calls for pricing decisions that intelligently trade off between maximizing revenue generated by a current product and probing to gain information for future benefit. Due to dependence on price history, realized demand can reflect delayed consequences of earlier pricing decisions. As such, inference entails attribution of outcomes to prior decisions and effective exploration requires planning price sequences that yield informative future outcomes. Despite the considerable complexity of this problem, we offer a tractable systematic approach. In particular, we frame the problem as one of reinforcement learning and leverage Thompson sampling. We also establish a regret bound that provides graceful guarantees on how performance improves as data is gathered and how this depends on the complexity of the demand model. We illustrate merits of the approach through simulations. version:1
arxiv-1708-08976 | Shared Memory Parallelization of MTTKRP for Dense Tensors | http://arxiv.org/abs/1708.08976 | id:1708.08976 author:Koby Hayashi, Grey Ballard, Jeffrey Jiang, Michael Tobia category:cs.DC  published:2017-08-29 summary:The matricized-tensor times Khatri-Rao product (MTTKRP) is the computational bottleneck for algorithms computing CP decompositions of tensors. In this paper, we develop shared-memory parallel algorithms for MTTKRP involving dense tensors. The algorithms cast nearly all of the computation as matrix operations in order to use optimized BLAS subroutines, and they avoid reordering tensor entries in memory. We benchmark sequential and parallel performance of our implementations, demonstrating high sequential performance and efficient parallel scaling. We use our parallel implementation to compute a CP decomposition of a neuroimaging data set and achieve a speedup of up to $7.4\times$ over existing parallel software. version:1
arxiv-1707-09627 | Learning to Infer Graphics Programs from Hand-Drawn Images | http://arxiv.org/abs/1707.09627 | id:1707.09627 author:Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, Joshua B. Tenenbaum category:cs.AI 68T05  published:2017-07-30 summary:We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of $\LaTeX$. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. This set of drawing primitives is like an execution trace for a graphics program. From this trace we use program synthesis techniques to recover a graphics program with constructs such as variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network, cluster drawings by use of similar high-level geometric structures, and extrapolate drawings. Taken together these results are a step towards agents that induce useful, human-readable programs from perceptual input. version:3
arxiv-1709-00322 | Disintegration and Bayesian Inversion, Both Abstractly and Concretely | http://arxiv.org/abs/1709.00322 | id:1709.00322 author:Kenta Cho, Bart Jacobs category:cs.AI  published:2017-08-29 summary:The notions of disintegration and Bayesian inversion are fundamental in conditional probability theory. They produce channels, as conditional probabilities, from a joint state, or from an already given channel (in opposite direction). These notions exist in the literature, in concrete situations, but are presented here in abstract graphical formulations. The resulting abstract descriptions are used for proving basic results in conditional probability theory. The existence of disintegration and Bayesian inversion is discussed for discrete probability, and also for measure-theoretic probability --- via standard Borel spaces and via likelihoods. Finally, the usefulness of disintegration and Bayesian inversion is illustrated in several non-trivial examples. version:1
arxiv-1708-08670 | Performance Analysis of Open Source Machine Learning Frameworks for Various Parameters in Single-Threaded and Multi-Threaded Modes | http://arxiv.org/abs/1708.08670 | id:1708.08670 author:Yuriy Kochura, Sergii Stirenko, Oleg Alienin, Michail Novotarskiy, Yuri Gordienko category:cs.LG cs.CV cs.DC cs.PF  published:2017-08-29 summary:The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation Also, we present the results of testing neural networks architectures on H2O platform for various activation functions, stopping metrics, and other parameters of machine learning algorithm. It was demonstrated for the use case of MNIST database of handwritten digits in single-threaded mode that blind selection of these parameters can hugely increase (by 2-3 orders) the runtime without the significant increase of precision. This result can have crucial influence for optimization of available and new machine learning methods, especially for image recognition problems. version:1
arxiv-1708-08638 | Kernelized Movement Primitives | http://arxiv.org/abs/1708.08638 | id:1708.08638 author:Yanlong Huang, Leonel Rozo, João Silvério, Darwin G. Caldwell category:cs.RO  published:2017-08-29 summary:Imitation learning has been studied widely due to its convenient transfer of human experiences to robots. This learning approach models human demonstrations by extracting relevant motion patterns as well as adaptation to different situations. In order to address unpredicted situations such as obstacles and external perturbations, motor skills adaptation is crucial and non-trivial, particularly in dynamic or unstructured environments. In this paper, we propose to tackle this problem using a novel kernelized movement primitive (KMP) adaptation, which not only allows the robot to adapt its motor skills and meet a variety of additional task constraints arising over the course of the task, but also renders fewer open parameters unlike methods built on basis functions. Moreover, we extend our approach by introducing the concept of local frames, which represent coordinate systems of interest for tasks and could be modulated in accordance with external task parameters, endowing KMP with reliable extrapolation abilities in a broader domain. Several examples of trajectory modulations and extrapolations verify the effectiveness of our method. version:1
arxiv-1708-08917 | CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices | http://arxiv.org/abs/1708.08917 | id:1708.08917 author:Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo, Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian Tang, Qinru Qiu, Xue Lin, Bo Yuan category:cs.CV cs.AI cs.LG stat.ML  published:2017-08-29 summary:Large-scale deep neural networks (DNNs) are both compute and memory intensive. As the size of DNNs continues to grow, it is critical to improve the energy efficiency and performance while maintaining accuracy. For DNNs, the model size is an important factor affecting performance, scalability and energy efficiency. Weight pruning achieves good compression ratios but suffers from three drawbacks: 1) the irregular network structure after pruning; 2) the increased training complexity; and 3) the lack of rigorous guarantee of compression ratio and inference accuracy. To overcome these limitations, this paper proposes CirCNN, a principled approach to represent weights and process neural networks using block-circulant matrices. CirCNN utilizes the Fast Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the computational complexity (both in inference and training) from O(n2) to O(nlogn) and the storage complexity from O(n2) to O(n), with negligible accuracy loss. Compared to other approaches, CirCNN is distinct due to its mathematical rigor: it can converge to the same effectiveness as DNNs without compression. The CirCNN architecture, a universal DNN inference engine that can be implemented on various hardware/software platforms with configurable network architecture. To demonstrate the performance and energy efficiency, we test CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN architecture achieves very high energy efficiency and performance with a small hardware footprint. Based on the FPGA implementation and ASIC synthesis results, CirCNN achieves 6-102X energy efficiency improvements compared with the best state-of-the-art results. version:1
arxiv-1608-05794 | Accelerating finite-rate chemical kinetics with coprocessors: comparing vectorization methods on GPUs, MICs, and CPUs | http://arxiv.org/abs/1608.05794 | id:1608.05794 author:Christopher P. Stone, Andrew T. Alferman, Kyle E. Niemeyer category:physics.comp-ph cs.DC physics.chem-ph  published:2016-08-20 summary:Efficient ordinary differential equation solvers for chemical kinetics must take into account the available thread and instruction-level parallelism of the underlying hardware, especially on many-core coprocessors, as well as the numerical efficiency. A stiff Rosenbrock and nonstiff Runge-Kutta solver are implemented using the single instruction, multiple thread (SIMT) and single instruction, multiple data (SIMD) paradigms with OpenCL. The performances of these parallel implementations were measured with three chemical kinetic models across several multicore and many-core platforms. Two runtime benchmarks were conducted to clearly determine any performance advantage offered by either method: evaluating the right-hand-side source terms in parallel, and integrating a series of constant-pressure homogeneous reactors using the Rosenbrock and Runge-Kutta solvers. The right-hand-side evaluations with SIMD parallelism on the host multicore Xeon CPU and many-core Xeon Phi co-processor performed approximately three times faster than the baseline multithreaded code. The SIMT model on the host and Phi was 13-35% slower than the baseline while the SIMT model on the GPU provided approximately the same performance as the SIMD model on the Phi. The runtimes for both ODE solvers decreased 2.5-2.7x with the SIMD implementations on the host CPU and 4.7-4.9x with the Xeon Phi coprocessor compared to the baseline parallel code. The SIMT implementations on the GPU ran 1.4-1.6 times faster than the baseline multithreaded CPU code; however, this was significantly slower than the SIMD versions on the host CPU or the Xeon Phi. The performance difference between the three platforms was attributed to thread divergence caused by the adaptive step-sizes within the ODE integrators. Analysis showed that the wider vector width of the GPU incurs a higher level of divergence than the narrower Sandy Bridge or Xeon Phi. version:2
arxiv-1708-08559 | DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars | http://arxiv.org/abs/1708.08559 | id:1708.08559 author:Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray category:cs.SE cs.AI cs.LG  published:2017-08-28 summary:Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explores different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge. version:1
arxiv-1708-08551 | Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks | http://arxiv.org/abs/1708.08551 | id:1708.08551 author:Mohammad Amin Nabian, Hadi Meidani category:cs.CE cs.AI stat.ML  published:2017-08-28 summary:Natural disasters can have catastrophic impacts on the functionality of infrastructure systems and cause severe physical and socio-economic losses. Given budget constraints, it is crucial to optimize decisions regarding mitigation, preparedness, response, and recovery practices for these systems. This requires accurate and efficient means to evaluate the infrastructure system reliability. While numerous research efforts have addressed and quantified the impact of natural disasters on infrastructure systems, typically using the Monte Carlo approach, they still suffer from high computational cost and, thus, are of limited applicability to large systems. This paper presents a deep learning framework for accelerating infrastructure system reliability analysis. In particular, two distinct deep neural network surrogates are constructed and studied: (1) A classifier surrogate which speeds up the connectivity determination of networks, and (2) An end-to-end surrogate that replaces a number of components such as roadway status realization, connectivity determination, and connectivity averaging. The proposed approach is applied to a simulation-based study of the two-terminal connectivity of a California transportation network subject to extreme probabilistic earthquake events. Numerical results highlight the effectiveness of the proposed approach in accelerating the transportation system two-terminal reliability analysis with extremely high prediction accuracy. version:1
arxiv-1708-02378 | Investigating Reinforcement Learning Agents for Continuous State Space Environments | http://arxiv.org/abs/1708.02378 | id:1708.02378 author:David Von Dollen category:cs.AI  published:2017-08-08 summary:Given an environment with continuous state spaces and discrete actions, we investigate using a Double Deep Q-learning Reinforcement Agent to find optimal policies using the LunarLander-v2 OpenAI gym environment. version:2
arxiv-1708-08435 | Analyzing Query Performance and Attributing Blame for Contentions in a Cluster Computing Framework | http://arxiv.org/abs/1708.08435 | id:1708.08435 author:Prajakta Kalmegh, Shivnath Babu, Sudeepa Roy category:cs.DC cs.DB  published:2017-08-28 summary:Analyzing contention for resources in a cluster computing environment accurately is critical in order to understand the performance interferences faced by a query due to concurrent query executions, and to better manage the workload in the cluster. Today no tools exist to help an admin perform a deep analysis of resource contentions taking into account the complex interactions among different queries, their stages, and tasks in a shared cluster. In this paper, we present ProtoXplore - a Proto or first system to eXplore the interactions between concurrent queries in a shared cluster. We construct a multi-level directed acyclic graph called ProtoGraph to formally capture different types of explanations that link the performance of concurrent queries. In particular, (a) we designate the components of a query's lost (wait) time as Immediate Explanations towards its observed performance, (b) represent the rate of contention per machine as Deep Explanations, and (c) assign responsibility to concurrent queries through Blame Explanations. We develop new metrics to accurately quantify the impact and distribute the blame among concurrent queries. We perform an extensive experimental evaluation using ProtoXplore to analyze the query interactions of TPCDS queries on Apache Spark using microbenchmarks illustrating the effectiveness of our approach, and illustrate how the output from ProtoXplore can be used by alternate scheduling and task placement strategies to help improve the performance of affected queries in recurring executions. version:1
arxiv-1708-08430 | Deep Belief Networks used on High Resolution Multichannel Electroencephalography Data for Seizure Detection | http://arxiv.org/abs/1708.08430 | id:1708.08430 author:JT Turner, Adam Page, Tinoosh Mohsenin, Tim Oates category:cs.CV cs.AI  published:2017-08-28 summary:Ubiquitous bio-sensing for personalized health monitoring is slowly becoming a reality with the increasing availability of small, diverse, robust, high fidelity sensors. This oncoming flood of data begs the question of how we will extract useful information from it. In this paper we explore the use of a variety of representations and machine learning algorithms applied to the task of seizure detection in high resolution, multichannel EEG data. We explore classification accuracy, computational complexity and memory requirements with a view toward understanding which approaches are most suitable for such tasks as the number of people involved and the amount of data they produce grows to be quite large. In particular, we show that layered learning approaches such as Deep Belief Networks excel along these dimensions. version:1
arxiv-1708-08416 | Real-Time Area Coverage and Target Localization using Receding-Horizon Ergodic Exploration | http://arxiv.org/abs/1708.08416 | id:1708.08416 author:Anastasia Mavrommati, Emmanouil Tzorakoleftherakis, Ian Abraham, Todd D. Murphey category:cs.RO  published:2017-08-28 summary:Although a number of solutions exist for the problems of coverage, search and target localization---commonly addressed separately---whether there exists a unified strategy that addresses these objectives in a coherent manner without being application-specific remains a largely open research question. In this paper, we develop a receding-horizon ergodic control approach, based on hybrid systems theory, that has the potential to fill this gap. The nonlinear model predictive control algorithm plans real-time motions that optimally improve ergodicity with respect to a distribution defined by the expected information density across the sensing domain. We establish a theoretical framework for global stability guarantees with respect to a distribution. Moreover, the approach is distributable across multiple agents, so that each agent can independently compute its own control while sharing statistics of its coverage across a communication network. We demonstrate the method in both simulation and in experiment in the context of target localization, illustrating that the algorithm is independent of the number of targets being tracked and can be run in real-time on computationally limited hardware platforms. version:1
arxiv-1708-08399 | Towards a More Reliable and Available Docker-based Container Cloud | http://arxiv.org/abs/1708.08399 | id:1708.08399 author:Mudit Verma, Mohan Dhawan category:cs.DC  published:2017-08-28 summary:Operating System-level virtualization technology, or containers as they are commonly known, represents the next generation of light-weight virtualization, and is primarily represented by Docker. However, Docker's current design does not complement the SLAs from Docker-based container cloud offerings promising both reliability and high availability. The tight coupling between the containers and the Docker daemon proves fatal for the containers' uptime during daemon's unavailability due to either failure or upgrade. We present the design and implementation of HYDRA, which fundamentally isolates the containers from the running daemon. Our evaluation shows that HYDRA imposes only moderate overheads even under load, while achieving much higher container availability. version:1
arxiv-1708-02918 | The Tensor Memory Hypothesis | http://arxiv.org/abs/1708.02918 | id:1708.02918 author:Volker Tresp, Yunpu Ma category:cs.AI cs.LG q-bio.NC stat.ML  published:2017-08-09 summary:We discuss memory models which are based on tensor decompositions using latent representations of entities and events. We show how episodic memory and semantic memory can be realized and discuss how new memory traces can be generated from sensory input: Existing memories are the basis for perception and new memories are generated via perception. We relate our mathematical approach to the hippocampal memory indexing theory. We describe the first detailed mathematical models for the complete processing pipeline from sensory input and its semantic decoding, i.e., perception, to the formation of episodic and semantic memories and their declarative semantic decodings. Our main hypothesis is that perception includes an active semantic decoding process, which relies on latent representations of entities and predicates, and that episodic and semantic memories depend on the same decoding process. We contribute to the debate between the leading memory consolidation theories, i.e., the standard consolidation theory (SCT) and the multiple trace theory (MTT). The latter is closely related to the complementary learning systems (CLS) framework. In particular, we show explicitly how episodic memory can teach the neocortex to form a semantic memory, which is a core issue in MTT and CLS. version:2
arxiv-1708-08309 | A Dual Digraph Approach for Leaderless Atomic Broadcast (Extended Version) | http://arxiv.org/abs/1708.08309 | id:1708.08309 author:Marius Poke, Colin W. Glass category:cs.DC  published:2017-08-28 summary:Many distributed systems work on a common shared state; in such systems, distributed agreement is necessary for consistency. With increasing number of servers, systems become more susceptible to single-server failures, increasing the relevance of fault-tolerance. Atomic broadcast enables fault-tolerant distributed agreement, yet it is costly to solve. Current leader-based solutions result in quadratic work complexity on the leader. AllConcur -- a leaderless approach -- improves the work complexity by connecting the servers via a resilient overlay network; yet, this resiliency entails redundancy, which reduces performance. In this work, we propose AllConcur+, an extension of AllConcur. During intervals with no failures, it uses an overlay network with no redundancy and automatically switches to a resilient overlay network when failures occur. Our performance estimation shows that if no failures occur, AllConcur+ achieves up to 10x higher throughput and up to 5x lower latency than AllConcur. In the presence of occasional failures, AllConcur+ still outperforms AllConcur significantly. In the worst case, AllConcur+'s performance is worse than AllConcur's, yet, this requires frequent failures at very specific intervals. Thus, for realistic use cases, leveraging redundancy-free distributed agreement, during intervals with no failures, increases the expected performance. version:1
arxiv-1708-08299 | The Convergence of Machine Learning and Communications | http://arxiv.org/abs/1708.08299 | id:1708.08299 author:Wojciech Samek, Slawomir Stanczak, Thomas Wiegand category:cs.NI cs.AI cs.CY  published:2017-08-28 summary:The areas of machine learning and communication technology are converging. Today's communications systems generate a huge amount of traffic data, which can help to significantly enhance the design and management of networks and communication components when combined with advanced machine learning methods. Furthermore, recently developed end-to-end training procedures offer new ways to jointly optimize the components of a communication system. Also in many emerging application fields of communication technology, e.g., smart cities or internet of things, machine learning methods are of central importance. This paper gives an overview over the use of machine learning in different areas of communications and discusses two exemplar applications in wireless networking. Furthermore, it identifies promising future research topics and discusses their potential impact. version:1
arxiv-1708-06975 | Generating Visual Representations for Zero-Shot Classification | http://arxiv.org/abs/1708.06975 | id:1708.06975 author:Maxime Bucher, Stéphane Herbin, Frédéric Jurie category:cs.CV cs.AI cs.LG  published:2017-08-23 summary:This paper addresses the task of learning an image clas-sifier when some categories are defined by semantic descriptions only (e.g. visual attributes) while the others are defined by exemplar images as well. This task is often referred to as the Zero-Shot classification task (ZSC). Most of the previous methods rely on learning a common embedding space allowing to compare visual features of unknown categories with semantic descriptions. This paper argues that these approaches are limited as i) efficient discrimi-native classifiers can't be used ii) classification tasks with seen and unseen categories (Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently. In contrast, this paper suggests to address ZSC and GZSC by i) learning a conditional generator using seen classes ii) generate artificial training examples for the categories without exemplars. ZSC is then turned into a standard supervised learning problem. Experiments with 4 generative models and 5 datasets experimentally validate the approach, giving state-of-the-art results on both ZSC and GZSC. version:2
arxiv-1708-08296 | Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models | http://arxiv.org/abs/1708.08296 | id:1708.08296 author:Wojciech Samek, Thomas Wiegand, Klaus-Robert Müller category:cs.AI cs.CY cs.NE stat.ML  published:2017-08-28 summary:With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks. version:1
arxiv-1708-08291 | On Type-Aware Entity Retrieval | http://arxiv.org/abs/1708.08291 | id:1708.08291 author:Darío Garigliotti, Krisztian Balog category:cs.IR cs.AI cs.CL H.3.3  published:2017-08-28 summary:Today, the practice of returning entities from a knowledge base in response to search queries has become widespread. One of the distinctive characteristics of entities is that they are typed, i.e., assigned to some hierarchically organized type system (type taxonomy). The primary objective of this paper is to gain a better understanding of how entity type information can be utilized in entity retrieval. We perform this investigation in an idealized "oracle" setting, assuming that we know the distribution of target types of the relevant entities for a given query. We perform a thorough analysis of three main aspects: (i) the choice of type taxonomy, (ii) the representation of hierarchical type information, and (iii) the combination of type-based and term-based similarity in the retrieval model. Using a standard entity search test collection based on DBpedia, we find that type information proves most useful when using large type taxonomies that provide very specific types. We provide further insights on the extensional coverage of entities and on the utility of target types. version:1
arxiv-1708-08289 | Generating Query Suggestions to Support Task-Based Search | http://arxiv.org/abs/1708.08289 | id:1708.08289 author:Darío Garigliotti, Krisztian Balog category:cs.IR cs.AI cs.CL H.3.3  published:2017-08-28 summary:We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the first place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model. version:1
arxiv-1708-08286 | A Scalable and Extensible Checkpointing Scheme for Massively Parallel Simulations | http://arxiv.org/abs/1708.08286 | id:1708.08286 author:Nils Kohl, Johannes Hötzer, Florian Schornbaum, Martin Bauer, Christian Godenschwager, Harald Köstler, Britta Nestler, Ulrich Rüde category:cs.DC  published:2017-08-28 summary:Realistic simulations in engineering or in the materials sciences can consume enormous computing resources and thus require the use of massively parallel supercomputers. The probability of a failure increases both with the runtime and with the number of system components. For future exascale systems it is therefore considered critical that strategies are developed to make software resilient against failures. In this article, we present a scalable, distributed, diskless, and resilient checkpointing scheme that can create and recover snapshots of a partitioned simulation domain. We demonstrate the efficiency and scalability of the checkpoint strategy for simulations with up to 40 billion computational cells executing on more than 400 billion floating point values. A checkpoint creation is shown to require only a few seconds and the new checkpointing scheme scales almost perfectly up to more than 30 000 ($2^{15}$) processes. To recover from a diskless checkpoint during runtime, we realize the recovery algorithms using ULFM MPI. The checkpointing mechanism is fully integrated in a state-of-the-art high-performance multi-physics simulation framework. We demonstrate the efficiency and robustness of the method with a realistic phase-field simulation originating in the material sciences. version:1
arxiv-1708-08255 | Cops and robber on grids and tori | http://arxiv.org/abs/1708.08255 | id:1708.08255 author:Fabrizio Luccio, Linda Pagli category:cs.DC cs.DM F.2.2; G.2.2  published:2017-08-28 summary:This paper is a contribution to the classical cops and robber problem on a graph, directed to two-dimensional grids and toroidal grids. These studies are generally aimed at determining the minimum number of cops needed to capture the robber and proposing algorithms for the capture. We apply some new concepts to propose a new solution to the problem on grids that was already solved under a different approach, and apply these concepts to give efficient algorithms for the capture on toroidal grids. As for grids, we show that two cops suffice even in a semi-torus (i.e. a grid with toroidal closure in one dimension) and three cops are necessary and sufficient in a torus. Then we treat the problem in function of any number k of cops, giving efficient algorithms for grids and tori and computing lower and upper bounds on the capture time. Conversely we determine the minimum value of k needed for any given capture time and study a possible speed-up phenomenon. version:1
arxiv-1708-07732 | Multi-Agent Q-Learning for Minimizing Demand-Supply Power Deficit in Microgrids | http://arxiv.org/abs/1708.07732 | id:1708.07732 author:Raghuram Bharadwaj Diddigi, D. Sai Koti Reddy, Shalabh Bhatnagar category:cs.SY cs.AI  published:2017-08-25 summary:We consider the problem of minimizing the difference in the demand and the supply of power using microgrids. We setup multiple microgrids, that provide electricity to a village. They have access to the batteries that can store renewable power and also the electrical lines from the main grid. During each time period, these microgrids need to take decision on the amount of renewable power to be used from the batteries as well as the amount of power needed from the main grid. We formulate this problem in the framework of Markov Decision Process (MDP), similar to the one discussed in [1]. The power allotment to the village from the main grid is fixed and bounded, whereas the renewable energy generation is uncertain in nature. Therefore we adapt a distributed version of the popular Reinforcement learning technique, Multi-Agent Q-Learning to the problem. Finally, we also consider a variant of this problem where the cost of power production at the main site is taken into consideration. In this scenario the microgrids need to minimize the demand-supply deficit, while maintaining the desired average cost of the power production. version:2
arxiv-1708-08155 | ByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning | http://arxiv.org/abs/1708.08155 | id:1708.08155 author:Zhixiong Yang, Waheed U. Bajwa category:cs.LG cs.DC math.OC stat.ML  published:2017-08-28 summary:Distributed machine learning algorithms enable processing of datasets that are distributed over a network without gathering the data at a centralized location. While efficient distributed algorithms have been developed under the assumption of faultless networks, failures that can render these algorithms nonfunctional indeed happen in the real world. This paper focuses on the problem of Byzantine failures, which are the hardest to safeguard against in distributed algorithms. While Byzantine fault tolerance has a rich history, existing work does not translate into efficient and practical algorithms for high-dimensional distributed learning tasks. In this paper, two variants of an algorithm termed Byzantine-resilient distributed coordinate descent (ByRDiE) are developed and analyzed that solve distributed learning problems in the presence of Byzantine failures. Theoretical analysis as well as numerical experiments presented in the paper highlight the usefulness of ByRDiE for high-dimensional distributed learning in the presence of Byzantine failures. version:1
arxiv-1708-08150 | Inclined Surface Locomotion Strategies for Spherical Tensegrity Robots | http://arxiv.org/abs/1708.08150 | id:1708.08150 author:Lee-Huang Chen, Brian Cera, Edward L. Zhu, Riley Edmunds, Franklin Rice, Antonia Bronars, Ellande Tang, Saunon R. Malekshahi, Osvaldo Romero, Adrian K. Agogino, Alice M. Agogino category:cs.RO  published:2017-08-27 summary:This paper presents a new teleoperated spherical tensegrity robot capable of performing locomotion on steep inclined surfaces. With a novel control scheme centered around the simultaneous actuation of multiple cables, the robot demonstrates robust climbing on inclined surfaces in hardware experiments and speeds significantly faster than previous spherical tensegrity models. This robot is an improvement over other iterations in the TT-series and the first tensegrity to achieve reliable locomotion on inclined surfaces of up to 24\degree. We analyze locomotion in simulation and hardware under single and multi-cable actuation, and introduce two novel multi-cable actuation policies, suited for steep incline climbing and speed, respectively. We propose compelling justifications for the increased dynamic ability of the robot and motivate development of optimization algorithms able to take advantage of the robot's increased control authority. version:1
arxiv-1708-08133 | Methods for applying the Neural Engineering Framework to neuromorphic hardware | http://arxiv.org/abs/1708.08133 | id:1708.08133 author:Aaron R. Voelker, Chris Eliasmith category:q-bio.NC cs.AI cs.SY math.DS  published:2017-08-27 summary:We review our current software tools and theoretical methods for applying the Neural Engineering Framework to state-of-the-art neuromorphic hardware. These methods can be used to implement linear and nonlinear dynamical systems that exploit axonal transmission time-delays, and to fully account for nonideal mixed-analog-digital synapses that exhibit higher-order dynamics with heterogeneous time-constants. This summarizes earlier versions of these methods that have been discussed in a more biological context (Voelker & Eliasmith, 2017) or regarding a specific neuromorphic architecture (Voelker et al., 2017). version:1
arxiv-1708-08127 | RIOT: a Novel Stochastic Method for Rapidly Configuring Cloud-Based Workflows | http://arxiv.org/abs/1708.08127 | id:1708.08127 author:Jianfeng Chen, Tim Menzies category:cs.SE cs.AI cs.NE  published:2017-08-27 summary:Traditional tools for configuring cloud services can run much slower than the workflows they are trying to optimize. For example, in the case studies reported here, we find cases where (using traditional methods) it takes hours to find ways to make a workflow terminate in tens of seconds. Such slow optimizers are a poor choice of tools for reacting to changing operational environmental conditions. Hence, they are unsuited for cloud services that support rapidly changing workflows, e.g., scientific workflows or workflows from the media or telecommunication industries. To solve this problem, this paper presents RIOT (Randomized Instance Order Types), a new configuration tool. RIOT has a very low optimization overhead-- often, less than 10\% of the system runtime, especially for every complex workflow. Instead of simulating many configurations, RIOT uses a novel surrogate sampling method to quickly find promising solutions. As shown by this paper, RIOT achieves comparable results to the other approaches but does so in a fraction of the time. version:1
arxiv-1708-08113 | Novel Sensor Scheduling Scheme for Intruder Tracking in Energy Efficient Sensor Networks | http://arxiv.org/abs/1708.08113 | id:1708.08113 author:Raghuram Bharadwaj Diddigi, Prabuchandran K. J., Shalabh Bhatnagar category:cs.AI cs.SY  published:2017-08-27 summary:We consider the problem of tracking an intruder using a network of wireless sensors. For tracking the intruder at each instant, the optimal number and the right configuration of sensors has to be powered. As powering the sensors consumes energy, there is a trade off between accurately tracking the position of the intruder at each instant and the energy consumption of sensors. This problem has been formulated in the framework of Partially Observable Markov Decision Process (POMDP). Even for the simplest model considered in [1], the curse of dimensionality renders the problem intractable. We formulate this problem with a suitable state-action space in the framework of POMDP and develop a reinforcement learning algorithm utilising the Upper Confidence Tree Search (UCT) method to mitigate the state-action space explosion. Through simulations, we illustrate that our algorithm scales well with the increasing state and action space. version:1
arxiv-1708-08079 | Local Gaussian Processes for Efficient Fine-Grained Traffic Speed Prediction | http://arxiv.org/abs/1708.08079 | id:1708.08079 author:Truc Viet Le, Richard J. Oentaryo, Siyuan Liu, Hoong Chuin Lau category:cs.AI cs.LG  published:2017-08-27 summary:Traffic speed is a key indicator for the efficiency of an urban transportation system. Accurate modeling of the spatiotemporally varying traffic speed thus plays a crucial role in urban planning and development. This paper addresses the problem of efficient fine-grained traffic speed prediction using big traffic data obtained from static sensors. Gaussian processes (GPs) have been previously used to model various traffic phenomena, including flow and speed. However, GPs do not scale with big traffic data due to their cubic time complexity. In this work, we address their efficiency issues by proposing local GPs to learn from and make predictions for correlated subsets of data. The main idea is to quickly group speed variables in both spatial and temporal dimensions into a finite number of clusters, so that future and unobserved traffic speed queries can be heuristically mapped to one of such clusters. A local GP corresponding to that cluster can then be trained on the fly to make predictions in real-time. We call this method localization. We use non-negative matrix factorization for localization and propose simple heuristics for cluster mapping. We additionally leverage on the expressiveness of GP kernel functions to model road network topology and incorporate side information. Extensive experiments using real-world traffic data collected in the two U.S. cities of Pittsburgh and Washington, D.C., show that our proposed local GPs significantly improve both runtime performances and prediction accuracies compared to the baseline global and local GPs. version:1
arxiv-1708-06127 | Practical Minimum Cut Algorithms | http://arxiv.org/abs/1708.06127 | id:1708.06127 author:Monika Henzinger, Alexander Noe, Christian Schulz, Darren Strash category:cs.DS cs.DC  published:2017-08-21 summary:The minimum cut problem for an undirected edge-weighted graph asks us to divide its set of nodes into two blocks while minimizing the weight sum of the cut edges. Here, we introduce a linear-time algorithm to compute near-minimum cuts. Our algorithm is based on cluster contraction using label propagation and Padberg and Rinaldi's contraction heuristics [SIAM Review, 1991]. We give both sequential and shared-memory parallel implementations of our algorithm. Extensive experiments on both real-world and generated instances show that our algorithm finds the optimal cut on nearly all instances significantly faster than other state-of-the-art algorithms while our error rate is lower than that of other heuristic algorithms. In addition, our parallel algorithm shows good scalability. version:2
arxiv-1708-08028 | Status of Serverless Computing and Function-as-a-Service(FaaS) in Industry and Research | http://arxiv.org/abs/1708.08028 | id:1708.08028 author:Geoffrey C. Fox, Vatche Ishakian, Vinod Muthusamy, Aleksander Slominski category:cs.DC  published:2017-08-27 summary:This whitepaper summarizes issues raised during the First International Workshop on Serverless Computing (WoSC) 2017 held June 5th 2017 and especially in the panel and associated discussion that concluded the workshop. We also include comments from the keynote and submitted papers. A glossary at the end (section 8) defines many technical terms used in this report. version:1
arxiv-1709-01142 | Implementation and Evaluation of a Framework to calculate Impact Measures for Wikipedia Authors | http://arxiv.org/abs/1709.01142 | id:1709.01142 author:Sebastian Neef category:cs.DL cs.DB cs.DC cs.SI  published:2017-08-26 summary:Wikipedia, an open collaborative website, can be edited by anyone, even anonymously, thus becoming victim to ill-intentioned changes. Therefore, ranking Wikipedia authors by calculating impact measures based on the edit history can help to identify reputational users or harmful activity such as vandalism \cite{Adler:2008:MAC:1822258.1822279}. However, processing millions of edits on one system can take a long time. The author implements an open source framework to calculate such rankings in a distributed way (MapReduce) and evaluates its performance on various sized datasets. A reimplementation of the contribution measures by \citeauthor{Adler:2008:MAC:1822258.1822279} demonstrates its extensibility and usability, as well as problems of handling huge datasets and their possible resolutions. The results put different performance optimizations into perspective and show that horizontal scaling can decrease the total processing time. version:1
arxiv-1708-03184 | Energy-efficient Analytics for Geographically Distributed Big Data | http://arxiv.org/abs/1708.03184 | id:1708.03184 author:Peng Zhao, Shusen Yang, Xinyu Yang, Wei Yu, Jie Lin category:cs.DC  published:2017-08-10 summary:Big data analytics on geographically distributed datasets (across data centers or clusters) has been attracting increasing interests from both academia and industry, but also significantly complicates the system and algorithm designs. In this article, we systematically investigate the geo-distributed big-data analytics framework by analyzing the fine-grained paradigm and the key design principles. We present a dynamic global manager selection algorithm (GMSA) to minimize energy consumption cost by fully exploiting the system diversities in geography and variation over time. The algorithm makes real-time decisions based on the measurable system parameters through stochastic optimization methods, while achieving the performance balances between energy cost and latency. Extensive trace-driven simulations verify the effectiveness and efficiency of the proposed algorithm. We also highlight several potential research directions that remain open and require future elaborations in analyzing geo-distributed big data. version:2
arxiv-1708-07969 | 3D Object Reconstruction from a Single Depth View with Adversarial Learning | http://arxiv.org/abs/1708.07969 | id:1708.07969 author:Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew Markham, Niki Trigoni category:cs.CV cs.AI cs.LG cs.RO  published:2017-08-26 summary:In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the complete 3D structure of a given object from a single arbitrary depth view using generative adversarial networks. Unlike the existing work which typically requires multiple views of the same object or class labels to recover the full 3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of a depth view of the object as input, and is able to generate the complete 3D occupancy grid by filling in the occluded/missing regions. The key idea is to combine the generative capabilities of autoencoders and the conditional Generative Adversarial Networks (GAN) framework, to infer accurate and fine-grained 3D structures of objects in high-dimensional voxel space. Extensive experiments on large synthetic datasets show that the proposed 3D-RecGAN significantly outperforms the state of the art in single view 3D object reconstruction, and is able to reconstruct unseen types of objects. Our code and data are available at: https://github.com/Yang7879/3D-RecGAN. version:1
arxiv-1708-07941 | LERC: Coordinated Cache Management for Data-Parallel Systems | http://arxiv.org/abs/1708.07941 | id:1708.07941 author:Yinghao Yu, Wei Wang, Jun Zhang, Khaled B. Letaief category:cs.DC  published:2017-08-26 summary:Memory caches are being aggressively used in today's data-parallel frameworks such as Spark, Tez and Storm. By caching input and intermediate data in memory, compute tasks can witness speedup by orders of magnitude. To maximize the chance of in-memory data access, existing cache algorithms, be it recency- or frequency-based, settle on cache hit ratio as the optimization objective. However, unlike the conventional belief, we show in this paper that simply pursuing a higher cache hit ratio of individual data blocks does not necessarily translate into faster task completion in data-parallel environments. A data-parallel task typically depends on multiple input data blocks. Unless all of these blocks are cached in memory, no speedup will result. To capture this all-or-nothing property, we propose a more relevant metric, called effective cache hit ratio. Specifically, a cache hit of a data block is said to be effective if it can speed up a compute task. In order to optimize the effective cache hit ratio, we propose the Least Effective Reference Count (LERC) policy that persists the dependent blocks of a compute task as a whole in memory. We have implemented the LERC policy as a memory manager in Spark and evaluated its performance through Amazon EC2 deployment. Evaluation results demonstrate that LERC helps speed up data-parallel jobs by up to 37% compared with the widely employed least-recently-used (LRU) policy. version:1
arxiv-1708-07940 | Navigation Objects Extraction for Better Content Structure Understanding | http://arxiv.org/abs/1708.07940 | id:1708.07940 author:Kui Zhao, Bangpeng Li, Zilun Peng, Jiajun Bu, Can Wang category:cs.AI cs.IR  published:2017-08-26 summary:Existing works for extracting navigation objects from webpages focus on navigation menus, so as to reveal the information architecture of the site. However, web 2.0 sites such as social networks, e-commerce portals etc. are making the understanding of the content structure in a web site increasingly difficult. Dynamic and personalized elements such as top stories, recommended list in a webpage are vital to the understanding of the dynamic nature of web 2.0 sites. To better understand the content structure in web 2.0 sites, in this paper we propose a new extraction method for navigation objects in a webpage. Our method will extract not only the static navigation menus, but also the dynamic and personalized page-specific navigation lists. Since the navigation objects in a webpage naturally come in blocks, we first cluster hyperlinks into different blocks by exploiting spatial locations of hyperlinks, the hierarchical structure of the DOM-tree and the hyperlink density. Then we identify navigation objects from those blocks using the SVM classifier with novel features such as anchor text lengths etc. Experiments on real-world data sets with webpages from various domains and styles verified the effectiveness of our method. version:1
arxiv-1708-07938 | Deep Style Match for Complementary Recommendation | http://arxiv.org/abs/1708.07938 | id:1708.07938 author:Kui Zhao, Xia Hu, Jiajun Bu, Can Wang category:cs.AI  published:2017-08-26 summary:Humans develop a common sense of style compatibility between items based on their attributes. We seek to automatically answer questions like "Does this shirt go well with that pair of jeans?" In order to answer these kinds of questions, we attempt to model human sense of style compatibility in this paper. The basic assumption of our approach is that most of the important attributes for a product in an online store are included in its title description. Therefore it is feasible to learn style compatibility from these descriptions. We design a Siamese Convolutional Neural Network architecture and feed it with title pairs of items, which are either compatible or incompatible. Those pairs will be mapped from the original space of symbolic words into some embedded style space. Our approach takes only words as the input with few preprocessing and there is no laborious and expensive feature engineering. version:1
arxiv-1709-09575 | An Assessment of Data Transfer Performance for Large-Scale Climate Data Analysis and Recommendations for the Data Infrastructure for CMIP6 | http://arxiv.org/abs/1709.09575 | id:1709.09575 author:Eli Dart, Michael F. Wehner, Prabhat category:cs.DC cs.DB physics.ao-ph  published:2017-08-26 summary:We document the data transfer workflow, data transfer performance, and other aspects of staging approximately 56 terabytes of climate model output data from the distributed Coupled Model Intercomparison Project (CMIP5) archive to the National Energy Research Supercomputing Center (NERSC) at the Lawrence Berkeley National Laboratory required for tracking and characterizing extratropical storms, a phenomena of importance in the mid-latitudes. We present this analysis to illustrate the current challenges in assembling multi-model data sets at major computing facilities for large-scale studies of CMIP5 data. Because of the larger archive size of the upcoming CMIP6 phase of model intercomparison, we expect such data transfers to become of increasing importance, and perhaps of routine necessity. We find that data transfer rates using the ESGF are often slower than what is typically available to US residences and that there is significant room for improvement in the data transfer capabilities of the ESGF portal and data centers both in terms of workflow mechanics and in data transfer performance. We believe performance improvements of at least an order of magnitude are within technical reach using current best practices, as illustrated by the performance we achieved in transferring the complete raw data set between two high performance computing facilities. To achieve these performance improvements, we recommend: that current best practices (such as the Science DMZ model) be applied to the data servers and networks at ESGF data centers; that sufficient financial and human resources be devoted at the ESGF data centers for systems and network engineering tasks to support high performance data movement; and that performance metrics for data transfer between ESGF data centers and major computing facilities used for climate data analysis be established, regularly tested, and published. version:1
arxiv-1708-07918 | Robust Task Clustering for Deep Many-Task Learning | http://arxiv.org/abs/1708.07918 | id:1708.07918 author:Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Gerald Tesauro, Haoyu Wang, Bowen Zhou category:cs.LG cs.AI cs.CL stat.ML  published:2017-08-26 summary:We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. To overcome these limitations, we propose a novel task-clustering algorithm by using the matrix completion technique. The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs. We then use a matrix completion algorithm to complete the similarity matrix. Our theoretical analysis shows that under mild constraints, the proposed algorithm will perfectly recover the underlying "true" similarity matrix with a high probability. Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks. Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when the tasks are diverse. version:1
arxiv-1612-05665 | PAM: Parallel Augmented Maps | http://arxiv.org/abs/1612.05665 | id:1612.05665 author:Yihan Sun, Daniel Ferizovic, Guy E. Blelloch category:cs.DS cs.DB cs.DC  published:2016-12-16 summary:Ordered (key-value) maps are an important and widely-used data structure for large-scale data processing frameworks. Beyond simple search, insertion and deletion, more advanced operations such as range extraction, filtering, and bulk updates form a critical part of these frameworks. We describe an interface for ordered maps that is augmented to support fast range queries and sums, and introduce a parallel and concurrent library called PAM (Parallel Augmented Maps) that implements the interface. The interface includes a wide variety of functions on augmented maps ranging from basic insertion and deletion to more interesting functions such as union, intersection, filtering, extracting ranges, splitting, and range-sums. We describe algorithms for these functions that are efficient both in theory and practice. As examples of the use of the interface and the performance of PAM we apply the library to four applications: simple range sums, interval trees, 2D range trees, and word index searching. The interface greatly simplifies the implementation of these data structures over direct implementations. Sequentially the code achieves performance that matches or exceeds exiting libraries designed specially for a single application, and in parallel our implementation gets speedups ranging from 40 to 90 on 72 cores with 2-way hyperthreading. version:2
arxiv-1708-01289 | Independently Controllable Factors | http://arxiv.org/abs/1708.01289 | id:1708.01289 author:Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, Yoshua Bengio category:cs.LG cs.AI stat.ML  published:2017-08-03 summary:It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal. version:2
arxiv-1708-07902 | Deep Learning for Video Game Playing | http://arxiv.org/abs/1708.07902 | id:1708.07902 author:Niels Justesen, Philip Bontrager, Julian Togelius, Sebastian Risi category:cs.AI  published:2017-08-25 summary:In this paper we review recent Deep Learning advances in the context of how they have been applied to play different types of video games such as first-person shooters, arcade games or real-time strategy games. We analyze the unique requirements that different game genres pose to a deep learning system and highlight important open challenges in the context of applying these machine learning methods to video games, such as general game playing, dealing with extremely large decision spaces and sparse rewards. version:1
arxiv-1708-07883 | Streaming Graph Challenge: Stochastic Block Partition | http://arxiv.org/abs/1708.07883 | id:1708.07883 author:Edward Kao, Vijay Gadepally, Michael Hurley, Michael Jones, Jeremy Kepner, Sanjeev Mohindra, Paul Monticciolo, Albert Reuther, Siddharth Samsi, William Song, Diane Staheli, Steven Smith category:cs.DC cs.DS cs.PF cs.SI  published:2017-08-25 summary:An important objective for analyzing real-world graphs is to achieve scalable performance on large, streaming graphs. A challenging and relevant example is the graph partition problem. As a combinatorial problem, graph partition is NP-hard, but existing relaxation methods provide reasonable approximate solutions that can be scaled for large graphs. Competitive benchmarks and challenges have proven to be an effective means to advance state-of-the-art performance and foster community collaboration. This paper describes a graph partition challenge with a baseline partition algorithm of sub-quadratic complexity. The algorithm employs rigorous Bayesian inferential methods based on a statistical model that captures characteristics of the real-world graphs. This strong foundation enables the algorithm to address limitations of well-known graph partition approaches such as modularity maximization. This paper describes various aspects of the challenge including: (1) the data sets and streaming graph generator, (2) the baseline partition algorithm with pseudocode, (3) an argument for the correctness of parallelizing the Bayesian inference, (4) different parallel computation strategies such as node-based parallelism and matrix-based parallelism, (5) evaluation metrics for partition correctness and computational requirements, (6) preliminary timing of a Python-based demonstration code and the open source C++ code, and (7) considerations for partitioning the graph in streaming fashion. Data sets and source code for the algorithm as well as metrics, with detailed documentation are available at GraphChallenge.org. version:1
arxiv-1708-07867 | Accelerating Dependency Graph Learning from Heterogeneous Categorical Event Streams via Knowledge Transfer | http://arxiv.org/abs/1708.07867 | id:1708.07867 author:Chen Luo, Zhengzhang Chen, Lu-An Tang, Anshumali Shrivastava, Zhichun Li category:cs.AI  published:2017-08-25 summary:Dependency graph, as a heterogeneous graph representing the intrinsic relationships between different pairs of system entities, is essential to many data analysis applications, such as root cause diagnosis, intrusion detection, etc. Given a well-trained dependency graph from a source domain and an immature dependency graph from a target domain, how can we extract the entity and dependency knowledge from the source to enhance the target? One way is to directly apply a mature dependency graph learned from a source domain to the target domain. But due to the domain variety problem, directly using the source dependency graph often can not achieve good performance. Traditional transfer learning methods mainly focus on numerical data and are not applicable. In this paper, we propose ACRET, a knowledge transfer based model for accelerating dependency graph learning from heterogeneous categorical event streams. In particular, we first propose an entity estimation model to filter out irrelevant entities from the source domain based on entity embedding and manifold learning. Only the entities with statistically high correlations are transferred to the target domain. On the surviving entities, we propose a dependency construction model for constructing the unbiased dependency relationships by solving a two-constraint optimization problem. The experimental results on synthetic and real-world datasets demonstrate the effectiveness and efficiency of ACRET. We also apply ACRET to a real enterprise security system for intrusion detection. Our method is able to achieve superior detection performance at least 20 days lead lag time in advance with more than 70% accuracy. version:1
arxiv-1708-07863 | $k$-Nearest Neighbor Augmented Neural Networks for Text Classification | http://arxiv.org/abs/1708.07863 | id:1708.07863 author:Zhiguo Wang, Wael Hamza, Linfeng Song category:cs.CL cs.AI  published:2017-08-25 summary:In recent years, many deep-learning based models are proposed for text classification. This kind of models well fits the training set from the statistical point of view. However, it lacks the capacity of utilizing instance-level information from individual instances in the training set. In this work, we propose to enhance neural network models by allowing them to leverage information from $k$-nearest neighbor (kNN) of the input text. Our model employs a neural network that encodes texts into text embeddings. Moreover, we also utilize $k$-nearest neighbor of the input text as an external memory, and utilize it to capture instance-level information from the training set. The final prediction is made based on features from both the neural network encoder and the kNN memory. Experimental results on several standard benchmark datasets show that our model outperforms the baseline model on all the datasets, and it even beats a very deep neural network model (with 29 layers) in several datasets. Our model also shows superior performance when training instances are scarce, and when the training set is severely unbalanced. Our model also leverages techniques such as semi-supervised training and transfer learning quite well. version:1
arxiv-1708-07775 | Subspace Approximation for Approximate Nearest Neighbor Search in NLP | http://arxiv.org/abs/1708.07775 | id:1708.07775 author:Jing Wang category:cs.AI  published:2017-08-25 summary:Most natural language processing tasks can be formulated as the approximated nearest neighbor search problem, such as word analogy, document similarity, machine translation. Take the question-answering task as an example, given a question as the query, the goal is to search its nearest neighbor in the training dataset as the answer. However, existing methods for approximate nearest neighbor search problem may not perform well owing to the following practical challenges: 1) there are noise in the data; 2) the large scale dataset yields a huge retrieval space and high search time complexity. In order to solve these problems, we propose a novel approximate nearest neighbor search framework which i) projects the data to a subspace based spectral analysis which eliminates the influence of noise; ii) partitions the training dataset to different groups in order to reduce the search space. Specifically, the retrieval space is reduced from $O(n)$ to $O(\log n)$ (where $n$ is the number of data points in the training dataset). We prove that the retrieved nearest neighbor in the projected subspace is the same as the one in the original feature space. We demonstrate the outstanding performance of our framework on real-world natural language processing tasks. version:1
arxiv-1708-07767 | Non-FPT lower bounds for structural restrictions of decision DNNF | http://arxiv.org/abs/1708.07767 | id:1708.07767 author:Andrea Calì, Florent Capelli, Igor Razgon category:cs.AI cs.CC  published:2017-08-25 summary:We give a non-FPT lower bound on the size of structured decision DNNF and OBDD with decomposable AND-nodes representing CNF-formulas of bounded incidence treewidth. Both models are known to be of FPT size for CNFs of bounded primal treewidth. To the best of our knowledge this is the first parameterized separation of primal treewidth and incidence treewidth for knowledge compilation models. version:1
arxiv-1708-07689 | Understanding and Comparing Deep Neural Networks for Age and Gender Classification | http://arxiv.org/abs/1708.07689 | id:1708.07689 author:Sebastian Lapuschkin, Alexander Binder, Klaus-Robert Müller, Wojciech Samek category:stat.ML cs.AI cs.CV cs.IR cs.LG 68  published:2017-08-25 summary:Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects. In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition. version:1
arxiv-1708-07677 | An Experimental Microarchitecture for a Superconducting Quantum Processor | http://arxiv.org/abs/1708.07677 | id:1708.07677 author:X. Fu, M. A. Rol, C. C. Bultink, J. van Someren, N. Khammassi, I. Ashraf, R. F. L. Vermeulen, J. C. de Sterke, W. J. Vlothuizen, R. N. Schouten, C. G. Almudever, L. DiCarlo, K. Bertels category:quant-ph cs.AR cs.ET C.0; B.1.5; C.1.3  published:2017-08-25 summary:Quantum computers promise to solve certain problems that are intractable for classical computers, such as factoring large numbers and simulating quantum systems. To date, research in quantum computer engineering has focused primarily at opposite ends of the required system stack: devising high-level programming languages and compilers to describe and optimize quantum algorithms, and building reliable low-level quantum hardware. Relatively little attention has been given to using the compiler output to fully control the operations on experimental quantum processors. Bridging this gap, we propose and build a prototype of a flexible control microarchitecture supporting quantum-classical mixed code for a superconducting quantum processor. The microarchitecture is based on three core elements: (i) a codeword-based event control scheme, (ii) queue-based precise event timing control, and (iii) a flexible multilevel instruction decoding mechanism for control. We design a set of quantum microinstructions that allows flexible control of quantum operations with precise timing. We demonstrate the microarchitecture and microinstruction set by performing a standard gate-characterization experiment on a transmon qubit. version:1
arxiv-1708-07619 | Design of Adiabatic MTJ-CMOS Hybrid Circuits | http://arxiv.org/abs/1708.07619 | id:1708.07619 author:Fazel Sharifi, Z. M. Saifullah, Abdel-Hameed Badawy category:cs.ET cs.AR  published:2017-08-25 summary:Low-power designs are a necessity with the increasing demand of portable devices which are battery operated. In many of such devices the operational speed is not as important as battery life. Logic-in-memory structures using nano-devices and adiabatic designs are two methods to reduce the static and dynamic power consumption respectively. Magnetic tunnel junction (MTJ) is an emerging technology which has many advantages when used in logic-in-memory structures in conjunction with CMOS. In this paper, we introduce a novel adiabatic hybrid MTJ/CMOS structure which is used to design AND/NAND, XOR/XNOR and 1-bit full adder circuits. We simulate the designs using HSPICE with 32nm CMOS technology and compared it with a non-adiabatic hybrid MTJ/CMOS circuits. The proposed adiabatic MTJ/CMOS full adder design has more than 7 times lower power consumtion compared to the previous MTJ/CMOS full adder. version:1
arxiv-1708-07607 | Reinforcement Mechanism Design for e-commerce | http://arxiv.org/abs/1708.07607 | id:1708.07607 author:Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, Yiwei Zhang category:cs.MA cs.AI  published:2017-08-25 summary:We study the problem of allocating impressions to sellers in e-commerce websites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue generated by the platform. When a buyer searches for a keyword, the website presents the buyer with a list of different sellers for this item, together with the corresponding prices. This can be seen as an instance of a resource allocation problem in which the sellers choose their prices at each step and the platform decides how to allocate the impressions, based on the chosen prices and the historical transactions of each seller. Due to the complexity of the system, most e-commerce platforms employ heuristic allocation algorithms that mainly depend on the sellers' transaction records and without taking the rationality of the sellers into account, which makes them susceptible to price manipulations. In this paper, we put forward a general framework of reinforcement mechanism design, which uses deep reinforcement learning to design efficient algorithms, taking the strategic behaviour of the sellers into account. We apply the framework to the problem of allocating impressions to sellers in large e-commerce websites, a problem which is modeled as a Markov decision process, where the states encode the history of impressions, prices, transactions and generated revenue and the actions are the possible impression allocations at each round. To tackle the problem of continuity and high-dimensionality of states and actions, we adopt the ideas of the DDPG algorithm to design an actor-critic gradient policy algorithm which takes advantage of the problem domain in order to achieve convergence and stability. Our algorithm is compared against natural heuristics and it outperforms all of them in terms of the total revenue generated. Finally, contrary to the DDPG algorithm, our algorithm is robust to settings with variable sellers and easy to converge. version:1
arxiv-1708-07303 | Learning Grasping Interaction with Geometry-aware 3D Representations | http://arxiv.org/abs/1708.07303 | id:1708.07303 author:Xinchen Yan, Mohi Khansari, Yunfei Bai, Jasmine Hsu, Arkanath Pathak, Abhinav Gupta, James Davidson, Honglak Lee category:cs.RO cs.AI cs.CV cs.LG  published:2017-08-24 summary:Learning to interact with objects in the environment is a fundamental AI problem involving perception, motion planning, and control. However, learning representations of such interactions is very challenging due to a high dimensional state space, difficulty in collecting large-scale data, and many variations of an object's visual appearance (i.e. geometry, material, texture, and illumination). We argue that knowledge of 3D geometry is at the heart of grasping interactions and propose the notion of a geometry-aware learning agent. Our key idea is constraining and regularizing interaction learning through 3D geometry prediction. Specifically, we formulate the learning process of a geometry-aware agent as a two-step procedure: First, the agent learns to construct its geometry-aware representation of the scene from 2D sensory input via generative 3D shape modeling. Finally, it learns to predict grasping outcome with its built-in geometry-aware representation. The geometry-aware representation plays a key role in relating geometry and interaction via a novel learning-free depth projection layer. Our contributions are threefold: (1) we build a grasping dataset from demonstrations in virtual reality (VR) with rich sensory and interaction annotations; (2) we demonstrate that the learned geometry-aware representation results in a more robust grasping outcome prediction compared to a baseline model; and (3) we demonstrate the benefits of the learned geometry-aware representation in grasping planning. version:2
arxiv-1708-07580 | Achieving Proportional Representation via Voting | http://arxiv.org/abs/1708.07580 | id:1708.07580 author:Haris Aziz, Barton Lee category:cs.GT cs.AI 91A12  68Q15 F.2; J.4  published:2017-08-25 summary:Proportional representation (PR) is often discussed in voting settings as a major desideratum. For the past century or so, it is common both in practice and in the academic literature to jump to STV (Single Transferable Vote) as the solution for achieving PR. Some of the most prominent electoral reform movements around the globe are pushing for the adoption of STV. It has been termed a major open problem to design a voting rule that satisfies the same PR properties as STV and better monotonicity properties. We present a rule called EAR (Expanding Approvals Rule) that satisfies properties stronger than the central PR axiom satisfied by STV, can handle indifferences in a convenient and computationally efficient manner, and also satisfies better candidate monotonicity properties. In view of this, our proposed rule seems to be a compelling solution for achieving proportional representation in voting settings. version:1
arxiv-1708-09014 | 13th European Dependable Computing Conference (EDCC 2017): Fast Abstracts and Student Forum Proceedings | http://arxiv.org/abs/1708.09014 | id:1708.09014 author:Naghmeh Ivaki category:cs.DC  published:2017-08-24 summary:Fast Abstracts are short presentations of work in progress or opinion pieces and aim to serve as a rapid and flexible mechanism to (i) Report on current work that may or may not be complete; (ii) Introduce new ideas to the community; (iii) State positions on controversial issues or open problems. Student Forum is a vibrant and friendly environment where students can present their work, exchange ideas and experiences, get feedback on their work, get new inspirations and points of view. In addition, the forum stimulates interaction between young researchers, experienced researchers, and industry. version:1
arxiv-1708-07401 | Communication Lower Bounds for Matricized Tensor Times Khatri-Rao Product | http://arxiv.org/abs/1708.07401 | id:1708.07401 author:Grey Ballard, Nicholas Knight, Kathryn Rouse category:cs.DC  published:2017-08-24 summary:The matricized-tensor times Khatri-Rao product computation is the typical bottleneck in algorithms for computing a CP decomposition of a tensor. In order to develop high performance sequential and parallel algorithms, we establish communication lower bounds that identify how much data movement is required for this computation in the case of dense tensors. We also present sequential and parallel algorithms that attain the lower bounds and are therefore communication optimal. In particular, we show that the structure of the computation allows for less communication than the straightforward approach of casting the computation as a matrix multiplication operation. version:1
arxiv-1708-07379 | Results of the Survey: Failures in Robotics and Intelligent Systems | http://arxiv.org/abs/1708.07379 | id:1708.07379 author:Johannes Wienke, Sebastian Wrede category:cs.RO cs.SE  published:2017-08-24 summary:In January 2015 we distributed an online survey about failures in robotics and intelligent systems across robotics researchers. The aim of this survey was to find out which types of failures currently exist, what their origins are, and how systems are monitored and debugged - with a special focus on performance bugs. This report summarizes the findings of the survey. version:1
arxiv-1708-07285 | Area Protection in Adversarial Path-Finding Scenarios with Multiple Mobile Agents on Graphs: a theoretical and experimental study of target-allocation strategies for defense coordination | http://arxiv.org/abs/1708.07285 | id:1708.07285 author:Marika Ivanová, Pavel Surynek category:cs.AI cs.MA  published:2017-08-24 summary:We address a problem of area protection in graph-based scenarios with multiple agents. The problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams. Agents are placed in vertices of the graph; at most one agent can occupy a vertex; and they can move into adjacent vertices in a conflict free way. Teams have asymmetric goals: the aim of one team - attackers - is to invade into given area while the aim of the opponent team - defenders - is to protect the area from being entered by attackers by occupying selected vertices. We study strategies for allocating vertices to be occupied by the team of defenders to block attacking agents. We show that the decision version of the problem of area protection is PSPACE-hard under the assumption that agents can allocate their target vertices multiple times. Further we develop various on-line vertex-allocation strategies for the defender team in a simplified variant of the problem with single stage vertex allocation and evaluated their performance in multiple benchmarks. The success of a strategy is heavily dependent on the type of the instance, and so one of the contributions of this work is that we identify suitable vertex-allocation strategies for diverse instance types. In particular, we introduce a simulation-based method that identifies and tries to capture bottlenecks in the graph, that are frequently used by the attackers. Our experimental evaluation suggests that this method often allows a successful defense even in instances where the attackers significantly outnumber the defenders. version:1
arxiv-1708-07280 | Learning Generalized Reactive Policies using Deep Neural Networks | http://arxiv.org/abs/1708.07280 | id:1708.07280 author:Edward Groshev, Aviv Tamar, Siddharth Srivastava, Pieter Abbeel category:cs.AI  published:2017-08-24 summary:We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A*. Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation. version:1
arxiv-1708-07252 | A Study on Neural Network Language Modeling | http://arxiv.org/abs/1708.07252 | id:1708.07252 author:Dengliang Shi category:cs.CL cs.AI  published:2017-08-24 summary:An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed. version:1
arxiv-1708-07244 | On the Compressive Power of Deep Rectifier Networks for High Resolution Representation of Class Boundaries | http://arxiv.org/abs/1708.07244 | id:1708.07244 author:Senjian An, Mohammed Bennamoun, Farid Boussaid category:cs.LG cs.AI stat.ML  published:2017-08-24 summary:This paper provides a theoretical justification of the superior classification performance of deep rectifier networks over shallow rectifier networks from the geometrical perspective of piecewise linear (PWL) classifier boundaries. We show that, for a given threshold on the approximation error, the required number of boundary facets to approximate a general smooth boundary grows exponentially with the dimension of the data, and thus the number of boundary facets, referred to as boundary resolution, of a PWL classifier is an important quality measure that can be used to estimate a lower bound on the classification errors. However, learning naively an exponentially large number of boundary facets requires the determination of an exponentially large number of parameters and also requires an exponentially large number of training patterns. To overcome this issue of "curse of dimensionality", compressive representations of high resolution classifier boundaries are required. To show the superior compressive power of deep rectifier networks over shallow rectifier networks, we prove that the maximum boundary resolution of a single hidden layer rectifier network classifier grows exponentially with the number of units when this number is smaller than the dimension of the patterns. When the number of units is larger than the dimension of the patterns, the growth rate is reduced to a polynomial order. Consequently, the capacity of generating a high resolution boundary will increase if the same large number of units are arranged in multiple layers instead of a single hidden layer. Taking high dimensional spherical boundaries as examples, we show how deep rectifier networks can utilize geometric symmetries to approximate a boundary with the same accuracy but with a significantly fewer number of parameters than single hidden layer nets. version:1
arxiv-1708-07239 | Finding Streams in Knowledge Graphs to Support Fact Checking | http://arxiv.org/abs/1708.07239 | id:1708.07239 author:Prashant Shiralkar, Alessandro Flammini, Filippo Menczer, Giovanni Luca Ciampaglia category:cs.AI cs.SI  published:2017-08-24 summary:The volume and velocity of information that gets generated online limits current journalistic practices to fact-check claims at the same rate. Computational approaches for fact checking may be the key to help mitigate the risks of massive misinformation spread. Such approaches can be designed to not only be scalable and effective at assessing veracity of dubious claims, but also to boost a human fact checker's productivity by surfacing relevant facts and patterns to aid their analysis. To this end, we present a novel, unsupervised network-flow based approach to determine the truthfulness of a statement of fact expressed in the form of a (subject, predicate, object) triple. We view a knowledge graph of background information about real-world entities as a flow network, and knowledge as a fluid, abstract commodity. We show that computational fact checking of such a triple then amounts to finding a "knowledge stream" that emanates from the subject node and flows toward the object node through paths connecting them. Evaluation on a range of real-world and hand-crafted datasets of facts related to entertainment, business, sports, geography and more reveals that this network-flow model can be very effective in discerning true statements from false ones, outperforming existing algorithms on many test cases. Moreover, the model is expressive in its ability to automatically discover several useful path patterns and surface relevant facts that may help a human fact checker corroborate or refute a claim. version:1
arxiv-1708-06834 | Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks | http://arxiv.org/abs/1708.06834 | id:1708.06834 author:Victor Campos, Brendan Jou, Xavier Giro-i-Nieto, Jordi Torres, Shih-Fu Chang category:cs.AI cs.CV  published:2017-08-22 summary:Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/ . version:2
arxiv-1708-07233 | Reliability and Fault-Tolerance by Choreographic Design | http://arxiv.org/abs/1708.07233 | id:1708.07233 author:Ian Cassar, Adrian Francalanza, Claudio Antares Mezzina, Emilio Tuosto category:cs.PL cs.DC cs.FL  published:2017-08-24 summary:Distributed programs are hard to get right because they are required to be open, scalable, long-running, and tolerant to faults. In particular, the recent approaches to distributed software based on (micro-)services where different services are developed independently by disparate teams exacerbate the problem. In fact, services are meant to be composed together and run in open context where unpredictable behaviours can emerge. This makes it necessary to adopt suitable strategies for monitoring the execution and incorporate recovery and adaptation mechanisms so to make distributed programs more flexible and robust. The typical approach that is currently adopted is to embed such mechanisms in the program logic, which makes it hard to extract, compare and debug. We propose an approach that employs formal abstractions for specifying failure recovery and adaptation strategies. Although implementation agnostic, these abstractions would be amenable to algorithmic synthesis of code, monitoring and tests. We consider message-passing programs (a la Erlang, Go, or MPI) that are gaining momentum both in academia and industry. Our research agenda consists of (1) the definition of formal behavioural models encompassing failures, (2) the specification of the relevant properties of adaptation and recovery strategy, (3) the automatic generation of monitoring, recovery, and adaptation logic in target languages of interest. version:1
arxiv-1708-07738 | A Function Approximation Method for Model-based High-Dimensional Inverse Reinforcement Learning | http://arxiv.org/abs/1708.07738 | id:1708.07738 author:Kun Li, Joel W. Burdick category:cs.LG cs.RO  published:2017-08-23 summary:This works handles the inverse reinforcement learning problem in high-dimensional state spaces, which relies on an efficient solution of model-based high-dimensional reinforcement learning problems. To solve the computationally expensive reinforcement learning problems, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function based on the observed human actions for inverse reinforcement learning problems. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle high-dimensional even continuous state spaces efficiently. We test the proposed method in a simulated environment to show its accuracy, and three clinical tasks to show how it can be used to evaluate a doctor's proficiency. version:1
arxiv-1708-07149 | Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses | http://arxiv.org/abs/1708.07149 | id:1708.07149 author:Ryan Lowe, Michael Noseworthy, Iulian V. Serban, Nicolas Angelard-Gontier, Yoshua Bengio, Joelle Pineau category:cs.CL cs.AI cs.LG  published:2017-08-23 summary:Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality. Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model's predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation. version:1
arxiv-1708-07129 | A Survey of Human Activity Recognition Using WiFi CSI | http://arxiv.org/abs/1708.07129 | id:1708.07129 author:Siamak Yousefi, Hirokazu Narui, Sankalp Dayal, Stefano Ermon, Shahrokh Valaee category:cs.AI  published:2017-08-23 summary:In this article, we present a survey of recent advances in passive human behaviour recognition in indoor areas using the channel state information (CSI) of commercial WiFi systems. Movement of human body causes a change in the wireless signal reflections, which results in variations in the CSI. By analyzing the data streams of CSIs for different activities and comparing them against stored models, human behaviour can be recognized. This is done by extracting features from CSI data streams and using machine learning techniques to build models and classifiers. The techniques from the literature that are presented herein have great performances, however, instead of the machine learning techniques employed in these works, we propose to use deep learning techniques such as long-short term memory (LSTM) recurrent neural network (RNN), and show the improved performance. We also discuss about different challenges such as environment change, frame rate selection, and multi-user scenario, and suggest possible directions for future work. version:1
arxiv-1708-07050 | Capturing Long-term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition | http://arxiv.org/abs/1708.07050 | id:1708.07050 author:Soheil Khorram, Zakaria Aldeneh, Dimitrios Dimitriadis, Melvin McInnis, Emily Mower Provost category:cs.SD cs.AI  published:2017-08-23 summary:The goal of continuous emotion recognition is to assign an emotion value to every frame in a sequence of acoustic features. We show that incorporating long-term temporal dependencies is critical for continuous emotion recognition tasks. To this end, we first investigate architectures that use dilated convolutions. We show that even though such architectures outperform previously reported systems, the output signals produced from such architectures undergo erratic changes between consecutive time steps. This is inconsistent with the slow moving ground-truth emotion labels that are obtained from human annotators. To deal with this problem, we model a downsampled version of the input signal and then generate the output signal through upsampling. Not only does the resulting downsampling/upsampling network achieve good performance, it also generates smooth output trajectories. Our method yields the best known audio-only performance on the RECOLA dataset. version:1
arxiv-1708-07038 | Non-linear Convolution Filters for CNN-based Learning | http://arxiv.org/abs/1708.07038 | id:1708.07038 author:Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, Petros Daras category:cs.CV cs.AI  published:2017-08-23 summary:During the last years, Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in image classification. Their architectures have largely drawn inspiration by models of the primate visual system. However, while recent research results of neuroscience prove the existence of non-linear operations in the response of complex visual cells, little effort has been devoted to extend the convolution technique to non-linear forms. Typical convolutional layers are linear systems, hence their expressiveness is limited. To overcome this, various non-linearities have been used as activation functions inside CNNs, while also many pooling strategies have been applied. We address the issue of developing a convolution method in the context of a computational model of the visual cortex, exploring quadratic forms through the Volterra kernels. Such forms, constituting a more rich function space, are used as approximations of the response profile of visual cells. Our proposed second-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a network which combines linear and non-linear filters in its convolutional layers, can outperform networks that use standard linear filters with the same architecture, yielding results competitive with the state-of-the-art on these datasets. version:1
arxiv-1708-07123 | Intent Communication between Autonomous Vehicles and Pedestrians | http://arxiv.org/abs/1708.07123 | id:1708.07123 author:Milecia Matthews, Girish Chowdhary, Emily Kieson category:cs.HC cs.RO  published:2017-08-23 summary:When pedestrians encounter vehicles, they typically stop and wait for a signal from the driver to either cross or wait. What happens when the car is autonomous and there isn't a human driver to signal them? This paper seeks to address this issue with an intent communication system (ICS) that acts in place of a human driver. This intent system has been developed to take into account the psychology behind what pedestrians are familiar with and what they expect from machines. The system integrates those expectations into the design of physical systems and mathematical algorithms. The goal of the system is to ensure that communication is simple, yet effective without leaving pedestrians with a sense of distrust in autonomous vehicles. To validate the ICS, two types of experiments have been run: field tests with an autonomous vehicle to determine how humans actually interact with the ICS and simulations to account for multiple potential behaviors.The results from both experiments show that humans react positively and more predictably when the intent of the vehicle is communicated compared to when the intent of the vehicle is unknown. In particular, the results from the simulation specifically showed a 142 percent difference between the pedestrian's trust in the vehicle's actions when the ICS is enabled and the pedestrian has prior knowledge of the vehicle than when the ICS is not enabled and the pedestrian having no prior knowledge of the vehicle. version:1
arxiv-1708-06989 | A Neural Network Approach for Mixing Language Models | http://arxiv.org/abs/1708.06989 | id:1708.06989 author:Youssef Oualil, Dietrich Klakow category:cs.CL cs.AI 97K50 I.2.7  published:2017-08-23 summary:The performance of Neural Network (NN)-based language models is steadily improving due to the emergence of new architectures, which are able to learn different natural language characteristics. This paper presents a novel framework, which shows that a significant improvement can be achieved by combining different existing heterogeneous models in a single architecture. This is done through 1) a feature layer, which separately learns different NN-based models and 2) a mixture layer, which merges the resulting model features. In doing so, this architecture benefits from the learning capabilities of each model with no noticeable increase in the number of model parameters or the training time. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures. version:1
arxiv-1708-06962 | Towards Cooperative Motion Planning for Automated Vehicles in Mixed Traffic | http://arxiv.org/abs/1708.06962 | id:1708.06962 author:Maximilian Naumann, Christoph Stiller category:cs.RO cs.MA  published:2017-08-23 summary:While motion planning techniques for automated vehicles in a reactive and anticipatory manner are already widely presented, approaches to cooperative motion planning are still remaining. In this paper, we present an approach to enhance common motion planning algorithms, that allows for cooperation with human-driven vehicles. Unlike previous approaches, we integrate the prediction of other traffic participants into the motion planning, such that the influence of the ego vehicle's behavior on the other traffic participants can be taken into account. For this purpose, a new cost functional is presented, containing the cost for all relevant traffic participants in the scene. Finally, we propose a path-velocity-decomposing sampling-based implementation of our approach for selected scenarios, which is evaluated in a simulation. version:1
arxiv-1708-06947 | Proof-Labeling Schemes: Broadcast, Unicast and In Between | http://arxiv.org/abs/1708.06947 | id:1708.06947 author:Boaz Patt-Shamir, Mor Perry category:cs.DC  published:2017-08-23 summary:We study the effect of limiting the number of different messages a node can transmit simultaneously on the verification complexity of proof-labeling schemes (PLS). In a PLS, each node is given a label, and the goal is to verify, by exchanging messages over each link in each direction, that a certain global predicate is satisfied by the system configuration. We consider a single parameter r that bounds the number of distinct messages that can be sent concurrently by any node: in the case r=1, each node may only send the same message to all its neighbors (the broadcast model), in the case r is at least Delta, where Delta is the largest node degree in the system, each neighbor may be sent a distinct message (the unicast model), and in general, for r between 1 and Delta, each of the r messages is destined to a subset of the neighbors. We show that message compression linear in r is possible for verifying fundamental problems such as the agreement between edge endpoints on the edge state. Some problems, including verification of maximal matching, exhibit a large gap in complexity between r=1 and r>1. For some other important predicates, the verification complexity is insensitive to r, e.g., the question whether a subset of edges constitutes a spanning-tree. We also consider the congested clique model. We show that the crossing technique for proving lower bounds on the verification complexity can be applied in the case of congested clique only if r=1. Together with a new upper bound, this allows us to determine the verification complexity of MST in the broadcast clique. Finally, we establish a general connection between the deterministic and randomized verification complexity for any given number r. version:1
arxiv-1708-06939 | Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid | http://arxiv.org/abs/1708.06939 | id:1708.06939 author:Marco Melis, Ambra Demontis, Battista Biggio, Gavin Brown, Giorgio Fumera, Fabio Roli category:cs.LG cs.RO stat.ML  published:2017-08-23 summary:Deep neural networks have been widely adopted in recent years, exhibiting impressive performances in several application domains. It has however been shown that they can be fooled by adversarial examples, i.e., images altered by a barely-perceivable adversarial noise, carefully crafted to mislead classification. In this work, we aim to evaluate the extent to which robot-vision systems embodying deep-learning algorithms are vulnerable to adversarial examples, and propose a computationally efficient countermeasure to mitigate this threat, based on rejecting classification of anomalous inputs. We then provide a clearer understanding of the safety properties of deep networks through an intuitive empirical analysis, showing that the mapping learned by such networks essentially violates the smoothness assumption of learning algorithms. We finally discuss the main limitations of this work, including the creation of real-world adversarial examples, and sketch promising research directions. version:1
arxiv-1709-10396 | Analysis and Design of Cost-Effective, High-Throughput LDPC Decoders | http://arxiv.org/abs/1709.10396 | id:1709.10396 author:Thien Truong Nguyen-Ly, Valentin Savin, Khoa Le, David Declercq, Fakhreddine Ghaffari, Oana Boncalo category:eess.SP cs.AR  published:2017-08-23 summary:This paper introduces a new approach to cost-effective, high-throughput hardware designs for Low Density Parity Check (LDPC) decoders. The proposed approach, called Non-Surjective Finite Alphabet Iterative Decoders (NS-FAIDs), exploits the robustness of message-passing LDPC decoders to inaccuracies in the calculation of exchanged messages, and it is shown to provide a unified framework for several designs previously proposed in the literature. NS-FAIDs are optimized by density evolution for regular and irregular LDPC codes, and are shown to provide different trade-offs between hardware complexity and decoding performance. Two hardware architectures targeting high-throughput applications are also proposed, integrating both Min-Sum (MS) and NS-FAID decoding kernels. ASIC post synthesis implementation results on 65nm CMOS technology show that NS-FAIDs yield significant improvements in the throughput to area ratio, by up to 58.75% with respect to the MS decoder, with even better or only slightly degraded error correction performance. version:1
arxiv-1708-06931 | Bringing Fault-Tolerant GigaHertz-Computing to Space: A Multi-Stage Software-Side Fault-Tolerance Approach for Miniaturized Spacecraft | http://arxiv.org/abs/1708.06931 | id:1708.06931 author:Christian M. Fuchs, Todor Stefanov, Nadia Murillo, Aske Plaat category:cs.DC  published:2017-08-23 summary:Modern embedded technology is a driving factor in satellite miniaturization, contributing to a massive boom in satellite launches and a rapidly evolving new space industry. Miniaturized satellites, however, suffer from low reliability, as traditional hardware-based fault-tolerance (FT) concepts are ineffective for on-board computers (OBCs) utilizing modern systems-on-a-chip (SoC). Therefore, larger satellites continue to rely on proven processors with large feature sizes. Software-based concepts have largely been ignored by the space industry as they were researched only in theory, and have not yet reached the level of maturity necessary for implementation. We present the first integral, real-world solution to enable fault-tolerant general-purpose computing with modern multiprocessor-SoCs (MPSoCs) for spaceflight, thereby enabling their use in future high-priority space missions. The presented multi-stage approach consists of three FT stages, combining coarse-grained thread-level distributed self-validation, FPGA reconfiguration, and mixed criticality to assure long-term FT and excellent scalability for both resource constrained and critical high-priority space missions. Early benchmark results indicate a drastic performance increase over state-of-the-art radiation-hard OBC designs and considerably lower software- and hardware development costs. This approach was developed for a 4-year European Space Agency (ESA) project, and we are implementing a tiled MPSoC prototype jointly with two industrial partners. version:1
arxiv-1708-06884 | Big Data Meets HPC Log Analytics: Scalable Approach to Understanding Systems at Extreme Scale | http://arxiv.org/abs/1708.06884 | id:1708.06884 author:Byung H. Park, Saurabh Hukerikar, Ryan Adamson, Christian Engelmann category:cs.DC cs.DB  published:2017-08-23 summary:Today's high-performance computing (HPC) systems are heavily instrumented, generating logs containing information about abnormal events, such as critical conditions, faults, errors and failures, system resource utilization, and about the resource usage of user applications. These logs, once fully analyzed and correlated, can produce detailed information about the system health, root causes of failures, and analyze an application's interactions with the system, providing valuable insights to domain scientists and system administrators. However, processing HPC logs requires a deep understanding of hardware and software components at multiple layers of the system stack. Moreover, most log data is unstructured and voluminous, making it more difficult for system users and administrators to manually inspect the data. With rapid increases in the scale and complexity of HPC systems, log data processing is becoming a big data challenge. This paper introduces a HPC log data analytics framework that is based on a distributed NoSQL database technology, which provides scalability and high availability, and the Apache Spark framework for rapid in-memory processing of the log data. The analytics framework enables the extraction of a range of information about the system so that system administrators and end users alike can obtain necessary insights for their specific needs. We describe our experience with using this framework to glean insights from the log data about system behavior from the Titan supercomputer at the Oak Ridge National Laboratory. version:1
arxiv-1708-07422 | Resilience Design Patterns: A Structured Approach to Resilience at Extreme Scale | http://arxiv.org/abs/1708.07422 | id:1708.07422 author:Saurabh Hukerikar, Christian Engelmann category:cs.DC cs.SE  published:2017-08-23 summary:Reliability is a serious concern for future extreme-scale high-performance computing (HPC) systems. While the HPC community has developed various resilience solutions, the solution space remains fragmented. There are no formal methods and metrics to integrate the various HPC resilience techniques into composite solutions, nor are there methods to holistically evaluate the adequacy and efficacy of such solutions in terms of their protection coverage, and their performance & power efficiency characteristics. In this paper, we develop a structured approach to the design, evaluation and optimization of HPC resilience using the concept of design patterns. A design pattern is a general repeatable solution to a commonly occurring problem. We identify the problems caused by various types of faults, errors and failures in HPC systems and the techniques used to deal with these events. Each well-known solution that addresses a specific HPC resilience challenge is described in the form of a pattern. We develop a complete catalog of such resilience design patterns, which may be used as essential building blocks when designing and deploying resilience solutions. We also develop a design framework that enhances a designer's understanding the opportunities for integrating multiple patterns across layers of the system stack and the important constraints during implementation of the individual patterns. It is also useful for defining mechanisms and interfaces to coordinate flexible fault management across hardware and software components. The overall goal of this work is to establish a systematic methodology for the design and evaluation of resilience technologies in extreme-scale HPC systems that keep scientific applications running to a correct solution in a timely and cost-efficient manner despite frequent faults, errors, and failures of various types. version:1
arxiv-1708-06881 | On Relationship between Primal-Dual Method of Multipliers and Kalman Filter | http://arxiv.org/abs/1708.06881 | id:1708.06881 author:Guoqiang Zhang, W. Bastiaan Kleijn, Richard Heusdens category:math.OC cs.DC cs.IT math.IT  published:2017-08-23 summary:Recently the primal-dual method of multipliers (PDMM), a novel distributed optimization method, was proposed for solving a general class of decomposable convex optimizations over graphic models. In this work, we first study the convergence properties of PDMM for decomposable quadratic optimizations over tree-structured graphs. We show that with proper parameter selection, PDMM converges to its optimal solution in finite number of iterations. We then apply PDMM for the causal estimation problem over a statistical linear state-space model. We show that PDMM and the Kalman filter have the same update expressions, where PDMM can be interpreted as solving a sequence of quadratic optimizations over a growing chain graph. version:1
arxiv-1708-06877 | The Reachability of Computer Programs | http://arxiv.org/abs/1708.06877 | id:1708.06877 author:Reginaldo I. Silva Filho, Ricardo L. Azevedo da Rocha, Camila Leite Silva, Ricardo H. Gracini Guiraldelli category:cs.IT cs.AI math.IT E.4  published:2017-08-23 summary:Would it be possible to explain the emergence of new computational ideas using the computation itself? Would it be feasible to describe the discovery process of new algorithmic solutions using only mathematics? This study is the first effort to analyze the nature of such inquiry from the viewpoint of effort to find a new algorithmic solution to a given problem. We define program reachability as a probability function whose argument is a form of the energetic cost (algorithmic entropy) of the problem. version:1
arxiv-1708-06866 | Static Graph Challenge: Subgraph Isomorphism | http://arxiv.org/abs/1708.06866 | id:1708.06866 author:Siddharth Samsi, Vijay Gadepally, Michael Hurley, Michael Jones, Edward Kao, Sanjeev Mohindra, Paul Monticciolo, Albert Reuther, Steven Smith, William Song, Diane Staheli, Jeremy Kepner category:cs.DC cs.DS  published:2017-08-23 summary:The rise of graph analytic systems has created a need for ways to measure and compare the capabilities of these systems. Graph analytics present unique scalability difficulties. The machine learning, high performance computing, and visual analytics communities have wrestled with these difficulties for decades and developed methodologies for creating challenges to move these communities forward. The proposed Subgraph Isomorphism Graph Challenge draws upon prior challenges from machine learning, high performance computing, and visual analytics to create a graph challenge that is reflective of many real-world graph analytics processing systems. The Subgraph Isomorphism Graph Challenge is a holistic specification with multiple integrated kernels that can be run together or independently. Each kernel is well defined mathematically and can be implemented in any programming environment. Subgraph isomorphism is amenable to both vertex-centric implementations and array-based implementations (e.g., using the GraphBLAS.org standard). The computations are simple enough that performance predictions can be made based on simple computing hardware models. The surrounding kernels provide the context for each kernel that allows rigorous definition of both the input and the output for each kernel. Furthermore, since the proposed graph challenge is scalable in both problem size and hardware, it can be used to measure and quantitatively compare a wide range of present day and future systems. Serial implementations in C++, Python, Python with Pandas, Matlab, Octave, and Julia have been implemented and their single threaded performance have been measured. Specifications, data, and software are publicly available at GraphChallenge.org. version:1
arxiv-1708-06846 | On Relaxing Determinism in Arithmetic Circuits | http://arxiv.org/abs/1708.06846 | id:1708.06846 author:Arthur Choi, Adnan Darwiche category:cs.AI cs.LG  published:2017-08-22 summary:The past decade has seen a significant interest in learning tractable probabilistic representations. Arithmetic circuits (ACs) were among the first proposed tractable representations, with some subsequent representations being instances of ACs with weaker or stronger properties. In this paper, we provide a formal basis under which variants on ACs can be compared, and where the precise roles and semantics of their various properties can be made more transparent. This allows us to place some recent developments on ACs in a clearer perspective and to also derive new results for ACs. This includes an exponential separation between ACs with and without determinism; completeness and incompleteness results; and tractability results (or lack thereof) when computing most probable explanations (MPEs). version:1
arxiv-1708-06832 | Anytime Neural Networks via Joint Optimization of Auxiliary Losses | http://arxiv.org/abs/1708.06832 | id:1708.06832 author:Hanzhang Hu, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert category:cs.LG cs.AI  published:2017-08-22 summary:We address the problem of anytime prediction in neural networks. An anytime predictor automatically adjusts to and utilizes available test-time budget: it produces a crude initial result quickly and continuously refines the result afterwards. Traditional feed-forward networks achieve state-of-the-art performance on many machine learning tasks, but cannot produce anytime predictions during their typically expensive computation. In this work, we propose to add auxiliary predictions in a residual network to generate anytime predictions, and optimize these predictions simultaneously. We solve this multi-objective optimization by minimizing a carefully constructed weighted sum of losses. We also oscillate weightings of the losses in each iteration to avoid spurious solutions that are optimal for the sum but not for each individual loss. The proposed approach produces competitive results if computation is interrupted early, and the same level of performance as the original network once computation is finished. Observing that the relative performance gap between the optimal and our proposed anytime network shrinks as the network is near completion, we propose a method to combine anytime networks to achieve more accurate anytime predictions with a constant fraction of additional cost. We evaluate the proposed methods on real-world visual recognition data-sets to demonstrate their anytime performance. version:1
arxiv-1708-06828 | Classification of Radiology Reports Using Neural Attention Models | http://arxiv.org/abs/1708.06828 | id:1708.06828 author:Bonggun Shin, Falgun H. Chokshi, Timothy Lee, Jinho D. Choi category:cs.CL cs.AI cs.IR  published:2017-08-22 summary:The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure "black-box" models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies. version:1
arxiv-1708-06816 | Analysis of the Impact of Negative Sampling on Link Prediction in Knowledge Graphs | http://arxiv.org/abs/1708.06816 | id:1708.06816 author:Bhushan Kotnis, Vivi Nastase category:cs.AI  published:2017-08-22 summary:Knowledge graphs are large, useful, but incomplete knowledge repositories. They encode knowledge through entities and relations which define each other through the connective structure of the graph. This has inspired methods for the joint embedding of entities and relations in continuous low-dimensional vector spaces, that can be used to induce new edges in the graph, i.e., link prediction in knowledge graphs. Learning these representations relies on contrasting positive instances with negative ones. Knowledge graphs include only positive relation instances, leaving the door open for a variety of methods for selecting negative examples. In this paper we present an empirical study on the impact of negative sampling on the learned embeddings, assessed through the task of link prediction. We use state-of-the-art knowledge graph embeddings -- \rescal , TransE, DistMult and ComplEX -- and evaluate on benchmark datasets -- FB15k and WN18. We compare well known methods for negative sampling and additionally propose embedding based sampling methods. We note a marked difference in the impact of these sampling methods on the two datasets, with the "traditional" corrupting positives method leading to best results on WN18, while embedding based methods benefiting the task on FB15k. version:1
arxiv-1708-06794 | Human Action Recognition System using Good Features and Multilayer Perceptron Network | http://arxiv.org/abs/1708.06794 | id:1708.06794 author:Jonti Talukdar, Bhavana Mehta category:cs.CV cs.AI cs.HC  published:2017-08-22 summary:Human action recognition involves the characterization of human actions through the automated analysis of video data and is integral in the development of smart computer vision systems. However, several challenges like dynamic backgrounds, camera stabilization, complex actions, occlusions etc. make action recognition in a real time and robust fashion difficult. Several complex approaches exist but are computationally intensive. This paper presents a novel approach of using a combination of good features along with iterative optical flow algorithm to compute feature vectors which are classified using a multilayer perceptron (MLP) network. The use of multiple features for motion descriptors enhances the quality of tracking. Resilient backpropagation algorithm is used for training the feedforward neural network reducing the learning time. The overall system accuracy is improved by optimizing the various parameters of the multilayer perceptron network. version:1
arxiv-1708-09011 | Real-Time Pose Estimation for Event Cameras with Stacked Spatial LSTM Networks | http://arxiv.org/abs/1708.09011 | id:1708.09011 author:Anh Nguyen, Thanh-Toan Do, Darwin G. Caldwell, Nikos G. Tsagarakis category:cs.CV cs.RO  published:2017-08-22 summary:We present a new method to estimate the 6DOF pose of the event camera solely based on the event stream. Our method first creates the event image from a list of events that occurs in a very short time interval, then a Stacked Spatial LSTM Network (SP-LSTM) is used to learn and estimate the camera pose. Our SP-LSTM comprises a CNN to learn deep features from the event images and a stack of LSTM to learn spatial dependencies in the image features space. We show that the spatial dependency plays an important role in the pose estimation task and the SP-LSTM can effectively learn that information. The experimental results on the public dataset show that our approach outperforms recent methods by a substantial margin. Overall, our proposed method reduces about 6 times the position error and 3 times the orientation error over the state of the art. The source code and trained models will be released. version:1
arxiv-1708-06716 | What caused what? An irreducible account of actual causation | http://arxiv.org/abs/1708.06716 | id:1708.06716 author:Larissa Albantakis, William Marshall, Erik Hoel, Giulio Tononi category:cs.AI math.ST stat.TH  published:2017-08-22 summary:Actual causation is concerned with the question "what caused what?". Consider a transition between two subsequent observations within a system of elements. Even under perfect knowledge of the system, a straightforward answer to this question may not be available. Counterfactual accounts of actual causation based on graphical models, paired with system interventions, have demonstrated initial success in addressing specific problem cases. We present a formal account of actual causation, applicable to discrete dynamical systems of interacting elements, that considers all counterfactual states of a state transition from t-1 to t. Within such a transition, causal links are considered from two complementary points of view: we can ask if any occurrence at time t has an actual cause at t-1, but also if any occurrence at time t-1 has an actual effect at t. We address the problem of identifying such actual causes and actual effects in a principled manner by starting from a set of basic requirements for causation (existence, composition, information, integration, and exclusion). We present a formal framework to implement these requirements based on system manipulations and partitions. This framework is used to provide a complete causal account of the transition by identifying and quantifying the strength of all actual causes and effects linking two occurrences. Finally, we examine several exemplary cases and paradoxes of causation and show that they can be illuminated by the proposed framework for quantifying actual causation. version:1
arxiv-1610-00782 | Network Structure Inference, A Survey: Motivations, Methods, and Applications | http://arxiv.org/abs/1610.00782 | id:1610.00782 author:Ivan Brugere, Brian Gallagher, Tanya Y. Berger-Wolf category:cs.SI cs.AI physics.soc-ph  published:2016-10-03 summary:Networks are used to represent relationships between entities in many complex systems, spanning from online social networks to biological cell development and brain connectivity. These networks model relationships which present various challenges. In many cases, relationships between entities are unambiguously known: are two users friends in a social network? Do two researchers collaborate on a published paper? Do two road segments in a transportation system intersect? These are unambiguous and directly observable in the system in question. In most cases, relationship between nodes are not directly observable and must be inferred: does one gene regulate the expression of another? Do two animals who physically co-locate have a social bond? Who infected whom in a disease outbreak? Existing approaches use specialized knowledge in different home domains to infer and measure the goodness of inferred network for a specific task. However, current research lacks a rigorous methodology which employs standard statistical validation. In this survey, we examine how network representations are learned from non-network data, the variety of questions and tasks on these data over several domains, and validation strategies for measuring the inferred network's capability of answering questions on the original system of interest. version:2
arxiv-1708-06652 | Build your own visual-inertial odometry aided cost-effective and open-source autonomous drone | http://arxiv.org/abs/1708.06652 | id:1708.06652 author:Inkyu Sa, Mina Kamel, Michael Burri, Michael Bloesch, Raghav Khanna, Marija Popovic, Juan Nieto, Roland Siegwart category:cs.RO  published:2017-08-22 summary:This paper describes an approach to building a cost-effective and research grade visual-inertial odometry aided vertical taking-off and landing (VTOL) platform. We utilize an off-the-shelf visual-inertial sensor, an onboard computer, and a quadrotor platform that are factory-calibrated and mass-produced, thereby sharing similar hardware and sensor specifications (e.g., mass, dimensions, intrinsic and extrinsic of camera-IMU systems, and signal-to-noise ratio). We then perform a system calibration and identification enabling the use of our visual-inertial odometry, multi-sensor fusion, and model predictive control frameworks with the off-the-shelf products. This implies that we can partially avoid tedious parameter tuning procedures for building a full system. The complete system is extensively evaluated both indoors using a motion capture system and outdoors using a laser tracker while performing hover and step responses, and trajectory following tasks in the presence of external wind disturbances. We achieve root-mean-square (RMS) pose errors between a reference and actual trajectories of 0.036m, while performing hover. We also conduct relatively long distance flight (~180m) experiments on a farm site and achieve 0.82% drift error of the total distance flight. This paper conveys the insights we acquired about the platform and sensor module and returns to the community as open-source code with tutorial documentation. version:1
arxiv-1708-01425 | The Argument Reasoning Comprehension Task | http://arxiv.org/abs/1708.01425 | id:1708.01425 author:Ivan Habernal, Henning Wachsmuth, Iryna Gurevych, Benno Stein category:cs.CL cs.AI  published:2017-08-04 summary:Reasoning is a crucial part of natural language argumentation. In order to comprehend an argument, one has to reconstruct and analyze its reasoning. As arguments are highly contextualized, most reasoning-related content is left implicit and usually presupposed. Thus, argument comprehension requires not only language understanding and logic skills, but it also heavily depends on common sense. In this article we define a new task, argument reasoning comprehension. Given a natural language argument with a reason and a claim, the goal is to choose the correct implicit reasoning from two options. The challenging factor is that both options are plausible and lexically very close while leading to contradicting claims. To provide an empirical common ground for the task, we propose a complex, yet scalable crowdsourcing process, and we create a new freely licensed dataset based on authentic arguments from news comments. While the resulting 2k high-quality instances are also suitable for other argumentation-related tasks, such as stance detection, argument component identification, and abstractive argument summarization, we focus on the argument reasoning comprehension task and experiment with several systems based on neural attention and language models. Our results clearly reveal that current methods lack the capability to solve the task. version:2
arxiv-1709-01440 | Locality-Aware Hybrid Coded MapReduce for Server-Rack Architecture | http://arxiv.org/abs/1709.01440 | id:1709.01440 author:Sneh Gupta, V. Lalitha category:cs.DC cs.IT math.IT  published:2017-08-22 summary:MapReduce is a widely used framework for distributed computing. Data shuffling between the Map phase and Reduce phase of a job involves a large amount of data transfer across servers, which in turn accounts for increase in job completion time. Recently, Coded MapReduce has been proposed to offer savings with respect to the communication cost incurred in data shuffling. This is achieved by creating coded multicast opportunities for shuffling through repeating Map tasks at multiple servers. We consider a server-rack architecture for MapReduce and in this architecture, propose to divide the total communication cost into two: intra-rack communication cost and cross-rack communication cost. Having noted that cross-rack data transfer operates at lower speed as compared to intra-rack data transfer, we present a scheme termed as Hybrid Coded MapReduce which results in lower cross-rack communication than Coded MapReduce at the cost of increase in intra-rack communication. In addition, we pose the problem of assigning Map tasks to servers to maximize data locality in the framework of Hybrid Coded MapReduce as a constrained integer optimization problem. We show through simulations that data locality can be improved considerably by using the solution of optimization to assign Map tasks to servers. version:1
arxiv-1708-06564 | The Continuous Hint Factory - Providing Hints in Vast and Sparsely Populated Edit Distance Spaces | http://arxiv.org/abs/1708.06564 | id:1708.06564 author:Benjamin Paaßen, Barbara Hammer, Thomas William Price, Tiffany Barnes, Sebastian Gross, Niels Pinkwart category:cs.AI  published:2017-08-22 summary:Intelligent tutoring systems can support students in solving multi-step tasks by providing a hint regarding what to do next. However, engineering such next-step hints manually or using an expert model becomes infeasible if the space of possible states is too large. Therefore, several approaches have emerged to infer next-step hints automatically, relying on past student's data. Such hints typically have the form of an edit which could have been performed by capable students in the given situation, based on what past capable students have done. In this contribution we provide a mathematical framework to analyze edit-based hint policies and, based on this theory, propose a novel hint policy to provide edit hints for learning tasks with a vast state space and sparse student data. We call this technique the continuous hint factory because it embeds student data in a continuous space, in which the most likely edit can be inferred in a probabilistic sense, similar to the hint factory. In our experimental evaluation we demonstrate that the continuous hint factory can predict what capable students would do in solving a multi-step programming task and that hints provided by the continuous hint factory match to some extent the edit hints that human tutors would have given in the same situation. version:1
arxiv-1708-05997 | A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models | http://arxiv.org/abs/1708.05997 | id:1708.05997 author:Youssef Oualil, Dietrich Klakow category:cs.CL cs.AI 97K50 I.2.7  published:2017-08-20 summary:Training large vocabulary Neural Network Language Models (NNLMs) is a difficult task due to the explicit requirement of the output layer normalization, which typically involves the evaluation of the full softmax function over the complete vocabulary. This paper proposes a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate this problem. This is achieved by reducing the vocabulary, at each time step, to the target words in the batch and then replacing the softmax by the noise contrastive estimation approach, where these words play the role of targets and noise samples at the same time. In doing so, the proposed approach can be fully formulated and implemented using optimal dense matrix operations. Applying B-NCE to train different NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion Word Benchmark (OBWB) shows a significant reduction of the training time with no noticeable degradation of the models performance. This paper also presents a new baseline comparative study of different standard NNLMs on the large OBWB on a single Titan-X GPU. version:2
arxiv-1708-06542 | A Self-Stabilizing General De Bruijn Graph | http://arxiv.org/abs/1708.06542 | id:1708.06542 author:Michael Feldmann, Christian Scheideler category:cs.DC  published:2017-08-22 summary:Searching for other participants is one of the most important operations in a distributed system. We are interested in topologies in which it is possible to route a packet in a fixed number of hops until it arrives at its destination. Given a constant $d$, this paper introduces a new self-stabilizing protocol for the $q$-ary $d$-dimensional de Bruijn graph ($q = \sqrt[d]{n}$) that is able to route any search request in at most $d$ hops w.h.p., while significantly lowering the node degree compared to the clique: We require nodes to have a degree of $\mathcal O(\sqrt[d]{n})$, which is asymptotically optimal for a fixed diameter $d$. The protocol keeps the expected amount of edge redirections per node in $\mathcal O(\sqrt[d]{n})$, when the number of nodes in the system increases by factor $2^d$. The number of messages that are periodically sent out by nodes is constant. version:1
arxiv-1708-06519 | Learning Efficient Convolutional Networks through Network Slimming | http://arxiv.org/abs/1708.06519 | id:1708.06519 author:Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang category:cs.CV cs.AI cs.LG  published:2017-08-22 summary:The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations. version:1
arxiv-1708-05106 | The Mean and Median Criterion for Automatic Kernel Bandwidth Selection for Support Vector Data Description | http://arxiv.org/abs/1708.05106 | id:1708.05106 author:Arin Chaudhuri, Deovrat Kakde, Carol Sadek, Laura Gonzalez, Seunghyun Kong category:cs.LG cs.AI stat.ML I.2.7  published:2017-08-16 summary:Support vector data description (SVDD) is a popular technique for detecting anomalies. The SVDD classifier partitions the whole space into an inlier region, which consists of the region near the training data, and an outlier region, which consists of points away from the training data. The computation of the SVDD classifier requires a kernel function, and the Gaussian kernel is a common choice for the kernel function. The Gaussian kernel has a bandwidth parameter, whose value is important for good results. A small bandwidth leads to overfitting, and the resulting SVDD classifier overestimates the number of anomalies. A large bandwidth leads to underfitting, and the classifier fails to detect many anomalies. In this paper we present a new automatic, unsupervised method for selecting the Gaussian kernel bandwidth. The selected value can be computed quickly, and it is competitive with existing bandwidth selection methods. version:2
arxiv-1708-06445 | Planning Based System for Child-Robot Interaction in Dynamic Play Environments | http://arxiv.org/abs/1708.06445 | id:1708.06445 author:Vicky Charisi, Bram Ridder, Jaebok Kim, Vanessa Evers category:cs.RO cs.HC  published:2017-08-21 summary:This paper describes the initial steps towards the design of a robotic system that intends to perform actions autonomously in a naturalistic play environment. At the same time it aims for social human-robot interaction~(HRI), focusing on children. We draw on existing theories of child development and on dimensional models of emotions to explore the design of a dynamic interaction framework for natural child-robot interaction. In this dynamic setting, the social HRI is defined by the ability of the system to take into consideration the socio-emotional state of the user and to plan appropriately by selecting appropriate strategies for execution. The robot needs a temporal planning system, which combines features of task-oriented actions and principles of social human robot interaction. We present initial results of an empirical study for the evaluation of the proposed framework in the context of a collaborative sorting game. version:1
arxiv-1708-00588 | Hidden Physics Models: Machine Learning of Nonlinear Partial Differential Equations | http://arxiv.org/abs/1708.00588 | id:1708.00588 author:Maziar Raissi, George Em Karniadakis category:cs.AI cs.LG math.AP stat.ML  published:2017-08-02 summary:While there is currently a lot of enthusiasm about "big data", useful data is usually "small" and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from {\em small} data. In particular, we introduce \emph{hidden physics models}, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier-Stokes, Schr\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data. version:2
arxiv-1708-06425 | SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness | http://arxiv.org/abs/1708.06425 | id:1708.06425 author:Mustafa A. Kocak, David Ramirez, Elza Erkip, Dennis E. Shasha category:cs.LG cs.AI math.ST stat.TH  published:2017-08-21 summary:SafePredict is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, $1-\epsilon$, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm on occasion so that the error rate on non-refused predictions does not exceed $\epsilon$. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate $\epsilon$, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the art confidence based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software (currently in Python) is included in the supplementary material. version:1
arxiv-1708-06423 | Practical Evaluation of the Lasp Programming Model at Large Scale - An Experience Report | http://arxiv.org/abs/1708.06423 | id:1708.06423 author:Christopher S. Meiklejohn, Vitor Enes, Junghun Yoo, Carlos Baquero, Peter Van Roy, Annette Bieniusa category:cs.DC  published:2017-08-21 summary:Programming models for building large-scale distributed applications assist the developer in reasoning about consistency and distribution. However, many of the programming models for weak consistency, which promise the largest scalability gains, have little in the way of evaluation to demonstrate the promised scalability. We present an experience report on the implementation and large-scale evaluation of one of these models, Lasp, originally presented at PPDP `15, which provides a declarative, functional programming style for distributed applications. We demonstrate the scalability of Lasp's prototype runtime implementation up to 1024 nodes in the Amazon cloud computing environment. It achieves high scalability by uniquely combining hybrid gossip with a programming model based on convergent computation. We report on the engineering challenges of this implementation and its evaluation, specifically related to operating research prototypes in a production cloud environment. version:1
arxiv-1612-00445 | Near-Memory Address Translation | http://arxiv.org/abs/1612.00445 | id:1612.00445 author:Javier Picorel, Djordje Jevdjic, Babak Falsafi category:cs.AR cs.OS  published:2016-12-01 summary:Memory and logic integration on the same chip is becoming increasingly cost effective, creating the opportunity to offload data-intensive functionality to processing units placed inside memory chips. The introduction of memory-side processing units (MPUs) into conventional systems faces virtual memory as the first big showstopper: without efficient hardware support for address translation MPUs have highly limited applicability. Unfortunately, conventional translation mechanisms fall short of providing fast translations as contemporary memories exceed the reach of TLBs, making expensive page walks common. In this paper, we are the first to show that the historically important flexibility to map any virtual page to any page frame is unnecessary in today's servers. We find that while limiting the associativity of the virtual-to-physical mapping incurs no penalty, it can break the translate-then-fetch serialization if combined with careful data placement in the MPU's memory, allowing for translation and data fetch to proceed independently and in parallel. We propose the Distributed Inverted Page Table (DIPTA), a near-memory structure in which the smallest memory partition keeps the translation information for its data share, ensuring that the translation completes together with the data fetch. DIPTA completely eliminates the performance overhead of translation, achieving speedups of up to 3.81x and 2.13x over conventional translation using 4KB and 1GB pages respectively. version:2
arxiv-1708-06343 | Aerial Rock Fragmentation Analysis in Low-Light Condition Using UAV Technology | http://arxiv.org/abs/1708.06343 | id:1708.06343 author:Thomas Bamford, Kamran Esmaeili, Angela P. Schoellig category:cs.RO  published:2017-08-21 summary:In recent years, Unmanned Aerial Vehicle (UAV) technology has been introduced into the mining industry to conduct terrain surveying. This work investigates the application of UAVs with artificial lighting for measurement of rock fragmentation under poor lighting conditions, representing night shifts in surface mines or working conditions in underground mines. The study relies on indoor and outdoor experiments for rock fragmentation analysis using a quadrotor UAV. Comparison of the rock size distributions in both cases show that adequate artificial lighting enables similar accuracy to ideal lighting conditions. version:1
arxiv-1708-06301 | Dense Disparity Estimation in Ego-motion Reduced Search Space | http://arxiv.org/abs/1708.06301 | id:1708.06301 author:Luka Fućek, Ivan Marković, Igor Cvišić, Ivan Petrović category:cs.RO  published:2017-08-21 summary:Depth estimation from stereo images remains a challenge even though studied for decades. The KITTI benchmark shows that the state-of-the-art solutions offer accurate depth estimation, but are still computationally complex and often require a GPU or FPGA implementation. In this paper we aim at increasing the accuracy of depth map estimation and reducing the computational complexity by using information from previous frames. We propose to transform the disparity map of the previous frame into the current frame, relying on the estimated ego-motion, and use this map as the prediction for the Kalman filter in the disparity space. Then, we update the predicted disparity map using the newly matched one. This way we reduce disparity search space and flickering between consecutive frames, thus increasing the computational efficiency of the algorithm. In the end, we validate the proposed approach on real-world data from the KITTI benchmark suite and show that the proposed algorithm yields more accurate results, while at the same time reducing the disparity search space. version:1
arxiv-1708-06276 | The CARESSES EU-Japan project: making assistive robots culturally competent | http://arxiv.org/abs/1708.06276 | id:1708.06276 author:Barbara Bruno, Nak Young Chong, Hiroko Kamide, Sanjeev Kanoria, Jaeryoung Lee, Yuto Lim, Amit Kumar Pandey, Chris Papadopoulos, Irena Papadopoulos, Federico Pecora, Alessandro Saffiotti, Antonio Sgorbissa category:cs.RO cs.AI cs.CY cs.HC I.2.9  published:2017-08-21 summary:The nursing literature shows that cultural competence is an important requirement for effective healthcare. We claim that personal assistive robots should likewise be culturally competent, that is, they should be aware of general cultural characteristics and of the different forms they take in different individuals, and take these into account while perceiving, reasoning, and acting. The CARESSES project is an Europe-Japan collaborative effort that aims at designing, developing and evaluating culturally competent assistive robots. These robots will be able to adapt the way they behave, speak and interact to the cultural identity of the person they assist. This paper describes the approach taken in the CARESSES project, its initial steps, and its future plans. version:1
arxiv-1708-06274 | This Far, No Further: Introducing Virtual Borders to Mobile Robots Using a Laser Pointer | http://arxiv.org/abs/1708.06274 | id:1708.06274 author:Dennis Sprute, Klaus Tönnies, Matthias König category:cs.RO  published:2017-08-21 summary:In this paper, we address the problem of controlling the workspace of a 3-DoF mobile robot. This problem arises due to the emerging coexistence between humans and robots resulting in a shared space. In such an environment, robots should navigate in a human-acceptable way according to the users' demands. For this purpose, we propose a method that gives a non-expert user the possibility to intuitively define virtual borders by means of a laser pointer. In detail, we contribute a method and implementation based on a previously developed framework using a laser pointer as human-robot interface to change the robot's navigational behavior. Furthermore, we extend the framework to increase the flexibility by considering different types of virtual borders, i.e. polygons and curves separating an area. We qualitatively and quantitatively evaluated our method concerning correctness, accuracy and teaching effort. The experimental results revealed a high accuracy and low teaching effort while correctly incorporating the virtual borders into the robot's navigational map. version:1
arxiv-1708-06266 | Probabilistic Relation Induction in Vector Space Embeddings | http://arxiv.org/abs/1708.06266 | id:1708.06266 author:Zied Bouraoui, Shoaib Jameel, Steven Schockaert category:cs.AI cs.CL  published:2017-08-21 summary:Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge. However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way. In this paper, we propose two probabilistic models to address this issue. The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting. Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words. Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding. version:1
arxiv-1708-06257 | Notes: A Continuous Model of Neural Networks. Part I: Residual Networks | http://arxiv.org/abs/1708.06257 | id:1708.06257 author:Zhen Li, Zuoqiang Shi category:cs.LG cs.AI cs.NE  published:2017-08-21 summary:In this series of notes, we try to model neural networks as as discretizations of continuous flows on the space of data, which can be called flow model. The idea comes from an observation of their similarity in mathematical structures. This conceptual analogy has not been proven useful yet, but it seems interesting to explore. In this part, we start with a linear transport equation (with nonlinear transport velocity field) and obtain a class of residual type neural networks. If the transport velocity field has a special form, the obtained network is found similar to the original ResNet. This neural network can be regarded as a discretization of the continuous flow defined by the transport flow. In the end, a summary of the correspondence between neural networks and transport equations is presented, followed by some general discussions. version:1
arxiv-1708-06233 | Fake News in Social Networks | http://arxiv.org/abs/1708.06233 | id:1708.06233 author:Christoph Aymanns, Jakob Foerster, Co-Pierre Georg category:cs.AI cs.MA cs.SI physics.soc-ph q-fin.EC  published:2017-08-21 summary:We model the spread of news as a social learning game on a network. Agents can either endorse or oppose a claim made in a piece of news, which itself may be either true or false. Agents base their decision on a private signal and their neighbors' past actions. Given these inputs, agents follow strategies derived via multi-agent deep reinforcement learning and receive utility from acting in accordance with the veracity of claims. Our framework yields strategies with agent utility close to a theoretical, Bayes optimal benchmark, while remaining flexible to model re-specification. Optimized strategies allow agents to correctly identify most false claims, when all agents receive unbiased private signals. However, an adversary's attempt to spread fake news by targeting a subset of agents with a biased private signal can be successful. Even more so when the adversary has information about agents' network position or private signal. When agents are aware of the presence of an adversary they re-optimize their strategies in the training stage and the adversary's attack is less effective. Hence, exposing agents to the possibility of fake news can be an effective way to curtail the spread of fake news in social networks. Our results also highlight that information about the users' private beliefs and their social network structure can be extremely valuable to adversaries and should be well protected. version:1
arxiv-1708-07481 | Preconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge | http://arxiv.org/abs/1708.07481 | id:1708.07481 author:David Zhuzhunashvili, Andrew Knyazev category:cs.MS cs.DC cs.DS stat.CO stat.ML H.3.3; I.5.3  published:2017-08-21 summary:Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is demonstrated to efficiently solve eigenvalue problems for graph Laplacians that appear in spectral clustering. For static graph partitioning, 10-20 iterations of LOBPCG without preconditioning result in ~10x error reduction, enough to achieve 100% correctness for all Challenge datasets with known truth partitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7) seconds, compared to over 5,000 (30,000) seconds needed by the baseline Python code. Our Python code 100% correctly determines 98 (160) clusters from the Challenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using 10GB (50GB) of memory. Our single-precision MATLAB code calculates the same clusters at half time and memory. For streaming graph partitioning, LOBPCG is initiated with approximate eigenvectors of the graph Laplacian already computed for the previous graph, in many cases reducing 2-3 times the number of required LOBPCG iterations, compared to the static case. Our spectral clustering is generic, i.e. assuming nothing specific of the block model or streaming, used to generate the graphs for the Challenge, in contrast to the base code. Nevertheless, in 10-stage streaming comparison with the base code for the 5K graph, the quality of our clusters is similar or better starting at stage 4 (7) for emerging edging (snowballing) streaming, while the computations are over 100-1000 faster. version:1
arxiv-1708-06183 | Optimally Gathering Two Robots | http://arxiv.org/abs/1708.06183 | id:1708.06183 author:Adam Heriban, Xavier Défago, Sébastien Tixeuil category:cs.DC cs.CC cs.DS cs.RO  published:2017-08-21 summary:We present an algorithm that ensures in finite time the gathering of two robots in the non-rigid ASYNC model. To circumvent established impossibility results, we assume robots are equipped with 2-colors lights and are able to measure distances between one another. Aside from its light, a robot has no memory of its past actions, and its protocol is deterministic. Since, in the same model, gathering is impossible when lights have a single color, our solution is optimal with respect to the number of used colors. version:1
arxiv-1708-03604 | Porting of the DBCSR library for Sparse Matrix-Matrix Multiplications to Intel Xeon Phi systems | http://arxiv.org/abs/1708.03604 | id:1708.03604 author:Iain Bethune, Andeas Gloess, Juerg Hutter, Alfio Lazzaro, Hans Pabst, Fiona Reid category:cs.DC  published:2017-08-11 summary:Multiplication of two sparse matrices is a key operation in the simulation of the electronic structure of systems containing thousands of atoms and electrons. The highly optimized sparse linear algebra library DBCSR (Distributed Block Compressed Sparse Row) has been specifically designed to efficiently perform such sparse matrix-matrix multiplications. This library is the basic building block for linear scaling electronic structure theory and low scaling correlated methods in CP2K. It is parallelized using MPI and OpenMP, and can exploit GPU accelerators by means of CUDA. We describe a performance comparison of DBCSR on systems with Intel Xeon Phi Knights Landing (KNL) processors, with respect to systems with Intel Xeon CPUs (including systems with GPUs). We find that the DBCSR on Cray XC40 KNL-based systems is 11%-14% slower than on a hybrid Cray XC50 with Nvidia P100 cards, at the same number of nodes. When compared to a Cray XC40 system equipped with dual-socket Intel Xeon CPUs, the KNL is up to 24% faster. version:2
arxiv-1708-06151 | Scalable Kernelization for Maximum Independent Sets | http://arxiv.org/abs/1708.06151 | id:1708.06151 author:Demian Hespe, Christian Schulz, Darren Strash category:cs.DS cs.DC  published:2017-08-21 summary:The most efficient algorithms for finding maximum independent sets in both theory and practice use reduction rules to obtain a much smaller problem instance called a kernel. The kernel can then be solved quickly using exact or heuristic algorithms - or by repeatedly kernelizing recursively in the branch-and-reduce paradigm. It is of critical importance for these algorithms that kernelization is fast and returns a small kernel. Current algorithms are either slow but produce a small kernel, or fast and give a large kernel. We attempt to accomplish both of these goals simultaneously, by giving an efficient parallel kernelization algorithm based on graph partitioning and parallel bipartite maximum matching. We combine our parallelization techniques with two techniques to accelerate kernelization further: dependency checking that prunes reductions that cannot be applied, and reduction tracking that allows us to stop kernelization when reductions become less fruitful. Our algorithm produces kernels that are orders of magnitude smaller than the fastest kernelization methods, while having a similar execution time. Furthermore, our algorithm is able to compute kernels with size comparable to the smallest known kernels, but up to two orders of magnitude faster than previously possible. Finally, we show that our kernelization algorithm can be used to accelerate existing state-of-the-art heuristic algorithms, allowing us to find larger independent sets faster on large real-world networks and synthetic instances. version:1
arxiv-1605-06814 | Space-Time Tradeoffs for Distributed Verification | http://arxiv.org/abs/1605.06814 | id:1605.06814 author:Rafail Ostrovsky, Mor Perry, Will Rosenbaum category:cs.DC cs.CC F.1.2; F.2.3  published:2016-05-22 summary:Verifying that a network configuration satisfies a given boolean predicate is a fundamental problem in distributed computing. Many variations of this problem have been studied, for example, in the context of proof labeling schemes (PLS), locally checkable proofs (LCP), and non-deterministic local decision (NLD). In all of these contexts, verification time is assumed to be constant. Korman, Kutten and Masuzawa [PODC 2011] presented a proof-labeling scheme for MST, with poly-logarithmic verification time, and logarithmic memory at each vertex. In this paper we introduce the notion of a $t$-PLS, which allows the verification procedure to run for super-constant time. Our work analyzes the tradeoffs of $t$-PLS between time, label size, message length, and computation space. We construct a universal $t$-PLS and prove that it uses the same amount of total communication as a known one-round universal PLS, and $t$ factor smaller labels. In addition, we provide a general technique to prove lower bounds for space-time tradeoffs of $t$-PLS. We use this technique to show an optimal tradeoff for testing that a network is acyclic (cycle free). Our optimal $t$-PLS for acyclicity uses label size and computation space $O((\log n)/t)$. We further describe a recursive $O(\log^* n)$ space verifier for acyclicity which does not assume previous knowledge of the run-time $t$. version:2
arxiv-1708-06067 | Finding shorter paths for robot arms using their redundancy | http://arxiv.org/abs/1708.06067 | id:1708.06067 author:Scott Paulin, Tom Botterill, XiaoQi Chen, Richard Green category:cs.RO  published:2017-08-21 summary:Many robot arms can accomplish one task using many different joint configurations. Often only one of these configurations is used as a goal by the path planner. Ideally the robot's path planner would be able to use the extra configurations to find higher quality paths. In this paper we use the extra goal configurations to find significantly shorter paths that are faster to execute compared to a planner that chooses one goal configuration arbitrarily. In a grape vine pruning robot arm experiment our proposed approach reduced execution times by 58%. version:1
arxiv-1708-06056 | Integrating asymptotically-optimal path planning with local optimization | http://arxiv.org/abs/1708.06056 | id:1708.06056 author:Scott Paulin, Tom Botterill, XiaoQi Chen, Richard Green category:cs.RO  published:2017-08-21 summary:Many robots operating in unpredictable environments require an online path planning algorithm that can quickly compute high quality paths. Asymptotically optimal planners are capable of finding the optimal path, but can be slow to converge. Local optimisation algorithms are capable of quickly improving a solution, but are not guaranteed to converge to the optimal solution. In this paper we develop a new way to integrate an asymptotically optimal planners with a local optimiser. We test our approach using RRTConnect* with a short-cutting local optimiser. Our approach results in a significant performance improvement when compared with the state-of-the-art RRTConnect* asymptotically optimal planner and computes paths that are 31\% faster to execute when both are given 3 seconds of planning time. version:1
arxiv-1708-03310 | Thinking, Fast and Slow: Combining Vector Spaces and Knowledge Graphs | http://arxiv.org/abs/1708.03310 | id:1708.03310 author:Sudip Mittal, Anupam Joshi, Tim Finin category:cs.AI  published:2017-08-10 summary:Knowledge graphs and vector space models are robust knowledge representation techniques with individual strengths and weaknesses. Vector space models excel at determining similarity between concepts, but are severely constrained when evaluating complex dependency relations and other logic-based operations that are a strength of knowledge graphs. We describe the VKG structure that helps unify knowledge graphs and vector representation of entities, and enables powerful inference methods and search capabilities that combine their complementary strengths. We analogize this to thinking `fast' in vector space along with thinking 'slow' and `deeply' by reasoning over the knowledge graph. We have created a query processing engine that takes complex queries and decomposes them into subqueries optimized to run on the respective knowledge graph or vector view of a VKG. We show that the VKG structure can process specific queries that are not efficiently handled by vector spaces or knowledge graphs alone. We also demonstrate and evaluate the VKG structure and the query processing engine by developing a system called Cyber-All-Intel for knowledge extraction, representation and querying in an end-to-end pipeline grounded in the cybersecurity informatics domain. version:2
arxiv-1708-06040 | Neural Block Sampling | http://arxiv.org/abs/1708.06040 | id:1708.06040 author:Tongzhou Wang, Yi Wu, David A. Moore, Stuart J. Russell category:cs.AI cs.LG stat.ML  published:2017-08-21 summary:Efficient Monte Carlo inference often requires manual construction of model-specific proposals. We propose an approach to automated proposal construction by training neural networks to provide fast approximations to block Gibbs conditionals. The learned proposals generalize to occurrences of common structural motifs both within a given model and across models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler's ability to escape local modes yields higher final F1 scores than single-site Gibbs. version:1
arxiv-1708-06039 | More cat than cute? Interpretable Prediction of Adjective-Noun Pairs | http://arxiv.org/abs/1708.06039 | id:1708.06039 author:Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier Giro-i-Nieto, Brendan Jou, Shih-Fu Chang category:cs.CV cs.AI cs.MM  published:2017-08-21 summary:The increasing availability of affect-rich multimedia resources has bolstered interest in understanding sentiment and emotions in and from visual content. Adjective-noun pairs (ANP) are a popular mid-level semantic construct for capturing affect via visually detectable concepts such as "cute dog" or "beautiful landscape". Current state-of-the-art methods approach ANP prediction by considering each of these compound concepts as individual tokens, ignoring the underlying relationships in ANPs. This work aims at disentangling the contributions of the `adjectives' and `nouns' in the visual prediction of ANPs. Two specialised classifiers, one trained for detecting adjectives and another for nouns, are fused to predict 553 different ANPs. The resulting ANP prediction model is more interpretable as it allows us to study contributions of the adjective and noun components. Source code and models are available at https://imatge-upc.github.io/affective-2017-musa2/ . version:1
arxiv-1708-06000 | Efficient Online Inference for Infinite Evolutionary Cluster models with Applications to Latent Social Event Discovery | http://arxiv.org/abs/1708.06000 | id:1708.06000 author:Wei Wei, Kennth Joseph, Kathleen Carley category:cs.AI cs.CL cs.SI  published:2017-08-20 summary:The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical method for modeling evolving clusters in large scale social media data. With the RCRP, one can allow both the number of clusters and the cluster parameters in a model to change over time. However, application of the RCRP has largely been limited due to the non-conjugacy between the cluster evolutionary priors and the Multinomial likelihood. This non-conjugacy makes inference di cult and restricts the scalability of models which use the RCRP, leading to the RCRP being applied only in simple problems, such as those that can be approximated by a single Gaussian emission. In this paper, we provide a novel solution for the non-conjugacy issues for the RCRP and an example of how to leverage our solution for one speci c problem - the social event discovery problem. By utilizing Sequential Monte Carlo methods in inference, our approach can be massively paralleled and is highly scalable, to the extent it can work on tens of millions of documents. We are able to generate high quality topical and location distributions of the clusters that can be directly interpreted as real social events, and our experimental results suggest that the approaches proposed achieve much better predictive performance than techniques reported in prior work. We also demonstrate how the techniques we develop can be used in a much more general ways toward similar problems. version:1
arxiv-1708-05965 | On the topology effects in wireless sensor networks based prognostics and health management | http://arxiv.org/abs/1708.05965 | id:1708.05965 author:Ahmad Farhat, Abdallah Makhoul, Christophe Guyeux, Rami Tawil, Ali Jaber, Abbas Hijazi category:cs.DC cs.CY  published:2017-08-20 summary:In this work, we consider the usage of wireless sensor networks (WSN) to monitor an area of interest, in order to diagnose on real time its state. Each sensor node forwards information about relevant features towards the sink where the data is processed. Nevertheless, energy conservation is a key issue in the design of such networks and once a sensor exhausts its resources, it will be dropped from the network. This will lead to broken links and data loss. It is therefore important to keep the network running for as long as possible by preserving the energy held by the nodes. Indeed, saving the quality of service (QoS) of a wireless sensor network for a long period is very important in order to ensure accurate data. Then, the area diagnosing will be more accurate. From another side, packet transmission is the phase that consumes the highest amount of energy comparing to other activities in the network. Therefore, we can see that the network topology has an important impact on energy efficiency, and thus on data and diagnosis accuracies. In this paper, we study and compare four network topologies: distributed, hierarchical, centralized, and decentralized topology and show their impact on the resulting estimation of diagnostics. We have used six diagnostic algorithms, to evaluate both prognostic and health management with the variation of type of topology in WSN. version:1
arxiv-1512-02456 | A study of Time-varying Cost Parameter Estimation Methods in Traffic Networks for Mobile Robots | http://arxiv.org/abs/1512.02456 | id:1512.02456 author:Pragna Das, Lluís Ribas Xirgo category:cs.RO  published:2015-12-08 summary:Industrial robust controlling systems built using automated guided vehicles (AGVs) requires planning which depends on cost parameters like time and energy of the mobile robots functioning in the system. This work addresses the problem of on-line traversal time identification and estimation for proper mobility of mobile robots on systems' traffic networks. Several filtering and estimation methods have been investigated with respect to proper identification of traversal time of arcs of systems' transportation graphs. We have found that traversal times vary along time due to a variety of factors, including the battery charge of the robot, and that the best method to predict the next value must account not only for these but for a high variance. Results show that path planning and navigation for each of the mobile robots can be made much more efficient with this approach. version:2
arxiv-1708-05935 | Software-Defined Robotics -- Idea & Approach | http://arxiv.org/abs/1708.05935 | id:1708.05935 author:Ali Al-Bayaty category:cs.RO cs.AI I.2.9; I.2.11  published:2017-08-20 summary:The methodology of Software-Defined Robotics hierarchical-based and stand-alone framework can be designed and implemented to program and control different sets of robots, regardless of their manufacturers' parameters and specifications, with unified commands and communications. This framework approach will increase the capability of (re)programming a specific group of robots during the runtime without affecting the others as desired in the critical missions and industrial operations, expand the shared bandwidth, enhance the reusability of code, leverage the computational processing power, decrease the unnecessary analyses of vast supplemental electrical components for each robot, as well as get advantages of the most state-of-the-art industrial trends in the cloud-based computing, Virtual Machines (VM), and Robot-as-a-Service (RaaS) technologies. version:1
arxiv-1708-04754 | Specification and Implementation of Replicated List: The Jupiter Protocol Revisited | http://arxiv.org/abs/1708.04754 | id:1708.04754 author:Hengfeng Wei, Yu Huang, Jian Lu category:cs.DC  published:2017-08-16 summary:The replicated list object has been frequently used to model the core functionality (e.g., insertion, deletion, and read) of replicated collaborative text editing systems. In this paper we revisit the specification and implementation of a replicated list object, specifically the weak list specification proposed recently by Attiya et al. and the Jupiter protocol designed in the 1990s. We prove that Jupiter indeed satisfies the weak list specification, solving the conjecture of Attiya et al. To address the mismatch between the global property of weak list specification and the local views each replica maintains in the Jupiter protocol, we propose the CSS (Compact State-space) Jupiter protocol, which at a high level, maintains only a single novel $n$-ary ordered state-space for a client/server system with $n$ clients. By contrast, the original Jupiter protocol, we call the CSCW protocol, needs to maintain $2n$ 2D state-spaces where replica states are dispersed. We first show that the CSS protocol and the CSCW protocol are equivalent in the sense that their behaviors are the same under the same schedule of operations/messages. Then, we prove that the CSS protocol satisfies the weak list specification. We further extend the CSS protocol to a distributed setting, by orthogonally integrating the compact $n$-ary ordered state-space with a distributed scheme to totally order operations. version:2
arxiv-1708-05930 | Solving a New 3D Bin Packing Problem with Deep Reinforcement Learning Method | http://arxiv.org/abs/1708.05930 | id:1708.05930 author:Haoyuan Hu, Xiaodong Zhang, Xiaowei Yan, Longfei Wang, Yinghui Xu category:cs.AI  published:2017-08-20 summary:In this paper, a new type of 3D bin packing problem (BPP) is proposed, in which a number of cuboid-shaped items must be put into a bin one by one orthogonally. The objective is to find a way to place these items that can minimize the surface area of the bin. This problem is based on the fact that there is no fixed-sized bin in many real business scenarios and the cost of a bin is proportional to its surface area. Our research shows that this problem is NP-hard. Based on previous research on 3D BPP, the surface area is determined by the sequence, spatial locations and orientations of items. Among these factors, the sequence of items plays a key role in minimizing the surface area. Inspired by recent achievements of deep reinforcement learning (DRL) techniques, especially Pointer Network, on combinatorial optimization problems such as TSP, a DRL-based method is applied to optimize the sequence of items to be packed into the bin. Numerical results show that the method proposed in this paper achieve about 5% improvement than heuristic method. version:1
arxiv-1708-06345 | Robust Optimal Planning and Control of Non-Periodic Bipedal Locomotion with A Centroidal Momentum Model | http://arxiv.org/abs/1708.06345 | id:1708.06345 author:Ye Zhao, Benito R. Fernandez, Luis Sentis category:cs.RO cs.SY  published:2017-08-19 summary:This study presents a theoretical method for planning and controlling agile bipedal locomotion based on robustly tracking a set of non-periodic keyframe states. Based on centroidal momentum dynamics, we formulate a hybrid phase-space planning and control method which includes the following key components: (i) a step transition solver that enables dynamically tracking non-periodic keyframe states over various types of terrains, (ii) a robust hybrid automaton to effectively formulate planning and control algorithms, (iii) a steering direction model to control the robot's heading, (iv) a phase-space metric to measure distance to the planned locomotion manifolds, and (v) a hybrid control method based on the previous distance metric to produce robust dynamic locomotion under external disturbances. Compared to other locomotion methodologies, we have a large focus on non-periodic gait generation and robustness metrics to deal with disturbances. Such focus enables the proposed control method to robustly track non-periodic keyframe states over various challenging terrains and under external disturbances as illustrated through several simulations. version:1
arxiv-1708-05875 | A novel agent-based simulation framework for sensing in complex adaptive environments | http://arxiv.org/abs/1708.05875 | id:1708.05875 author:Muaz A. Niazi, Amir Hussain category:cs.NI cs.AI cs.MA cs.SE nlin.AO  published:2017-08-19 summary:In this paper we present a novel Formal Agent-Based Simulation framework (FABS). FABS uses formal specification as a means of clear description of wireless sensor networks (WSN) sensing a Complex Adaptive Environment. This specification model is then used to develop an agent-based model of both the wireless sensor network as well as the environment. As proof of concept, we demonstrate the application of FABS to a boids model of self-organized flocking of animals monitored by a random deployment of proximity sensors. version:1
arxiv-1708-05872 | Agent-based computing from multi-agent systems to agent-based Models: a visual survey | http://arxiv.org/abs/1708.05872 | id:1708.05872 author:Muaz A. Niazi, Amir Hussain category:cs.SI cs.AI cs.DL cs.MA nlin.AO  published:2017-08-19 summary:Agent-Based Computing is a diverse research domain concerned with the building of intelligent software based on the concept of "agents". In this paper, we use Scientometric analysis to analyze all sub-domains of agent-based computing. Our data consists of 1,064 journal articles indexed in the ISI web of knowledge published during a twenty year period: 1990-2010. These were retrieved using a topic search with various keywords commonly used in sub-domains of agent-based computing. In our proposed approach, we have employed a combination of two applications for analysis, namely Network Workbench and CiteSpace - wherein Network Workbench allowed for the analysis of complex network aspects of the domain, detailed visualization-based analysis of the bibliographic data was performed using CiteSpace. Our results include the identification of the largest cluster based on keywords, the timeline of publication of index terms, the core journals and key subject categories. We also identify the core authors, top countries of origin of the manuscripts along with core research institutes. Finally, our results have interestingly revealed the strong presence of agent-based computing in a number of non-computing related scientific domains including Life Sciences, Ecological Sciences and Social Sciences. version:1
arxiv-1708-05855 | Practical Distance Functions for Path-Planning in Planar Domains | http://arxiv.org/abs/1708.05855 | id:1708.05855 author:Renjie Chen, Craig Gotsman, Kai Hormann category:cs.RO 31A15  68T40  published:2017-08-19 summary:Path planning is an important problem in robotics. One way to plan a path between two points $x,y$ within a (not necessarily simply-connected) planar domain $\Omega$, is to define a non-negative distance function $d(x,y)$ on $\Omega\times\Omega$ such that following the (descending) gradient of this distance function traces such a path. This presents two equally important challenges: A mathematical challenge -- to define $d$ such that $d(x,y)$ has a single minimum for any fixed $y$ (and this is when $x=y$), since a local minimum is in effect a "dead end", A computational challenge -- to define $d$ such that it may be computed efficiently. In this paper, given a description of $\Omega$, we show how to assign coordinates to each point of $\Omega$ and define a family of distance functions between points using these coordinates, such that both the mathematical and the computational challenges are met. This is done using the concepts of \emph{harmonic measure} and \emph{$f$-divergences}. In practice, path planning is done on a discrete network defined on a finite set of \emph{sites} sampled from $\Omega$, so any method that works well on the continuous domain must be adapted so that it still works well on the discrete domain. Given a set of sites sampled from $\Omega$, we show how to define a network connecting these sites such that a \emph{greedy routing} algorithm (which is the discrete equivalent of continuous gradient descent) based on the distance function mentioned above is guaranteed to generate a path in the network between any two such sites. In many cases, this network is close to a (desirable) planar graph, especially if the set of sites is dense. version:1
arxiv-1708-05840 | A Data and Model-Parallel, Distributed and Scalable Framework for Training of Deep Networks in Apache Spark | http://arxiv.org/abs/1708.05840 | id:1708.05840 author:Disha Shrivastava, Santanu Chaudhury, Dr. Jayadeva category:stat.ML cs.AI cs.CV cs.DC cs.LG  published:2017-08-19 summary:Training deep networks is expensive and time-consuming with the training period increasing with data size and growth in model parameters. In this paper, we provide a framework for distributed training of deep networks over a cluster of CPUs in Apache Spark. The framework implements both Data Parallelism and Model Parallelism making it suitable to use for deep networks which require huge training data and model parameters which are too big to fit into the memory of a single machine. It can be scaled easily over a cluster of cheap commodity hardware to attain significant speedup and obtain better results making it quite economical as compared to farm of GPUs and supercomputers. We have proposed a new algorithm for training of deep networks for the case when the network is partitioned across the machines (Model Parallelism) along with detailed cost analysis and proof of convergence of the same. We have developed implementations for Fully-Connected Feedforward Networks, Convolutional Neural Networks, Recurrent Neural Networks and Long Short-Term Memory architectures. We present the results of extensive simulations demonstrating the speedup and accuracy obtained by our framework for different sizes of the data and model parameters with variation in the number of worker cores/partitions; thereby showing that our proposed framework can achieve significant speedup (upto 11X for CNN) and is also quite scalable. version:1
arxiv-1708-05824 | Applying Deep Bidirectional LSTM and Mixture Density Network for Basketball Trajectory Prediction | http://arxiv.org/abs/1708.05824 | id:1708.05824 author:Yu Zhao, Rennong Yang, Guillaume Chevalier, Rajiv Shah, Rob Romijnders category:cs.AI  published:2017-08-19 summary:Data analytics helps basketball teams to create tactics. However, manual data collection and analytics are costly and ineffective. Therefore, we applied a deep bidirectional long short-term memory (BLSTM) and mixture density network (MDN) approach. This model is not only capable of predicting a basketball trajectory based on real data, but it also can generate new trajectory samples. It is an excellent application to help coaches and players decide when and where to shoot. Its structure is particularly suitable for dealing with time series problems. BLSTM receives forward and backward information at the same time, while stacking multiple BLSTMs further increases the learning ability of the model. Combined with BLSTMs, MDN is used to generate a multi-modal distribution of outputs. Thus, the proposed model can, in principle, represent arbitrary conditional probability distributions of output variables. We tested our model with two experiments on three-pointer datasets from NBA SportVu data. In the hit-or-miss classification experiment, the proposed model outperformed other models in terms of the convergence speed and accuracy. In the trajectory generation experiment, eight model-generated trajectories at a given time closely matched real trajectories. version:1
arxiv-1708-05714 | A Stronger Foundation for Computer Science and P=NP | http://arxiv.org/abs/1708.05714 | id:1708.05714 author:Mark Inman category:cs.CC cs.AI cs.LO  published:2017-08-18 summary:This article constructs a Turing Machine which can solve for $\beta^{'}$ which is RE-complete. Such a machine is only possible if there is something wrong with the foundations of computer science and mathematics. We therefore check our work by looking very closely at Cantor's diagonalization and construct a novel formal language as an Abelian group which allows us, through equivalence relations, to provide a non-trivial counterexample to Cantor's argument. As if that wasn't enough, we then discover that the impredicative nature of G\"odel's diagonalization lemma leads to logical tautology, invalidating any meaning behind the method, leaving no doubt that diagonalization is flawed. Our discovery in regards to these foundational arguments opens the door to solving the P vs NP problem. version:1
arxiv-1708-05746 | Sparkle: Optimizing Spark for Large Memory Machines and Analytics | http://arxiv.org/abs/1708.05746 | id:1708.05746 author:Mijung Kim, Jun Li, Haris Volos, Manish Marwah, Alexander Ulanov, Kimberly Keeton, Joseph Tucek, Lucy Cherkasova, Le Xu, Pradeep Fernando category:cs.DC  published:2017-08-18 summary:Spark is an in-memory analytics platform that targets commodity server environments today. It relies on the Hadoop Distributed File System (HDFS) to persist intermediate checkpoint states and final processing results. In Spark, immutable data are used for storing data updates in each iteration, making it inefficient for long running, iterative workloads. A non-deterministic garbage collector further worsens this problem. Sparkle is a library that optimizes memory usage in Spark. It exploits large shared memory to achieve better data shuffling and intermediate storage. Sparkle replaces the current TCP/IP-based shuffle with a shared memory approach and proposes an off-heap memory store for efficient updates. We performed a series of experiments on scale-out clusters and scale-up machines. The optimized shuffle engine leveraging shared memory provides 1.3x to 6x faster performance relative to Vanilla Spark. The off-heap memory store along with the shared-memory shuffle engine provides more than 20x performance increase on a probabilistic graph processing workload that uses a large-scale real-world hyperlink graph. While Sparkle benefits at most from running on large memory machines, it also achieves 1.6x to 5x performance improvements over scale out cluster with equivalent hardware setting. version:1
arxiv-1708-06665 | Software engineering and the SP theory of intelligence | http://arxiv.org/abs/1708.06665 | id:1708.06665 author:J Gerard Wolff category:cs.SE cs.AI  published:2017-08-18 summary:This paper describes a novel approach to software engineering derived from the "SP theory of intelligence" and its realisation in the "SP computer model". These are the bases of a projected industrial-strength "SP machine" which, when mature, is anticipated to be the vehicle for software engineering as described in this paper. Potential benefits of this new approach to software engineering include: the automation of semi-automation of software development, with non-automatic programming of the SP system where necessary; allowing programmers to concentrate on 'real-world' parallelism, without worries about parallelism to speed up processing; the ambitious long-term goal of programming the SP system via written or spoken natural language; reducing or eliminating the distinction between 'design' and 'implementation'; reducing or eliminating operations like compiling or interpretation; reducing or eliminating the need for verification of software; reducing the need for an explicit process of validation of software; no formal distinction between program and database; potential for substantial reductions in the number of types of data file and the number of computer languages; benefits for version control; and reducing technical debt. version:1
arxiv-1708-05732 | Security, Privacy and Safety Evaluation of Dynamic and Static Fleets of Drones | http://arxiv.org/abs/1708.05732 | id:1708.05732 author:Raja Naeem Akram, Konstantinos Markantonakis, Keith Mayes, Oussama Habachi, Damien Sauveron, Andreas Steyven, Serge Chaumette category:cs.CR cs.AI cs.NE cs.RO  published:2017-08-18 summary:Inter-connected objects, either via public or private networks are the near future of modern societies. Such inter-connected objects are referred to as Internet-of-Things (IoT) and/or Cyber-Physical Systems (CPS). One example of such a system is based on Unmanned Aerial Vehicles (UAVs). The fleet of such vehicles are prophesied to take on multiple roles involving mundane to high-sensitive, such as, prompt pizza or shopping deliveries to your homes to battlefield deployment for reconnaissance and combat missions. Drones, as we refer to UAVs in this paper, either can operate individually (solo missions) or part of a fleet (group missions), with and without constant connection with the base station. The base station acts as the command centre to manage the activities of the drones. However, an independent, localised and effective fleet control is required, potentially based on swarm intelligence, for the reasons: 1) increase in the number of drone fleets, 2) number of drones in a fleet might be multiple of tens, 3) time-criticality in making decisions by such fleets in the wild, 4) potential communication congestions/lag, and 5) in some cases working in challenging terrains that hinders or mandates-limited communication with control centre (i.e., operations spanning long period of times or military usage of such fleets in enemy territory). This self-ware, mission-focused and independent fleet of drones that potential utilises swarm intelligence for a) air-traffic and/or flight control management, b) obstacle avoidance, c) self-preservation while maintaining the mission criteria, d) collaboration with other fleets in the wild (autonomously) and e) assuring the security, privacy and safety of physical (drones itself) and virtual (data, software) assets. In this paper, we investigate the challenges faced by fleet of drones and propose a potential course of action on how to overcome them. version:1
arxiv-1603-02339 | Distributed TensorFlow with MPI | http://arxiv.org/abs/1603.02339 | id:1603.02339 author:Abhinav Vishnu, Charles Siegel, Jeffrey Daily category:cs.DC  published:2016-03-07 summary:Machine Learning and Data Mining (MLDM) algorithms are becoming increasingly important in analyzing large volume of data generated by simulations, experiments and mobile devices. With increasing data volume, distributed memory systems (such as tightly connected supercomputers or cloud computing systems) are becoming important in designing in-memory and massively parallel MLDM algorithms. Yet, the majority of open source MLDM software is limited to sequential execution with a few supporting multi-core/many-core execution. In this paper, we extend recently proposed Google TensorFlow for execution on large scale clusters using Message Passing Interface (MPI). Our approach requires minimal changes to the TensorFlow runtime -- making the proposed implementation generic and readily usable to increasingly large users of TensorFlow. We evaluate our implementation using an InfiniBand cluster and several well knowndatasets. Our evaluation indicates the efficiency of our proposed implementation. version:2
arxiv-1708-05680 | Asymptotic Analysis of Plausible Tree Hash Modes for SHA-3 | http://arxiv.org/abs/1708.05680 | id:1708.05680 author:Kevin Atighehchi, Alexis Bonnecaze category:cs.CR cs.DC  published:2017-08-18 summary:Discussions about the choice of a tree hash mode of operation for a standardization have recently been undertaken. It appears that a single tree mode cannot address adequately all possible uses and specifications of a system. In this paper, we review the tree modes which have been proposed, we discuss their problems and propose remedies. We make the reasonable assumption that communicating systems have different specifications and that software applications are of different types (securing stored content or live-streamed content). Finally, we propose new modes of operation that address the resource usage problem for the three most representative categories of devices and we analyse their asymptotic behavior. version:1
arxiv-1708-05629 | Learning to Transfer | http://arxiv.org/abs/1708.05629 | id:1708.05629 author:Ying Wei, Yu Zhang, Qiang Yang category:cs.AI cs.LG stat.ML  published:2017-08-18 summary:Transfer learning borrows knowledge from a source domain to facilitate learning in a target domain. Two primary issues to be addressed in transfer learning are what and how to transfer. For a pair of domains, adopting different transfer learning algorithms results in different knowledge transferred between them. To discover the optimal transfer learning algorithm that maximally improves the learning performance in the target domain, researchers have to exhaustively explore all existing transfer learning algorithms, which is computationally intractable. As a trade-off, a sub-optimal algorithm is selected, which requires considerable expertise in an ad-hoc way. Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices. Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences. We establish the L2T framework in two stages: 1) we first learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer for a newly arrived pair of domains by optimizing the reflection function. Extensive experiments demonstrate the L2T's superiority over several state-of-the-art transfer learning algorithms and its effectiveness on discovering more transferable knowledge. version:1
arxiv-1708-06252 | Mixture Reduction on Matrix Lie Groups | http://arxiv.org/abs/1708.06252 | id:1708.06252 author:Josip Cesic, Ivan Markovic, Ivan Petrovic category:cs.SY cs.RO  published:2017-08-18 summary:Many physical systems evolve on matrix Lie groups and mixture filtering designed for such manifolds represent an inevitable tool for challenging estimation problems. However, mixture filtering faces the issue of a constantly growing number of components, hence require appropriate mixture reduction techniques. In this letter we propose a mixture reduction approach for distributions on matrix Lie groups, called the concentrated Gaussian distributions (CGDs). This entails appropriate reparametrization of CGD parameters to compute the KL divergence, pick and merge the mixture components. Furthermore, we also introduce a multitarget tracking filter on Lie groups as a mixture filtering study example for the proposed reduction method. In particular, we implemented the probability hypothesis density filter on matrix Lie groups. We validate the filter performance using the optimal subpattern assignment metric on a synthetic dataset consisting of 100 randomly generated multitarget scenarios. version:1
arxiv-1708-05563 | Induction of Decision Trees based on Generalized Graph Queries | http://arxiv.org/abs/1708.05563 | id:1708.05563 author:Pedro Almagro-Blanco, Fernando Sancho-Caparrini category:cs.LG cs.AI 68T05 I.2.6; H.2.8  published:2017-08-18 summary:Usually, decision tree induction algorithms are limited to work with non relational data. Given a record, they do not take into account other objects attributes even though they can provide valuable information for the learning task. In this paper we present GGQ-ID3, a multi-relational decision tree learning algorithm that uses Generalized Graph Queries (GGQ) as predicates in the decision nodes. GGQs allow to express complex patterns (including cycles) and they can be refined step-by-step. Also, they can evaluate structures (not only single records) and perform Regular Pattern Matching. GGQ are built dynamically (pattern mining) during the GGQ-ID3 tree construction process. We will show how to use GGQ-ID3 to perform multi-relational machine learning keeping complexity under control. Finally, some real examples of automatically obtained classification trees and semantic patterns are shown. ----- Normalmente, los algoritmos de inducci\'on de \'arboles de decisi\'on trabajan con datos no relacionales. Dado un registro, no tienen en cuenta los atributos de otros objetos a pesar de que \'estos pueden proporcionar informaci\'on \'util para la tarea de aprendizaje. En este art\'iculo presentamos GGQ-ID3, un algoritmo de aprendizaje de \'arboles de decisiones multi-relacional que utiliza Generalized Graph Queries (GGQ) como predicados en los nodos de decisi\'on. Los GGQs permiten expresar patrones complejos (incluyendo ciclos) y pueden ser refinados paso a paso. Adem\'as, pueden evaluar estructuras (no solo registros) y llevar a cabo Regular Pattern Matching. En GGQ-ID3, los GGQ son construidos din\'amicamente (pattern mining) durante el proceso de construcci\'on del \'arbol. Adem\'as, se muestran algunos ejemplos reales de \'arboles de clasificaci\'on multi-relacionales y patrones sem\'anticos obtenidos autom\'aticamente. version:1
arxiv-1708-05551 | On wrapping the Kalman filter and estimating with the SO(2) group | http://arxiv.org/abs/1708.05551 | id:1708.05551 author:Ivan Markovic, Josip Cesic, Ivan Petrovic category:cs.RO  published:2017-08-18 summary:This paper analyzes directional tracking in 2D with the extended Kalman filter on Lie groups (LG-EKF). The study stems from the problem of tracking objects moving in 2D Euclidean space, with the observer measuring direction only, thus rendering the measurement space and object position on the circle---a non-Euclidean geometry. The problem is further inconvenienced if we need to include higher-order dynamics in the state space, like angular velocity which is a Euclidean variables. The LG-EKF offers a solution to this issue by modeling the state space as a Lie group or combination thereof, e.g., SO(2) or its combinations with Rn. In the present paper, we first derive the LG-EKF on SO(2) and subsequently show that this derivation, based on the mathematically grounded framework of filtering on Lie groups, yields the same result as heuristically wrapping the angular variable within the EKF framework. This result applies only to the SO(2) and SO(2)xRn LG-EKFs and is not intended to be extended to other Lie groups or combinations thereof. In the end, we showcase the SO(2)xR2 LG-EKF, as an example of a constant angular acceleration model, on the problem of speaker tracking with a microphone array for which real-world experiments are conducted and accuracy is evaluated with ground truth data obtained by a motion capture system. version:1
arxiv-1708-05548 | Moving object tracking employing rigid body motion on matrix Lie groups | http://arxiv.org/abs/1708.05548 | id:1708.05548 author:Josip Cesic, Ivan Markovic, Ivan Petrovic category:cs.RO  published:2017-08-18 summary:In this paper we propose a novel method for estimating rigid body motion by modeling the object state directly in the space of the rigid body motion group SE(2). It has been recently observed that a noisy manoeuvring object in SE(2) exhibits banana-shaped probability density contours in its pose. For this reason, we propose and investigate two state space models for moving object tracking: (i) a direct product SE(2)xR3 and (ii) a direct product of the two rigid body motion groups SE(2)xSE(2). The first term within these two state space constructions describes the current pose of the rigid body, while the second one employs its second order dynamics, i.e., the velocities. By this, we gain the flexibility of tracking omnidirectional motion in the vein of a constant velocity model, but also accounting for the dynamics in the rotation component. Since the SE(2) group is a matrix Lie group, we solve this problem by using the extended Kalman filter on matrix Lie groups and provide a detailed derivation of the proposed filters. We analyze the performance of the filters on a large number of synthetic trajectories and compare them with (i) the extended Kalman filter based constant velocity and turn rate model and (ii) the linear Kalman filter based constant velocity model. The results show that the proposed filters outperform the other two filters on a wide spectrum of types of motion. version:1
arxiv-1708-04846 | Maximum A Posteriori Inference in Sum-Product Networks | http://arxiv.org/abs/1708.04846 | id:1708.04846 author:Jun Mei, Yong Jiang, Kewei Tu category:cs.AI  published:2017-08-16 summary:Sum-product networks (SPNs) are a class of probabilistic graphical models that allow tractable marginal inference. However, the maximum a posteriori (MAP) inference in SPNs is NP-hard. We investigate MAP inference in SPNs from both theoretical and algorithmic perspectives. For the theoretical part, we reduce general MAP inference to its special case without evidence and hidden variables; we also show that it is NP-hard to approximate the MAP problem to $2^{n^\epsilon}$ for fixed $0 \leq \epsilon < 1$, where $n$ is the input size. For the algorithmic part, we first present an exact MAP solver that runs reasonably fast and could handle SPNs with up to 1k variables and 150k arcs in our experiments. We then present a new approximate MAP solver with a good balance between speed and accuracy, and our comprehensive experiments on real-world datasets show that it has better overall performance than existing approximate solvers. version:2
arxiv-1708-05522 | Exploring Directional Path-Consistency for Solving Constraint Networks | http://arxiv.org/abs/1708.05522 | id:1708.05522 author:Shufeng Kong, Sanjiang Li, Michael Sioutis category:cs.AI  published:2017-08-18 summary:Among the local consistency techniques used for solving constraint networks, path-consistency (PC) has received a great deal of attention. However, enforcing PC is computationally expensive and sometimes even unnecessary. Directional path-consistency (DPC) is a weaker notion of PC that considers a given variable ordering and can thus be enforced more efficiently than PC. This paper shows that DPC (the DPC enforcing algorithm of Dechter and Pearl) decides the constraint satisfaction problem (CSP) of a constraint language if it is complete and has the variable elimination property (VEP). However, we also show that no complete VEP constraint language can have a domain with more than 2 values. We then present a simple variant of the DPC algorithm, called DPC*, and show that the CSP of a constraint language can be decided by DPC* if it is closed under a majority operation. In fact, DPC* is sufficient for guaranteeing backtrack-free search for such constraint networks. Examples of majority-closed constraint classes include the classes of connected row-convex (CRC) constraints and tree-preserving constraints, which have found applications in various domains, such as scene labeling, temporal reasoning, geometric reasoning, and logical filtering. Our experimental evaluations show that DPC* significantly outperforms the state-of-the-art algorithms for solving majority-closed constraints. version:1
arxiv-1708-05514 | Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR and Panoramic Camera Using a Printed Chessboard | http://arxiv.org/abs/1708.05514 | id:1708.05514 author:Weimin Wang, Ken Sakurada, Nobuo Kawaguchi category:cs.CV cs.RO  published:2017-08-18 summary:This paper presents a novel method for fully automatic and convenient extrinsic calibration of a 3D LiDAR and a panoramic camera with a normally printed chessboard. The proposed method is based on the 3D corner estimation of the chessboard from the sparse point cloud generated by one frame scan of the LiDAR. To estimate the corners, we formulate a full-scale model of the chessboard and fit it to the segmented 3D points of the chessboard. The model is fitted by optimizing the cost function under constraints of correlation between the reflectance intensity of laser and the color of the chessboard's patterns. Powell's method is introduced for resolving the discontinuity problem in optimization. The corners of the fitted model are considered as the 3D corners of the chessboard. Once the corners of the chessboard in the 3D point cloud are estimated, the extrinsic calibration of the two sensors is converted to a 3D-2D matching problem. The corresponding 3D-2D points are used to calculate the absolute pose of the two sensors with Unified Perspective-n-Point (UPnP). Further, the calculated parameters are regarded as initial values and are refined using the Levenberg-Marquardt method. The performance of the proposed corner detection method from the 3D point cloud is evaluated using simulations. The results of experiments, conducted on a Velodyne HDL-32e LiDAR and a Ladybug3 camera under the proposed re-projection error metric, qualitatively and quantitatively demonstrate the accuracy and stability of the final extrinsic calibration parameters. version:1
arxiv-1708-05448 | On Ensuring that Intelligent Machines Are Well-Behaved | http://arxiv.org/abs/1708.05448 | id:1708.05448 author:Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, Emma Brunskill category:cs.AI  published:2017-08-17 summary:Machine learning algorithms are everywhere, ranging from simple data analysis and pattern recognition tools used across the sciences to complex systems that achieve super-human performance on various tasks. Ensuring that they are well-behaved---that they do not, for example, cause harm to humans or act in a racist or sexist way---is therefore not a hypothetical problem to be dealt with in the future, but a pressing one that we address here. We propose a new framework for designing machine learning algorithms that simplifies the problem of specifying and regulating undesirable behaviors. To show the viability of this new framework, we use it to create new machine learning algorithms that preclude the sexist and harmful behaviors exhibited by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning. version:1
arxiv-1708-05425 | A Heuristic Approach to Protocol Tuning for High Performance Data Transfers | http://arxiv.org/abs/1708.05425 | id:1708.05425 author:Engin Arslan, Tevfik Kosar category:cs.DC  published:2017-08-17 summary:Obtaining optimal data transfer performance is of utmost importance to today's data-intensive distributed applications and wide-area data replication services. Doing so necessitates effectively utilizing available network bandwidth and resources, yet in practice transfers seldom reach the levels of utilization they potentially could. Tuning protocol parameters such as pipelining, parallelism, and concurrency can significantly increase utilization and performance, however determining the best settings for these parameters is a difficult problem, as network conditions can vary greatly between sites and over time. Nevertheless, it is an important problem, since poor tuning can cause either under- or over-utilization of network resources and thus degrade transfer performance. In this paper, we present three algorithms for application-level tuning of different protocol parameters for maximizing transfer throughput in wide-area networks. Our algorithms dynamically tune the number of parallel data streams per file (for large file optimization), the level of control channel pipelining (for small file optimization), and the number of concurrent file transfers to increase I/O throughput (a technique useful for all types of files). The proposed heuristic algorithms improve the transfer throughput up to 10x compared to the baseline and 7x compared to the state of the art solutions. version:1
arxiv-1708-05357 | Efficient Use of Limited-Memory Resources to Accelerate Linear Learning | http://arxiv.org/abs/1708.05357 | id:1708.05357 author:Celestine Dünner, Thomas Parnell, Martin Jaggi category:cs.LG cs.DC math.OC stat.ML 90C25  68W15  68W10 G.1.6; C.1.4  published:2017-08-17 summary:We propose a generic algorithmic building block to accelerate training of machine learning models on heterogenous compute systems. The scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique builds upon primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. We provide a strong theoretical motivation for our gap-based selection scheme and provide an efficient practical implementation thereof. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on large scale datasets exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches. version:1
arxiv-1708-05346 | General AI Challenge - Round One: Gradual Learning | http://arxiv.org/abs/1708.05346 | id:1708.05346 author:Jan Feyereisl, Matej Nikl, Martin Poliak, Martin Stransky, Michal Vlasak category:cs.AI  published:2017-08-17 summary:The General AI Challenge is an initiative to encourage the wider artificial intelligence community to focus on important problems in building intelligent machines with more general scope than is currently possible. The challenge comprises of multiple rounds, with the first round focusing on gradual learning, i.e. the ability to re-use already learned knowledge for efficiently learning to solve subsequent problems. In this article, we will present details of the first round of the challenge, its inspiration and aims. We also outline a more formal description of the challenge and present a preliminary analysis of its curriculum, based on ideas from computational mechanics. We believe, that such formalism will allow for a more principled approach towards investigating tasks in the challenge, building new curricula and for potentially improving consequent challenge rounds. version:1
arxiv-1708-05327 | Analysis of Static and Dynamic Configurability of Existing Group Communication Systems | http://arxiv.org/abs/1708.05327 | id:1708.05327 author:Johannes Köstler, Hans P. Reiser category:cs.DC  published:2017-08-17 summary:Active replication following the state machine replication (SMR) approach is a way to make existing systems and services more reliable and fault-tolerant. The additional communication overhead has a negative impact on the system's throughput and overall request latency. Today's systems should be highly optimized to their execution environment and usage scenario in order to remedy the performance loss introduced by such group communication systems (GCS). In addition to that, systems should be able to adapt to changing environmental conditions. This report analyzes the available configuration options of three existing GCSs. Therefore, it explains the available configuration parameters and describes the given reconfiguration mechanisms. The found parameters are then classified in a parameter scheme. version:1
arxiv-1708-05325 | Learning Musical Relations using Gated Autoencoders | http://arxiv.org/abs/1708.05325 | id:1708.05325 author:Stefan Lattner, Maarten Grachten, Gerhard Widmer category:cs.SD cs.AI cs.LG  published:2017-08-17 summary:Music is usually highly structured and it is still an open question how to design models which can successfully learn to recognize and represent musical structure. A fundamental problem is that structurally related patterns can have very distinct appearances, because the structural relationships are often based on transformations of musical material, like chromatic or diatonic transposition, inversion, retrograde, or rhythm change. In this preliminary work, we study the potential of two unsupervised learning techniques - Restricted Boltzmann Machines (RBMs) and Gated Autoencoders (GAEs) - to capture pre-defined transformations from constructed data pairs. We evaluate the models by using the learned representations as inputs in a discriminative task where for a given type of transformation (e.g. diatonic transposition), the specific relation between two musical patterns must be recognized (e.g. an upward transposition of diatonic steps). Furthermore, we measure the reconstruction error of models when reconstructing musical transformed patterns. Lastly, we test the models in an analogy-making task. We find that it is difficult to learn musical transformations with the RBM and that the GAE is much more adequate for this task, since it is able to learn representations of specific transformations that are largely content-invariant. We believe these results show that models such as GAEs may provide the basis for more encompassing music analysis systems, by endowing them with a better understanding of the structures underlying music. version:1
arxiv-1708-05264 | Design, Configuration, Implementation, and Performance of a Simple 32 Core Raspberry Pi Cluster | http://arxiv.org/abs/1708.05264 | id:1708.05264 author:Vincent A. Cicirello category:cs.DC  published:2017-08-17 summary:In this report, I describe the design and implementation of an inexpensive, eight node, 32 core, cluster of raspberry pi single board computers, as well as the performance of this cluster on two computational tasks, one that requires significant data transfer relative to computational time requirements, and one that does not. We have two use-cases for the cluster: (a) as an educational tool for classroom usage, such as covering parallel algorithms in an algorithms course; and (b) as a test system for use during the development of parallel metaheuristics, essentially serving as a personal desktop parallel computing cluster. Our preliminary results show that the slow 100 Mbps networking of the raspberry pi significantly limits such clusters to parallel computational tasks that are either long running relative to data communications requirements, or that which requires very little internode communications. Additionally, although the raspberry pi 3 has a quad-core processor, parallel speedup degrades during attempts to utilize all four cores of all cluster nodes for a parallel computation, likely due to resource contention with operating system level processes. However, distributing a task across three cores of each cluster node does enable linear (or near linear) speedup. version:1
arxiv-1708-05688 | Human Uncertainty and Ranking Error -- The Secret of Successful Evaluation in Predictive Data Mining | http://arxiv.org/abs/1708.05688 | id:1708.05688 author:Kevin Jasberg, Sergej Sizov category:cs.HC cs.AI  published:2017-08-17 summary:One of the most crucial issues in data mining is to model human behaviour in order to provide personalisation, adaptation and recommendation. This usually involves implicit or explicit knowledge, either by observing user interactions, or by asking users directly. But these sources of information are always subject to the volatility of human decisions, making utilised data uncertain to a particular extent. In this contribution, we elaborate on the impact of this human uncertainty when it comes to comparative assessments of different data mining approaches. In particular, we reveal two problems: (1) biasing effects on various metrics of model-based prediction and (2) the propagation of uncertainty and its thus induced error probabilities for algorithm rankings. For this purpose, we introduce a probabilistic view and prove the existence of those problems mathematically, as well as provide possible solution strategies. We exemplify our theory mainly in the context of recommender systems along with the metric RMSE as a prominent example of precision quality measures. version:1
arxiv-1509-01423 | Optical-Flow based Self-Supervised Learning of Obstacle Appearance applied to MAV Landing | http://arxiv.org/abs/1509.01423 | id:1509.01423 author:H. W. Ho, C. De Wagter, B. D. W. Remes, G. C. H. E. de Croon category:cs.RO  published:2015-09-04 summary:Monocular optical flow has been widely used to detect obstacles in Micro Air Vehicles (MAVs) during visual navigation. However, this approach requires significant movement, which reduces the efficiency of navigation and may even introduce risks in narrow spaces. In this paper, we introduce a novel setup of self-supervised learning (SSL), in which optical flow cues serve as a scaffold to learn the visual appearance of obstacles in the environment. We apply it to a landing task, in which initially 'surface roughness' is estimated from the optical flow field in order to detect obstacles. Subsequently, a linear regression function is learned that maps appearance features represented by texton distributions to the roughness estimate. After learning, the MAV can detect obstacles by just analyzing a still image. This allows the MAV to search for a landing spot without moving. We first demonstrate this principle to work with offline tests involving images captured from an on-board camera, and then demonstrate the principle in flight. Although surface roughness is a property of the entire flow field in the global image, the appearance learning even allows for the pixel-wise segmentation of obstacles. version:3
arxiv-1708-05192 | Multiform Adaptive Robot Skill Learning from Humans | http://arxiv.org/abs/1708.05192 | id:1708.05192 author:Leidi Zhao, Raheem Lawhorn, Siddharth Patil, Steve Susanibar, Lu Lu, Cong Wang, Bo Ouyang category:cs.RO  published:2017-08-17 summary:Object manipulation is a basic element in everyday human lives. Robotic manipulation has progressed from maneuvering single-rigid-body objects with firm grasping to maneuvering soft objects and handling contact-rich actions. Meanwhile, technologies such as robot learning from demonstration have enabled humans to intuitively train robots. This paper discusses a new level of robotic learning-based manipulation. In contrast to the single form of learning from demonstration, we propose a multiform learning approach that integrates additional forms of skill acquisition, including adaptive learning from definition and evaluation. Moreover, going beyond state-of-the-art technologies of handling purely rigid or soft objects in a pseudo-static manner, our work allows robots to learn to handle partly rigid partly soft objects with time-critical skills and sophisticated contact control. Such capability of robotic manipulation offers a variety of new possibilities in human-robot interaction. version:1
arxiv-1708-05136 | More Iterations per Second, Same Quality -- Why Asynchronous Algorithms may Drastically Outperform Traditional Ones | http://arxiv.org/abs/1708.05136 | id:1708.05136 author:Robert Hannah, Wotao Yin category:math.OC cs.DC math.NA stat.CO  published:2017-08-17 summary:In this paper, we consider the convergence of a very general asynchronous-parallel algorithm called ARock, that takes many well-known asynchronous algorithms as special cases (gradient descent, proximal gradient, Douglas Rachford, ADMM, etc.). In asynchronous-parallel algorithms, the computing nodes simply use the most recent information that they have access to, instead of waiting for a full update from all nodes in the system. This means that nodes do not have to waste time waiting for information, which can be a major bottleneck, especially in distributed systems. When the system has $p$ nodes, asynchronous algorithms may complete $\Theta(\ln(p))$ more iterations than synchronous algorithms in a given time period ("more iterations per second"). Although asynchronous algorithms may compute more iterations per second, there is error associated with using outdated information. How many more iterations in total are needed to compensate for this error is still an open question. The main results of this paper aim to answer this question. We prove, loosely, that as the size of the problem becomes large, the number of additional iterations that asynchronous algorithms need becomes negligible compared to the total number ("same quality" of the iterations). Taking these facts together, our results provide solid evidence of the potential of asynchronous algorithms to vastly speed up certain distributed computations. version:1
arxiv-1708-04733 | Geometric Enclosing Networks | http://arxiv.org/abs/1708.04733 | id:1708.04733 author:Trung Le, Hung Vu, Tu Dinh Nguyen, Dinh Phung category:cs.LG cs.AI stat.ML  published:2017-08-16 summary:Training model to generate data has increasingly attracted research attention and become important in modern world applications. We propose in this paper a new geometry-based optimization approach to address this problem. Orthogonal to current state-of-the-art density-based approaches, most notably VAE and GAN, we present a fresh new idea that borrows the principle of minimal enclosing ball to train a generator G\left(\bz\right) in such a way that both training and generated data, after being mapped to the feature space, are enclosed in the same sphere. We develop theory to guarantee that the mapping is bijective so that its inverse from feature space to data space results in expressive nonlinear contours to describe the data manifold, hence ensuring data generated are also lying on the data manifold learned from training data. Our model enjoys a nice geometric interpretation, hence termed Geometric Enclosing Networks (GEN), and possesses some key advantages over its rivals, namely simple and easy-to-control optimization formulation, avoidance of mode collapsing and efficiently learn data manifold representation in a completely unsupervised manner. We conducted extensive experiments on synthesis and real-world datasets to illustrate the behaviors, strength and weakness of our proposed GEN, in particular its ability to handle multi-modal data and quality of generated data. version:2
arxiv-1708-05122 | Evaluating Visual Conversational Agents via Cooperative Human-AI Games | http://arxiv.org/abs/1708.05122 | id:1708.05122 author:Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh category:cs.HC cs.AI cs.CL cs.CV  published:2017-08-17 summary:As AI continues to advance, human-AI teams are inevitable. However, progress in AI is routinely measured in isolation, without a human in the loop. It is crucial to benchmark progress in AI, not just in isolation, but also in terms of how it translates to helping humans perform certain tasks, i.e., the performance of human-AI teams. In this work, we design a cooperative game - GuessWhich - to measure human-AI team performance in the specific context of the AI being a visual conversational agent. GuessWhich involves live interaction between the human and the AI. The AI, which we call ALICE, is provided an image which is unseen by the human. Following a brief description of the image, the human questions ALICE about this secret image to identify it from a fixed pool of images. We measure performance of the human-ALICE team by the number of guesses it takes the human to correctly identify the secret image after a fixed number of dialog rounds with ALICE. We compare performance of the human-ALICE teams for two versions of ALICE. Our human studies suggest a counterintuitive trend - that while AI literature shows that one version outperforms the other when paired with an AI questioner bot, we find that this improvement in AI-AI performance does not translate to improved human-AI performance. This suggests a mismatch between benchmarking of AI in isolation and in the context of human-AI teams. version:1
arxiv-1708-05682 | An Improved Residual LSTM Architecture for Acoustic Modeling | http://arxiv.org/abs/1708.05682 | id:1708.05682 author:Lu Huang, Jiasong Sun, Ji Xu, Yi Yang category:cs.CL cs.AI cs.SD  published:2017-08-17 summary:Long Short-Term Memory (LSTM) is the primary recurrent neural networks architecture for acoustic modeling in automatic speech recognition systems. Residual learning is an efficient method to help neural networks converge easier and faster. In this paper, we propose several types of residual LSTM methods for our acoustic modeling. Our experiments indicate that, compared with classic LSTM, our architecture shows more than 8% relative reduction in Phone Error Rate (PER) on TIMIT tasks. At the same time, our residual fast LSTM approach shows 4% relative reduction in PER on the same task. Besides, we find that all this architecture could have good results on THCHS-30, Librispeech and Switchboard corpora. version:1
arxiv-1708-05076 | SOCRATES: A System For Scalable Graph Analytics | http://arxiv.org/abs/1708.05076 | id:1708.05076 author:Cetin Savkli, Ryan Carr, Matthew Chapman, Brant Chee, David Minch category:cs.DC cs.DS  published:2017-08-16 summary:A distributed semantic graph processing system that provides locality control, indexing, graph query, and parallel processing capabilities is presented. version:1
arxiv-1708-04198 | A scalable multi-core architecture with heterogeneous memory structures for Dynamic Neuromorphic Asynchronous Processors (DYNAPs) | http://arxiv.org/abs/1708.04198 | id:1708.04198 author:Saber Moradi, Ning Qiao, Fabio Stefanini, Giacomo Indiveri category:cs.AR cs.AI  published:2017-08-14 summary:Neuromorphic computing systems comprise networks of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multi-core neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor (DVS) at high speed. version:2
arxiv-1708-04988 | Warp: a method for neural network interpretability applied to gene expression profiles | http://arxiv.org/abs/1708.04988 | id:1708.04988 author:Trofimov Assya, Lemieux Sebastien, Perreault Claude category:q-bio.GN cs.AI  published:2017-08-16 summary:We show a proof of principle for warping, a method to interpret the inner working of neural networks in the context of gene expression analysis. Warping is an efficient way to gain insight to the inner workings of neural nets and make them more interpretable. We demonstrate the ability of warping to recover meaningful information for a given class on a samplespecific individual basis. We found warping works well in both linearly and nonlinearly separable datasets. These encouraging results show that warping has a potential to be the answer to neural networks interpretability in computational biology. version:1
arxiv-1708-04983 | Visualizing and Exploring Dynamic High-Dimensional Datasets with LION-tSNE | http://arxiv.org/abs/1708.04983 | id:1708.04983 author:Andrey Boytsov, Francois Fouquet, Thomas Hartmann, Yves LeTraon category:cs.AI I.2.m  published:2017-08-16 summary:T-distributed stochastic neighbor embedding (tSNE) is a popular and prize-winning approach for dimensionality reduction and visualizing high-dimensional data. However, tSNE is non-parametric: once visualization is built, tSNE is not designed to incorporate additional data into existing representation. It highly limits the applicability of tSNE to the scenarios where data are added or updated over time (like dashboards or series of data snapshots). In this paper we propose, analyze and evaluate LION-tSNE (Local Interpolation with Outlier coNtrol) - a novel approach for incorporating new data into tSNE representation. LION-tSNE is based on local interpolation in the vicinity of training data, outlier detection and a special outlier mapping algorithm. We show that LION-tSNE method is robust both to outliers and to new samples from existing clusters. We also discuss multiple possible improvements for special cases. We compare LION-tSNE to a comprehensive list of possible benchmark approaches that include multiple interpolation techniques, gradient descent for new data, and neural network approximation. version:1
arxiv-1708-04922 | Optimal Alarms for Vehicular Collision Detection | http://arxiv.org/abs/1708.04922 | id:1708.04922 author:Michael Motro, Joydeep Ghosh, Chandra Bhat category:stat.ML cs.RO  published:2017-08-16 summary:An important application of intelligent vehicles is advance detection of dangerous events such as collisions. This problem is framed as a problem of optimal alarm choice given predictive models for vehicle location and motion. Techniques for real-time collision detection are surveyed and grouped into three classes: random Monte Carlo sampling, faster deterministic approximations, and machine learning models trained by simulation. Theoretical guarantees on the performance of these collision detection techniques are provided where possible, and empirical analysis is provided for two example scenarios. Results validate Monte Carlo sampling as a robust solution despite its simplicity. version:1
arxiv-1708-04911 | Improving Multi-Application Concurrency Support Within the GPU Memory System | http://arxiv.org/abs/1708.04911 | id:1708.04911 author:Rachata Ausavarungnirun, Christopher J. Rossbach, Vance Miller, Joshua Landgraf, Saugata Ghose, Jayneel Gnadhi, Adwait Jog, Onur Mutlu category:cs.AR  published:2017-08-16 summary:GPUs exploit a high degree of thread-level parallelism to hide long-latency stalls. Due to the heterogeneous compute requirements of different applications, there is a growing need to share the GPU across multiple applications in large-scale computing environments. However, while CPUs offer relatively seamless multi-application concurrency, and are an excellent fit for multitasking and for virtualized environments, GPUs currently offer only primitive support for multi-application concurrency. Much of the problem in a contemporary GPU lies within the memory system, where multi-application execution requires virtual memory support to manage the address spaces of each application and to provide memory protection. In this work, we perform a detailed analysis of the major problems in state-of-the-art GPU virtual memory management that hinders multi-application execution. Existing GPUs are designed to share memory between the CPU and GPU, but do not handle multi-application support within the GPU well. We find that when multiple applications spatially share the GPU, there is a significant amount of inter-core thrashing on the shared TLB within the GPU. The TLB contention is high enough to prevent the GPU from successfully hiding stall latencies, thus becoming a first-order performance concern. We introduce MASK, a memory hierarchy design that provides low-overhead virtual memory support for the concurrent execution of multiple applications. MASK extends the GPU memory hierarchy to efficiently support address translation through the use of multi-level TLBs, and uses translation-aware memory and cache management to maximize throughput in the presence of inter-application contention. version:1
arxiv-1708-04863 | Formal Specification and Safety Proof of a Leaderless Concurrent Atomic Broadcast Algorithm | http://arxiv.org/abs/1708.04863 | id:1708.04863 author:Marius Poke, Colin W. Glass category:cs.DC  published:2017-08-16 summary:Agreement plays a central role in distributed systems working on a common task. The increasing size of modern distributed systems makes them more susceptible to single component failures. Fault-tolerant distributed agreement protocols rely for the most part on leader-based atomic broadcast algorithms, such as Paxos. Such protocols are mostly used for data replication, which requires only a small number of servers to reach agreement. Yet, their centralized nature makes them ill-suited for distributed agreement at large scales. The recently introduced atomic broadcast algorithm AllConcur enables high throughput for distributed agreement while being completely decentralized. In this paper, we extend the work on AllConcur in two ways. First, we provide a formal specification of AllConcur that enables a better understanding of the algorithm. Second, we formally prove AllConcur's safety property on the basis of this specification. Therefore, our work not only ensures operators safe usage of AllConcur, but also facilitates the further improvement of distributed agreement protocols based on AllConcur. version:1
arxiv-1509-01066 | Safe Controller Optimization for Quadrotors with Gaussian Processes | http://arxiv.org/abs/1509.01066 | id:1509.01066 author:Felix Berkenkamp, Angela P. Schoellig, Andreas Krause category:cs.RO  published:2015-09-03 summary:One of the most fundamental problems when designing controllers for dynamic systems is the tuning of the controller parameters. Typically, a model of the system is used to obtain an initial controller, but ultimately the controller parameters must be tuned manually on the real system to achieve the best performance. To avoid this manual tuning step, methods from machine learning, such as Bayesian optimization, have been used. However, as these methods evaluate different controller parameters on the real system, safety-critical system failures may happen. In this paper, we overcome this problem by applying, for the first time, a recently developed safe optimization algorithm, SafeOpt, to the problem of automatic controller parameter tuning. Given an initial, low-performance controller, SafeOpt automatically optimizes the parameters of a control law while guaranteeing safety. It models the underlying performance measure as a Gaussian process and only explores new controller parameters whose performance lies above a safe performance threshold with high probability. Experimental results on a quadrotor vehicle indicate that the proposed method enables fast, automatic, and safe optimization of controller parameters without human intervention. version:4
arxiv-1708-04838 | A Template for Implementing Fast Lock-free Trees Using HTM | http://arxiv.org/abs/1708.04838 | id:1708.04838 author:Trevor Brown category:cs.DC D.1.3  published:2017-08-16 summary:Algorithms that use hardware transactional memory (HTM) must provide a software-only fallback path to guarantee progress. The design of the fallback path can have a profound impact on performance. If the fallback path is allowed to run concurrently with hardware transactions, then hardware transactions must be instrumented, adding significant overhead. Otherwise, hardware transactions must wait for any processes on the fallback path, causing concurrency bottlenecks, or move to the fallback path. We introduce an approach that combines the best of both worlds. The key idea is to use three execution paths: an HTM fast path, an HTM middle path, and a software fallback path, such that the middle path can run concurrently with each of the other two. The fast path and fallback path do not run concurrently, so the fast path incurs no instrumentation overhead. Furthermore, fast path transactions can move to the middle path instead of waiting or moving to the software path. We demonstrate our approach by producing an accelerated version of the tree update template of Brown et al., which can be used to implement fast lock-free data structures based on down-trees. We used the accelerated template to implement two lock-free trees: a binary search tree (BST), and an (a,b)-tree (a generalization of a B-tree). Experiments show that, with 72 concurrent processes, our accelerated (a,b)-tree performs between 4.0x and 4.2x as many operations per second as an implementation obtained using the original tree update template. version:1
arxiv-1708-01267 | Viewing Robot Navigation in Human Environment as a Cooperative Activity | http://arxiv.org/abs/1708.01267 | id:1708.01267 author:Harmish Khambhaita, Rachid Alami category:cs.RO  published:2017-08-03 summary:We claim that navigation in human environments can be viewed as cooperative activity especially in constrained situations. Humans concurrently aid and comply with each other while moving in a shared space. Cooperation helps pedestrians to efficiently reach their own goals and respect conventions such as the personal space of others. To meet human comparable efficiency, a robot needs to predict the human trajectories and plan its own trajectory correspondingly in the same shared space. In this work, we present a navigation planner that is able to plan such cooperative trajectories, simultaneously enforcing the robot's kinematic constraints and avoiding other non-human dynamic obstacles. Using robust social constraints of projected time to a possible future collision, compatibility of human-robot motion direction, and proxemics, our planner is able to replicate human-like navigation behavior not only in open spaces but also in confined areas. Besides adapting the robot trajectory, the planner is also able to proactively propose co-navigation solutions by jointly computing human and robot trajectories within the same optimization framework. We demonstrate richness and performance of the cooperative planner with simulated and real world experiments on multiple interactive navigation scenarios. version:2
arxiv-1708-04828 | Multi-task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs | http://arxiv.org/abs/1708.04828 | id:1708.04828 author:Yi Tay, Luu Anh Tuan, Minh C. Phan, Siu Cheung Hui category:cs.AI cs.IR  published:2017-08-16 summary:Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete attributes for each entity. Intuitively, these attributes such as height, price or population count are able to richly characterize entities in knowledge graphs. This additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for attribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and attributes that encode information about both tasks. Moreover, such attributes are not only central to many predictive tasks as an information source but also as a prediction target. Therefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly attractive as well. We show that our approach outperforms many state-of-the-art methods for the tasks of relational triplet classification and attribute value prediction. version:1
arxiv-1708-04801 | Weighted parallel SGD for distributed unbalanced-workload training system | http://arxiv.org/abs/1708.04801 | id:1708.04801 author:Cheng Daning, Li Shigang, Zhang Yunquan category:cs.LG cs.AI stat.ML  published:2017-08-16 summary:Stochastic gradient descent (SGD) is a popular stochastic optimization method in machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel SGD, often require all nodes to have the same performance or to consume equal quantities of data. However, these requirements are difficult to satisfy when the parallel SGD algorithms run in a heterogeneous computing environment; low-performance nodes will exert a negative influence on the final result. In this paper, we propose an algorithm called weighted parallel SGD (WP-SGD). WP-SGD combines weighted model parameters from different nodes in the system to produce the final output. WP-SGD makes use of the reduction in standard deviation to compensate for the loss from the inconsistency in performance of nodes in the cluster, which means that WP-SGD does not require that all nodes consume equal quantities of data. We also analyze the theoretical feasibility of running two other parallel SGD algorithms combined with WP-SGD in a heterogeneous environment. The experimental results show that WP-SGD significantly outperforms the traditional parallel SGD algorithms on distributed training systems with an unbalanced workload. version:1
arxiv-1708-01465 | Brain Responses During Robot-Error Observation | http://arxiv.org/abs/1708.01465 | id:1708.01465 author:Dominik Welke, Joos Behncke, Marina Hader, Robin Tibor Schirrmeister, Andreas Schönau, Boris Eßmann, Oliver Müller, Wolfram Burgard, Tonio Ball category:cs.HC cs.LG cs.RO  published:2017-08-04 summary:Brain-controlled robots are a promising new type of assistive device for severely impaired persons. Little is however known about how to optimize the interaction of humans and brain-controlled robots. Information about the human's perceived correctness of robot performance might provide a useful teaching signal for adaptive control algorithms and thus help enhancing robot control. Here, we studied whether watching robots perform erroneous vs. correct action elicits differential brain responses that can be decoded from single trials of electroencephalographic (EEG) recordings, and whether brain activity during human-robot interaction is modulated by the robot's visual similarity to a human. To address these topics, we designed two experiments. In experiment I, participants watched a robot arm pour liquid into a cup. The robot performed the action either erroneously or correctly, i.e. it either spilled some liquid or not. In experiment II, participants observed two different types of robots, humanoid and non-humanoid, grabbing a ball. The robots either managed to grab the ball or not. We recorded high-resolution EEG during the observation tasks in both experiments to train a Filter Bank Common Spatial Pattern (FBCSP) pipeline on the multivariate EEG signal and decode for the correctness of the observed action, and for the type of the observed robot. Our findings show that it was possible to decode both correctness and robot type for the majority of participants significantly, although often just slightly, above chance level. Our findings suggest that non-invasive recordings of brain responses elicited when observing robots indeed contain decodable information about the correctness of the robot's action and the type of observed robot. version:2
arxiv-1708-04796 | Strategies for Big Data Analytics through Lambda Architectures in Volatile Environments | http://arxiv.org/abs/1708.04796 | id:1708.04796 author:Alexandre Da Silva Veith, Julio C. S. dos Anjos, Edison Pignaton de Freitas, Thomas Lampoltshammer, Claudio Geyer category:cs.DC cs.PF  published:2017-08-16 summary:Expectations regarding the future growth of Internet of Things (IoT)-related technologies are high. These expectations require the realization of a sustainable general purpose application framework that is capable to handle these kinds of environments with their complexity in terms of heterogeneity and volatility. The paradigm of the Lambda architecture features key characteristics (such as robustness, fault tolerance, scalability, generalization, extensibility, ad-hoc queries, minimal maintenance, and low-latency reads and updates) to cope with this complexity. The paper at hand suggest a basic set of strategies to handle the arising challenges regarding the volatility, heterogeneity, and desired low latency execution by reducing the overall system timing (scheduling, execution, monitoring, and faults recovery) as well as possible faults (churn, no answers to executions). The proposed strategies make use of services such as migration, replication, MapReduce simulation, and combined processing methods (batch- and streaming-based). Via these services, a distribution of tasks for the best balance of computational resources is achieved, while monitoring and management can be performed asynchronously in the background. %An application of batch and stream-based methods are proposed to reduce the latency. version:1
arxiv-1708-04790 | Evaluation of Human-Robot Collaboration Models for Fluent Operations in Industrial Tasks | http://arxiv.org/abs/1708.04790 | id:1708.04790 author:Lior Sayfeld, Ygal Peretz, Roy Someshwar, Yael Edan category:cs.RO cs.SY  published:2017-08-16 summary:In this study we evaluated human-robot collaboration models in an integrated human-robot operational system. An integrated work cell which includes a robotic arm working collaboratively with a human worker was specially designed for executing a real-time assembly task. Eighty industrial engineering students aged 22-27 participated in experiments in which timing and sensor based models were compared to an adaptive model developed within this framework. Performance measures included total assembly time and total idle time. The results showed conclusively that the adaptive system improved the examined parameters and provided an improvement of 7% in total assembly time and 60% in total idle time when compared to timing and sensory based models. version:1
arxiv-1708-02835 | ExaGeoStat: A High Performance Unified Framework for Geostatistics on Manycore Systems | http://arxiv.org/abs/1708.02835 | id:1708.02835 author:Sameh Abdulah, Hatem Ltaief, Ying Sun, Marc G. Genton, David E. Keyes category:cs.DC  published:2017-08-09 summary:We present ExaGeoStat, a high performance framework for geospatial statistics in climate and environment modeling. In contrast to simulation based on partial differential equations derived from first-principles modeling, ExaGeoStat employs a statistical model based on the evaluation of the Gaussian log-likelihood function, which operates on a large dense covariance matrix. Generated by the parametrizable Matern covariance function, the resulting matrix is symmetric and positive definite. The computational tasks involved during the evaluation of the Gaussian log-likelihood function become daunting as the number n of geographical locations grows, as O(n2) storage and O(n3) operations are required. While many approximation methods have been devised from the side of statistical modeling to ameliorate these polynomial complexities, we are interested here in the complementary approach of evaluating the exact algebraic result by exploiting advances in solution algorithms and many-core computer architectures. Using state-of-the-art high performance dense linear algebra libraries associated with various leading edge parallel architectures (Intel KNLs, NVIDIA GPUs, and distributed-memory systems), ExaGeoStat raises the game for statistical applications from climate and environmental science. ExaGeoStat provides a reference evaluation of statistical parameters, with which to assess the validity of the various approaches based on approximation. The framework takes a first step in the merger of large-scale data analytics and extreme computing for geospatial statistical applications, to be followed by additional complexity reducing improvements from the solver side that can be implemented under the same interface. Thus, a single uncompromised statistical model can ultimately be executed in a wide variety of emerging exascale environments. version:2
arxiv-1708-04782 | StarCraft II: A New Challenge for Reinforcement Learning | http://arxiv.org/abs/1708.04782 | id:1708.04782 author:Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, Rodney Tsing category:cs.LG cs.AI  published:2017-08-16 summary:This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the StarCraft II game. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures. version:1
arxiv-1708-04757 | Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction | http://arxiv.org/abs/1708.04757 | id:1708.04757 author:Hossein Soleimani, James Hensman, Suchi Saria category:stat.ML cs.AI cs.LG  published:2017-08-16 summary:Missing data and noisy observations pose significant challenges for reliably predicting events from irregularly sampled multivariate time series (longitudinal) data. Imputation methods, which are typically used for completing the data prior to event prediction, lack a principled mechanism to account for the uncertainty due to missingness. Alternatively, state-of-the-art joint modeling techniques can be used for jointly modeling the longitudinal and event data and compute event probabilities conditioned on the longitudinal observations. These approaches, however, make strong parametric assumptions and do not easily scale to multivariate signals with many observations. Our proposed approach consists of several key innovations. First, we develop a flexible and scalable joint model based upon sparse multiple-output Gaussian processes. Unlike state-of-the-art joint models, the proposed model can explain highly challenging structure including non-Gaussian noise while scaling to large data. Second, we derive an optimal policy for predicting events using the distribution of the event occurrence estimated by the joint model. The derived policy trades-off the cost of a delayed detection versus incorrect assessments and abstains from making decisions when the estimated event probability does not satisfy the derived confidence criteria. Experiments on a large dataset show that the proposed framework significantly outperforms state-of-the-art techniques in event prediction. version:1
arxiv-1708-05296 | A Survey of Parallel A* | http://arxiv.org/abs/1708.05296 | id:1708.05296 author:Alex Fukunaga, Adi Botea, Yuu Jinnai, Akihiro Kishimoto category:cs.AI  published:2017-08-16 summary:A* is a best-first search algorithm for finding optimal-cost paths in graphs. A* benefits significantly from parallelism because in many applications, A* is limited by memory usage, so distributed memory implementations of A* that use all of the aggregate memory on the cluster enable problems that can not be solved by serial, single-machine implementations to be solved. We survey approaches to parallel A*, focusing on decentralized approaches to A* which partition the state space among processors. We also survey approaches to parallel, limited-memory variants of A* such as parallel IDA*. version:1
arxiv-1708-08985 | Limiting the Reconstruction Capability of Generative Neural Network using Negative Learning | http://arxiv.org/abs/1708.08985 | id:1708.08985 author:Asim Munawar, Phongtharin Vinayavekhin, Giovanni De Magistris category:cs.CV cs.AI cs.LG  published:2017-08-16 summary:Generative models are widely used for unsupervised learning with various applications, including data compression and signal restoration. Training methods for such systems focus on the generality of the network given limited amount of training data. A less researched type of techniques concerns generation of only a single type of input. This is useful for applications such as constraint handling, noise reduction and anomaly detection. In this paper we present a technique to limit the generative capability of the network using negative learning. The proposed method searches the solution in the gradient direction for the desired input and in the opposite direction for the undesired input. One of the application can be anomaly detection where the undesired inputs are the anomalous data. In the results section we demonstrate the features of the algorithm using MNIST handwritten digit dataset and latter apply the technique to a real-world obstacle detection problem. The results clearly show that the proposed learning technique can significantly improve the performance for anomaly detection. version:1
arxiv-1708-04701 | Performance Characterization of Multi-threaded Graph Processing Applications on Intel Many-Integrated-Core Architecture | http://arxiv.org/abs/1708.04701 | id:1708.04701 author:Xu Liu, Langshi Chen, Jesun S. Firoz, Judy Qiu, Lei Jiang category:cs.DC  published:2017-08-15 summary:Intel Xeon Phi many-integrated-core (MIC) architectures usher in a new era of terascale integration. Among emerging killer applications, parallel graph processing has been a critical technique to analyze connected data. In this paper, we empirically evaluate various computing platforms including an Intel Xeon E5 CPU, a Nvidia Geforce GTX1070 GPU and an Xeon Phi 7210 processor codenamed Knights Landing (KNL) in the domain of parallel graph processing. We show that the KNL gains encouraging performance when processing graphs, so that it can become a promising solution to accelerating multi-threaded graph applications. We further characterize the impact of KNL architectural enhancements on the performance of a state-of-the art graph framework.We have four key observations: 1 Different graph applications require distinctive numbers of threads to reach the peak performance. For the same application, various datasets need even different numbers of threads to achieve the best performance. 2 Only a few graph applications benefit from the high bandwidth MCDRAM, while others favor the low latency DDR4 DRAM. 3 Vector processing units executing AVX512 SIMD instructions on KNLs are underutilized when running the state-of-the-art graph framework. 4 The sub-NUMA cache clustering mode offering the lowest local memory access latency hurts the performance of graph benchmarks that are lack of NUMA awareness. At last, We suggest future works including system auto-tuning tools and graph framework optimizations to fully exploit the potential of KNL for parallel graph processing. version:1
arxiv-1708-04677 | New Directions: Wireless Robotic Materials | http://arxiv.org/abs/1708.04677 | id:1708.04677 author:Nikolaus Correll, Prabal Dutta, Richard Han, Kristofer Pister category:cs.RO cs.CY  published:2017-08-15 summary:We describe opportunities and challenges with wireless robotic materials. Robotic materials are multi-functional composites that tightly integrate sensing, actuation, computation and communication to create smart composites that can sense their environment and change their physical properties in an arbitrary programmable manner. Computation and communication in such materials are based on miniature, possibly wireless, devices that are scattered in the material and interface with sensors and actuators inside the material. Whereas routing and processing of information within the material build upon results from the field of sensor networks, robotic materials are pushing the limits of sensor networks in both size (down to the order of microns) and numbers of devices (up to the order of millions). In order to solve the algorithmic and systems challenges of such an approach, which will involve not only computer scientists, but also roboticists, chemists and material scientists, the community requires a common platform - much like the "Mote" that bootstrapped the widespread adoption of the field of sensor networks - that is small, provides ample of computation, is equipped with basic networking functionalities, and preferably can be powered wirelessly. version:1
arxiv-1708-04668 | Beating the Multiplicative Weights Update Algorithm | http://arxiv.org/abs/1708.04668 | id:1708.04668 author:Abhinav Aggarwal, José Abel Castellanos Joo, Diksha Gupta category:cs.DC  published:2017-08-15 summary:Multiplicative weights update algorithms have been used extensively in designing iterative algorithms for many computational tasks. The core idea is to maintain a distribution over a set of experts and update this distribution in an online fashion based on the parameters of the underlying optimization problem. In this report, we study the behavior of a special MWU algorithm used for generating a global coin flip in the presence of an adversary that tampers the experts' advice. Specifically, we focus our attention on two adversarial strategies: (1) non-adaptive, in which the adversary chooses a fixed set of experts a priori and corrupts their advice in each round; and (2) adaptive, in which this set is chosen as the rounds of the algorithm progress. We formulate these adversarial strategies as being greedy in terms of trying to maximize the share of the corrupted experts in the final weighted advice the MWU computes and provide the underlying optimization problem that needs to be solved to achieve this goal. We provide empirical results to show that in the presence of either of the above adversaries, the MWU algorithm takes $\mathcal{O}(n)$ rounds in expectation to produce the desired output. This result compares well with the current state of the art of $\mathcal{O}(n^3)$ for the general Byzantine consensus problem. Finally, we briefly discuss the extension of these adversarial strategies for a general MWU algorithm and provide an outline for the framework in that setting. version:1
arxiv-1708-04649 | Machine Learning for Survival Analysis: A Survey | http://arxiv.org/abs/1708.04649 | id:1708.04649 author:Ping Wang, Yan Li, Chandan K. Reddy category:cs.AI  published:2017-08-15 summary:Accurately predicting the time of occurrence of an event of interest is a critical problem in longitudinal data analysis. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. Such a phenomenon is called censoring which can be effectively handled using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the representative statistical methods along with the machine learning techniques used in survival analysis and provide a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. We hope that this paper will provide a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data. version:1
arxiv-1708-04592 | Gold Standard Online Debates Summaries and First Experiments Towards Automatic Summarization of Online Debate Data | http://arxiv.org/abs/1708.04592 | id:1708.04592 author:Nattapong Sanchan, Ahmet Aker, Kalina Bontcheva category:cs.CL cs.AI cs.IR  published:2017-08-15 summary:Usage of online textual media is steadily increasing. Daily, more and more news stories, blog posts and scientific articles are added to the online volumes. These are all freely accessible and have been employed extensively in multiple research areas, e.g. automatic text summarization, information retrieval, information extraction, etc. Meanwhile, online debate forums have recently become popular, but have remained largely unexplored. For this reason, there are no sufficient resources of annotated debate data available for conducting research in this genre. In this paper, we collected and annotated debate data for an automatic summarization task. Similar to extractive gold standard summary generation our data contains sentences worthy to include into a summary. Five human annotators performed this task. Inter-annotator agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for Krippendorff's alpha. Moreover, we also implement an extractive summarization system for online debates and discuss prominent features for the task of summarizing online debate data automatically. version:1
arxiv-1708-04587 | Automatic Summarization of Online Debates | http://arxiv.org/abs/1708.04587 | id:1708.04587 author:Nattapong Sanchan, Ahmet Aker, Kalina Bontcheva category:cs.CL cs.AI cs.IR  published:2017-08-15 summary:Debate summarization is one of the novel and challenging research areas in automatic text summarization which has been largely unexplored. In this paper, we develop a debate summarization pipeline to summarize key topics which are discussed or argued in the two opposing sides of online debates. We view that the generation of debate summaries can be achieved by clustering, cluster labeling, and visualization. In our work, we investigate two different clustering approaches for the generation of the summaries. In the first approach, we generate the summaries by applying purely term-based clustering and cluster labeling. The second approach makes use of X-means for clustering and Mutual Information for labeling the clusters. Both approaches are driven by ontologies. We visualize the results using bar charts. We think that our results are a smooth entry for users aiming to receive the first impression about what is discussed within a debate topic containing waste number of argumentations. version:1
arxiv-1702-07134 | Diverse Weighted Bipartite b-Matching | http://arxiv.org/abs/1702.07134 | id:1702.07134 author:Faez Ahmed, John P. Dickerson, Mark Fuge category:cs.DS cs.AI  published:2017-02-23 summary:Bipartite matching, where agents on one side of a market are matched to agents or items on the other, is a classical problem in computer science and economics, with widespread application in healthcare, education, advertising, and general resource allocation. A practitioner's goal is typically to maximize a matching market's economic efficiency, possibly subject to some fairness requirements that promote equal access to resources. A natural balancing act exists between fairness and efficiency in matching markets, and has been the subject of much research. In this paper, we study a complementary goal---balancing diversity and efficiency---in a generalization of bipartite matching where agents on one side of the market can be matched to sets of agents on the other. Adapting a classical definition of the diversity of a set, we propose a quadratic programming-based approach to solving a supermodular minimization problem that balances diversity and total weight of the solution. We also provide a scalable greedy algorithm with theoretical performance bounds. We then define the price of diversity, a measure of the efficiency loss due to enforcing diversity, and give a worst-case theoretical bound. Finally, we demonstrate the efficacy of our methods on three real-world datasets, and show that the price of diversity is not bad in practice. version:2
arxiv-1708-03366 | Resilient Linear Classification: An Approach to Deal with Attacks on Training Data | http://arxiv.org/abs/1708.03366 | id:1708.03366 author:Sangdon Park, James Weimer, Insup Lee category:cs.LG cs.AI cs.CR cs.SY  published:2017-08-10 summary:Data-driven techniques are used in cyber-physical systems (CPS) for controlling autonomous vehicles, handling demand responses for energy management, and modeling human physiology for medical devices. These data-driven techniques extract models from training data, where their performance is often analyzed with respect to random errors in the training data. However, if the training data is maliciously altered by attackers, the effect of these attacks on the learning algorithms underpinning data-driven CPS have yet to be considered. In this paper, we analyze the resilience of classification algorithms to training data attacks. Specifically, a generic metric is proposed that is tailored to measure resilience of classification algorithms with respect to worst-case tampering of the training data. Using the metric, we show that traditional linear classification algorithms are resilient under restricted conditions. To overcome these limitations, we propose a linear classification algorithm with a majority constraint and prove that it is strictly more resilient than the traditional algorithms. Evaluations on both synthetic data and a real-world retrospective arrhythmia medical case-study show that the traditional algorithms are vulnerable to tampered training data, whereas the proposed algorithm is more resilient (as measured by worst-case tampering). version:2
arxiv-1708-04441 | Localizing the Object Contact through Matching Tactile Features with Visual Map | http://arxiv.org/abs/1708.04441 | id:1708.04441 author:Shan Luo, Wenxuan Mou, Kaspar Althoefer, Hongbin Liu category:cs.RO  published:2017-08-15 summary:This paper presents a novel framework for integration of vision and tactile sensing by localizing tactile readings in a visual object map. Intuitively, there are some correspondences, e.g., prominent features, between visual and tactile object identification. To apply it in robotics, we propose to localize tactile readings in visual images by sharing same sets of feature descriptors through two sensing modalities. It is then treated as a probabilistic estimation problem solved in a framework of recursive Bayesian filtering. Feature-based measurement model and Gaussian based motion model are thus built. In our tests, a tactile array sensor is utilized to generate tactile images during interaction with objects and the results have proven the feasibility of our proposed framework. version:1
arxiv-1708-04436 | Iterative Closest Labeled Point for Tactile Object Shape Recognition | http://arxiv.org/abs/1708.04436 | id:1708.04436 author:Shan Luo, Wenxuan Mou, Kaspar Althoefer, Hongbin Liu category:cs.RO  published:2017-08-15 summary:Tactile data and kinesthetic cues are two important sensing sources in robot object recognition and are complementary to each other. In this paper, we propose a novel algorithm named Iterative Closest Labeled Point (iCLAP) to recognize objects using both tactile and kinesthetic information.The iCLAP first assigns different local tactile features with distinct label numbers. The label numbers of the tactile features together with their associated 3D positions form a 4D point cloud of the object. In this manner, the two sensing modalities are merged to form a synthesized perception of the touched object. To recognize an object, the partial 4D point cloud obtained from a number of touches iteratively matches with all the reference cloud models to identify the best fit. An extensive evaluation study with 20 real objects shows that our proposed iCLAP approach outperforms those using either of the separate sensing modalities, with a substantial recognition rate improvement of up to 18%. version:1
arxiv-1708-04403 | Theoretical Foundation of Co-Training and Disagreement-Based Algorithms | http://arxiv.org/abs/1708.04403 | id:1708.04403 author:Wei Wang, Zhi-Hua Zhou category:cs.LG cs.AI stat.ML  published:2017-08-15 summary:Disagreement-based approaches generate multiple classifiers and exploit the disagreement among them with unlabeled data to improve learning performance. Co-training is a representative paradigm of them, which trains two classifiers separately on two sufficient and redundant views; while for the applications where there is only one view, several successful variants of co-training with two different classifiers on single-view data instead of two views have been proposed. For these disagreement-based approaches, there are several important issues which still are unsolved, in this article we present theoretical analyses to address these issues, which provides a theoretical foundation of co-training and disagreement-based approaches. version:1
arxiv-1708-04391 | Learning body-affordances to simplify action spaces | http://arxiv.org/abs/1708.04391 | id:1708.04391 author:Nicholas Guttenberg, Martin Biehl, Ryota Kanai category:cs.AI cs.RO  published:2017-08-15 summary:Controlling embodied agents with many actuated degrees of freedom is a challenging task. We propose a method that can discover and interpolate between context dependent high-level actions or body-affordances. These provide an abstract, low-dimensional interface indexing high-dimensional and time- extended action policies. Our method is related to recent ap- proaches in the machine learning literature but is conceptually simpler and easier to implement. More specifically our method requires the choice of a n-dimensional target sensor space that is endowed with a distance metric. The method then learns an also n-dimensional embedding of possibly reactive body-affordances that spread as far as possible throughout the target sensor space. version:1
arxiv-1708-03951 | Optimization of Ensemble Supervised Learning Algorithms for Increased Sensitivity, Specificity, and AUC of Population-Based Colorectal Cancer Screenings | http://arxiv.org/abs/1708.03951 | id:1708.03951 author:Anirudh Kamath, Aditya Singh, Raj Ramnani, Ayush Vyas, Jay Shenoy category:stat.ML cs.AI q-bio.QM  published:2017-08-13 summary:Over 150,000 new people in the United States are diagnosed with colorectal cancer each year. Nearly a third die from it (American Cancer Society). The only approved noninvasive diagnosis tools currently involve fecal blood count tests (FOBTs) or stool DNA tests. Fecal blood count tests take only five minutes and are available over the counter for as low as \$15. They are highly specific, yet not nearly as sensitive, yielding a high percentage (25%) of false negatives (Colon Cancer Alliance). Moreover, FOBT results are far too generalized, meaning that a positive result could mean much more than just colorectal cancer, and could just as easily mean hemorrhoids, anal fissure, proctitis, Crohn's disease, diverticulosis, ulcerative colitis, rectal ulcer, rectal prolapse, ischemic colitis, angiodysplasia, rectal trauma, proctitis from radiation therapy, and others. Stool DNA tests, the modern benchmark for CRC screening, have a much higher sensitivity and specificity, but also cost \$600, take two weeks to process, and are not for high-risk individuals or people with a history of polyps. To yield a cheap and effective CRC screening alternative, a unique ensemble-based classification algorithm is put in place that considers the FIT result, BMI, smoking history, and diabetic status of patients. This method is tested under ten-fold cross validation to have a .95 AUC, 92% specificity, 89% sensitivity, .88 F1, and 90% precision. Once clinically validated, this test promises to be cheaper, faster, and potentially more accurate when compared to a stool DNA test. version:2
arxiv-1708-02255 | Generative Statistical Models with Self-Emergent Grammar of Chord Sequences | http://arxiv.org/abs/1708.02255 | id:1708.02255 author:Hiroaki Tsushima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii category:cs.AI cs.CL cs.SD  published:2017-08-07 summary:Generative statistical models of chord sequences play crucial roles in music processing. To capture syntactic similarities among certain chords (e.g. in C major key, between G and G7 and between F and Dm), we study hidden Markov models and probabilistic context-free grammar models with latent variables describing syntactic categories of chord symbols and their unsupervised learning techniques for inducing the latent grammar from data. Surprisingly, we find that these models often outperform conventional Markov models in predictive power, and the self-emergent categories often correspond to traditional harmonic functions. This implies the need for chord categories in harmony models from the informatics perspective. version:2
arxiv-1708-04357 | Graph Classification via Deep Learning with Virtual Nodes | http://arxiv.org/abs/1708.04357 | id:1708.04357 author:Trang Pham, Truyen Tran, Hoa Dam, Svetha Venkatesh category:cs.LG cs.AI stat.ML  published:2017-08-14 summary:Learning representation for graph classification turns a variable-size graph into a fixed-size vector (or matrix). Such a representation works nicely with algebraic manipulations. Here we introduce a simple method to augment an attributed graph with a virtual node that is bidirectionally connected to all existing nodes. The virtual node represents the latent aspects of the graph, which are not immediately available from the attributes and local connectivity structures. The expanded graph is then put through any node representation method. The representation of the virtual node is then the representation of the entire graph. In this paper, we use the recently introduced Column Network for the expanded graph, resulting in a new end-to-end graph classification model dubbed Virtual Column Network (VCN). The model is validated on two tasks: (i) predicting bio-activity of chemical compounds, and (ii) finding software vulnerability from source code. Results demonstrate that VCN is competitive against well-established rivals. version:1
arxiv-1708-04352 | Benchmark Environments for Multitask Learning in Continuous Domains | http://arxiv.org/abs/1708.04352 | id:1708.04352 author:Peter Henderson, Wei-Di Chang, Florian Shkurti, Johanna Hansen, David Meger, Gregory Dudek category:cs.AI  published:2017-08-14 summary:As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains. version:1
arxiv-1708-04927 | TheoSea: Marching Theory to Light | http://arxiv.org/abs/1708.04927 | id:1708.04927 author:Mark A. Stalzer, Chao Ju category:cs.AI  published:2017-08-14 summary:There is sufficient information in the far-field of a radiating dipole antenna to rediscover the Maxwell Equations and the wave equations of light, including the speed of light $c.$ TheoSea is a Julia program that does this in about a second, and the key insight is that the compactness of theories drives the search. The program is a computational embodiment of the scientific method: observation, consideration of candidate theories, and validation. version:1
arxiv-1708-04321 | Distance and Similarity Measures Effect on the Performance of K-Nearest Neighbor Classifier - A Review | http://arxiv.org/abs/1708.04321 | id:1708.04321 author:V. B. Surya Prasath, Haneen Arafat Abu Alfeilat, Omar Lasassmeh, Ahmad B. A. Hassanat category:cs.LG cs.AI  published:2017-08-14 summary:The K-nearest neighbor (KNN) classifier is one of the simplest and most common classifiers, yet its performance competes with the most complex classifiers in the literature. The core of this classifier depends mainly on measuring the distance or similarity between the tested example and the training examples. This raises a major question about which distance measures to be used for the KNN classifier among a large number of distance and similarity measures? This review attempts to answer the previous question through evaluating the performance (measured by accuracy, precision and recall) of the KNN using a large number of distance measures, tested on a number of real world datasets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, the results showed large gaps between the performances of different distances. We found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances. In addition, the performance of the KNN degraded only about $20\%$ while the noise level reaches $90\%$, this is true for all the distances used. This means that the KNN classifier using any of the top $10$ distances tolerate noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing to other distances. version:1
arxiv-1708-04318 | Cyber-Physical Interference Modeling for Predictable Reliability of Inter-Vehicle Communications | http://arxiv.org/abs/1708.04318 | id:1708.04318 author:Chuan Li, Hongwei Zhang, Jayanthi Rao, Le Yi Wang, George Yin category:cs.DC cs.NI  published:2017-08-14 summary:Predictable inter-vehicle communication reliability is a basis for the paradigm shift from the traditional singlevehicle-oriented safety and efficiency control to networked vehicle control. The lack of predictable interference control in existing mechanisms of inter-vehicle communications, however, makes them incapable of ensuring predictable communication reliability. For predictable interference control, we propose the Cyber-Physical Scheduling (CPS) framework that leverages the PRK interference model and addresses the challenges of vehicle mobility to PRK-based scheduling. In particular, CPS leverage physical locations of vehicles to define the gPRK interference model, a geometric approximation of the PRK model, for effective interference relation estimation, and CPS leverages cyber-physical structures of vehicle traffic flows (particularly, spatiotemporal interference correlation as well as macro- and micro-scopic vehicle dynamics) for effective use of the gPRK model. Through experimental analysis with high-fidelity ns-3 and SUMO simulation, we observe that CPS enables predictable reliability while achieving high throughput and low delay in communication. To the best of our knowledge, CPS is the first field deployable method that ensures predictable interference control and thus reliability in inter-vehicle communications. version:1
arxiv-1708-04290 | The Complexity of Distributed Edge Coloring with Small Palettes | http://arxiv.org/abs/1708.04290 | id:1708.04290 author:Yi-Jun Chang, Qizheng He, Wenzheng Li, Seth Pettie, Jara Uitto category:cs.DC cs.DS  published:2017-08-14 summary:The complexity of distributed edge coloring depends heavily on the palette size as a function of the maximum degree $\Delta$. In this paper we explore the complexity of edge coloring in the LOCAL model in different palette size regimes. 1. We simplify the \emph{round elimination} technique of Brandt et al. and prove that $(2\Delta-2)$-edge coloring requires $\Omega(\log_\Delta \log n)$ time w.h.p. and $\Omega(\log_\Delta n)$ time deterministically, even on trees. The simplified technique is based on two ideas: the notion of an irregular running time and some general observations that transform weak lower bounds into stronger ones. 2. We give a randomized edge coloring algorithm that can use palette sizes as small as $\Delta + \tilde{O}(\sqrt{\Delta})$, which is a natural barrier for randomized approaches. The running time of the algorithm is at most $O(\log\Delta \cdot T_{LLL})$, where $T_{LLL}$ is the complexity of a permissive version of the constructive Lovasz local lemma. 3. We develop a new distributed Lovasz local lemma algorithm for tree-structured dependency graphs, which leads to a $(1+\epsilon)\Delta$-edge coloring algorithm for trees running in $O(\log\log n)$ time. This algorithm arises from two new results: a deterministic $O(\log n)$-time LLL algorithm for tree-structured instances, and a randomized $O(\log\log n)$-time graph shattering method for breaking the dependency graph into independent $O(\log n)$-size LLL instances. 4. A natural approach to computing $(\Delta+1)$-edge colorings (Vizing's theorem) is to extend partial colorings by iteratively re-coloring parts of the graph. We prove that this approach may be viable, but in the worst case requires recoloring subgraphs of diameter $\Omega(\Delta\log n)$. This stands in contrast to distributed algorithms for Brooks' theorem, which exploit the existence of $O(\log_\Delta n)$-length augmenting paths. version:1
arxiv-1708-04202 | Learning to Plan Chemical Syntheses | http://arxiv.org/abs/1708.04202 | id:1708.04202 author:Marwin H. S. Segler, Mike Preuss, Mark P. Waller category:cs.AI cs.LG physics.chem-ph  published:2017-08-14 summary:From medicines to materials, small organic molecules are indispensable for human well-being. To plan their syntheses, chemists employ a problem solving technique called retrosynthesis. In retrosynthesis, target molecules are recursively transformed into increasingly simpler precursor compounds until a set of readily available starting materials is obtained. Computer-aided retrosynthesis would be a highly valuable tool, however, past approaches were slow and provided results of unsatisfactory quality. Here, we employ Monte Carlo Tree Search (MCTS) to efficiently discover retrosynthetic routes. MCTS was combined with an expansion policy network that guides the search, and an "in-scope" filter network to pre-select the most promising retrosynthetic steps. These deep neural networks were trained on 12 million reactions, which represents essentially all reactions ever published in organic chemistry. Our system solves almost twice as many molecules and is 30 times faster in comparison to the traditional search method based on extracted rules and hand-coded heuristics. Finally after a 60 year history of computer-aided synthesis planning, chemists can no longer distinguish between routes generated by a computer system and real routes taken from the scientific literature. We anticipate that our method will accelerate drug and materials discovery by assisting chemists to plan better syntheses faster, and by enabling fully automated robot synthesis. version:1
arxiv-1708-04196 | Understanding and Visualizing the District of Columbia Capital Bikeshare System Using Data Analysis for Balancing Purposes | http://arxiv.org/abs/1708.04196 | id:1708.04196 author:Kiana Roshan Zamir, Ali Shafahi, Ali Haghani category:cs.AI  published:2017-08-14 summary:Bike sharing systems' popularity has consistently been rising during the past years. Managing and maintaining these emerging systems are indispensable parts of these systems. Visualizing the current operations can assist in getting a better grasp on the performance of the system. In this paper, a data mining approach is used to identify and visualize some important factors related to bike-share operations and management. To consolidate the data, we cluster stations that have a similar pickup and drop-off profiles during weekdays and weekends. We provide the temporal profile of the center of each cluster which can be used as a simple and practical approach for approximating the number of pickups and drop-offs of the stations. We also define two indices based on stations' shortages and surpluses that reflect the degree of balancing aid a station needs. These indices can help stakeholders improve the quality of the bike-share user experience in at-least two ways. It can act as a complement to balancing optimization efforts, and it can identify stations that need expansion. We mine the District of Columbia's regional bike-share data and discuss the findings of this data set. We examine the bike-share system during different quarters of the year and during both peak and non-peak hours. Findings reflect that on weekdays most of the pickups and drop-offs happen during the morning and evening peaks whereas on weekends pickups and drop-offs are spread out throughout the day. We also show that throughout the day, more than 40% of the stations are relatively self-balanced. Not worrying about these stations during ordinary days can allow the balancing efforts to focus on a fewer stations and therefore potentially improve the efficiency of the balancing optimization models. version:1
arxiv-1708-04185 | Active vision for dexterous grasping of novel objects | http://arxiv.org/abs/1708.04185 | id:1708.04185 author:Ermano Arruda, Jeremy Wyatt, Marek Kopicki category:cs.RO  published:2017-08-14 summary:How should a robot direct active vision so as to ensure reliable grasping? We answer this question for the case of dexterous grasping of unfamiliar objects. By dexterous grasping we simply mean grasping by any hand with more than two fingers, such that the robot has some choice about where to place each finger. Such grasps typically fail in one of two ways, either unmodeled objects in the scene cause collisions or object reconstruction is insufficient to ensure that the grasp points provide a stable force closure. These problems can be solved more easily if active sensing is guided by the anticipated actions. Our approach has three stages. First, we take a single view and generate candidate grasps from the resulting partial object reconstruction. Second, we drive the active vision approach to maximise surface reconstruction quality around the planned contact points. During this phase, the anticipated grasp is continually refined. Third, we direct gaze to improve the safety of the planned reach to grasp trajectory. We show, on a dexterous manipulator with a camera on the wrist, that our approach (80.4% success rate) outperforms a randomised algorithm (64.3% success rate). version:1
arxiv-1708-04236 | Motion Planning under Partial Observability using Game-Based Abstraction | http://arxiv.org/abs/1708.04236 | id:1708.04236 author:Leonore Winterer, Sebastian Junges, Ralf Wimmer, Nils Jansen, Ufuk Topcu, Joost-Pieter Katoen, Bernd Becker category:cs.RO cs.AI  published:2017-08-14 summary:We study motion planning problems where agents move inside environments that are not fully observable and subject to uncertainties. The goal is to compute a strategy for an agent that is guaranteed to satisfy certain safety and performance specifications. Such problems are naturally modelled by partially observable Markov decision processes (POMDPs). Because of the potentially huge or even infinite belief space of POMDPs, verification and strategy synthesis is in general computationally intractable. We tackle this difficulty by exploiting typical structural properties of such scenarios; for instance, we assume that agents have the ability to observe their own positions inside an environment. Ambiguity in the state of the environment is abstracted into non-deterministic choices over the possible states of the environment. Technically, this abstraction transforms POMDPs into probabilistic two-player games (PGs). For these PGs, efficient verification tools are able to determine strategies that approximate certain measures on the POMDP. If an approximation is too coarse to provide guarantees, an abstraction refinement scheme further resolves the belief space of the POMDP. We demonstrate that our method improves the state of the art by orders of magnitude compared to a direct solution of the POMDP. version:1
