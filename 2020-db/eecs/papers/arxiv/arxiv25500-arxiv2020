arxiv-1702-06336 | Hybrid Dialog State Tracker with ASR Features | http://arxiv.org/abs/1702.06336 | id:1702.06336 author:Miroslav Vodolán, Rudolf Kadlec, Jan Kleindienst category:cs.CL  published:2017-02-21 summary:This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding (SLU) for slot-filling dialog systems. Our architecture is inspired by previously proposed neural-network-based belief-tracking systems. In addition, we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machine-learning based systems. For evaluation, we used the Dialog State Tracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new state-of-the-art result in three out of four categories within the DSTC2. version:1
arxiv-1702-06332 | Just DIAL: DomaIn Alignment Layers for Unsupervised Domain Adaptation | http://arxiv.org/abs/1702.06332 | id:1702.06332 author:Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, Samuel Rota Bulò category:cs.CV  published:2017-02-21 summary:The empirical fact that classifiers, trained on given data collections, perform poorly when tested on data acquired in different settings is theoretically explained in domain adaptation through a shift among distributions of the source and target domains. Alleviating the domain shift problem, especially in the challenging setting where no labeled data are available for the target domain, is paramount for having visual recognition systems working in the wild. As the problem stems from a shift among distributions, intuitively one should try to align them. In the literature, this has resulted in a stream of works attempting to align the feature representations learned from the source and target domains. Here we take a different route. Rather than introducing regularization terms aiming to promote the alignment of the two representations, we act at the distribution level through the introduction of \emph{DomaIn Alignment Layers} (\DIAL), able to match the observed source and target data distributions to a reference one. Thorough experiments on three different public benchmarks we confirm the power of our approach. version:1
arxiv-1702-06329 | Towards a Common Implementation of Reinforcement Learning for Multiple Robotic Tasks | http://arxiv.org/abs/1702.06329 | id:1702.06329 author:Angel Martínez-Tenor, Juan Antonio Fernández-Madrigal, Ana Cruz-Martín, Javier González-Jiménez category:cs.AI cs.LG cs.RO 68T40 I.2.6; I.2.9  published:2017-02-21 summary:Mobile robots are increasingly being employed for performing complex tasks in dynamic environments. Reinforcement learning (RL) methods are recognized to be promising for specifying such tasks in a relatively simple manner. However, the strong dependency between the learning method and the task to learn is a well-known problem that restricts practical implementations of RL in robotics, often requiring major modifications of parameters and adding other techniques for each particular task. In this paper we present a practical core implementation of RL which enables the learning process for multiple robotic tasks with minimal per-task tuning or none. Based on value iteration methods, this implementation includes a novel approach for action selection, called Q-biased softmax regression (QBIASSR), which avoids poor performance of the learning process when the robot reaches new unexplored states. Our approach takes advantage of the structure of the state space by attending the physical variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.), thus experienced sets of states may favor the decision-making process of unexplored or rarely-explored states. This improvement has a relevant role in reducing the tuning of the algorithm for particular tasks. Experiments with real and simulated robots, performed with the software framework also introduced here, show that our implementation is effectively able to learn different robotic tasks without tuning the learning method. Results also suggest that the combination of true online SARSA({\lambda}) with QBIASSR can outperform the existing RL core algorithms in low-dimensional robotic tasks. version:1
arxiv-1702-06318 | Is Saki #delicious? The Food Perception Gap on Instagram and Its Relation to Health | http://arxiv.org/abs/1702.06318 | id:1702.06318 author:Ferda Ofli, Yusuf Aytar, Ingmar Weber, Raggi al Hammouri, Antonio Torralba category:cs.CY cs.CV cs.SI  published:2017-02-21 summary:Food is an integral part of our life and what and how much we eat crucially affects our health. Our food choices largely depend on how we perceive certain characteristics of food, such as whether it is healthy, delicious or if it qualifies as a salad. But these perceptions differ from person to person and one person's "single lettuce leaf" might be another person's "side salad". Studying how food is perceived in relation to what it actually is typically involves a laboratory setup. Here we propose to use recent advances in image recognition to tackle this problem. Concretely, we use data for 1.9 million images from Instagram from the US to look at systematic differences in how a machine would objectively label an image compared to how a human subjectively does. We show that this difference, which we call the "perception gap", relates to a number of health outcomes observed at the county level. To the best of our knowledge, this is the first time that image recognition is being used to study the "misalignment" of how people describe food images vs. what they actually depict. version:1
arxiv-1702-04649 | Generative Temporal Models with Memory | http://arxiv.org/abs/1702.04649 | id:1702.04649 author:Mevlana Gemici, Chia-Chun Hung, Adam Santoro, Greg Wayne, Shakir Mohamed, Danilo J. Rezende, David Amos, Timothy Lillicrap category:cs.LG cs.NE stat.ML  published:2017-02-15 summary:We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs. version:2
arxiv-1702-06294 | Learning Compact Appearance Representation for Video-based Person Re-Identification | http://arxiv.org/abs/1702.06294 | id:1702.06294 author:Wei Zhang, Shengnan Hu, Kan Liu category:cs.CV  published:2017-02-21 summary:This paper presents a novel approach for video-based person re-identification using multiple Convolutional Neural Networks (CNNs). Unlike previous work, we intend to extract a compact yet discriminative appearance representation from several frames rather than the whole sequence. Specifically, given a video, the representative frames are selected based on the walking profile of consecutive frames. A multiple CNN architecture incorporated with feature pooling is proposed to learn and compile the features of the selected representative frames into a compact description about the pedestrian for identification. Experiments are conducted on benchmark datasets to demonstrate the superiority of the proposed method over existing person re-identification approaches. version:1
arxiv-1702-06291 | Visual Tracking by Reinforced Decision Making | http://arxiv.org/abs/1702.06291 | id:1702.06291 author:Janghoon Choi, Junseok Kwon, Kyoung Mu Lee category:cs.CV  published:2017-02-21 summary:One of the major challenges of model-free visual tracking problem has been the difficulty originating from the unpredictable and drastic changes in the appearance of objects we target to track. Existing methods tackle this problem by updating the appearance model on-line in order to adapt to the changes in the appearance. Despite the success of these methods however, inaccurate and erroneous updates of the appearance model result in a tracker drift. In this paper, we introduce a novel visual tracking algorithm based on a template selection strategy constructed by deep reinforcement learning methods. The tracking algorithm utilizes this strategy to choose the best template for tracking a given frame. The template selection strategy is self-learned by utilizing a simple policy gradient method on numerous training episodes randomly generated from a tracking benchmark dataset. Our proposed reinforcement learning framework is generally applicable to other confidence map based tracking algorithms. The experiment shows that our tracking algorithm effectively decides the best template for visual tracking. version:1
arxiv-1702-06286 | Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection | http://arxiv.org/abs/1702.06286 | id:1702.06286 author:Emre Çakır, Giambattista Parascandolo, Toni Heittola, Heikki Huttunen, Tuomas Virtanen category:cs.LG cs.SD  published:2017-02-21 summary:Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events. version:1
arxiv-1702-06280 | On the (Statistical) Detection of Adversarial Examples | http://arxiv.org/abs/1702.06280 | id:1702.06280 author:Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, Patrick McDaniel category:cs.CR cs.LG stat.ML  published:2017-02-21 summary:Machine Learning (ML) models are applied in a variety of tasks such as network intrusion detection or malware classification. Yet, these models are vulnerable to a class of malicious inputs known as adversarial examples. These are slightly perturbed inputs that are classified incorrectly by the ML model. The mitigation of these adversarial inputs remains an open problem. As a step towards a model-agnostic defense against adversarial examples, we show that they are not drawn from the same distribution than the original data, and can thus be detected using statistical tests. As the number of malicious points included in samples presented to the test diminishes, its detection confidence decreases. Hence, we introduce a complimentary approach to identify specific inputs that are adversarial among sets of inputs flagged by the statistical test. Specifically, we augment our ML model with an additional output, in which the model is trained to classify all adversarial inputs. We evaluate our approach on multiple adversarial example crafting methods (including the fast gradient sign and Jacobian-based saliency map methods) with several datasets. The statistical test flags sample sets containing adversarial inputs with confidence above 80%. Furthermore, our augmented model either detects adversarial examples with high accuracy (>80%) or increases the adversary's cost---the perturbation added---by more than 150%. In this way, we show that statistical properties of adversarial examples are essential to their detection. version:1
arxiv-1702-06850 | Scene Recognition by Combining Local and Global Image Descriptors | http://arxiv.org/abs/1702.06850 | id:1702.06850 author:Jobin Wilson, Muhammad Arif category:cs.CV cs.LG  published:2017-02-21 summary:Object recognition is an important problem in computer vision, having diverse applications. In this work, we construct an end-to-end scene recognition pipeline consisting of feature extraction, encoding, pooling and classification. Our approach simultaneously utilize global feature descriptors as well as local feature descriptors from images, to form a hybrid feature descriptor corresponding to each image. We utilize DAISY features associated with key points within images as our local feature descriptor and histogram of oriented gradients (HOG) corresponding to an entire image as a global descriptor. We make use of a bag-of-visual-words encoding and apply Mini- Batch K-Means algorithm to reduce the complexity of our feature encoding scheme. A 2-level pooling procedure is used to combine DAISY and HOG features corresponding to each image. Finally, we experiment with a multi-class SVM classifier with several kernels, in a cross-validation setting, and tabulate our results on the fifteen scene categories dataset. The average accuracy of our model was 76.4% in the case of a 40%-60% random split of images into training and testing datasets respectively. The primary objective of this work is to clearly outline the practical implementation of a basic screne-recognition pipeline having a reasonable accuracy, in python, using open-source libraries. A full implementation of the proposed model is available in our github repository. version:1
arxiv-1702-04267 | On Detecting Adversarial Perturbations | http://arxiv.org/abs/1702.04267 | id:1702.04267 author:Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff category:stat.ML cs.AI cs.CV cs.LG  published:2017-02-14 summary:Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small "detector" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack. version:2
arxiv-1702-06278 | Column normalization of a random measurement matrix | http://arxiv.org/abs/1702.06278 | id:1702.06278 author:Shahar Mendelson category:stat.ML  published:2017-02-21 summary:In this note we answer a question of G. Lecu\'{e}, by showing that column normalization of a random matrix with iid entries need not lead to good sparse recovery properties, even if the generating random variable has a reasonable moment growth. Specifically, for every $2 \leq p \leq c_1\log d$ we construct a random vector $X \in R^d$ with iid, mean-zero, variance $1$ coordinates, that satisfies $\sup_{t \in S^{d-1}} \ <X,t>\ _{L_q} \leq c_2\sqrt{q}$ for every $2\leq q \leq p$. We show that if $m \leq c_3\sqrt{p}d^{1/p}$ and $\tilde{\Gamma}:R^d \to R^m$ is the column-normalized matrix generated by $m$ independent copies of $X$, then with probability at least $1-2\exp(-c_4m)$, $\tilde{\Gamma}$ does not satisfy the exact reconstruction property of order $2$. version:1
arxiv-1702-06277 | Projection based advanced motion model for cubic mapping for 360-degree video | http://arxiv.org/abs/1702.06277 | id:1702.06277 author:Li Li, Zhu Li, Madhukar Budagavi, Houqiang Li category:cs.MM cs.CV  published:2017-02-21 summary:This paper proposes a novel advanced motion model to handle the irregular motion for the cubic map projection of 360-degree video. Since the irregular motion is mainly caused by the projection from the sphere to the cube map, we first try to project the pixels in both the current picture and reference picture from unfolding cube back to the sphere. Then through utilizing the characteristic that most of the motions in the sphere are uniform, we can derive the relationship between the motion vectors of various pixels in the unfold cube. The proposed advanced motion model is implemented in the High Efficiency Video Coding reference software. Experimental results demonstrate that quite obvious performance improvement can be achieved for the sequences with obvious motions. version:1
arxiv-1702-06269 | Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox | http://arxiv.org/abs/1702.06269 | id:1702.06269 author:Jialei Wang, Weiran Wang, Nathan Srebro category:cs.LG  published:2017-02-21 summary:We present and analyze statistically optimal, communication and memory efficient distributed stochastic optimization algorithms with near-linear speedups (up to $\log$-factors). This improves over prior work which includes methods with near-linear speedups but polynomial communication requirements (accelerated minibatch SGD) and communication efficient methods which do not exhibit any runtime speedups over a naive single-machine approach. We first analyze a distributed SVRG variant as a distributed stochastic optimization method and show that it can achieve near-linear speedups with logarithmic rounds of communication, at the cost of high memory requirements. We then present a novel method, stochastic DANE, which trades off memory for communication and still allows for optimization with communication which scales only logarithmically with the desired accuracy while also being memory efficient. Stochastic DANE is based on a minibatch prox procedure, solving a non-linearized subproblem on a minibatch at each iteration. We provide a novel analysis for this procedure which achieves the statistical optimal rate regardless of minibatch size and smoothness, and thus significantly improving on prior work. version:1
arxiv-1702-06533 | Stochastic Canonical Correlation Analysis | http://arxiv.org/abs/1702.06533 | id:1702.06533 author:Chao Gao, Dan Garber, Nathan Srebro, Jialei Wang, Weiran Wang category:cs.LG stat.ML  published:2017-02-21 summary:We tightly analyze the sample complexity of CCA, provide a learning algorithm that achieves optimal statistical performance in time linear in the required number of samples (up to log factors), as well as a streaming algorithm with similar guarantees. version:1
arxiv-1702-06257 | The Power of Sparsity in Convolutional Neural Networks | http://arxiv.org/abs/1702.06257 | id:1702.06257 author:Soravit Changpinyo, Mark Sandler, Andrey Zhmoginov category:cs.CV  published:2017-02-21 summary:Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3. version:1
arxiv-1702-06247 | SAR: A Semantic Analysis Approach for Recommendation | http://arxiv.org/abs/1702.06247 | id:1702.06247 author:Han Xiao, Minlie Huang, Xiaoyan Zhu category:cs.IR cs.LG  published:2017-02-21 summary:Recommendation system is a common demand in daily life and matrix completion is a widely adopted technique for this task. However, most matrix completion methods lack semantic interpretation and usually result in weak-semantic recommendations. To this end, this paper proposes a {\bf S}emantic {\bf A}nalysis approach for {\bf R}ecommendation systems \textbf{(SAR)}, which applies a two-level hierarchical generative process that assigns semantic properties and categories for user and item. SAR learns semantic representations of users/items merely from user ratings on items, which offers a new path to recommendation by semantic matching with the learned representations. Extensive experiments demonstrate SAR outperforms other state-of-the-art baselines substantially. version:1
arxiv-1702-06240 | Best Linear Predictor with Missing Response: Locally Robust Approach | http://arxiv.org/abs/1702.06240 | id:1702.06240 author:Victor Chernozhukov, Vira Semenova category:stat.ME stat.ML  published:2017-02-21 summary:This paper provides asymptotic theory for Inverse Probability Weighing (IPW) and Locally Robust Estimator (LRE) of Best Linear Predictor where the response missing at random (MAR), but not completely at random (MCAR). We relax previous assumptions in the literature about the first-step nonparametric components, requiring only their mean square convergence. This relaxation allows to use a wider class of machine leaning methods for the first-step, such as lasso. For a generic first-step, IPW incurs a first-order bias unless the model it approximates is truly linear in the predictors. In contrast, LRE remains first-order unbiased provided one can estimate the conditional expectation of the response with sufficient accuracy. An additional novelty is allowing the dimension of Best Linear Predictor to grow with sample size. These relaxations are important for estimation of best linear predictor of teacher-specific and hospital-specific effects with large number of individuals. version:1
arxiv-1702-06239 | Reinforcement Learning Based Argument Component Detection | http://arxiv.org/abs/1702.06239 | id:1702.06239 author:Yang Gao, Hao Wang, Chen Zhang, Wei Wang category:cs.CL  published:2017-02-21 summary:Argument component detection (ACD) is an important sub-task in argumentation mining. ACD aims at detecting and classifying different argument components in natural language texts. Historical annotations (HAs) are important features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by existing automatic ACD techniques. Reinforcement learning (RL) has proven to be an effective method for using HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance on two well-annotated corpora. Results suggest that, in terms of classification accuracy, HAs-augmented RL outperforms plain RL by at most 17.85%, and outperforms the state-of-the-art supervised learning algorithm by at most 11.94%. version:1
arxiv-1702-06238 | Sample Efficient Policy Search for Optimal Stopping Domains | http://arxiv.org/abs/1702.06238 | id:1702.06238 author:Karan Goel, Christoph Dann, Emma Brunskill category:cs.AI cs.LG  published:2017-02-21 summary:Arising naturally in many fields, optimal stopping problems consider the question of deciding when to stop an observation-generating process. We examine the problem of simultaneously learning and planning in such domains, when data is collected directly from the environment. We propose GFSE, a simple and flexible model-free policy search method that reuses data for sample efficiency by leveraging problem structure. We bound the sample complexity of our approach to guarantee uniform convergence of policy value estimates, tightening existing PAC bounds to achieve logarithmic dependence on horizon length for our setting. We also examine the benefit of our method against prevalent model-based and model-free approaches on 3 domains taken from diverse fields. version:1
arxiv-1702-06235 | Learning to generate one-sentence biographies from Wikidata | http://arxiv.org/abs/1702.06235 | id:1702.06235 author:Andrew Chisholm, Will Radford, Ben Hachey category:cs.CL  published:2017-02-21 summary:We investigate the generation of one-sentence Wikipedia biographies from facts derived from Wikidata slot-value pairs. We train a recurrent neural network sequence-to-sequence model with attention to select facts and generate textual summaries. Our model incorporates a novel secondary objective that helps ensure it generates sentences that contain the input facts. The model achieves a BLEU score of 41, improving significantly upon the vanilla sequence-to-sequence model and scoring roughly twice that of a simple template baseline. Human preference evaluation suggests the model is nearly as good as the Wikipedia reference. Manual analysis explores content selection, suggesting the model can trade the ability to infer knowledge against the risk of hallucinating incorrect information. version:1
arxiv-1702-06234 | On a Class of First-order Primal-Dual Algorithms for Composite Convex Minimization Problems | http://arxiv.org/abs/1702.06234 | id:1702.06234 author:Seyoon Ko, Donghyeon Yu, Joong-Ho Won category:stat.ML  published:2017-02-21 summary:Many statistical learning problems can be posed as minimization of sum of two convex functions, one typically non-smooth. Popular algorithms for solving such problems, e.g., ADMM, often involve non-trivial optimization subproblems or smoothing approximation. We study two classes of algorithms that do not incur these difficulties, and unify them from a perspective of monotone operator theory. The result is a class of preconditioned forward-backward algorithms with a novel family of preconditioners. We analyze convergence of the whole class of algorithms, and obtain their rates of convergence for the range of algorithm parameters where convergence is known but rates have been missing. We demonstrate the scalability of our algorithm class with a distributed implementation. version:1
arxiv-1702-06228 | Learning to Generate Posters of Scientific Papers by Probabilistic Graphical Models | http://arxiv.org/abs/1702.06228 | id:1702.06228 author:Yu-ting Qiang, Yanwei Fu, Xiao Yu, Yanwen Guo, Zhi-Hua Zhou, Leonid Sigal category:cs.CV cs.GR cs.HC cs.MM  published:2017-02-21 summary:Researchers often summarize their work in the form of scientific posters. Posters provide a coherent and efficient way to convey core ideas expressed in scientific papers. Generating a good scientific poster, however, is a complex and time consuming cognitive task, since such posters need to be readable, informative, and visually aesthetic. In this paper, for the first time, we study the challenging problem of learning to generate posters from scientific papers. To this end, a data-driven framework, that utilizes graphical models, is proposed. Specifically, given content to display, the key elements of a good poster, including attributes of each panel and arrangements of graphical elements are learned and inferred from data. During the inference stage, an MAP inference framework is employed to incorporate some design principles. In order to bridge the gap between panel attributes and the composition within each panel, we also propose a recursive page splitting algorithm to generate the panel layout for a poster. To learn and validate our model, we collect and release a new benchmark dataset, called NJU-Fudan Paper-Poster dataset, which consists of scientific papers and corresponding posters with exhaustively labelled panels and attributes. Qualitative and quantitative results indicate the effectiveness of our approach. version:1
arxiv-1702-06221 | Determination of hysteresis in finite-state random walks using Bayesian cross validation | http://arxiv.org/abs/1702.06221 | id:1702.06221 author:Joshua C. Chang category:stat.ME cs.LG physics.data-an q-bio.QM  published:2017-02-21 summary:Consider the problem of modeling hysteresis for finite-state random walks using higher-order Markov chains. This Letter introduces a Bayesian framework to determine, from data, the number of prior states of recent history upon which a trajectory is statistically dependent. The general recommendation is to use leave-one-out cross validation, using an easily-computable formula that is provided in closed form. Importantly, Bayes factors using flat model priors are biased in favor of too-complex a model (more hysteresis) when a large amount of data is present and the Akaike information criterion (AIC) is biased in favor of too-sparse a model (less hysteresis) when few data are present. version:1
arxiv-1702-06219 | An Online Optimization Approach for Multi-Agent Tracking of Dynamic Parameters in the Presence of Adversarial Noise | http://arxiv.org/abs/1702.06219 | id:1702.06219 author:Shahin Shahrampour, Ali Jadbabaie category:math.OC cs.MA stat.ML  published:2017-02-21 summary:This paper addresses tracking of a moving target in a multi-agent network. The target follows a linear dynamics corrupted by an adversarial noise, i.e., the noise is not generated from a statistical distribution. The location of the target at each time induces a global time-varying loss function, and the global loss is a sum of local losses, each of which is associated to one agent. Agents noisy observations could be nonlinear. We formulate this problem as a distributed online optimization where agents communicate with each other to track the minimizer of the global loss. We then propose a decentralized version of the Mirror Descent algorithm and provide the non-asymptotic analysis of the problem. Using the notion of dynamic regret, we measure the performance of our algorithm versus its offline counterpart in the centralized setting. We prove that the bound on dynamic regret scales inversely in the network spectral gap, and it represents the adversarial noise causing deviation with respect to the linear dynamics. Our result subsumes a number of results in the distributed optimization literature. Finally, in a numerical experiment, we verify that our algorithm can be simply implemented for multi-agent tracking with nonlinear observations. version:1
arxiv-1702-06813 | RenderMap: Exploiting the Link Between Perception and Rendering for Dense Mapping | http://arxiv.org/abs/1702.06813 | id:1702.06813 author:Julian Ryde, Xuchu, Ding category:cs.CV cs.RO  published:2017-02-21 summary:We introduce an approach for the real-time (2Hz) creation of a dense map and alignment of a moving robotic agent within that map by rendering using a Graphics Processing Unit (GPU). This is done by recasting the scan alignment part of the dense mapping process as a rendering task. Alignment errors are computed from rendering the scene, comparing with range data from the sensors, and minimized by an optimizer. The proposed approach takes advantage of the advances in rendering techniques for computer graphics and GPU hardware to accelerate the algorithm. Moreover, it allows one to exploit information not used in classic dense mapping algorithms such as Iterative Closest Point (ICP) by rendering interfaces between the free space, occupied space and the unknown. The proposed approach leverages directly the rendering capabilities of the GPU, in contrast to other GPU-based approaches that deploy the GPU as a general purpose parallel computation platform. We argue that the proposed concept is a general consequence of treating perception problems as inverse problems of rendering. Many perception problems can be recast into a form where much of the computation is replaced by render operations. This is not only efficient since rendering is fast, but also simpler to implement and will naturally benefit from future advancements in GPU speed and rendering techniques. Furthermore, this general concept can go beyond addressing perception problems and can be used for other problem domains such as path planning. version:1
arxiv-1702-06216 | Filtering Tweets for Social Unrest | http://arxiv.org/abs/1702.06216 | id:1702.06216 author:Alan Mishler, Kevin Wonus, Wendy Chambers, Michael Bloodgood category:cs.CL cs.IR cs.LG stat.ML  published:2017-02-20 summary:Since the events of the Arab Spring, there has been increased interest in using social media to anticipate social unrest. While efforts have been made toward automated unrest prediction, we focus on filtering the vast volume of tweets to identify tweets relevant to unrest, which can be provided to downstream users for further analysis. We train a supervised classifier that is able to label Arabic language tweets as relevant to unrest with high reliability. We examine the relationship between training data size and performance and investigate ways to optimize the model building process while minimizing cost. We also explore how confidence thresholds can be set to achieve desired levels of performance. version:1
arxiv-1702-06212 | Efficient Dense Labeling of Human Activity Sequences from Wearables using Fully Convolutional Networks | http://arxiv.org/abs/1702.06212 | id:1702.06212 author:Rui Yao, Guosheng Lin, Qinfeng Shi, Damith Ranasinghe category:cs.CV cs.HC  published:2017-02-20 summary:Recognizing human activities in a sequence is a challenging area of research in ubiquitous computing. Most approaches use a fixed size sliding window over consecutive samples to extract features---either handcrafted or learned features---and predict a single label for all samples in the window. Two key problems emanate from this approach: i) the samples in one window may not always share the same label. Consequently, using one label for all samples within a window inevitably lead to loss of information; ii) the testing phase is constrained by the window size selected during training while the best window size is difficult to tune in practice. We propose an efficient algorithm that can predict the label of each sample, which we call dense labeling, in a sequence of human activities of arbitrary length using a fully convolutional network. In particular, our approach overcomes the problems posed by the sliding window step. Additionally, our algorithm learns both the features and classifier automatically. We release a new daily activity dataset based on a wearable sensor with hospitalized patients. We conduct extensive experiments and demonstrate that our proposed approach is able to outperform the state-of-the-arts in terms of classification and label misalignment measures on three challenging datasets: Opportunity, Hand Gesture, and our new dataset. version:1
arxiv-1702-06209 | Uniform Inference for High-dimensional Quantile Regression: Linear Functionals and Regression Rank Scores | http://arxiv.org/abs/1702.06209 | id:1702.06209 author:Jelena Bradic, Mladen Kolar category:stat.ML  published:2017-02-20 summary:Hypothesis tests in models whose dimension far exceeds the sample size can be formulated much like the classical studentized tests only after the initial bias of estimation is removed successfully. The theory of debiased estimators can be developed in the context of quantile regression models for a fixed quantile value. However, it is frequently desirable to formulate tests based on the quantile regression process, as this leads to more robust tests and more stable confidence sets. Additionally, inference in quantile regression requires estimation of the so called sparsity function, which depends on the unknown density of the error. In this paper we consider a debiasing approach for the uniform testing problem. We develop high-dimensional regression rank scores and show how to use them to estimate the sparsity function, as well as how to adapt them for inference involving the quantile regression process. Furthermore, we develop a Kolmogorov-Smirnov test in a location-shift high-dimensional models and confidence sets that are uniformly valid for many quantile values. The main technical result are the development of a Bahadur representation of the debiasing estimator that is uniform over a range of quantiles and uniform convergence of the quantile process to the Brownian bridge process, which are of independent interest. Simulation studies illustrate finite sample properties of our procedure. version:1
arxiv-1702-06175 | Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization | http://arxiv.org/abs/1702.06175 | id:1702.06175 author:Mahdi Soltanolkotabi category:cs.LG cs.IT math.FA math.IT math.OC stat.ML  published:2017-02-20 summary:This paper concerns the problem of recovering an unknown but structured signal $x \in R^n$ from $m$ quadratic measurements of the form $y_r= <a_r,x> ^2$ for $r=1,2,...,m$. We focus on the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal ($m<<n$). We formulate the recovery problem as a nonconvex optimization problem where prior structural information about the signal is enforced through constrains on the optimization variables. We prove that projected gradient descent, when initialized in a neighborhood of the desired signal, converges to the unknown signal at a linear rate. These results hold for any constraint set (convex or nonconvex) providing convergence guarantees to the global optimum even when the objective function and constraint set is nonconvex. Furthermore, these results hold with a number of measurements that is only a constant factor away from the minimal number of measurements required to uniquely identify the unknown signal. Our results provide the first provably tractable algorithm for this data-poor regime, breaking local sample complexity barriers that have emerged in recent literature. In a companion paper we demonstrate favorable properties for the optimization problem that may enable similar results to continue to hold more globally (over the entire ambient space). Collectively these two papers utilize and develop powerful tools for uniform convergence of empirical processes that may have broader implications for rigorous understanding of constrained nonconvex optimization heuristics. The mathematical results in this paper also pave the way for a new generation of data-driven phase-less imaging systems that can utilize prior information to significantly reduce acquisition time and enhance image reconstruction, enabling nano-scale imaging at unprecedented speeds and resolutions. version:1
arxiv-1702-06151 | Developing a comprehensive framework for multimodal feature extraction | http://arxiv.org/abs/1702.06151 | id:1702.06151 author:Quinten McNamara, Alejandro de la Vega, Tal Yarkoni category:cs.CV cs.IR cs.LG cs.MM  published:2017-02-20 summary:Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions---ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (video, images, audio, and text), and is expressly with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of complex feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its major advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct sophisticated feature extraction workflows while increasing code clarity and maintainability. version:1
arxiv-1702-06106 | An Attention-Based Deep Net for Learning to Rank | http://arxiv.org/abs/1702.06106 | id:1702.06106 author:Baiyang Wang, Diego Klabjan category:cs.LG  published:2017-02-20 summary:In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets. version:1
arxiv-1702-06103 | An Improved Parametrization and Analysis of the EXP3++ Algorithm for Stochastic and Adversarial Bandits | http://arxiv.org/abs/1702.06103 | id:1702.06103 author:Yevgeny Seldin, Gábor Lugosi category:cs.LG stat.ML  published:2017-02-20 summary:We present a new strategy for gap estimation in randomized algorithms for multiarmed bandits and combine it with the EXP3++ algorithm of Seldin and Slivkins (2014). In the stochastic regime the strategy reduces dependence of regret on a time horizon from $(\ln t)^3$ to $(\ln t)^2$ and replaces an additive factor of order $\Delta e^{1/\Delta^2}$ by an additive factor of order $1/\Delta^7$, where $\Delta$ is the minimal gap of a problem instance. In the adversarial regime regret guarantee remains unchanged. version:1
arxiv-1702-06086 | Label Distribution Learning Forests | http://arxiv.org/abs/1702.06086 | id:1702.06086 author:Wei Shen, Kai Zhao, Yilu Guo, Alan Yuille category:cs.LG cs.CV  published:2017-02-20 summary:Label distribution learning (LDL) is a general learning framework, which assigns a distribution over a set of labels to an instance rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by the mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning, e.g., to learn deep features in an end-to-end manner. We define a distribution-based loss function for forests, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on two LDL problems, including age estimation and crowd opinion prediction on movies, showing significant improvements to the state-of-the-art LDL methods. version:1
arxiv-1702-06085 | Synthesis versus analysis in patch-based image priors | http://arxiv.org/abs/1702.06085 | id:1702.06085 author:Mario A. T. Figueiredo category:cs.CV 94A08 I.4.0  published:2017-02-20 summary:In global models/priors (for example, using wavelet frames), there is a well known analysis vs synthesis dichotomy in the way signal/image priors are formulated. In patch-based image models/priors, this dichotomy is also present in the choice of how each patch is modeled. This paper shows that there is another analysis vs synthesis dichotomy, in terms of how the whole image is related to the patches, and that all existing patch-based formulations that provide a global image prior belong to the analysis category. We then propose a synthesis formulation, where the image is explicitly modeled as being synthesized by additively combining a collection of independent patches. We formally establish that these analysis and synthesis formulations are not equivalent in general and that both formulations are compatible with analysis and synthesis formulations at the patch level. Finally, we present an instance of the alternating direction method of multipliers (ADMM) that can be used to perform image denoising under the proposed synthesis formulation, showing its computational feasibility. Rather than showing the superiority of the synthesis or analysis formulations, the contributions of this paper is to establish the existence of both alternatives, thus closing the corresponding gap in the field of patch-based image processing. version:1
arxiv-1702-06081 | Learning Non-Discriminatory Predictors | http://arxiv.org/abs/1702.06081 | id:1702.06081 author:Blake Woodworth, Suriya Gunasekar, Mesrob I. Ohannessian, Nathan Srebro category:cs.LG  published:2017-02-20 summary:We consider learning a predictor which is non-discriminatory with respect to a "protected attribute" according to the notion of "equalized odds" proposed by Hardt et al. [2016]. We study the problem of learning such a non-discriminatory predictor from a finite training set, both statistically and computationally. We show that a post-hoc correction approach, as suggested by Hardt et al, can be highly suboptimal, present a nearly-optimal statistical procedure, argue that the associated computational problem is intractable, and suggest a second moment relaxation of the non-discrimination definition for which learning is tractable. version:1
arxiv-1702-06064 | RESPARC: A Reconfigurable and Energy-Efficient Architecture with Memristive Crossbars for Deep Spiking Neural Networks | http://arxiv.org/abs/1702.06064 | id:1702.06064 author:Aayush Ankit, Abhronil Sengupta, Priyadarshini Panda, Kaushik Roy category:cs.ET cs.NE  published:2017-02-20 summary:Neuromorphic computing using post-CMOS technologies is gaining immense popularity due to its promising abilities to address the memory and power bottlenecks in von-Neumann computing systems. In this paper, we propose RESPARC - a reconfigurable and energy efficient architecture built-on Memristive Crossbar Arrays (MCA) for deep Spiking Neural Networks (SNNs). Prior works were primarily focused on device and circuit implementations of SNNs on crossbars. RESPARC advances this by proposing a complete system for SNN acceleration and its subsequent analysis. RESPARC utilizes the energy-efficiency of MCAs for inner-product computation and realizes a hierarchical reconfigurable design to incorporate the data-flow patterns in an SNN in a scalable fashion. We evaluate the proposed architecture on different SNNs ranging in complexity from 2k-230k neurons and 1.2M-5.5M synapses. Simulation results on these networks show that compared to the baseline digital CMOS architecture, RESPARC achieves 500X (15X) efficiency in energy benefits at 300X (60X) higher throughput for multi-layer perceptrons (deep convolutional networks). Furthermore, RESPARC is a technology-aware architecture that maps a given SNN topology to the most optimized MCA size for the given crossbar technology. version:1
arxiv-1702-06054 | Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning | http://arxiv.org/abs/1702.06054 | id:1702.06054 author:Sahil Sharma, Aravind S. Lakshminarayanan, Balaraman Ravindran category:cs.LG cs.AI cs.NE  published:2017-02-20 summary:Reinforcement Learning algorithms can learn complex behavioral patterns for sequential decision making tasks wherein an agent interacts with an environment and acquires feedback in the form of rewards sampled from it. Traditionally, such algorithms make decisions, i.e., select actions to execute, at every single time step of the agent-environment interactions. In this paper, we propose a novel framework, Fine Grained Action Repetition (FiGAR), which enables the agent to decide the action as well as the time scale of repeating it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm which maintains an explicit policy estimate by enabling temporal abstractions in the action space. We empirically demonstrate the efficacy of our framework by showing performance improvements on top of three policy search algorithms in different domains: Asynchronous Advantage Actor Critic in the Atari 2600 domain, Trust Region Policy Optimization in Mujoco domain and Deep Deterministic Policy Gradients in the TORCS car racing domain. version:1
arxiv-1702-06027 | Parent Oriented Teacher Selection Causes Language Diversity | http://arxiv.org/abs/1702.06027 | id:1702.06027 author:Ibrahim Cimentepe, Haluk O. Bingol category:cs.CL  published:2017-02-20 summary:An evolutionary model for emergence of diversity in language is developed. We investigated the effects of two real life observations, namely, people prefer people that they communicate with and people interact with people that are physically close to each other. Clearly these groups are relatively small compared to the entire population. We restrict selection of the teachers from such small groups, called imitation sets, around parents. Then child learns language from a teacher selected within the imitation set of her parent. As a result, there are subcommunities with their own languages developed. Within subcommunity comprehension is found to be high. The number of languages is related to relative size of imitation set by a power law. version:1
arxiv-1702-05993 | An Extended Framework for Marginalized Domain Adaptation | http://arxiv.org/abs/1702.05993 | id:1702.05993 author:Gabriela Csurka, Boris Chidlovski, Stephane Clinchant, Sophia Michel category:cs.CV cs.LG  published:2017-02-20 summary:We propose an extended framework for marginalized domain adaptation, aimed at addressing unsupervised, supervised and semi-supervised scenarios. We argue that the denoising principle should be extended to explicitly promote domain-invariant features as well as help the classification task. Therefore we propose to jointly learn the data auto-encoders and the target classifiers. First, in order to make the denoised features domain-invariant, we propose a domain regularization that may be either a domain prediction loss or a maximum mean discrepancy between the source and target data. The noise marginalization in this case is reduced to solving the linear matrix system $AX=B$ which has a closed-form solution. Second, in order to help the classification, we include a class regularization term. Adding this component reduces the learning problem to solving a Sylvester linear matrix equation $AX+BX=C$, for which an efficient iterative procedure exists as well. We did an extensive study to assess how these regularization terms improve the baseline performance in the three domain adaptation scenarios and present experimental results on two image and one text benchmark datasets, conventionally used for validating domain adaptation methods. We report our findings and comparison with state-of-the-art methods. version:1
arxiv-1702-05983 | Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN | http://arxiv.org/abs/1702.05983 | id:1702.05983 author:Weiwei Hu, Ying Tan category:cs.LG cs.CR  published:2017-02-20 summary:Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms. Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples' malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work. version:1
arxiv-1702-05962 | Latent Variable Dialogue Models and their Diversity | http://arxiv.org/abs/1702.05962 | id:1702.05962 author:Kris Cao, Stephen Clark category:cs.CL  published:2017-02-20 summary:We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the `boring output' issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model. version:1
arxiv-1702-05958 | Reflection Separation Using Guided Annotation | http://arxiv.org/abs/1702.05958 | id:1702.05958 author:Ofer Springer, Yair Weiss category:cs.CV  published:2017-02-20 summary:Photographs taken through a glass surface often contain an approximately linear superposition of reflected and transmitted layers. Decomposing an image into these layers is generally an ill-posed task and the use of an additional image prior and user provided cues is presently necessary in order to obtain good results. Current annotation approaches rely on a strong sparsity assumption. For images with significant texture this assumption does not typically hold, thus rendering the annotation process unviable. In this paper we show that using a Gaussian Mixture Model patch prior, the correct local decomposition can almost always be found as one of 100 likely modes of the posterior. Thus, the user need only choose one of these modes in a sparse set of patches and the decomposition may then be completed automatically. We demonstrate the performance of our method using synthesized and real reflection images. version:1
arxiv-1702-05941 | SurvivalNet: Predicting patient survival from diffusion weighted magnetic resonance images using cascaded fully convolutional and 3D convolutional neural networks | http://arxiv.org/abs/1702.05941 | id:1702.05941 author:Patrick Ferdinand Christ, Florian Ettlinger, Georgios Kaissis, Sebastian Schlecht, Freba Ahmaddy, Felix Grün, Alexander Valentinitsch, Seyed-Ahmad Ahmadi, Rickmer Braren, Bjoern Menze category:cs.CV  published:2017-02-20 summary:Automatic non-invasive assessment of hepatocellular carcinoma (HCC) malignancy has the potential to substantially enhance tumor treatment strategies for HCC patients. In this work we present a novel framework to automatically characterize the malignancy of HCC lesions from DWI images. We predict HCC malignancy in two steps: As a first step we automatically segment HCC tumor lesions using cascaded fully convolutional neural networks (CFCN). A 3D neural network (SurvivalNet) then predicts the HCC lesions' malignancy from the HCC tumor segmentation. We formulate this task as a classification problem with classes being "low risk" and "high risk" represented by longer or shorter survival times than the median survival. We evaluated our method on DWI of 31 HCC patients. Our proposed framework achieves an end-to-end accuracy of 65% with a Dice score for the automatic lesion segmentation of 69% and an accuracy of 68% for tumor malignancy classification based on expert annotations. We compared the SurvivalNet to classical handcrafted features such as Histogram and Haralick and show experimentally that SurvivalNet outperforms the handcrafted features in HCC malignancy classification. End-to-end assessment of tumor malignancy based on our proposed fully automatic framework corresponds to assessment based on expert annotations with high significance (p>0.95). version:1
arxiv-1702-05939 | An Efficient Method for online Detection of Polychronous Patterns in Spiking Neural Network | http://arxiv.org/abs/1702.05939 | id:1702.05939 author:Joseph Chrol-Cannon, Yaochu Jin, André Grüning category:cs.NE q-bio.NC 00-01  99-00  published:2017-02-20 summary:Polychronous neural groups are effective structures for the recognition of precise spike-timing patterns but the detection method is an inefficient multi-stage brute force process that works off-line on pre-recorded simulation data. This work presents a new model of polychronous patterns that can capture precise sequences of spikes directly in the neural simulation. In this scheme, each neuron is assigned a randomized code that is used to tag the post-synaptic neurons whenever a spike is transmitted. This creates a polychronous code that preserves the order of pre-synaptic activity and can be registered in a hash table when the post-synaptic neuron spikes. A polychronous code is a sub-component of a polychronous group that will occur, along with others, when the group is active. We demonstrate the representational and pattern recognition ability of polychronous codes on a direction selective visual task involving moving bars that is typical of a computation performed by simple cells in the cortex. The computational efficiency of the proposed algorithm far exceeds existing polychronous group detection methods and is well suited for online detection. version:1
arxiv-1702-05931 | The importance of stain normalization in colorectal tissue classification with convolutional networks | http://arxiv.org/abs/1702.05931 | id:1702.05931 author:Francesco Ciompi, Oscar Geessink, Babak Ehteshami Bejnordi, Gabriel Silva de Souza, Alexi Baidoshvili, Geert Litjens, Bram van Ginneken, Iris Nagtegaal, Jeroen van der Laak category:cs.CV cs.LG  published:2017-02-20 summary:The development of reliable imaging biomarkers for the analysis of colorectal cancer (CRC) in hematoxylin and eosin (H&E) stained histopathology images requires an accurate and reproducible classification of the main tissue components in the image. In this paper, we propose a system for CRC tissue classification based on convolutional networks (ConvNets). We investigate the importance of stain normalization in tissue classification of CRC tissue samples in H&E-stained images. Furthermore, we report the performance of ConvNets on a cohort of rectal cancer samples and on an independent publicly available dataset of colorectal H&E images. version:1
arxiv-1702-06120 | On the Consistency of $k$-means++ algorithm | http://arxiv.org/abs/1702.06120 | id:1702.06120 author:Mieczysław A. Kłopotek category:cs.LG  published:2017-02-20 summary:We prove in this paper that the expected value of the objective function of the $k$-means++ algorithm for samples converges to population expected value. As $k$-means++, for samples, provides with constant factor approximation for $k$-means objectives, such an approximation can be achieved for the population with increase of the sample size. This result is of potential practical relevance when one is considering using subsampling when clustering large data sets (large data bases). version:1
arxiv-1702-05911 | Efficient Large-scale Approximate Nearest Neighbor Search on the GPU | http://arxiv.org/abs/1702.05911 | id:1702.05911 author:Patrick Wieschollek, Oliver Wang, Alexander Sorkine-Hornung, Hendrik P. A. Lensch category:cs.CV  published:2017-02-20 summary:We present a new approach for efficient approximate nearest neighbor (ANN) search in high dimensional spaces, extending the idea of Product Quantization. We propose a two-level product and vector quantization tree that reduces the number of vector comparisons required during tree traversal. Our approach also includes a novel highly parallelizable re-ranking method for candidate vectors by efficiently reusing already computed intermediate values. Due to its small memory footprint during traversal, the method lends itself to an efficient, parallel GPU implementation. This Product Quantization Tree (PQT) approach significantly outperforms recent state of the art methods for high dimensional nearest neighbor queries on standard reference datasets. Ours is the first work that demonstrates GPU performance superior to CPU performance on high dimensional, large scale ANN problems in time-critical real-world applications, like loop-closing in videos. version:1
arxiv-1702-05891 | Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification | http://arxiv.org/abs/1702.05891 | id:1702.05891 author:Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, Xiaogang Wang category:cs.CV  published:2017-02-20 summary:Multi-label image classification is a fundamental but challenging task in computer vision. Great progress has been achieved by exploiting semantic relations between labels in recent years. However, conventional approaches are unable to model the underlying spatial relations between labels in multi-label images, because spatial annotations of the labels are generally not provided. In this paper, we propose a unified deep neural network that exploits both semantic and spatial relations between labels with only image-level supervisions. Given a multi-label image, our proposed Spatial Regularization Network (SRN) generates attention maps for all labels and captures the underlying relations between them via learnable convolutions. By aggregating the regularized classification results with original results by a ResNet-101 network, the classification performance can be consistently improved. The whole deep neural network is trained end-to-end with only image-level annotations, thus requires no additional efforts on image annotations. Extensive evaluations on 3 public datasets with different types of labels show that our approach significantly outperforms state-of-the-arts and has strong generalization capability. Analysis of the learned SRN model demonstrates that it can effectively capture both semantic and spatial relations of labels for improving classification performance. version:1
arxiv-1702-05888 | Memory Efficient Max Flow for Multi-label Submodular MRFs | http://arxiv.org/abs/1702.05888 | id:1702.05888 author:Thalaiyasingam Ajanthan, Richard Hartley, Mathieu Salzmann category:cs.DS cs.CV G.2.2; F.2.2; I.4.0  published:2017-02-20 summary:Multi-label submodular Markov Random Fields (MRFs) have been shown to be solvable using max-flow based on an encoding of the labels proposed by Ishikawa, in which each variable $X_i$ is represented by $\ell$ nodes (where $\ell$ is the number of labels) arranged in a column. However, this method in general requires $2\,\ell^2$ edges for each pair of neighbouring variables. This makes it inapplicable to realistic problems with many variables and labels, due to excessive memory requirement. In this paper, we introduce a variant of the max-flow algorithm that requires much less storage. Consequently, our algorithm makes it possible to optimally solve multi-label submodular problems involving large numbers of variables and labels on a standard computer. version:1
arxiv-1702-05882 | Phase Diagram of Restricted Boltzmann Machines and Generalised Hopfield Networks with Arbitrary Priors | http://arxiv.org/abs/1702.05882 | id:1702.05882 author:Adriano Barra, Giuseppe Genovese, Daniele Tantari, Peter Sollich category:cond-mat.dis-nn cs.LG physics.data-an stat.ML  published:2017-02-20 summary:Restricted Boltzmann Machines are described by the Gibbs measure of a bipartite spin glass, which in turn corresponds to the one of a generalised Hopfield network. This equivalence allows us to characterise the state of these systems in terms of retrieval capabilities, at both low and high load. We study the paramagnetic-spin glass and the spin glass-retrieval phase transitions, as the pattern (i.e. weight) distribution and spin (i.e. unit) priors vary smoothly from Gaussian real variables to Boolean discrete variables. Our analysis shows that the presence of a retrieval phase is robust and not peculiar to the standard Hopfield model with Boolean patterns. The retrieval region is larger when the pattern entries and retrieval units get more peaked and, conversely, when the hidden units acquire a broader prior and therefore have a stronger response to high fields. Moreover, at low load retrieval always exists below some critical temperature, for every pattern distribution ranging from the Boolean to the Gaussian case. version:1
arxiv-1702-05878 | From Photo Streams to Evolving Situations | http://arxiv.org/abs/1702.05878 | id:1702.05878 author:Mengfan Tang, Feiping Nie, Siripen Pongpaichet, Ramesh Jain category:cs.MM cs.CV  published:2017-02-20 summary:Photos are becoming spontaneous, objective, and universal sources of information. This paper develops evolving situation recognition using photo streams coming from disparate sources combined with the advances of deep learning. Using visual concepts in photos together with space and time information, we formulate the situation detection into a semi-supervised learning framework and propose new graph-based models to solve the problem. To extend the method for unknown situations, we introduce a soft label method which enables the traditional semi-supervised learning framework to accurately predict predefined labels as well as effectively form new clusters. To overcome the noisy data which degrades graph quality, leading to poor recognition results, we take advantage of two kinds of noise-robust norms which can eliminate the adverse effects of outliers in visual concepts and improve the accuracy of situation recognition. Finally, we demonstrate the idea and the effectiveness of the proposed model on Yahoo Flickr Creative Commons 100 Million. version:1
arxiv-1702-05865 | Hemingway: Modeling Distributed Optimization Algorithms | http://arxiv.org/abs/1702.05865 | id:1702.05865 author:Xinghao Pan, Shivaram Venkataraman, Zizheng Tai, Joseph Gonzalez category:cs.DC cs.AI cs.LG  published:2017-02-20 summary:Distributed optimization algorithms are widely used in many industrial machine learning applications. However choosing the appropriate algorithm and cluster size is often difficult for users as the performance and convergence rate of optimization algorithms vary with the size of the cluster. In this paper we make the case for an ML-optimizer that can select the appropriate algorithm and cluster size to use for a given problem. To do this we propose building two models: one that captures the system level characteristics of how computation, communication change as we increase cluster sizes and another that captures how convergence rates change with cluster sizes. We present preliminary results from our prototype implementation called Hemingway and discuss some of the challenges involved in developing such a system. version:1
arxiv-1702-06392 | A 7.663-TOPS 8.2-W Energy-efficient FPGA Accelerator for Binary Convolutional Neural Networks | http://arxiv.org/abs/1702.06392 | id:1702.06392 author:Yixing Li, Zichuan Liu, Kai Xu, Hao Yu, Fengbo Ren category:cs.DC cs.AR cs.CV cs.LG C.3  published:2017-02-20 summary:FPGA-based hardware accelerators for convolutional neural networks (CNNs) have obtained great attentions due to their higher energy efficiency than GPUs. However, it is challenging for FPGA-based solutions to achieve a higher throughput than GPU counterparts. In this paper, we demonstrate that FPGA acceleration can be a superior solution in terms of both throughput and energy efficiency when a CNN is trained with binary constraints on weights and activations. Specifically, we propose an optimized accelerator architecture tailored for bitwise convolution and normalization that features massive spatial parallelism with deep pipelines stages. Experiment results show that the proposed architecture is 8.3x faster and 75x more energy-efficient than a Titan X GPU for processing online individual requests (in small batch size). For processing static data (in large batch size), the proposed solution is on a par with a Titan X GPU in terms of throughput while delivering 9.5x higher energy efficiency. version:1
arxiv-1702-05839 | Progressively Diffused Networks for Semantic Image Segmentation | http://arxiv.org/abs/1702.05839 | id:1702.05839 author:Ruimao Zhang, Wei Yang, Zhanglin Peng, Xiaogang Wang, Liang Lin category:cs.CV  published:2017-02-20 summary:This paper introduces Progressively Diffused Networks (PDNs) for unifying multi-scale context modeling with deep feature learning, by taking semantic image segmentation as an exemplar application. Prior neural networks, such as ResNet, tend to enhance representational power by increasing the depth of architectures and driving the training objective across layers. However, we argue that spatial dependencies in different layers, which generally represent the rich contexts among data elements, are also critical to building deep and discriminative representations. To this end, our PDNs enables to progressively broadcast information over the learned feature maps by inserting a stack of information diffusion layers, each of which exploits multi-dimensional convolutional LSTMs (Long-Short-Term Memory Structures). In each LSTM unit, a special type of atrous filters are designed to capture the short range and long range dependencies from various neighbors to a certain site of the feature map and pass the accumulated information to the next layer. From the extensive experiments on semantic image segmentation benchmarks (e.g., ImageNet Parsing, PASCAL VOC2012 and PASCAL-Part), our framework demonstrates the effectiveness to substantially improve the performances over the popular existing neural network models, and achieves state-of-the-art on ImageNet Parsing for large scale semantic segmentation. version:1
arxiv-1702-05821 | Post-edit Analysis of Collective Biography Generation | http://arxiv.org/abs/1702.05821 | id:1702.05821 author:Bo Han, Will Radford, Anaïs Cadilhac, Art Harol, Andrew Chisholm, Ben Hachey category:cs.CL  published:2017-02-20 summary:Text generation is increasingly common but often requires manual post-editing where high precision is critical to end users. However, manual editing is expensive so we want to ensure this effort is focused on high-value tasks. And we want to maintain stylistic consistency, a particular challenge in crowd settings. We present a case study, analysing human post-editing in the context of a template-based biography generation system. An edit flow visualisation combined with manual characterisation of edits helps identify and prioritise work for improving end-to-end efficiency and accuracy. version:1
arxiv-1702-05815 | Compressive Embedding and Visualization using Graphs | http://arxiv.org/abs/1702.05815 | id:1702.05815 author:Johan Paratte, Nathanaël Perraudin, Pierre Vandergheynst category:cs.LG stat.ML  published:2017-02-19 summary:Visualizing high-dimensional data has been a focus in data analysis communities for decades, which has led to the design of many algorithms, some of which are now considered references (such as t-SNE for example). In our era of overwhelming data volumes, the scalability of such methods have become more and more important. In this work, we present a method which allows to apply any visualization or embedding algorithm on very large datasets by considering only a fraction of the data as input and then extending the information to all data points using a graph encoding its global similarity. We show that in most cases, using only $\mathcal{O}(\log(N))$ samples is sufficient to diffuse the information to all $N$ data points. In addition, we propose quantitative methods to measure the quality of embeddings and demonstrate the validity of our technique on both synthetic and real-world datasets. version:1
arxiv-1702-05803 | Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images | http://arxiv.org/abs/1702.05803 | id:1702.05803 author:Babak Ehteshami Bejnordi, Jimmy Linz, Ben Glass, Maeve Mullooly, Gretchen L Gierach, Mark E Sherman, Nico Karssemeijer, Jeroen van der Laak, Andrew H Beck category:cs.CV  published:2017-02-19 summary:Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor-associated stroma as a diagnostic biomarker. version:1
arxiv-1702-05800 | Revisiting Distributed Synchronous SGD | http://arxiv.org/abs/1702.05800 | id:1702.05800 author:Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, Rafal Jozefowicz category:cs.DC cs.AI cs.LG  published:2017-02-19 summary:Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies. version:1
arxiv-1702-05796 | Collaborative Deep Reinforcement Learning | http://arxiv.org/abs/1702.05796 | id:1702.05796 author:Kaixiang Lin, Shu Wang, Jiayu Zhou category:cs.LG  published:2017-02-19 summary:Besides independent learning, human learning process is highly improved by summarizing what has been learned, communicating it with peers, and subsequently fusing knowledge from different sources to assist the current learning goal. This collaborative learning procedure ensures that the knowledge is shared, continuously refined, and concluded from different perspectives to construct a more profound understanding. The idea of knowledge transfer has led to many advances in machine learning and data mining, but significant challenges remain, especially when it comes to reinforcement learning, heterogeneous model structures, and different learning tasks. Motivated by human collaborative learning, in this paper we propose a collaborative deep reinforcement learning (CDRL) framework that performs adaptive knowledge transfer among heterogeneous learning agents. Specifically, the proposed CDRL conducts a novel deep knowledge distillation method to address the heterogeneity among different learning tasks with a deep alignment network. Furthermore, we present an efficient collaborative Asynchronous Advantage Actor-Critic (cA3C) algorithm to incorporate deep knowledge distillation into the online training of agents, and demonstrate the effectiveness of the CDRL framework using extensive empirical evaluation on OpenAI gym. version:1
arxiv-1702-05793 | Harmonic Grammar, Optimality Theory, and Syntax Learnability: An Empirical Exploration of Czech Word Order | http://arxiv.org/abs/1702.05793 | id:1702.05793 author:Ann Irvine, Mark Dredze category:cs.CL  published:2017-02-19 summary:This work presents a systematic theoretical and empirical comparison of the major algorithms that have been proposed for learning Harmonic and Optimality Theory grammars (HG and OT, respectively). By comparing learning algorithms, we are also able to compare the closely related OT and HG frameworks themselves. Experimental results show that the additional expressivity of the HG framework over OT affords performance gains in the task of predicting the surface word order of Czech sentences. We compare the perceptron with the classic Gradual Learning Algorithm (GLA), which learns OT grammars, as well as the popular Maximum Entropy model. In addition to showing that the perceptron is theoretically appealing, our work shows that the performance of the HG model it learns approaches that of the upper bound in prediction accuracy on a held out test set and that it is capable of accurately modeling observed variation. version:1
arxiv-1702-05747 | A Survey on Deep Learning in Medical Image Analysis | http://arxiv.org/abs/1702.05747 | id:1702.05747 author:Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A. W. M. van der Laak, Bram van Ginneken, Clara I. Sánchez category:cs.CV  published:2017-02-19 summary:Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks and provide concise overviews of studies per application area. Open challenges and directions for future research are discussed. version:1
arxiv-1702-06118 | Derivative Based Focal Plane Array Nonuniformity Correction | http://arxiv.org/abs/1702.06118 | id:1702.06118 author:G. Ness, A. Oved, I. Kakon category:cs.CV  published:2017-02-19 summary:This paper presents a fast and robust method for fixed pattern noise nonuniformity correction of infrared focal plane arrays. The proposed method requires neither shutter nor elaborate calibrations and therefore enables a real time correction with no interruptions. Based on derivative estimation of the fixed pattern noise from pixel sized translations of the focal plane array, the proposed method has the advantages of being invariant to the noise magnitude and robust to unknown camera and inter-scene movements while being virtually transparent to the end-user. version:1
arxiv-1702-05729 | Person Search with Natural Language Description | http://arxiv.org/abs/1702.05729 | id:1702.05729 author:Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, Xiaogang Wang category:cs.CV  published:2017-02-19 summary:Searching persons in large-scale image databases with different types of queries is a challenging problem, but has important applications in intelligent surveillance. Existing methods mainly focused on searching persons with image-based or attribute-based queries. We argue that such queries have major limitations for practical applications. In this paper, we propose to solve a novel problem on person search with natural language description. Given one or several descriptive sentences of a person, the algorithm is required to rank all images in the database according to the sentence-image affinities, and retrieve the most related images to the description. Since there is no existing person dataset supporting this new research direction, we propose a large-scale person description dataset with language annotations on detailed information of person images from various sources. An innovative Recurrent Neural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to solve the new person search problem. Extensive experiments and comparisons with a wide range of possible solutions and baselines demonstrate the effectiveness of our proposed GNA-RNN framework. version:1
arxiv-1702-05711 | Zoom Out-and-In Network with Recursive Training for Object Proposal | http://arxiv.org/abs/1702.05711 | id:1702.05711 author:Hongyang Li, Yu Liu, Wanli Ouyang, Xiaogang Wang category:cs.CV  published:2017-02-19 summary:In this paper, we propose a zoom-out-and-in network for generating object proposals. We utilize different resolutions of feature maps in the network to detect object instances of various sizes. Specifically, we divide the anchor candidates into three clusters based on the scale size and place them on feature maps of distinct strides to detect small, medium and large objects, respectively. Deeper feature maps contain region-level semantics which can help shallow counterparts to identify small objects. Therefore we design a zoom-in sub-network to increase the resolution of high level features via a deconvolution operation. The high-level features with high resolution are then combined and merged with low-level features to detect objects. Furthermore, we devise a recursive training pipeline to consecutively regress region proposals at the training stage in order to match the iterative regression at the testing stage. We demonstrate the effectiveness of the proposed method on ILSVRC DET and MS COCO datasets, where our algorithm performs better than the state-of-the-arts in various evaluation metrics. It also increases average precision by around 2% in the detection system. version:1
arxiv-1702-05698 | Online Robust Principal Component Analysis with Change Point Detection | http://arxiv.org/abs/1702.05698 | id:1702.05698 author:Wei Xiao, Xiaolin Huang, Jorge Silva, Saba Emrani, Arin Chaudhuri category:cs.LG cs.CV stat.AP stat.CO stat.ML  published:2017-02-19 summary:Robust PCA methods are typically batch algorithms which requires loading all observations into memory before processing. This makes them inefficient to process big data. In this paper, we develop an efficient online robust principal component methods, namely online moving window robust principal component analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can successfully track not only slowly changing subspace but also abruptly changed subspace. By embedding hypothesis testing into the algorithm, OMWRPCA can detect change points of the underlying subspaces. Extensive simulation studies demonstrate the superior performance of OMWRPCA comparing with other state-of-art approach. We also apply the algorithm for real-time background subtraction of surveillance video. version:1
arxiv-1702-05695 | Non-negative Tensor Factorization for Human Behavioral Pattern Mining in Online Games | http://arxiv.org/abs/1702.05695 | id:1702.05695 author:Anna Sapienza, Alessandro Bessi, Emilio Ferrara category:cs.LG cs.HC cs.SI physics.soc-ph  published:2017-02-19 summary:Multiplayer online battle arena has become a popular game genre. It also received increasing attention from our research community because they provide a wealth of information about human interactions and behaviors. A major problem is extracting meaningful patterns of activity from this type of data, in a way that is also easy to interpret. Here, we propose to exploit tensor decomposition techniques, and in particular Non-negative Tensor Factorization, to discover hidden correlated behavioral patterns of play in a popular game: League of Legends. We first collect the entire gaming history of a group of about one thousand players, totaling roughly $100K$ matches. By applying our methodological framework, we then separate players into groups that exhibit similar features and playing strategies, as well as similar temporal trajectories, i.e., behavioral progressions over the course of their gaming history: this will allow us to investigate how players learn and improve their skills. version:1
arxiv-1702-05693 | CityPersons: A Diverse Dataset for Pedestrian Detection | http://arxiv.org/abs/1702.05693 | id:1702.05693 author:Shanshan Zhang, Rodrigo Benenson, Bernt Schiele category:cs.CV  published:2017-02-19 summary:Convnets have enabled significant progress in pedestrian detection recently, but there are still open questions regarding suitable architectures and training data. We revisit CNN design and point out key adaptations, enabling plain FasterRCNN to obtain state-of-the-art results on the Caltech dataset. To achieve further improvement from more and better data, we introduce CityPersons, a new set of person annotations on top of the Cityscapes dataset. The diversity of CityPersons allows us for the first time to train one single CNN model that generalizes well over multiple benchmarks. Moreover, with additional training with CityPersons, we obtain top results using FasterRCNN on Caltech, improving especially for more difficult cases (heavy occlusion and small scale) and providing higher localization quality. version:1
arxiv-1702-05678 | An Adaptivity Hierarchy Theorem for Property Testing | http://arxiv.org/abs/1702.05678 | id:1702.05678 author:Clement Canonne, Tom Gur category:cs.DS cs.LG  published:2017-02-19 summary:Adaptivity is known to play a crucial role in property testing. In particular, there exist properties for which there is an exponential gap between the power of \emph{adaptive} testing algorithms, wherein each query may be determined by the answers received to prior queries, and their \emph{non-adaptive} counterparts, in which all queries are independent of answers obtained from previous queries. In this work, we investigate the role of adaptivity in property testing at a finer level. We first quantify the degree of adaptivity of a testing algorithm by considering the number of "rounds of adaptivity" it uses. More accurately, we say that a tester is $k$-(round) adaptive if it makes queries in $k+1$ rounds, where the queries in the $i$'th round may depend on the answers obtained in the previous $i-1$ rounds. Then, we ask the following question: Does the power of testing algorithms smoothly grow with the number of rounds of adaptivity? We provide a positive answer to the foregoing question by proving an adaptivity hierarchy theorem for property testing. Specifically, our main result shows that for every $n\in \mathbb{N}$ and $0 \le k \le n^{0.99}$ there exists a property $\mathcal{P}_{n,k}$ of functions for which (1) there exists a $k$-adaptive tester for $\mathcal{P}_{n,k}$ with query complexity $\tilde{O}(k)$, yet (2) any $(k-1)$-adaptive tester for $\mathcal{P}_{n,k}$ must make $\Omega(n)$ queries. In addition, we show that such a qualitative adaptivity hierarchy can be witnessed for testing natural properties of graphs. version:1
arxiv-1702-05677 | Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC Classes | http://arxiv.org/abs/1702.05677 | id:1702.05677 author:Lunjia Hu, Ruihan Wu, Tianhong Li, Liwei Wang category:cs.LG cs.AI stat.ML  published:2017-02-18 summary:In this work we study the quantitative relation between the recursive teaching dimension (RTD) and the VC dimension (VCD) of concept classes of finite sizes. The RTD of a concept class $\mathcal C \subseteq \{0, 1\}^n$, introduced by Zilles et al. (2011), is a combinatorial complexity measure characterized by the worst-case number of examples necessary to identify a concept in $\mathcal C$ according to the recursive teaching model. For any finite concept class $\mathcal C \subseteq \{0,1\}^n$ with $\mathrm{VCD}(\mathcal C)=d$, Simon & Zilles (2015) posed an open problem $\mathrm{RTD}(\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD? Previously, the best known result is an exponential upper bound $\mathrm{RTD}(\mathcal C) = O(d \cdot 2^d)$, due to Chen et al. (2016). In this paper, we show a quadratic upper bound: $\mathrm{RTD}(\mathcal C) = O(d^2)$, much closer to an answer to the open problem. We also discuss the challenges in fully solving the problem. version:1
arxiv-1702-05664 | Robust Shape Registration using Fuzzy Correspondences | http://arxiv.org/abs/1702.05664 | id:1702.05664 author:Abhishek Kolagunda, Scott Sorensen, Philip Saponaro, Wayne Treible, Chandra Kambhamettu category:cs.CV  published:2017-02-18 summary:Shape registration is the process of aligning one 3D model to another. Most previous methods to align shapes with no known correspondences attempt to solve for both the transformation and correspondences iteratively. We present a shape registration approach that solves for the transformation using fuzzy correspondences to maximize the overlap between the given shape and the target shape. A coarse to fine approach with Levenberg-Marquardt method is used for optimization. Real and synthetic experiments show our approach is robust and outperforms other state of the art methods when point clouds are noisy, sparse, and have non-uniform density. Experiments show our method is more robust to initialization and can handle larger scale changes and rotation than other methods. We also show that the approach can be used for 2D-3D alignment via ray-point alignment. version:1
arxiv-1702-05663 | The Game Imitation: Deep Supervised Convolutional Networks for Quick Video Game AI | http://arxiv.org/abs/1702.05663 | id:1702.05663 author:Zhao Chen, Darvin Yi category:cs.CV  published:2017-02-18 summary:We present a vision-only model for gaming AI which uses a late integration deep convolutional network architecture trained in a purely supervised imitation learning context. Although state-of-the-art deep learning models for video game tasks generally rely on more complex methods such as deep-Q learning, we show that a supervised model which requires substantially fewer resources and training time can already perform well at human reaction speeds on the N64 classic game Super Smash Bros. We frame our learning task as a 30-class classification problem, and our CNN model achieves 80% top-1 and 95% top-3 validation accuracy. With slight test-time fine-tuning, our model is also competitive during live simulation with the highest-level AI built into the game. We will further show evidence through network visualizations that the network is successfully leveraging temporal information during inference to aid in decision making. Our work demonstrates that supervised CNN models can provide good performance in challenging policy prediction tasks while being significantly simpler and more lightweight than alternatives. version:1
arxiv-1702-05659 | On Loss Functions for Deep Neural Networks in Classification | http://arxiv.org/abs/1702.05659 | id:1702.05659 author:Katarzyna Janocha, Wojciech Marian Czarnecki category:cs.LG  published:2017-02-18 summary:Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones. version:1
arxiv-1702-05658 | MAT: A Multimodal Attentive Translator for Image Captioning | http://arxiv.org/abs/1702.05658 | id:1702.05658 author:Chang Liu, Fuchun Sun, Changhu Wang, Feng Wang, Alan Yuille category:cs.CV  published:2017-02-18 summary:In this work we formulate the problem of image captioning as a multimodal translation task. Analogous to machine translation, we present a sequence-to-sequence recurrent neural networks (RNN) model for image caption generation. Different from most existing work where the whole image is represented by convolutional neural network (CNN) feature, we propose to represent the input image as a sequence of detected objects which feeds as the source sequence of the RNN model. In this way, the sequential representation of an image can be naturally translated to a sequence of words, as the target sequence of the RNN model. To represent the image in a sequential way, we extract the objects features in the image and arrange them in a order using convolutional neural networks. To further leverage the visual information from the encoded objects, a sequential attention layer is introduced to selectively attend to the objects that are related to generate corresponding words in the sentences. Extensive experiments are conducted to validate the proposed approach on popular benchmark dataset, i.e., MS COCO, and the proposed model surpasses the state-of-the-art methods in all metrics following the dataset splits of previous work. The proposed approach is also evaluated by the evaluation server of MS COCO captioning challenge, and achieves very competitive results, e.g., a CIDEr of 1.029 (c5) and 1.064 (c40). version:1
arxiv-1702-05650 | Revisiting Graph Construction for Fast Image Segmentation | http://arxiv.org/abs/1702.05650 | id:1702.05650 author:Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang category:cs.CV  published:2017-02-18 summary:In this paper, we propose a simple but effective method for fast image segmentation. We re-examine the locality-preserving character of spectral clustering by constructing a graph over image regions with both global and local connections. Our novel approach to build graph connections relies on two key observations: 1) local region pairs that co-occur frequently will have a high probability to reside on a common object; 2) spatially distant regions in a common object often exhibit similar visual saliency, which implies their neighborship in a manifold. We present a novel energy function to efficiently conduct graph partitioning. Based on multiple high quality partitions, we show that the generated eigenvector histogram based representation can automatically drive effective unary potentials for a hierarchical random field model to produce multi-class segmentation. Sufficient experiments on the BSDS500 benchmark demonstrate the competitive segmentation accuracy and significantly improved efficiency of our proposed method compared with other state of the arts. version:1
arxiv-1702-05639 | Deep Stochastic Configuration Networks: Universal Approximation and Learning Representation | http://arxiv.org/abs/1702.05639 | id:1702.05639 author:Dianhui Wang, Ming Li category:cs.LG cs.NE  published:2017-02-18 summary:This paper focuses on the development of randomized approaches for building deep neural networks. A supervisory mechanism is proposed to constrain the random assignment of the hidden parameters (i.e., all biases and weights within the hidden layers). Full-rank oriented criterion is suggested and utilized as a termination condition to determine the number of nodes for each hidden layer, and a pre-defined error tolerance is used as a global indicator to decide the depth of the learner model. The read-out weights attached with all direct links from each hidden layer to the output layer are incrementally evaluated by the least squares method. Such a class of randomized leaner models with deep architecture is termed as deep stochastic configuration networks (DeepSCNs), of which the universal approximation property is verified with rigorous proof. Given abundant samples from a continuous distribution, DeepSCNs can speedily produce a learning representation, that is, a collection of random basis functions with the cascaded inputs together with the read-out weights. Simulation results with comparisons on function approximation align with the theoretical findings. version:1
arxiv-1702-05638 | A Stylometric Inquiry into Hyperpartisan and Fake News | http://arxiv.org/abs/1702.05638 | id:1702.05638 author:Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, Benno Stein category:cs.CL  published:2017-02-18 summary:This paper reports on a writing style analysis of hyperpartisan (i.e., extremely one-sided) news in connection to fake news. It presents a large corpus of 1,627 articles that were manually fact-checked by professional journalists from BuzzFeed. The articles originated from 9 well-known political publishers, 3 each from the mainstream, the hyperpartisan left-wing, and the hyperpartisan right-wing. In sum, the corpus contains 299 fake news, 97% of which originated from hyperpartisan publishers. We propose and demonstrate a new way of assessing style similarity between text categories via Unmasking---a meta-learning approach originally devised for authorship verification---, revealing that the style of left-wing and right-wing news have a lot more in common than any of the two have with the mainstream. Furthermore, we show that hyperpartisan news can be discriminated well by its style from the mainstream (F1=0.78), as can be satire from both (F1=0.81). Unsurprisingly, style-based fake news detection does not live up to scratch (F1=0.46). Nevertheless, the former results are important to implement pre-screening for fake news detectors. version:1
arxiv-1702-05624 | Reproducing and learning new algebraic operations on word embeddings using genetic programming | http://arxiv.org/abs/1702.05624 | id:1702.05624 author:Roberto Santana category:cs.CL  published:2017-02-18 summary:Word-vector representations associate a high dimensional real-vector to every word from a corpus. Recently, neural-network based methods have been proposed for learning this representation from large corpora. This type of word-to-vector embedding is able to keep, in the learned vector space, some of the syntactic and semantic relationships present in the original word corpus. This, in turn, serves to address different types of language classification tasks by doing algebraic operations defined on the vectors. The general practice is to assume that the semantic relationships between the words can be inferred by the application of a-priori specified algebraic operations. Our general goal in this paper is to show that it is possible to learn methods for word composition in semantic spaces. Instead of expressing the compositional method as an algebraic operation, we will encode it as a program, which can be linear, nonlinear, or involve more intricate expressions. More remarkably, this program will be evolved from a set of initial random programs by means of genetic programming (GP). We show that our method is able to reproduce the same behavior as human-designed algebraic operators. Using a word analogy task as benchmark, we also show that GP-generated programs are able to obtain accuracy values above those produced by the commonly used human-designed rule for algebraic manipulation of word vectors. Finally, we show the robustness of our approach by executing the evolved programs on the word2vec GoogleNews vectors, learned over 3 billion running words, and assessing their accuracy in the same word analogy task. version:1
arxiv-1702-05619 | 3D Face Reconstruction with Geometry Details from a Single Image | http://arxiv.org/abs/1702.05619 | id:1702.05619 author:Luo Jiang, Juyong Zhang, Bailin Deng, Hao Li, Ligang Liu category:cs.CV I.4.5; I.5.4  published:2017-02-18 summary:3D face reconstruction from a single image is a classical and challenging problem, with wide applications in many areas. Inspired by recent works in face animation from RGB-D or monocular video inputs, we develop a novel method for reconstructing 3D faces from unconstrained 2D images, using a coarse-to-fine optimization strategy. First, a smooth coarse 3D face is generated from an example-based bilinear face model, by aligning the projection of 3D face landmarks with 2D landmarks detected from the input image. Afterwards, using global corrective deformation fields, the coarse 3D face is refined using photometric consistency constraints, resulting in a medium face shape. Finally, a shape-from-shading method is applied on the medium face to recover fine geometric details. Our method outperforms state-of-the-art approaches in terms of accuracy and detail recovery, which is demonstrated in extensive experiments using real world models and publicly available datasets. version:1
arxiv-1702-05596 | Brain Inspired Cognitive Model with Attention for Self-Driving Cars | http://arxiv.org/abs/1702.05596 | id:1702.05596 author:Shitao Chen, Songyi Zhang, Jinghao Shang, Badong Chen, Nanning Zheng category:cs.CV cs.RO  published:2017-02-18 summary:Perception-driven approach and end-to-end system are two major vision-based frameworks for self-driving cars. However, it is difficult to introduce attention and historical information of autonomous driving process, which are the essential factors for achieving human-like driving into these two methods. In this paper, we propose a novel model for self-driving cars named brain-inspired cognitive model with attention (CMA). This model consists of three parts: a convolutional neural network for simulating human visual cortex, a cognitive map built to describe relationships between objects in complex traffic scene and a recurrent neural network that combines with the real-time updated cognitive map to implement attention mechanism and long-short term memory. The benefit of our model is that can accurately solve three tasks simultaneously:1) detection of the free space and boundaries of the current and adjacent lanes. 2)estimation of obstacle distance and vehicle attitude, and 3) learning of driving behavior and decision making from human driver. More significantly, the proposed model could accept external navigating instructions during an end-to-end driving process. For evaluation, we build a large-scale road-vehicle dataset which contains more than forty thousand labeled road images captured by three cameras on our self-driving car. Moreover, human driving activities and vehicle states are recorded in the meanwhile. version:1
arxiv-1702-05594 | Riemannian stochastic variance reduced gradient | http://arxiv.org/abs/1702.05594 | id:1702.05594 author:Hiroyuki Sato, Hiroyuki Kasai, Bamdev Mishra category:cs.LG math.OC stat.ML  published:2017-02-18 summary:Stochastic variance reduction algorithms have recently become popular for minimizing the average of a large but finite number of loss functions. In this paper, we propose a novel Riemannian extension of the Euclidean stochastic variance reduced gradient algorithm (R-SVRG) to a manifold search space. The key challenges of averaging, adding, and subtracting multiple gradients are addressed with retraction and vector transport. We present a global convergence analysis of the proposed algorithm with a decay step size and a local convergence rate analysis under a fixed step size under some natural assumptions. The proposed algorithm is applied to problems on the Grassmann manifold, such as principal component analysis, low-rank matrix completion, and computation of the Karcher mean of subspaces, and outperforms the standard Riemannian stochastic gradient descent algorithm in each case. version:1
arxiv-1702-05581 | Revisiting Perceptron: Efficient and Label-Optimal Active Learning of Halfspaces | http://arxiv.org/abs/1702.05581 | id:1702.05581 author:Songbai Yan, Chicheng Zhang category:cs.LG stat.ML  published:2017-02-18 summary:It has been a long-standing problem to efficiently learn a linear separator using as few labels as possible. In this work, we propose an efficient perceptron-based algorithm for actively learning homogeneous linear separators under uniform distribution. Under bounded noise, where each label is flipped with probability at most $\eta$, our algorithm achieves near-optimal $\tilde{O}\left(\frac{d}{(1-2\eta)^2}\log\frac{1}{\epsilon}\right)$ label complexity in time $\tilde{O}\left(\frac{d^2}{\epsilon(1-2\eta)^2}\right)$, and significantly improves over the best known result (Awasthi et al., 2016). Under adversarial noise, where at most $\nu$ fraction of labels can be flipped, our algorithm achieves near-optimal $\tilde{O}\left(d\log\frac{1}{\epsilon}\right)$ label complexity in time $\tilde{O}\left(\frac{d^2}{\epsilon}\right)$, which is better than the best known label complexity and time complexity in Awasthi et al. (2014). version:1
arxiv-1702-05575 | A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics | http://arxiv.org/abs/1702.05575 | id:1702.05575 author:Yuchen Zhang, Percy Liang, Moses Charikar category:cs.LG math.OC stat.ML  published:2017-02-18 summary:We study the Stochastic Gradient Langevin Dynamics (SGLD) algorithm for non-convex optimization. The algorithm performs a stochastic gradient descent, where in each step it injects appropriately scaled Gaussian noise to the update. We analyze the algorithm's hitting time to an arbitrary subset of the parameter space. Two results follow from our general theory: First, we prove that for empirical risk minimization, if the empirical risk is point-wise close to the (smooth) population risk, then the algorithm achieves an approximate local minimum of the population risk in polynomial time, escaping suboptimal local minima that only exist in the empirical risk. Second, we show that SGLD improves on one of the best known learnability results for learning linear classifiers under the zero-one loss. version:1
arxiv-1702-05574 | Sample complexity of population recovery | http://arxiv.org/abs/1702.05574 | id:1702.05574 author:Yury Polyanskiy, Ananda Theertha Suresh, Yihong Wu category:math.ST cs.IT math.IT stat.ML stat.TH  published:2017-02-18 summary:The problem of population recovery refers to estimating a distribution based on incomplete or corrupted samples. Consider a random poll of sample size $n$ conducted on a population of individuals, where each pollee is asked to answer $d$ binary questions. We consider one of the two polling impediments: (a) in lossy population recovery, a pollee may skip each question with probability $\epsilon$; (b) in noisy population recovery, a pollee may lie on each question with probability $\epsilon$. Given $n$ lossy or noisy samples, the goal is to estimate the probabilities of all $2^d$ binary vectors simultaneously within accuracy $\delta$ with high probability. This paper settles the sample complexity of population recovery. For lossy model, the optimal sample complexity is $\tilde\Theta(\delta^{ -2\max\{\frac{\epsilon}{1-\epsilon},1\}})$, improving the state of the art by Moitra and Saks (2013) in several ways: a lower bound is established, the upper bound is improved and the result is dimension-free. Surprisingly, the sample complexity undergoes a phase transition from parametric to nonparametric rate when $\epsilon$ exceeds $1/2$. For noisy population recovery, the sharp sample complexity turns out to be dimension-dependent and scales as $\exp(\Theta(d^{1/3} \log^{2/3}(1/\delta)))$ except for the trivial cases of $\epsilon=0,1/2$ or $1$. For both models, our estimators simply compute the empirical mean of a certain function, which is found by pre-solving a linear program (LP). Curiously, the dual LP can be understood as Le Cam's method for lower-bounding the minimax risk, thus establishing the statistical optimality of the proposed estimators. The value of the LP is determined by complex-analytic methods. version:1
arxiv-1702-05573 | Collaborative Deep Reinforcement Learning for Joint Object Search | http://arxiv.org/abs/1702.05573 | id:1702.05573 author:Xiangyu Kong, Bo Xin, Yizhou Wang, Gang Hua category:cs.CV  published:2017-02-18 summary:We examine the problem of joint top-down active search of multiple objects under interaction, e.g., person riding a bicycle, cups held by the table, etc.. Such objects under interaction often can provide contextual cues to each other to facilitate more efficient search. By treating each detector as an agent, we present the first collaborative multi-agent deep reinforcement learning algorithm to learn the optimal policy for joint active object localization, which effectively exploits such beneficial contextual information. We learn inter-agent communication through cross connections with gates between the Q-networks, which is facilitated by a novel multi-agent deep Q-learning algorithm with joint exploitation sampling. We verify our proposed method on multiple object detection benchmarks. Not only does our model help to improve the performance of state-of-the-art active localization models, it also reveals interesting co-detection patterns that are intuitively interpretable. version:1
arxiv-1702-05571 | Thresholding based Efficient Outlier Robust PCA | http://arxiv.org/abs/1702.05571 | id:1702.05571 author:Yeshwanth Cherapanamjeri, Prateek Jain, Praneeth Netrapalli category:cs.LG  published:2017-02-18 summary:We consider the problem of outlier robust PCA (OR-PCA) where the goal is to recover principal directions despite the presence of outlier data points. That is, given a data matrix $M^*$, where $(1-\alpha)$ fraction of the points are noisy samples from a low-dimensional subspace while $\alpha$ fraction of the points can be arbitrary outliers, the goal is to recover the subspace accurately. Existing results for \OR-PCA have serious drawbacks: while some results are quite weak in the presence of noise, other results have runtime quadratic in dimension, rendering them impractical for large scale applications. In this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity at most linear in the data size. Moreover, the fraction of outliers, $\alpha$, that our method can handle is tight up to constants while providing nearly optimal computational complexity for a general noise setting. For the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian noise, we show that a modification of our thresholding based method leads to significant improvement in recovery error (of the subspace) even in the presence of a large fraction of outliers. version:1
arxiv-1702-05564 | The Ciona17 Dataset for Semantic Segmentation of Invasive Species in a Marine Aquaculture Environment | http://arxiv.org/abs/1702.05564 | id:1702.05564 author:Angus Galloway, Graham W. Taylor, Aaron Ramsay, Medhat Moussa category:cs.CV  published:2017-02-18 summary:An original dataset for semantic segmentation, Ciona17, is introduced, which to the best of the authors' knowledge, is the first dataset of its kind with pixel-level annotations pertaining to invasive species in a marine environment. Diverse outdoor illumination, a range of object shapes, colour, and severe occlusion provide a significant real world challenge for the computer vision community. An accompanying ground-truthing tool for superpixel labeling, Truth and Crop, is also introduced. Finally, we provide a baseline using a variant of Fully Convolutional Networks, and report results in terms of the standard mean intersection over union (mIoU) metric. version:1
arxiv-1702-05555 | Defect detection for patterned fabric images based on GHOG and low-rank decomposition | http://arxiv.org/abs/1702.05555 | id:1702.05555 author:Chunlei Li, Guangshuai Gao, Zhoufeng Liu, Di Huang, Sheng Liu, Miao Yu category:cs.CV  published:2017-02-18 summary:In order to accurately detect defects in patterned fabric images, a novel detection algorithm based on Gabor-HOG (GHOG) and low-rank decomposition is proposed in this paper. Defect-free pattern fabric images have the specified direction, while defects damage their regularity of direction. Therefore, a direction-aware descriptor is designed, denoted as GHOG, a combination of Gabor and HOG, which is extremely valuable for localizing the defect region. Upon devising a powerful directional descriptor, an efficient low-rank decomposition model is constructed to divide the matrix generated by the directional feature extracted from image blocks into a low-rank matrix (background information) and a sparse matrix (defect information). A nonconvex log det(.) as a smooth surrogate function for the rank instead of the nuclear norm is also exploited to improve the efficiency of the low-rank model. Moreover, the computational efficiency is further improved by utilizing the alternative direction method of multipliers (ADMM). Thereafter, the saliency map generated by the sparse matrix is segmented via the optimal threshold algorithm to locate the defect regions. Experimental results show that the proposed method can effectively detect patterned fabric defects and outperform the state-of-the-art methods. version:1
arxiv-1702-05552 | Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection | http://arxiv.org/abs/1702.05552 | id:1702.05552 author:Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes category:cs.CV cs.NE  published:2017-02-18 summary:As humans we possess an intuitive ability for navigation which we master through years of practice; however existing approaches to model this trait for diverse tasks including monitoring pedestrian flow and detecting abnormal events have been limited by using a variety of hand-crafted features. Recent research in the area of deep-learning has demonstrated the power of learning features directly from the data; and related research in recurrent neural networks has shown exemplary results in sequence-to-sequence problems such as neural machine translation and neural image caption generation. Motivated by these approaches, we propose a novel method to predict the future motion of a pedestrian given a short history of their, and their neighbours, past behaviour. The novelty of the proposed method is the combined attention model which utilises both "soft attention" as well as "hard-wired" attention in order to map the trajectory information from the local neighbourhood to the future positions of the pedestrian of interest. We illustrate how a simple approximation of attention weights (i.e hard-wired) can be merged together with soft attention weights in order to make our model applicable for challenging real world scenarios with hundreds of neighbours. The navigational capability of the proposed method is tested on two challenging publicly available surveillance databases where our model outperforms the current-state-of-the-art methods. Additionally, we illustrate how the proposed architecture can be directly applied for the task of abnormal event detection without handcrafting the features. version:1
arxiv-1702-05548 | Bi-Level Online Control without Regret | http://arxiv.org/abs/1702.05548 | id:1702.05548 author:Andrey Bernstein category:math.OC cs.LG cs.SY  published:2017-02-18 summary:This paper considers a bi-level discrete-time control framework with real-time constraints, consisting of several local controllers and a central controller. The objective is to bridge the gap between the online convex optimization and real-time control literature by proposing an online control algorithm with small dynamic regret, which is a natural performance criterion in nonstationary environments related to real-time control problems. We illustrate how the proposed algorithm can be applied to real-time control of power setpoints in an electrical grid. version:1
arxiv-1702-05538 | Dataset Augmentation in Feature Space | http://arxiv.org/abs/1702.05538 | id:1702.05538 author:Terrance DeVries, Graham W. Taylor category:stat.ML cs.LG  published:2017-02-17 summary:Dataset augmentation, the practice of applying a wide array of domain-specific transformations to synthetically expand a training set, is a standard tool in supervised learning. While effective in tasks such as visual recognition, the set of transformations must be carefully designed, implemented, and tested for every new domain, limiting its re-use and generality. In this paper, we adopt a simpler, domain-agnostic approach to dataset augmentation. We start with existing data points and apply simple transformations such as adding noise, interpolating, or extrapolating between them. Our main insight is to perform the transformation not in input space, but in a learned feature space. A re-kindling of interest in unsupervised representation learning makes this technique timely and more effective. It is a simple proposal, but to-date one that has not been tested empirically. Working in the space of context vectors generated by sequence-to-sequence models, we demonstrate a technique that is effective for both static and sequential data. version:1
arxiv-1702-05536 | Beyond the Hazard Rate: More Perturbation Algorithms for Adversarial Multi-armed Bandits | http://arxiv.org/abs/1702.05536 | id:1702.05536 author:Zifan Li, Ambuj Tewari category:cs.LG cs.GT stat.ML  published:2017-02-17 summary:Recent work on follow the perturbed leader (FTPL) algorithms for the adversarial multi-armed bandit problem has highlighted the role of the hazard rate of the distribution generating the perturbations. Assuming that the hazard rate is bounded, it is possible to provide regret analyses for a variety of FTPL algorithms for the multi-armed bandit problem. This paper pushes the inquiry into regret bounds for FTPL algorithms beyond the bounded hazard rate condition. There are good reasons to do so: natural distributions such as the uniform and Gaussian violate the condition. We give regret bounds for both bounded support and unbounded support distributions without assuming the hazard rate condition. We also disprove a conjecture that the Gaussian distribution cannot lead to a low-regret algorithm. In fact, it turns out that it leads to near optimal regret, up to logarithmic factors. A key ingredient in our approach is the introduction of a new notion called the generalized hazard rate. version:1
arxiv-1702-05531 | Analysis and Optimization of fastText Linear Text Classifier | http://arxiv.org/abs/1702.05531 | id:1702.05531 author:Vladimir Zolotov, David Kung category:cs.CL  published:2017-02-17 summary:The paper [1] shows that simple linear classifier can compete with complex deep learning algorithms in text classification applications. Combining bag of words (BoW) and linear classification techniques, fastText [1] attains same or only slightly lower accuracy than deep learning algorithms [2-9] that are orders of magnitude slower. We proved formally that fastText can be transformed into a simpler equivalent classifier, which unlike fastText does not have any hidden layer. We also proved that the necessary and sufficient dimensionality of the word vector embedding space is exactly the number of document classes. These results help constructing more optimal linear text classifiers with guaranteed maximum classification capabilities. The results are proven exactly by pure formal algebraic methods without attracting any empirical data. version:1
arxiv-1702-04825 | Learning to Use Learners' Advice | http://arxiv.org/abs/1702.04825 | id:1702.04825 author:Adish Singla, Hamed Hassani, Andreas Krause category:cs.LG  published:2017-02-16 summary:In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback. We consider each expert as a learning entity, seeking to more accurately reflecting certain real-world applications. In our setting, the feedback at any time $t$ is limited in a sense that it is only available to the expert $i^t$ that has been selected by the central algorithm (forecaster), \emph{i.e.}, only the expert $i^t$ receives feedback from the environment and gets to learn at time $t$. We consider a generic black-box approach whereby the forecaster does not control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert $j$ vanishes at a rate of at least $O(t_j^{\regretRate-1})$ with $t_j$ learning steps where $\regretRate \in [0, 1]$ is a parameter. In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to "guide" the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, \emph{i.e.}, not allowing the selected expert $i^t$ to learn at time $t$ for some time steps. Then, we design a novel no-regret learning algorithm \algo for this problem setting by carefully guiding the feedbacks observed by experts. We prove that \algo achieves the worst-case expected cumulative regret of $O(\Time^\frac{1}{2 - \regretRate})$ after $\Time$ time steps. version:2
arxiv-1702-05506 | An Unsupervised Approach for Overlapping Cervical Cell Cytoplasm Segmentation | http://arxiv.org/abs/1702.05506 | id:1702.05506 author:Pranav Kumar, S L Happy, Swarnadip Chatterjee, Debdoot Sheet, Aurobinda Routray category:cs.CV  published:2017-02-17 summary:The poor contrast and the overlapping of cervical cell cytoplasm are the major issues in the accurate segmentation of cervical cell cytoplasm. This paper presents an automated unsupervised cytoplasm segmentation approach which can effectively find the cytoplasm boundaries in overlapping cells. The proposed approach first segments the cell clumps from the cervical smear image and detects the nuclei in each cell clump. A modified Otsu method with prior class probability is proposed for accurate segmentation of nuclei from the cell clumps. Using distance regularized level set evolution, the contour around each nucleus is evolved until it reaches the cytoplasm boundaries. Promising results were obtained by experimenting on ISBI 2015 challenge dataset. version:1
arxiv-1702-05464 | Adversarial Discriminative Domain Adaptation | http://arxiv.org/abs/1702.05464 | id:1702.05464 author:Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell category:cs.CV  published:2017-02-17 summary:Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task. version:1
arxiv-1702-05462 | Objective Bayesian Analysis for Change Point Problems | http://arxiv.org/abs/1702.05462 | id:1702.05462 author:Laurentiu Hinoveanu, Fabrizio Leisen, Cristiano Villa category:stat.ME math.ST stat.AP stat.CO stat.ML stat.TH  published:2017-02-17 summary:In this paper we present an objective approach to change point analysis. In particular, we look at the problem from two perspectives. The first focuses on the definition of an objective prior when the number of change points is known a priori. The second contribution aims to estimate the number of change points by using an objective approach, recently introduced in the literature, based on losses. The latter considers change point estimation as a model selection exercise. We show the performance of the proposed approach on simulated data and on real data sets. version:1
arxiv-1702-05448 | Learning to Detect Human-Object Interactions | http://arxiv.org/abs/1702.05448 | id:1702.05448 author:Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, Jia Deng category:cs.CV  published:2017-02-17 summary:In this paper we study the problem of detecting human-object interactions (HOI) in static images, defined as predicting a human and an object bounding box with an interaction class label that connects them. HOI detection is a fundamental problem in computer vision as it provides semantic information about the interactions among the detected objects. We introduce HICO-DET, a new large benchmark for HOI detection, by augmenting the current HICO classification benchmark with instance annotations. We propose Human-Object Region-based Convolutional Neural Networks (HO-RCNN), a novel DNN-based framework for HOI detection. At the core of our HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the spatial relations between two bounding boxes. We validate the effectiveness of our HO-RCNN using HICO-DET. Experiments demonstrate that our HO-RCNN, by exploiting human-object spatial relations through Interaction Patterns, significantly improves the performance of HOI detection over baseline approaches. version:1
arxiv-1702-05443 | How close are the eigenvectors and eigenvalues of the sample and actual covariance matrices? | http://arxiv.org/abs/1702.05443 | id:1702.05443 author:Andreas Loukas category:stat.ML math.ST stat.TH  published:2017-02-17 summary:How many samples are sufficient to guarantee that the eigenvectors and eigenvalues of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance. Our findings imply non-asymptotic concentration bounds for eigenvectors, eigenspaces, and eigenvalues. They also provide conditions for distinguishing principal components based on a constant number of samples. version:1
arxiv-1702-05423 | Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for Constrained Convex Optimization | http://arxiv.org/abs/1702.05423 | id:1702.05423 author:Yangyang Xu, Shuzhong Zhang category:math.OC cs.NA stat.ML 90C25  95C06  68W20  published:2017-02-17 summary:Block Coordinate Update (BCU) methods enjoy low per-update computational complexity because every time only one or a few block variables would need to be updated among possibly a large number of blocks. They are also easily parallelized and thus have been particularly popular for solving problems involving large-scale dataset and/or variables. In this paper, we propose a primal-dual BCU method for solving linearly constrained convex program in multi-block variables. The method is an accelerated version of a primal-dual algorithm proposed by the authors, which applies randomization in selecting block variables to update and establishes an $O(1/t)$ convergence rate under weak convexity assumption. We show that the rate can be accelerated to $O(1/t^2)$ if the objective is strongly convex. In addition, if one block variable is independent of the others in the objective, we then show that the algorithm can be modified to achieve a linear rate of convergence. The numerical experiments show that the accelerated method performs stably with a single set of parameters while the original method needs to tune the parameters for different datasets in order to achieve a comparable level of performance. version:1
arxiv-1702-05419 | A Random Matrix Approach to Neural Networks | http://arxiv.org/abs/1702.05419 | id:1702.05419 author:Cosme Louart, Zhenyu Liao, Romain Couillet category:math.PR cs.LG  published:2017-02-17 summary:This article studies the Gram random matrix model $G=\frac1T\Sigma^{\rm T}\Sigma$, $\Sigma=\sigma(WX)$, classically found in random neural networks, where $X=[x_1,\ldots,x_T]\in\mathbb{R}^{p\times T}$ is a (data) matrix of bounded norm, $W\in\mathbb{R}^{n\times p}$ is a matrix of independent zero-mean unit variance entries, and $\sigma:\mathbb{R}\to\mathbb{R}$ is a Lipschitz continuous (activation) function --- $\sigma(WX)$ being understood entry-wise. We prove that, as $n,p,T$ grow large at the same rate, the resolvent $Q=(G+\gamma I_T)^{-1}$, for $\gamma>0$, has a similar behavior as that met in sample covariance matrix models, involving notably the moment $\Phi=\frac{T}n\mathrm{E}[G]$, which provides in passing a deterministic equivalent for the empirical spectral measure of $G$. This result, established by means of concentration of measure arguments, enables the estimation of the asymptotic performance of single-layer random neural networks. This in turn provides practical insights into the underlying mechanisms into play in random neural networks, entailing several unexpected consequences, as well as a fast practical means to tune the network hyperparameters. version:1
arxiv-1702-06188 | Forest understory trees revealed using sufficiently dense airborne laser scanning point clouds | http://arxiv.org/abs/1702.06188 | id:1702.06188 author:Hamid Hamraz, Marco A. Contreras, Jun Zhang category:cs.CV cs.CG  published:2017-02-17 summary:Airborne laser scanning (lidar) point clouds can be process to extract tree-level information over large forested landscapes. Existing procedures typically detect more than 90% of overstory trees, yet they barely detect 60% of understory trees because of reduced number of lidar points penetrating the top canopy layer. Although understory trees provide limited financial value, they offer habitat for numerous wildlife species and are important for stand development. Here we model tree identification accuracy according to point cloud density by decomposing lidar point cloud into overstory and multiple understory canopy layers, estimating the fraction of points representing the different layers, and inspecting tree identification accuracy as a function of point density. We show at a density of about 170 pt/m2 understory tree identification accuracy likely plateaus, which we regard as the required point density for reasonable identification of understory trees. Given the advancements of lidar sensor technology, point clouds can feasibly reach the required density to enable effective identification of individual understory trees, ultimately making remote quantification of forest resources more accurate. The layer decomposition methodology can also be adopted for other similar remote sensing or advanced imaging applications such as geological subsurface modelling or biomedical tissue analysis. version:1
arxiv-1702-05413 | 3D Cell Nuclei Segmentation with Balanced Graph Partitioning | http://arxiv.org/abs/1702.05413 | id:1702.05413 author:Julian Arz, Peter Sanders, Johannes Stegmaier, Ralf Mikut category:cs.CV cs.DS  published:2017-02-17 summary:Cell nuclei segmentation is one of the most important tasks in the analysis of biomedical images. With ever-growing sizes and amounts of three-dimensional images to be processed, there is a need for better and faster segmentation methods. Graph-based image segmentation has seen a rise in popularity in recent years, but is seen as very costly with regard to computational demand. We propose a new segmentation algorithm which overcomes these limitations. Our method uses recursive balanced graph partitioning to segment foreground components of a fast and efficient binarization. We construct a model for the cell nuclei to guide the partitioning process. Our algorithm is compared to other state-of-the-art segmentation algorithms in an experimental evaluation on two sets of realistically simulated inputs. Our method is faster, has similar or better quality and an acceptable memory overhead. version:1
arxiv-1702-05398 | Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks | http://arxiv.org/abs/1702.05398 | id:1702.05398 author:Pradeep Dasigi, Gully A. P. C. Burns, Eduard Hovy, Anita de Waard category:cs.CL  published:2017-02-17 summary:We propose a deep learning model for identifying structure within experiment narratives in scientific literature. We take a sequence labeling approach to this problem, and label clauses within experiment narratives to identify the different parts of the experiment. Our dataset consists of paragraphs taken from open access PubMed papers labeled with rhetorical information as a result of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells that labels clauses. The clause representations are computed by combining word representations using a novel attention mechanism that involves a separate RNN. We compare this model against LSTMs where the input layer has simple or no attention and a feature rich CRF model. Furthermore, we describe how our work could be useful for information extraction from scientific literature. version:1
arxiv-1702-05390 | Approximate Bayes learning of stochastic differential equations | http://arxiv.org/abs/1702.05390 | id:1702.05390 author:Philipp Batz, Andreas Ruttor, Manfred Opper category:physics.data-an stat.ML  published:2017-02-17 summary:We introduce a nonparametric approach for estimating drift and diffusion functions in systems of stochastic differential equations from observations of the state vector. Gaussian processes are used as flexible models for these functions and estimates are calculated directly from dense data sets using Gaussian process regression. We also develop an approximate expectation maximization algorithm to deal with the unobserved, latent dynamics between sparse observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the maximum a posteriori estimation of the drift is facilitated by a sparse Gaussian process approximation. version:1
arxiv-1702-05388 | Vehicle Speed Detecting App | http://arxiv.org/abs/1702.05388 | id:1702.05388 author:Itoro Ikon category:cs.CV I.5.4; D.2.9  published:2017-02-17 summary:The report presents the measurement of vehicular speed using a smartphone camera. The speed measurement is accomplished by detecting the position of the vehicle on a camera frame using the LBP cascade classifier of OpenCV API, the displacement of the detected vehicle with time is used to compute the speed. Conversion coefficient is determined to map the pixel displacement to actual vehicle distance. The speeds measured are proportional to the ground truth speeds. version:1
arxiv-1702-05386 | Predicting Surgery Duration with Neural Heteroscedastic Regression | http://arxiv.org/abs/1702.05386 | id:1702.05386 author:Nathan Ng, Rodney A Gabriel, Julian McAuley, Charles Elkan, Zachary C Lipton category:stat.ML cs.LG cs.NE  published:2017-02-17 summary:Scheduling surgeries is a challenging task due to the fundamental uncertainty of the clinical environment, as well as the risks and costs associated with under- and over-booking. We investigate neural regression algorithms to estimate the parameters of surgery case durations, focusing on the issue of heteroscedasticity. We seek to simultaneously estimate the duration of each surgery, as well as a surgery-specific notion of our uncertainty about its duration. Estimating this uncertainty can lead to more nuanced and effective scheduling strategies, as we are able to schedule surgeries more efficiently while allowing an informed and case-specific margin of error. Using surgery records from the UC San Diego Health System, we demonstrate potential improvements on the order of 18% (in terms of minutes overbooked) compared to current scheduling techniques, as well as strong baselines that fail to account for heteroscedasticity. version:1
arxiv-1702-05376 | Towards a Unified Taxonomy of Biclustering Methods | http://arxiv.org/abs/1702.05376 | id:1702.05376 author:Dmitry I. Ignatov, Bruce W. Watson category:cs.AI cs.DM stat.ML 06B99  62H30  published:2017-02-17 summary:Being an unsupervised machine learning and data mining technique, biclustering and its multimodal extensions are becoming popular tools for analysing object-attribute data in different domains. Apart from conventional clustering techniques, biclustering is searching for homogeneous groups of objects while keeping their common description, e.g., in binary setting, their shared attributes. In bioinformatics, biclustering is used to find genes, which are active in a subset of situations, thus being candidates for biomarkers. However, the authors of those biclustering techniques that are popular in gene expression analysis, may overlook the existing methods. For instance, BiMax algorithm is aimed at finding biclusters, which are well-known for decades as formal concepts. Moreover, even if bioinformatics classify the biclustering methods according to reasonable domain-driven criteria, their classification taxonomies may be different from survey to survey and not full as well. So, in this paper we propose to use concept lattices as a tool for taxonomy building (in the biclustering domain) and attribute exploration as means for cross-domain taxonomy completion. version:1
arxiv-1702-05374 | Domain Adaptation for Visual Applications: A Comprehensive Survey | http://arxiv.org/abs/1702.05374 | id:1702.05374 author:Gabriela Csurka category:cs.CV  published:2017-02-17 summary:The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions. version:1
arxiv-1702-05327 | Anchored Regression: Solving Random Convex Equations via Convex Programming | http://arxiv.org/abs/1702.05327 | id:1702.05327 author:Sohail Bahmani, Justin Romberg category:cs.LG cs.IT math.IT math.OC math.PR stat.ML  published:2017-02-17 summary:We consider the problem of estimating a solution to (random) systems of equations that involve convex nonlinearities which has applications in machine learning and signal processing. Conventional estimators based on empirical risk minimization generally lead to non-convex programs that are often computationally intractable. We propose anchored regression, a new approach that utilizes an anchor vector to formulate an estimator based on a simple convex program. We analyze accuracy of this method and specify the required sample complexity. The proposed convex program is formulated in the natural space of the problem rather than a lifted domain, which makes it computationally favorable. This feature of anchored regression also provides great flexibility as structural priors (e.g., sparsity) can be seamlessly incorporated through convex regularization. We also provide recipes for constructing the anchor vector from the data. version:1
arxiv-1702-05982 | Wages of wins: could an amateur make money from match outcome predictions? | http://arxiv.org/abs/1702.05982 | id:1702.05982 author:Albrecht Zimmermann category:stat.AP cs.LG  published:2017-02-17 summary:Evaluating the accuracies of models for match outcome predictions is nice and well but in the end the real proof is in the money to be made by betting. To evaluate the question whether the models developed by us could be used easily to make money via sports betting, we evaluate three cases: NCAAB post-season, NBA season, and NFL season, and find that it is possible yet not without its pitfalls. In particular, we illustrate that high accuracy does not automatically equal high pay-out, by looking at the type of match-ups that are predicted correctly by different models. version:1
arxiv-1702-05308 | Hierarchy Influenced Differential Evolution: A Motor Operation Inspired Approach | http://arxiv.org/abs/1702.05308 | id:1702.05308 author:Shubham Dokania, Ayush Chopra, Feroz Ahmad, Om Prakash Verma category:cs.NE  published:2017-02-17 summary:Operational maturity of biological control systems have fuelled the inspiration for a large number of mathematical and logical models for control, automation and optimisation. The human brain represents the most sophisticated control architecture known to us and is a central motivation for several research attempts across various domains. In the present work, we introduce an algorithm for mathematical optimisation that derives its intuition from the hierarchical and distributed operations of the human motor system. The system comprises global leaders, local leaders and an effector population that adapt dynamically to attain global optimisation via a feedback mechanism coupled with the structural hierarchy. The hierarchical system operation is distributed into local control for movement and global controllers that facilitate gross motion and decision making. We present our algorithm as a variant of the classical Differential Evolution algorithm, introducing a hierarchical crossover operation. The discussed approach is tested exhaustively on standard test functions from the CEC 2017 benchmark. Our algorithm significantly outperforms various standard algorithms as well as their popular variants as discussed in the results. version:1
arxiv-1702-04858 | Deep Hybrid Similarity Learning for Person Re-identification | http://arxiv.org/abs/1702.04858 | id:1702.04858 author:Jianqing Zhu, Huanqiang Zeng, Shengcai Liao, Zhen Lei, Canhui Cai, LiXin Zheng category:cs.CV  published:2017-02-16 summary:Person Re-IDentification (Re-ID) aims to match person images captured from two non-overlapping cameras. In this paper, a deep hybrid similarity learning (DHSL) method for person Re-ID based on a convolution neural network (CNN) is proposed. In our approach, a CNN learning feature pair for the input image pair is simultaneously extracted. Then, both the element-wise absolute difference and multiplication of the CNN learning feature pair are calculated. Finally, a hybrid similarity function is designed to measure the similarity between the feature pair, which is realized by learning a group of weight coefficients to project the element-wise absolute difference and multiplication into a similarity score. Consequently, the proposed DHSL method is able to reasonably assign parameters of feature learning and metric learning in a CNN so that the performance of person Re-ID is improved. Experiments on three challenging person Re-ID databases, QMUL GRID, VIPeR and CUHK03, illustrate that the proposed DHSL method is superior to multiple state-of-the-art person Re-ID methods. version:2
arxiv-1702-05289 | Observable dictionary learning for high-dimensional statistical inference | http://arxiv.org/abs/1702.05289 | id:1702.05289 author:Lionel Mathelin, Kévin Kasper, Hisham Abou-Kandil category:stat.ML  published:2017-02-17 summary:This paper introduces a method for efficiently inferring a high-dimensional distributed quantity from a few observations. The quantity of interest (QoI) is approximated in a basis (dictionary) learned from a training set. The coefficients associated with the approximation of the QoI in the basis are determined by minimizing the misfit with the observations. To obtain a probabilistic estimate of the quantity of interest, a Bayesian approach is employed. The QoI is treated as a random field endowed with a hierarchical prior distribution so that closed-form expressions can be obtained for the posterior distribution. The main contribution of the present work lies in the derivation of \emph{a representation basis consistent with the observation chain} used to infer the associated coefficients. The resulting dictionary is then tailored to be both observable by the sensors and accurate in approximating the posterior mean. An algorithm for deriving such an observable dictionary is presented. The method is illustrated with the estimation of the velocity field of an open cavity flow from a handful of wall-mounted point sensors. Comparison with standard estimation approaches relying on Principal Component Analysis and K-SVD dictionaries is provided and illustrates the superior performance of the present approach. version:1
arxiv-1702-05270 | Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision | http://arxiv.org/abs/1702.05270 | id:1702.05270 author:Sandro Pezzelle, Marco Marelli, Raffaella Bernardi category:cs.CL cs.AI cs.CV  published:2017-02-17 summary:People can refer to quantities in a visual scene by using either exact cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few, most, all). In humans, these two processes underlie fairly different cognitive and neural mechanisms. Inspired by this evidence, the present study proposes two models for learning the objective meaning of cardinals and quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a 'fuzzy' measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished when information about number is provided. version:1
arxiv-1702-04407 | Sequential Dirichlet Process Mixtures of Multivariate Skew t-distributions for Model-based Clustering of Flow Cytometry Data | http://arxiv.org/abs/1702.04407 | id:1702.04407 author:Boris P. Hejblum, Chariff Alkhassim, Raphael Gottardo, François Caron, Rodolphe Thiébaut category:stat.ML  published:2017-02-14 summary:Flow cytometry is a high-throughput technology used to quantify multiple surface and intracellular markers at the level of a single cell. This enables to identify cell sub-types, and to determine their relative proportions. Improvements of this technology allow to describe millions of individual cells from a blood sample using multiple markers. This results in high-dimensional datasets, whose manual analysis is highly time-consuming and poorly reproducible. While several methods have been developed to perform automatic recognition of cell populations, most of them treat and analyze each sample independently. However, in practice, individual samples are rarely independent (e.g. longitudinal studies). Here, we propose to use a Bayesian nonparametric approach with Dirichlet process mixture (DPM) of multivariate skew $t$-distributions to perform model based clustering of flow-cytometry data. DPM models directly estimate the number of cell populations from the data, avoiding model selection issues, and skew $t$-distributions provides robustness to outliers and non-elliptical shape of cell populations. To accommodate repeated measurements, we propose a sequential strategy relying on a parametric approximation of the posterior. We illustrate the good performance of our method on simulated data, on an experimental benchmark dataset, and on new longitudinal data from the DALIA-1 trial which evaluates a therapeutic vaccine against HIV. On the benchmark dataset, the sequential strategy outperforms all other methods evaluated, and similarly, leads to improved performance on the DALIA-1 data. We have made the method available for the community in the R package NPflow. version:2
arxiv-1702-05243 | Estimating Nonlinear Dynamics with the ConvNet Smoother | http://arxiv.org/abs/1702.05243 | id:1702.05243 author:Luca Ambrogioni, Umut Güçlü, Eric Maris, Marcel van Gerven category:stat.ML  published:2017-02-17 summary:The estimation of the state of a dynamical system from a series of noise-corrupted observations is fundamental in many areas of science and engineering. The most well-known method, the Kalman smoother, relies on assumptions of linearity and Gaussianity that are rarely met in practice. In this paper, we introduced a new smoothing method that exploits the remarkable capabilities of constitutional neural networks to approximate complex non-linear functions. The main idea is to generate a training set composed by both latent states and observations from a simulator and to train the deep network to recover the former from the latter. Importantly, this method only requires the availability of a simulator and can therefore be applied in situations in which either the latent dynamical model or the observation model cannot be expressed in closed form. In our simulation studies, we showed that the resulting ConvNet smoother has almost optimal performance in the Gaussian case and can be successfully applied to extremely non-linear and non-Gaussian systems. Finally, we showed an example of analysis on real brain signals. version:1
arxiv-1702-04077 | Mutual Kernel Matrix Completion | http://arxiv.org/abs/1702.04077 | id:1702.04077 author:Tsuyoshi Kato, Rachelle Rivero category:cs.LG cs.NA stat.ML  published:2017-02-14 summary:With the huge influx of various data nowadays, extracting knowledge from them has become an interesting but tedious task among data scientists, particularly when the data come in heterogeneous form and have missing information. Many data completion techniques had been introduced, especially in the advent of kernel methods. However, among the many data completion techniques available in the literature, studies about mutually completing several incomplete kernel matrices have not been given much attention yet. In this paper, we present a new method, called Mutual Kernel Matrix Completion (MKMC) algorithm, that tackles this problem of mutually inferring the missing entries of multiple kernel matrices by combining the notions of data fusion and kernel matrix completion, applied on biological data sets to be used for classification task. We first introduced an objective function that will be minimized by exploiting the EM algorithm, which in turn results to an estimate of the missing entries of the kernel matrices involved. The completed kernel matrices are then combined to produce a model matrix that can be used to further improve the obtained estimates. An interesting result of our study is that the E-step and the M-step are given in closed form, which makes our algorithm efficient in terms of time and memory. After completion, the (completed) kernel matrices are then used to train an SVM classifier to test how well the relationships among the entries are preserved. Our empirical results show that the proposed algorithm bested the traditional completion techniques in preserving the relationships among the data points, and in accurately recovering the missing kernel matrix entries. By far, MKMC offers a promising solution to the problem of mutual estimation of a number of relevant incomplete kernel matrices. version:2
arxiv-1702-05222 | Direct Estimation of Information Divergence Using Nearest Neighbor Ratios | http://arxiv.org/abs/1702.05222 | id:1702.05222 author:Morteza Noshad, Kevin R. Moon, Salimeh Yasaei Sekeh, Alfred O. Hero III category:cs.IT cs.AI math.IT stat.ML  published:2017-02-17 summary:We propose a direct estimation method for R\'{e}nyi and f-divergence measures based on a new graph theoretical interpretation. Suppose that we are given two sample sets $X$ and $Y$, respectively with $N$ and $M$ samples, where $\eta:=M/N$ is a constant value. Considering the $k$-nearest neighbor ($k$-NN) graph of $Y$ in the joint data set $(X,Y)$, we show that the average powered ratio of the number of $X$ points to the number of $Y$ points among all $k$-NN points is proportional to R\'{e}nyi divergence of $X$ and $Y$ densities. A similar method can also be used to estimate f-divergence measures. We derive bias and variance rates, and show that for the class of $\gamma$-H\"{o}lder smooth functions, the estimator achieves the MSE rate of $O(N^{-2\gamma/(\gamma+d)})$. Furthermore, by using a weighted ensemble estimation technique, for density functions with continuous and bounded derivatives of up to the order $d$, and some extra conditions at the support set boundary, we derive an ensemble estimator that achieves the parametric MSE rate of $O(1/N)$. Our estimators are more computationally tractable than other competing estimators, which makes them appealing in many practical applications. version:1
arxiv-1702-04782 | Precise Recovery of Latent Vectors from Generative Adversarial Networks | http://arxiv.org/abs/1702.04782 | id:1702.04782 author:Zachary C. Lipton, Subarna Tripathi category:cs.LG cs.NE stat.ML  published:2017-02-15 summary:Generative adversarial networks (GANs) transform latent vectors into visually plausible images. It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space. We introduce a simple, gradient-based technique called stochastic clipping. In experiments, for images generated by the GAN, we precisely recover their latent vector pre-images 100% of the time. Additional experiments demonstrate that this method is robust to noise. Finally, we show that even for unseen images, our method appears to recover unique encodings. version:2
arxiv-1702-05192 | Cloud-based Deep Learning of Big EEG Data for Epileptic Seizure Prediction | http://arxiv.org/abs/1702.05192 | id:1702.05192 author:Mohammad-Parsa Hosseini, Hamid Soltanian-Zadeh, Kost Elisevich, Dario Pompili category:cs.LG stat.ML  published:2017-02-17 summary:Developing a Brain-Computer Interface~(BCI) for seizure prediction can help epileptic patients have a better quality of life. However, there are many difficulties and challenges in developing such a system as a real-life support for patients. Because of the nonstationary nature of EEG signals, normal and seizure patterns vary across different patients. Thus, finding a group of manually extracted features for the prediction task is not practical. Moreover, when using implanted electrodes for brain recording massive amounts of data are produced. This big data calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based BCI system for the analysis of this big EEG data is presented. First, a dimensionality-reduction technique is developed to increase classification accuracy as well as to decrease the communication bandwidth and computation time. Second, following a deep-learning approach, a stacked autoencoder is trained in two steps for unsupervised feature extraction and classification. Third, a cloud-computing solution is proposed for real-time analysis of big EEG data. The results on a benchmark clinical dataset illustrate the superiority of the proposed patient-specific BCI as an alternative method and its expected usefulness in real-life support of epilepsy patients. version:1
arxiv-1702-05186 | The Simulator: Understanding Adaptive Sampling in the Moderate-Confidence Regime | http://arxiv.org/abs/1702.05186 | id:1702.05186 author:Max Simchowitz, Kevin Jamieson, Benjamin Recht category:cs.LG stat.ML  published:2017-02-16 summary:We propose a novel technique for analyzing adaptive sampling called the {\em Simulator}. Our approach differs from the existing methods by considering not how much information could be gathered by any fixed sampling strategy, but how difficult it is to distinguish a good sampling strategy from a bad one given the limited amount of data collected up to any given time. This change of perspective allows us to match the strength of both Fano and change-of-measure techniques, without succumbing to the limitations of either method. For concreteness, we apply our techniques to a structured multi-arm bandit problem in the fixed-confidence pure exploration setting, where we show that the constraints on the means imply a substantial gap between the moderate-confidence sample complexity, and the asymptotic sample complexity as $\delta \to 0$ found in the literature. We also prove the first instance-based lower bounds for the top-k problem which incorporate the appropriate log-factors. Moreover, our lower bounds zero-in on the number of times each \emph{individual} arm needs to be pulled, uncovering new phenomena which are drowned out in the aggregate sample complexity. Our new analysis inspires a simple and near-optimal algorithm for the best-arm and top-k identification, the first {\em practical} algorithm of its kind for the latter problem which removes extraneous log factors, and outperforms the state-of-the-art in experiments. version:1
arxiv-1702-05184 | Completing a joint PMF from projections: a low-rank coupled tensor factorization approach | http://arxiv.org/abs/1702.05184 | id:1702.05184 author:Nikos Kargas, Nicholas D. Sidiropoulos category:cs.LG cs.DM stat.ML  published:2017-02-16 summary:There has recently been considerable interest in completing a low-rank matrix or tensor given only a small fraction (or few linear combinations) of its entries. Related approaches have found considerable success in the area of recommender systems, under machine learning. From a statistical estimation point of view, the gold standard is to have access to the joint probability distribution of all pertinent random variables, from which any desired optimal estimator can be readily derived. In practice high-dimensional joint distributions are very hard to estimate, and only estimates of low-dimensional projections may be available. We show that it is possible to identify higher-order joint PMFs from lower-order marginalized PMFs using coupled low-rank tensor factorization. Our approach features guaranteed identifiability when the full joint PMF is of low-enough rank, and effective approximation otherwise. We provide an algorithmic approach to compute the sought factors, and illustrate the merits of our approach using rating prediction as an example. version:1
arxiv-1702-05181 | RIPML: A Restricted Isometry Property based Approach to Multilabel Learning | http://arxiv.org/abs/1702.05181 | id:1702.05181 author:Akshay Soni, Yashar Mehdad category:cs.IR cs.LG stat.ML  published:2017-02-16 summary:The multilabel learning problem with large number of labels, features, and data-points has generated a tremendous interest recently. A recurring theme of these problems is that only a few labels are active in any given datapoint as compared to the total number of labels. However, only a small number of existing work take direct advantage of this inherent extreme sparsity in the label space. By the virtue of Restricted Isometry Property (RIP), satisfied by many random ensembles, we propose a novel procedure for multilabel learning known as RIPML. During the training phase, in RIPML, labels are projected onto a random low-dimensional subspace followed by solving a least-square problem in this subspace. Inference is done by a k-nearest neighbor (kNN) based approach. We demonstrate the effectiveness of RIPML by conducting extensive simulations and comparing results with the state-of-the-art linear dimensionality reduction based approaches. version:1
arxiv-1702-05174 | Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation | http://arxiv.org/abs/1702.05174 | id:1702.05174 author:Michal Drozdzal, Gabriel Chartrand, Eugene Vorontsov, Lisa Di Jorio, An Tang, Adriana Romero, Yoshua Bengio, Chris Pal, Samuel Kadoury category:cs.CV  published:2017-02-16 summary:In this paper, we introduce a simple, yet powerful pipeline for medical image segmentation that combines Fully Convolutional Networks (FCNs) with Fully Convolutional Residual Networks (FC-ResNets). We propose and examine a design that takes particular advantage of recent advances in the understanding of both Convolutional Neural Networks as well as ResNets. Our approach focuses upon the importance of a trainable pre-processing when using FC-ResNets and we show that a low-capacity FCN model can serve as a pre-processor to normalize medical input data. In our image segmentation pipeline, we use FCNs to obtain normalized images, which are then iteratively refined by means of a FC-ResNet to generate a segmentation prediction. As in other fully convolutional approaches, our pipeline can be used off-the-shelf on different image modalities. We show that using this pipeline, we exhibit state-of-the-art performance on the challenging Electron Microscopy benchmark, when compared to other 2D methods. We improve segmentation results on CT images of liver lesions, when contrasting with standard FCN methods. Moreover, when applying our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we reach results that are competitive even when compared to 3D methods. The obtained results illustrate the strong potential and versatility of the pipeline by achieving highly accurate results on multi-modality images from different anatomical regions and organs. version:1
arxiv-1702-05156 | An Analysis of Parallelized Motion Masking Using Dual-Mode Single Gaussian Models | http://arxiv.org/abs/1702.05156 | id:1702.05156 author:Peter Henderson, Matthew Vertescher category:cs.CV cs.DC  published:2017-02-16 summary:Motion detection in video is important for a number of applications and fields. In video surveillance, motion detection is an essential accompaniment to activity recognition for early warning systems. Robotics also has much to gain from motion detection and segmentation, particularly in high speed motion tracking for tactile systems. There are a myriad of techniques for detecting and masking motion in an image. Successful systems have used Gaussian Models to discern background from foreground in an image (motion from static imagery). However, particularly in the case of a moving camera or frame of reference, it is necessary to compensate for the motion of the camera when attempting to discern objects moving in the foreground. For example, it is possible to estimate motion of the camera through optical flow methods or temporal differencing and then compensate for this motion in a background subtraction model. We selection a method by Yi et al. using Dual-Mode Single Gaussian Models which does just this. We implement the technique in Intel's Thread Building Blocks (TBB) and NVIDIA's CUDA libraries. We then compare parallelization improvements with a theoretical analysis of speedups based on the characteristics of our selected model and attributes of both TBB and CUDA. We make our implementation available to the public. version:1
arxiv-1702-04690 | Simple rules for complex decisions | http://arxiv.org/abs/1702.04690 | id:1702.04690 author:Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, Daniel G. Goldstein category:stat.AP stat.ML  published:2017-02-15 summary:From doctors diagnosing patients to judges setting bail, experts often base their decisions on experience and intuition rather than on statistical models. While understandable, relying on intuition over models has often been found to result in inferior outcomes. Here we present a new method, select-regress-and-round, for constructing simple rules that perform well for complex decisions. These rules take the form of a weighted checklist, can be applied mentally, and nonetheless rival the performance of modern machine learning algorithms. Our method for creating these rules is itself simple, and can be carried out by practitioners with basic statistics knowledge. We demonstrate this technique with a detailed case study of judicial decisions to release or detain defendants while they await trial. In this application, as in many policy settings, the effects of proposed decision rules cannot be directly observed from historical data: if a rule recommends releasing a defendant that the judge in reality detained, we do not observe what would have happened under the proposed action. We address this key counterfactual estimation problem by drawing on tools from causal inference. We find that simple rules significantly outperform judges and are on par with decisions derived from random forests trained on all available features. Generalizing to 22 varied decision-making domains, we find this basic result replicates. We conclude with an analytical framework that helps explain why these simple decision rules perform as well as they do. version:2
arxiv-1702-05150 | BubbleView: an alternative to eye-tracking for crowdsourcing image importance | http://arxiv.org/abs/1702.05150 | id:1702.05150 author:Nam Wook Kim, Zoya Bylinskii, Michelle A. Borkin, Krzysztof Z. Gajos, Aude Oliva, Fredo Durand, Hanspeter Pfister category:cs.HC cs.CV  published:2017-02-16 summary:We present BubbleView, a methodology to replace eye-tracking with mouse clicks. Participants are presented with a series of blurred images and click to reveal "bubbles" - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. We evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView can be used to successfully approximate eye fixations on different images, and that the regions where people click using BubbleView can also be used to rank image and design elements by importance. BubbleView is designed to measure which information people consciously choose to examine, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. Compared to related methodologies based on a moving-window approach, BubbleView provides more reliable and less noisy data. version:1
arxiv-1702-05148 | Latent Laplacian Maximum Entropy Discrimination for Detection of High-Utility Anomalies | http://arxiv.org/abs/1702.05148 | id:1702.05148 author:Elizabeth Hou, Kumar Sricharan, Alfred O. Hero category:stat.ML cs.CR cs.LG  published:2017-02-16 summary:Data-driven anomaly detection methods suffer from the drawback of detecting all instances that are statistically rare, irrespective of whether the detected instances have real-world significance or not. In this paper, we are interested in the problem of specifically detecting anomalous instances that are known to have high real-world utility, while ignoring the low-utility statistically anomalous instances. To this end, we propose a novel method called Latent Laplacian Maximum Entropy Discrimination (LatLapMED) as a potential solution. This method uses the EM algorithm to simultaneously incorporate the Geometric Entropy Minimization principle for identifying statistical anomalies, and the Maximum Entropy Discrimination principle to incorporate utility labels, in order to detect high-utility anomalies. We apply our method in both simulated and real datasets to demonstrate that it has superior performance over existing alternatives that independently pre-process with unsupervised anomaly detection algorithms before classifying. version:1
arxiv-1702-05147 | Automatic Handgun Detection Alarm in Videos Using Deep Learning | http://arxiv.org/abs/1702.05147 | id:1702.05147 author:Roberto Olmos, Siham Tabik, Francisco Herrera category:cs.CV  published:2017-02-16 summary:Current surveillance and control systems still require human supervision and intervention. This work presents a novel automatic handgun detection system in videos appropriate for both, surveillance and control purposes. We reformulate this detection problem into the problem of minimizing false positives and solve it by building the key training data-set guided by the results of a deep Convolutional Neural Networks (CNN) classifier, then assessing the best classification model under two approaches, the sliding window approach and region proposal approach. The most promising results are obtained by Faster R-CNN based model trained on our new database. The best detector show a high potential even in low quality youtube videos and provides satisfactory results as automatic alarm system. Among 30 scenes, it successfully activates the alarm after five successive true positives in less than 0.2 seconds, in 27 scenes. We also define a new metric, Alarm Activation per Interval (AApI), to assess the performance of a detection model as an automatic detection system in videos. version:1
arxiv-1702-05137 | Semi-supervised Learning for Discrete Choice Models | http://arxiv.org/abs/1702.05137 | id:1702.05137 author:Jie Yang, Sergey Shebalov, Diego Klabjan category:stat.ML cs.LG  published:2017-02-16 summary:We introduce a semi-supervised discrete choice model to calibrate discrete choice models when relatively few requests have both choice sets and stated preferences but the majority only have the choice sets. Two classic semi-supervised learning algorithms, the expectation maximization algorithm and the cluster-and-label algorithm, have been adapted to our choice modeling problem setting. We also develop two new algorithms based on the cluster-and-label algorithm. The new algorithms use the Bayesian Information Criterion to evaluate a clustering setting to automatically adjust the number of clusters. Two computational studies including a hotel booking case and a large-scale airline itinerary shopping case are presented to evaluate the prediction accuracy and computational effort of the proposed algorithms. Algorithmic recommendations are rendered under various scenarios. version:1
arxiv-1702-05089 | Improving Text Proposals for Scene Images with Fully Convolutional Networks | http://arxiv.org/abs/1702.05089 | id:1702.05089 author:Dena Bazazian, Raul Gomez, Anguelos Nicolaou, Lluis Gomez, Dimosthenis Karatzas, Andrew D. Bagdanov category:cs.CV  published:2017-02-16 summary:Text Proposals have emerged as a class-dependent version of object proposals - efficient approaches to reduce the search space of possible text object locations in an image. Combined with strong word classifiers, text proposals currently yield top state of the art results in end-to-end scene text recognition. In this paper we propose an improvement over the original Text Proposals algorithm of Gomez and Karatzas (2016), combining it with Fully Convolutional Networks to improve the ranking of proposals. Results on the ICDAR RRC and the COCO-text datasets show superior performance over current state-of-the-art. version:1
arxiv-1702-05085 | KEPLER: Keypoint and Pose Estimation of Unconstrained Faces by Learning Efficient H-CNN Regressors | http://arxiv.org/abs/1702.05085 | id:1702.05085 author:Amit Kumar, Azadeh Alavi, Rama Chellappa category:cs.CV  published:2017-02-16 summary:Keypoint detection is one of the most important pre-processing steps in tasks such as face modeling, recognition and verification. In this paper, we present an iterative method for Keypoint Estimation and Pose prediction of unconstrained faces by Learning Efficient H-CNN Regressors (KEPLER) for addressing the face alignment problem. Recent state of the art methods have shown improvements in face keypoint detection by employing Convolution Neural Networks (CNNs). Although a simple feed forward neural network can learn the mapping between input and output spaces, it cannot learn the inherent structural dependencies. We present a novel architecture called H-CNN (Heatmap-CNN) which captures structured global and local features and thus favors accurate keypoint detecion. HCNN is jointly trained on the visibility, fiducials and 3D-pose of the face. As the iterations proceed, the error decreases making the gradients small and thus requiring efficient training of DCNNs to mitigate this. KEPLER performs global corrections in pose and fiducials for the first four iterations followed by local corrections in the subsequent stage. As a by-product, KEPLER also provides 3D pose (pitch, yaw and roll) of the face accurately. In this paper, we show that without using any 3D information, KEPLER outperforms state of the art methods for alignment on challenging datasets such as AFW and AFLW. version:1
arxiv-1702-05068 | Discovering objects and their relations from entangled scene representations | http://arxiv.org/abs/1702.05068 | id:1702.05068 author:David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, Peter Battaglia category:cs.LG cs.CV  published:2017-02-16 summary:Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) - a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning. version:1
arxiv-1702-05063 | A new concentration inequality for the excess risk in least-squares regression with random design and heteroscedastic noise | http://arxiv.org/abs/1702.05063 | id:1702.05063 author:Adrien Saumard category:math.ST stat.ML stat.TH  published:2017-02-16 summary:We prove a new concentration inequality for the excess risk of a M-estimator in least-squares regression with random design and heteroscedastic noise. This kind of result is a central tool in modern model selection theory, as well as in recent achievements concerning the behavior of regularized estimators such as LASSO, group LASSO and SLOPE. version:1
arxiv-1702-05056 | An Empirical Bayes Approach for High Dimensional Classification | http://arxiv.org/abs/1702.05056 | id:1702.05056 author:Yunbo Ouyang, Feng Liang category:stat.ML stat.ME  published:2017-02-16 summary:We propose an empirical Bayes estimator based on Dirichlet process mixture model for estimating the sparse normalized mean difference, which could be directly applied to the high dimensional linear classification. In theory, we build a bridge to connect the estimation error of the mean difference and the misclassification error, also provide sufficient conditions of sub-optimal classifiers and optimal classifiers. In implementation, a variational Bayes algorithm is developed to compute the posterior efficiently and could be parallelized to deal with the ultra-high dimensional case. version:1
arxiv-1702-05053 | Addressing the Data Sparsity Issue in Neural AMR Parsing | http://arxiv.org/abs/1702.05053 | id:1702.05053 author:Xiaochang Peng, Chuan Wang, Daniel Gildea, Nianwen Xue category:cs.CL  published:2017-02-16 summary:Neural attention models have achieved great success in different NLP tasks. How- ever, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we de- scribe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural atten- tion model and our results are also compet- itive against state-of-the-art systems that do not use extra linguistic resources. version:1
arxiv-1702-05043 | Unbiased Online Recurrent Optimization | http://arxiv.org/abs/1702.05043 | id:1702.05043 author:Corentin Tallec, Yann Ollivier category:cs.NE cs.LG  published:2017-02-16 summary:The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is a modification of NoBackTrack that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Computationally, UORO is as costly as Truncated Backpropagation Through Time (TBPTT). Contrary to TBPTT, UORO is guaranteed to provide unbiased gradient estimates, and does not favor short-term dependencies. The downside is added noise, requiring smaller learning rates. On synthetic tasks, UORO is found to overcome several deficiencies of TBPTT. For instance, when a parameter has a positive short-term but negative long-term influence, TBPTT may require truncation lengths substantially larger than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients. version:1
arxiv-1702-05037 | Additive Models with Trend Filtering | http://arxiv.org/abs/1702.05037 | id:1702.05037 author:Veeranjaneyulu Sadhanala, Ryan J. Tibshirani category:stat.ML  published:2017-02-16 summary:We consider additive models built with trend filtering, i.e., additive models whose components are each regularized by the (discrete) total variation of their $(k+1)$st (discrete) derivative, for a chosen integer $k \geq 0$. This results in $k$th degree piecewise polynomial components, (e.g., $k=0$ gives piecewise constant components, $k=1$ gives piecewise linear, $k=2$ gives piecewise quadratic, etc.). In univariate nonparametric regression, the localized nature of the total variation regularizer used by trend filtering has been shown to produce estimates with superior local adaptivity to those from smoothing splines (and linear smoothers, more generally) (Tibshirani [2014]). Further, the structured nature of this regularizer has been shown to lead to highly efficient computational routines for trend filtering (Kim et al. [2009], Ramdas and Tibshirani [2016]). In this paper, we argue that both of these properties carry over to the additive models setting. We derive fast error rates for additive trend filtering estimates, and prove that these rates are minimax optimal when the underlying function is itself additive and has components whose derivatives are of bounded variation. We argue that backfitting provides an efficient algorithm for additive trend filtering, as it is built around the fast univariate trend filtering solvers; furthermore, we describe a modified backfitting procedure whose iterations can be run in parallel. Finally, we present experiments that examine the empirical properties of additive trend filtering models, and outline some interesting extensions. version:1
arxiv-1702-05008 | Tree Ensembles with Rule Structured Horseshoe Regularization | http://arxiv.org/abs/1702.05008 | id:1702.05008 author:Malte Nalenz, Mattias Villani category:stat.ME stat.ML  published:2017-02-16 summary:We propose a new Bayesian model for flexible nonlinear regression and classification using tree ensembles. The model is based on the RuleFit approach in Friedman and Popescu (2008) where rules from decision trees and linear terms are used in a L1-regularized regression. We modify RuleFit by replacing the L1-regularization by a horseshoe prior, which is well known to give aggressive shrinkage of noise predictor while leaving the important signal essentially untouched. This is especially important when a large number of rules are used as predictors as many of them only contribute noise. Our horseshoe prior has an additional hierarchical layer that applies more shrinkage a priori to rules with a large number of splits, and to rules that are only satisfied by a few observations. The aggressive noise shrinkage of our prior also makes it possible to complement the rules from boosting in Friedman and Popescu (2008) with an additional set of trees from random forest, which brings a desirable diversity to the ensemble. We sample from the posterior distribution using a very efficient and easily implemented Gibbs sampler. The new model is shown to outperform state-of-the-art methods like RuleFit, BART and random forest on 16 datasets. The model and its interpretation is demonstrated on the well known Boston housing data, and on gene expression data for cancer classification. The posterior sampling, prediction and graphical tools for interpreting the model results are implemented in a publicly available R package. version:1
arxiv-1702-04956 | Reflexive Regular Equivalence for Bipartite Data | http://arxiv.org/abs/1702.04956 | id:1702.04956 author:Aaron Gerow, Mingyang Zhou, Stan Matwin, Feng Shi category:cs.LG cs.AI stat.ML  published:2017-02-16 summary:Bipartite data is common in data engineering and brings unique challenges, particularly when it comes to clustering tasks that impose on strong structural assumptions. This work presents an unsupervised method for assessing similarity in bipartite data. Similar to some co-clustering methods, the method is based on regular equivalence in graphs. The algorithm uses spectral properties of a bipartite adjacency matrix to estimate similarity in both dimensions. The method is reflexive in that similarity in one dimension is used to inform similarity in the other. Reflexive regular equivalence can also use the structure of transitivities -- in a network sense -- the contribution of which is controlled by the algorithm's only free-parameter, $\alpha$. The method is completely unsupervised and can be used to validate assumptions of co-similarity, which are required but often untested, in co-clustering analyses. Three variants of the method with different normalizations are tested on synthetic data. The method is found to be robust to noise and well-suited to asymmetric co-similar structure, making it particularly informative for cluster analysis and recommendation in bipartite data of unknown structure. In experiments, the convergence and speed of the algorithm are found to be stable for different levels of noise. Real-world data from a network of malaria genes are analyzed, where the similarity produced by the reflexive method is shown to out-perform other measures' ability to correctly classify genes. version:1
arxiv-1702-04938 | Fast and unsupervised methods for multilingual cognate clustering | http://arxiv.org/abs/1702.04938 | id:1702.04938 author:Taraka Rama, Johannes Wahle, Pavel Sofroniev, Gerhard Jäger category:cs.CL  published:2017-02-16 summary:In this paper we explore the use of unsupervised methods for detecting cognates in multilingual word lists. We use online EM to train sound segment similarity weights for computing similarity between two words. We tested our online systems on geographically spread sixteen different language groups of the world and show that the Online PMI system (Pointwise Mutual Information) outperforms a HMM based system and two linguistically motivated systems: LexStat and ALINE. Our results suggest that a PMI system trained in an online fashion can be used by historical linguists for fast and accurate identification of cognates in not so well-studied language families. version:1
arxiv-1702-04877 | Generalizing Jensen and Bregman divergences with comparative convexity and the statistical Bhattacharyya distances with comparable means | http://arxiv.org/abs/1702.04877 | id:1702.04877 author:Frank Nielsen, Richard Nock category:cs.IT cs.LG math.IT  published:2017-02-16 summary:Comparative convexity is a generalization of convexity relying on abstract notions of means. We define the Jensen divergence and the Jensen diversity from the viewpoint of comparative convexity, and show how to obtain the generalized Bregman divergences as limit cases of skewed Jensen divergences. In particular, we report explicit formula of these generalized Bregman divergences when considering quasi-arithmetic means. Finally, we introduce a generalization of the Bhattacharyya statistical distances based on comparative means using relative convexity. version:1
arxiv-1702-04479 | Recognizing Dynamic Scenes with Deep Dual Descriptor based on Key Frames and Key Segments | http://arxiv.org/abs/1702.04479 | id:1702.04479 author:Sungeun Hong, Jongbin Ryu, Woobin Im, Hyun S. Yang category:cs.CV  published:2017-02-15 summary:Recognizing dynamic scenes is one of the fundamental problems in scene understanding, which categorizes moving scenes such as a forest fire, landslide, or avalanche. While existing methods focus on reliable capturing of static and dynamic information, few works have explored frame selection from a dynamic scene sequence. In this paper, we propose dynamic scene recognition using a deep dual descriptor based on `key frames' and `key segments.' Key frames that reflect the feature distribution of the sequence with a small number are used for capturing salient static appearances. Key segments, which are captured from the area around each key frame, provide an additional discriminative power by dynamic patterns within short time intervals. To this end, two types of transferred convolutional neural network features are used in our approach. A fully connected layer is used to select the key frames and key segments, while the convolutional layer is used to describe them. We conducted experiments using public datasets as well as a new dataset comprised of 23 dynamic scene classes with 10 videos per class. The evaluation results demonstrated the state-of-the-art performance of the proposed method. version:2
arxiv-1702-04869 | Improving automated multiple sclerosis lesion segmentation with a cascaded 3D convolutional neural network approach | http://arxiv.org/abs/1702.04869 | id:1702.04869 author:Sergi Valverde, Mariano Cabezas, Eloy Roura, Sandra González-Villà, Deborah Pareto, Joan-Carles Vilanova, LLuís Ramió-Torrentà, Àlex Rovira, Arnau Oliver, Xavier Lladó category:cs.CV  published:2017-02-16 summary:In this paper, we present a novel automated method for White Matter (WM) lesion segmentation of Multiple Sclerosis (MS) patient images. Our approach is based on a cascade of two 3D patch-wise convolutional neural networks (CNN). The first network is trained to be more sensitive revealing possible candidate lesion voxels while the second network is trained to reduce the number of misclassified voxels coming from the first network. This cascaded CNN architecture tends to learn well from small sets of training data, which can be very interesting in practice, given the difficulty to obtain manual label annotations and the large amount of available unlabeled Magnetic Resonance Imaging (MRI) data. We evaluate the accuracy of the proposed method on the public MS lesion segmentation challenge MICCAI2008 dataset, comparing it with respect to other state-of-the-art MS lesion segmentation tools. Furthermore, the proposed method is also evaluated on two private MS clinical datasets, where the performance of our method is also compared with different recent public available state-of-the-art MS lesion segmentation methods. At the time of writing this paper, our method is the best ranked approach on the MICCAI2008 challenge, outperforming the rest of 60 participant methods when using all the available input modalities (T1-w, T2-w and FLAIR), while still in the top-rank (3rd position) when using only T1-w and FLAIR modalities. On clinical MS data, our approach exhibits a significant increase in the accuracy segmenting of WM lesions when compared with the rest of evaluated methods, highly correlating ($r \ge 0.97$) also with the expected lesion volume. version:1
arxiv-1702-04069 | SSPP-DAN: Deep Domain Adaptation Network for Face Recognition with Single Sample Per Person | http://arxiv.org/abs/1702.04069 | id:1702.04069 author:Sungeun Hong, Woobin Im, Jongbin Ryu, Hyun S. Yang category:cs.CV  published:2017-02-14 summary:Real-world face recognition using a single sample per person (SSPP) is a challenging task. The problem is exacerbated if the conditions under which the gallery image and the probe set are captured are completely different. To address these issues from the perspective of domain adaptation, we introduce an SSPP domain adaptation network (SSPP-DAN). In the proposed approach, domain adaptation, feature extraction, and classification are performed jointly using a deep architecture with domain-adversarial training. However, the SSPP characteristic of one training sample per class is insufficient to train the deep architecture. To overcome this shortage, we generate synthetic images with varying poses using a 3D face model. Experimental evaluations using a realistic SSPP dataset show that deep domain adaptation and image synthesis complement each other and dramatically improve accuracy. Experiments on a benchmark dataset using the proposed approach show state-of-the-art performance. version:2
arxiv-1702-04488 | Transfer Learning for Low-Resource Chinese Word Segmentation with a Novel Neural Network | http://arxiv.org/abs/1702.04488 | id:1702.04488 author:Jingjing Xu, Xu Sun category:cs.CL  published:2017-02-15 summary:Recent works have been shown effective in using neural networks for Chinese word segmentation. However, these models rely on large-scale data and are less effective for low-resource datasets because of insufficient training data. Thus, we propose a transfer learning method to improve low-resource word segmentation by leveraging high-resource corpora. First, we train a teacher model on high-resource corpora and then use the learned knowledge to initialize a student model. Second, a weighted data similarity method is proposed to train the student model on low-resource data with the help of high-resource corpora. Finally, given that insufficient data puts forward higher requirements for feature extraction, we propose a novel neural network which improves feature learning. Experiment results show that our work significantly improves the performance on low-resource datasets: 2.3% and 1.5% F-score on PKU and CTB datasets. Furthermore, this paper achieves state-of-the-art results: 96.1%, and 96.2% F-score on PKU and CTB datasets. Besides, we explore an asynchronous parallel method on neural word segmentation to speed up training. The parallel method accelerates training substantially and is almost five times faster than a serial mode. version:2
arxiv-1702-04843 | Chord Angle Deviation using Tangent (CADT), an Efficient and Robust Contour-based Corner Detector | http://arxiv.org/abs/1702.04843 | id:1702.04843 author:Mohammad Asiful Hossain, Abdul Kawsar Tushar category:cs.CV  published:2017-02-16 summary:Detection of corner is the most essential process in a large number of computer vision and image processing applications. We have mentioned a number of popular contour-based corner detectors in our paper. Among all these detectors chord to triangular arm angle (CTAA) has been demonstrated as the most dominant corner detector in terms of average repeatability. We introduce a new effective method to calculate the value of curvature in this paper. By demonstrating experimental results, our proposed technique outperforms CTAA and other detectors mentioned in this paper. The results exhibit that our proposed method is simple yet efficient at finding out corners more accurately and reliably. version:1
arxiv-1702-04832 | Dynamic Partition Models | http://arxiv.org/abs/1702.04832 | id:1702.04832 author:Marc Goessling, Yali Amit category:stat.ML cs.LG  published:2017-02-16 summary:We present a new approach for learning compact and intuitive distributed representations with binary encoding. Rather than summing up expert votes as in products of experts, we employ for each variable the opinion of the most reliable expert. Data points are hence explained through a partitioning of the variables into expert supports. The partitions are dynamically adapted based on which experts are active. During the learning phase we adopt a smoothed version of this model that uses separate mixtures for each data dimension. In our experiments we achieve accurate reconstructions of high-dimensional data points with at most a dozen experts. version:1
arxiv-1702-04811 | An Analysis of Machine Learning Intelligence | http://arxiv.org/abs/1702.04811 | id:1702.04811 author:John P. Lalor, Hao Wu, Tsendsuren Munkhdalai, Hong Yu category:cs.CL  published:2017-02-15 summary:Deep neural networks (DNNs) have set state of the art results in many machine learning and NLP tasks. However, we do not have a strong understanding of what DNN models learn. In this paper, we examine learning in DNNs through analysis of their outputs. We compare DNN performance directly to a human population, and use characteristics of individual data points such as difficulty to see how well models perform on easy and hard examples. We investigate how training size and the incorporation of noise affect a DNN's ability to generalize and learn. Our experiments show that unlike traditional machine learning models (e.g., Naive Bayes, Decision Trees), DNNs exhibit human-like learning properties. As they are trained with more data, they are more able to distinguish between easy and difficult items, and performance on easy items improves at a higher rate than difficult items. We find that different DNN models exhibit different strengths in learning and are robust to noise in training data. version:1
arxiv-1702-04775 | Bayesian Additive Adaptive Basis Tensor Product Models for Modeling High Dimensional Surfaces: An application to high-throughput toxicity testing | http://arxiv.org/abs/1702.04775 | id:1702.04775 author:Matthew W. Wheeler category:stat.ML  published:2017-02-15 summary:Many modern data sets are sampled with error from complex high-dimensional surfaces, which are often difficult to model. Methods such as tensor product splines or Gaussian processes are effective characterizing a surface in two or three dimensions but may suffer from difficulties when representing higher dimensional surfaces. Motivated by high throughput toxicity testing where observed dose-response curves are cross sections of a larger surface defined by a chemical's structural properties. A model is developed to characterize this surface to predict untested chemicals' dose-responses. This manuscript proposes a novel approach that models the multidimensional surface as adaptive sum of learned basis functions formed as the tensor product of lower dimensional functions. The model is described, a Gibbs sampling algorithm proposed, and applied to simulated data as well as data taken from the US EPA's ToxCast high throughput toxicity testing platform. version:1
arxiv-1702-04770 | Training Language Models Using Target-Propagation | http://arxiv.org/abs/1702.04770 | id:1702.04770 author:Sam Wiseman, Sumit Chopra, Marc'Aurelio Ranzato, Arthur Szlam, Ruoyu Sun, Soumith Chintala, Nicolas Vasilache category:cs.CL cs.LG cs.NE  published:2017-02-15 summary:While Truncated Back-Propagation through Time (BPTT) is the most popular approach to training Recurrent Neural Networks (RNNs), it suffers from being inherently sequential (making parallelization difficult) and from truncating gradient flow between distant time-steps. We investigate whether Target Propagation (TPROP) style approaches can address these shortcomings. Unfortunately, extensive experiments suggest that TPROP generally underperforms BPTT, and we end with an analysis of this phenomenon, and suggestions for future work. version:1
arxiv-1702-04767 | Efficient Computation of Moments in Sum-Product Networks | http://arxiv.org/abs/1702.04767 | id:1702.04767 author:Han Zhao, Geoff Gordon category:cs.LG cs.AI  published:2017-02-15 summary:Bayesian online learning algorithms for Sum-Product Networks (SPNs) need to compute moments of model parameters under the one-step update posterior distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. We propose a linear-time algorithm that works even when the SPN is a directed acyclic graph (DAG). We achieve this goal by reducing the moment computation problem into a joint inference problem in SPNs and by taking advantage of a special structure of the one-step update posterior distribution: it is a multilinear polynomial with exponentially many monomials, and we can evaluate moments by differentiating. The latter is known as the \emph{differential trick}. We apply the proposed algorithm to develop a linear time assumed density filter (ADF) for SPN parameter learning. As an additional contribution, we conduct extensive experiments comparing seven different online learning algorithms for SPNs on 20 benchmark datasets. The new linear-time ADF method consistently achieves low runtime due to the efficient linear-time algorithm for moment computation; however, we discover that two other methods (CCCP and SMA) typically perform better statistically, while a third (BMM) is comparable to ADF. Interestingly, CCCP can be viewed as implicitly using the same differentiation trick that we make explicit here. The fact that two of the top four fastest methods use this trick suggests that the same trick might find other uses for SPN learning in the future. version:1
arxiv-1702-04710 | Multi-Task Convolutional Neural Network for Face Recognition | http://arxiv.org/abs/1702.04710 | id:1702.04710 author:Xi Yin, Xiaoming Liu category:cs.CV  published:2017-02-15 summary:This paper explores multi-task learning (MTL) for face recognition. We answer the questions of how and why MTL can improve the face recognition performance. First, we propose a multi-task Convolutional Neural Network (CNN) for face recognition where identity recognition is the main task and pose, illumination, and expression estimations are the side tasks. Second, we develop a dynamic-weighting scheme to automatically assign the loss weight to each side task. Third, we propose a pose-directed multi-task CNN by grouping different poses to learn pose-specific identity features, simultaneously across all poses. We observe that the side tasks serve as regularizations to disentangle the variations from the learnt identity features. Extensive experiments on the entire Multi-PIE dataset demonstrate the effectiveness of the proposed approach. To the best of our knowledge, this is the first work using all data in Multi-PIE for face recognition. Our approach is also applicable to in-the-wild datasets for pose-invariant face recognition and we achieve comparable or better performance than state of the art on LFW, CFP, and IJB-A. version:1
arxiv-1702-04686 | Support Vector Machines and generalisation in HEP | http://arxiv.org/abs/1702.04686 | id:1702.04686 author:Adrian Bevan, Rodrigo Gamboa Goñi, Jon Hays, Tom Stevenson category:physics.data-an cs.LG hep-ex  published:2017-02-15 summary:We review the concept of Support Vector Machines (SVMs) and discuss examples of their use in a number of scenarios. Several SVM implementations have been used in HEP and we exemplify this algorithm using the Toolkit for Multivariate Analysis (TMVA) implementation. We discuss examples relevant to HEP including background suppression for $H\to\tau^+\tau^-$ at the LHC with several different kernel functions. Performance benchmarking leads to the issue of generalisation of hyper-parameter selection. The avoidance of fine tuning (over training or over fitting) in MVA hyper-parameter optimisation, i.e. the ability to ensure generalised performance of an MVA that is independent of the training, validation and test samples, is of utmost importance. We discuss this issue and compare and contrast performance of hold-out and k-fold cross-validation. We have extended the SVM functionality and introduced tools to facilitate cross validation in TMVA and present results based on these improvements. version:1
arxiv-1702-04684 | Nearest Labelset Using Double Distances for Multi-label Classification | http://arxiv.org/abs/1702.04684 | id:1702.04684 author:Hyukjun Gweon, Matthias Schonlau, Stefan Steiner category:stat.ML cs.LG  published:2017-02-15 summary:Multi-label classification is a type of supervised learning where an instance may belong to multiple labels simultaneously. Predicting each label independently has been criticized for not exploiting any correlation between labels. In this paper we propose a novel approach, Nearest Labelset using Double Distances (NLDD), that predicts the labelset observed in the training data that minimizes a weighted sum of the distances in both the feature space and the label space to the new instance. The weights specify the relative tradeoff between the two distances. The weights are estimated from a binomial regression of the number of misclassified labels as a function of the two distances. Model parameters are estimated by maximum likelihood. NLDD only considers labelsets observed in the training data, thus implicitly taking into account label dependencies. Experiments on benchmark multi-label data sets show that the proposed method on average outperforms other well-known approaches in terms of Hamming loss, 0/1 loss, and multi-label accuracy and ranks second after ECC on the F-measure. version:1
arxiv-1702-04683 | Distributed deep learning on edge-devices: feasibility via adaptive compression | http://arxiv.org/abs/1702.04683 | id:1702.04683 author:Corentin Hardy, Erwan Le Merrer, Bruno Sericola category:cs.LG  published:2017-02-15 summary:The state-of-the-art results provided by deep learning come at the price of an intensive use of computing resources. The leading frameworks (eg., TensorFlow) are executed on GPUs or on high-end servers in datacenters. On the other end, there is a proliferation of personal devices with possibly free CPU cycles. In this paper, we ask the following question: Is distributed deep learning computation on WAN connected devices feasible, in spite of the traffic caused by learning tasks? We show that such a setup rises some important challenges, most notably the ingress traffic that the servers hosting the up-to-date model have to sustain. In order to reduce this stress, we propose AdaComp, a new algorithm for compressing worker updates to the model on the server. Applicable to stochastic gradient descent based approaches, it combines efficient gradient selection and learning rate modulation. We then experiment and measure the impact of compression and device reliability on the accuracy of learned models. To do so, we leverage an emulator platform we developed, that embeds the TensorFlow code into Linux containers. We report a reduction of the total amount of data sent by workers to the server by two order of magnitude (eg., 191-fold reduction for a convolutional network on the MNIST dataset), when compared to the standard algorithm based on asynchronous stochastic gradient descent, while maintaining model accuracy. version:1
arxiv-1702-04680 | Visual Discovery at Pinterest | http://arxiv.org/abs/1702.04680 | id:1702.04680 author:Andrew Zhai, Dmitry Kislyuk, Yushi Jing, Michael Feng, Eric Tzeng, Jeff Donahue, Yue Li Du, Trevor Darrell category:cs.CV  published:2017-02-15 summary:Over the past three years Pinterest has experimented with several visual search and recommendation services, including Related Pins (2014), Similar Looks (2015), Flashlight (2016) and Lens (2017). This paper presents an overview of our visual discovery engine powering these services, and shares the rationales behind our technical and product decisions such as the use of object detection and interactive user interfaces. We conclude that this visual discovery engine significantly improves engagement in both search and recommendation tasks. version:1
arxiv-1702-03196 | Universal Semantic Parsing | http://arxiv.org/abs/1702.03196 | id:1702.03196 author:Siva Reddy, Oscar Täckström, Slav Petrov, Mark Steedman, Mirella Lapata category:cs.CL  published:2017-02-10 summary:Universal Dependencies (UD) provides a cross-linguistically uniform syntactic representation, with the aim of advancing multilingual applications of parsing and natural language understanding. Reddy et al. (2016) recently developed a semantic interface for (English) Stanford Dependencies, based on the lambda calculus. In this work, we introduce UDepLambda, a similar semantic interface for UD, which allows mapping natural language to logical forms in an almost language-independent framework. We evaluate our approach on semantic parsing for the task of question answering against Freebase. To facilitate multilingual evaluation, we provide German and Spanish translations of the WebQuestions and GraphQuestions datasets. Results show that UDepLambda outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions. version:2
arxiv-1702-04663 | Handwritten Arabic Numeral Recognition using Deep Learning Neural Networks | http://arxiv.org/abs/1702.04663 | id:1702.04663 author:Akm Ashiquzzaman, Abdul Kawsar Tushar category:cs.CV  published:2017-02-15 summary:Handwritten character recognition is an active area of research with applications in numerous fields. Past and recent works in this field have concentrated on various languages. Arabic is one language where the scope of research is still widespread, with it being one of the most popular languages in the world and being syntactically different from other major languages. Das et al. \cite{DBLP:journals/corr/abs-1003-1891} has pioneered the research for handwritten digit recognition in Arabic. In this paper, we propose a novel algorithm based on deep learning neural networks using appropriate activation function and regularization layer, which shows significantly improved accuracy compared to the existing Arabic numeral recognition methods. The proposed model gives 97.4 percent accuracy, which is the recorded highest accuracy of the dataset used in the experiment. We also propose a modification of the method described in \cite{DBLP:journals/corr/abs-1003-1891}, where our method scores identical accuracy as that of \cite{DBLP:journals/corr/abs-1003-1891}, with the value of 93.8 percent. version:1
arxiv-1702-04657 | Computational Model for Predicting Visual Fixations from Childhood to Adulthood | http://arxiv.org/abs/1702.04657 | id:1702.04657 author:Olivier Le Meur, Antoine Coutrot, Zhi Liu, Adrien Le Roch, Andrea Helo, Pia Rama category:cs.CV  published:2017-02-15 summary:How people look at visual information reveals fundamental information about themselves, their interests and their state of mind. While previous visual attention models output static 2-dimensional saliency maps, saccadic models aim to predict not only where observers look at but also how they move their eyes to explore the scene. Here we demonstrate that saccadic models are a flexible framework that can be tailored to emulate observer's viewing tendencies. More specifically, we use the eye data from 101 observers split in 5 age groups (adults, 8-10 y.o., 6-8 y.o., 4-6 y.o. and 2 y.o.) to train our saccadic model for different stages of the development of the human visual system. We show that the joint distribution of saccade amplitude and orientation is a visual signature specific to each age group, and can be used to generate age-dependent scanpaths. Our age-dependent saccadic model not only outputs human-like, age-specific visual scanpath, but also significantly outperforms other state-of-the-art saliency models. In this paper, we demonstrate that the computational modelling of visual attention, through the use of saccadic model, can be efficiently adapted to emulate the gaze behavior of a specific group of observers. version:1
arxiv-1702-04656 | Robust Regression via Mutivariate Regression Depth | http://arxiv.org/abs/1702.04656 | id:1702.04656 author:Chao Gao category:math.ST stat.ML stat.TH  published:2017-02-15 summary:This paper studies robust regression in the settings of Huber's $\epsilon$-contamination models. We consider estimators that are maximizers of multivariate regression depth functions. These estimators are shown to achieve minimax rates in the settings of $\epsilon$-contamination models for various regression problems including nonparametric regression, sparse linear regression, reduced rank regression, etc. We also discuss a general notion of depth function for linear operators that has potential applications in robust functional linear regression. version:1
arxiv-1702-04641 | Filling missing data in point clouds by merging structured and unstructured point clouds | http://arxiv.org/abs/1702.04641 | id:1702.04641 author:Franziska Lippoldt, Hartmut Schwandt category:cs.CG cs.CV cs.DM 53A05 F.2.2; G.2.1; I.3.5  published:2017-02-15 summary:Point clouds arising from structured data, mainly as a result of CT scans, provides special properties on the distribution of points and the distances between those. Yet often, the amount of data provided can not compare to unstructured point clouds, i.e. data that arises from 3D light scans or laser scans. This article hereby proposes an approach to extend structured data and enhance the quality by inserting selected points from an unstructured point cloud. The resulting point cloud still has a partial structure that is called "half-structure". In this way, missing data that can not be optimally recovered through other surface reconstruction methods can be completed. version:1
arxiv-1702-04595 | Visualizing Deep Neural Network Decisions: Prediction Difference Analysis | http://arxiv.org/abs/1702.04595 | id:1702.04595 author:Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling category:cs.CV cs.AI  published:2017-02-15 summary:This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans). version:1
arxiv-1702-04577 | On the Discrepancy Between Kleinberg's Clustering Axioms and $k$-Means Clustering Algorithm Behavior | http://arxiv.org/abs/1702.04577 | id:1702.04577 author:Robert Kłopotek, Mieczysław Kłopotek category:cs.LG cs.AI I.5.2; I.2.6  published:2017-02-15 summary:This paper investigates the validity of Kleinberg's axioms for clustering functions with respect to the quite popular clustering algorithm called $k$-means. While Kleinberg's axioms have been discussed heavily in the past, we concentrate here on the case predominantly relevant for $k$-means algorithm, that is behavior embedded in Euclidean space. We point at some contradictions and counter intuitiveness aspects of this axiomatic set within $\mathbb{R}^m$ that were evidently not discussed so far. Our results suggest that apparently without defining clearly what kind of clusters we expect we will not be able to construct a valid axiomatic system. In particular we look at the shape and the gaps between the clusters. Finally we demonstrate that there exist several ways to reconcile the formulation of the axioms with their intended meaning and that under this reformulation the axioms stop to be contradictory and the real-world $k$-means algorithm conforms to this axiomatic system. version:1
arxiv-1702-04562 | Normalized Total Gradient: A New Measure for Multispectral Image Registration | http://arxiv.org/abs/1702.04562 | id:1702.04562 author:Shu-Jie Chen, Hui-Liang Shen category:cs.CV  published:2017-02-15 summary:Image registration is a fundamental issue in multispectral image processing. In filter wheel based multispectral imaging systems, the non-coplanar placement of the filters always causes the misalignment of multiple channel images. The selective characteristic of spectral response in multispectral imaging raises two challenges to image registration. First, the intensity levels of a local region may be different in individual channel images. Second, the local intensity may vary rapidly in some channel images while keeps stationary in others. Conventional multimodal measures, such as mutual information, correlation coefficient, and correlation ratio, can register images with different regional intensity levels, but will fail in the circumstance of severe local intensity variation. In this paper, a new measure, namely normalized total gradient (NTG), is proposed for multispectral image registration. The NTG is applied on the difference between two channel images. This measure is based on the key assumption (observation) that the gradient of difference image between two aligned channel images is sparser than that between two misaligned ones. A registration framework, which incorporates image pyramid and global/local optimization, is further introduced for rigid transform. Experimental results validate that the proposed method is effective for multispectral image registration and performs better than conventional methods. version:1
arxiv-1702-04561 | Probing for sparse and fast variable selection with model-based boosting | http://arxiv.org/abs/1702.04561 | id:1702.04561 author:Janek Thomas, Tobias Hepp, Andreas Mayr, Bernd Bischl category:stat.ML stat.CO  published:2017-02-15 summary:We present a new variable selection method based on model-based gradient boosting and randomly permuted variables. Model-based boosting is a tool to fit a statistical model while performing variable selection at the same time. A drawback of the fitting lies in the need of multiple model fits on slightly altered data (e.g. cross-validation or bootstrap) to find the optimal number of boosting iterations and prevent overfitting. In our proposed approach, we augment the data set with randomly permuted versions of the true variables, so called shadow variables, and stop the step-wise fitting as soon as such a variable would be added to the model. This allows variable selection in a single fit of the model without requiring further parameter tuning. We show that our probing approach can compete with state-of-the-art selection methods like stability selection in a high-dimensional classification benchmark and apply it on gene expression data for the estimation of riboflavin production of Bacillus subtilis. version:1
arxiv-1702-04521 | Frustratingly Short Attention Spans in Neural Language Modeling | http://arxiv.org/abs/1702.04521 | id:1702.04521 author:Michał Daniluk, Tim Rocktäschel, Johannes Welbl, Sebastian Riedel category:cs.CL cs.AI cs.LG cs.NE  published:2017-02-15 summary:Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning mid- and long-range dependencies. However, conventional attention mechanisms used in memory-augmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memory-augmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models. version:1
arxiv-1702-04510 | A Dependency-Based Neural Reordering Model for Statistical Machine Translation | http://arxiv.org/abs/1702.04510 | id:1702.04510 author:Christian Hadiwinoto, Hwee Tou Ng category:cs.CL  published:2017-02-15 summary:In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-the-art statistical MT system that uses sparse dependency-based reordering features. version:1
arxiv-1702-04471 | Deep Heterogeneous Feature Fusion for Template-Based Face Recognition | http://arxiv.org/abs/1702.04471 | id:1702.04471 author:Navaneeth Bodla, Jingxiao Zheng, Hongyu Xu, Jun-Cheng Chen, Carlos Castillo, Rama Chellappa category:cs.CV  published:2017-02-15 summary:Although deep learning has yielded impressive performance for face recognition, many studies have shown that different networks learn different feature maps: while some networks are more receptive to pose and illumination others appear to capture more local information. Thus, in this work, we propose a deep heterogeneous feature fusion network to exploit the complementary information present in features generated by different deep convolutional neural networks (DCNNs) for template-based face recognition, where a template refers to a set of still face images or video frames from different sources which introduces more blur, pose, illumination and other variations than traditional face datasets. The proposed approach efficiently fuses the discriminative information of different deep features by 1) jointly learning the non-linear high-dimensional projection of the deep features and 2) generating a more discriminative template representation which preserves the inherent geometry of the deep features in the feature space. Experimental results on the IARPA Janus Challenge Set 3 (Janus CS3) dataset demonstrate that the proposed method can effectively improve the recognition performance. In addition, we also present a series of covariate experiments on the face verification task for in-depth qualitative evaluations for the proposed approach. version:1
arxiv-1702-04463 | Analyzing the Weighted Nuclear Norm Minimization and Nuclear Norm Minimization based on Group Sparse Representation | http://arxiv.org/abs/1702.04463 | id:1702.04463 author:Zhiyuan Zha, Xinggan Zhang, Qiong Wang, Yechao Bai, Lan Tang category:cs.CV  published:2017-02-15 summary:The nuclear norm minimization (NNM) tends to over-shrink the rank components and treats the different rank components equally, thus limits its capability and flexibility in practical applications. Recent advances have suggested that the weighted nuclear norm minimization (WNNM) is expected to be more appropriate than NNM. However, it still lacks a plausible mathematical explanation why WNNM is more appropriate than NNM. In this paper, we analyze the WNNM and NNM from a point of the group sparse representation (GSR). Firstly, an adaptive dictionary for each group is designed. Then we show mathematically that WNNM is more appropriate than NNM. We exploit the proposed scheme to two typical low level vision tasks, including image deblurring and image compressive sensing (CS) recovery. Experimental results have demonstrated that the proposed scheme outperforms many state-of-the-art methods. version:1
arxiv-1702-04459 | Building Robust Stochastic Configuration Networks with Kernel Density Estimation | http://arxiv.org/abs/1702.04459 | id:1702.04459 author:Dianhui Wang, Ming Li category:cs.NE cs.LG stat.ML  published:2017-02-15 summary:This paper aims at developing robust data modelling techniques using stochastic configuration networks (SCNs), where a weighted least squares method with the well-known kernel density estimation (KDE) is used in the design of SCNs. The alternating optimization (AO) technique is applied for iteratively building a robust SCN model that can reduce some negative impacts, caused by corrupted data or outliers, in learning process. Simulation studies are carried out on a function approximation and four benchmark datasets, also a case study on industrial application is reported. Comparisons against other robust modelling techniques, including the probabilistic robust learning algorithm for neural networks with random weights (PRNNRW) and an Improved RVFL, demonstrate that our proposed robust stochastic configuration algorithm with KDE (RSC-KED) perform favourably. version:1
arxiv-1702-04455 | Learning from Ambiguously Labeled Face Images | http://arxiv.org/abs/1702.04455 | id:1702.04455 author:Ching-Hui Chen, Vishal M. Patel, Rama Chellappa category:cs.CV  published:2017-02-15 summary:Learning a classifier from ambiguously labeled face images is challenging since training images are not always explicitly-labeled. For instance, face images of two persons in a news photo are not explicitly labeled by their names in the caption. We propose a Matrix Completion for Ambiguity Resolution (MCar) method for predicting the actual labels from ambiguously labeled images. This step is followed by learning a standard supervised classifier from the disambiguated labels to classify new images. To prevent the majority labels from dominating the result of MCar, we generalize MCar to a weighted MCar (WMCar) that handles label imbalance. Since WMCar outputs a soft labeling vector of reduced ambiguity for each instance, we can iteratively refine it by feeding it as the input to WMCar. Nevertheless, such an iterative implementation can be affected by the noisy soft labeling vectors, and thus the performance may degrade. Our proposed Iterative Candidate Elimination (ICE) procedure makes the iterative ambiguity resolution possible by gradually eliminating a portion of least likely candidates in ambiguously labeled face. We further extend MCar to incorporate the labeling constraints between instances when such prior knowledge is available. Compared to existing methods, our approach demonstrates improvement on several ambiguously labeled datasets. version:1
arxiv-1702-04179 | Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification | http://arxiv.org/abs/1702.04179 | id:1702.04179 author:Lin Wu, Yang Wang category:cs.CV  published:2017-02-14 summary:Given a pedestrian image as a query, the purpose of person re-identification is to identify the correct match from a large collection of gallery images depicting the same person captured by disjoint camera views. The critical challenge is how to construct a robust yet discriminative feature representation to capture the compounded variations in pedestrian appearance. To this end, deep learning methods have been proposed to extract hierarchical features against extreme variability of appearance. However, existing methods in this category generally neglect the efficiency in the matching stage whereas the searching speed of a re-identification system is crucial in real-world applications. In this paper, we present a novel deep hashing framework with Convolutional Neural Networks (CNNs) for fast person re-identification. Technically, we simultaneously learn both CNN features and hash functions/codes to get robust yet discriminative features and similarity-preserving hash codes. Thereby, person re-identification can be resolved by efficiently computing and ranking the Hamming distances between images. A structured loss function defined over positive pairs and hard negatives is proposed to formulate a novel optimization problem so that fast convergence and more stable optimized solution can be obtained. Extensive experiments on two benchmarks CUHK03 \cite{FPNN} and Market-1501 \cite{Market1501} show that the proposed deep architecture is efficacy over state-of-the-arts. version:2
arxiv-1702-03849 | Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis | http://arxiv.org/abs/1702.03849 | id:1702.03849 author:Maxim Raginsky, Alexander Rakhlin, Matus Telgarsky category:cs.LG math.OC math.PR stat.ML  published:2017-02-13 summary:Stochastic Gradient Langevin Dynamics (SGLD) is a popular variant of Stochastic Gradient Descent, where properly scaled isotropic Gaussian noise is added to an unbiased estimate of the gradient at each iteration. This modest change allows SGLD to escape local minima and suffices to guarantee asymptotic convergence to global minimizers for sufficiently regular non-convex objectives (Gelfand and Mitter, 1991). The present work provides a nonasymptotic analysis in the context of non-convex learning problems: SGLD requires $\tilde{O}(\varepsilon^{-4})$ iterations to sample $\tilde{O}(\varepsilon)$-approximate minimizers of both empirical and population risk, where $\tilde{O}(\cdot)$ hides polynomial dependence on a temperature parameter, the model dimension, and a certain spectral gap parameter. As in the asymptotic setting, our analysis relates the discrete-time SGLD Markov chain to a continuous-time diffusion process. A new tool that drives the results is the use of weighted transportation cost inequalities to quantify the rate of convergence of SGLD to a stationary distribution in the Euclidean $2$-Wasserstein distance. version:2
arxiv-1702-04423 | Efficient Multi-task Feature and Relationship Learning | http://arxiv.org/abs/1702.04423 | id:1702.04423 author:Han Zhao, Otilia Stretcu, Renato Negrinho, Alex Smola, Geoff Gordon category:cs.LG cs.AI  published:2017-02-14 summary:In this paper we propose a multi-convex framework for multi-task learning that improves predictions by learning relationships both between tasks and between features. Our framework is a generalization of related methods in multi-task learning, that either learn task relationships, or feature relationships, but not both. We start with a hierarchical Bayesian model, and use the empirical Bayes method to transform the underlying inference problem into a multi-convex optimization problem. We propose a coordinate-wise minimization algorithm that has a closed form solution for each block subproblem. Naively these solutions would be expensive to compute, but by using the theory of doubly stochastic matrices, we are able to reduce the underlying matrix optimization subproblem into a minimum weight perfect matching problem on a complete bipartite graph, and solve it analytically and efficiently. To solve the weight learning subproblem, we propose three different strategies, including a gradient descent method with linear convergence guarantee when the instances are not shared by multiple tasks, and a numerical solution based on Sylvester equation when instances are shared. We demonstrate the efficiency of our method on both synthetic datasets and real-world datasets. Experiments show that the proposed optimization method is orders of magnitude faster than an off-the-shelf projected gradient method, and our model is able to exploit the correlation structures among multiple tasks and features. version:1
arxiv-1702-04415 | Small Boxes Big Data: A Deep Learning Approach to Optimize Variable Sized Bin Packing | http://arxiv.org/abs/1702.04415 | id:1702.04415 author:Feng Mao, Edgar Blanco, Mingang Fu, Rohit Jain, Anurag Gupta, Sebastien Mancel, Rong Yuan, Stephen Guo, Sai Kumar, Yayang Tian category:cs.LG stat.ML I.1.2; I.2.8  published:2017-02-14 summary:Bin Packing problems have been widely studied because of their broad applications in different domains. Known as a set of NP-hard problems, they have different vari- ations and many heuristics have been proposed for obtaining approximate solutions. Specifically, for the 1D variable sized bin packing problem, the two key sets of optimization heuristics are the bin assignment and the bin allocation. Usually the performance of a single static optimization heuristic can not beat that of a dynamic one which is tailored for each bin packing instance. Building such an adaptive system requires modeling the relationship between bin features and packing perform profiles. The primary drawbacks of traditional AI machine learnings for this task are the natural limitations of feature engineering, such as the curse of dimensionality and feature selection quality. We introduce a deep learning approach to overcome the drawbacks by applying a large training data set, auto feature selection and fast, accurate labeling. We show in this paper how to build such a system by both theoretical formulation and engineering practices. Our prediction system achieves up to 89% training accuracy and 72% validation accuracy to select the best heuristic that can generate a better quality bin packing solution. version:1
arxiv-1702-04405 | ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes | http://arxiv.org/abs/1702.04405 | id:1702.04405 author:Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner category:cs.CV  published:2017-02-14 summary:A key requirement for leveraging supervised deep learning methods is the availability of large, labeled datasets. Unfortunately, in the context of RGB-D scene understanding, very little data is available -- current datasets cover a small range of scene views and have limited semantic annotations. To address this issue, we introduce ScanNet, an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with 3D camera poses, surface reconstructions, and semantic segmentations. To collect this data, we designed an easy-to-use and scalable RGB-D capture system that includes automated surface reconstruction and crowdsourced semantic annotation. We show that using this data helps achieve state-of-the-art performance on several 3D scene understanding tasks, including 3D object classification, semantic voxel labeling, and CAD model retrieval. The dataset is freely available at http://www.scan-net.org. version:1
arxiv-1702-04377 | Enhanced Facial Recognition Framework based on Skin Tone and False Alarm Rejection | http://arxiv.org/abs/1702.04377 | id:1702.04377 author:Ali Sharifara, Mohd Shafry Mohd Rahim, Farhad Navabifar, Dylan Ebert, Amir Ghaderi, Michalis Papakostas category:cs.CV  published:2017-02-14 summary:Face detection is one of the challenging tasks in computer vision. Human face detection plays an essential role in the first stage of face processing applications such as face recognition, face tracking, image database management, etc. In these applications, face objects often come from an inconsequential part of images that contain variations, namely different illumination, poses, and occlusion. These variations can decrease face detection rate noticeably. Most existing face detection approaches are not accurate, as they have not been able to resolve unstructured images due to large appearance variations and can only detect human faces under one particular variation. Existing frameworks of face detection need enhancements to detect human faces under the stated variations to improve detection rate and reduce detection time. In this study, an enhanced face detection framework is proposed to improve detection rate based on skin color and provide a validation process. A preliminary segmentation of the input images based on skin color can significantly reduce search space and accelerate the process of human face detection. The primary detection is based on Haar-like features and the Adaboost algorithm. A validation process is introduced to reject non-face objects, which might occur during the face detection process. The validation process is based on two-stage Extended Local Binary Patterns. The experimental results on the CMU-MIT and Caltech 10000 datasets over a wide range of facial variations in different colors, positions, scales, and lighting conditions indicated a successful face detection rate. version:1
arxiv-1702-04372 | A case study on using speech-to-translation alignments for language documentation | http://arxiv.org/abs/1702.04372 | id:1702.04372 author:Antonios Anastasopoulos, David Chiang category:cs.CL  published:2017-02-14 summary:For many low-resource or endangered languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Recent work exploits such annotations to produce speech-to-translation alignments, without access to any text transcriptions. We investigate whether providing such information can aid in producing better (mismatched) crowdsourced transcriptions, which in turn could be valuable for training speech recognition systems, and show that they can indeed be beneficial through a small-scale case study as a proof-of-concept. We also present a simple phonetically aware string averaging technique that produces transcriptions of higher quality. version:1
arxiv-1702-04333 | On the Relevance of Auditory-Based Gabor Features for Deep Learning in Automatic Speech Recognition | http://arxiv.org/abs/1702.04333 | id:1702.04333 author:Angel Mario Castro Martinez, Sri Harish Mallidi, Bernd T. Meyer category:cs.CL  published:2017-02-14 summary:Previous studies support the idea of merging auditory-based Gabor features with deep learning architectures to achieve robust automatic speech recognition, however, the cause behind the gain of such combination is still unknown. We believe these representations provide the deep learning decoder with more discriminable cues. Our aim with this paper is to validate this hypothesis by performing experiments with three different recognition tasks (Aurora 4, CHiME 2 and CHiME 3) and assess the discriminability of the information encoded by Gabor filterbank features. Additionally, to identify the contribution of low, medium and high temporal modulation frequencies subsets of the Gabor filterbank were used as features (dubbed LTM, MTM and HTM respectively). With temporal modulation frequencies between 16 and 25 Hz, HTM consistently outperformed the remaining ones in every condition, highlighting the robustness of these representations against channel distortions, low signal-to-noise ratios and acoustically challenging real-life scenarios with relative improvements from 11 to 56% against a Mel-filterbank-DNN baseline. To explain the results, a measure of similarity between phoneme classes from DNN activations is proposed and linked to their acoustic properties. We find this measure to be consistent with the observed error rates and highlight specific differences on phoneme level to pinpoint the benefit of the proposed features. version:1
arxiv-1702-05421 | The Effect of Color Space Selection on Detectability and Discriminability of Colored Objects | http://arxiv.org/abs/1702.05421 | id:1702.05421 author:Amir Rasouli, John K. Tsotsos category:cs.CV  published:2017-02-14 summary:In this paper, we investigate the effect of color space selection on detectability and discriminability of colored objects under various conditions. 20 color spaces from the literature are evaluated on a large dataset of simulated and real images. We measure the suitability of color spaces from two different perspectives: detectability and discriminability of various color groups. Through experimental evaluation, we found that there is no single optimal color space suitable for all color groups. The color spaces have different levels of sensitivity to different color groups and they are useful depending on the color of the sought object. Overall, the best results were achieved in both simulated and real images using color spaces C1C2C3, UVW and XYZ. In addition, using a simulated environment, we show a practical application of color space selection in the context of top-down control in active visual search. The results indicate that on average color space C1C2C3 followed by HSI and XYZ achieve the best time in searching for objects of various colors. Here, the right choice of color space can improve time of search on average by 20%. As part of our contribution, we also introduce a large dataset of simulated 3D objects version:1
arxiv-1702-04292 | Integrating Three Mechanisms of Visual Attention for Active Visual Search | http://arxiv.org/abs/1702.04292 | id:1702.04292 author:Amir Rasouli, John K. Tsotsos category:cs.CV  published:2017-02-14 summary:Algorithms for robotic visual search can benefit from the use of visual attention methods in order to reduce computational costs. Here, we describe how three distinct mechanisms of visual attention can be integrated and productively used to improve search performance. The first is viewpoint selection as has been proposed earlier using a greedy search over a probabilistic occupancy grid representation. The second is top-down object-based attention using a histogram backprojection method, also previously described. The third is visual saliency. This is novel in the sense that it is not used as a region-of-interest method for the current image but rather as a noncombinatorial form of look-ahead in search for future viewpoint selection. Additionally, the integration of these three attentional schemes within a single framework is unique and not previously studied. We examine our proposed method in scenarios where little or no information regarding the environment is available. Through extensive experiments on a mobile robot, we show that our method improves visual search performance by reducing the time and number of actions required. version:1
arxiv-1702-04289 | Regularities and Irregularities in Order Flow Data | http://arxiv.org/abs/1702.04289 | id:1702.04289 author:Martin Theissen, Sebastian M. Krause, Thomas Guhr category:q-fin.TR stat.ML  published:2017-02-14 summary:We identify and analyze statistical regularities and irregularities in the recent order flow of different NASDAQ stocks, focusing on the positions where orders are placed in the orderbook. This includes limit orders being placed outside of the spread, inside the spread and (effective) market orders. We find that limit order placement inside the spread is strongly determined by the dynamics of the spread size. Most orders, however, arrive outside of the spread. While for some stocks order placement on or next to the quotes is dominating, deeper price levels are more important for other stocks. As market orders are usually adjusted to the quote volume, the impact of market orders depends on the orderbook structure, which we find to be quite diverse among the analyzed stocks as a result of the way limit order placement takes place. version:1
arxiv-1702-04283 | Exploring loss function topology with cyclical learning rates | http://arxiv.org/abs/1702.04283 | id:1702.04283 author:Leslie N. Smith, Nicholay Topin category:cs.LG cs.NE  published:2017-02-14 summary:We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. Files to replicate these results are available at https://github.com/lnsmith54/exploring-loss version:1
arxiv-1702-04174 | FERA 2017 - Addressing Head Pose in the Third Facial Expression Recognition and Analysis Challenge | http://arxiv.org/abs/1702.04174 | id:1702.04174 author:Michel F. Valstar, Enrique Sánchez-Lozano, Jeffrey F. Cohn, László A. Jeni, Jeffrey M. Girard, Zheng Zhang, Lijun Yin, Maja Pantic category:cs.CV  published:2017-02-14 summary:The field of Automatic Facial Expression Analysis has grown rapidly in recent years. However, despite progress in new approaches as well as benchmarking efforts, most evaluations still focus on either posed expressions, near-frontal recordings, or both. This makes it hard to tell how existing expression recognition approaches perform under conditions where faces appear in a wide range of poses (or camera views), displaying ecologically valid expressions. The main obstacle for assessing this is the availability of suitable data, and the challenge proposed here addresses this limitation. The FG 2017 Facial Expression Recognition and Analysis challenge (FERA 2017) extends FERA 2015 to the estimation of Action Units occurrence and intensity under different camera views. In this paper we present the third challenge in automatic recognition of facial expressions, to be held in conjunction with the 12th IEEE conference on Face and Gesture Recognition, May 2017, in Washington, United States. Two sub-challenges are defined: the detection of AU occurrence, and the estimation of AU intensity. In this work we outline the evaluation protocol, the data used, and the results of a baseline method for both sub-challenges. version:1
arxiv-1702-04165 | Crossmatching variable objects with the Gaia data | http://arxiv.org/abs/1702.04165 | id:1702.04165 author:Lorenzo Rimoldini, Krzysztof Nienartowicz, Maria Süveges, Jonathan Charnas, Leanne P. Guy, Grégory Jevardat de Fombelle, Berry Holl, Isabelle Lecoeur-Taïbi, Nami Mowlavi, Diego Ordóñez-Blanco, Laurent Eyer category:astro-ph.IM cs.LG  published:2017-02-14 summary:Tens of millions of new variable objects are expected to be identified in over a billion time series from the Gaia mission. Crossmatching known variable sources with those from Gaia is crucial to incorporate current knowledge, understand how these objects appear in the Gaia data, train supervised classifiers to recognise known classes, and validate the results of the Variability Processing and Analysis Coordination Unit (CU7) within the Gaia Data Analysis and Processing Consortium (DPAC). The method employed by CU7 to crossmatch variables for the first Gaia data release includes a binary classifier to take into account positional uncertainties, proper motion, targeted variability signals, and artefacts present in the early calibration of the Gaia data. Crossmatching with a classifier makes it possible to automate all those decisions which are typically made during visual inspection. The classifier can be trained with objects characterized by a variety of attributes to ensure similarity in multiple dimensions (astrometry, photometry, time-series features), with no need for a-priori transformations to compare different photometric bands, or of predictive models of the motion of objects to compare positions. Other advantages as well as some disadvantages of the method are discussed. Implementation steps from the training to the assessment of the crossmatch classifier and selection of results are described. version:1
arxiv-1702-04126 | Gaussian-Dirichlet Posterior Dominance in Sequential Learning | http://arxiv.org/abs/1702.04126 | id:1702.04126 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.LG math.PR  published:2017-02-14 summary:We consider the problem of sequential learning from categorical observations bounded in [0,1]. We establish an ordering between the Dirichlet posterior over categorical outcomes and a Gaussian posterior under observations with N(0,1) noise. We establish that, conditioned upon identical data with at least two observations, the posterior mean of the categorical distribution will always second-order stochastically dominate the posterior mean of the Gaussian distribution. These results provide a useful tool for the analysis of sequential learning under categorical outcomes. version:1
arxiv-1702-04125 | One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network | http://arxiv.org/abs/1702.04125 | id:1702.04125 author:Vedran Vukotić, Silvia-Laura Pintea, Christian Raymond, Guillaume Gravier, Jan Van Gemert category:cs.CV  published:2017-02-14 summary:There is an inherent need for machines to have a notion of how entities within their environment behave and to anticipate changes in the near future. In this work, we focus on anticipating future appearance given the current frame of a video. Typical methods are used either to predict the next frame of a video or to predict future optical flow or trajectories based on a single video frame. This work presents an experiment on stretching the ability of CNNs to predict %not the next frame, but an anticipation of appearance at an arbitrarily given future time. We condition our predicted video frames on a continuous time variable that allows us to anticipate future frames at a given temporal distance, directly from the current input video frame. We show that CNNs can learn an intrinsic representation of typical appearance changes over time and successfully generate realistic predictions in one step - at a deliberate time difference in the near future. The method is evaluated on the KTH human actions dataset and compared to a baseline consisting of an analogous CNN architecture that is not time-aware. version:1
arxiv-1702-04121 | Practical Learning of Predictive State Representations | http://arxiv.org/abs/1702.04121 | id:1702.04121 author:Carlton Downey, Ahmed Hefny, Geoffrey Gordon category:stat.ML cs.LG  published:2017-02-14 summary:Over the past decade there has been considerable interest in spectral algorithms for learning Predictive State Representations (PSRs). Spectral algorithms have appealing theoretical guarantees; however, the resulting models do not always perform well on inference tasks in practice. One reason for this behavior is the mismatch between the intended task (accurate filtering or prediction) and the loss function being optimized by the algorithm (estimation error in model parameters). A natural idea is to improve performance by refining PSRs using an algorithm such as EM. Unfortunately it is not obvious how to apply apply an EM style algorithm in the context of PSRs as the Log Likelihood is not well defined for all PSRs. We show that it is possible to overcome this problem using ideas from Predictive State Inference Machines. We combine spectral algorithms for PSRs as a consistent and efficient initialization with PSIM-style updates to refine the resulting model parameters. By combining these two ideas we develop Inference Gradients, a simple, fast, and robust method for practical learning of PSRs. Inference Gradients performs gradient descent in the PSR parameter space to optimize an inference-based loss function like PSIM. Because Inference Gradients uses a spectral initialization we get the same consistency benefits as PSRs. We show that Inference Gradients outperforms both PSRs and PSIMs on real and synthetic data sets. version:1
arxiv-1702-04114 | Graph Based Over-Segmentation Methods for 3D Point Clouds | http://arxiv.org/abs/1702.04114 | id:1702.04114 author:Yizhak Ben-Shabat, Tamar Avraham, Michael Lindenbaum, Anath Fischer category:cs.CV  published:2017-02-14 summary:Over-segmentation, or super-pixel generation, is a common preliminary stage for many computer vision applications. New acquisition technologies enable the capturing of 3D point clouds that contain color and geometrical information. This 3D information introduces a new conceptual change that can be utilized to improve the results of over-segmentation, which uses mainly color information, and to generate clusters of points we call super-points. We consider a variety of possible 3D extensions of the Local Variation (LV) graph based over-segmentation algorithms, and compare them thoroughly. We consider different alternatives for constructing the connectivity graph, for assigning the edge weights, and for defining the merge criterion, which must now account for the geometric information and not only color. Following this evaluation, we derive a new generic algorithm for over-segmentation of 3D point clouds. We call this new algorithm Point Cloud Local Variation (PCLV). The advantages of the new over-segmentation algorithm are demonstrated on both outdoor and cluttered indoor scenes. Performance analysis of the proposed approach compared to state-of-the-art 2D and 3D over-segmentation algorithms shows significant improvement according to the common performance measures. version:1
arxiv-1702-04111 | Efficient Algorithms for Moral Lineage Tracing | http://arxiv.org/abs/1702.04111 | id:1702.04111 author:Markus Rempfler, Jan-Hendrik Lange, Florian Jug, Corinna Blasse, Eugene W. Myers, Bjoern H. Menze, Bjoern Andres category:cs.CV  published:2017-02-14 summary:Lineage tracing, the joint segmentation and tracking of living cells as they move and divide in a sequence of light microscopy images, is a challenging task. Jug et al. have proposed a mathematical abstraction of this task, the moral lineage tracing problem (MLTP) whose feasible solutions define a segmentation of every image and a lineage forest of cells. Their branch-and-cut algorithm, however, is prone to many cuts and slow convergences for large instances. To address this problem, we make three contributions: Firstly, we improve the branch-and-cut algorithm by separating tighter cutting planes. Secondly, we define two primal feasible local search algorithms for the MLTP. Thirdly, we show in experiments that our algorithms decrease the runtime on the problem instances of Jug et al. considerably and find solutions on larger instances in reasonable time. version:1
arxiv-1702-04066 | JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction | http://arxiv.org/abs/1702.04066 | id:1702.04066 author:Courtney Napoles, Keisuke Sakaguchi, Joel Tetreault category:cs.CL  published:2017-02-14 summary:We present a new parallel corpus, JHU FLuency-Extended GUG corpus (JFLEG) for developing and evaluating grammatical error correction (GEC). Unlike other corpora, it represents a broad range of language proficiency levels and uses holistic fluency edits to not only correct grammatical errors but also make the original text more native sounding. We describe the types of corrections made and benchmark four leading GEC systems on this corpus, identifying specific areas in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC. version:1
arxiv-1702-04040 | A Graphical Social Topology Model for Multi-Object Tracking | http://arxiv.org/abs/1702.04040 | id:1702.04040 author:Shan Gao, Xiaogang Chen, Qixiang Ye, Junliang Xing, Arjan Kuijper, Xiangyang Ji category:cs.CV  published:2017-02-14 summary:Tracking multiple objects is a challenging task when objects move in groups and occlude each other. Existing methods have investigated the problems of group division and group energy-minimization; however, lacking overall object-group topology modeling limits their ability in handling complex object and group dynamics. Inspired with the social affinity property of moving objects, we propose a Graphical Social Topology (GST) model, which estimates the group dynamics by jointly modeling the group structure and the states of objects using a topological representation. With such topology representation, moving objects are not only assigned to groups, but also dynamically connected with each other, which enables in-group individuals to be correctly associated and the cohesion of each group to be precisely modeled. Using well-designed topology learning modules and topology training, we infer the birth/death and merging/splitting of dynamic groups. With the GST model, the proposed multi-object tracker can naturally facilitate the occlusion problem by treating the occluded object and other in-group members as a whole while leveraging overall state transition. Experiments on both RGB and RGB-D datasets confirm that the proposed multi-object tracker improves the state-of-the-arts especially in crowded scenes. version:1
arxiv-1702-04037 | Evolution-Preserving Dense Trajectory Descriptors | http://arxiv.org/abs/1702.04037 | id:1702.04037 author:Yang Wang, Vinh Tran, Minh Hoai category:cs.CV  published:2017-02-14 summary:Recently Trajectory-pooled Deep-learning Descriptors were shown to achieve state-of-the-art human action recognition results on a number of datasets. This paper improves their performance by applying rank pooling to each trajectory, encoding the temporal evolution of deep learning features computed along the trajectory. This leads to Evolution-Preserving Trajectory (EPT) descriptors, a novel type of video descriptor that significantly outperforms Trajectory-pooled Deep-learning Descriptors. EPT descriptors are defined based on dense trajectories, and they provide complimentary benefits to video descriptors that are not based on trajectories. In particular, we show that the combination of EPT descriptors and VideoDarwin leads to state-of-the-art performance on Hollywood2 and UCF101 datasets. version:1
arxiv-1702-04018 | Intercomparison of Machine Learning Methods for Statistical Downscaling: The Case of Daily and Extreme Precipitation | http://arxiv.org/abs/1702.04018 | id:1702.04018 author:Thomas Vandal, Evan Kodra, Auroop R Ganguly category:stat.ML  published:2017-02-13 summary:Statistical downscaling of global climate models (GCMs) allows researchers to study local climate change effects decades into the future. A wide range of statistical models have been applied to downscaling GCMs but recent advances in machine learning have not been explored. In this paper, we compare four fundamental statistical methods, Bias Correction Spatial Disaggregation (BCSD), Ordinary Least Squares, Elastic-Net, and Support Vector Machine, with three more advanced machine learning methods, Multi-task Sparse Structure Learning (MSSL), BCSD coupled with MSSL, and Convolutional Neural Networks to downscale daily precipitation in the Northeast United States. Metrics to evaluate of each method's ability to capture daily anomalies, large scale climate shifts, and extremes are analyzed. We find that linear methods, led by BCSD, consistently outperform non-linear approaches. The direct application of state-of-the-art machine learning methods to statistical downscaling does not provide improvements over simpler, longstanding approaches. version:1
arxiv-1702-04013 | Is a Data-Driven Approach still Better than Random Choice with Naive Bayes classifiers? | http://arxiv.org/abs/1702.04013 | id:1702.04013 author:Piotr Szymański, Tomasz Kajdanowicz category:cs.LG stat.ML  published:2017-02-13 summary:We study the performance of data-driven, a priori and random approaches to label space partitioning for multi-label classification with a Gaussian Naive Bayes classifier. Experiments were performed on 12 benchmark data sets and evaluated on 5 established measures of classification quality: micro and macro averaged F1 score, Subset Accuracy and Hamming loss. Data-driven methods are significantly better than an average run of the random baseline. In case of F1 scores and Subset Accuracy - data driven approaches were more likely to perform better than random approaches than otherwise in the worst case. There always exists a method that performs better than a priori methods in the worst case. The advantage of data-driven methods against a priori methods with a weak classifier is lesser than when tree classifiers are used. version:1
arxiv-1702-04008 | Soft Weight-Sharing for Neural Network Compression | http://arxiv.org/abs/1702.04008 | id:1702.04008 author:Karen Ullrich, Edward Meeds, Max Welling category:stat.ML cs.LG  published:2017-02-13 summary:The success of deep learning in numerous application domains created the de- sire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2015a) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of soft weight-sharing (Nowlan & Hinton, 1992). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle. version:1
arxiv-1702-03994 | metboost: Exploratory regression analysis with hierarchically clustered data | http://arxiv.org/abs/1702.03994 | id:1702.03994 author:Patrick J. Miller, Daniel B. McArtor, Gitta H. Lubke category:stat.ML  published:2017-02-13 summary:As data collections become larger, exploratory regression analysis becomes more important but more challenging. When observations are hierarchically clustered the problem is even more challenging because model selection with mixed effect models can produce misleading results when nonlinear effects are not included into the model (Bauer and Cai, 2009). A machine learning method called boosted decision trees (Friedman, 2001) is a good approach for exploratory regression analysis in real data sets because it can detect predictors with nonlinear and interaction effects while also accounting for missing data. We propose an extension to boosted decision decision trees called metboost for hierarchically clustered data. It works by constraining the structure of each tree to be the same across groups, but allowing the terminal node means to differ. This allows predictors and split points to lead to different predictions within each group, and approximates nonlinear group specific effects. Importantly, metboost remains computationally feasible for thousands of observations and hundreds of predictors that may contain missing values. We apply the method to predict math performance for 15,240 students from 751 schools in data collected in the Educational Longitudinal Study 2002 (Ingels et al., 2007), allowing 76 predictors to have unique effects for each school. When comparing results to boosted decision trees, metboost has 15% improved prediction performance. Results of a large simulation study show that metboost has up to 70% improved variable selection performance and up to 30% improved prediction performance compared to boosted decision trees when group sizes are small version:1
arxiv-1702-03993 | The Causal Role of Astrocytes in Slow-Wave Rhythmogenesis: A Computational Modelling Study | http://arxiv.org/abs/1702.03993 | id:1702.03993 author:Leo Kozachkov, Konstantinos P. Michmizos category:q-bio.NC cs.NE q-bio.CB  published:2017-02-13 summary:Finding the origin of slow and infra-slow oscillations could reveal or explain brain mechanisms in health and disease. Here, we present a biophysically constrained computational model of a neural network where the inclusion of astrocytes introduced slow and infra-slow-oscillations, through two distinct mechanisms. Specifically, we show how astrocytes can modulate the fast network activity through their slow inter-cellular calcium wave speed and amplitude and possibly cause the oscillatory imbalances observed in diseases commonly known for such abnormalities, namely Alzheimer's disease, Parkinson's disease, epilepsy, depression and ischemic stroke. This work aims to increase our knowledge on how astrocytes and neurons synergize to affect brain function and dysfunction. version:1
arxiv-1702-03970 | End-to-End Interpretation of the French Street Name Signs Dataset | http://arxiv.org/abs/1702.03970 | id:1702.03970 author:Raymond Smith, Chunhui Gu, Dar-Shyang Lee, Huiyi Hu, Ranjith Unnikrishnan, Julian Ibarz, Sacha Arnoud, Sophia Lin category:cs.CV  published:2017-02-13 summary:We introduce the French Street Name Signs (FSNS) Dataset consisting of more than a million images of street name signs cropped from Google Street View images of France. Each image contains several views of the same street name sign. Every image has normalized, title case folded ground-truth text as it would appear on a map. We believe that the FSNS dataset is large and complex enough to train a deep network of significant complexity to solve the street name extraction problem "end-to-end" or to explore the design trade-offs between a single complex engineered network and multiple sub-networks designed and trained to solve sub-problems. We present such an "end-to-end" network/graph for Tensor Flow and its results on the FSNS dataset. version:1
arxiv-1702-03964 | The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations | http://arxiv.org/abs/1702.03964 | id:1702.03964 author:Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik van Noord, Pierre Ludmann, Duc-Duy Nguyen, Johan Bos category:cs.CL  published:2017-02-13 summary:The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory. These steps are performed using statistical models trained in a semi-supervised manner. The employed annotation models are all language-neutral. Our first results are promising. version:1
arxiv-1702-03946 | Differential Evolution for Quantum Robust Control: Algorithm, Applications and Experiments | http://arxiv.org/abs/1702.03946 | id:1702.03946 author:Daoyi Dong, Xi Xing, Hailan Ma, Chunlin Chen, Zhixin Liu, Herschel Rabitz category:quant-ph cs.NE cs.SY  published:2017-02-13 summary:Robust control design for quantum systems has been recognized as a key task in quantum information technology, molecular chemistry and atomic physics. In this paper, an improved differential evolution algorithm of msMS_DE is proposed to search robust fields for various quantum control problems. In msMS_DE, multiple samples are used for fitness evaluation and a mixed strategy is employed for mutation operation. In particular, the msMS_DE algorithm is applied to the control problem of open inhomogeneous quantum ensembles and the consensus problem of a quantum network with uncertainties. Numerical results are presented to demonstrate the excellent performance of the improved DE algorithm for these two classes of quantum robust control problems. Furthermore, msMS_DE is experimentally implemented on femtosecond laser control systems to generate good signals of two photon absorption and control fragmentation of halomethane molecules CH2BrI. Experimental results demonstrate excellent performance of msMS_DE in searching effective femtosecond laser pulses for various tasks. version:1
arxiv-1702-03920 | Cognitive Mapping and Planning for Visual Navigation | http://arxiv.org/abs/1702.03920 | id:1702.03920 author:Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik category:cs.CV cs.AI cs.LG cs.RO  published:2017-02-13 summary:We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person viewpoints and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the planner, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. Our experiments demonstrate that CMP outperforms both reactive strategies and standard memory-based architectures and performs well in novel environments. Furthermore, we show that CMP can also achieve semantically specified goals, such as 'go to a chair'. version:1
arxiv-1702-03877 | Approximate Kernel-based Conditional Independence Tests for Fast Non-Parametric Causal Discovery | http://arxiv.org/abs/1702.03877 | id:1702.03877 author:Eric V. Strobl, Kun Zhang, Shyam Visweswaran category:stat.ME stat.ML  published:2017-02-13 summary:Constraint-based causal discovery (CCD) algorithms require fast and accurate conditional independence (CI) testing. The Kernel Conditional Independence Test (KCIT) is currently one of the most popular CI tests in the non-parametric setting, but many investigators cannot use KCIT with large datasets because the test scales cubicly with sample size. We therefore devise two relaxations called the Randomized Conditional Independence Test (RCIT) and the Randomized conditional Correlation Test (RCoT) which both approximate KCIT by utilizing random Fourier features. In practice, both of the proposed tests scale linearly with sample size and return accurate p-values much faster than KCIT in the large sample size context. CCD algorithms run with RCIT or RCoT also return graphs at least as accurate as the same algorithms run with KCIT but with large reductions in run time. version:1
arxiv-1701-09175 | Skip Connections as Effective Symmetry-Breaking | http://arxiv.org/abs/1701.09175 | id:1701.09175 author:A. Emin Orhan category:cs.NE cs.LG  published:2017-01-31 summary:Skip connections made the training of very deep neural networks possible and have become an indispendable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep neural networks. We argue that skip connections help break symmetries inherent in the loss landscapes of deep networks, leading to drastically simplified landscapes. In particular, skip connections between adjacent layers in a multilayer network break the permutation symmetry of nodes in a given layer, and the recently proposed DenseNet architecture, where each layer projects skip connections to every layer above it, also breaks the rescaling symmetry of connectivity matrices between different layers. This hypothesis is supported by evidence from a toy model with binary weights and from experiments with fully-connected networks suggesting (i) that skip connections do not necessarily improve training unless they help break symmetries and (ii) that alternative ways of breaking the symmetries also lead to significant performance improvements in training deep networks, hence there is nothing special about skip connections in this respect. We find, however, that skip connections confer additional benefits over and above symmetry-breaking, such as the ability to deal effectively with the vanishing gradients problem. version:4
arxiv-1702-03865 | Next-Step Conditioned Deep Convolutional Neural Networks Improve Protein Secondary Structure Prediction | http://arxiv.org/abs/1702.03865 | id:1702.03865 author:Akosua Busia, Navdeep Jaitly category:cs.LG q-bio.BM  published:2017-02-13 summary:Recently developed deep learning techniques have significantly improved the accuracy of various speech and image recognition systems. In this paper we show how to adapt some of these techniques to create a novel chained convolutional architecture with next-step conditioning for improving performance on protein sequence prediction problems. We explore its value by demonstrating its ability to improve performance on eight-class secondary structure prediction. We first establish a state-of-the-art baseline by adapting recent advances in convolutional neural networks which were developed for vision tasks. This model achieves 70.0% per amino acid accuracy on the CB513 benchmark dataset without use of standard performance-boosting techniques such as ensembling or multitask learning. We then improve upon this state-of-the-art result using a novel chained prediction approach which frames the secondary structure prediction as a next-step prediction problem. This sequential model achieves 70.3% Q8 accuracy on CB513 with a single model; an ensemble of these models produces 71.4% Q8 accuracy on the same test set, improving upon the previous overall state of the art for the eight-class secondary structure problem. Our models are implemented using TensorFlow, an open-source machine learning software library available at TensorFlow.org; we aim to release the code for these experiments as part of the TensorFlow repository. version:1
arxiv-1702-03859 | Offline bilingual word vectors, orthogonal transformations and the inverted softmax | http://arxiv.org/abs/1702.03859 | id:1702.03859 author:Samuel L. Smith, David H. P. Turban, Steven Hamblin, Nils Y. Hammerla category:cs.CL cs.AI cs.IR  published:2017-02-13 summary:Usually bilingual word vectors are trained "online". Mikolov et al. showed they can also be found "offline", whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel "inverted softmax" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a "pseudo-dictionary" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%. version:1
arxiv-1702-03856 | Towards speech-to-text translation without speech recognition | http://arxiv.org/abs/1702.03856 | id:1702.03856 author:Sameer Bansal, Herman Kamper, Adam Lopez, Sharon Goldwater category:cs.CL  published:2017-02-13 summary:We explore the problem of translating speech to text in low-resource scenarios where neither automatic speech recognition (ASR) nor machine translation (MT) are available, but we have training data in the form of audio paired with text translations. We present the first system for this problem applied to a realistic multi-speaker dataset, the CALLHOME Spanish-English speech translation corpus. Our approach uses unsupervised term discovery (UTD) to cluster repeated patterns in the audio, creating a pseudotext, which we pair with translations to create a parallel text and train a simple bag-of-words MT model. We identify the challenges faced by the system, finding that the difficulty of cross-speaker UTD results in low recall, but that our system is still able to correctly translate some content words in test data. version:1
arxiv-1702-04265 | Design of a Time Delay Reservoir Using Stochastic Logic: A Feasibility Study | http://arxiv.org/abs/1702.04265 | id:1702.04265 author:Cory Merkel category:stat.ML cs.ET  published:2017-02-13 summary:This paper presents a stochastic logic time delay reservoir design. The reservoir is analyzed using a number of metrics, such as kernel quality, generalization rank, performance on simple benchmarks, and is also compared to a deterministic design. A novel re-seeding method is introduced to reduce the adverse effects of stochastic noise, which may also be implemented in other stochastic logic reservoir computing designs, such as echo state networks. Benchmark results indicate that the proposed design performs well on noise-tolerant classification problems, but more work needs to be done to improve the stochastic logic time delay reservoir's robustness for regression problems. version:1
arxiv-1702-03833 | Estimation of the volume of the left ventricle from MRI images using deep neural networks | http://arxiv.org/abs/1702.03833 | id:1702.03833 author:Fangzhou Liao, Xi Chen, Xiaolin Hu, Sen Song category:cs.CV  published:2017-02-13 summary:Segmenting human left ventricle (LV) in magnetic resonance imaging (MRI) images and calculating its volume are important for diagnosing cardiac diseases. In 2016, Kaggle organized a competition to estimate the volume of LV from MRI images. The dataset consisted of a large number of cases, but only provided systole and diastole volumes as labels. We designed a system based on neural networks to solve this problem. It began with a detector combined with a neural network classifier for detecting regions of interest (ROIs) containing LV chambers. Then a deep neural network named hypercolumns fully convolutional network was used to segment LV in ROIs. The 2D segmentation results were integrated across different images to estimate the volume. With ground-truth volume labels, this model was trained end-to-end. To improve the result, an additional dataset with only segmentation label was used. The model was trained alternately on these two datasets with different types of teaching signals. We also proposed a variance estimation method for the final prediction. Our algorithm ranked the 4th on the test set in this competition. version:1
arxiv-1702-03791 | DNN Filter Bank Cepstral Coefficients for Spoofing Detection | http://arxiv.org/abs/1702.03791 | id:1702.03791 author:Hong Yu, Zheng-Hua Tan, Zhanyu Ma, Jun Guo category:cs.SD cs.CR cs.LG  published:2017-02-13 summary:With the development of speech synthesis techniques, automatic speaker verification systems face the serious challenge of spoofing attack. In order to improve the reliability of speaker verification systems, we develop a new filter bank based cepstral feature, deep neural network filter bank cepstral coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The deep neural network filter bank is automatically generated by training a filter bank neural network (FBNN) using natural and synthetic speech. By adding restrictions on the training rules, the learned weight matrix of FBNN is band-limited and sorted by frequency, similar to the normal filter bank. Unlike the manually designed filter bank, the learned filter bank has different filter shapes in different channels, which can capture the differences between natural and synthetic speech more effectively. The experimental results on the ASVspoof {2015} database show that the Gaussian mixture model maximum-likelihood (GMM-ML) classifier trained by the new feature performs better than the state-of-the-art linear frequency cepstral coefficients (LFCC) based classifier, especially on detecting unknown attacks. version:1
arxiv-1702-03767 | Is Big Data Sufficient for a Reliable Detection of Non-Technical Losses? | http://arxiv.org/abs/1702.03767 | id:1702.03767 author:Patrick Glauner, Angelo Migliosi, Jorge Meira, Eric Aislan Antonelo, Petko Valtchev, Radu State, Franck Bettinger category:cs.LG cs.AI  published:2017-02-13 summary:Non-technical losses (NTL) occur during the distribution of electricity in power grids and include, but are not limited to, electricity theft and faulty meters. In emerging countries, they may range up to 40% of the total electricity distributed. In order to detect NTLs, machine learning methods are used that learn irregular consumption patterns from customer data and inspection results. The Big Data paradigm followed in modern machine learning reflects the desire of deriving better conclusions from simply analyzing more data, without the necessity of looking at theory and models. However, the sample of inspected customers may be biased, i.e. it does not represent the population of all customers. As a consequence, machine learning models trained on these inspection results are biased as well and therefore lead to unreliable predictions of whether customers cause NTL or not. In machine learning, this issue is called covariate shift and has not been addressed in the literature on NTL detection yet. In this work, we present a novel framework for quantifying and visualizing covariate shift. We apply it to a commercial data set from Brazil that consists of 3.6M customers and 820K inspection results. We show that some features have a stronger covariate shift than others, making predictions less reliable. In particular, previous inspections were focused on certain neighborhoods or customer classes and that they were not sufficiently spread among the population of customers. This framework is about to be deployed in a commercial product for NTL detection. version:1
arxiv-1702-03713 | Feature Space Modeling Through Surrogate Illumination | http://arxiv.org/abs/1702.03713 | id:1702.03713 author:Adam Gaier, Alexander Asteroth, Jean-Baptiste Mouret category:cs.NE stat.ML  published:2017-02-13 summary:The MAP-Elites algorithm produces a set of high-performing solutions that vary according to features defined by the user. This technique has the potential to be a powerful tool for design space exploration, but is limited by the need for numerous evaluations. The Surrogate-Assisted Illumination algorithm (SAIL), introduced here, integrates approximative models and intelligent sampling of the objective function to minimize the number of evaluations required by MAP-Elites. The ability of SAIL to efficiently produce both accurate models and diverse high performing solutions is illustrated on a 2D airfoil design problem. The search space is divided into bins, each holding a design with a different combination of features. In each bin SAIL produces a better performing solution than MAP-Elites, and requires several orders of magnitude fewer evaluations. The CMA-ES algorithm was used to produce an optimal design in each bin: with the same number of evaluations required by CMA-ES to find a near-optimal solution in a single bin, SAIL finds solutions of similar quality in every bin. version:1
arxiv-1702-03706 | Multitask Learning with Deep Neural Networks for Community Question Answering | http://arxiv.org/abs/1702.03706 | id:1702.03706 author:Daniele Bonadiman, Antonio Uva, Alessandro Moschitti category:cs.CL  published:2017-02-13 summary:In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it. version:1
arxiv-1702-03690 | An Efficient Decomposition Framework for Discriminative Segmentation with Supermodular Losses | http://arxiv.org/abs/1702.03690 | id:1702.03690 author:Jiaqian Yu, Matthew B. Blaschko category:cs.CV  published:2017-02-13 summary:Several supermodular losses have been shown to improve the perceptual quality of image segmentation in a discriminative framework such as a structured output support vector machine (SVM). These loss functions do not necessarily have the same structure as the one used by the segmentation inference algorithm, and in general, we may have to resort to generic submodular minimization algorithms for loss augmented inference. Although these come with polynomial time guarantees, they are not practical to apply to image scale data. Many supermodular losses come with strong optimization guarantees, but are not readily incorporated in a loss augmented graph cuts procedure. This motivates our strategy of employing the alternating direction method of multipliers (ADMM) decomposition for loss augmented inference. In doing so, we create a new API for the structured SVM that separates the maximum a posteriori (MAP) inference of the model from the loss augmentation during training. In this way, we gain computational efficiency, making new choices of loss functions practical for the first time, while simultaneously making the inference algorithm employed during training closer to the test time procedure. We show improvement both in accuracy and computational performance on the Microsoft Research Grabcut database and a brain structure segmentation task, empirically validating the use of several supermodular loss functions during training, and the improved computational properties of the proposed ADMM approach over the Fujishige-Wolfe minimum norm point algorithm. version:1
arxiv-1702-03684 | Unsupervised temporal context learning using convolutional neural networks for laparoscopic workflow analysis | http://arxiv.org/abs/1702.03684 | id:1702.03684 author:Sebastian Bodenstedt, Martin Wagner, Darko Katić, Patrick Mietkowski, Benjamin Mayer, Hannes Kenngott, Beat Müller-Stich, Rüdiger Dillmann, Stefanie Speidel category:cs.CV  published:2017-02-13 summary:Computer-assisted surgery (CAS) aims to provide the surgeon with the right type of assistance at the right moment. Such assistance systems are especially relevant in laparoscopic surgery, where CAS can alleviate some of the drawbacks that surgeons incur. For many assistance functions, e.g. displaying the location of a tumor at the appropriate time or suggesting what instruments to prepare next, analyzing the surgical workflow is a prerequisite. Since laparoscopic interventions are performed via endoscope, the video signal is an obvious sensor modality to rely on for workflow analysis. Image-based workflow analysis tasks in laparoscopy, such as phase recognition, skill assessment, video indexing or automatic annotation, require a temporal distinction between video frames. Generally computer vision based methods that generalize from previously seen data are used. For training such methods, large amounts of annotated data are necessary. Annotating surgical data requires expert knowledge, therefore collecting a sufficient amount of data is difficult, time-consuming and not always feasible. In this paper, we address this problem by presenting an unsupervised method for training a convolutional neural network (CNN) to differentiate between laparoscopic video frames on a temporal basis. We extract video frames at regular intervals from 324 unlabeled laparoscopic interventions, resulting in a dataset of approximately 2.2 million images. From this dataset, we extract image pairs from the same video and train a CNN to determine their temporal order. To solve this problem, the CNN has to extract features that are relevant for comprehending laparoscopic workflow. Furthermore, we demonstrate that such a CNN can be adapted for surgical workflow segmentation. We performed image-based workflow segmentation on a publicly available dataset of 7 cholecystectomies and 9 colorectal interventions. version:1
arxiv-1702-03654 | A Morphology-aware Network for Morphological Disambiguation | http://arxiv.org/abs/1702.03654 | id:1702.03654 author:Eray Yildiz, Caglar Tirkaz, H. Bahadir Sahin, Mustafa Tolga Eren, Ozan Sonmez category:cs.CL  published:2017-02-13 summary:Agglutinative languages such as Turkish, Finnish and Hungarian require morphological disambiguation before further processing due to the complex morphology of words. A morphological disambiguator is used to select the correct morphological analysis of a word. Morphological disambiguation is important because it generally is one of the first steps of natural language processing and its performance affects subsequent analyses. In this paper, we propose a system that uses deep learning techniques for morphological disambiguation. Many of the state-of-the-art results in computer vision, speech recognition and natural language processing have been obtained through deep learning models. However, applying deep learning techniques to morphologically rich languages is not well studied. In this work, while we focus on Turkish morphological disambiguation we also present results for French and German in order to show that the proposed architecture achieves high accuracy with no language-specific feature engineering or additional resource. In the experiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation accuracy among the ambiguous words for Turkish, German and French respectively. version:1
arxiv-1702-03644 | Coresets for Kernel Regression | http://arxiv.org/abs/1702.03644 | id:1702.03644 author:Yan Zheng, Jeff M. Phillips category:cs.LG cs.DS  published:2017-02-13 summary:Kernel regression is an essential and ubiquitous tool for non-parametric data analysis, particularly popular among time series and spatial data. However, the central operation which is performed many times, evaluating a kernel on the data set, takes linear time. This is impractical for modern large data sets. In this paper we describe coresets for kernel regression: compressed data sets which can be used as proxy for the original data and have provably bounded worst case error. The size of the coresets are independent of the raw number of data points, rather they only depend on the error guarantee, and in some cases the size of domain and amount of smoothing. We evaluate our methods on very large time series and spatial data, and demonstrate that they incur negligible error, can be constructed extremely efficiently, and allow for great computational gains. version:1
arxiv-1702-03614 | Multitask diffusion adaptation over networks with common latent representations | http://arxiv.org/abs/1702.03614 | id:1702.03614 author:Jie Chen, Cédric Richard, Ali H. Sayed category:cs.MA stat.ML  published:2017-02-13 summary:Online learning with streaming data in a distributed and collaborative manner can be useful in a wide range of applications. This topic has been receiving considerable attention in recent years with emphasis on both single-task and multitask scenarios. In single-task adaptation, agents cooperate to track an objective of common interest, while in multitask adaptation agents track multiple objectives simultaneously. Regularization is one useful technique to promote and exploit similarity among tasks in the latter scenario. This work examines an alternative way to model relations among tasks by assuming that they all share a common latent feature representation. As a result, a new multitask learning formulation is presented and algorithms are developed for its solution in a distributed online manner. We present a unified framework to analyze the mean-square-error performance of the adaptive strategies, and conduct simulations to illustrate the theoretical findings and potential applications. version:1
arxiv-1702-03613 | A Multi-model Combination Approach for Probabilistic Wind Power Forecasting | http://arxiv.org/abs/1702.03613 | id:1702.03613 author:You Lin, Ming Yang, Can Wan, Jianhui Wang, Yonghua Song category:cs.LG stat.AP  published:2017-02-13 summary:Short-term probabilistic wind power forecasting can provide critical quantified uncertainty information of wind generation for power system operation and control. As the complicated characteristics of wind power prediction error, it would be difficult to develop a universal forecasting model dominating over other alternative models. Therefore, a novel multi-model combination (MMC) approach for short-term probabilistic wind generation forecasting is proposed in this paper to exploit the advantages of different forecasting models. The proposed approach can combine different forecasting models those provide different kinds of probability density functions to improve the probabilistic forecast accuracy. Three probabilistic forecasting models based on the sparse Bayesian learning, kernel density estimation and beta distribution fitting are used to form the combined model. The parameters of the MMC model are solved based on Bayesian framework. Numerical tests illustrate the effectiveness of the proposed MMC approach. version:1
arxiv-1701-07926 | Boosting hazard regression with time-varying covariates | http://arxiv.org/abs/1701.07926 | id:1701.07926 author:Donald K. K. Lee, Ningyuan Chen category:stat.ML 62N02  62G05  published:2017-01-27 summary:Consider a left-truncated right-censored survival process whose evolution depends on time-varying covariates. Given functional data samples from the process, we propose a gradient boosting procedure for estimating its log-intensity function in a flexible manner to capture time-covariate interactions. The estimator is shown to be consistent if the model is correctly specified. Alternatively an oracle inequality can be demonstrated for tree-based models. We use the procedure to shed new light on a question from the operations literature concerning the effect of workload on service rates in an emergency department. To avoid overfitting, boosting employs several regularization devices. One of them is step-size restriction, but the rationale for this is somewhat mysterious from the viewpoint of consistency: In theoretical treatments of classification and regression problems, unrestricted greedy step-sizes appear to suffice. Given that the partial log-likelihood functional for hazard regression has unbounded curvature, our study suggests that step-size restriction might be a mechanism for preventing the curvature of the risk from derailing convergence. version:2
arxiv-1702-03605 | Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection | http://arxiv.org/abs/1702.03605 | id:1702.03605 author:Lijie Chen, Jian Li, Mingda Qiao category:cs.LG cs.DS stat.ML  published:2017-02-13 summary:In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each associated with an unknown reward distribution. We are required to identify the $k$ arms with the largest means by taking as few samples as possible. In this paper, we make progress towards a complete characterization of the instance-wise sample complexity bounds for the Best-$k$-Arm problem. On the lower bound side, we obtain a novel complexity term to measure the sample complexity that every Best-$k$-Arm instance requires. This is derived by an interesting and nontrivial reduction from the Best-$1$-Arm problem. We also provide an elimination-based algorithm that matches the instance-wise lower bound within doubly-logarithmic factors. The sample complexity of our algorithm strictly dominates the state-of-the-art for Best-$k$-Arm (module constant factors). version:1
arxiv-1702-03600 | Underwater Optical Image Processing: A Comprehensive Review | http://arxiv.org/abs/1702.03600 | id:1702.03600 author:Huimin Lu, Yujie Li, Yudong Zhang, Min Chen, Seiichi Serikawa, Hyoungseop Kim category:cs.CV 78  published:2017-02-13 summary:Underwater cameras are widely used to observe the sea floor. They are usually included in autonomous underwater vehicles, unmanned underwater vehicles, and in situ ocean sensor networks. Despite being an important sensor for monitoring underwater scenes, there exist many issues with recent underwater camera sensors. Because of lights transportation characteristics in water and the biological activity at the sea floor, the acquired underwater images often suffer from scatters and large amounts of noise. Over the last five years, many methods have been proposed to overcome traditional underwater imaging problems. This paper aims to review the state-of-the-art techniques in underwater image processing by highlighting the contributions and challenges presented in over 40 papers. We present an overview of various underwater image processing approaches, such as underwater image descattering, underwater image color restoration, and underwater image quality assessments. Finally, we summarize the future trends and challenges in designing and processing underwater imaging sensors. version:1
arxiv-1702-03537 | Supervised Learning for Controlled Dynamical System Learning | http://arxiv.org/abs/1702.03537 | id:1702.03537 author:Ahmed Hefny, Carlton Downey, Geoffrey J. Gordon category:stat.ML  published:2017-02-12 summary:We develop a framework for reducing the identification of controlled dynamical systems to solving a small set of supervised learning problems. We do this by adapting the two-stage regression framework proposed in (Hefny et. al. 2015) to controlled systems, which are more subtle than uncontrolled systems since they require a state representation that tolerates changes in the action policy. We then use the proposed framework to develop a non-parametric controlled system identification method that approximates the Hilbert-Space Embedding of a PSR (HSE-PSR) using random Fourier features, resulting in significant gains in learning speed. We also propose an iterative procedure for improving model parameters given an initial estimate. We report promising results on multiple experiments. version:1
arxiv-1702-04638 | A Spacetime Approach to Generalized Cognitive Reasoning in Multi-scale Learning | http://arxiv.org/abs/1702.04638 | id:1702.04638 author:Mark Burgess category:cs.AI cs.LG  published:2017-02-12 summary:In modern machine learning, pattern recognition replaces realtime semantic reasoning. The mapping from input to output is learned with fixed semantics by training outcomes deliberately. This is an expensive and static approach which depends heavily on the availability of a very particular kind of prior raining data to make inferences in a single step. Conventional semantic network approaches, on the other hand, base multi-step reasoning on modal logics and handcrafted ontologies, which are {\em ad hoc}, expensive to construct, and fragile to inconsistency. Both approaches may be enhanced by a hybrid approach, which completely separates reasoning from pattern recognition. In this report, a quasi-linguistic approach to knowledge representation is discussed, motivated by spacetime structure. Tokenized patterns from diverse sources are integrated to build a lightly constrained and approximately scale-free network. This is then be parsed with very simple recursive algorithms to generate `brainstorming' sets of reasoned knowledge. version:1
arxiv-1702-03525 | Learning to Parse and Translate Improves Neural Machine Translation | http://arxiv.org/abs/1702.03525 | id:1702.03525 author:Akiko Eriguchi, Yoshimasa Tsuruoka, Kyunghyun Cho category:cs.CL  published:2017-02-12 summary:There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RG. version:1
arxiv-1702-03515 | Sparse Representation based Multi-sensor Image Fusion: A Review | http://arxiv.org/abs/1702.03515 | id:1702.03515 author:Qiang Zhang, Yi Liu, Rick S. Blum, Jungong Han, Dacheng Tao category:cs.CV  published:2017-02-12 summary:As a result of several successful applications in computer vision and image processing, sparse representation (SR) has attracted significant attention in multi-sensor image fusion. Unlike the traditional multiscale transforms (MSTs) that presume the basis functions, SR learns an over-complete dictionary from a set of training images for image fusion, and it achieves more stable and meaningful representations of the source images. By doing so, the SR-based fusion methods generally outperform the traditional MST-based image fusion methods in both subjective and objective tests. In addition, they are less susceptible to mis-registration among the source images, thus facilitating the practical applications. This survey paper proposes a systematic review of the SR-based multi-sensor image fusion literature, highlighting the pros and cons of each category of approaches. Specifically, we start by performing a theoretical investigation of the entire system from three key algorithmic aspects, (1) sparse representation models; (2) dictionary learning methods; and (3) activity levels and fusion rules. Subsequently, we show how the existing works address these scientific problems and design the appropriate fusion rules for each application, such as multi-focus image fusion and multi-modality (e.g., infrared and visible) image fusion. At last, we carry out some experiments to evaluate the impact of these three algorithmic components on the fusion performance when dealing with different applications. This article is expected to serve as a tutorial and source of reference for researchers preparing to enter the field or who desire to employ the sparse representation theory in other fields. version:1
arxiv-1702-03500 | Concept Drift Adaptation by Exploiting Historical Knowledge | http://arxiv.org/abs/1702.03500 | id:1702.03500 author:Yu Sun, Ke Tang, Zexuan Zhu, Xin Yao category:cs.LG  published:2017-02-12 summary:Incremental learning with concept drift has often been tackled by ensemble methods, where models built in the past can be re-trained to attain new models for the current data. Two design questions need to be addressed in developing ensemble methods for incremental learning with concept drift, i.e., which historical (i.e., previously trained) models should be preserved and how to utilize them. A novel ensemble learning method, namely Diversity and Transfer based Ensemble Learning (DTEL), is proposed in this paper. Given newly arrived data, DTEL uses each preserved historical model as an initial model and further trains it with the new data via transfer learning. Furthermore, DTEL preserves a diverse set of historical models, rather than a set of historical models that are merely accurate in terms of classification accuracy. Empirical studies on 15 synthetic data streams and 4 real-world data streams (all with concept drifts) demonstrate that DTEL can handle concept drift more effectively than 4 other state-of-the-art methods. version:1
arxiv-1702-03470 | Vector Embedding of Wikipedia Concepts and Entities | http://arxiv.org/abs/1702.03470 | id:1702.03470 author:Ehsan Sherkat, Evangelos Milios category:cs.CL  published:2017-02-12 summary:Using deep learning for different machine learning tasks such as image classification and word embedding has recently gained many attentions. Its appealing performance reported across specific Natural Language Processing (NLP) tasks in comparison with other approaches is the reason for its popularity. Word embedding is the task of mapping words or phrases to a low dimensional numerical vector. In this paper, we use deep learning to embed Wikipedia Concepts and Entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many English Entities, Phrases, and Concepts. Each Wikipedia page is considered as a concept. Some concepts correspond to entities, such as a person's name, an organization or a place. Contrary to word embedding, Wikipedia Concepts Embedding is not ambiguous, so there are different vectors for concepts with similar surface form but different mentions. We proposed several approaches and evaluated their performance based on Concept Analogy and Concept Similarity tasks. The results show that proposed approaches have the performance comparable and in some cases even higher than the state-of-the-art methods. version:1
arxiv-1702-03465 | Enabling Robots to Communicate their Objectives | http://arxiv.org/abs/1702.03465 | id:1702.03465 author:Sandy H. Huang, David Held, Pieter Abbeel, Anca D. Dragan category:cs.RO cs.LG  published:2017-02-11 summary:Our ultimate goal is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. This behavior is often a direct result of the robot's underlying objective function. Our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, and then selecting those behaviors that are maximally informative. The problem of computing a posterior over objectives from observed behavior is known as Inverse Reinforcement Learning (IRL), and has been applied to robots learning human objectives. We consider the problem where the roles of human and robot are swapped. Our main contribution is to recognize that unlike robots, humans will not be \emph{exact} in their IRL inference. We thus introduce two factors to define candidate approximate-inference models for human learning in this setting, and analyze them in a user study in the autonomous driving domain. We show that certain approximate-inference models lead to the robot generating example behaviors that better enable users to anticipate what the robot will do in test situations. Our results also suggest, however, that additional research is needed in modeling how humans extrapolate from examples of robot behavior. version:1
arxiv-1702-03464 | Gromov-Hausdorff limit of Wasserstein spaces on point clouds | http://arxiv.org/abs/1702.03464 | id:1702.03464 author:Nicolas Garcia Trillos category:math.MG math.AP math.PR math.ST stat.ML stat.TH  published:2017-02-11 summary:We consider a point cloud $X_n := \{ x_1, \dots, x_n \}$ uniformly distributed on the flat torus $\mathbb{T}^d : = \mathbb{R}^d / \mathbb{Z}^d $, and construct a geometric graph on the cloud by connecting points that are within distance $\epsilon$ of each other. We let $\mathcal{P}(X_n)$ be the space of probability measures on $X_n$ and endow it with a discrete Wasserstein distance $W_n$ as defined by Maas. We show that as long as $\epsilon= \epsilon_n$ decays towards zero slower than an explicit rate depending on the level of uniformity of $X_n$, then the space $(\mathcal{P}(X_n), W_n)$ converges in the Gromov-Hausdorff sense towards the space of probability measures on $\mathbb{T}^d$ endowed with the Wasserstein distance. version:1
arxiv-1702-03447 | A Collective, Probabilistic Approach to Schema Mapping: Appendix | http://arxiv.org/abs/1702.03447 | id:1702.03447 author:Angelika Kimmig, Alex Memory, Renee J. Miller, Lise Getoor category:cs.DB cs.LG  published:2017-02-11 summary:In this appendix we provide additional supplementary material to "A Collective, Probabilistic Approach to Schema Mapping." We include an additional extended example, supplementary experiment details, and proof for the complexity result stated in the main paper. version:1
arxiv-1702-03446 | On the Global-Local Dichotomy in Sparsity Modeling | http://arxiv.org/abs/1702.03446 | id:1702.03446 author:Dmitry Batenkov, Yaniv Romano, Michael Elad category:cs.IT math.IT stat.ML  published:2017-02-11 summary:The traditional sparse modeling approach, when applied to inverse problems with large data such as images, essentially assumes a sparse model for small overlapping data patches. While producing state-of-the-art results, this methodology is suboptimal, as it does not attempt to model the entire global signal in any meaningful way - a nontrivial task by itself. In this paper we propose a way to bridge this theoretical gap by constructing a global model from the bottom up. Given local sparsity assumptions in a dictionary, we show that the global signal representation must satisfy a constrained underdetermined system of linear equations, which can be solved efficiently by modern optimization methods such as Alternating Direction Method of Multipliers (ADMM). We investigate conditions for unique and stable recovery, and provide numerical evidence corroborating the theory. version:1
arxiv-1702-03443 | Group Scissor: Scaling Neuromorphic Computing Design to Big Neural Networks | http://arxiv.org/abs/1702.03443 | id:1702.03443 author:Yandan Wang, Wei Wen, Beiye Liu, Donald Chiarulli, Hai Li category:cs.NE cs.AI C.1.3  I.2.6  I.5.1  published:2017-02-11 summary:Synapse crossbar is an elementary structure in Neuromorphic Computing Systems (NCS). However, the limited size of crossbars and heavy routing congestion impedes the NCS implementations of big neural networks. In this paper, we propose a two-step framework (namely, \textit{group scissor}) to scale NCS designs to big neural networks. The first step is \textit{rank clipping}, which integrates low-rank approximation into the training to reduce total crossbar area. The second step is \textit{group connection deletion}, which structurally prunes connections to reduce routing congestion between crossbars. Tested on convolutional neural networks of \textit{LeNet} on MNIST database and \textit{ConvNet} on CIFAR-10 database, our experiments show significant reduction of crossbar area and routing area in NCS designs. Without accuracy loss, rank clipping reduces total crossbar area to 13.62\% and 51.81\% in the NCS designs of \textit{LeNet} and \textit{ConvNet}, respectively. Following rank clipping, group connection deletion further reduces the routing area of \textit{LeNet} and \textit{ConvNet} to 8.1\% and 52.06\%, respectively. version:1
arxiv-1702-03435 | Distributed Mapping with Privacy and Communication Constraints: Lightweight Algorithms and Object-based Models | http://arxiv.org/abs/1702.03435 | id:1702.03435 author:Siddharth Choudhary, Luca Carlone, Carlos Nieto, John Rogers, Henrik I. Christensen, Frank Dellaert category:cs.RO cs.CV  published:2017-02-11 summary:We consider the following problem: a team of robots is deployed in an unknown environment and it has to collaboratively build a map of the area without a reliable infrastructure for communication. The backbone for modern mapping techniques is pose graph optimization, which estimates the trajectory of the robots, from which the map can be easily built. The first contribution of this paper is a set of distributed algorithms for pose graph optimization: rather than sending all sensor data to a remote sensor fusion server, the robots exchange very partial and noisy information to reach an agreement on the pose graph configuration. Our approach can be considered as a distributed implementation of the two-stage approach of Carlone et al., where we use the Successive Over-Relaxation (SOR) and the Jacobi Over-Relaxation (JOR) as workhorses to split the computation among the robots. As a second contribution, we extend %and demonstrate the applicability of the proposed distributed algorithms to work with object-based map models. The use of object-based models avoids the exchange of raw sensor measurements (e.g., point clouds) further reducing the communication burden. Our third contribution is an extensive experimental evaluation of the proposed techniques, including tests in realistic Gazebo simulations and field experiments in a military test facility. Abundant experimental evidence suggests that one of the proposed algorithms (the Distributed Gauss-Seidel method or DGS) has excellent performance. The DGS requires minimal information exchange, has an anytime flavor, scales well to large teams, is robust to noise, and is easy to implement. Our field tests show that the combined use of our distributed algorithms and object-based models reduces the communication requirements by several orders of magnitude and enables distributed mapping with large teams of robots in real-world problems. version:1
arxiv-1702-03431 | Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation | http://arxiv.org/abs/1702.03431 | id:1702.03431 author:Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao category:cs.CV  published:2017-02-11 summary:State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks. version:1
arxiv-1702-06397 | Fast Resampling of 3D Point Clouds via Graphs | http://arxiv.org/abs/1702.06397 | id:1702.06397 author:Siheng Chen, Dong Tian, Chen Feng, Anthony Vetro, Jelena Kovačević category:cs.CV  published:2017-02-11 summary:To reduce cost in storing, processing and visualizing a large-scale point cloud, we consider a randomized resampling strategy to select a representative subset of points while preserving application-dependent features. The proposed strategy is based on graphs, which can represent underlying surfaces and lend themselves well to efficient computation. We use a general feature-extraction operator to represent application-dependent features and propose a general reconstruction error to evaluate the quality of resampling. We obtain a general form of optimal resampling distribution by minimizing the reconstruction error. The proposed optimal resampling distribution is guaranteed to be shift, rotation and scale-invariant in the 3D space. We next specify the feature-extraction operator to be a graph filter and study specific resampling strategies based on all-pass, low-pass, high-pass graph filtering and graph filter banks. We finally apply the proposed methods to three applications: large-scale visualization, accurate registration and robust shape modeling. The empirical performance validates the effectiveness and efficiency of the proposed resampling methods. version:1
arxiv-1702-03410 | ArtGAN: Artwork Synthesis with Conditional Categorial GANs | http://arxiv.org/abs/1702.03410 | id:1702.03410 author:Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, Kiyoshi Tanaka category:cs.CV  published:2017-02-11 summary:This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ARTGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ARTGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10. version:1
arxiv-1702-03407 | Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth | http://arxiv.org/abs/1702.03407 | id:1702.03407 author:Vanya V. Valindria, Ioannis Lavdas, Wenjia Bai, Konstantinos Kamnitsas, Eric O. Aboagye, Andrea G. Rockall, Daniel Rueckert, Ben Glocker category:cs.CV  published:2017-02-11 summary:When integrating computational tools such as automatic segmentation into clinical practice, it is of utmost importance to be able to assess the level of accuracy on new data, and in particular, to detect when an automatic method fails. However, this is difficult to achieve due to absence of ground truth. Segmentation accuracy on clinical data might be different from what is found through cross-validation because validation data is often used during incremental method development, which can lead to overfitting and unrealistic performance expectations. Before deployment, performance is quantified using different metrics, for which the predicted segmentation is compared to a reference segmentation, often obtained manually by an expert. But little is known about the real performance after deployment when a reference is unavailable. In this paper, we introduce the concept of reverse classification accuracy (RCA) as a framework for predicting the performance of a segmentation method on new data. In RCA we take the predicted segmentation from a new image to train a reverse classifier which is evaluated on a set of reference images with available ground truth. The hypothesis is that if the predicted segmentation is of good quality, then the reverse classifier will perform well on at least some of the reference images. We validate our approach on multi-organ segmentation with different classifiers and segmentation methods. Our results indicate that it is indeed possible to predict the quality of individual segmentations, in the absence of ground truth. Thus, RCA is ideal for integration into automatic processing pipelines in clinical routine and as part of large-scale image analysis studies. version:1
arxiv-1702-03402 | Parallel Long Short-Term Memory for Multi-stream Classification | http://arxiv.org/abs/1702.03402 | id:1702.03402 author:Mohamed Bouaziz, Mohamed Morchid, Richard Dufour, Georges Linarès, Renato De Mori category:cs.LG cs.CL  published:2017-02-11 summary:Recently, machine learning methods have provided a broad spectrum of original and efficient algorithms based on Deep Neural Networks (DNN) to automatically predict an outcome with respect to a sequence of inputs. Recurrent hidden cells allow these DNN-based models to manage long-term dependencies such as Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these RNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM) directions. But most of the information available nowadays is from multistreams or multimedia documents, and require RNNs to process these information synchronously during the training. This paper presents an original LSTM-based architecture, named Parallel LSTM (PLSTM), that carries out multiple parallel synchronized input sequences in order to predict a common output. The proposed PLSTM method could be used for parallel sequence classification purposes. The PLSTM approach is evaluated on an automatic telecast genre sequences classification task and compared with different state-of-the-art architectures. Results show that the proposed PLSTM method outperforms the baseline n-gram models as well as the state-of-the-art LSTM approach. version:1
arxiv-1702-00288 | Low-Dose CT with a Residual Encoder-Decoder Convolutional Neural Network (RED-CNN) | http://arxiv.org/abs/1702.00288 | id:1702.00288 author:Hu Chen, Yi Zhang, Mannudeep K. Kalra, Feng Lin, Peixi Liao, Jiliu Zhou, Ge Wang category:physics.med-ph cs.NE  published:2017-02-01 summary:Given the potential X-ray radiation risk to the patient, low-dose CT has attracted a considerable interest in the medical imaging field. The current main stream low-dose CT methods include vendor-specific sinogram domain filtration and iterative reconstruction, but they need to access original raw data whose formats are not transparent to most users. Due to the difficulty of modeling the statistical characteristics in the image domain, the existing methods for directly processing reconstructed images cannot eliminate image noise very well while keeping structural details. Inspired by the idea of deep learning, here we combine the autoencoder, the deconvolution network, and shortcut connections into the residual encoder-decoder convolutional neural network (RED-CNN) for low-dose CT imaging. After patch-based training, the proposed RED-CNN achieves a competitive performance relative to the-state-of-art methods in both simulated and clinical cases. Especially, our method has been favorably evaluated in terms of noise suppression, structural preservation and lesion detection. version:2
arxiv-1702-03389 | Whale swarm algorithm for function optimization | http://arxiv.org/abs/1702.03389 | id:1702.03389 author:Bing Zeng, Liang Gao, Xinyu Li category:cs.NE  published:2017-02-11 summary:Increasing nature-inspired metaheuristic algorithms are applied to solving the real-world optimization problems, as they have some advantages over the classical methods of numerical optimization. This paper has proposed a new nature-inspired metaheuristic called Whale Swarm Algorithm for function optimization, which is inspired by the whales behavior of communicating with each other via ultrasound for hunting. The proposed Whale Swarm Algorithm has been compared with several popular metaheuristic algorithms on comprehensive performance metrics. According to the experimental results, Whale Swarm Algorithm has a quite competitive performance when compared with other algorithms. version:1
arxiv-1702-03380 | Training Deep Neural Networks via Optimization Over Graphs | http://arxiv.org/abs/1702.03380 | id:1702.03380 author:Guoqiang Zhang, W. Bastiaan Kleijn category:cs.LG cs.DC  published:2017-02-11 summary:In this work, we propose to train a deep neural network by distributed optimization over a graph. Two nonlinear functions are considered: the rectified linear unit (ReLU) and a linear unit with both lower and upper cutoffs (DCutLU). The problem reformulation over a graph is realized by explicitly representing ReLU or DCutLU using a set of slack variables. We then apply the alternating direction method of multipliers (ADMM) to update the weights of the network layerwise by solving subproblems of the reformulated problem. Empirical results suggest that by proper parameter selection, the ADMM- based method converges considerably faster than gradient descent method. version:1
arxiv-1702-04615 | Automated Identification of Drug-Drug Interactions in Pediatric Congestive Heart Failure Patients | http://arxiv.org/abs/1702.04615 | id:1702.04615 author:Daniel Miller category:cs.CL  published:2017-02-11 summary:Congestive Heart Failure, or CHF, is a serious medical condition that can result in fluid buildup in the body as a result of a weak heart. When the heart can't pump enough blood to efficiently deliver nutrients and oxygen to the body, kidney function may be impaired, resulting in fluid retention. CHF patients require a broad drug regimen to maintain the delicate system balance, particularly between their heart and kidneys. These drugs include ACE inhibitors and Beta Blockers to control blood pressure, anticoagulants to prevent blood clots, and diuretics to reduce fluid overload. Many of these drugs may interact, and potential effects of these interactions must be weighed against their benefits. For this project, we consider a set of 44 drugs identified as specifically relevant for treating CHF by pediatric cardiologists at Lucile Packard Children's Hospital. This list was generated as part of our current work at the LPCH Heart Center. The goal of this project is to identify and evaluate potentially harmful drug-drug interactions (DDIs) within pediatric patients with Congestive Heart Failure. This identification will be done autonomously, so that it may continuously update by evaluating newly published literature. version:1
arxiv-1702-02211 | Fixing the Infix: Unsupervised Discovery of Root-and-Pattern Morphology | http://arxiv.org/abs/1702.02211 | id:1702.02211 author:Tarek Sakakini, Suma Bhat, Pramod Viswanath category:cs.CL  published:2017-02-07 summary:We present an unsupervised and language-agnostic method for learning root-and-pattern morphology in Semitic languages. This form of morphology, abundant in Semitic languages, has not been handled in prior unsupervised approaches. We harness the syntactico-semantic information in distributed word representations to solve the long standing problem of root-and-pattern discovery in Semitic languages. Moreover, we construct an unsupervised root extractor based on the learned rules. We prove the validity of learned rules across Arabic, Hebrew, and Amharic, alongside showing that our root extractor compares favorably with a widely used, carefully engineered root extractor: ISRI. version:2
arxiv-1702-02212 | MORSE: Semantic-ally Drive-n MORpheme SEgment-er | http://arxiv.org/abs/1702.02212 | id:1702.02212 author:Tarek Sakakini, Suma Bhat, Pramod Viswanath category:cs.CL  published:2017-02-07 summary:We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results. version:2
arxiv-1702-03349 | Enhanced Local Binary Patterns for Automatic Face Recognition | http://arxiv.org/abs/1702.03349 | id:1702.03349 author:Pavel Král, Antonín Vrba category:cs.CV  published:2017-02-10 summary:This paper presents a novel automatic face recognition approach based on local binary patterns (LBP). LBP descriptor considers a local neighbourhood of a pixel to compute the features. This method is not very robust to handle image noise, variances and different illumination conditions. In this paper, we address these issues and extend the original LBP operator by considering more pixels and different neighbourhoods to compute the feature vector. The proposed method is evaluated on two benchmark corpora, namely UFI and FERET face datasets. We experimentally show that our approach is very efficient because it significantly outperforms several other state-of-the-art methods and is efficient particularly in the real conditions where the above mentioned issues are obvious. version:1
arxiv-1702-03345 | Multi-Resolution Dual-Tree Wavelet Scattering Network for Signal Classification | http://arxiv.org/abs/1702.03345 | id:1702.03345 author:Amarjot Singh, Nick Kingsbury category:cs.CV  published:2017-02-10 summary:This paper introduces a Deep Scattering network that utilizes Dual-Tree complex wavelets to extract translation invariant representations from an input signal. The computationally efficient Dual-Tree wavelets decompose the input signal into densely spaced representations over scales. Translation invariance is introduced in the representations by applying a non-linearity over a region followed by averaging. The discriminatory information in the densely spaced, locally smooth, signal representations aids the learning of the classifier. The proposed network is shown to outperform Mallat's ScatterNet on four datasets with different modalities on classification accuracy. version:1
arxiv-1702-03342 | Learning Concept Embeddings for Efficient Bag-of-Concepts Densification | http://arxiv.org/abs/1702.03342 | id:1702.03342 author:Walid Shalaby, Wlodek Zadrozny category:cs.CL  published:2017-02-10 summary:Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main topics of these structures. That so called bag-of-concepts representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. In this paper we propose two neural embedding models in order to learn continuous concept vectors. Once learned, we propose an efficient vector aggregation method to generate fully dense bag-of-concepts representations. Empirical results on a benchmark dataset for measuring entity semantic relatedness show superior performance over other concept embedding models. In addition, by utilizing our efficient aggregation method, we demonstrate the effectiveness of the densified vector representation over the typical sparse representations for dataless classification where we can achieve at least same or better accuracy with much less dimensions. version:1
arxiv-1702-03334 | Batch Policy Gradient Methods for Improving Neural Conversation Models | http://arxiv.org/abs/1702.03334 | id:1702.03334 author:Kirthevasan Kandasamy, Yoram Bachrach, Ryota Tomioka, Daniel Tarlow, David Carter category:stat.ML cs.LG  published:2017-02-10 summary:We study reinforcement learning of chatbots with recurrent neural network architectures when the rewards are noisy and expensive to obtain. For instance, a chatbot used in automated customer service support can be scored by quality assurance agents, but this process can be expensive, time consuming and noisy. Previous reinforcement learning work for natural language processing uses on-policy updates and/or is designed for on-line learning settings. We demonstrate empirically that such strategies are not appropriate for this setting and develop an off-policy batch policy gradient method (BPG). We demonstrate the efficacy of our method via a series of synthetic experiments and an Amazon Mechanical Turk experiment on a restaurant recommendations dataset. version:1
arxiv-1702-03307 | Generative Mixture of Networks | http://arxiv.org/abs/1702.03307 | id:1702.03307 author:Ershad Banijamali, Ali Ghodsi, Pascal Poupart category:cs.LG stat.ML  published:2017-02-10 summary:A generative model based on training deep architectures is proposed. The model consists of K networks that are trained together to learn the underlying distribution of a given data set. The process starts with dividing the input data into K clusters and feeding each of them into a separate network. After few iterations of training networks separately, we use an EM-like algorithm to train the networks together and update the clusters of the data. We call this model Mixture of Networks. The provided model is a platform that can be used for any deep structure and be trained by any conventional objective function for distribution modeling. As the components of the model are neural networks, it has high capability in characterizing complicated data distributions as well as clustering data. We apply the algorithm on MNIST hand-written digits and Yale face datasets. We also demonstrate the clustering ability of the model using some real-world and toy examples. version:1
arxiv-1702-03305 | Universal Dependencies to Logical Forms with Negation Scope | http://arxiv.org/abs/1702.03305 | id:1702.03305 author:Federico Fancellu, Siva Reddy, Adam Lopez, Bonnie Webber category:cs.CL 03B65  published:2017-02-10 summary:Many language technology applications would benefit from the ability to represent negation and its scope on top of widely-used linguistic resources. In this paper, we investigate the possibility of obtaining a first-order logic representation with negation scope marked using Universal Dependencies. To do so, we enhance UDepLambda, a framework that converts dependency graphs to logical forms. The resulting UDepLambda$\lnot$ is able to handle phenomena related to scope by means of an higher-order type theory, relevant not only to negation but also to universal quantification and other complex semantic phenomena. The initial conversion we did for English is promising, in that one can represent the scope of negation also in the presence of more complex phenomena such as universal quantifiers. version:1
arxiv-1702-03275 | Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models | http://arxiv.org/abs/1702.03275 | id:1702.03275 author:Sergey Ioffe category:cs.LG  published:2017-02-10 summary:Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency. version:1
arxiv-1702-03274 | Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning | http://arxiv.org/abs/1702.03274 | id:1702.03274 author:Jason D. Williams, Kavosh Asadi, Geoffrey Zweig category:cs.AI cs.CL  published:2017-02-10 summary:End-to-end learning of recurrent neural networks (RNNs) is an attractive solution for dialog systems; however, current techniques are data-intensive and require thousands of dialogs to learn simple behaviors. We introduce Hybrid Code Networks (HCNs), which combine an RNN with domain-specific knowledge encoded as software and system action templates. Compared to existing end-to-end approaches, HCNs considerably reduce the amount of training data required, while retaining the key benefit of inferring a latent representation of dialog state. In addition, HCNs can be optimized with supervised learning, reinforcement learning, or a mixture of both. HCNs attain state-of-the-art performance on the bAbI dialog dataset, and outperform two commercially deployed customer-facing dialog systems. version:1
arxiv-1702-03267 | Dual-Tree Wavelet Scattering Network with Parametric Log Transformation for Object Classification | http://arxiv.org/abs/1702.03267 | id:1702.03267 author:Amarjot Singh, Nick Kingsbury category:cs.CV  published:2017-02-10 summary:We introduce a ScatterNet that uses a parametric log transformation with Dual-Tree complex wavelets to extract translation invariant representations from a multi-resolution image. The parametric transformation aids the OLS pruning algorithm by converting the skewed distributions into relatively mean-symmetric distributions while the Dual-Tree wavelets improve the computational efficiency of the network. The proposed network is shown to outperform Mallat's ScatterNet on two image datasets, both for classification accuracy and computational efficiency. The advantages of the proposed network over other supervised and some unsupervised methods are also presented using experiments performed on different training dataset sizes. version:1
arxiv-1702-03260 | A Deterministic and Generalized Framework for Unsupervised Learning with Restricted Boltzmann Machines | http://arxiv.org/abs/1702.03260 | id:1702.03260 author:Eric W. Tramel, Marylou Gabrié, Andre Manoel, Francesco Caltagirone, Florent Krzakala category:cs.LG cond-mat.dis-nn cs.NE stat.ML  published:2017-02-10 summary:Restricted Boltzmann machines (RBMs) are energy-based neural-networks which are commonly used as the building blocks for deep architectures neural architectures. In this work, we derive a deterministic framework for the training, evaluation, and use of RBMs based upon the Thouless-Anderson-Palmer (TAP) mean-field approximation of widely-connected systems with weak interactions coming from spin-glass theory. While the TAP approach has been extensively studied for fully-visible binary spin systems, our construction is generalized to latent-variable models, as well as to arbitrarily distributed real-valued spin systems with bounded support. In our numerical experiments, we demonstrate the effective deterministic training of our proposed models and are able to show interesting features of unsupervised learning which could not be directly observed with sampling. Additionally, we demonstrate how to utilize our TAP-based framework for leveraging trained RBMs as joint priors in denoising problems. version:1
arxiv-1702-03258 | Soft tensegrity robots | http://arxiv.org/abs/1702.03258 | id:1702.03258 author:John Rieffel, Jean-Baptiste Mouret category:cs.RO cs.LG cs.SY  published:2017-02-10 summary:Living organisms intertwine soft (e.g., muscle) and hard (e.g., bones) materials, giving them an intrinsic flexibility and resiliency often lacking in conventional rigid robots. The emerging field of soft robotics seeks to harness these same properties in order to create resilient machines. The nature of soft materials, however, presents considerable challenges to aspects of design, construction, and control -- and up until now, the vast majority of gaits for soft robots have been hand-designed through empirical trial-and-error. This manuscript describes an easy-to-assemble tensegrity-based soft robot capable of highly dynamic locomotive gaits and demonstrating structural and behavioral resilience in the face of physical damage. Enabling this is the use of a machine learning algorithm able to discover novel gaits with a minimal number of physical trials. These results lend further credence to soft-robotic approaches that seek to harness the interaction of complex material dynamics in order to generate a wealth of dynamical behaviors. version:1
arxiv-1702-03244 | $L_2$Boosting for Economic Applications | http://arxiv.org/abs/1702.03244 | id:1702.03244 author:Ye Luo, Martin Spindler category:stat.ML stat.ME  published:2017-02-10 summary:In the recent years more and more high-dimensional data sets, where the number of parameters $p$ is high compared to the number of observations $n$ or even larger, are available for applied researchers. Boosting algorithms represent one of the major advances in machine learning and statistics in recent years and are suitable for the analysis of such data sets. While Lasso has been applied very successfully for high-dimensional data sets in Economics, boosting has been underutilized in this field, although it has been proven very powerful in fields like Biostatistics and Pattern Recognition. We attribute this to missing theoretical results for boosting. The goal of this paper is to fill this gap and show that boosting is a competitive method for inference of a treatment effect or instrumental variable (IV) estimation in a high-dimensional setting. First, we present the $L_2$Boosting with componentwise least squares algorithm and variants which are tailored for regression problems which are the workhorse for most Econometric problems. Then we show how $L_2$Boosting can be used for estimation of treatment effects and IV estimation. We highlight the methods and illustrate them with simulations and empirical examples. For further results and technical details we refer to Luo and Spindler (2016, 2017) and to the online supplement of the paper. version:1
arxiv-1702-01975 | Learning what matters - Sampling interesting patterns | http://arxiv.org/abs/1702.01975 | id:1702.01975 author:Vladimir Dzyuba, Matthijs van Leeuwen category:stat.ML cs.AI cs.DB  published:2017-02-07 summary:In the field of exploratory data mining, local structure in data can be described by patterns and discovered by mining algorithms. Although many solutions have been proposed to address the redundancy problems in pattern mining, most of them either provide succinct pattern sets or take the interests of the user into account-but not both. Consequently, the analyst has to invest substantial effort in identifying those patterns that are relevant to her specific interests and goals. To address this problem, we propose a novel approach that combines pattern sampling with interactive data mining. In particular, we introduce the LetSIP algorithm, which builds upon recent advances in 1) weighted sampling in SAT and 2) learning to rank in interactive pattern mining. Specifically, it exploits user feedback to directly learn the parameters of the sampling distribution that represents the user's interests. We compare the performance of the proposed algorithm to the state-of-the-art in interactive pattern mining by emulating the interests of a user. The resulting system allows efficient and interleaved learning and sampling, thus user-specific anytime data exploration. Finally, LetSIP demonstrates favourable trade-offs concerning both quality-diversity and exploitation-exploration when compared to existing methods. version:2
arxiv-1702-01238 | Large-scale Image Geo-Localization Using Dominant Sets | http://arxiv.org/abs/1702.01238 | id:1702.01238 author:Eyasu Zemene, Yonatan Tariku, Haroon Idrees, Andrea Prati, Marcello Pelillo, Mubarak Shah category:cs.CV  published:2017-02-04 summary:This paper presents a new approach for the challenging problem of geo-locating an image using image matching in a structured database of city-wide reference images with known GPS coordinates. We cast the geo-localization as a clustering problem on local image features. Akin to existing approaches on the problem, our framework builds on low-level features which allow partial matching between images. For each local feature in the query image, we find its approximate nearest neighbors in the reference set. Next, we cluster the features from reference images using Dominant Set clustering, which affords several advantages over existing approaches. First, it permits variable number of nodes in the cluster which we use to dynamically select the number of nearest neighbors (typically coming from multiple reference images) for each query feature based on its discrimination value. Second, as we also quantify in our experiments, this approach is several orders of magnitude faster than existing approaches. Thus, we obtain multiple clusters (different local maximizers) and obtain a robust final solution to the problem using multiple weak solutions through constrained Dominant Set clustering on global image features, where we enforce the constraint that the query image must be included in the cluster. This second level of clustering also bypasses heuristic approaches to voting and selecting the reference image that matches to the query. We evaluated the proposed framework on an existing dataset of 102k street view images as well as a new dataset of 300k images, and show that it outperforms the state-of-the-art by 20% and 7%, respectively, on the two datasets. version:2
arxiv-1702-03197 | Arabic Language Sentiment Analysis on Health Services | http://arxiv.org/abs/1702.03197 | id:1702.03197 author:Abdulaziz M. Alayba, Vasile Palade, Matthew England, Rahat Iqbal category:cs.CL cs.NE cs.SI I.2.7; I.2.6  published:2017-02-10 summary:The social media network phenomenon leads to a massive amount of valuable data that is available online and easy to access. Many users share images, videos, comments, reviews, news and opinions on different social networks sites, with Twitter being one of the most popular ones. Data collected from Twitter is highly unstructured, and extracting useful information from tweets is a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. While there has been a lot of research on sentiment analysis in English, the amount of researches and datasets in Arabic language is limited. This paper introduces an Arabic language dataset which is about opinions on health services and has been collected from Twitter. The paper will first detail the process of collecting the data from Twitter and also the process of filtering, pre-processing and annotating the Arabic text in order to build a big sentiment analysis dataset in Arabic. Several Machine Learning algorithms (Naive Bayes, Support Vector Machine and Logistic Regression) alongside Deep and Convolutional Neural Networks were utilized in our experiments of sentiment analysis on our health dataset. version:1
arxiv-1702-03192 | Improving the Performance of Fully Connected Neural Networks by Out-of-Place Matrix Transpose | http://arxiv.org/abs/1702.03192 | id:1702.03192 author:Shaohuai Shi, Pengfei Xu, Xiaowen Chu category:cs.DC cs.LG  published:2017-02-10 summary:Fully connected network has been widely used in deep learning, and its computation efficiency is highly benefited from the matrix multiplication algorithm with cuBLAS on GPU. However, We found that, there exist some drawbacks of cuBLAS in calculating matrix $\textbf{A}$ multiplies the transpose of matrix $\textbf{B}$ (i.e., NT operation). To reduce the impact of NT operation by cuBLAS, we exploit the out-of-place transpose of matrix $\textbf{B}$ to avoid using NT operation, and then we apply our method to Caffe, which is a popular deep learning tool. Our contribution is two-fold. First, we propose a naive method (TNN) and model-based method (MTNN) to increase the performance in calculating $\textbf{A}\times \textbf{B}^T$, and it achieves about 4.7 times performance enhancement in our tested cases on GTX1080 card. Second, we integrate MTNN method into Caffe to enhance the efficiency in training fully connected networks, which achieves about 70% speedup compared to the original Caffe in our configured fully connected networks on GTX1080 card. version:1
arxiv-1702-03176 | A clustering approach to heterogeneous change detection | http://arxiv.org/abs/1702.03176 | id:1702.03176 author:Luigi Tommaso Luppino, Stian Normann Anfinsen, Gabriele Moser, Robert Jenssen, Filippo Maria Bianchi, Sebastiano Serpico, Gregoire Mercier category:cs.CV  published:2017-02-10 summary:Change detection in heterogeneous multitemporal satellite images is a challenging and still not much studied topic in remote sensing and earth observation. This paper focuses on comparison of image pairs covering the same geographical area and acquired by two different sensors, one optical radiometer and one synthetic aperture radar, at two different times. We propose a clustering-based technique to detect changes, identified as clusters that split or merge in the different images. To evaluate potentials and limitations of our method, we perform experiments on real data. Preliminary results confirm the relationship between splits and merges of clusters and the occurrence of changes. However, it becomes evident that it is necessary to incorporate prior, ancillary, or application-specific information to improve the interpretation of clustering results and to identify unambiguously the areas of change. version:1
arxiv-1702-03121 | Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction | http://arxiv.org/abs/1702.03121 | id:1702.03121 author:Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Sayeed, Manfred Pinkal category:cs.CL cs.AI stat.ML  published:2017-02-10 summary:Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect. version:1
arxiv-1702-03115 | Texture Characterization by Using Shape Co-occurrence Patterns | http://arxiv.org/abs/1702.03115 | id:1702.03115 author:Gui-Song Xia, Gang Liu, Xiang Bai, Liangpei Zhang category:cs.CV  published:2017-02-10 summary:Texture characterization is a key problem in image understanding and pattern recognition. In this paper, we present a flexible shape-based texture representation using shape co-occurrence patterns. More precisely, texture images are first represented by tree of shapes, each of which is associated with several geometrical and radiometric attributes. Then four typical kinds of shape co-occurrence patterns based on the hierarchical relationship of the shapes in the tree are learned as codewords. Three different coding methods are investigated to learn the codewords, with which, any given texture image can be encoded into a descriptive vector. In contrast with existing works, the proposed method not only inherits the strong ability to depict geometrical aspects of textures and the high robustness to variations of imaging conditions from the shape-based method, but also provides a flexible way to consider shape relationships and to compute high-order statistics on the tree. To our knowledge, this is the first time to use co-occurrence patterns of explicit shapes as a tool for texture analysis. Experiments on various texture datasets and scene datasets demonstrate the efficiency of the proposed method. version:1
arxiv-1702-03105 | Graph Fourier Transform with Negative Edges for Depth Image Coding | http://arxiv.org/abs/1702.03105 | id:1702.03105 author:Weng-Tai Su, Gene Cheung, Chia-Wen Lin category:cs.CV  published:2017-02-10 summary:Recent advent in graph signal processing (GSP) has led to the development of new graph-based transforms and wavelets for image / video coding, where the underlying graph describes inter-pixel correlations. In this paper, we develop a new transform called signed graph Fourier transform (SGFT), where the underlying graph G contains negative edges that describe anti-correlations between pixel pairs. Specifically, we first construct a one-state Markov process that models both inter-pixel correlations and anti-correlations. We then derive the corresponding precision matrix, and show that the loopy graph Laplacian matrix Q of a graph G with a negative edge and two self-loops at its end nodes is approximately equivalent. This proves that the eigenvectors of Q - called SGFT - approximates the optimal Karhunen-Lo`eve Transform (KLT). We show the importance of the self-loops in G to ensure Q is positive semi-definite. We prove that the first eigenvector of Q is piecewise constant (PWC), and thus can well approximate a piecewise smooth (PWS) signal like a depth image. Experimental results show that a block-based coding scheme based on SGFT outperforms a previous scheme using graph transforms with only positive edges for several depth images. version:1
arxiv-1702-03082 | UsingWord Embedding for Cross-Language Plagiarism Detection | http://arxiv.org/abs/1702.03082 | id:1702.03082 author:J. Ferrero, F. Agnes, L. Besacier, D. Schwab category:cs.CL  published:2017-02-10 summary:This paper proposes to use distributed representation of words (word embeddings) in cross-language textual similarity detection. The main contributions of this paper are the following: (a) we introduce new cross-language similarity detection methods based on distributed representation of words; (b) we combine the different methods proposed to verify their complementarity and finally obtain an overall F1 score of 89.15% for English-French similarity detection at chunk level (88.5% at sentence level) on a very challenging corpus. version:1
arxiv-1702-03070 | PCA in Data-Dependent Noise (Correlated-PCA): Nearly Optimal Finite Sample Guarantees | http://arxiv.org/abs/1702.03070 | id:1702.03070 author:Namrata Vaswani, Praneeth Narayanamurthy category:cs.IT math.IT stat.ML  published:2017-02-10 summary:We study Principal Component Analysis (PCA) in a setting where a part of the corrupting noise is data-dependent and, hence, the noise and the true data are correlated. Under a bounded-ness assumption on both the true data and noise, and a few assumptions on the data-noise correlation, we obtain a sample complexity bound for the most common PCA solution, singular value decomposition (SVD). This bound, which is within a logarithmic factor of the best achievable, significantly improves upon our bound from recent work (NIPS 2016) where we first studied this "correlated-PCA" problem. version:1
arxiv-1702-02555 | A Modified Construction for a Support Vector Classifier to Accommodate Class Imbalances | http://arxiv.org/abs/1702.02555 | id:1702.02555 author:Matt Parker, Colin Parker category:stat.ML cs.LG  published:2017-02-08 summary:Given a training set with binary classification, the Support Vector Machine identifies the hyperplane maximizing the margin between the two classes of training data. This general formulation is useful in that it can be applied without regard to variance differences between the classes. Ignoring these differences is not optimal, however, as the general SVM will give the class with lower variance an unjustifiably wide berth. This increases the chance of misclassification of the other class and results in an overall loss of predictive performance. An alternate construction is proposed in which the margins of the separating hyperplane are different for each class, each proportional to the standard deviation of its class along the direction perpendicular to the hyperplane. The construction agrees with the SVM in the case of equal class variances. This paper will then examine the impact to the dual representation of the modified constraint equations. version:2
arxiv-1702-03824 | Online People Tracking and Identification with RFID and Kinect | http://arxiv.org/abs/1702.03824 | id:1702.03824 author:Xinyu Li, Yanyi Zhang, Ivan Marsic, Randall S. Burd category:cs.CV  published:2017-02-10 summary:We introduce a novel, accurate and practical system for real-time people tracking and identification. We used a Kinect V2 sensor for tracking that generates a body skeleton for up to six people in the view. We perform identification using both Kinect and passive RFID, by first measuring the velocity vector of person's skeleton and of their RFID tag using the position of the RFID reader antennas as reference points and then finding the best match between skeletons and tags. We introduce a method for synchronizing Kinect data, which is captured regularly, with irregular or missing RFID data readouts. Our experiments show centimeter-level people tracking resolution with 80% average identification accuracy for up to six people in indoor environments, which meets the needs of many applications. Our system can preserve user privacy and work with different lighting. version:1
arxiv-1702-03056 | Sparse modeling approach to analytical continuation of imaginary-time quantum Monte Carlo data | http://arxiv.org/abs/1702.03056 | id:1702.03056 author:Junya Otsuki, Masayuki Ohzeki, Hiroshi Shinaoka, Kazuyoshi Yoshimi category:cond-mat.str-el cond-mat.stat-mech stat.ML  published:2017-02-10 summary:A new approach of solving the ill-conditioned inverse problem for analytical continuation is proposed. The root of the problem lies in the fact that even tiny noise of imaginary-time input data has a serious impact on the inferred real-frequency spectra. By means of a modern regularization technique, we eliminate redundant degrees of freedom that essentially carry the noise, leaving only relevant information unaffected by the noise. The resultant spectrum is represented with minimal bases and thus a stable analytical continuation is achieved. This framework further provides a tool to clarify to what extent the fine structure of the true spectral function can be reconstructed from imaginary-time data. version:1
arxiv-1702-03054 | Compression of imaginary-time data using intermediate representation of analytical continuation | http://arxiv.org/abs/1702.03054 | id:1702.03054 author:Hiroshi Shinaoka, Junya Otsuki, Masayuki Ohzeki, Kazuyoshi Yoshimi category:cond-mat.str-el cond-mat.stat-mech stat.ML  published:2017-02-10 summary:New model-independent compact representations of imaginary-time data are presented in terms of the intermediate representation (IR) of analytical continuation. This is motivated by a recent numerical finding by the authors [J. Otsuki et al. (2017)]. We demonstrate the efficiency of the present method through continuous-time quantum Monte Carlo calculations of an Anderson impurity model. We find that the IR yields a significantly compact form of various types of correlation functions. The present framework will provide general ways to boost the power of cutting-edge diagrammatic/quantum Monte Carlo treatments of many-body systems. version:1
arxiv-1702-03044 | Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights | http://arxiv.org/abs/1702.03044 | id:1702.03044 author:Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen category:cs.CV cs.AI cs.NE  published:2017-02-10 summary:This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code will be made publicly available. version:1
arxiv-1702-03041 | Reconstruction for Feature Disentanglement in Pose-invariant Face Recognition | http://arxiv.org/abs/1702.03041 | id:1702.03041 author:Xi Peng, Xiang Yu, Kihyuk Sohn, Dimitris Metaxas, Manmohan Chandraker category:cs.CV  published:2017-02-10 summary:Deep neural networks (DNNs) trained on large-scale datasets have recently achieved impressive improvements in face recognition. But a persistent challenge remains to develop methods capable of handling large pose variations that are relatively under-represented in training data. This paper presents a method for learning a feature representation that is invariant to pose, without requiring extensive pose coverage in training data. We first propose to use a synthesis network for generating non-frontal views from a single frontal image, in order to increase the diversity of training data while preserving accurate facial details that are critical for identity discrimination. Our next contribution is a multi-source multi-task DNN that seeks a rich embedding representing identity information, as well as information such as pose and landmark locations. Finally, we propose a Siamese network to explicitly disentangle identity and pose, by demanding alignment between the feature reconstructions through various combinations of identity and pose features obtained from two images of the same subject. Experiments on face datasets in both controlled and wild scenarios, such as MultiPIE, LFW and 300WLP, show that our method consistently outperforms the state-of-the-art, especially on images with large head pose variations. version:1
arxiv-1702-03040 | Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities | http://arxiv.org/abs/1702.03040 | id:1702.03040 author:Ruitong Huang, Tor Lattimore, András György, Csaba Szepesvári category:cs.LG  published:2017-02-10 summary:The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are convex and positively curved. In this paper we ask whether there are other "lucky" settings when FTL achieves sublinear, "small" regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polytope domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL. version:1
arxiv-1702-03037 | Multi-agent Reinforcement Learning in Sequential Social Dilemmas | http://arxiv.org/abs/1702.03037 | id:1702.03037 author:Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, Thore Graepel category:cs.MA cs.AI cs.GT cs.LG  published:2017-02-10 summary:Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We characterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation. version:1
arxiv-1702-03033 | Local System Voting Feature for Machine Translation System Combination | http://arxiv.org/abs/1702.03033 | id:1702.03033 author:Markus Freitag, Jan-Thorsten Peter, Stephan Peitz, Minwei Feng, Hermann Ney category:cs.CL  published:2017-02-10 summary:In this paper, we enhance the traditional confusion network system combination approach with an additional model trained by a neural network. This work is motivated by the fact that the commonly used binary system voting models only assign each input system a global weight which is responsible for the global impact of each input system on all translations. This prevents individual systems with low system weights from having influence on the system combination output, although in some situations this could be helpful. Further, words which have only been seen by one or few systems rarely have a chance of being present in the combined output. We train a local system voting model by a neural network which is based on the words themselves and the combinatorial occurrences of the different system outputs. This gives system combination the option to prefer other systems at different word positions even for the same sentence. version:1
arxiv-1701-04245 | Learning Traffic as Images: A Deep Convolution Neural Network for Large-scale Transportation Network Speed Prediction | http://arxiv.org/abs/1701.04245 | id:1701.04245 author:Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Na, Yong Wang, Yunpeng Wang category:cs.LG stat.ML  published:2017-01-16 summary:This paper proposes a convolution neural network (CNN)-based method that learns traffic as images and predicts large-scale, network-wide traffic speed with high accuracy. Spatiotemporal traffic dynamics is converted to images describing the time and space relations of traffic flow via a two-dimensional time-space matrix. CNN is applied to the image following two consecutive steps: abstract traffic feature extraction and network-wide traffic speed prediction. The effectiveness of the proposed method is evaluated by taking two real-world transportation networks, the second ring road and north-east transportation network in Beijing, as examples, and comparing the method with four prevailing algorithms, namely, ordinary least squares, k-nearest neighbors, artificial neural network, and random forest. The results show that the proposed method outperforms the four algorithms by an average accuracy improvement of 27.96% within acceptable execution time. The CNN can train the model in reasonable time and thus are suitable for large-scale transportation networks. version:3
arxiv-1702-03023 | A New Rank Constraint on Multi-view Fundamental Matrices, and its Application to Camera Location Recovery | http://arxiv.org/abs/1702.03023 | id:1702.03023 author:Soumyadip Sengupta, Tal Amir, Meirav Galun, Tom Goldstein, David W. Jacobs, Amit Singer, Ronen Basri category:cs.CV  published:2017-02-10 summary:Accurate estimation of camera matrices is an important step in structure from motion algorithms. In this paper we introduce a novel rank constraint on collections of fundamental matrices in multi-view settings. We show that in general, with the selection of proper scale factors, a matrix formed by stacking fundamental matrices between pairs of images has rank 6. Moreover, this matrix forms the symmetric part of a rank 3 matrix whose factors relate directly to the corresponding camera matrices. We use this new characterization to produce better estimations of fundamental matrices by optimizing an L1-cost function using Iterative Re-weighted Least Squares and Alternate Direction Method of Multiplier. We further show that this procedure can improve the recovery of camera locations, particularly in multi-view settings in which fewer images are available. version:1
arxiv-1702-03006 | Multi-step Off-policy Learning Without Importance Sampling Ratios | http://arxiv.org/abs/1702.03006 | id:1702.03006 author:Ashique Rupam Mahmood, Huizhen Yu, Richard S. Sutton category:cs.LG  published:2017-02-09 summary:To estimate the value functions of policies from exploratory data, most model-free off-policy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart. version:1
arxiv-1702-03000 | A large comparison of feature-based approaches for buried target classification in forward-looking ground-penetrating radar | http://arxiv.org/abs/1702.03000 | id:1702.03000 author:Joseph A. Camilo, Leslie M. Collins, Jordan M. Malof category:cs.CV  published:2017-02-09 summary:Forward-looking ground-penetrating radar (FLGPR) has recently been investigated as a remote sensing modality for buried target detection (e.g., landmines). In this context, raw FLGPR data is beamformed into images and then computerized algorithms are applied to automatically detect subsurface buried targets. Most existing algorithms are supervised, meaning they are trained to discriminate between labeled target and non-target imagery, usually based on features extracted from the imagery. A large number of features have been proposed for this purpose, however thus far it is unclear which are the most effective. The first goal of this work is to provide a comprehensive comparison of detection performance using existing features on a large collection of FLGPR data. Fusion of the decisions resulting from processing each feature is also considered. The second goal of this work is to investigate two modern feature learning approaches from the object recognition literature: the bag-of-visual-words and the Fisher vector for FLGPR processing. The results indicate that the new feature learning approaches outperform existing methods. Results also show that fusion between existing features and new features yields little additional performance improvements. version:1
arxiv-1702-02982 | Fixing an error in Caponnetto and de Vito (2007) | http://arxiv.org/abs/1702.02982 | id:1702.02982 author:Dougal J. Sutherland category:stat.ML cs.LG math.ST stat.TH  published:2017-02-09 summary:The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal rates for kernel ridge regression in a very general setting. Its proof, however, contains an error in its bound on the effective dimensionality. In this note, we explain the mistake, provide a correct bound, and show that the main theorem remains true. version:1
arxiv-1701-07422 | An Iterative Algorithm for Sparse Recovery of Missing Image Samples Using a New Similarity Index | http://arxiv.org/abs/1701.07422 | id:1701.07422 author:Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti category:cs.LG stat.ML  published:2017-01-25 summary:This paper investigates the problem of recovering missing samples using methods based on sparse representation adapted especially for image signals. Instead of $l_2$-norm or Mean Square Error (MSE), a new perceptual quality measure is used as the similarity criterion between the original and the reconstructed images. The proposed metric called Convex SIMilarity (CSIM) index is a modified version of the Structural SIMilarity (SSIM) index which despite its predecessor, is convex and uni-modal. We also propose an iterative sparse recovery method based on a constrained $l_1$-norm minimization problem involving CSIM as the fidelity criterion. This optimization problem which is adopted for missing sample recovery of images is efficiently solved via an algorithm based on Alternating Direction Method of Multipliers (ADMM). Simulation results show the performance of the new similarity index as well as the proposed algorithm for missing sample recovery of test images. version:2
arxiv-1702-02925 | EAC-Net: A Region-based Deep Enhancing and Cropping Approach for Facial Action Unit Detection | http://arxiv.org/abs/1702.02925 | id:1702.02925 author:Wei Li, Farnaz Abtahi, Zhigang Zhu, Lijun Yin category:cs.CV  published:2017-02-09 summary:In this paper, we propose a deep learning based approach for facial action unit detection by enhancing and cropping the regions of interest. The approach is implemented by adding two novel nets (layers): the enhancing layers and the cropping layers, to a pretrained CNN model. For the enhancing layers, we designed an attention map based on facial landmark features and applied it to a pretrained neural network to conduct enhanced learning (The E-Net). For the cropping layers, we crop facial regions around the detected landmarks and design convolutional layers to learn deeper features for each facial region (C-Net). We then fuse the E-Net and the C-Net to obtain our Enhancing and Cropping (EAC) Net, which can learn both feature enhancing and region cropping functions. Our approach shows significant improvement in performance compared to the state-of-the-art methods applied to BP4D and DISFA AU datasets. version:1
arxiv-1702-02914 | Spatial Filtering for EEG-Based Regression Problems in Brain-Computer Interface (BCI) | http://arxiv.org/abs/1702.02914 | id:1702.02914 author:Dongrui Wu, Jung-Tai King, Chun-Hsiang Chuang, Chin-Teng Lin, Tzyy-Ping Jung category:cs.LG cs.HC  published:2017-02-09 summary:Electroencephalogram (EEG) signals are frequently used in brain-computer interfaces (BCIs), but they are easily contaminated by artifacts and noises, so preprocessing must be done before they are fed into a machine learning algorithm for classification or regression. Spatial filters have been widely used to increase the signal-to-noise ratio of EEG for BCI classification problems, but their applications in BCI regression problems have been very limited. This paper proposes two common spatial pattern (CSP) filters for EEG-based regression problems in BCI, which are extended from the CSP filter for classification, by making use of fuzzy sets. Experimental results on EEG-based response speed estimation from a large-scale study, which collected 143 sessions of sustained-attention psychomotor vigilance task data from 17 subjects during a 5-month period, demonstrate that the two proposed spatial filters can significantly increase the EEG signal quality. When used in LASSO and k-nearest neighbors regression for user response speed estimation, the spatial filters can reduce the root mean square estimation error by 10.02-19.77%, and at the same time increase the correlation to the true response speed by 19.39-86.47%. version:1
arxiv-1702-02906 | Switching EEG Headsets Made Easy: Reducing Offline Calibration Effort Using Active Weighted Adaptation Regularization | http://arxiv.org/abs/1702.02906 | id:1702.02906 author:Dongrui Wu, Vernon J. Lawhern, W. David Hairston, Brent J. Lance category:cs.LG cs.HC  published:2017-02-09 summary:Electroencephalography (EEG) headsets are the most commonly used sensing devices for Brain-Computer Interface. In real-world applications, there are advantages to extrapolating data from one user session to another. However, these advantages are limited if the data arise from different hardware systems, which often vary between application spaces. Currently, this creates a need to recalibrate classifiers, which negatively affects people's interest in using such systems. In this paper, we employ active weighted adaptation regularization (AwAR), which integrates weighted adaptation regularization (wAR) and active learning, to expedite the calibration process. wAR makes use of labeled data from the previous headset and handles class-imbalance, and active learning selects the most informative samples from the new headset to label. Experiments on single-trial event-related potential classification show that AwAR can significantly increase the classification accuracy, given the same number of labeled samples from the new headset. In other words, AwAR can effectively reduce the number of labeled samples required from the new headset, given a desired classification accuracy, suggesting value in collating data for use in wide scale transfer-learning applications. version:1
arxiv-1702-02901 | Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR) | http://arxiv.org/abs/1702.02901 | id:1702.02901 author:Dongrui Wu, Vernon J. Lawhern, Stephen Gordon, Brent J. Lance, Chin-Teng Lin category:cs.LG cs.HC  published:2017-02-09 summary:One big challenge that hinders the transition of brain-computer interfaces (BCIs) from laboratory settings to real-life applications is the availability of high-performance and robust learning algorithms that can effectively handle individual differences, i.e., algorithms that can be applied to a new subject with zero or very little subject-specific calibration data. Transfer learning and domain adaptation have been extensively used for this purpose. However, most previous works focused on classification problems. This paper considers an important regression problem in BCI, namely, online driver drowsiness estimation from EEG signals. By integrating fuzzy sets with domain adaptation, we propose a novel online weighted adaptation regularization for regression (OwARR) algorithm to reduce the amount of subject-specific calibration data, and also a source domain selection (SDS) approach to save about half of the computational cost of OwARR. Using a simulated driving dataset with 15 subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller estimation errors than several other approaches. We also provide comprehensive analyses on the robustness of OwARR and OwARR-SDS. version:1
arxiv-1702-02897 | Online and Offline Domain Adaptation for Reducing BCI Calibration Effort | http://arxiv.org/abs/1702.02897 | id:1702.02897 author:Dongrui Wu category:cs.LG cs.HC  published:2017-02-09 summary:Many real-world brain-computer interface (BCI) applications rely on single-trial classification of event-related potentials (ERPs) in EEG signals. However, because different subjects have different neural responses to even the same stimulus, it is very difficult to build a generic ERP classifier whose parameters fit all subjects. The classifier needs to be calibrated for each individual subject, using some labeled subject-specific data. This paper proposes both online and offline weighted adaptation regularization (wAR) algorithms to reduce this calibration effort, i.e., to minimize the amount of labeled subject-specific EEG data required in BCI calibration, and hence to increase the utility of the BCI system. We demonstrate using a visually-evoked potential oddball task and three different EEG headsets that both online and offline wAR algorithms significantly outperform several other algorithms. Moreover, through source domain selection, we can reduce their computational cost by about 50%, making them more suitable for real-time applications. version:1
arxiv-1702-02896 | Efficient Policy Learning | http://arxiv.org/abs/1702.02896 | id:1702.02896 author:Susan Athey, Stefan Wager category:math.ST cs.LG stat.ML stat.TH  published:2017-02-09 summary:There has been considerable interest across several fields in methods that reduce the problem of learning good treatment assignment policies to the problem of accurate policy evaluation. Given a class of candidate policies, these methods first effectively evaluate each policy individually, and then learn a policy by optimizing the estimated value function; such approaches are guaranteed to be risk-consistent whenever the policy value estimates are uniformly consistent. However, despite the wealth of proposed methods, the literature remains largely silent on questions of statistical efficiency: there are only limited results characterizing which policy evaluation strategies lead to better learned policies than others, or what the optimal policy evaluation strategies are. In this paper, we build on classical results in semiparametric efficiency theory to develop quasi-optimal methods for policy learning; in particular, we propose a class of policy value estimators that, when optimized, yield regret bounds for the learned policy that scale with the semiparametric efficient variance for policy evaluation. On a practical level, our result suggests new methods for policy learning motivated by semiparametric efficiency theory. version:1
arxiv-1702-01460 | A scikit-based Python environment for performing multi-label classification | http://arxiv.org/abs/1702.01460 | id:1702.01460 author:Piotr Szymański, Tomasz Kajdanowicz category:cs.LG cs.MS  published:2017-02-05 summary:Scikit-multilearn is a Python library for performing multi-label classification. The library is compatible with the scikit/scipy ecosystem and uses sparse matrices for all internal operations. It provides native Python implementations of popular multi-label classification methods alongside novel framework for label space partitioning and division. It includes graph-based community detection methods that utilize the powerful igraph library for extracting label dependency information. In addition its code is well test covered and follows PEP8. Source code and documentation can be downloaded from http://scikit.ml and also via pip. The library follows scikit's BSD licencing scheme. version:3
arxiv-1702-02849 | Coordinated Online Learning With Applications to Learning User Preferences | http://arxiv.org/abs/1702.02849 | id:1702.02849 author:Christoph Hirnschall, Adish Singla, Sebastian Tschiatschek, Andreas Krause category:cs.LG stat.ML  published:2017-02-09 summary:We study an online multi-task learning setting, in which instances of related tasks arrive sequentially, and are handled by task-specific online learners. We consider an algorithmic framework to model the relationship of these tasks via a set of convex constraints. To exploit this relationship, we design a novel algorithm -- COOL -- for coordinating the individual online learners: Our key idea is to coordinate their parameters via weighted projections onto a convex set. By adjusting the rate and accuracy of the projection, the COOL algorithm allows for a trade-off between the benefit of coordination and the required computation/communication. We derive regret bounds for our approach and analyze how they are influenced by these trade-off factors. We apply our results on the application of learning users' preferences on the Airbnb marketplace with the goal of incentivizing users to explore under-reviewed apartments. version:1
arxiv-1702-02828 | Minimax Lower Bounds for Ridge Combinations Including Neural Nets | http://arxiv.org/abs/1702.02828 | id:1702.02828 author:Jason M. Klusowski, Andrew R. Barron category:stat.ML cs.LG 62J02  62G08  68T05  published:2017-02-09 summary:Estimation of functions of $ d $ variables is considered using ridge combinations of the form $ \textstyle\sum_{k=1}^m c_{1,k} \phi(\textstyle\sum_{j=1}^d c_{0,j,k}x_j-b_k) $ where the activation function $ \phi $ is a function with bounded value and derivative. These include single-hidden layer neural networks, polynomials, and sinusoidal models. From a sample of size $ n $ of possibly noisy values at random sites $ X \in B = [-1,1]^d $, the minimax mean square error is examined for functions in the closure of the $ \ell_1 $ hull of ridge functions with activation $ \phi $. It is shown to be of order $ d/n $ to a fractional power (when $ d $ is of smaller order than $ n $), and to be of order $ (\log d)/n $ to a fractional power (when $ d $ is of larger order than $ n $). Dependence on constraints $ v_0 $ and $ v_1 $ on the $ \ell_1 $ norms of inner parameter $ c_0 $ and outer parameter $ c_1 $, respectively, is also examined. Also, lower and upper bounds on the fractional power are given. The heart of the analysis is development of information-theoretic packing numbers for these classes of functions. version:1
arxiv-1702-02817 | Graph Based Relational Features for Collective Classification | http://arxiv.org/abs/1702.02817 | id:1702.02817 author:Immanuel Bayer, Uwe Nagel, Steffen Rendle category:cs.IR cs.AI cs.LG  published:2017-02-09 summary:Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods. However, in contrast to feature based ML, collective inference methods require complex inference procedures and often depend on the strong assumption of label consistency among related samples. In this paper, we introduce new relational features for standard ML methods by extracting information from direct and indirect relations. We show empirically on three standard benchmark datasets that our relational features yield results comparable to collective inference methods. Finally we show that our proposal outperforms these methods when additional information is available. version:1
arxiv-1702-02805 | Attribute-controlled face photo synthesis from simple line drawing | http://arxiv.org/abs/1702.02805 | id:1702.02805 author:Qi Guo, Ce Zhu, Zhiqiang Xia, Zhengtao Wang, Yipeng Liu category:cs.CV  published:2017-02-09 summary:Face photo synthesis from simple line drawing is a one-to-many task as simple line drawing merely contains the contour of human face. Previous exemplar-based methods are over-dependent on the datasets and are hard to generalize to complicated natural scenes. Recently, several works utilize deep neural networks to increase the generalization, but they are still limited in the controllability of the users. In this paper, we propose a deep generative model to synthesize face photo from simple line drawing controlled by face attributes such as hair color and complexion. In order to maximize the controllability of face attributes, an attribute-disentangled variational auto-encoder (AD-VAE) is firstly introduced to learn latent representations disentangled with respect to specified attributes. Then we conduct photo synthesis from simple line drawing based on AD-VAE. Experiments show that our model can well disentangle the variations of attributes from other variations of face photos and synthesize detailed photorealistic face images with desired attributes. Regarding background and illumination as the style and human face as the content, we can also synthesize face photos with the target style of a style photo. version:1
arxiv-1702-02779 | On-the-Fly Adaptation of Regression Forests for Online Camera Relocalisation | http://arxiv.org/abs/1702.02779 | id:1702.02779 author:Tommaso Cavallari, Stuart Golodetz, Nicholas A. Lord, Julien Valentin, Luigi Di Stefano, Philip H. S. Torr category:cs.CV  published:2017-02-09 summary:Camera relocalisation is a key problem in computer vision, with applications as diverse as simultaneous localisation and mapping, virtual/augmented reality and navigation. Common techniques either match the current image against keyframes with known poses coming from a tracker, or establish 2D-to-3D correspondences between keypoints in the current image and points in the scene in order to estimate the camera pose. Recently, regression forests have become a popular alternative to establish such correspondences. They achieve accurate results, but must be trained offline on the target scene, preventing relocalisation in new environments. In this paper, we show how to circumvent this limitation by adapting a pre-trained forest to a new scene on the fly. Our adapted forests achieve relocalisation performance that is on par with that of offline forests, and our approach runs in under 150ms, making it desirable for real-time systems that require online relocalisation. version:1
arxiv-1701-02854 | Decoding as Continuous Optimization in Neural Machine Translation | http://arxiv.org/abs/1701.02854 | id:1701.02854 author:Cong Duy Vu Hoang, Gholamreza Haffari, Trevor Cohn category:cs.CL cs.AI  published:2017-01-11 summary:We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. The resulting optimisation problem is then tackled using constrained gradient optimisation. Our powerful decoding framework, enables decoding intractable models such as the intersection of left-to-right and right-to-left (bidirectional) as well as source-to-target and target-to-source (bilingual) NMT models. Our empirical results show that our decoding framework is effective, and leads to substantial improvements in translations generated from the intersected models where the typical greedy or beam search is infeasible. version:3
arxiv-1702-02363 | Automatically Annotated Turkish Corpus for Named Entity Recognition and Text Categorization using Large-Scale Gazetteers | http://arxiv.org/abs/1702.02363 | id:1702.02363 author:H. Bahadir Sahin, Caglar Tirkaz, Eray Yildiz, Mustafa Tolga Eren, Ozan Sonmez category:cs.CL  published:2017-02-08 summary:Turkish Wikipedia Named-Entity Recognition and Text Categorization (TWNERTC) dataset is a collection of automatically categorized and annotated sentences obtained from Wikipedia. We constructed large-scale gazetteers by using a graph crawler algorithm to extract relevant entity and domain information from a semantic knowledge base, Freebase. The constructed gazetteers contains approximately 300K entities with thousands of fine-grained entity types under 77 different domains. Since automated processes are prone to ambiguity, we also introduce two new content specific noise reduction methodologies. Moreover, we map fine-grained entity types to the equivalent four coarse-grained types: person, loc, org, misc. Eventually, we construct six different dataset versions and evaluate the quality of annotations by comparing ground truths from human annotators. We make these datasets publicly available to support studies on Turkish named-entity recognition (NER) and text categorization (TC). version:2
arxiv-1702-05441 | Toward Abstraction from Multi-modal Data: Empirical Studies on Multiple Time-scale Recurrent Models | http://arxiv.org/abs/1702.05441 | id:1702.05441 author:Junpei Zhong, Angelo Cangelosi, Tetsuya Ogata category:cs.NE  published:2017-02-07 summary:The abstraction tasks are challenging for multi- modal sequences as they require a deeper semantic understanding and a novel text generation for the data. Although the recurrent neural networks (RNN) can be used to model the context of the time-sequences, in most cases the long-term dependencies of multi-modal data make the back-propagation through time training of RNN tend to vanish in the time domain. Recently, inspired from Multiple Time-scale Recurrent Neural Network (MTRNN), an extension of Gated Recurrent Unit (GRU), called Multiple Time-scale Gated Recurrent Unit (MTGRU), has been proposed to learn the long-term dependencies in natural language processing. Particularly it is also able to accomplish the abstraction task for paragraphs given that the time constants are well defined. In this paper, we compare the MTRNN and MTGRU in terms of its learning performances as well as their abstraction representation on higher level (with a slower neural activation). This was done by conducting two studies based on a smaller data- set (two-dimension time sequences from non-linear functions) and a relatively large data-set (43-dimension time sequences from iCub manipulation tasks with multi-modal data). We conclude that gated recurrent mechanisms may be necessary for learning long-term dependencies in large dimension multi-modal data-sets (e.g. learning of robot manipulation), even when natural language commands was not involved. But for smaller learning tasks with simple time-sequences, generic version of recurrent models, such as MTRNN, were sufficient to accomplish the abstraction task. version:1
arxiv-1605-07913 | Solution of linear ill-posed problems using random dictionaries | http://arxiv.org/abs/1605.07913 | id:1605.07913 author:Pawan Gupta, Marianna Pensky category:math.ST stat.ME stat.ML stat.TH  published:2016-05-25 summary:In the present paper we consider application of overcomplete dictionaries to solution of general ill-posed linear inverse problems. In the context of regression problems, there have been enormous amount of effort to recover an unknown function using such dictionaries. One of the most popular methods, Lasso and its versions, is based on minimizing empirical likelihood and, unfortunately, requires stringent assumptions on the dictionary, the, so called, compatibility conditions. While these conditions may be satisfied for the functions in the original dictionary, they usually do not hold for their images due to contraction imposed by the linear operator. Pensky (2016) showed that this difficulty can be bypassed by inverting each of the dictionary functions and matching the resulting expansion to the true function. However, even then the approach requires a compatibility condition which is difficult to check. In the present paper, we propose a solution which utilizes structured and unstructured random dictionaries, the technique that have not been applied so far to the solution of ill-posed linear inverse problems. We put a theoretical foundation under the suggested methodology and study its performance via simulations. version:2
