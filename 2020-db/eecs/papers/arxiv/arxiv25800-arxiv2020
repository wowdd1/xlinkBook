arxiv-1703-02116 | Evaluation of Machine Learning Methods to Predict Coronary Artery Disease Using Metabolomic Data | http://arxiv.org/abs/1703.02116 | id:1703.02116 author:Henrietta Forssen, Riyaz S. Patel, Natalie Fitzpatrick, Aroon Hingorani, Adam Timmis, Harry Hemingway, Spiros C. Denaxas category:cs.LG  published:2017-02-28 summary:Metabolomic data can potentially enable accurate, non-invasive and low-cost prediction of coronary artery disease. Regression-based analytical approaches however might fail to fully account for interactions between metabolites, rely on a priori selected input features and thus might suffer from poorer accuracy. Supervised machine learning methods can potentially be used in order to fully exploit the dimensionality and richness of the data. In this paper, we systematically implement and evaluate a set of supervised learning methods (L1 regression, random forest classifier) and compare them to traditional regression-based approaches for disease prediction using metabolomic data. version:1
arxiv-1702-08740 | Weakly- and Semi-Supervised Object Detection with Expectation-Maximization Algorithm | http://arxiv.org/abs/1702.08740 | id:1702.08740 author:Ziang Yan, Jian Liang, Weishen Pan, Jin Li, Changshui Zhang category:cs.CV  published:2017-02-28 summary:Object detection when provided image-level labels instead of instance-level labels (i.e., bounding boxes) during training is an important problem in computer vision, since large scale image datasets with instance-level labels are extremely costly to obtain. In this paper, we address this challenging problem by developing an Expectation-Maximization (EM) based object detection method using deep convolutional neural networks (CNNs). Our method is applicable to both the weakly-supervised and semi-supervised settings. Extensive experiments on PASCAL VOC 2007 benchmark show that (1) in the weakly supervised setting, our method provides significant detection performance improvement over current state-of-the-art methods, (2) having access to a small number of strongly (instance-level) annotated images, our method can almost match the performace of the fully supervised Fast RCNN. We share our source code at https://github.com/ZiangYan/EM-WSD. version:1
arxiv-1702-08734 | Billion-scale similarity search with GPUs | http://arxiv.org/abs/1702.08734 | id:1702.08734 author:Jeff Johnson, Matthijs Douze, Hervé Jégou category:cs.CV cs.DB cs.DS  published:2017-02-28 summary:Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility. version:1
arxiv-1702-08727 | Improving the Neural GPU Architecture for Algorithm Learning | http://arxiv.org/abs/1702.08727 | id:1702.08727 author:Karlis Freivalds, Renars Liepins category:cs.NE  published:2017-02-28 summary:Algorithm learning is a core problem in artificial intelligence with significant implications on automation level that can be achieved by machines. Recently deep learning methods are emerging for synthesizing an algorithm from its input-output examples, the most successful being the Neural GPU, capable of learning multiplication. We present several improvements to the Neural GPU that substantially reduces training time and improves generalization. We introduce a technique of general applicability to use hard nonlinearities with saturation cost. We also introduce a technique of diagonal gates that can be applied to active-memory models. The proposed architecture is the first capable of learning decimal multiplication end-to-end. version:1
arxiv-1702-08513 | Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work | http://arxiv.org/abs/1702.08513 | id:1702.08513 author:Nizar Massouh, Francesca Babiloni, Tatiana Tommasi, Jay Young, Nick Hawes, Barbara Caputo category:cs.CV cs.DB cs.LG cs.RO  published:2017-02-28 summary:Deep networks thrive when trained on large scale data collections. This has given ImageNet a central role in the development of deep architectures for visual object classification. However, ImageNet was created during a specific period in time, and as such it is prone to aging, as well as dataset bias issues. Moving beyond fixed training datasets will lead to more robust visual systems, especially when deployed on robots in new environments which must train on the objects they encounter there. To make this possible, it is important to break free from the need for manual annotators. Recent work has begun to investigate how to use the massive amount of images available on the Web in place of manual image annotations. We contribute to this research thread with two findings: (1) a study correlating a given level of noisily labels to the expected drop in accuracy, for two deep architectures, on two different types of noise, that clearly identifies GoogLeNet as a suitable architecture for learning from Web data; (2) a recipe for the creation of Web datasets with minimal noise and maximum visual variability, based on a visual and natural language processing concept expansion strategy. By combining these two results, we obtain a method for learning powerful deep object models automatically from the Web. We confirm the effectiveness of our approach through object categorization experiments using our Web-derived version of ImageNet on a popular robot vision benchmark database, and on a lifelong object discovery task on a mobile robot. version:1
arxiv-1702-08717 | An Extensive Technique to Detect and Analyze Melanoma: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2017 | http://arxiv.org/abs/1702.08717 | id:1702.08717 author:G Wiselin Jiji, P Johnson Durai Raj category:cs.CV  published:2017-02-28 summary:An automated method to detect and analyze the melanoma is presented to improve diagnosis which will leads to the exact treatment. Image processing techniques such as segmentation, feature descriptors and classification models are involved in this method. In the First phase the lesion region is segmented using CIELAB Color space Based Segmentation. Then feature descriptors such as shape, color and texture are extracted. Finally, in the third phase lesion region is classified as melanoma, seborrheic keratosis or nevus using multi class O-A SVM model. Experiment with ISIC 2017 Archive skin image database has been done and analyzed the results. version:1
arxiv-1702-08712 | Algorithmic stability and hypothesis complexity | http://arxiv.org/abs/1702.08712 | id:1702.08712 author:Tongliang Liu, Gábor Lugosi, Gergely Neu, Dacheng Tao category:stat.ML cs.LG  published:2017-02-28 summary:We introduce a notion of algorithmic stability of learning algorithms---that we term hypothesis stability---that captures stability of the hypothesis output by the learning algorithm in the normed space of functions from which hypotheses are selected. The main result of the paper bounds the generalization error of any learning algorithm in terms of its hypothesis stability. The bounds are based on martingale inequalities in the Banach space to which the hypotheses belong. We apply the general bounds to bound the performance of some learning algorithms based on empirical risk minimization and stochastic gradient descent. version:1
arxiv-1702-08704 | Optimal algorithms for smooth and strongly convex distributed optimization in networks | http://arxiv.org/abs/1702.08704 | id:1702.08704 author:Kevin Scaman, Francis Bach, Sébastien Bubeck, Yin Tat Lee, Laurent Massoulié category:math.OC stat.ML  published:2017-02-28 summary:In this paper, we determine the optimal convergence rates for strongly convex and smooth distributed optimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. master/slave) algorithms, we show that distributing Nesterov's accelerated gradient descent is optimal and achieves a precision $\varepsilon \textgreater{} 0$ in time $O(\sqrt{\kappa\_g}(1+\Delta\tau)\ln(1/\varepsilon))$, where $\kappa\_g$ is the condition number of the (global) function to optimize, $\Delta$ is the diameter of the network, and $\tau$ (resp. $1$) is the time needed to communicate values between two neighbors (resp. perform local computations). For decentralized algorithms based on gossip, we provide the first optimal algorithm, called the multi-step dual accelerated (MSDA) method, that achieves a precision $\varepsilon \textgreater{} 0$ in time $O(\sqrt{\kappa\_l}(1+\frac{\tau}{\sqrt{\gamma}})\ln(1/\varepsilon))$, where $\kappa\_l$ is the condition number of the local functions and $\gamma$ is the (normalized) eigengap of the gossip matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two problems: least-squares regression and classification by logistic regression. version:1
arxiv-1702-08701 | Learning rates for classification with Gaussian kernels | http://arxiv.org/abs/1702.08701 | id:1702.08701 author:Shao-Bo Lin, Jinshan Zeng, Xiangyu Chang category:cs.LG math.OC stat.ML  published:2017-02-28 summary:This paper aims at refined error analysis for binary classification using support vector machine (SVM) with Gaussian kernel and convex loss. Our first result shows that for some loss functions such as the logistic loss and exponential loss, SVM with Gaussian kernel can reach the almost optimal learning rate, provided the regression function is smooth. Our second result shows that, for a large number of loss functions, under some Tsybakov noise assumption, if the regression function is infinitely smooth, then SVM with Gaussian kernel can achieve the learning rate of order $m^{-1}$, where $m$ is the number of samples. version:1
arxiv-1702-08694 | Significant Pattern Mining on Continuous Variables | http://arxiv.org/abs/1702.08694 | id:1702.08694 author:Mahito Sugiyama, Karsten M. Borgwardt category:stat.ML cs.LG stat.ME  published:2017-02-28 summary:Significant pattern mining, the search for sets of binary features that are statistically significantly enriched in a class of objects, is of fundamental importance in a wide range of applications from economics to statistical genetics. Still, all existing approaches make the restrictive assumption that the features are binary and require a binarization of continuous data during preprocessing, which often leads to a loss of information. Here, we solve the open problem of significant pattern mining on continuous variables. Our approach detects all patterns that are statistically significantly associated with a class of interest, while rigorously correcting for multiple testing. Key to this approach is the use of Spearman's rank correlation coefficient to represent the frequency of a pattern. Our experiments demonstrate that our novel approach detects true patterns with higher precision and recall than competing methods that require a prior binarization of the data. version:1
arxiv-1702-08692 | Cascade one-vs-rest detection network for fine-grained recognition without part annotations | http://arxiv.org/abs/1702.08692 | id:1702.08692 author:Long Chen, Junyu Dong, ShengKe Wang, Kin-Man Lam, Muwei Jian, Hua Zhang, XiaoChun Cao category:cs.CV  published:2017-02-28 summary:Fine-grained recognition is a challenging task due to the small intra-category variances. Most of top-performing fine-grained recognition methods leverage parts of objects for better performance. Therefore, part annotations which are extremely computationally expensive are required. In this paper, we propose a novel cascaded deep CNN detection framework for fine-grained recognition which is trained to detect the whole object without considering parts. Nevertheless, most of current top-performing detection networks use the N+1 class (N object categories plus background) softmax loss, and the background category with much more training samples dominates the feature learning progress so that the features are not good for object categories with fewer samples. To bridge this gap, we introduce a cascaded structure to eliminate background and exploit a one-vs-rest loss to capture more minute variances among different subordinate categories. Experiments show that our proposed recognition framework achieves comparable performance with state-of-the-art, part-free, fine-grained recognition methods on the CUB-200-2011 Bird dataset. Moreover, our method even outperforms most of part-based methods while does not need part annotations at the training stage and is free from any annotations at test stage. version:1
arxiv-1702-08690 | Borrowing Treasures from the Wealthy: Deep Transfer Learning through Selective Joint Fine-tuning | http://arxiv.org/abs/1702.08690 | id:1702.08690 author:Weifeng Ge, Yizhou Yu category:cs.CV cs.AI cs.LG cs.NE  published:2017-02-28 summary:Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task. Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model. version:1
arxiv-1702-08681 | MIML-FCN+: Multi-instance Multi-label Learning via Fully Convolutional Networks with Privileged Information | http://arxiv.org/abs/1702.08681 | id:1702.08681 author:Hao Yang, Joey Tianyi Zhou, Jianfei Cai, Yew Soon Ong category:cs.CV  published:2017-02-28 summary:Multi-instance multi-label (MIML) learning has many interesting applications in computer visions, including multi-object recognition and automatic image tagging. In these applications, additional information such as bounding-boxes, image captions and descriptions is often available during training phrase, which is referred as privileged information (PI). However, as existing works on learning using PI only consider instance-level PI (privileged instances), they fail to make use of bag-level PI (privileged bags) available in MIML learning. Therefore, in this paper, we propose a two-stream fully convolutional network, named MIML-FCN+, unified by a novel PI loss to solve the problem of MIML learning with privileged bags. Compared to the previous works on PI, the proposed MIML-FCN+ utilizes the readily available privileged bags, instead of hard-to-obtain privileged instances, making the system more general and practical in real world applications. As the proposed PI loss is convex and SGD compatible and the framework itself is a fully convolutional network, MIML-FCN+ can be easily integrated with state of-the-art deep learning networks. Moreover, the flexibility of convolutional layers allows us to exploit structured correlations among instances to facilitate more effective training and testing. Experimental results on three benchmark datasets demonstrate the effectiveness of the proposed MIML-FCN+, outperforming state-of-the-art methods in the application of multi-object recognition. version:1
arxiv-1702-08675 | 3D Shape Segmentation via Shape Fully Convolutional Networks | http://arxiv.org/abs/1702.08675 | id:1702.08675 author:Pengyu Wang, Yuan Gan, Yan Zhang, Panpan Shui category:cs.CV  published:2017-02-28 summary:We propose a novel fully convolutional networks architecture for shapes, denoted as Shape Fully Convolutional Networks (SFCN). Similar to convolution and pooling operation on image, the 3D shape is represented as a graph structure in the SFCN architecture, based on which we first propose and implement shape convolution and pooling operation. Meanwhile, to build our SFCN architecture in the original image segmentation FCN architecture, we also design and implement the generating operation with bridging function. This ensures that the convolution and pooling operation we designed can be successfully applied in the original FCN architecture. In this paper,we also present a new shape segmentation based on SFCN. In contrast to existing state-of-the-art shape segmentation methods that require the same types of shapes as input, we allow the more general and challenging input such as mixed datasets of different types of shapes. In our approach, SFCNs are first trained end-to-end, triangles-to-triangles by three low-level geometric features. Then, based on the trained SFCNs, we can complete the shape segmentation task with high quality. Finally, The feature voting-based multilabel graph cuts is adopted to optimize the segmentation results obtained by SFCN prediction. The experiment results show that our method can effectively learn and predict mixed shape datasets of either similar or different characters, and achieve excellent segmentation results. version:1
arxiv-1702-08670 | On architectural choices in deep learning: From network structure to gradient convergence and parameter estimation | http://arxiv.org/abs/1702.08670 | id:1702.08670 author:Vamsi K Ithapu, Sathya N Ravi, Vikas Singh category:cs.LG math.OC stat.ML  published:2017-02-28 summary:We study mechanisms to characterize how the asymptotic convergence of backpropagation in deep architectures, in general, is related to the network structure, and how it may be influenced by other design choices including activation type, denoising and dropout rate. We seek to analyze whether network architecture and input data statistics may guide the choices of learning parameters and vice versa. Given the broad applicability of deep architectures, this issue is interesting both from theoretical and a practical standpoint. Using properties of general nonconvex objectives (with first-order information), we first build the association between structural, distributional and learnability aspects of the network vis-\`a-vis their interaction with parameter convergence rates. We identify a nice relationship between feature denoising and dropout, and construct families of networks that achieve the same level of convergence. We then derive a workflow that provides systematic guidance regarding the choice of network sizes and learning parameters often mediated4 by input statistics. Our technical results are corroborated by an extensive set of evaluations, presented in this paper as well as independent empirical observations reported by other groups. We also perform experiments showing the practical implications of our framework for choosing the best fully-connected design for a given problem. version:1
arxiv-1702-08658 | Towards Deeper Understanding of Variational Autoencoding Models | http://arxiv.org/abs/1702.08658 | id:1702.08658 author:Shengjia Zhao, Jiaming Song, Stefano Ermon category:cs.LG stat.ML  published:2017-02-28 summary:We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential VAE model that can generate sharp samples on the LSUN image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features. version:1
arxiv-1702-08653 | Scaffolding Networks for Teaching and Learning to Comprehend | http://arxiv.org/abs/1702.08653 | id:1702.08653 author:Asli Celikyilmaz, Li Deng, Lihong Li, Chong Wang category:cs.CL  published:2017-02-28 summary:In scaffolding teaching, students are gradually asked questions to build background knowledge, clear up confusions, learn to be attentive, and improve comprehension. Inspired by this approach, we explore methods for teaching machines to learn to reason over text documents through asking questions about the past information. We address three key challenges in teaching and learning to reason: 1) the need for an effective architecture that learns from the information in text and keeps it in memory; 2) the difficulty of self-assessing what is learned at any given point and what is left to be learned; 3) the difficulty of teaching reasoning in a scalable way. To address the first challenge, we present the Scaffolding Network, an attention-based neural network agent that can reason over a dynamic memory. It learns a policy using reinforcement learning to incrementally register new information about concepts and their relations. For the second challenge, we describe a question simulator as part of the scaffolding network that learns to continuously question the agent about the information processed so far. Through questioning, the agent learns to correctly answer as many questions as possible. For the last challenge, we explore training with reduced annotated data. We evaluate on synthetic and real datasets, demonstrating that our model competes well with the state-of-the-art methods, especially when less supervision is used. version:1
arxiv-1702-08651 | Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimizations | http://arxiv.org/abs/1702.08651 | id:1702.08651 author:Pan Xu, Jian Ma, Quanquan Gu category:stat.ML cs.LG  published:2017-02-28 summary:We study the estimation of the latent variable Gaussian graphical model (LVGGM), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix. In order to speed up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it. Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. In addition, we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision. Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory. version:1
arxiv-1702-08648 | Deep Clustering using Auto-Clustering Output Layer | http://arxiv.org/abs/1702.08648 | id:1702.08648 author:Ozsel Kilinc, Ismail Uysal category:cs.LG  published:2017-02-28 summary:In this paper, we propose a novel method to enrich the representation provided to the output layer of feedforward neural networks in the form of an auto-clustering output layer (ACOL) which enables the network to naturally create sub-clusters under the provided main class la- bels. In addition, a novel regularization term is introduced which allows ACOL to encourage the neural network to reveal its own explicit clustering objective. While the underlying process of finding the subclasses is completely unsupervised, semi-supervised learning is also possible based on the provided classification objective. The results show that ACOL can achieve a 99.2% clustering accuracy for the semi-supervised case when partial class labels are presented and a 96% accuracy for the unsupervised clustering case. These findings represent a paradigm shift especially when it comes to harnessing the power of deep networks for primary and secondary clustering applications in large datasets. version:1
arxiv-1702-08646 | Joint Spatio-Temporal Boundary Detection and Boundary Flow Prediction with a Fully Convolutional Siamese Network | http://arxiv.org/abs/1702.08646 | id:1702.08646 author:Peng Lei, Fuxin Li, Sinisa Todorovic category:cs.CV  published:2017-02-28 summary:This paper addresses a new problem of joint object boundary detection and boundary motion estimation in videos, which we named boundary flow estimation. Boundary flow is an important mid-level visual cue as boundaries characterize spatial extents of objects, and the flow indicates motions and interactions of objects. Yet, most prior work on motion estimation has focused on dense object motion or feature points that may not necessarily reside on boundaries. For boundary flow estimation, we specify a new fully convolutional Siamese network (FCSN) that jointly estimates object-level boundaries in two consecutive frames. Boundary correspondences in the two frames are predicted by the same FCSN with a new, unconventional deconvolution approach. Finally, the boundary flow estimate is improved with an edgelet based filtering. Evaluation is conducted on three tasks: boundary detection in videos, boundary flow estimation, and optical flow estimation. On boundary detection, we achieve the state-of-the-art performance on the benchmark VSB100 dataset. On boundary flow estimation, we present the first results on the Sintel training dataset. For optical flow estimation, we run the recent approach CPM-Flow but on the augmented input with our boundary flow matches, and achieve significant performance improvement on the Sintel benchmark. version:1
arxiv-1702-08635 | Learning What Data to Learn | http://arxiv.org/abs/1702.08635 | id:1702.08635 author:Yang Fan, Fei Tian, Tao Qin, Jiang Bian, Tie-Yan Liu category:cs.LG cs.AI stat.ML  published:2017-02-28 summary:Machine learning is essentially the sciences of playing with data. An adaptive data selection strategy, enabling to dynamically choose different data at various training stages, can reach a more effective model in a more efficient way. In this paper, we propose a deep reinforcement learning framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter} (\textbf{NDF}), to explore automatic and adaptive data selection in the training process. In particular, NDF takes advantage of a deep neural network to adaptively select and filter important data instances from a sequential stream of training data, such that the future accumulative reward (e.g., the convergence speed) is maximized. In contrast to previous studies in data selection that is mainly based on heuristic strategies, NDF is quite generic and thus can be widely suitable for many machine learning tasks. Taking neural network training with stochastic gradient descent (SGD) as an example, comprehensive experiments with respect to various neural network modeling (e.g., multi-layer perceptron networks, convolutional neural networks and recurrent neural networks) and several applications (e.g., image classification and text understanding) demonstrate that NDF powered SGD can achieve comparable accuracy with standard SGD process by using less data and fewer iterations. version:1
arxiv-1702-08628 | Analysis of Agent Expertise in Ms. Pac-Man using Value-of-Information-based Policies | http://arxiv.org/abs/1702.08628 | id:1702.08628 author:Isaac J. Sledge, Jose C. Principe category:cs.LG cs.AI cs.IT math.IT  published:2017-02-28 summary:Conventional reinforcement learning methods for Markov decision processes rely on weakly-guided, stochastic searches to drive the learning process. It can therefore be difficult to predict what agent behaviors might emerge. In this paper, we consider an information-theoretic approach for performing constrained stochastic searches that promote the formation of risk-averse to risk-favoring behaviors. Our approach is based on the value of information, a criterion that provides an optimal trade-off between the expected return of a policy and the policy's complexity. As the policy complexity is reduced, there is a high chance that the agents will eschew risky actions that increase the long-term rewards. The agents instead focus on simply completing their main objective in an expeditious fashion. As the policy complexity increases, the agents will take actions, regardless of the risk, that seek to decrease the long-term costs. A minimal-cost policy is sought in either case; the obtainable cost depends on a single, tunable parameter that regulates the degree of policy complexity. We evaluate the performance of value-of-information-based policies on a stochastic version of Ms. Pac-Man. A major component of this paper is demonstrating that ranges of policy complexity values yield different game-play styles and analyzing why this occurs. We show that low-complexity policies aim to only clear the environment of pellets while avoiding invulnerable ghosts. Higher-complexity policies implement multi-modal strategies that compel the agent to seek power-ups and chase after vulnerable ghosts, both of which reduce the long-term costs. version:1
arxiv-1702-08627 | An Inexact Proximal Alternating Direction Method for Non-convex and Non-smooth Matrix Factorization and Beyond | http://arxiv.org/abs/1702.08627 | id:1702.08627 author:Yiyang Wang, Risheng Liu, Xiaoliang Song, Zhixun Su category:cs.CV math.OC  published:2017-02-28 summary:Since Non-convex and Non-smooth Matrix Factorization (NNMF) problems have great realistic significance in applications, they attract extensive attention in the fields of image processing and machine learning. We in this paper propose an inexact proximal alternating direction (IPAD) method for solving various complex NNMF problems. Our IPAD method is not a single algorithm, but a general and flexible framework which can fuse various numerical methods into. With a special designed error condition, the convergence properties of IPAD are analyzed for a general formulation, and can be extended to a wider range of problems. Moreover, an implementation method for checking the inexactness criterion is theoretically analyzed, which is more valid than the previously naive criteria in practice. Our IPAD algorithm is applied to a widely-concerned sparse dictionary learning problem on both synthetic and real-world data. The experimental results with detailed analyses and discussions are given to verify the efficiency of IPAD method. version:1
arxiv-1702-08626 | Show, Attend and Interact: Perceivable Human-Robot Social Interaction through Neural Attention Q-Network | http://arxiv.org/abs/1702.08626 | id:1702.08626 author:Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, Hiroshi Ishiguro category:cs.RO cs.AI cs.CV stat.ML  published:2017-02-28 summary:For a safe, natural and effective human-robot social interaction, it is essential to develop a system that allows a robot to demonstrate the perceivable responsive behaviors to complex human behaviors. We introduce the Multimodal Deep Attention Recurrent Q-Network using which the robot exhibits human-like social interaction skills after 14 days of interacting with people in an uncontrolled real world. Each and every day during the 14 days, the system gathered robot interaction experiences with people through a hit-and-trial method and then trained the MDARQN on these experiences using end-to-end reinforcement learning approach. The results of interaction based learning indicate that the robot has learned to respond to complex human behaviors in a perceivable and socially acceptable manner. version:1
arxiv-1702-08623 | Process Progress Estimation and Phase Detection | http://arxiv.org/abs/1702.08623 | id:1702.08623 author:Xinyu Li, Yanyi Zhang, Jianyu Zhang, Yueyang Chen, Shuhong Chen, Yue Gu, Moliang Zhou, Richard A. Farneth, Ivan Marsic, Randall S. Burd category:cs.LG cs.HC  published:2017-02-28 summary:Process modeling and understanding is fundamental for advanced human-computer interfaces and automation systems. Recent research focused on activity recognition, but little work has focused on process progress detection from sensor data. We introduce a real-time, sensor-based system for modeling, recognizing and estimating the completeness of a process. We implemented a multimodal CNN-LSTM structure to extract the spatio-temporal features from different sensory datatypes. We used a novel deep regression structure for overall completeness estimation. By combining process completeness estimation with a Gaussian mixture model, our system can predict the process phase using the estimated completeness. We also introduce the rectified hyperbolic tangent (rtanh) activation function and conditional loss to help the training process. Using the completeness estimation result and performance speed calculations, we also implemented an online estimator of remaining time. We tested this system using data obtained from a medical process (trauma resuscitation) and sport events (swim competition). Our system outperformed existing implementations for phase prediction during trauma resuscitation and achieved over 80% of process phase detection accuracy with less than 9% completeness estimation error and time remaining estimation error less than 18% of duration in both dataset. version:1
arxiv-1702-08606 | The Active Atlas: Combining 3D Anatomical Models with Texture Detectors | http://arxiv.org/abs/1702.08606 | id:1702.08606 author:Yuncong Chen, David Kleinfeld, Yoav Freund category:cs.CV  published:2017-02-28 summary:While modern imaging technologies such as fMRI have opened exciting new possibilities for studying the brain in vivo, histological sections remain the best way to study the anatomy of the brain at the level of single neurons. The histological atlas changed little since 1909 and localizing brain regions is a still a labor intensive process performed only by experienced neuro-anatomists. Existing digital atlases such as the Allen Brain atlas are limited to low resolution images which cannot identify the detailed structure of the neurons. We have developed a digital atlas methodology that combines information about the 3D organization of the brain and the detailed texture of neurons in different structures. Using the methodology we developed an atlas for the mouse brainstem and mid-brain, two regions for which there are currently no good atlases. Our atlas is "active" in that it can be used to automatically align a histological stack to the atlas, thus reducing the work of the neuroanatomist. version:1
arxiv-1702-08601 | Accurate, Scalable and Parallel Structure from Motion | http://arxiv.org/abs/1702.08601 | id:1702.08601 author:Siyu Zhu, Tianwei Shen, Lei Zhou, Runze Zhang, Tian Fang, Long Quan category:cs.CV  published:2017-02-28 summary:In this paper, we tackle the accurate Structure from Motion (SfM) problem, in particular camera registration, far exceeding the memory of a single compute node. Different from the previous methods which drastically simplify the parameters of SfM, we preserve as many cameras, tracks and their corresponding connectivity as possible for a highly consistent and accurate SfM. By means of a camera clustering algorithm, we divide all the cameras and associated images into clusters and leverage such formulation to process the subsequent track generation, local incremental SfM and final bundle adjustment in a scalable and parallel scheme. Taking the advantages of both incremental and global SfM methods, we apply the relative motions from local incremental SfM to the global motion averaging framework and obtain more accurate and robust global camera poses than the state-of-the-art methods. We intensively demonstrate the superior performance of our method on the benchmark, Internet and our own challenging city-scale data-sets. version:1
arxiv-1702-08597 | Enabling Sparse Winograd Convolution by Native Pruning | http://arxiv.org/abs/1702.08597 | id:1702.08597 author:Sheng Li, Jongsoo Park, Ping Tak Peter Tang category:cs.CV  published:2017-02-28 summary:Sparse methods and the use of Winograd convolutions are two orthogonal approaches each of which significantly accelerates convolution computations in modern CNNs. Sparse Winograd merges these two and thus has the potential to offer a combined performance benefit. Nevertheless, training convolution layers so that the resulting Winograd kernels are sparse has not hitherto been very successful. We introduce here a novel construction of Sparse Winograd by introducing a substitute for the convolution layer that we call, naturally, the Winograd layer. Such a construction allows us to prune parameters natively in the Winograd domain. This paper presents the technical details related to the Winograd layer and our train-and-prune procedures. We achieved more than 90% sparsity in the Winograd parameters while maintaining the original accuracy of AlexNet on ImageNet dataset. Our detailed performance model also projects a speedup over convolution by dense Winograd kernels in excess of twofold. version:1
arxiv-1702-08591 | The Shattered Gradients Problem: If resnets are the answer, then what is the question? | http://arxiv.org/abs/1702.08591 | id:1702.08591 author:David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, Brian McWilliams category:cs.NE cs.LG stat.ML  published:2017-02-28 summary:A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. The problem has largely been overcome through the introduction of carefully constructed initializations and batch normalization. Nevertheless, architectures incorporating skip-connections such as resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise. In contrast, the gradients in architectures with skip-connections are far more resistant to shattering decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new "looks linear" (LL) initialization that prevents shattering. Preliminary experiments show the new initialization allows to train very deep networks without the addition of skip-connections. version:1
arxiv-1702-08586 | Can Boltzmann Machines Discover Cluster Updates ? | http://arxiv.org/abs/1702.08586 | id:1702.08586 author:Lei Wang category:physics.comp-ph cond-mat.stat-mech cs.LG stat.ML  published:2017-02-28 summary:Boltzmann machines are physics informed generative models with wide applications in machine learning. They can learn the probability distribution from an input dataset and generate new samples accordingly. Applying them back to physics, the Boltzmann machines are ideal recommender systems to accelerate Monte Carlo simulation of physical systems due to their flexibility and effectiveness. More intriguingly, we show that the generative sampling of the Boltzmann Machines can even discover unknown cluster Monte Carlo algorithms. The creative power comes from the latent representation of the Boltzmann machines, which learn to mediate complex interactions and identify clusters of the physical system. We demonstrate these findings with concrete examples of the classical Ising model with and without four spin plaquette interactions. Our results endorse a fresh research paradigm where intelligent machines are designed to create or inspire human discovery of innovative algorithms. version:1
arxiv-1702-08580 | Depth Creates No Bad Local Minima | http://arxiv.org/abs/1702.08580 | id:1702.08580 author:Haihao Lu, Kenji Kawaguchi category:cs.LG cs.NE math.OC stat.ML  published:2017-02-27 summary:In deep learning, \textit{depth}, as well as \textit{nonlinearity}, create non-convex loss surfaces. Then, does depth alone create bad local minima? In this paper, we prove that without nonlinearity, depth alone does not create bad local minima, although it induces non-convex loss surface. Using this insight, we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima. Our theoretical result generalizes previous results with fewer assumptions. version:1
arxiv-1702-08575 | Learning Latent Networks in Vector Auto Regressive Models | http://arxiv.org/abs/1702.08575 | id:1702.08575 author:Saber Salehkaleybar, Jalal Etesami, Negar Kiyavash category:cs.LG stat.ML  published:2017-02-27 summary:We study the problem of learning the dependency graph between random processes in a vector auto regressive (VAR) model from samples when a subset of the variables are latent. We show that the dependencies among the observed processes can be identified successfully under some conditions on the VAR model. Moreover, we can recover the length of all directed paths between any two observed processes which pass through latent part. By utilizing this information, we reconstruct the latent subgraph with minimum number of nodes uniquely if its topology is a directed tree. Furthermore, we propose an algorithm that finds all possible minimal latent networks if there exists at most one directed path of each length between any two observed nodes through the latent part. Experimental results on various synthetic and real-world datasets validate our theoretical results. version:1
arxiv-1702-08568 | eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys | http://arxiv.org/abs/1702.08568 | id:1702.08568 author:Joshua Saxe, Konstantin Berlin category:cs.CR cs.LG  published:2017-02-27 summary:For years security machine learning research has promised to obviate the need for signature based detection by automatically learning to detect indicators of attack. Unfortunately, this vision hasn't come to fruition: in fact, developing and maintaining today's security machine learning systems can require engineering resources that are comparable to that of signature-based detection systems, due in part to the need to develop and continuously tune the "features" these machine learning systems look at as attacks evolve. Deep learning, a subfield of machine learning, promises to change this by operating on raw input signals and automating the process of feature design and extraction. In this paper we propose the eXpose neural network, which uses a deep learning approach we have developed to take generic, raw short character strings as input (a common case for security inputs, which include artifacts like potentially malicious URLs, file paths, named pipes, named mutexes, and registry keys), and learns to simultaneously extract features and classify using character-level embeddings and convolutional neural network. In addition to completely automating the feature design and extraction process, eXpose outperforms manual feature extraction based baselines on all of the intrusion detection problems we tested it on, yielding a 5%-10% detection rate gain at 0.1% false positive rate compared to these baselines. version:1
arxiv-1702-08567 | Optimal Experiment Design for Causal Discovery from Fixed Number of Experiments | http://arxiv.org/abs/1702.08567 | id:1702.08567 author:AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash category:cs.LG cs.AI stat.ML  published:2017-02-27 summary:We study the problem of causal structure learning over a set of random variables when the experimenter is allowed to perform at most $M$ experiments in a non-adaptive manner. We consider the optimal learning strategy in terms of minimizing the portions of the structure that remains unknown given the limited number of experiments in both Bayesian and minimax setting. We characterize the theoretical optimal solution and propose an algorithm, which designs the experiments efficiently in terms of time complexity. We show that for bounded degree graphs, in the minimax case and in the Bayesian case with uniform priors, our proposed algorithm is a $\rho$-approximation algorithm, where $\rho$ is independent of the order of the underlying graph. Simulations on both synthetic and real data show that the performance of our algorithm is very close to the optimal solution. version:1
arxiv-1702-08565 | Nearly Maximally Predictive Features and Their Dimensions | http://arxiv.org/abs/1702.08565 | id:1702.08565 author:Sarah E. Marzen, James P. Crutchfield category:cond-mat.stat-mech cs.IT math.IT nlin.CD stat.ML  published:2017-02-27 summary:Scientific explanation often requires inferring maximally predictive features from a given data set. Unfortunately, the collection of minimal maximally predictive features for most stochastic processes is uncountably infinite. In such cases, one compromises and instead seeks nearly maximally predictive features. Here, we derive upper-bounds on the rates at which the number and the coding cost of nearly maximally predictive features scales with desired predictive power. The rates are determined by the fractal dimensions of a process' mixed-state distribution. These results, in turn, show how widely-used finite-order Markov models can fail as predictors and that mixed-state predictive features offer a substantial improvement. version:1
arxiv-1702-08563 | Improving Machine Learning Ability with Fine-Tuning | http://arxiv.org/abs/1702.08563 | id:1702.08563 author:John P. Lalor, Hao Wu, Hong Yu category:cs.CL  published:2017-02-27 summary:Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose fine-tuning as a new training process, where a model pre-trained on a large dataset is fine-tuned with a small supplemental training set. Our results show that fine-tuning can improve the ability of a state-of-the-art DNN model for Recognizing Textual Entailment tasks. version:1
arxiv-1702-08558 | DepthSynth: Real-Time Realistic Synthetic Data Generation from CAD Models for 2.5D Recognition | http://arxiv.org/abs/1702.08558 | id:1702.08558 author:Benjamin Planche, Ziyan Wu, Kai Ma, Shanhui Sun, Stefan Kluckner, Terrence Chen, Andreas Hutter, Sergey Zakharov, Harald Kosch, Jan Ernst category:cs.CV  published:2017-02-27 summary:Recent progress in computer vision has been dominated by deep neural networks trained with large amount of labeled data. Collecting and annotating such datasets is however a tedious, and in some contexts impossible task; hence a recent surge in approaches that rely solely on synthetically generated data from 3D models for their training. For depth images however, the discrepancies with real scans noticeably affect the performance of such methods. In this paper, we propose an innovative end-to-end framework which simulate the whole mechanism of these devices, synthetically generating realistic depth data from 3D CAD models by comprehensively modeling vital factors such as sensor noise, material reflectance, surface geometry, etc. Besides covering a wider range of sensors than state-of-the-art methods, the proposed one also results in more realistic data. Going further than previous works, we not only qualitatively evaluate the generated scans, but also quantitatively measure through extensive experiments and comparisons how they impact the training of neural network algorithms for different 3D recognition tasks, demonstrating how our pipeline seamlessly integrates such architectures; and how it consistently and significantly enhances their performance-irrespective of the selected feature space or intermediate representations. version:1
arxiv-1702-08557 | Multimodal Clustering for Community Detection | http://arxiv.org/abs/1702.08557 | id:1702.08557 author:Dmitry I. Ignatov, Alexander Semenov, Daria Komissarova, Dmitry V. Gnatyshak category:cs.SI cs.DM stat.ML 62H30  91C20  62H30 I.5.3; J.4  published:2017-02-27 summary:Multimodal clustering is an unsupervised technique for mining interesting patterns in $n$-adic binary relations or $n$-mode networks. Among different types of such generalized patterns one can find biclusters and formal concepts (maximal bicliques) for 2-mode case, triclusters and triconcepts for 3-mode case, closed $n$-sets for $n$-mode case, etc. Object-attribute biclustering (OA-biclustering) for mining large binary datatables (formal contexts or 2-mode networks) arose by the end of the last decade due to intractability of computation problems related to formal concepts; this type of patterns was proposed as a meaningful and scalable approximation of formal concepts. In this paper, our aim is to present recent advance in OA-biclustering and its extensions to mining multi-mode communities in SNA setting. We also discuss connection between clustering coefficients known in SNA community for 1-mode and 2-mode networks and OA-bicluster density, the main quality measure of an OA-bicluster. Our experiments with 2-, 3-, and 4-mode large real-world networks show that this type of patterns is suitable for community detection in multi-mode cases within reasonable time even though the number of corresponding $n$-cliques is still unknown due to computation difficulties. An interpretation of OA-biclusters for 1-mode networks is provided as well. version:1
arxiv-1702-08553 | Diameter-Based Active Learning | http://arxiv.org/abs/1702.08553 | id:1702.08553 author:Christopher Tosh, Sanjoy Dasgupta category:cs.LG stat.ML  published:2017-02-27 summary:To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance. version:1
arxiv-1702-08540 | Active Learning Using Uncertainty Information | http://arxiv.org/abs/1702.08540 | id:1702.08540 author:Yazhou Yang, Marco Loog category:stat.ML cs.LG  published:2017-02-27 summary:Many active learning methods belong to the retraining-based approaches, which select one unlabeled instance, add it to the training set with its possible labels, retrain the classification model, and evaluate the criteria that we base our selection on. However, since the true label of the selected instance is unknown, these methods resort to calculating the average-case or worse-case performance with respect to the unknown label. In this paper, we propose a different method to solve this problem. In particular, our method aims to make use of the uncertainty information to enhance the performance of retraining-based models. We apply our method to two state-of-the-art algorithms and carry out extensive experiments on a wide variety of real-world datasets. The results clearly demonstrate the effectiveness of the proposed method and indicate it can reduce human labeling efforts in many real-life applications. version:1
arxiv-1702-08536 | Fast Threshold Tests for Detecting Discrimination | http://arxiv.org/abs/1702.08536 | id:1702.08536 author:Emma Pierson, Sam Corbett-Davies, Sharad Goel category:stat.ML cs.LG  published:2017-02-27 summary:Threshold tests have recently been proposed as a robust method for detecting bias in lending, hiring, and policing decisions. For example, in the case of credit extensions, these tests aim to estimate the bar for granting loans to white and minority applicants, with a higher inferred threshold for minorities indicative of discrimination. This technique, however, requires fitting a Bayesian latent variable model for which inference is often computationally challenging. Here we develop a method for fitting threshold tests that is more than 75 times faster than the existing approach, reducing computation from hours to minutes. We demonstrate this technique by analyzing 2.7 million police stops of pedestrians in New York City between 2008 and 2012. To achieve these performance gains, we introduce and analyze a flexible family of probability distributions on the interval [0, 1] -- which we call discriminant distributions -- that is computationally efficient to work with. These discriminant distributions may aid inference in a variety of applications beyond threshold tests. version:1
arxiv-1702-08534 | Image Analysis Using a Dual-Tree $M$-Band Wavelet Transform | http://arxiv.org/abs/1702.08534 | id:1702.08534 author:Caroline Chaux, Laurent Duval, Jean-Christophe Pesquet category:physics.data-an cs.CV math.FA  published:2017-02-27 summary:We propose a 2D generalization to the $M$-band case of the dual-tree decomposition structure (initially proposed by N. Kingsbury and further investigated by I. Selesnick) based on a Hilbert pair of wavelets. We particularly address (\textit{i}) the construction of the dual basis and (\textit{ii}) the resulting directional analysis. We also revisit the necessary pre-processing stage in the $M$-band case. While several reconstructions are possible because of the redundancy of the representation, we propose a new optimal signal reconstruction technique, which minimizes potential estimation errors. The effectiveness of the proposed $M$-band decomposition is demonstrated via denoising comparisons on several image types (natural, texture, seismics), with various $M$-band wavelets and thresholding strategies. Significant improvements in terms of both overall noise reduction and direction preservation are observed. version:1
arxiv-1702-08533 | Competing Bandits: Learning under Competition | http://arxiv.org/abs/1702.08533 | id:1702.08533 author:Yishay Mansour, Aleksandrs Slivkins, Zhiwei Steven Wu category:cs.GT cs.LG  published:2017-02-27 summary:Most modern systems strive to learn from interactions with users, and many engage in \emph{exploration}: making potentially suboptimal choices for the sake of acquiring new information. We initiate a study of the interplay between \emph{exploration and competition}---how such systems balance the exploration for learning and the competition for users. Here the users play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing systems. As a model, we consider competition between two multi-armed bandit algorithms faced with the same bandit instance. Users arrive one by one and choose among the two algorithms, so that each algorithm makes progress if and only if it is chosen. We ask whether and to which extent competition incentivizes \emph{innovation}: adoption of better algorithms. We investigate this issue for several models of user response, as we vary the degree of rationality and competitiveness in the model. Effectively, we map out the "competition vs. innovation" relationship, a well-studied theme in economics. version:1
arxiv-1702-08530 | Semi-parametric Network Structure Discovery Models | http://arxiv.org/abs/1702.08530 | id:1702.08530 author:Amir Dezfouli, Edwin V. Bonilla, Richard Nock category:cs.LG stat.ML I.2.6; I.5.1  published:2017-02-27 summary:We propose a network structure discovery model for continuous observations that generalizes linear causal models by incorporating a Gaussian process (GP) prior on a network-independent component, and random sparsity and weight matrices as the network-dependent parameters. This approach provides flexible modeling of network-independent trends in the observations as well as uncertainty quantification around the discovered network structure. We establish a connection between our model and multi-task GPs and develop an efficient stochastic variational inference algorithm for it. Furthermore, we formally show that our approach is numerically stable and in fact numerically easy to carry out almost everywhere on the support of the random variables involved. Finally, we evaluate our model on three applications, showing that it outperforms previous approaches. We provide a qualitative and quantitative analysis of the structures discovered for domains such as the study of the full genome regulation of the yeast Saccharomyces cerevisiae. version:1
arxiv-1702-08503 | SGD Learns the Conjugate Kernel Class of the Network | http://arxiv.org/abs/1702.08503 | id:1702.08503 author:Amit Daniely category:cs.LG cs.DS stat.ML  published:2017-02-27 summary:We show that the standard stochastic gradient decent (SGD) algorithm is guaranteed to learn, in polynomial time, a function that is competitive with the best function in the conjugate kernel space, as defined in Daniely, Frostig and Singer (2016). The result holds for log-depth networks from a rich family of architectures. To the best of our knowledge, it is the first polynomial-time guarantee for the standard neural network learning algorithm for networks of depth $\ge 3$. version:1
arxiv-1702-08502 | Understanding Convolution for Semantic Segmentation | http://arxiv.org/abs/1702.08502 | id:1702.08502 author:Panqu Wang, Pengfei Chen, Ye Yuan, Ding Liu, Zehua Huang, Xiaodi Hou, Garrison Cottrell category:cs.CV  published:2017-02-27 summary:Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are better for practical use. First, we implement dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields of the network to aggregate global information; 2) alleviates what we call the "gridding issue" caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a new state-of-art result of 80.1% mIOU in the test set. We also are state-of-the-art overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Pretrained models are available at https://goo.gl/DQMeun version:1
arxiv-1702-08489 | Depth Separation for Neural Networks | http://arxiv.org/abs/1702.08489 | id:1702.08489 author:Amit Daniely category:cs.LG cs.CC stat.ML  published:2017-02-27 summary:Let $f:\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}\to\mathbb{S}$ be a function of the form $f(\mathbf{x},\mathbf{x}') = g(\langle\mathbf{x},\mathbf{x}'\rangle)$ for $g:[-1,1]\to \mathbb{R}$. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial. Moreover, for many $g$'s, such as $g(x)=\sin(\pi d^3x)$, the number of neurons must be $2^{\Omega\left(d\log(d)\right)}$. Furthermore, the result holds w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$. version:1
arxiv-1702-08484 | Boosted Generative Models | http://arxiv.org/abs/1702.08484 | id:1702.08484 author:Aditya Grover, Stefano Ermon category:cs.LG cs.AI stat.ML  published:2017-02-27 summary:We propose a new approach for using unsupervised boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our meta-algorithmic framework can leverage any existing base learner that permits likelihood evaluation, including recent latent variable models. Further, our approach allows the ensemble to include discriminative models trained to distinguish real data from model-generated data. We show theoretical conditions under which incorporating a new model in the ensemble will improve the fit and empirically demonstrate the effectiveness of boosting on density estimation and sample generation on synthetic and benchmark real datasets. version:1
arxiv-1702-08481 | Memory-Efficient Global Refinement of Decision-Tree Ensembles and its Application to Face Alignment | http://arxiv.org/abs/1702.08481 | id:1702.08481 author:Nenad Markuš, Ivan Gogić, Igor S. Pandžić, Jörgen Ahlberg category:cs.CV cs.NE  published:2017-02-27 summary:Ren et al. recently introduced a method for aggregating multiple decision trees into a strong predictor by interpreting a path taken by a sample down each tree as a binary vector and performing linear regression on top of these vectors stacked together. They provided experimental evidence that the method offers advantages over the usual approaches for combining decision trees (random forests and boosting). The method truly shines when the regression target is a large vector with correlated dimensions, such as a 2D face shape represented with the positions of several facial landmarks. However, we argue that their basic method is not applicable in many practical scenarios due to large memory requirements. This paper shows how this issue can be solved through the use of quantization and architectural changes of the predictor that maps decision tree-derived encodings to the desired output. version:1
arxiv-1702-08434 | Skin Lesion Classification Using Hybrid Deep Neural Networks | http://arxiv.org/abs/1702.08434 | id:1702.08434 author:Amirreza Mahbod, Rupert Ecker, Isabella Ellinger category:cs.CV  published:2017-02-27 summary:Skin cancer is one of the major types of cancers and its incidence has been increasing over the past decades. Skin lesions can arise from various dermatologic disorders and can be classified to various types according to their texture, structure, color and other morphological features. The accuracy of diagnosis of skin lesions, specifically the discrimination of benign and malignant lesions, is paramount to ensure appropriate patient treatment. Machine learning-based classification approaches are among popular automatic methods for skin lesion classification. While there are many existing methods, convolutional neural networks (CNN) have shown to be superior over other classical machine learning methods for object detection and classification tasks. In this work, a fully automatic computerized method is proposed, which employs well established pre-trained convolutional neural networks and ensembles learning to classify skin lesions. We trained the networks using 2000 skin lesion images available from the ISIC 2017 challenge, which has three main categories and includes 374 melanoma, 254 seborrheic keratosis and 1372 benign nevi images. The trained classifier was then tested on 150 unlabeled images. The results, evaluated by the challenge organizer and based on the area under the receiver operating characteristic curve (AUC), were 84.8% and 93.6% for Melanoma and seborrheic keratosis binary classification problem, respectively. The proposed method achieved competitive results to experienced dermatologist. Further improvement and optimization of the proposed method with a larger training dataset could lead to a more precise, reliable and robust method for skin lesion classification. version:1
arxiv-1702-08431 | Boundary-Seeking Generative Adversarial Networks | http://arxiv.org/abs/1702.08431 | id:1702.08431 author:R Devon Hjelm, Athul Paul Jacob, Tong Che, Kyunghyun Cho, Yoshua Bengio category:stat.ML cs.LG  published:2017-02-27 summary:We introduce a novel approach to training generative adversarial networks, where we train a generator to match a target distribution that converges to the data distribution at the limit of a perfect discriminator. This objective can be interpreted as training a generator to produce samples that lie on the decision boundary of a current discriminator in training at each update, and we call a GAN trained using this algorithm a boundary-seeking GAN (BS-GAN). This approach can be used to train a generator with discrete output when the generator outputs a parametric conditional distribution. We demonstrate the effectiveness of the proposed algorithm with discrete image data. In contrary to the proposed algorithm, we observe that the recently proposed Gumbel-Softmax technique for re-parametrizing the discrete variables does not work for training a GAN with discrete data. Finally, we notice that the proposed boundary-seeking algorithm works even with continuous variables, and demonstrate its effectiveness with two widely used image data sets, SVHN and CelebA. version:1
arxiv-1702-05683 | SAGA and Restricted Strong Convexity | http://arxiv.org/abs/1702.05683 | id:1702.05683 author:Chao Qu, Yan Li, Huan Xu category:stat.ML  published:2017-02-19 summary:SAGA is a fast incremental gradient method on the finite sum problem and its effectiveness has been tested on a vast of applications. In this paper, we analyze SAGA on a class of non-strongly convex and non-convex statistical problem such as Lasso, group Lasso, Logistic regression with $\ell_1$ regularization, linear regression with SCAD regularization and Correct Lasso. We prove that SAGA enjoys the linear convergence rate up to the statistical estimation accuracy, under the assumption of restricted strong convexity (RSC). It significantly extends the applicability of SAGA in convex and non-convex optimization. version:2
arxiv-1702-08423 | Age Progression/Regression by Conditional Adversarial Autoencoder | http://arxiv.org/abs/1702.08423 | id:1702.08423 author:Zhifei Zhang, Yang Song, Hairong Qi category:cs.CV  published:2017-02-27 summary:"If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5?" The answer is probably a "No." Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth. version:1
arxiv-1702-08420 | Embarrassingly parallel inference for Gaussian processes | http://arxiv.org/abs/1702.08420 | id:1702.08420 author:Michael M. Zhang, Sinead A. Williamson category:stat.ML  published:2017-02-27 summary:Training Gaussian process (GP) based models typically involves an O(N^3) computational bottleneck. Popular methods for overcoming the matrix inversion problem include sparse approximations of the covariance matrix through inducing variables or through dimensionality reduction via "local experts". However, these type of models cannot account for both long and short range correlations in the GP functions. Furthermore, these methods are often ill-suited for cases where the input data are not uniformly distributed. We present an embarrassingly parallel method that takes advantage of the computational ease of inverting block diagonal matrices, while maintaining much of the expressivity of a full covariance matrix. By using importance sampling to average over different realizations of low-rank approximations of the GP model, we ensure our algorithm is both asymptotically unbiased and embarrassingly parallel. version:1
arxiv-1702-08415 | An SDP-Based Algorithm for Linear-Sized Spectral Sparsification | http://arxiv.org/abs/1702.08415 | id:1702.08415 author:Yin Tat Lee, He Sun category:cs.DS cs.LG  published:2017-02-27 summary:For any undirected and weighted graph $G=(V,E,w)$ with $n$ vertices and $m$ edges, we call a sparse subgraph $H$ of $G$, with proper reweighting of the edges, a $(1+\varepsilon)$-spectral sparsifier if \[ (1-\varepsilon)x^{\intercal}L_Gx\leq x^{\intercal} L_{H} x\leq (1+\varepsilon) x^{\intercal} L_Gx \] holds for any $x\in\mathbb{R}^n$, where $L_G$ and $L_{H}$ are the respective Laplacian matrices of $G$ and $H$. Noticing that $\Omega(m)$ time is needed for any algorithm to construct a spectral sparsifier and a spectral sparsifier of $G$ requires $\Omega(n)$ edges, a natural question is to investigate, for any constant $\varepsilon$, if a $(1+\varepsilon)$-spectral sparsifier of $G$ with $O(n)$ edges can be constructed in $\tilde{O}(m)$ time, where the $\tilde{O}$ notation suppresses polylogarithmic factors. All previous constructions on spectral sparsification require either super-linear number of edges or $m^{1+\Omega(1)}$ time. In this work we answer this question affirmatively by presenting an algorithm that, for any undirected graph $G$ and $\varepsilon>0$, outputs a $(1+\varepsilon)$-spectral sparsifier of $G$ with $O(n/\varepsilon^2)$ edges in $\tilde{O}(m/\varepsilon^{O(1)})$ time. Our algorithm is based on three novel techniques: (1) a new potential function which is much easier to compute yet has similar guarantees as the potential functions used in previous references; (2) an efficient reduction from a two-sided spectral sparsifier to a one-sided spectral sparsifier; (3) constructing a one-sided spectral sparsifier by a semi-definite program. version:1
arxiv-1702-08402 | Latent Correlation Gaussian Processes | http://arxiv.org/abs/1702.08402 | id:1702.08402 author:Sami Remes, Markus Heinonen, Samuel Kaski category:stat.ML  published:2017-02-27 summary:We introduce a novel kernel that models input-dependent couplings across multiple latent processes. The pairwise kernel measures covariance both along inputs and across different latent signals in a mutually-dependent fashion. The latent correlation Gaussian process (LCGP) model combines these non-stationary latent components into multiple outputs by an input-dependent mixing matrix. Probit classification and support for multiple observation sets are derived by Variational Bayesian inference. Results on several datasets indicate that the LCGP model can recover the correlations between latent signals while simultaneously achieving state-of-the-art performance. We highlight the latent covariances with an EEG classification dataset where latent brain processes and their couplings simultaneously emerge from the model. version:1
arxiv-1702-08398 | McGan: Mean and Covariance Feature Matching GAN | http://arxiv.org/abs/1702.08398 | id:1702.08398 author:Youssef Mroueh, Tom Sercu, Vaibhava Goel category:cs.LG stat.ML  published:2017-02-27 summary:We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions. version:1
arxiv-1702-08396 | Learning Hierarchical Features from Generative Models | http://arxiv.org/abs/1702.08396 | id:1702.08396 author:Shengjia Zhao, Jiaming Song, Stefano Ermon category:cs.LG stat.ML  published:2017-02-27 summary:Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that certain classes of hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative flat architecture that learns meaningful and disentangled features on natural images. version:1
arxiv-1702-06295 | Convolution Aware Initialization | http://arxiv.org/abs/1702.06295 | id:1702.06295 author:Armen Aghajanyan category:cs.LG stat.ML  published:2017-02-21 summary:Initialization of parameters in deep neural networks has been shown to have a big impact on the performance of the networks (Mishkin & Matas, 2015). The initialization scheme devised by He et al, allowed convolution activations to carry a constrained mean which allowed deep networks to be trained effectively (He et al., 2015a). Orthogonal initializations and more generally orthogonal matrices in standard recurrent networks have been proved to eradicate the vanishing and exploding gradient problem (Pascanu et al., 2012). Majority of current initialization schemes do not take fully into account the intrinsic structure of the convolution operator. Using the duality of the Fourier transform and the convolution operator, Convolution Aware Initialization builds orthogonal filters in the Fourier space, and using the inverse Fourier transform represents them in the standard space. With Convolution Aware Initialization we noticed not only higher accuracy and lower loss, but faster convergence. We achieve new state of the art on the CIFAR10 dataset, and achieve close to state of the art on various other tasks. version:3
arxiv-1702-08389 | Equivariance Through Parameter-Sharing | http://arxiv.org/abs/1702.08389 | id:1702.08389 author:Siamak Ravanbakhsh, Jeff Schneider, Barnabas Poczos category:stat.ML cs.NE  published:2017-02-27 summary:We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group G that acts discretely on the input and output of a standard neural network layer $\phi_W$, we show that equivariance of $\phi_W$ is linked to the symmetry group of network parameters W. We then propose a sparse parameter-sharing scheme to induce the desirable symmetry on W. Under some conditions on the action of G, our procedure for tying the parameters achieves G-equivariance and guarantee sensitivity to all other permutation groups outside G. We demonstrate the relation of our approach to recently-proposed "structured" neural layers such as group-convolution and graph-convolution which leads to new insights and improvement of these operations. version:1
arxiv-1702-08388 | Stance Classification of Social Media Users in Independence Movements | http://arxiv.org/abs/1702.08388 | id:1702.08388 author:Arkaitz Zubiaga, Bo Wang, Maria Liakata, Rob Procter category:cs.CL cs.SI  published:2017-02-27 summary:Social media and data mining are increasingly being used to analyse political and societal issues. Characterisation of users into socio-demographic groups is crucial to improve these analyses. Here we undertake the classification of social media users as supporting or opposing ongoing independence movements in their territories. Independence movements occur in territories whose citizens have conflicting national identities; users with opposing national identities will then support or oppose the sense of being part of an independent nation that differs from the officially recognised country. We describe a methodology that relies on users' self-reported location to build datasets for three territories -- Catalonia, the Basque Country and Scotland -- and we test language-independent classifiers using four types of features. We show the effectiveness of the approach to build large annotated datasets, and the ability to achieve accurate, language-independent classification performances ranging from 85% to 97% for the three territories under study. version:1
arxiv-1702-08379 | Revealing Hidden Potentials of q-Space Imaging in Breast Cancer | http://arxiv.org/abs/1702.08379 | id:1702.08379 author:Paul Jaeger, Sebastian Bickelhaupt, Frederik Bernd Laun, Wolfgang Lederer, Daniel Heidi, Tristan Anselm Kuder, Daniel Paech, David Bonekamp, Alexander Radbruch, Stefan Delorme, Heinz-Peter Schlemmer, Franziska Steudle, Klaus H. Maier-Hein category:cs.CV  published:2017-02-27 summary:Mammography screening for early detection of breast lesions currently suffers from high amounts of false positive findings, which result in unnecessary invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many of these false-positive findings prior to biopsy. Current approaches estimate tissue properties by means of quantitative parameters taken from generative, biophysical models fit to the q-space encoded signal under certain assumptions regarding noise and spatial homogeneity. This process is prone to fitting instability and partial information loss due to model simplicity. We reveal previously unexplored potentials of the signal by integrating all data processing components into a convolutional neural network (CNN) architecture that is designed to propagate clinical target information down to the raw input images. This approach enables simultaneous and target-specific optimization of image normalization, signal exploitation, global representation learning and classification. Using a multicentric data set of 222 patients, we demonstrate that our approach significantly improves clinical decision making with respect to the current state of the art. version:1
arxiv-1702-08360 | Neural Map: Structured Memory for Deep Reinforcement Learning | http://arxiv.org/abs/1702.08360 | id:1702.08360 author:Emilio Parisotto, Ruslan Salakhutdinov category:cs.LG  published:2017-02-27 summary:A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training. version:1
arxiv-1702-08359 | Dynamic Word Embeddings via Skip-Gram Filtering | http://arxiv.org/abs/1702.08359 | id:1702.08359 author:Robert Bamler, Stephan Mandt category:stat.ML cs.LG  published:2017-02-27 summary:We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov, 2013]. These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms---skip-gram smoothing and skip-gram filtering---that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices. version:1
arxiv-1702-08343 | Approximate Inference with Amortised MCMC | http://arxiv.org/abs/1702.08343 | id:1702.08343 author:Yingzhen Li, Richard E. Turner, Qiang Liu category:stat.ML cs.LG  published:2017-02-27 summary:We propose a novel approximate inference algorithm that approximates a target distribution by amortising the dynamics of a user-selected MCMC sampler. The idea is to initialise MCMC using samples from an approximation network, apply the MCMC operator to improve these samples, and finally use the samples to update the approximation network thereby improving its quality. This provides a new generic framework for approximate inference, allowing us to deploy highly complex, or implicitly defined approximation families with intractable densities, including approximations produced by warping a source of randomness through a deep neural network. Experiments consider image modelling with deep generative models as a challenging test for the method. Deep models trained using amortised MCMC are shown to generate realistic looking samples as well as producing diverse imputations for images with regions of missing pixels. version:1
arxiv-1702-08336 | Multi-Label Segmentation via Residual-Driven Adaptive Regularization | http://arxiv.org/abs/1702.08336 | id:1702.08336 author:Byung-Woo Hong, Ja-Keoung Koo, Stefano Soatto category:cs.CV  published:2017-02-27 summary:We present a variational multi-label segmentation algorithm based on a robust Huber loss for both the data and the regularizer, minimized within a convex optimization framework. We introduce a novel constraint on the common areas, to bias the solution towards mutually exclusive regions. We also propose a regularization scheme that is adapted to the spatial statistics of the residual at each iteration, resulting in a varying degree of regularization being applied as the algorithm proceeds: the effect of the regularizer is strongest at initialization, and wanes as the solution increasingly fits the data. This minimizes the bias induced by the regularizer at convergence. We design an efficient convex optimization algorithm based on the alternating direction method of multipliers using the equivalent relation between the Huber function and the proximal operator of the one-norm. We empirically validate our proposed algorithm on synthetic and real images and offer an information-theoretic derivation of the cost-function that highlights the modeling choices made. version:1
arxiv-1702-05777 | Exponentially vanishing sub-optimal local minima in multilayer neural networks | http://arxiv.org/abs/1702.05777 | id:1702.05777 author:Daniel Soudry, Elad Hoffer category:stat.ML  published:2017-02-19 summary:We examine a multilayer neural network with piecewise linear units, input of dimension $d_{0}$, one hidden layer of width $d_{1}$, a single output, and a quadratic loss, trained on $N$ datapoints. We prove that in the limit that $N\rightarrow\infty$, the volume of differentiable regions of the loss containing sub-optimal differentiable local minima is exponentially vanishing in comparison with the same volume of global minima, given standard normal input, $d_{0}\left(N\right)=\tilde{\Omega}\left(\sqrt{N}\right)$ and an asymptotically "mild" over-parameterization: $\#\mathrm{parameters=\,}\tilde{\Omega}\left(N\right)$. Previous results on vanishing local minima so far required many more parameters: $\#\mathrm{parameters=\,}\Omega\left(Nd_{0}\left(N\right)\right)$, which is typically worse. version:3
arxiv-1702-08320 | An Efficient Pseudo-likelihood Method for Sparse Binary Pairwise Markov Network Estimation | http://arxiv.org/abs/1702.08320 | id:1702.08320 author:Sinong Geng, Zhaobin Kuang, David Page category:stat.ML  published:2017-02-27 summary:The pseudo-likelihood method is one of the most popular algorithms for learning sparse binary pairwise Markov networks. In this paper, we formulate the $L_1$ regularized pseudo-likelihood problem as a sparse multiple logistic regression problem. In this way, many insights and optimization procedures for sparse logistic regression can be applied to the learning of discrete Markov networks. Specifically, we use the coordinate descent algorithm for generalized linear models with convex penalties, combined with strong screening rules, to solve the pseudo-likelihood problem with $L_1$ regularization. Our method offers a substantial speedup without losing any accuracy. Furthermore, the proposed method is more stable than the node-wise logistic regression approach on unbalanced high-dimensional data when penalized by small regularization parameters. Thorough numerical experiments on simulated data and real world data demonstrate the advantages of the proposed method. version:1
arxiv-1702-08319 | Visual Translation Embedding Network for Visual Relation Detection | http://arxiv.org/abs/1702.08319 | id:1702.08319 author:Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, Tat-Seng Chua category:cs.CV I.4  published:2017-02-27 summary:Visual relations, such as "person ride bike" and "bike next to car", offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate $\approx$ object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-to-end relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu's multi-modal model with language priors. version:1
arxiv-1702-08318 | Efficient Privacy Preserving Viola-Jones Type Object Detection via Random Base Image Representation | http://arxiv.org/abs/1702.08318 | id:1702.08318 author:Xin Jin, Peng Yuan, Xiaodong Li, Chenggen Song, Shiming Ge, Geng Zhao, Yingya Chen category:cs.CV  published:2017-02-27 summary:A cloud server spent a lot of time, energy and money to train a Viola-Jones type object detector with high accuracy. Clients can upload their photos to the cloud server to find objects. However, the client does not want the leakage of the content of his/her photos. In the meanwhile, the cloud server is also reluctant to leak any parameters of the trained object detectors. 10 years ago, Avidan & Butman introduced Blind Vision, which is a method for securely evaluating a Viola-Jones type object detector. Blind Vision uses standard cryptographic tools and is painfully slow to compute, taking a couple of hours to scan a single image. The purpose of this work is to explore an efficient method that can speed up the process. We propose the Random Base Image (RBI) Representation. The original image is divided into random base images. Only the base images are submitted randomly to the cloud server. Thus, the content of the image can not be leaked. In the meanwhile, a random vector and the secure Millionaire protocol are leveraged to protect the parameters of the trained object detector. The RBI makes the integral-image enable again for the great acceleration. The experimental results reveal that our method can retain the detection accuracy of that of the plain vision algorithm and is significantly faster than the traditional blind vision, with only a very low probability of the information leakage theoretically. version:1
arxiv-1702-03180 | Stochastic Configuration Networks: Fundamentals and Algorithms | http://arxiv.org/abs/1702.03180 | id:1702.03180 author:Dianhui Wang, Ming Li category:cs.NE  published:2017-02-10 summary:This paper contributes to a development of randomized methods for neural networks. The proposed learner model is generated incrementally by stochastic configuration (SC) algorithms, termed as Stochastic Configuration Networks (SCNs). In contrast to the existing randomised learning algorithms for single layer feed-forward neural networks (SLFNNs), we randomly assign the input weights and biases of the hidden nodes in the light of a supervisory mechanism, and the output weights are analytically evaluated in either constructive or selective manner. As fundamentals of SCN-based data modelling techniques, we establish some theoretical results on the universal approximation property. Three versions of SC algorithms are presented for regression problems (applicable for classification problems as well) in this work. Simulation results concerning both function approximation and real world data regression indicate some remarkable merits of our proposed SCNs in terms of less human intervention on the network size setting, the scope adaptation of random parameters, fast learning and sound generalization. version:4
arxiv-1702-08303 | Identifying beneficial task relations for multi-task learning in deep neural networks | http://arxiv.org/abs/1702.08303 | id:1702.08303 author:Joachim Bingel, Anders Søgaard category:cs.CL I.2.7  published:2017-02-27 summary:Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential to efficiently regularize models and to reduce the need for labeled data. While it has brought significant improvements in a number of NLP tasks, mixed results have been reported, and little is known about the conditions under which MTL leads to gains in NLP. This paper sheds light on the specific task relations that can lead to gains from MTL models over single-task setups. version:1
arxiv-1702-08283 | Adaptive Learning to Speed-Up Control of Prosthetic Hands: a Few Things Everybody Should Know | http://arxiv.org/abs/1702.08283 | id:1702.08283 author:Valentina Gregori, Arjan Gijsberts, Barbara Caputo category:cs.LG  published:2017-02-27 summary:A number of studies have proposed to use domain adaptation to reduce the training efforts needed to control an upper-limb prosthesis exploiting pre-trained models from prior subjects. These studies generally reported impressive reductions in the required number of training samples to achieve a certain level of accuracy for intact subjects. We further investigate two popular methods in this field to verify whether this result equally applies to amputees. Our findings show instead that this improvement can largely be attributed to a suboptimal hyperparameter configuration. When hyperparameters are appropriately tuned, the standard approach that does not exploit prior information performs on par with the more complicated transfer learning algorithms. Additionally, earlier studies erroneously assumed that the number of training samples relates proportionally to the efforts required from the subject. However, a repetition of a movement is the atomic unit for subjects and the total number of repetitions should therefore be used as reliable measure for training efforts. Also when correcting for this mistake, we do not find any performance increase due to the use of prior models. version:1
arxiv-1702-08451 | Approches d'analyse distributionnelle pour améliorer la désambiguïsation sémantique | http://arxiv.org/abs/1702.08451 | id:1702.08451 author:Mokhtar Billami, Núria Gala category:cs.CL  published:2017-02-27 summary:Word sense disambiguation (WSD) improves many Natural Language Processing (NLP) applications such as Information Retrieval, Machine Translation or Lexical Simplification. WSD is the ability of determining a word sense among different ones within a polysemic lexical unit taking into account the context. The most straightforward approach uses a semantic proximity measure between the word sense candidates of the target word and those of its context. Such a method very easily entails a combinatorial explosion. In this paper, we propose two methods based on distributional analysis which enable to reduce the exponential complexity without losing the coherence. We present a comparison between the selection of distributional neighbors and the linearly nearest neighbors. The figures obtained show that selecting distributional neighbors leads to better results. version:1
arxiv-1702-08450 | A Knowledge-Based Approach to Word Sense Disambiguation by distributional selection and semantic features | http://arxiv.org/abs/1702.08450 | id:1702.08450 author:Mokhtar Billami category:cs.CL  published:2017-02-27 summary:Word sense disambiguation improves many Natural Language Processing (NLP) applications such as Information Retrieval, Information Extraction, Machine Translation, or Lexical Simplification. Roughly speaking, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic similarity relatedness between two word senses and then extends it to all words from text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score (we can imagine the exponential complexity that returns this comprehensive approach). In this paper, we propose to use a combinatorial optimization metaheuristic for choosing the nearest neighbors obtained by distributional selection around the word to disambiguate. The test and the evaluation of our method concern a corpus written in French by means of the semantic network BabelNet. The obtained accuracy rate is 78 % on all names and verbs chosen for the evaluation. version:1
arxiv-1702-08259 | Fast and Accurate Inference with Adaptive Ensemble Prediction in Image Classification with Deep Neural Networks | http://arxiv.org/abs/1702.08259 | id:1702.08259 author:Hiroshi Inoue category:cs.LG cs.CV stat.ML  published:2017-02-27 summary:Ensembling multiple predictions is a widely used technique to improve the accuracy of various machine learning tasks. In image classification tasks, for example, averaging the predictions for multiple patches extracted from the input image significantly improves accuracy. Using multiple networks trained independently to make predictions improves accuracy further. One obvious drawback of the ensembling technique is its higher execution cost during inference. If we average 100 predictions, the execution cost will be 100 times as high as the cost without the ensemble. This higher cost limits the real-world use of ensembling, even though using it is almost the norm to win image classification competitions. In this paper, we describe a new technique called adaptive ensemble prediction, which achieves the benefits of ensembling with much smaller additional execution costs. Our observation behind this technique is that many easy-to-predict inputs do not require ensembling. Hence we calculate the confidence level of the prediction for each input on the basis of the probability of the predicted label, i.e. the outputs from the softmax, during the ensembling computation. If the prediction for an input reaches a high enough probability on the basis of the confidence level, we stop ensembling for this input to avoid wasting computation power. We evaluated the adaptive ensembling by using various datasets and showed that it reduces the computation time significantly while achieving similar accuracy to the naive ensembling. version:1
arxiv-1702-08249 | Uniform Deviation Bounds for Unbounded Loss Functions like k-Means | http://arxiv.org/abs/1702.08249 | id:1702.08249 author:Olivier Bachem, Mario Lucic, S. Hamed Hassani, Andreas Krause category:stat.ML cs.LG  published:2017-02-27 summary:Uniform deviation bounds limit the difference between a model's expected loss and its loss on an empirical sample uniformly for all models in a learning problem. As such, they are a critical component to empirical risk minimization. In this paper, we provide a novel framework to obtain uniform deviation bounds for loss functions which are *unbounded*. In our main application, this allows us to obtain bounds for $k$-Means clustering under weak assumptions on the underlying distribution. If the fourth moment is bounded, we prove a rate of $\mathcal{O}\left(m^{-\frac12}\right)$ compared to the previously known $\mathcal{O}\left(m^{-\frac14}\right)$ rate. Furthermore, we show that the rate also depends on the kurtosis - the normalized fourth moment which measures the "tailedness" of a distribution. We further provide improved rates under progressively stronger assumptions, namely, bounded higher moments, subgaussianity and bounded support. version:1
arxiv-1702-08248 | Scalable and Distributed Clustering via Lightweight Coresets | http://arxiv.org/abs/1702.08248 | id:1702.08248 author:Olivier Bachem, Mario Lucic, Andreas Krause category:stat.ML cs.DC cs.DS cs.LG stat.CO  published:2017-02-27 summary:Coresets are compact representations of data sets such that models trained on a coreset are provably competitive with models trained on the full data set. As such, they have been successfully used to scale up clustering models to massive data sets. While existing approaches generally only allow for multiplicative approximation errors, we propose a novel notion of coresets called lightweight coresets that allows for both multiplicative and additive errors. We provide a single algorithm to construct light-weight coresets for k-Means clustering, Bregman clustering and maximum likelihood estimation of Gaussian mixture models. The algorithm is substantially faster than existing constructions, embarrassingly parallel and resulting coresets are smaller. In an extensive experimental evaluation, we demonstrate that the proposed method outperforms existing coreset constructions. version:1
arxiv-1702-04593 | Deep Multi-camera People Detection | http://arxiv.org/abs/1702.04593 | id:1702.04593 author:Tatjana Chavdarova, François Fleuret category:cs.CV  published:2017-02-15 summary:Deep architectures are currently the top performing methods for monocular pedestrian detection. Surprisingly, they have not been applied in the multi-camera set-up. This is probably in large part due to the lack of large-scale labeled multi-camera data-sets with overlapping fields of view. Our main contribution is a strategy in which we re-use a pre-trained object detection network, fine-tune it on a large-scale monocular pedestrian data-set, and train an architecture which combines multiple instances of it on a small multi-camera data-set. We estimate performance on both a new HD multi-view data-set, and the standard one - PETS 2009, on which we outperform state of the art methods. version:2
arxiv-1702-08239 | Bayesian inference on random simple graphs with power law degree distributions | http://arxiv.org/abs/1702.08239 | id:1702.08239 author:Juho Lee, Creighton Heaukulani, Zoubin Ghahramani, Lancelot F. James, Seungjin Choi category:stat.ML  published:2017-02-27 summary:We present a model for random simple graphs with a degree distribution that obeys a power law (i.e., is heavy-tailed). To attain this behavior, the edge probabilities in the graph are constructed from Bertoin-Fujita-Roynette-Yor (BFRY) random variables, which have been recently utilized in Bayesian statistics for the construction of power law models in several applications. Our construction readily extends to capture the structure of latent factors, similarly to stochastic blockmodels, while maintaining its power law degree distribution. The BFRY random variables are well approximated by gamma random variables in a variational Bayesian inference routine, which we apply to several network datasets for which power law degree distributions are a natural assumption. By learning the parameters of the BFRY distribution via probabilistic inference, we are able to automatically select the appropriate power law behavior from the data. In order to further scale our inference procedure, we adopt stochastic gradient ascent routines where the gradients are computed on minibatches (i.e., subsets) of the edges in the graph. version:1
arxiv-1702-08235 | Variational Inference using Implicit Distributions | http://arxiv.org/abs/1702.08235 | id:1702.08235 author:Ferenc Huszár category:stat.ML cs.LG  published:2017-02-27 summary:Generative adversarial networks (GANs) have given us a great tool to fit implicit generative models to data. Implicit distributions are ones we can sample from easily, and take derivatives of samples with respect to model parameters. These models are highly expressive and we argue they can prove just as useful for variational inference (VI) as they are for generative modelling. Several papers have proposed GAN-like algorithms for inference, however, connections to the theory of VI are not always well understood. This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more. Secondly, the paper provides a framework for building new algorithms: depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods, and show practical inference algorithms based on either density ratio estimation or denoising. version:1
arxiv-1702-08231 | Low-Precision Batch-Normalized Activations | http://arxiv.org/abs/1702.08231 | id:1702.08231 author:Benjamin Graham category:cs.NE cs.CV  published:2017-02-27 summary:Artificial neural networks can be trained with relatively low-precision floating-point and fixed-point arithmetic, using between one and 16 bits. Previous works have focused on relatively wide-but-shallow, feed-forward networks. We introduce a quantization scheme that is compatible with training very deep neural networks. Quantizing the network activations in the middle of each batch-normalization module can greatly reduce the amount of memory and computational power needed, with little loss in accuracy. version:1
arxiv-1702-07552 | Learning Rates for Kernel-Based Expectile Regression | http://arxiv.org/abs/1702.07552 | id:1702.07552 author:Muhammad Farooq, Ingo Steinwart category:stat.ML cs.LG  published:2017-02-24 summary:Conditional expectiles are becoming an increasingly important tool in finance as well as in other areas of applications. We analyse a support vector machine type approach for estimating conditional expectiles and establish learning rates that are minimax optimal modulo a logarithmic factor if Gaussian RBF kernels are used and the desired expectile is smooth in a Besov sense. As a special case, our learning rates improve the best known rates for kernel-based least squares regression in this scenario. Key ingredients of our statistical analysis are a general calibration inequality for the asymmetric least squares loss, a corresponding variance bound as well as an improved entropy number bound for Gaussian RBF kernels. version:2
arxiv-1702-08217 | A case study on English-Malayalam Machine Translation | http://arxiv.org/abs/1702.08217 | id:1702.08217 author:Sreelekha S, Pushpak Bhattacharyya category:cs.CL  published:2017-02-27 summary:In this paper we present our work on a case study on Statistical Machine Translation (SMT) and Rule based machine translation (RBMT) for translation from English to Malayalam and Malayalam to English. One of the motivations of our study is to make a three way performance comparison, such as, a) SMT and RBMT b) English to Malayalam SMT and Malayalam to English SMT c) English to Malayalam RBMT and Malayalam to English RBMT. We describe the development of English to Malayalam and Malayalam to English baseline phrase based SMT system and the evaluation of its performance compared against the RBMT system. Based on our study the observations are: a) SMT systems outperform RBMT systems, b) In the case of SMT, English - Malayalam systems perform better than that of Malayalam - English systems, c) In the case RBMT, Malayalam to English systems are performing better than English to Malayalam systems. Based on our evaluations and detailed error analysis, we describe the requirements of incorporating morphological processing into the SMT to improve the accuracy of translation. version:1
arxiv-1702-08212 | Anticipating many futures: Online human motion prediction and synthesis for human-robot collaboration | http://arxiv.org/abs/1702.08212 | id:1702.08212 author:Judith Bütepage, Hedvig Kjellström, Danica Kragic category:cs.RO cs.CV cs.HC  published:2017-02-27 summary:Fluent and safe interactions of humans and robots require both partners to anticipate the others' actions. A common approach to human intention inference is to model specific trajectories towards known goals with supervised classifiers. However, these approaches do not take possible future movements into account nor do they make use of kinematic cues, such as legible and predictable motion. The bottleneck of these methods is the lack of an accurate model of general human motion. In this work, we present a conditional variational autoencoder that is trained to predict a window of future human motion given a window of past frames. Using skeletal data obtained from RGB depth images, we show how this unsupervised approach can be used for online motion prediction for up to 1660 ms. Additionally, we demonstrate online target prediction within the first 300-500 ms after motion onset without the use of target specific training data. The advantage of our probabilistic approach is the possibility to draw samples of possible future motions. Finally, we investigate how movements and kinematic cues are represented on the learned low dimensional manifold. version:1
arxiv-1702-08211 | Online Nonparametric Learning, Chaining, and the Role of Partial Feedback | http://arxiv.org/abs/1702.08211 | id:1702.08211 author:Nicolò Cesa-Bianchi, Pierre Gaillard, Claudio Gentile, Sébastien Gerchinovitz category:stat.ML cs.LG math.ST stat.TH  published:2017-02-27 summary:We investigate contextual online learning with nonparametric (Lipschitz) comparison classes under different assumptions on losses and feedback information. For full information feedback and Lipschitz losses, we characterize the minimax regret up to log factors by proving an upper bound matching a previously known lower bound. In a partial feedback model motivated by second-price auctions, we prove upper bounds for Lipschitz and semi-Lipschitz losses that improve on the known bounds for standard bandit feedback. Our analysis combines novel results for contextual second-price auctions with a novel algorithmic approach based on chaining. When the context space is Euclidean, our chaining approach is efficient and delivers an even better regret bound. version:1
arxiv-1702-08192 | DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy | http://arxiv.org/abs/1702.08192 | id:1702.08192 author:Christian Wachinger, Martin Reuter, Tassilo Klein category:cs.CV cs.AI cs.LG  published:2017-02-27 summary:We introduce DeepNAT, a 3D Deep convolutional neural network for the automatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance images. DeepNAT is an end-to-end learning-based approach to brain segmentation that jointly learns an abstract feature representation and a multi-class classification. We propose a 3D patch-based approach, where we do not only predict the center voxel of the patch but also neighbors, which is formulated as multi-task learning. To address a class imbalance problem, we arrange two networks hierarchically, where the first one separates foreground from background, and the second one identifies 25 brain structures on the foreground. Since patches lack spatial context, we augment them with coordinates. To this end, we introduce a novel intrinsic parameterization of the brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As network architecture, we use three convolutional layers with pooling, batch normalization, and non-linearities, followed by fully connected layers with dropout. The final segmentation is inferred from the probabilistic output of the network with a 3D fully connected conditional random field, which ensures label agreement between close voxels. The roughly 2.7 million parameters in the network are learned with stochastic gradient descent. Our results show that DeepNAT compares favorably to state-of-the-art methods. Finally, the purely learning-based method may have a high potential for the adaptation to young, old, or diseased brains by fine-tuning the pre-trained network with a small training sample on the target application, where the availability of larger datasets with manual annotations may boost the overall segmentation accuracy in the future. version:1
arxiv-1702-08185 | An update on statistical boosting in biomedicine | http://arxiv.org/abs/1702.08185 | id:1702.08185 author:Andreas Mayr, Benjamin Hofner, Elisabeth Waldmann, Tobias Hepp, Olaf Gefeller, Matthias Schmid category:stat.AP stat.CO stat.ML  published:2017-02-27 summary:Statistical boosting algorithms have triggered a lot of research during the last decade. They combine a powerful machine-learning approach with classical statistical modelling, offering various practical advantages like automated variable selection and implicit regularization of effect estimates. They are extremely flexible, as the underlying base-learners (regression functions defining the type of effect for the explanatory variables) can be combined with any kind of loss function (target function to be optimized, defining the type of regression setting). In this review article, we highlight the most recent methodological developments on statistical boosting regarding variable selection, functional regression and advanced time-to-event modelling. Additionally, we provide a short overview on relevant applications of statistical boosting in biomedicine. version:1
arxiv-1702-08171 | Fixed-point optimization of deep neural networks with adaptive step size retraining | http://arxiv.org/abs/1702.08171 | id:1702.08171 author:Sungho Shin, Yoonho Boo, Wonyong Sung category:cs.LG  published:2017-02-27 summary:Fixed-point optimization of deep neural networks plays an important role in hardware based design and low-power implementations. Many deep neural networks show fairly good performance even with 2- or 3-bit precision when quantized weights are fine-tuned by retraining. We propose an improved fixedpoint optimization algorithm that estimates the quantization step size dynamically during the retraining. In addition, a gradual quantization scheme is also tested, which sequentially applies fixed-point optimizations from high- to low-precision. The experiments are conducted for feed-forward deep neural networks (FFDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs). version:1
arxiv-1702-08169 | Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis | http://arxiv.org/abs/1702.08169 | id:1702.08169 author:Dan Garber, Ohad Shamir, Nathan Srebro category:cs.LG  published:2017-02-27 summary:We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of $m$ stores a sample of $n$ points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all $mn$ samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large $n$. We also introduce an iterative distributed algorithm that is applicable in any regime of $n$, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters. version:1
arxiv-1702-08165 | Reinforcement Learning with Deep Energy-Based Policies | http://arxiv.org/abs/1702.08165 | id:1702.08165 author:Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine category:cs.LG cs.AI  published:2017-02-27 summary:We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model. version:1
arxiv-1702-08160 | HashBox: Hash Hierarchical Segmentation exploiting Bounding Box Object Detection | http://arxiv.org/abs/1702.08160 | id:1702.08160 author:Joachim Curto, Irene Zarza, Alexander J. Smola, Luc Van Gool category:cs.CV  published:2017-02-27 summary:We propose a novel approach to address the Simultaneous Detection and Segmentation problem. Using hierarchical structures we use an efficient and accurate procedure that exploits the hierarchy feature information using Locality Sensitive Hashing. We build on recent work that utilizes convolutional neural networks to detect bounding boxes in an image (Faster R-CNN) and then use the top similar hierarchical region that best fits each bounding box after hashing, we call this approach HashBox. We then refine our final segmentation results by automatic hierarchy pruning. HashBox introduces a train-free alternative to Hypercolumns. We conduct extensive experiments on Pascal VOC 2012 segmentation dataset, showing that HashBox gives competitive state-of-the-art object segmentations. version:1
arxiv-1702-08159 | F2F: A Library For Fast Kernel Expansions | http://arxiv.org/abs/1702.08159 | id:1702.08159 author:Joachim Curto, Irene Zarza, Feng Yang, Alexander J. Smola, Luc Van Gool category:cs.LG  published:2017-02-27 summary:F2F is a C++ library for large-scale machine learning. It contains a CPU optimized implementation of the Fastfood algorithm, that allows the computation of approximated kernel expansions in loglinear time. The algorithm requires to compute the product of Walsh-Hadamard Transform (WHT) matrices. A cache friendly SIMD Fast Walsh-Hadamard Transform (FWHT) that achieves compelling speed and outperforms current state-of-the-art methods has been developed. F2F allows to obtain non-linear classification combining Fastfood and a linear classifier. version:1
arxiv-1702-08155 | Multi-scale Image Fusion Between Pre-operative Clinical CT and X-ray Microtomography of Lung Pathology | http://arxiv.org/abs/1702.08155 | id:1702.08155 author:Holger R. Roth, Kai Nagara, Hirohisa Oda, Masahiro Oda, Tomoshi Sugiyama, Shota Nakamura, Kensaku Mori category:cs.CV  published:2017-02-27 summary:Computational anatomy allows the quantitative analysis of organs in medical images. However, most analysis is constrained to the millimeter scale because of the limited resolution of clinical computed tomography (CT). X-ray microtomography ($\mu$CT) on the other hand allows imaging of ex-vivo tissues at a resolution of tens of microns. In this work, we use clinical CT to image lung cancer patients before partial pneumonectomy (resection of pathological lung tissue). The resected specimen is prepared for $\mu$CT imaging at a voxel resolution of 50 $\mu$m (0.05 mm). This high-resolution image of the lung cancer tissue allows further insides into understanding of tumor growth and categorization. For making full use of this additional information, image fusion (registration) needs to be performed in order to re-align the $\mu$CT image with clinical CT. We developed a multi-scale non-rigid registration approach. After manual initialization using a few landmark points and rigid alignment, several levels of non-rigid registration between down-sampled (in the case of $\mu$CT) and up-sampled (in the case of clinical CT) representations of the image are performed. Any non-lung tissue is ignored during the computation of the similarity measure used to guide the registration during optimization. We are able to recover the volume differences introduced by the resection and preparation of the lung specimen. The average ($\pm$ std. dev.) minimum surface distance between $\mu$CT and clinical CT at the resected lung surface is reduced from 3.3 $\pm$ 2.9 (range: [0.1, 15.9]) to 2.3 mm $\pm$ 2.8 (range: [0.0, 15.3]) mm. The alignment of clinical CT with $\mu$CT will allow further registration with even finer resolutions of $\mu$CT (up to 10 $\mu$m resolution) and ultimately with histopathological microscopy images for further macro to micro image fusion that can aid medical image analysis. version:1
arxiv-1702-08142 | Tensor Balancing on Statistical Manifold | http://arxiv.org/abs/1702.08142 | id:1702.08142 author:Mahito Sugiyama, Hiroyuki Nakahara, Koji Tsuda category:stat.ME cs.IT cs.NA math.IT stat.ML  published:2017-02-27 summary:We solve tensor balancing, rescaling an Nth order nonnegative tensor by multiplying (N - 1)th order N tensors so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton's method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton's method, can be analytically obtained using the M\"obius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines. version:1
arxiv-1702-08139 | Improved Variational Autoencoders for Text Modeling using Dilated Convolutions | http://arxiv.org/abs/1702.08139 | id:1702.08139 author:Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, Taylor Berg-Kirkpatrick category:cs.NE cs.CL cs.LG  published:2017-02-27 summary:Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines. version:1
arxiv-1702-08138 | Deceiving Google's Perspective API Built for Detecting Toxic Comments | http://arxiv.org/abs/1702.08138 | id:1702.08138 author:Hossein Hosseini, Sreeram Kannan, Baosen Zhang, Radha Poovendran category:cs.LG cs.CY cs.SI  published:2017-02-27 summary:Social media platforms provide an environment where people can freely engage in discussions. Unfortunately, they also enable several problems, such as online harassment. Recently, Google and Jigsaw started a project called Perspective, which uses machine learning to automatically detect toxic language. A demonstration website has been also launched, which allows anyone to type a phrase in the interface and instantaneously see the toxicity score [1]. In this paper, we propose an attack on the Perspective toxic detection system based on the adversarial examples. We show that an adversary can subtly modify a highly toxic phrase in a way that the system assigns significantly lower toxicity score to it. We apply the attack on the sample phrases provided in the Perspective website and show that we can consistently reduce the toxicity scores to the level of the non-toxic phrases. The existence of such adversarial examples is very harmful for toxic detection systems and seriously undermines their usability. version:1
arxiv-1703-01887 | Co-evolutionary multi-task learning for dynamic time series prediction | http://arxiv.org/abs/1703.01887 | id:1703.01887 author:Rohitash Chandra, Yew-Soon Ong, Chi-Keong Goh category:cs.NE  published:2017-02-27 summary:Multi-task learning employs shared representation of knowledge for learning multiple instances from the same or related problems. Time series prediction consists of several instances that are defined by the way they are broken down into fixed windows known as embedding dimension. Finding the optimal values for embedding dimension is a computationally intensive task. Therefore, we introduce a new category of problem called dynamic time series prediction that requires a trained model to give prediction when presented with different values of the embedding dimension. This can be seen a new class of time series prediction where dynamic prediction is needed. In this paper, we propose a co-evolutionary multi-task learning method that provides a synergy between multi-task learning and coevolution. This enables neural networks to retain modularity during training for building blocks of knowledge for different instances of the problem. The effectiveness of the proposed method is demonstrated using one-step-ahead chaotic time series problems. The results show that the proposed method can effectively be used for different instances of the related time series problems while providing improved generalisation performance. version:1
arxiv-1702-07463 | Sequence Modeling via Segmentations | http://arxiv.org/abs/1702.07463 | id:1702.07463 author:Chong Wang, Yining Wang, Po-Sen Huang, Abdelrahman Mohamed, Dengyong Zhou, Li Deng category:stat.ML cs.LG  published:2017-02-24 summary:Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts. version:2
arxiv-1702-08115 | Bioplausible multiscale filtering in retino-cortical processing as a mechanism in perceptual grouping | http://arxiv.org/abs/1702.08115 | id:1702.08115 author:Nasim Nematzadeh, David M. W. Powers, Trent W. Lewis category:cs.CV  published:2017-02-27 summary:Why does our visual system fail to reconstruct reality, when we look at certain patterns? Where do Geometrical illusions start to emerge in the visual pathway? Should computational models of vision have the same visual ability to detect illusions as we do? This study addresses these questions, by focusing on a specific underlying neural mechanism involved in our visual experiences that affects our final perception. Among many types of visual illusion, 'Geometrical' and, in particular, 'Tilt' illusions are rather important, being characterized by misperception of geometric patterns involving lines and tiles in combination with contrasting orientation, size or position. Over the last decade, many new neurophysiological experiments have led to new insights as to how, when and where retinal processing takes place, and the encoding nature of the retinal representation that is sent to the cortex for further processing. Based on these neurobiological discoveries, we provide computer simulation evidence to suggest that visual Geometrical illusions are explained in part, by the interaction of multiscale visual processing performed in the retina. The output of our retinal stage model is presented for several types of Tilt illusion, in which the final tilt percept arises from multiple scale processing of Differences of Gaussian and the perceptual interaction of foreground and background elements. Our results suggest that this multilevel filtering explanation, which is a simplified simulation for Retinal Ganglion Cell's responses to these patterns is indeed the underlying mechanism connecting low-level filtering to mid- and high-level explanations such as 'anchoring theory' and 'perceptual grouping'. version:1
arxiv-1702-08112 | 3D Scanning System for Automatic High-Resolution Plant Phenotyping | http://arxiv.org/abs/1702.08112 | id:1702.08112 author:Chuong V Nguyen, Jurgen Fripp, David R Lovell, Robert Furbank, Peter Kuffner, Helen Daily, Xavier Sirault category:cs.CV  published:2017-02-26 summary:Thin leaves, fine stems, self-occlusion, non-rigid and slowly changing structures make plants difficult for three-dimensional (3D) scanning and reconstruction -- two critical steps in automated visual phenotyping. Many current solutions such as laser scanning, structured light, and multiview stereo can struggle to acquire usable 3D models because of limitations in scanning resolution and calibration accuracy. In response, we have developed a fast, low-cost, 3D scanning platform to image plants on a rotating stage with two tilting DSLR cameras centred on the plant. This uses new methods of camera calibration and background removal to achieve high-accuracy 3D reconstruction. We assessed the system's accuracy using a 3D visual hull reconstruction algorithm applied on 2 plastic models of dicotyledonous plants, 2 sorghum plants and 2 wheat plants across different sets of tilt angles. Scan times ranged from 3 minutes (to capture 72 images using 2 tilt angles), to 30 minutes (to capture 360 images using 10 tilt angles). The leaf lengths, widths, areas and perimeters of the plastic models were measured manually and compared to measurements from the scanning system: results were within 3-4% of each other. The 3D reconstructions obtained with the scanning system show excellent geometric agreement with all six plant specimens, even plants with thin leaves and fine stems. version:1
arxiv-1702-08088 | Selection of training populations (and other subset selection problems) with an accelerated genetic algorithm (STPGA: An R-package for selection of training populations with a genetic algorithm) | http://arxiv.org/abs/1702.08088 | id:1702.08088 author:Deniz Akdemir category:stat.ME cs.LG q-bio.GN q-bio.QM stat.AP  published:2017-02-26 summary:Optimal subset selection is an important task that has numerous algorithms designed for it and has many application areas. STPGA contains a special genetic algorithm supplemented with a tabu memory property (that keeps track of previously tried solutions and their fitness for a number of iterations), and with a regression of the fitness of the solutions on their coding that is used to form the ideal estimated solution (look ahead property) to search for solutions of generic optimal subset selection problems. I have initially developed the programs for the specific problem of selecting training populations for genomic prediction or association problems, therefore I give discussion of the theory behind optimal design of experiments to explain the default optimization criteria in STPGA, and illustrate the use of the programs in this endeavor. Nevertheless, I have picked a few other areas of application: supervised and unsupervised variable selection based on kernel alignment, supervised variable selection with design criteria, influential observation identification for regression, solving mixed integer quadratic optimization problems, balancing gains and inbreeding in a breeding population. Some of these illustrations pertain new statistical approaches. version:1
arxiv-1702-08074 | Learning Control for Air Hockey Striking using Deep Reinforcement Learning | http://arxiv.org/abs/1702.08074 | id:1702.08074 author:Ayal Taitler, Nahum Shimkin category:cs.LG cs.RO  published:2017-02-26 summary:We consider the task of learning control policies for a robotic mechanism striking a puck in an air hockey game. The control signal is a direct command to the robot's motors. We employ a model free deep reinforcement learning framework to learn the motoric skills of striking the puck accurately in order to score. We propose certain improvements to the standard learning scheme which make the deep Q-learning algorithm feasible when it might otherwise fail. Our improvements include integrating prior knowledge into the learning scheme, and accounting for the changing distribution of samples in the experience replay buffer. Finally we present our simulation results for aimed striking which demonstrate the successful learning of this task, and the improvement in algorithm stability due to the proposed modifications. version:1
arxiv-1702-07097 | Bidirectional Backpropagation: Towards Biologically Plausible Error Signal Transmission in Neural Networks | http://arxiv.org/abs/1702.07097 | id:1702.07097 author:Hongyin Luo, Jie Fu, James Glass category:cs.NE cs.LG  published:2017-02-23 summary:The back-propagation (BP) algorithm has been considered the de-facto method for training deep neural networks. It back-propagates errors from the output layer to the hidden layers in an exact manner using the transpose of the feedforward weights. However, it has been argued that this is not biologically plausible because back-propagating error signals with the exact incoming weights is not considered possible in biological neural systems. In this work, we propose a biologically plausible paradigm of neural architecture based on related literature in neuroscience and asymmetric BP-like methods. Specifically, we propose two bidirectional learning algorithms with trainable feedforward and feedback weights. The feedforward weights are used to relay activations from the inputs to target outputs. The feedback weights pass the error signals from the output layer to the hidden layers. Different from other asymmetric BP-like methods, the feedback weights are also plastic in our framework and are trained to approximate the forward activations. Preliminary results show that our models outperform other asymmetric BP-like methods on the MNIST and the CIFAR-10 datasets. version:2
arxiv-1702-08039 | Criticality and Deep Learning, Part I: Theory vs. Empirics | http://arxiv.org/abs/1702.08039 | id:1702.08039 author:Dan Oprisa, Peter Toth category:cs.AI cs.LG  published:2017-02-26 summary:Motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks, we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning. On the theoretical side, we use results from statistical physics to carry out critical point calculations in feed-forward/fully connected networks, while on the experimental side we set out to find traces of criticality in deep neural networks. This is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks. version:1
arxiv-1702-08021 | Friends and Enemies of Clinton and Trump: Using Context for Detecting Stance in Political Tweets | http://arxiv.org/abs/1702.08021 | id:1702.08021 author:Mirko Lai, Delia Irazú Hernández Farías, Viviana Patti, Paolo Rosso category:cs.CL  published:2017-02-26 summary:Stance detection, the task of identifying the speaker's opinion towards a particular target, has attracted the attention of researchers. This paper describes a novel approach for detecting stance in Twitter. We define a set of features in order to consider the context surrounding a target of interest with the final aim of training a model for predicting the stance towards the mentioned targets. In particular, we are interested in investigating political debates in social media. For this reason we evaluated our approach focusing on two targets of the SemEval-2016 Task6 on Detecting stance in tweets, which are related to the political campaign for the 2016 U.S. presidential elections: Hillary Clinton vs. Donald Trump. For the sake of comparison with the state of the art, we evaluated our model against the dataset released in the SemEval-2016 Task 6 shared task competition. Our results outperform the best ones obtained by participating teams, and show that information about enemies and friends of politicians help in detecting stance towards them. version:1
arxiv-1703-01977 | Linear, Machine Learning and Probabilistic Approaches for Time Series Analysis | http://arxiv.org/abs/1703.01977 | id:1703.01977 author:B. M. Pavlyshenko category:stat.AP cs.LG stat.ME  published:2017-02-26 summary:In this paper we study different approaches for time series modeling. The forecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine learning algorithm are described. Results of different model combinations are shown. For probabilistic modeling the approaches using copulas and Bayesian inference are considered. version:1
arxiv-1702-08019 | Support vector machine and its bias correction in high-dimension, low-sample-size settings | http://arxiv.org/abs/1702.08019 | id:1702.08019 author:Yugo Nakayama, Kazuyoshi Yata, Makoto Aoshima category:stat.ML cs.LG 62H30  62G20  published:2017-02-26 summary:In this paper, we consider asymptotic properties of the support vector machine (SVM) in high-dimension, low-sample-size (HDLSS) settings. We show that the hard-margin linear SVM holds a consistency property in which misclassification rates tend to zero as the dimension goes to infinity under certain severe conditions. We show that the SVM is very biased in HDLSS settings and its performance is affected by the bias directly. In order to overcome such difficulties, we propose a bias-corrected SVM (BC-SVM). We show that the BC-SVM gives preferable performances in HDLSS settings. We also discuss the SVMs in multiclass HDLSS settings. Finally, we check the performance of the classifiers in actual data analyses. version:1
arxiv-1702-08014 | Adversarial Networks for the Detection of Aggressive Prostate Cancer | http://arxiv.org/abs/1702.08014 | id:1702.08014 author:Simon Kohl, David Bonekamp, Heinz-Peter Schlemmer, Kaneschka Yaqubi, Markus Hohenfellner, Boris Hadaschik, Jan-Philipp Radtke, Klaus Maier-Hein category:cs.CV  published:2017-02-26 summary:Semantic segmentation constitutes an integral part of medical image analyses for which breakthroughs in the field of deep learning were of high relevance. The large number of trainable parameters of deep neural networks however renders them inherently data hungry, a characteristic that heavily challenges the medical imaging community. Though interestingly, with the de facto standard training of fully convolutional networks (FCNs) for semantic segmentation being agnostic towards the `structure' of the predicted label maps, valuable complementary information about the global quality of the segmentation lies idle. In order to tap into this potential, we propose utilizing an adversarial network which discriminates between expert and generated annotations in order to train FCNs for semantic segmentation. Because the adversary constitutes a learned parametrization of what makes a good segmentation at a global level, we hypothesize that the method holds particular advantages for segmentation tasks on complex structured, small datasets. This holds true in our experiments: We learn to segment aggressive prostate cancer utilizing MRI images of 152 patients and show that the proposed scheme is superior over the de facto standard in terms of the detection sensitivity and the dice-score for aggressive prostate cancer. The achieved relative gains are shown to be particularly pronounced in the small dataset limit. version:1
arxiv-1702-08009 | Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation | http://arxiv.org/abs/1702.08009 | id:1702.08009 author:Omid Hosseini Jafari, Oliver Groth, Alexander Kirillov, Michael Ying Yang, Carsten Rother category:cs.CV cs.RO  published:2017-02-26 summary:This paper addresses the task of designing a modular neural network architecture that jointly solves different tasks. As an example we use the tasks of depth estimation and semantic segmentation given a single RGB image. The main focus of this work is to analyze the cross-modality influence between depth and semantic prediction maps on their joint refinement. While most previous works solely focus on measuring improvements in accuracy, we propose a way to quantify the cross-modality influence. We show that there is a relationship between final accuracy and cross-modality influence, although not a simple linear one. Hence a larger cross-modality influence does not necessarily translate into an improved accuracy. We find that a beneficial balance between the cross-modality influences can be achieved by network architecture and conjecture that this relationship can be utilized to understand different network design choices. Towards this end we propose a Convolutional Neural Network (CNN) architecture that fuses the state of the state-of-the-art results for depth estimation and semantic labeling. By balancing the cross-modality influences between depth and semantic prediction, we achieve improved results for both tasks using the NYU-Depth v2 benchmark. version:1
arxiv-1702-08007 | Bayesian Nonparametric Unmixing of Hyperspectral Images | http://arxiv.org/abs/1702.08007 | id:1702.08007 author:Jürgen Hahn, Abdelhak M. Zoubir category:cs.CV  published:2017-02-26 summary:Hyperspectral imaging is an important tool in remote sensing, allowing for accurate analysis of vast areas. Due to a low spatial resolution, a pixel of a hyperspectral image rarely represents a single material, but rather a mixture of different spectra. HSU aims at estimating the pure spectra present in the scene of interest, referred to as endmembers, and their fractions in each pixel, referred to as abundances. Today, many HSU algorithms have been proposed, based either on a geometrical or statistical model. While most methods assume that the number of endmembers present in the scene is known, there is only little work about estimating this number from the observed data. In this work, we propose a Bayesian nonparametric framework that jointly estimates the number of endmembers, the endmembers itself, and their abundances, by making use of the Indian Buffet Process as a prior for the endmembers. Simulation results and experiments on real data demonstrate the effectiveness of the proposed algorithm, yielding results comparable with state-of-the-art methods while being able to reliably infer the number of endmembers. In scenarios with strong noise, where other algorithms provide only poor results, the proposed approach tends to overestimate the number of endmembers slightly. The additional endmembers, however, often simply represent noisy replicas of present endmembers and could easily be merged in a post-processing step. version:1
arxiv-1702-08001 | Bayesian Nonparametric Feature and Policy Learning for Decision-Making | http://arxiv.org/abs/1702.08001 | id:1702.08001 author:Jürgen Hahn, Abdelhak M. Zoubir category:cs.LG cs.CV  published:2017-02-26 summary:Learning from demonstrations has gained increasing interest in the recent past, enabling an agent to learn how to make decisions by observing an experienced teacher. While many approaches have been proposed to solve this problem, there is only little work that focuses on reasoning about the observed behavior. We assume that, in many practical problems, an agent makes its decision based on latent features, indicating a certain action. Therefore, we propose a generative model for the states and actions. Inference reveals the number of features, the features, and the policies, allowing us to learn and to analyze the underlying structure of the observed behavior. Further, our approach enables prediction of actions for new states. Simulations are used to assess the performance of the algorithm based upon this model. Moreover, the problem of learning a driver's behavior is investigated, demonstrating the performance of the proposed model in a real-world scenario. version:1
arxiv-1702-07998 | Detecting (Un)Important Content for Single-Document News Summarization | http://arxiv.org/abs/1702.07998 | id:1702.07998 author:Yinfei Yang, Forrest Sheng Bao, Ani Nenkova category:cs.CL  published:2017-02-26 summary:We present a robust approach for detecting intrinsic sentence importance in news, by training on two corpora of document-summary pairs. When used for single-document summarization, our approach, combined with the "beginning of document" heuristic, outperforms a state-of-the-art summarizer and the beginning-of-article baseline in both automatic and manual evaluations. These results represent an important advance because in the absence of cross-document repetition, single document summarizers for news have not been able to consistently outperform the strong beginning-of-article baseline. version:1
arxiv-1702-07985 | A multi-task convolutional neural network for mega-city analysis using very high resolution satellite imagery and geospatial data | http://arxiv.org/abs/1702.07985 | id:1702.07985 author:Fan Zhang, Bo Du, Liangpei Zhang category:cs.CV  published:2017-02-26 summary:Mega-city analysis with very high resolution (VHR) satellite images has been drawing increasing interest in the fields of city planning and social investigation. It is known that accurate land-use, urban density, and population distribution information is the key to mega-city monitoring and environmental studies. Therefore, how to generate land-use, urban density, and population distribution maps at a fine scale using VHR satellite images has become a hot topic. Previous studies have focused solely on individual tasks with elaborate hand-crafted features and have ignored the relationship between different tasks. In this study, we aim to propose a universal framework which can: 1) automatically learn the internal feature representation from the raw image data; and 2) simultaneously produce fine-scale land-use, urban density, and population distribution maps. For the first target, a deep convolutional neural network (CNN) is applied to learn the hierarchical feature representation from the raw image data. For the second target, a novel CNN-based universal framework is proposed to process the VHR satellite images and generate the land-use, urban density, and population distribution maps. To the best of our knowledge, this is the first CNN-based mega-city analysis method which can process a VHR remote sensing image with such a large data volume. A VHR satellite image (1.2 m spatial resolution) of the center of Wuhan covering an area of 2606 km2 was used to evaluate the proposed method. The experimental results confirm that the proposed method can achieve a promising accuracy for land-use, urban density, and population distribution maps. version:1
arxiv-1702-07983 | Maximum-Likelihood Augmented Discrete Generative Adversarial Networks | http://arxiv.org/abs/1702.07983 | id:1702.07983 author:Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song, Yoshua Bengio category:cs.AI cs.CL cs.LG  published:2017-02-26 summary:Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach. version:1
arxiv-1702-07976 | Ratio Utility and Cost Analysis for Privacy Preserving Subspace Projection | http://arxiv.org/abs/1702.07976 | id:1702.07976 author:Mert Al, Shibiao Wan, Sun-Yuan Kung category:stat.ML cs.LG  published:2017-02-26 summary:With a rapidly increasing number of devices connected to the internet, big data has been applied to various domains of human life. Nevertheless, it has also opened new venues for breaching users' privacy. Hence it is highly required to develop techniques that enable data owners to privatize their data while keeping it useful for intended applications. Existing methods, however, do not offer enough flexibility for controlling the utility-privacy trade-off and may incur unfavorable results when privacy requirements are high. To tackle these drawbacks, we propose a compressive-privacy based method, namely RUCA (Ratio Utility and Cost Analysis), which can not only maximize performance for a privacy-insensitive classification task but also minimize the ability of any classifier to infer private information from the data. Experimental results on Census and Human Activity Recognition data sets demonstrate that RUCA significantly outperforms existing privacy preserving data projection techniques for a wide range of privacy pricings. version:1
arxiv-1702-07975 | Building Fast and Compact Convolutional Neural Networks for Offline Handwritten Chinese Character Recognition | http://arxiv.org/abs/1702.07975 | id:1702.07975 author:Xuefeng Xiao, Lianwen Jin, Yafeng Yang, Weixin Yang, Jun Sun, Tianhai Chang category:cs.CV  published:2017-02-26 summary:Like other problems in computer vision, offline handwritten Chinese character recognition (HCCR) has achieved impressive results using convolutional neural network (CNN)-based methods. However, larger and deeper networks are needed to deliver state-of-the-art results in this domain. Such networks intuitively appear to incur high computational cost, and require the storage of a large number of parameters, which renders them unfeasible for deployment in portable devices. To solve this problem, we propose a Global Supervised Low-rank Expansion (GSLRE) method and an Adaptive Drop-weight (ADW) technique to solve the problems of speed and storage capacity. We design a nine-layer CNN for HCCR consisting of 3,755 classes, and devise an algorithm that can reduce the networks computational cost by nine times and compress the network to 1/18 of the original size of the baseline model, with only a 0.21% drop in accuracy. In tests, the proposed algorithm surpassed the best single-network performance reported thus far in the literature while requiring only 2.3 MB for storage. Furthermore, when integrated with our effective forward implementation, the recognition of an offline character image took only 9.7 ms on a CPU. Compared with the state-of-the-art CNN model for HCCR, our approach is approximately 30 times faster, yet 10 times more cost efficient. version:1
arxiv-1702-07971 | Seeing What Is Not There: Learning Context to Determine Where Objects Are Missing | http://arxiv.org/abs/1702.07971 | id:1702.07971 author:Jin Sun, David W. Jacobs category:cs.CV  published:2017-02-26 summary:Most of computer vision focuses on what is in an image. We propose to train a standalone object-centric context representation to perform the opposite task: seeing what is not there. Given an image, our context model can predict where objects should exist, even when no object instances are present. Combined with object detection results, we can perform a novel vision task: finding where objects are missing in an image. Our model is based on a convolutional neural network structure. With a specially designed training strategy, the model learns to ignore objects and focus on context only. It is fully convolutional thus highly efficient. Experiments show the effectiveness of the proposed approach in one important accessibility task: finding city street regions where curb ramps are missing, which could help millions of people with mobility disabilities. version:1
arxiv-1702-07966 | Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs | http://arxiv.org/abs/1702.07966 | id:1702.07966 author:Alon Brutzkus, Amir Globerson category:cs.LG math.OC stat.ML  published:2017-02-26 summary:Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex optimization problem. The key question is then under what conditions can one prove that optimization will succeed. Here we provide a strong result of this kind. We consider a neural net with one hidden layer and a convolutional structure with no overlap and a ReLU activation function. For this architecture we show that learning is NP-complete in the general case, but that when the input distribution is Gaussian, gradient descent converges to the global optimum in polynomial time. To the best of our knowledge, this is the first global optimality guarantee of gradient descent on a convolutional neural network with ReLU activations. version:1
arxiv-1702-07963 | Spatially Aware Melanoma Segmentation Using Hybrid Deep Learning Techniques | http://arxiv.org/abs/1702.07963 | id:1702.07963 author:M. Attia, M. Hossny, S. Nahavandi, A. Yazdabadi category:cs.CV  published:2017-02-26 summary:In this paper, we proposed using a hybrid method that utilises deep convolutional and recurrent neural networks for accurate delineation of skin lesion of images supplied with ISBI 2017 lesion segmentation challenge. The proposed method was trained using 1800 images and tested on 150 images from ISBI 2017 challenge. version:1
arxiv-1702-07959 | Supervised Learning of Labeled Pointcloud Differences via Cover-Tree Entropy Reduction | http://arxiv.org/abs/1702.07959 | id:1702.07959 author:Abraham Smith, Paul Bendich, John Harer, Jay Hineman category:cs.LG cs.CV stat.ML  published:2017-02-26 summary:We introduce a new algorithm, called CDER, for supervised machine learning that merges the multi-scale geometric properties of Cover Trees with the information-theoretic properties of entropy. CDER applies to a training set of labeled pointclouds embedded in a common Euclidean space. If typical pointclouds corresponding to distinct labels tend to differ at any scale in any sub-region, CDER can identify these differences in (typically) linear time, creating a set of distributional coordinates which act as a feature extraction mechanism for supervised learning. We describe theoretical properties and implementation details of CDER, and illustrate its benefits on several synthetic examples. version:1
arxiv-1702-07958 | Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$ Regret | http://arxiv.org/abs/1702.07958 | id:1702.07958 author:Alina Beygelzimer, Francesco Orabona, Chicheng Zhang category:cs.LG stat.ML  published:2017-02-25 summary:We present an efficient second-order algorithm with $\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass problem. The regret bound holds simultaneously with respect to a family of loss functions parameterized by $\eta$, for a range of $\eta$ restricted by the norm of the competitor. The family of loss functions ranges from hinge loss ($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for $\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our algorithm experimentally, showing that it also performs favorably against earlier algorithms. version:1
arxiv-1702-07944 | Stochastic Variance Reduction Methods for Policy Evaluation | http://arxiv.org/abs/1702.07944 | id:1702.07944 author:Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, Dengyong Zhou category:cs.LG cs.AI cs.SY math.OC stat.ML  published:2017-02-25 summary:Policy evaluation is a crucial step in many reinforcement-learning procedures, which estimates a value function that predicts states' long-term value under a given policy. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods. version:1
arxiv-1702-07942 | BARCHAN: Blob Alignment for Robust CHromatographic ANalysis | http://arxiv.org/abs/1702.07942 | id:1702.07942 author:Camille Couprie, Laurent Duval, Maxime Moreaud, Sophie Hénon, Mélinda Tebib, Vincent Souchon category:cs.CV physics.data-an  published:2017-02-25 summary:Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role into the elucidation of complex samples. The automation of the identification of peak areas is of prime interest to obtain a fast and repeatable analysis of chromatograms. To determine the concentration of compounds or pseudo-compounds, templates of blobs are defined and superimposed on a reference chromatogram. The templates then need to be modified when different chromatograms are recorded. In this study, we present a chromatogram and template alignment method based on peak registration called BARCHAN. Peaks are identified using a robust mathematical morphology tool. The alignment is performed by a probabilistic estimation of a rigid transformation along the first dimension, and a non-rigid transformation in the second dimension, taking into account noise, outliers and missing peaks in a fully automated way. Resulting aligned chromatograms and masks are presented on two datasets. The proposed algorithm proves to be fast and reliable. It significantly reduces the time to results for GCxGC analysis. version:1
arxiv-1702-07935 | Image Stitching by Line-guided Local Warping with Global Similarity Constraint | http://arxiv.org/abs/1702.07935 | id:1702.07935 author:Tianzhu Xiang, Gui-Song Xia, Xiang Bai, Liangpei Zhang category:cs.CV  published:2017-02-25 summary:Low-textured image stitching remains a challenging problem. It is difficult to achieve good alignment and is easy to break image structures, due to the insufficient and unreliable point correspondences. Besides, for the viewpoint variations between multiple images, the stitched images suffer from projective distortions. To this end, this paper presents a line-guided local warping with global similarity constraint for image stitching. A two-stage alignment scheme is adopted for good alignment. More precisely, the line correspondence is employed as alignment constraint to guide the accurate estimation of projective warp, then line feature constraints are integrated into mesh-based warping framework to refine the alignment while preserving image structures. To mitigate projectve distortions in non-overlapping regions, we combine global similarity constraint with the projective warps via a weight strategy, so that the final warp slowly changes from projective to similarity across the image. This is also integrated into local multiple homographies model for better parallax handling. Our method is evaluated on a series of images and compared with several other methods. Experiments demonstrate that the proposed method provides convincing stitching performance and outperforms other state-of-the-art methods. version:1
arxiv-1702-07933 | Efficient Learning of Graded Membership Models | http://arxiv.org/abs/1702.07933 | id:1702.07933 author:Zilong Tan, Sayan Mukherjee category:cs.LG stat.ML  published:2017-02-25 summary:We present an efficient algorithm for learning graded membership models when the number of variables $p$ is much larger than the number of hidden components $k$. This algorithm reduces the computational complexity of state-of-the-art tensor methods, which require decomposing an $O\left(p^3\right)$ tensor, to factorizing $O\left(p/k\right)$ sub-tensors each of size $O\left(k^3\right)$. In addition, we address the issue of negative entries in the empirical method of moments based estimators. We provide sufficient conditions under which our approach has provable guarantees. Our approach obtains competitive empirical results on both simulated and real data. version:1
arxiv-1702-07908 | CHAOS: A Parallelization Scheme for Training Convolutional Neural Networks on Intel Xeon Phi | http://arxiv.org/abs/1702.07908 | id:1702.07908 author:Andre Viebke, Suejb Memeti, Sabri Pllana, Ajith Abraham category:cs.DC cs.CV cs.LG  published:2017-02-25 summary:Deep learning is an important component of big-data analytic tools and intelligent applications, such as, self-driving cars, computer vision, speech recognition, or precision medicine. However, the training process is computationally intensive, and often requires a large amount of time if performed sequentially. Modern parallel computing systems provide the capability to reduce the required training time of deep neural networks. In this paper, we present our parallelization scheme for training convolutional neural networks (CNN) named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS). Major features of CHAOS include the support for thread and vector parallelism, non-instant updates of weight parameters during back-propagation without a significant delay, and implicit synchronization in arbitrary order. CHAOS is tailored for parallel computing systems that are accelerated with the Intel Xeon Phi. We evaluate our parallelization approach empirically using measurement techniques and performance modeling for various numbers of threads and CNN architectures. Experimental results for the MNIST dataset of handwritten digits using the total number of threads on the Xeon Phi show speedups of up to 103x compared to the execution on one thread of the Xeon Phi, 14x compared to the sequential execution on Intel Xeon E5, and 58x compared to the sequential execution on Intel Core i5. version:1
arxiv-1702-07904 | Coarse Grained Exponential Variational Autoencoders | http://arxiv.org/abs/1702.07904 | id:1702.07904 author:Ke Sun, Xiangliang Zhang category:cs.LG  published:2017-02-25 summary:Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a \emph{semi-continuous} latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete. We showcase the proposition by applying polynomial exponential family distributions as the posterior, which are universal probability density function generators. Our experimental results show consistent improvements over commonly used VAE models. version:1
arxiv-1702-07898 | Learning Deep NBNN Representations for Robust Place Categorization | http://arxiv.org/abs/1702.07898 | id:1702.07898 author:Massimiliano Mancini, Samuel Rota Bulò, Elisa Ricci, Barbara Caputo category:cs.RO cs.CV  published:2017-02-25 summary:This paper presents an approach for semantic place categorization using data obtained from RGB cameras. Previous studies on visual place recognition and classification have shown that, by considering features derived from pre-trained Convolutional Neural Networks (CNNs) in combination with part-based classification models, high recognition accuracy can be achieved, even in presence of occlusions and severe viewpoint changes. Inspired by these works, we propose to exploit local deep representations, representing images as set of regions applying a Na\"{i}ve Bayes Nearest Neighbor (NBNN) model for image classification. As opposed to previous methods where CNNs are merely used as feature extractors, our approach seamlessly integrates the NBNN model into a fully-convolutional neural network. Experimental results show that the proposed algorithm outperforms previous methods based on pre-trained CNN models and that, when employed in challenging robot place recognition tasks, it is robust to occlusions, environmental and sensor changes. version:1
arxiv-1702-06166 | Bayesian Boolean Matrix Factorisation | http://arxiv.org/abs/1702.06166 | id:1702.06166 author:Tammo Rukat, Chris C. Holmes, Michalis K. Titsias, Christopher Yau category:stat.ML cs.LG cs.NA q-bio.GN q-bio.QM stat.ME  published:2017-02-20 summary:Boolean matrix factorisation aims to decompose a binary data matrix into an approximate Boolean product of two low rank, binary matrices: one containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for Boolean matrix factorisation and derive a Metropolised Gibbs sampler that facilitates efficient parallel posterior inference. On real world and simulated data, our method outperforms all currently existing approaches for Boolean matrix factorisation and completion. This is the first method to provide full posterior inference for Boolean Matrix factorisation which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering and, crucially, improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware. version:2
arxiv-1702-07884 | An EM Based Probabilistic Two-Dimensional CCA with Application to Face Recognition | http://arxiv.org/abs/1702.07884 | id:1702.07884 author:Mehran Safayani, Seyed Hashem Ahmadi, Homayun Afrabandpey, Abdolreza Mirzaei category:cs.CV cs.LG stat.ML  published:2017-02-25 summary:Recently, two-dimensional canonical correlation analysis (2DCCA) has been successfully applied for image feature extraction. The method instead of concatenating the columns of the images to the one-dimensional vectors, directly works with two-dimensional image matrices. Although 2DCCA works well in different recognition tasks, it lacks a probabilistic interpretation. In this paper, we present a probabilistic framework for 2DCCA called probabilistic 2DCCA (P2DCCA) and an iterative EM based algorithm for optimizing the parameters. Experimental results on synthetic and real data demonstrate superior performance in loading factor estimation for P2DCCA compared to 2DCCA. For real data, three subsets of AR face database and also the UMIST face database confirm the robustness of the proposed algorithm in face recognition tasks with different illumination conditions, facial expressions, poses and occlusions. version:1
arxiv-1702-07870 | Online Learning with Many Experts | http://arxiv.org/abs/1702.07870 | id:1702.07870 author:Alon Cohen, Shie Mannor category:cs.LG  published:2017-02-25 summary:We study the problem of prediction with expert advice when the number of experts in question may be extremely large or even infinite. We devise an algorithm that obtains a tight regret bound of $\widetilde{O}(\epsilon T + N + \sqrt{NT})$, where $N$ is the empirical $\epsilon$-covering number of the sequence of loss functions generated by the environment. In addition, we present a hedging procedure that allows us to find the optimal $\epsilon$ in hindsight. Finally, we discuss a few interesting applications of our algorithm. We show how our algorithm is applicable in the approximately low rank experts model of Hazan et al. (2016), and discuss the case of experts with bounded variation, in which there is a surprisingly large gap between the regret bounds obtained in the statistical and online settings. version:1
arxiv-1702-06053 | Online Multi-Task Learning Using Active Sampling | http://arxiv.org/abs/1702.06053 | id:1702.06053 author:Sahil Sharma, Balaraman Ravindran category:cs.NE cs.LG  published:2017-02-20 summary:One of the long-standing challenges in Artificial Intelligence for goal-directed behavior is to build a single agent which can solve multiple tasks. Recent progress in multi-task learning for goal-directed sequential tasks has been in the form of distillation based learning wherein a single student network learns from multiple task-specific expert networks by mimicking the task-specific policies of the expert networks. While such approaches offer a promising solution to the multi-task learning problem, they require supervision from large task-specific (expert) networks which require extensive training. We propose a simple yet efficient multi-task learning framework which solves multiple goal-directed tasks in an online or active learning setup without the need for expert supervision. version:3
arxiv-1702-07841 | Transfer Learning for Domain Adaptation in MRI: Application in Brain Lesion Segmentation | http://arxiv.org/abs/1702.07841 | id:1702.07841 author:Mohsen Ghafoorian, Alireza Mehrtash, Tina Kapur, Nico Karssemeijer, Elena Marchiori, Mehran Pesteie, Charles R. G. Guttmann, Frank-Erik de Leeuw, Clare M. Tempany, Bram van Ginneken, Andriy Fedorov, Purang Abolmaesumi, Bram Platel, William M. Wells III category:cs.CV  published:2017-02-25 summary:Magnetic Resonance Imaging (MRI) is widely used in routine clinical diagnosis and treatment. However, variations in MRI acquisition protocols result in different appearances of normal and diseased tissue in the images. Convolutional neural networks (CNNs), which have shown to be successful in many medical image analysis tasks, are typically sensitive to the variations in imaging protocols. Therefore, in many cases, networks trained on data acquired with one MRI protocol, do not perform satisfactorily on data acquired with different protocols. This limits the use of models trained with large annotated legacy datasets on a new dataset with a different domain which is often a recurring situation in clinical settings. In this study, we aim to answer the following central questions regarding domain adaptation in medical image analysis: Given a fitted legacy model, 1) How much data from the new domain is required for a decent adaptation of the original network?; and, 2) What portion of the pre-trained model parameters should be retrained given a certain number of the new domain training samples? To address these questions, we conducted extensive experiments in white matter hyperintensity segmentation task. We trained a CNN on legacy MR images of brain and evaluated the performance of the domain-adapted network on the same task with images from a different domain. We then compared the performance of the model to the surrogate scenarios where either the same trained network is used or a new network is trained from scratch on the new dataset.The domain-adapted network tuned only by two training examples achieved a Dice score of 0.63 substantially outperforming a similar network trained on the same set of examples from scratch. version:1
arxiv-1702-07836 | Synthesizing Training Data for Object Detection in Indoor Scenes | http://arxiv.org/abs/1702.07836 | id:1702.07836 author:Georgios Georgakis, Arsalan Mousavian, Alexander C. Berg, Jana Kosecka category:cs.CV cs.RO  published:2017-02-25 summary:Detection of objects in cluttered indoor environments is one of the key enabling functionalities for service robots. The best performing object detection approaches in computer vision exploit deep Convolutional Neural Networks (CNN) to simultaneously detect and categorize the objects of interest in cluttered scenes. Training of such models typically requires large amounts of annotated training data which is time consuming and costly to obtain. In this work we explore the ability of using synthetically generated composite images for training state of the art object detectors. We superimpose 2D images of textured object models into images of real environments at variety of locations and scales. Our experiments evaluate different superimposition strategies ranging from purely image-based blending all the way to depth and semantics informed positioning of the object models to real scenes. We demonstrate the effectiveness of these object detector training strategies on publicly available datasets of GMU-Kitchens and Washington RGB-D Scenes v2, and show how object detectors can be trained with limited amounts of annotated real scenes with objects present. This charts new opportunities for training detectors for new objects by exploiting existing object model repositories in either a purely automatic fashion or with only a very small number of human-annotated examples. version:1
arxiv-1702-07835 | Critical Survey of the Freely Available Arabic Corpora | http://arxiv.org/abs/1702.07835 | id:1702.07835 author:Wajdi Zaghouani category:cs.CL  published:2017-02-25 summary:The availability of corpora is a major factor in building natural language processing applications. However, the costs of acquiring corpora can prevent some researchers from going further in their endeavours. The ease of access to freely available corpora is urgent needed in the NLP research community especially for language such as Arabic. Currently, there is not easy was to access to a comprehensive and updated list of freely available Arabic corpora. We present in this paper, the results of a recent survey conducted to identify the list of the freely available Arabic corpora and language resources. Our preliminary results showed an initial list of 66 sources. We presents our findings in the various categories studied and we provided the direct links to get the data when possible. version:1
arxiv-1702-07834 | Efficient coordinate-wise leading eigenvector computation | http://arxiv.org/abs/1702.07834 | id:1702.07834 author:Jialei Wang, Weiran Wang, Dan Garber, Nathan Srebro category:cs.NA cs.LG stat.ML  published:2017-02-25 summary:We develop and analyze efficient "coordinate-wise" methods for finding the leading eigenvector, where each step involves only a vector-vector product. We establish global convergence with overall runtime guarantees that are at least as good as Lanczos's method and dominate it for slowly decaying spectrum. Our methods are based on combining a shift-and-invert approach with coordinate-wise algorithms for linear regression. version:1
arxiv-1702-07826 | Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations | http://arxiv.org/abs/1702.07826 | id:1702.07826 author:Brent Harrison, Upol Ehsan, Mark O. Riedl category:cs.AI cs.CL cs.HC cs.LG  published:2017-02-25 summary:We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda. version:1
arxiv-1702-04837 | Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging | http://arxiv.org/abs/1702.04837 | id:1702.04837 author:Shusen Wang, Alex Gittens, Michael W. Mahoney category:stat.ML cs.LG cs.NA  published:2017-02-16 summary:We address the statistical and optimization impacts of using classical or Hessian sketch to approximately solve the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR---namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee, instead, the approximation error is governed by a subtle interplay between the "mass" in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR, these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching. version:2
arxiv-1702-07817 | An Unsupervised Learning Method Exploiting Sequential Output Statistics | http://arxiv.org/abs/1702.07817 | id:1702.07817 author:Yu Liu, Jianshu Chen, Li Deng category:cs.LG  published:2017-02-25 summary:We address a class of unsupervised learning problems where the same goal of supervised learning is aimed except with no output labels provided for training classifiers. This type of unsupervised learning is highly valuable in machine learning practice since obtaining labels in training data is often costly. Instead of pairing input-output samples, we exploit sequential statistics of output labels, in the form of N-gram language models, which can be obtained independently of input data and thus with low or no cost. We introduce a novel cost function in this unsupervised learning setting, whose profiles are analyzed and shown to be highly non-convex with large barriers near the global optimum. A new stochastic primal-dual gradient method is developed to optimize this very difficult type of cost function via the use of dual variables to reduce the barriers. We demonstrate in experimental evaluation, with both synthetic and real-world data sets, that the new method for unsupervised learning gives drastically lower errors and higher learning efficiency than the standard stochastic gradient descent, reaching classification errors about twice of those obtained by fully supervised learning. We also show the crucial role of labels' sequential statistics exploited for label-free training with the new method, reflected by the significantly lower classification errors when higher-order language models are used in unsupervised learning than low-order ones. version:1
arxiv-1702-07811 | Adaptive Neural Networks for Fast Test-Time Prediction | http://arxiv.org/abs/1702.07811 | id:1702.07811 author:Tolga Bolukbasi, Joseph Wang, Ofer Dekel, Venkatesh Saligrama category:cs.LG cs.CV cs.NE stat.ML  published:2017-02-25 summary:We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of classification performance. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. First, we pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. Building upon this approach, we then learn a network selection system that adaptively selects the network to be evaluated for each example. We exploit the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. By avoiding evaluation of these complex networks for a large fraction of examples, computational time can be dramatically reduced. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (less than 1%) loss of accuracy. version:1
arxiv-1702-07803 | Nonparanormal Information Estimation | http://arxiv.org/abs/1702.07803 | id:1702.07803 author:Shashank Singh, Barnabás Pøczos category:math.ST cs.IT math.IT stat.ML stat.TH  published:2017-02-24 summary:We study the problem of using i.i.d. samples from an unknown multivariate probability distribution $p$ to estimate the mutual information of $p$. This problem has recently received attention in two settings: (1) where $p$ is assumed to be Gaussian and (2) where $p$ is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when $p$ is not Gaussian. Estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimensions. As a result, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when $p$ is assumed to be a nonparanormal (a.k.a., Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scaling with dimensionality. version:1
arxiv-1702-07798 | Rank-to-engage: New Listwise Approaches to Maximize Engagement | http://arxiv.org/abs/1702.07798 | id:1702.07798 author:Swayambhoo Jain, Akshay Soni, Nikolay Laptev, Yashar Mehdad category:stat.ML cs.LG  published:2017-02-24 summary:For many internet businesses, presenting a given list of items in an order that maximizes a certain metric of interest (e.g., click-through-rate, average engagement time etc.) is crucial. We approach the aforementioned task from a learning-to-rank perspective which reveals a new problem setup. In traditional learning-to-rank literature, it is implicitly assumed that during the training data generation one has access to the \emph{best or desired} order for the given list of items. In this work, we consider a problem setup where we do not observe the desired ranking. We present two novel solutions: the first solution is an extension of already existing listwise learning-to-rank technique--Listwise maximum likelihood estimation (ListMLE)--while the second one is a generic machine learning based framework that tackles the problem in its entire generality. We discuss several challenges associated with this generic framework, and propose a simple \emph{item-payoff} and \emph{positional-gain} model that addresses these challenges. We provide training algorithms, inference procedures, and demonstrate the effectiveness of the two approaches over traditional ListMLE on synthetic as well as on real-life setting of ranking news articles for increased dwell time. version:1
arxiv-1702-07793 | Residual Convolutional CTC Networks for Automatic Speech Recognition | http://arxiv.org/abs/1702.07793 | id:1702.07793 author:Yisen Wang, Xuejiao Deng, Songbai Pu, Zhiheng Huang category:cs.CL  published:2017-02-24 summary:Deep learning approaches have been widely used in Automatic Speech Recognition (ASR) and they have achieved a significant accuracy improvement. Especially, Convolutional Neural Networks (CNNs) have been revisited in ASR recently. However, most CNNs used in existing work have less than 10 layers which may not be deep enough to capture all human speech signal information. In this paper, we propose a novel deep and wide CNN architecture denoted as RCNN-CTC, which has residual connections and Connectionist Temporal Classification (CTC) loss function. RCNN-CTC is an end-to-end system which can exploit temporal and spectral structures of speech signals simultaneously. Furthermore, we introduce a CTC-based system combination, which is different from the conventional frame-wise senone-based one. The basic subsystems adopted in the combination are different types and thus mutually complementary to each other. Experimental results show that our proposed single system RCNN-CTC can achieve the lowest word error rate (WER) on WSJ and Tencent Chat data sets, compared to several widely used neural network systems in ASR. In addition, the proposed system combination can offer a further error reduction on these two data sets, resulting in relative WER reductions of $14.91\%$ and $6.52\%$ on WSJ dev93 and Tencent Chat data sets respectively. version:1
arxiv-1702-07790 | Activation Ensembles for Deep Neural Networks | http://arxiv.org/abs/1702.07790 | id:1702.07790 author:Mark Harmon, Diego Klabjan category:stat.ML cs.LG  published:2017-02-24 summary:Many activation functions have been proposed in the past, but selecting an adequate one requires trial and error. We propose a new methodology of designing activation functions within a neural network at each layer. We call this technique an "activation ensemble" because it allows the use of multiple activation functions at each layer. This is done by introducing additional variables, $\alpha$, at each activation layer of a network to allow for multiple activation functions to be active at each neuron. By design, activations with larger $\alpha$ values at a neuron is equivalent to having the largest magnitude. Hence, those higher magnitude activations are "chosen" by the network. We implement the activation ensembles on a variety of datasets using an array of Feed Forward and Convolutional Neural Networks. By using the activation ensemble, we achieve superior results compared to traditional techniques. In addition, because of the flexibility of this methodology, we more deeply explore activation functions and the features that they capture. version:1
arxiv-1702-07787 | Convolutional Gated Recurrent Neural Network Incorporating Spatial Features for Audio Tagging | http://arxiv.org/abs/1702.07787 | id:1702.07787 author:Yong Xu, Qiuqiang Kong, Qiang Huang, Wenwu Wang, Mark D. Plumbley category:cs.SD cs.LG cs.NE  published:2017-02-24 summary:Environmental audio tagging is a newly proposed task to predict the presence or absence of a specific audio event in a chunk. Deep neural network (DNN) based methods have been successfully adopted for predicting the audio tags in the domestic audio scene. In this paper, we propose to use a convolutional neural network (CNN) to extract robust features from mel-filter banks (MFBs), spectrograms or even raw waveforms for audio tagging. Gated recurrent unit (GRU) based recurrent neural networks (RNNs) are then cascaded to model the long-term temporal structure of the audio signal. To complement the input information, an auxiliary CNN is designed to learn on the spatial features of stereo recordings. We evaluate our proposed methods on Task 4 (audio tagging) of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. Compared with our recent DNN-based method, the proposed structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the development set. The spatial features can further reduce the EER to 0.10. The performance of the end-to-end learning on raw waveforms is also comparable. Finally, on the evaluation set, we get the state-of-the-art performance with 0.12 EER while the performance of the best existing system is 0.15 EER. version:1
arxiv-1702-07780 | Changing Model Behavior at Test-Time Using Reinforcement Learning | http://arxiv.org/abs/1702.07780 | id:1702.07780 author:Augustus Odena, Dieterich Lawson, Christopher Olah category:stat.ML cs.LG  published:2017-02-24 summary:Machine learning models are often used at test-time subject to constraints and trade-offs not present at training-time. For example, a computer vision model operating on an embedded device may need to perform real-time inference, or a translation model operating on a cell phone may wish to bound its average compute time in order to be power-efficient. In this work we describe a mixture-of-experts model and show how to change its test-time resource-usage on a per-input basis using reinforcement learning. We test our method on a small MNIST-based example. version:1
arxiv-1702-08866 | Studying Positive Speech on Twitter | http://arxiv.org/abs/1702.08866 | id:1702.08866 author:Marina Sokolova, Vera Sazonova, Kanyi Huang, Rudraneel Chakraboty, Stan Matwin category:cs.CL I.2.6; I.2.7  published:2017-02-24 summary:We present results of empirical studies on positive speech on Twitter. By positive speech we understand speech that works for the betterment of a given situation, in this case relations between different communities in a conflict-prone country. We worked with four Twitter data sets. Through semi-manual opinion mining, we found that positive speech accounted for < 1% of the data . In fully automated studies, we tested two approaches: unsupervised statistical analysis, and supervised text classification based on distributed word representation. We discuss benefits and challenges of those approaches and report empirical evidence obtained in the study. version:1
arxiv-1702-07772 | Video and Accelerometer-Based Motion Analysis for Automated Surgical Skills Assessment | http://arxiv.org/abs/1702.07772 | id:1702.07772 author:Aneeq Zia, Yachna Sharma, Vinay Bettadapura, Eric L. Sarin, Irfan Essa category:cs.CV  published:2017-02-24 summary:Purpose: Basic surgical skills of suturing and knot tying are an essential part of medical training. Having an automated system for surgical skills assessment could help save experts time and improve training efficiency. There have been some recent attempts at automated surgical skills assessment using either video analysis or acceleration data. In this paper, we present a novel approach for automated assessment of OSATS based surgical skills and provide an analysis of different features on multi-modal data (video and accelerometer data). Methods: We conduct the largest study, to the best of our knowledge, for basic surgical skills assessment on a dataset that contained video and accelerometer data for suturing and knot-tying tasks. We introduce "entropy based" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy (XApEn), which quantify the amount of predictability and regularity of fluctuations in time-series data. The proposed features are compared to existing methods of Sequential Motion Texture (SMT), Discrete Cosine Transform (DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment. Results: We report average performance of different features across all applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows that the proposed entropy based features out-perform previous state-of-the-art methods using video data. For accelerometer data, our method performs better for suturing only. We also show that fusion of video and acceleration features can improve overall performance with the proposed entropy features achieving highest accuracy. Conclusions: Automated surgical skills assessment can be achieved with high accuracy using the proposed entropy features. Such a system can significantly improve the efficiency of surgical training in medical schools and teaching hospitals. version:1
arxiv-1702-07759 | Unifying local and non-local signal processing with graph CNNs | http://arxiv.org/abs/1702.07759 | id:1702.07759 author:Gilles Puy, Srdan Kitic, Patrick Pérez category:cs.CV  published:2017-02-24 summary:This paper deals with the unification of local and non-local signal processing on graphs within a single convolutional neural network (CNN) framework. Building upon recent works on graph CNNs, we propose to use convolutional layers that take as inputs two variables, a signal and a graph, allowing the network to adapt to changes in the graph structure. This also allows us to learn through training the optimal mixing of locality and non-locality, in cases where the graph is built on the input signal itself. We demonstrate the versatility and the effectiveness of our framework on several types of signals (greyscale and color images, color palettes and speech signals) and on several applications (style transfer, color transfer, and denoising). version:1
arxiv-1702-07752 | A supervised approach to time scale detection in dynamic networks | http://arxiv.org/abs/1702.07752 | id:1702.07752 author:Benjamin Fish, Rajmonda S. Caceres category:cs.SI cs.LG  published:2017-02-24 summary:For any stream of time-stamped edges that form a dynamic network, an important choice is the aggregation granularity that an analyst uses to bin the data. Picking such a windowing of the data is often done by hand, or left up to the technology that is collecting the data. However, the choice can make a big difference in the properties of the dynamic network. This is the time scale detection problem. In previous work, this problem is often solved with a heuristic as an unsupervised task. As an unsupervised problem, it is difficult to measure how well a given algorithm performs. In addition, we show that the quality of the windowing is dependent on which task an analyst wants to perform on the network after windowing. Therefore the time scale detection problem should not be handled independently from the rest of the analysis of the network. We introduce a framework that tackles both of these issues: By measuring the performance of the time scale detection algorithm based on how well a given task is accomplished on the resulting network, we are for the first time able to directly compare different time scale detection algorithms to each other. Using this framework, we introduce time scale detection algorithms that take a supervised approach: they leverage ground truth on training data to find a good windowing of the test data. We compare the supervised approach to previous approaches and several baselines on real data. version:1
arxiv-1702-07709 | Computationally Efficient Robust Estimation of Sparse Functionals | http://arxiv.org/abs/1702.07709 | id:1702.07709 author:Simon S. Du, Sivaraman Balakrishnan, Aarti Singh category:stat.ML cs.DS cs.LG  published:2017-02-24 summary:Many conventional statistical procedures are extremely sensitive to seemingly minor deviations from modeling assumptions. This problem is exacerbated in modern high-dimensional settings, where the problem dimension can grow with and possibly exceed the sample size. We consider the problem of robust estimation of sparse functionals, and provide a computationally and statistically efficient algorithm in the high-dimensional setting. Our theory identifies a unified set of deterministic conditions under which our algorithm guarantees accurate recovery. By further establishing that these deterministic conditions hold with high-probability for a wide range of statistical models, our theory applies to many problems of considerable interest including sparse mean and covariance estimation; sparse linear regression; and sparse generalized linear models. version:1
arxiv-1702-07694 | Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference Learning | http://arxiv.org/abs/1702.07694 | id:1702.07694 author:Stephen N. Pallone, Peter I. Frazier, Shane G. Henderson category:stat.ML cs.IT cs.LG math.IT  published:2017-02-24 summary:We analyze the problem of learning a single user's preferences in an active learning setting, sequentially and adaptively querying the user over a finite time horizon. Learning is conducted via choice-based queries, where the user selects her preferred option among a small subset of offered alternatives. These queries have been shown to be a robust and efficient way to learn an individual's preferences. We take a parametric approach and model the user's preferences through a linear classifier, using a Bayesian prior to encode our current knowledge of this classifier. The rate at which we learn depends on the alternatives offered at every time epoch. Under certain noise assumptions, we show that the Bayes-optimal policy for maximally reducing entropy of the posterior distribution of this linear classifier is a greedy policy, and that this policy achieves a linear lower bound when alternatives can be constructed from the continuum. Further, we analyze a different metric called misclassification error, proving that the performance of the optimal policy that minimizes misclassification error is bounded below by a linear function of differential entropy. Lastly, we numerically compare the greedy entropy reduction policy with a knowledge gradient policy under a number of scenarios, examining their performance under both differential entropy and misclassification error. version:1
arxiv-1702-07680 | Consistent Alignment of Word Embedding Models | http://arxiv.org/abs/1702.07680 | id:1702.07680 author:Cem Safak Sahin, Rajmonda S. Caceres, Brandon Oselio, William M. Campbell category:cs.CL cs.IR stat.ML  published:2017-02-24 summary:Word embedding models offer continuous vector representations that can capture rich contextual semantics based on their word co-occurrence patterns. While these word vectors can provide very effective features used in many NLP tasks such as clustering similar words and inferring learning relationships, many challenges and open research questions remain. In this paper, we propose a solution that aligns variations of the same model (or different models) in a joint low-dimensional latent space leveraging carefully generated synthetic data points. This generative process is inspired by the observation that a variety of linguistic relationships is captured by simple linear operations in embedded space. We demonstrate that our approach can lead to substantial improvements in recovering embeddings of local neighborhoods. version:1
arxiv-1702-07679 | A recommender system to restore images with impulse noise | http://arxiv.org/abs/1702.07679 | id:1702.07679 author:Alfredo Nava-Tudela category:cs.CV stat.ML  published:2017-02-24 summary:We build a collaborative filtering recommender system to restore images with impulse noise for which the noisy pixels have been previously identified. We define this recommender system in terms of a new color image representation using three matrices that depend on the noise-free pixels of the image to restore, and two parameters: $k$, the number of features; and $\lambda$, the regularization factor. We perform experiments on a well known image database to test our algorithm and we provide image quality statistics for the results obtained. We discuss the roles of bias and variance in the performance of our algorithm as determined by the values of $k$ and $\lambda$, and provide guidance on how to choose the values of these parameters. Finally, we discuss the possibility of using our collaborative filtering recommender system to perform image inpainting and super-resolution. version:1
arxiv-1702-07664 | How ConvNets model Non-linear Transformations | http://arxiv.org/abs/1702.07664 | id:1702.07664 author:Dipan K. Pal, Marios Savvides category:cs.CV cs.LG  published:2017-02-24 summary:In this paper, we theoretically address three fundamental problems involving deep convolutional networks regarding invariance, depth and hierarchy. We introduce the paradigm of Transformation Networks (TN) which are a direct generalization of Convolutional Networks (ConvNets). Theoretically, we show that TNs (and thereby ConvNets) are can be invariant to non-linear transformations of the input despite pooling over mere local translations. Our analysis provides clear insights into the increase in invariance with depth in these networks. Deeper networks are able to model much richer classes of transformations. We also find that a hierarchical architecture allows the network to generate invariance much more efficiently than a non-hierarchical network. Our results provide useful insight into these three fundamental problems in deep learning using ConvNets. version:1
arxiv-1702-07652 | Control of Gene Regulatory Networks with Noisy Measurements and Uncertain Inputs | http://arxiv.org/abs/1702.07652 | id:1702.07652 author:Mahdi Imani, Ulisses Braga-Neto category:q-bio.MN cs.LG stat.ML  published:2017-02-24 summary:This paper is concerned with the problem of stochastic control of gene regulatory networks (GRNs) observed indirectly through noisy measurements and with uncertainty in the intervention inputs. The partial observability of the gene states and uncertainty in the intervention process are accounted for by modeling GRNs using the partially-observed Boolean dynamical system (POBDS) signal model with noisy gene expression measurements. Obtaining the optimal infinite-horizon control strategy for this problem is not attainable in general, and we apply reinforcement learning and Gaussian process techniques to find a near-optimal solution. The POBDS is first transformed to a directly-observed Markov Decision Process in a continuous belief space, and the Gaussian process is used for modeling the cost function over the belief and intervention spaces. Reinforcement learning then is used to learn the cost function from the available gene expression data. In addition, we employ sparsification, which enables the control of large partially-observed GRNs. The performance of the resulting algorithm is studied through a comprehensive set of numerical experiments using synthetic gene expression data generated from a melanoma gene regulatory network. version:1
arxiv-1702-07630 | Inertia-Constrained Pixel-by-Pixel Nonnegative Matrix Factorisation: a Hyperspectral Unmixing Method Dealing with Intra-class Variability | http://arxiv.org/abs/1702.07630 | id:1702.07630 author:Charlotte Revel, Yannick Deville, Véronique Achard, Xavier Briottet category:stat.ME cs.CV physics.data-an stat.ML  published:2017-02-24 summary:Blind source separation is a common processing tool to analyse the constitution of pixels of hyperspectral images. Such methods usually suppose that pure pixel spectra (endmembers) are the same in all the image for each class of materials. In the framework of remote sensing, such an assumption is no more valid in the presence of intra-class variabilities due to illumination conditions, weathering, slight variations of the pure materials, etc... In this paper, we first describe the results of investigations highlighting intra-class variability measured in real images. Considering these results, a new formulation of the linear mixing model is presented leading to two new methods. Unconstrained Pixel-by-pixel NMF (UP-NMF) is a new blind source separation method based on the assumption of a linear mixing model, which can deal with intra-class variability. To overcome UP-NMF limitations an extended method is proposed, named Inertia-constrained Pixel-by-pixel NMF (IP-NMF). For each sensed spectrum, these extended versions of NMF extract a corresponding set of source spectra. A constraint is set to limit the spreading of each source's estimates in IP-NMF. The methods are tested on a semi-synthetic data set built with spectra extracted from a real hyperspectral image and then numerically mixed. We thus demonstrate the interest of our methods for realistic source variabilities. Finally, IP-NMF is tested on a real data set and it is shown to yield better performance than state of the art methods. version:1
arxiv-1702-07619 | Fast and robust curve skeletonization for real-world elongated objects | http://arxiv.org/abs/1702.07619 | id:1702.07619 author:Amy Tabb, Henry Medeiros category:cs.CV cs.GR  published:2017-02-24 summary:We consider the problem of extracting curve skeletons of three-dimensional, elongated objects given a noisy surface, which has applications in agricultural contexts such as extracting the branching structure of plants. We describe an efficient and robust method based on breadth-first search that can determine curve skeletons in these contexts. Our approach is capable of automatically detecting junction points as well as spurious segments and loops. All of that is accomplished with only one user-adjustable parameter. The run time of our method ranges from hundreds of milliseconds to less than four seconds on large, challenging datasets, which makes it appropriate for situations where real-time decision making is needed. Experiments on synthetic models as well as on data from real world objects, some of which were collected in challenging field conditions, show that our approach compares favorably to classical thinning algorithms as well as to recent contributions to the field. version:1
arxiv-1702-07611 | Automatic segmentation in dynamic outdoor environments | http://arxiv.org/abs/1702.07611 | id:1702.07611 author:Amy Tabb, Henry Medeiros category:cs.CV  published:2017-02-24 summary:Segmentation in dynamic outdoor environments can be difficult when the illumination levels and other aspects of the scene cannot be controlled. In this paper, we describe a method that uses superpixels to determine low texture regions of the image that correspond to the background material, and then show how this information can be integrated with the color distribution of the image to compute optimal segmentation parameters for traditional binary segmentation as well as to produce silhouette probability maps. We show results of this algorithm in the context of an application for tree modeling. version:1
arxiv-1702-07608 | Microwave breast cancer detection using Empirical Mode Decomposition features | http://arxiv.org/abs/1702.07608 | id:1702.07608 author:Hongchao Song, Yunpeng Li, Mark Coates, Aidong Men category:stat.ML  published:2017-02-24 summary:Microwave-based breast cancer detection has been proposed as a complementary approach to compensate for some drawbacks of existing breast cancer detection techniques. Among the existing microwave breast cancer detection methods, machine learning-type algorithms have recently become more popular. These focus on detecting the existence of breast tumours rather than performing imaging to identify the exact tumour position. A key step of the machine learning approaches is feature extraction. One of the most widely used feature extraction method is principle component analysis (PCA). However, it can be sensitive to signal misalignment. This paper presents an empirical mode decomposition (EMD)-based feature extraction method, which is more robust to the misalignment. Experimental results involving clinical data sets combined with numerically simulated tumour responses show that combined features from EMD and PCA improve the detection performance with an ensemble selection-based classifier. version:1
arxiv-1702-07600 | How hard is it to cross the room? -- Training (Recurrent) Neural Networks to steer a UAV | http://arxiv.org/abs/1702.07600 | id:1702.07600 author:Klaas Kelchtermans, Tinne Tuytelaars category:cs.CV  published:2017-02-24 summary:This work explores the feasibility of steering a drone with a (recurrent) neural network, based on input from a forward looking camera, in the context of a high-level navigation task. We set up a generic framework for training a network to perform navigation tasks based on imitation learning. It can be applied to both aerial and land vehicles. As a proof of concept we apply it to a UAV (Unmanned Aerial Vehicle) in a simulated environment, learning to cross a room containing a number of obstacles. So far only feedforward neural networks (FNNs) have been used to train UAV control. To cope with more complex tasks, we propose the use of recurrent neural networks (RNN) instead and successfully train an LSTM (Long-Short Term Memory) network for controlling UAVs. Vision based control is a sequential prediction problem, known for its highly correlated input data. The correlation makes training a network hard, especially an RNN. To overcome this issue, we investigate an alternative sampling method during training, namely window-wise truncated backpropagation through time (WW-TBPTT). Further, end-to-end training requires a lot of data which often is not available. Therefore, we compare the performance of retraining only the Fully Connected (FC) and LSTM control layers with networks which are trained end-to-end. Performing the relatively simple task of crossing a room already reveals important guidelines and good practices for training neural control networks. Different visualizations help to explain the behavior learned. version:1
arxiv-1702-07285 | Are Emojis Predictable? | http://arxiv.org/abs/1702.07285 | id:1702.07285 author:Francesco Barbieri, Miguel Ballesteros, Horacio Saggion category:cs.CL  published:2017-02-23 summary:Emojis are ideograms which are naturally combined with plain text to visually complement or condense the meaning of a message. Despite being widely used in social media, their underlying semantics have received little attention from a Natural Language Processing standpoint. In this paper, we investigate the relation between words and emojis, studying the novel task of predicting which emojis are evoked by text-based tweet messages. We train several models based on Long Short-Term Memory networks (LSTMs) in this task. Our experimental results show that our neural model outperforms two baselines as well as humans solving the same task, suggesting that computational models are able to better capture the underlying semantics of emojis. version:2
arxiv-1702-07560 | RNN Decoding of Linear Block Codes | http://arxiv.org/abs/1702.07560 | id:1702.07560 author:Eliya Nachmani, Elad Marciano, David Burshtein, Yair Be'ery category:cs.IT cs.LG cs.NE math.IT  published:2017-02-24 summary:Designing a practical, low complexity, close to optimal, channel decoder for powerful algebraic codes with short to moderate block length is an open research problem. Recently it has been shown that a feed-forward neural network architecture can improve on standard belief propagation decoding, despite the large example space. In this paper we introduce a recurrent neural network architecture for decoding linear block codes. Our method shows comparable bit error rate results compared to the feed-forward neural network with significantly less parameters. We also demonstrate improved performance over belief propagation on sparser Tanner graph representations of the codes. Furthermore, we demonstrate that the RNN decoder can be used to improve the performance or alternatively reduce the computational complexity of the mRRD algorithm for low complexity, close to optimal, decoding of short BCH codes. version:1
arxiv-1702-07549 | The Stochastic complexity of spin models: how simple are simple spin models? | http://arxiv.org/abs/1702.07549 | id:1702.07549 author:Alberto Beretta, Claudia Battistin, Clélia de Mulatier, Iacopo Mastromatteo, Matteo Marsili category:cond-mat.dis-nn stat.ML  published:2017-02-24 summary:Models can be simple for different reasons. In science, models are simple when they depend on few variables, they are meant to yield predictions that can be falsified. Parameters are often mutually constrained within scales. They spot symmetries and conservation laws specific of the phenomenon under study. Models in statistical learning are simple because they are easy to evaluate, train and/or to infer. They are simple to interpret in terms of low order dependencies (e.g. graphical models). Their aim is not to uncover fundamental laws but to "generalise well", i.e. to describe well yet unseen data. We show that, for spin models with interactions of arbitrary order, the information theoretic notion of complexity (or simplicity) provided by Minimum Description Length (MDL) conforms with the scientific notion of simplicity rather than with that in machine learning. Simple models in MDL have statistical dependencies concentrated in groups of few variables, they afford predictions on independencies that are easy to falsify. On the contrary, pairwise interacting models, which are often used in statistical learning, appear to be rather complex. version:1
arxiv-1702-07539 | Tight Bounds for Bandit Combinatorial Optimization | http://arxiv.org/abs/1702.07539 | id:1702.07539 author:Alon Cohen, Tamir Hazan, Tomer Koren category:cs.LG  published:2017-02-24 summary:We revisit the study of optimal regret rates in bandit combinatorial optimization---a fundamental framework for sequential decision making under uncertainty that abstracts numerous combinatorial prediction problems. We prove that the attainable regret in this setting grows as $\widetilde{\Theta}(k^{3/2}\sqrt{dT})$ where $d$ is the dimension of the problem and $k$ is a bound over the maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form $\widetilde{\Theta}(k\sqrt{dT})$. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012). version:1
arxiv-1702-07274 | Rotting Bandits | http://arxiv.org/abs/1702.07274 | id:1702.07274 author:Nir Levine, Koby Crammer, Shie Mannor category:stat.ML cs.LG  published:2017-02-23 summary:The Multi-Armed Bandits (MAB) framework highlights the tension between acquiring new knowledge (Exploration) and leveraging available knowledge (Exploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed \textit{Rotting Bandits}, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising, content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees. version:2
arxiv-1702-07508 | Toward high-performance online HCCR: a CNN approach with DropDistortion, path signature and spatial stochastic max-pooling | http://arxiv.org/abs/1702.07508 | id:1702.07508 author:Songxuan Lai, Lianwen Jin, Weixin Yang category:cs.CV  published:2017-02-24 summary:This paper presents an investigation of several techniques that increase the accuracy of online handwritten Chinese character recognition (HCCR). We propose a new training strategy named DropDistortion to train a deep convolutional neural network (DCNN) with distorted samples. DropDistortion gradually lowers the degree of character distortion during training, which allows the DCNN to better generalize. Path signature is used to extract effective features for online characters. Further improvement is achieved by employing spatial stochastic max-pooling as a method of feature map distortion and model averaging. Experiments were carried out on three publicly available datasets, namely CASIA-OLHWDB 1.0, CASIA-OLHWDB 1.1, and the ICDAR2013 online HCCR competition dataset. The proposed techniques yield state-of-the-art recognition accuracies of 97.67%, 97.30%, and 97.99%, respectively. version:1
arxiv-1702-07507 | Use Generalized Representations, But Do Not Forget Surface Features | http://arxiv.org/abs/1702.07507 | id:1702.07507 author:Nafise Sadat Moosavi, Michael Strube category:cs.CL  published:2017-02-24 summary:Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution. version:1
arxiv-1702-07495 | Dirichlet-vMF Mixture Model | http://arxiv.org/abs/1702.07495 | id:1702.07495 author:Shaohua Li category:cs.CL  published:2017-02-24 summary:This document is about the multi-document Von-Mises-Fisher mixture model with a Dirichlet prior, referred to as VMFMix. VMFMix is analogous to Latent Dirichlet Allocation (LDA) in that they can capture the co-occurrence patterns acorss multiple documents. The difference is that in VMFMix, the topic-word distribution is defined on a continuous n-dimensional hypersphere. Hence VMFMix is used to derive topic embeddings, i.e., representative vectors, from multiple sets of embedding vectors. An efficient Variational Expectation-Maximization inference algorithm is derived. The performance of VMFMix on two document classification tasks is reported, with some preliminary analysis. version:1
arxiv-1702-07492 | Robot gains Social Intelligence through Multimodal Deep Reinforcement Learning | http://arxiv.org/abs/1702.07492 | id:1702.07492 author:Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa, Hiroshi Ishiguro category:cs.RO cs.AI cs.CV stat.ML  published:2017-02-24 summary:For robots to coexist with humans in a social world like ours, it is crucial that they possess human-like social interaction skills. Programming a robot to possess such skills is a challenging task. In this paper, we propose a Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like interaction skills through a trial and error method. This paper aims to develop a robot that gathers data during its interaction with a human and learns human interaction behaviour from the high-dimensional sensory information using end-to-end reinforcement learning. This paper demonstrates that the robot was able to learn basic interaction skills successfully, after 14 days of interacting with people. version:1
arxiv-1702-07490 | Online Meta-learning by Parallel Algorithm Competition | http://arxiv.org/abs/1702.07490 | id:1702.07490 author:Stefan Elfwing, Eiji Uchibe, Kenji Doya category:cs.LG  published:2017-02-24 summary:The efficiency of reinforcement learning algorithms depends critically on a few meta-parameters that modulates the learning updates and the trade-off between exploration and exploitation. The adaptation of the meta-parameters is an open question in reinforcement learning, which arguably has become more of an issue recently with the success of deep reinforcement learning in high-dimensional state spaces. The long learning times in domains such as Atari 2600 video games makes it not feasible to perform comprehensive searches of appropriate meta-parameter values. We propose the Online Meta-learning by Parallel Algorithm Competition (OMPAC) method. In the OMPAC method, several instances of a reinforcement learning algorithm are run in parallel with small differences in the initial values of the meta-parameters. After a fixed number of episodes, the instances are selected based on their performance in the task at hand. Before continuing the learning, Gaussian noise is added to the meta-parameters with a predefined probability. We validate the OMPAC method by improving the state-of-the-art results in stochastic SZ-Tetris and in standard Tetris with a smaller, 10$\times$10, board, by 31% and 84%, respectively, and by improving the results for deep Sarsa($\lambda$) agents in three Atari 2600 games by 62% or more. The experiments also show the ability of the OMPAC method to adapt the meta-parameters according to the learning progress in different tasks. version:1
arxiv-1702-07486 | Deep representation learning for human motion prediction and classification | http://arxiv.org/abs/1702.07486 | id:1702.07486 author:Judith Bütepage, Michael Black, Danica Kragic, Hedvig Kjellström category:cs.CV  published:2017-02-24 summary:Generative models of 3D human motion are often restricted to a small number of activities and can therefore not generalize well to novel movements or applications. In this work we propose a deep learning framework for human motion capture data that learns a generic representation from a large corpus of motion capture data and generalizes well to new, unseen, motions. Using an encoding-decoding network that learns to predict future 3D poses from the most recent past, we extract a feature representation of human motion. Most work on deep learning for sequence prediction focuses on video and speech. Since skeletal data has a different structure, we present and evaluate different network architectures that make different assumptions about time dependencies and limb correlations. To quantify the learned features, we use the output of different layers for action classification and visualize the receptive fields of the network units. Our method outperforms the recent state of the art in skeletal motion prediction even though these use action specific training data. Our results show that deep feedforward networks, trained from a generic mocap database, can successfully be used for feature extraction from human motion data and that this representation can be used as a foundation for classification and prediction. version:1
arxiv-1702-07482 | Speckle Reduction with Trained Nonlinear Diffusion Filtering | http://arxiv.org/abs/1702.07482 | id:1702.07482 author:Wensen Feng, Yunjin Chen category:cs.CV  published:2017-02-24 summary:Speckle reduction is a prerequisite for many image processing tasks in synthetic aperture radar (SAR) images, as well as all coherent images. In recent years, predominant state-of-the-art approaches for despeckling are usually based on nonlocal methods which mainly concentrate on achieving utmost image restoration quality, with relatively low computational efficiency. Therefore, in this study we aim to propose an efficient despeckling model with both high computational efficiency and high recovery quality. To this end, we exploit a newly-developed trainable nonlinear reaction diffusion(TNRD) framework which has proven a simple and effective model for various image restoration problems. {In the original TNRD applications, the diffusion network is usually derived based on the direct gradient descent scheme. However, this approach will encounter some problem for the task of multiplicative noise reduction exploited in this study. To solve this problem, we employed a new architecture derived from the proximal gradient descent method.} {Taking into account the speckle noise statistics, the diffusion process for the despeckling task is derived. We then retrain all the model parameters in the presence of speckle noise. Finally, optimized nonlinear diffusion filtering models are obtained, which are specialized for despeckling with various noise levels. Experimental results substantiate that the trained filtering models provide comparable or even better results than state-of-the-art nonlocal approaches. Meanwhile, our proposed model merely contains convolution of linear filters with an image, which offers high level parallelism on GPUs. As a consequence, for images of size $512 \times 512$, our GPU implementation takes less than 0.1 seconds to produce state-of-the-art despeckling performance.} version:1
arxiv-1703-04512 | Normalisation de la langue et de lecriture arabe : enjeux culturels regionaux et mondiaux | http://arxiv.org/abs/1703.04512 | id:1703.04512 author:Henri Hudrisier, Ben Henda Mokhtar category:cs.CY cs.CL 68T50  97C50 H.2.3; I.2.7; I.7.1  published:2017-02-24 summary:Arabic language and writing are now facing a resurgence of international normative solutions that challenge most of their local or network based operating principles. Even if the multilingual digital coding solutions, especially those proposed by Unicode, have solved many difficulties of Arabic writing, the linguistic aspect is still in search of more adapted solutions. Terminology is one of the sectors in which the Arabic language requires a deep modernization of its classical productivity models. The normative approach, in particular that of the ISO TC37, is proposed as one of the solutions that would allow it to combine with international standards to better integrate the knowledge society under construction. La langue et lecriture arabe sont aujourdhui confrontees a une recrudescence de solutions normatives internationales qui remettent en cause la plupart de leurs principes de fonctionnement en site ou sur les reseaux. Meme si les solutions du codage numerique multilingue, notamment celles proposees par Unicode, ont resolu beaucoup de difficultes de lecriture arabe, le volet linguistique est encore en quete de solutions plus adaptees. La terminologie est lun des secteurs dans lequel la langue arabe necessite une modernisation profonde de ses modeles classiques de production. La voie normative, notamment celle du TC37 de ISO, est proposee comme une des solutions qui lui permettrait de se mettre en synergie avec les referentiels internationaux pour mieux integrer la societe du savoir en voie de construction. version:1
arxiv-1702-07475 | Sequence-based Multimodal Apprenticeship Learning For Robot Perception and Decision Making | http://arxiv.org/abs/1702.07475 | id:1702.07475 author:Fei Han, Xue Yang, Yu Zhang, Hao Zhang category:cs.RO cs.AI cs.CV  published:2017-02-24 summary:Apprenticeship learning has recently attracted a wide attention due to its capability of allowing robots to learn physical tasks directly from demonstrations provided by human experts. Most previous techniques assumed that the state space is known a priori or employed simple state representations that usually suffer from perceptual aliasing. Different from previous research, we propose a novel approach named Sequence-based Multimodal Apprenticeship Learning (SMAL), which is capable to simultaneously fusing temporal information and multimodal data, and to integrate robot perception with decision making. To evaluate the SMAL approach, experiments are performed using both simulations and real-world robots in the challenging search and rescue scenarios. The empirical study has validated that our SMAL approach can effectively learn plans for robots to make decisions using sequence of multimodal observations. Experimental results have also showed that SMAL outperforms the baseline methods using individual images. version:1
arxiv-1702-07474 | Simultaneous Feature and Body-Part Learning for Real-Time Robot Awareness of Human Behaviors | http://arxiv.org/abs/1702.07474 | id:1702.07474 author:Fei Han, Xue Yang, Christopher Reardon, Yu Zhang, Hao Zhang category:cs.CV cs.RO  published:2017-02-24 summary:Robot awareness of human actions is an essential research problem in robotics with many important real-world applications, including human-robot collaboration and teaming. Over the past few years, depth sensors have become a standard device widely used by intelligent robots for 3D perception, which can also offer human skeletal data in 3D space. Several methods based on skeletal data were designed to enable robot awareness of human actions with satisfactory accuracy. However, previous methods treated all body parts and features equally important, without the capability to identify discriminative body parts and features. In this paper, we propose a novel simultaneous Feature And Body-part Learning (FABL) approach that simultaneously identifies discriminative body parts and features, and efficiently integrates all available information together to enable real-time robot awareness of human behaviors. We formulate FABL as a regression-like optimization problem with structured sparsity-inducing norms to model interrelationships of body parts and features. We also develop an optimization algorithm to solve the formulated problem, which possesses a theoretical guarantee to find the optimal solution. To evaluate FABL, three experiments were performed using public benchmark datasets, including the MSR Action3D and CAD-60 datasets, as well as a Baxter robot in practical assistive living applications. Experimental results show that our FABL approach obtains a high recognition accuracy with a processing speed of the order-of-magnitude of 10e4 Hz, which makes FABL a promising method to enable real-time robot awareness of human behaviors in practical robotics applications. version:1
arxiv-1702-07472 | Learning Non-local Image Diffusion for Image Denoising | http://arxiv.org/abs/1702.07472 | id:1702.07472 author:Peng Qiao, Yong Dou, Wensen Feng, Yunjin Chen category:cs.CV  published:2017-02-24 summary:Image diffusion plays a fundamental role for the task of image denoising. Recently proposed trainable nonlinear reaction diffusion (TNRD) model defines a simple but very effective framework for image denoising. However, as the TNRD model is a local model, the diffusion behavior of which is purely controlled by information of local patches, it is prone to create artifacts in the homogenous regions and over-smooth highly textured regions, especially in the case of strong noise levels. Meanwhile, it is widely known that the non-local self-similarity (NSS) prior stands as an effective image prior for image denoising, which has been widely exploited in many non-local methods. In this work, we are highly motivated to embed the NSS prior into the TNRD model to tackle its weaknesses. In order to preserve the expected property that end-to-end training is available, we exploit the NSS prior by a set of non-local filters, and derive our proposed trainable non-local reaction diffusion (TNLRD) model for image denoising. Together with the local filters and influence functions, the non-local filters are learned by employing loss-specific training. The experimental results show that the trained TNLRD model produces visually plausible recovered images with more textures and less artifacts, compared to its local versions. Moreover, the trained TNLRD model can achieve strongly competitive performance to recent state-of-the-art image denoising methods in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). version:1
arxiv-1702-07464 | Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning | http://arxiv.org/abs/1702.07464 | id:1702.07464 author:Briland Hitaj, Giuseppe Ateniese, Fernando Perez-Cruz category:cs.CR cs.LG stat.ML  published:2017-02-24 summary:In recent years, a branch of machine learning called Deep Learning has become incredibly popular thanks to the ability of a new class of algorithms to model and interpret a large quantity of data in a similar way to humans. Properly training deep learning models involves collecting a vast amount of users' private data, including habits, geographical positions, interests, and much more. Another major issue is that it is possible to extract from trained models useful information about the training set and this hinders collaboration among distrustful participants or parties that deal with sensitive information. To tackle this problem, collaborative deep learning models have recently been proposed where parties share only a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy to make information extraction even more challenging, as shown by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates valid samples of the targeted training set that was meant to be private. Interestingly, we show that differential privacy applied to shared parameters of the model as suggested at CCS'15 and CCS'16 is utterly futile. In our generative model attack, all techniques adopted to scramble or obfuscate shared parameters in collaborative deep learning are rendered ineffective with no possibility of a remedy under the threat model considered. version:1
arxiv-1702-07462 | Hidden Community Detection in Social Networks | http://arxiv.org/abs/1702.07462 | id:1702.07462 author:Kun He, Yingru Li, Sucheta Soundarajan, John E. Hopcroft category:cs.SI physics.soc-ph stat.ML  published:2017-02-24 summary:We introduce a new paradigm that is important for community detection in the realm of network analysis. Networks contain a set of strong, dominant communities, which interfere with the detection of weak, natural community structure. When most of the members of the weak communities also belong to stronger communities, they are extremely hard to be uncovered. We call the weak communities the hidden community structure. We present a novel approach called HICODE (HIdden COmmunity DEtection) that identifies the hidden community structure as well as the dominant community structure. By weakening the strength of the dominant structure, one can uncover the hidden structure beneath. Likewise, by reducing the strength of the hidden structure, one can more accurately identify the dominant structure. In this way, HICODE tackles both tasks simultaneously. Extensive experiments on real-world networks demonstrate that HICODE outperforms several state-of-the-art community detection methods in uncovering both the dominant and the hidden structure. In the Facebook university social networks, we find multiple non-redundant sets of communities that are strongly associated with residential hall, year of registration or career position of the faculties or students, while the state-of-the-art algorithms mainly locate the dominant ground truth category. In the Due to the difficulty of labeling all ground truth communities in real-world datasets, HICODE provides a promising approach to pinpoint the existing latent communities and uncover communities for which there is no ground truth. Finding this unknown structure is an extremely important community detection problem. version:1
arxiv-1702-07451 | Viewpoint Adaptation for Rigid Object Detection | http://arxiv.org/abs/1702.07451 | id:1702.07451 author:Patrick Wang, Kenneth Morton, Peter Torrione, Leslie Collins category:cs.CV  published:2017-02-24 summary:An object detector performs suboptimally when applied to image data taken from a viewpoint different from the one with which it was trained. In this paper, we present a viewpoint adaptation algorithm that allows a trained single-view object detector to be adapted to a new, distinct viewpoint. We first illustrate how a feature space transformation can be inferred from a known homography between the source and target viewpoints. Second, we show that a variety of trained classifiers can be modified to behave as if that transformation were applied to each testing instance. The proposed algorithm is evaluated on a person detection task using images from the PETS 2007 and CAVIAR datasets, as well as from a new synthetic multi-view person detection dataset. It yields substantial performance improvements when adapting single-view person detectors to new viewpoints, and simultaneously reduces computational complexity. This work has the potential to improve detection performance for cameras viewing objects from arbitrary viewpoints, while simplifying data collection and feature extraction. version:1
arxiv-1702-07450 | Strongly-Typed Agents are Guaranteed to Interact Safely | http://arxiv.org/abs/1702.07450 | id:1702.07450 author:David Balduzzi category:cs.LG cs.AI cs.GT  published:2017-02-24 summary:As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The first result is that gradient descent converges to a Nash equilibrium in safe games. The paper provides sufficient conditions that guarantee safe interactions. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely. A series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation. The analysis introduce a new perspective on classical multilinear games based on tensor decomposition. version:1
arxiv-1702-07444 | Bandits with Movement Costs and Adaptive Pricing | http://arxiv.org/abs/1702.07444 | id:1702.07444 author:Tomer Koren, Roi Livni, Yishay Mansour category:cs.LG cs.GT  published:2017-02-24 summary:We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval $[0,1]$ and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of $\widetilde{O}(\sqrt{kT} + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. When the set of actions corresponds to whole $[0,1]$ interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of $\widetilde{\Theta}(T^{2/3})$, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of $\widetilde{O}(T^{2/3})$ compared to the best fixed price in hindsight, which outperform the previous regret bound of $\widetilde{O}(T^{3/4})$ for the problem. version:1
arxiv-1702-07432 | Multi-Context Attention for Human Pose Estimation | http://arxiv.org/abs/1702.07432 | id:1702.07432 author:Xiao Chu, Wei Yang, Wanli Ouyang, Cheng Ma, Alan L. Yuille, Xiaogang Wang category:cs.CV  published:2017-02-24 summary:In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on the detailed description for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semantic-consistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive field of the network. These units are extensions of residual units with a side branch incorporating filters with larger receptive fields, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. version:1
arxiv-1702-07426 | Control of the Correlation of Spontaneous Neuron Activity in Biological and Noise-activated CMOS Artificial Neural Microcircuits | http://arxiv.org/abs/1702.07426 | id:1702.07426 author:Ramin M. Hasani, Giorgio Ferrari, Hideaki Yamamoto, Sho Kono, Koji Ishihara, Soya Fujimori, Takashi Tanii, Enrico Prati category:cs.NE q-bio.NC q-bio.QM  published:2017-02-24 summary:There are several indications that brain is organized not on a basis of individual unreliable neurons, but on a micro-circuital scale providing Lego blocks employed to create complex architectures. At such an intermediate scale, the firing activity in the microcircuits is governed by collective effects emerging by the background noise soliciting spontaneous firing, the degree of mutual connections between the neurons, and the topology of the connections. We compare spontaneous firing activity of small populations of neurons adhering to an engineered scaffold with simulations of biologically plausible CMOS artificial neuron populations whose spontaneous activity is ignited by tailored background noise. We provide a full set of flexible and low-power consuming silicon blocks including neurons, excitatory and inhibitory synapses, and both white and pink noise generators for spontaneous firing activation. We achieve a comparable degree of correlation of the firing activity of the biological neurons by controlling the kind and the number of connection among the silicon neurons. The correlation between groups of neurons, organized as a ring of four distinct populations connected by the equivalent of interneurons, is triggered more effectively by adding multiple synapses to the connections than increasing the number of independent point-to-point connections. The comparison between the biological and the artificial systems suggests that a considerable number of synapses is active also in biological populations adhering to engineered scaffolds. version:1
arxiv-1702-07424 | Building Usage Profiles Using Deep Neural Nets | http://arxiv.org/abs/1702.07424 | id:1702.07424 author:Domenic Curro, Konstantinos G. Derpanis, Andriy V. Miranskyy category:cs.SE cs.CV  published:2017-02-23 summary:To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we consider publicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network (DCNN) to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos (published on YouTube). In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product. version:1
arxiv-1702-07405 | GapTV: Accurate and Interpretable Low-Dimensional Regression and Classification | http://arxiv.org/abs/1702.07405 | id:1702.07405 author:Wesley Tansey, James G. Scott category:stat.ML  published:2017-02-23 summary:We consider the problem of estimating a regression function in the common situation where the number of features is small, where interpretability of the model is a high priority, and where simple linear or additive models fail to provide adequate performance. To address this problem, we present GapTV, an approach that is conceptually related both to CART and to the more recent CRISP algorithm, a state-of-the-art alternative method for interpretable nonlinear regression. GapTV divides the feature space into blocks of constant value and fits the value of all blocks jointly via a convex optimization routine. Our method is fully data-adaptive, in that it incorporates highly robust routines for tuning all hyperparameters automatically. We compare our approach against CART and CRISP and demonstrate that GapTV finds a much better trade-off between accuracy and interpretability. version:1
arxiv-1702-07400 | Horseshoe Regularization for Feature Subset Selection | http://arxiv.org/abs/1702.07400 | id:1702.07400 author:Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson, Brandon Willard category:stat.ML stat.CO  published:2017-02-23 summary:Feature subset selection arises in many high-dimensional applications in machine learning and statistics, such as compressed sensing and genomics. The $\ell_0$ penalty is ideal for this task, the caveat being it requires the NP-hard combinatorial evaluation of all models. A recent area of considerable interest is to develop efficient algorithms to fit models with a non-convex $\ell_\gamma$ penalty for $\gamma\in (0,1)$, which results in sparser models than the convex $\ell_1$ or lasso penalty, but is harder to fit. We propose an alternative, termed the horseshoe regularization penalty for feature subset selection, and demonstrate its theoretical and computational advantages. The distinguishing feature from existing non-convex optimization approaches is a full probabilistic representation of the penalty as the negative of the logarithm of a suitable prior, which in turn enables an efficient expectation-maximization algorithm for optimization and MCMC for uncertainty quantification. In synthetic and real data, the resulting algorithm provides better statistical performance, and the computation requires a fraction of time of state of the art non-convex solvers. version:1
arxiv-1702-07343 | Improving high-pass fusion method using wavelets | http://arxiv.org/abs/1702.07343 | id:1702.07343 author:Hamid Reza Shahdoosti category:cs.CV  published:2017-02-23 summary:In an appropriate image fusion method, spatial information of the panchromatic image is injected into the multispectral images such that the spectral information is not distorted. The high-pass modulation method is a successful method in image fusion. However, the main drawback of this method is that this technique uses the boxcar filter to extract the high frequency information of the panchromatic image. Using the boxcar filter introduces the ringing effect into the fused image. To cope with this problem, we use the wavelet transform instead of boxcar filters. Then, the results of the proposed method and those of other methods such as, Brovey, IHS, and PCA ones are compared. Experiments show the superiority of the proposed method in terms of correlation coefficient and mutual information. version:1
arxiv-1702-07392 | WaterGAN: Unsupervised Generative Network to Enable Real-time Color Correction of Monocular Underwater Images | http://arxiv.org/abs/1702.07392 | id:1702.07392 author:Jie Li, Katherine A. Skinner, Ryan M. Eustice, Matthew Johnson-Roberson category:cs.CV cs.RO  published:2017-02-23 summary:This paper reports on WaterGAN, a generative adversarial network (GAN) for generating realistic underwater images from in-air image and depth pairings in an unsupervised pipeline used for color correction of monocular underwater images. Cameras onboard autonomous and remotely operated vehicles can capture high resolution images to map the seafloor, however, underwater image formation is subject to the complex process of light propagation through the water column. The raw images retrieved are characteristically different than images taken in air due to effects such as absorption and scattering, which cause attenuation of light at different rates for different wavelengths. While this physical process is well described theoretically, the model depends on many parameters intrinsic to the water column as well as the objects in the scene. These factors make recovery of these parameters difficult without simplifying assumptions or field calibration, hence, restoration of underwater images is a non-trivial problem. Deep learning has demonstrated great success in modeling complex nonlinear systems but requires a large amount of training data, which is difficult to compile in deep sea environments. Using WaterGAN, we generate a large training dataset of paired imagery, both raw underwater and true color in-air, as well as depth data. This data serves as input to a novel end-to-end network for color correction of monocular underwater images. Due to the depth-dependent water column effects inherent to underwater environments, we show that our end-to-end network implicitly learns a coarse depth estimate of the underwater scene from monocular underwater images. Our proposed pipeline is validated with testing on real data collected from both a pure water tank and from underwater surveys in field testing. Source code is made publicly available with sample datasets and pretrained models. version:1
arxiv-1702-07389 | Continuous-Time Visual-Inertial Trajectory Estimation with Event Cameras | http://arxiv.org/abs/1702.07389 | id:1702.07389 author:Elias Mueggler, Guillermo Gallego, Henri Rebecq, Davide Scaramuzza category:cs.RO cs.CV  published:2017-02-23 summary:In contrast to traditional cameras, which output images at a fixed rate, event cameras have independent pixels that output asynchronous pixel-level brightness changes with microsecond resolution. In this paper, we leverage a continuous-time framework to perform trajectory estimation by fusing visual data from a moving event camera with inertial data from an IMU. This framework allows direct integration of the asynchronous events with micro-second accuracy and the inertial measurements at high frequency. The pose trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show superior performance of the proposed technique compared to non-batch event-based algorithms. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework. version:1
arxiv-1702-07386 | Toward Streaming Synapse Detection with Compositional ConvNets | http://arxiv.org/abs/1702.07386 | id:1702.07386 author:Shibani Santurkar, David Budden, Alexander Matveev, Heather Berlin, Hayk Saribekyan, Yaron Meirovitch, Nir Shavit category:cs.CV  published:2017-02-23 summary:Connectomics is an emerging field in neuroscience that aims to reconstruct the 3-dimensional morphology of neurons from electron microscopy (EM) images. Recent studies have successfully demonstrated the use of convolutional neural networks (ConvNets) for segmenting cell membranes to individuate neurons. However, there has been comparatively little success in high-throughput identification of the intercellular synaptic connections required for deriving connectivity graphs. In this study, we take a compositional approach to segmenting synapses, modeling them explicitly as an intercellular cleft co-located with an asymmetric vesicle density along a cell membrane. Instead of requiring a deep network to learn all natural combinations of this compositionality, we train lighter networks to model the simpler marginal distributions of membranes, clefts and vesicles from just 100 electron microscopy samples. These feature maps are then combined with simple rules-based heuristics derived from prior biological knowledge. Our approach to synapse detection is both more accurate than previous state-of-the-art (7% higher recall and 5% higher F1-score) and yields a 20-fold speed-up compared to the previous fastest implementations. We demonstrate by reconstructing the first complete, directed connectome from the largest available anisotropic microscopy dataset (245 GB) of mouse somatosensory cortex (S1) in just 9.7 hours on a single shared-memory CPU system. We believe that this work marks an important step toward the goal of a microscope-pace streaming connectomics pipeline. version:1
arxiv-1702-07371 | Feasibility of Principal Component Analysis in hand gesture recognition system | http://arxiv.org/abs/1702.07371 | id:1702.07371 author:Tanu Srivastava, Raj Shree Singh, Sunil Kumar, Pavan Chakraborty category:cs.CV  published:2017-02-23 summary:Nowadays actions are increasingly being handled in electronic ways, instead of physical interaction. From earlier times biometrics is used in the authentication of a person. It recognizes a person by using a human trait associated with it like eyes (by calculating the distance between the eyes) and using hand gestures, fingerprint detection, face detection etc. Advantages of using these traits for identification are that they uniquely identify a person and cannot be forgotten or lost. These are unique features of a human being which are being used widely to make the human life simpler. Hand gesture recognition system is a powerful tool that supports efficient interaction between the user and the computer. The main moto of hand gesture recognition research is to create a system which can recognise specific hand gestures and use them to convey useful information for device control. This paper presents an experimental study over the feasibility of principal component analysis in hand gesture recognition system. PCA is a powerful tool for analyzing data. The primary goal of PCA is dimensionality reduction. Frames are extracted from the Sheffield KInect Gesture (SKIG) dataset. The implementation is done by creating a training set and then training the recognizer. It uses Eigen space by processing the eigenvalues and eigenvectors of the images in training set. Euclidean distance with the threshold value is used as similarity metric to recognize the gestures. The experimental results show that PCA is feasible to be used for hand gesture recognition system. version:1
arxiv-1702-07367 | Stochastic Newton and Quasi-Newton Methods for Large Linear Least-squares Problems | http://arxiv.org/abs/1702.07367 | id:1702.07367 author:Julianne Chung, Matthias Chung, J. Tanner Slagel, Luis Tenorio category:math.NA cs.NA stat.ML  published:2017-02-23 summary:We describe stochastic Newton and stochastic quasi-Newton approaches to efficiently solve large linear least-squares problems where the very large data sets present a significant computational burden (e.g., the size may exceed computer memory or data are collected in real-time). In our proposed framework, stochasticity is introduced in two different frameworks as a means to overcome these computational limitations, and probability distributions that can exploit structure and/or sparsity are considered. Theoretical results on consistency of the approximations for both the stochastic Newton and the stochastic quasi-Newton methods are provided. The results show, in particular, that stochastic Newton iterates, in contrast to stochastic quasi-Newton iterates, may not converge to the desired least-squares solution. Numerical examples, including an example from extreme learning machines, demonstrate the potential applications of these methods. version:1
arxiv-1702-07339 | A Converse to Banach's Fixed Point Theorem and its CLS Completeness | http://arxiv.org/abs/1702.07339 | id:1702.07339 author:Constantinos Daskalakis, Christos Tzamos, Manolis Zampetakis category:cs.CC cs.LG math.GN stat.ML  published:2017-02-23 summary:Banach's fixed point theorem for contraction maps has been widely used to analyze the convergence of iterative methods in non-convex problems. It is a common experience, however, that iterative maps fail to be globally contracting under the natural metric in their domain, making the applicability of Banach's theorem limited. We explore how generally we can apply Banach's fixed point theorem to establish the convergence of iterative methods when pairing it with carefully designed metrics. Our first result is a strong converse of Banach's theorem, showing that it is a universal analysis tool for establishing uniqueness of fixed points and for bounding the convergence rate of iterative maps to a unique fixed point. In other words, we show that, whenever an iterative map globally converges to a unique fixed point, there exists a metric under which the iterative map is contracting and which can be used to bound the number of iterations until convergence. We illustrate our approach in the widely used power method, providing a new way of bounding its convergence rate through contraction arguments. We next consider the computational complexity of Banach's fixed point theorem. Making the proof of our converse theorem constructive, we show that computing a fixed point whose existence is guaranteed by Banach's fixed point theorem is CLS-complete. We thus provide the first natural complete problem for the class CLS, which was defined in [Daskalakis-Papadimitriou 2011] to capture the complexity of problems such as P-matrix LCP, computing KKT-points, and finding mixed Nash equilibria in congestion and network coordination games. version:1
arxiv-1702-07333 | k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge | http://arxiv.org/abs/1702.07333 | id:1702.07333 author:David Alvarez, Monica Iglesias category:cs.CV  published:2017-02-23 summary:This abstract briefly describes a segmentation algorithm developed for the ISIC 2017 Skin Lesion Detection Competition hosted at [ref]. The objective of the competition is to perform a segmentation (in the form of a binary mask image) of skin lesions in dermoscopic images as close as possible to a segmentation performed by trained clinicians, which is taken as ground truth. This project only takes part in the segmentation phase of the challenge. The other phases of the competition (feature extraction and lesion identification) are not considered. The proposed algorithm consists of 4 steps: (1) lesion image preprocessing, (2) image segmentation using k-means clustering of pixel colors, (3) calculation of a set of features describing the properties of each segmented region, and (4) calculation of a final score for each region, representing the likelihood of corresponding to a suitable lesion segmentation. The scores in step (4) are obtained by averaging the results of 2 different regression models using the scores of each region as input. Before using the algorithm these regression models must be trained using the training set of images and ground truth masks provided by the Competition. Steps 2 to 4 are repeated with an increasing number of clusters (and therefore the image is segmented into more regions) until there is no further improvement of the calculated scores. version:1
arxiv-1702-07324 | Inherent Biases of Recurrent Neural Networks for Phonological Assimilation and Dissimilation | http://arxiv.org/abs/1702.07324 | id:1702.07324 author:Amanda Doucette category:cs.CL  published:2017-02-23 summary:A recurrent neural network model of phonological pattern learning is proposed. The model is a relatively simple neural network with one recurrent layer, and displays biases in learning that mimic observed biases in human learning. Single-feature patterns are learned faster than two-feature patterns, and vowel or consonant-only patterns are learned faster than patterns involving vowels and consonants, mimicking the results of laboratory learning experiments. In non-recurrent models, capturing these biases requires the use of alpha features or some other representation of repeated features, but with a recurrent neural network, these elaborations are not necessary. version:1
arxiv-1702-07319 | Learning to Draw Dynamic Agent Goals with Generative Adversarial Networks | http://arxiv.org/abs/1702.07319 | id:1702.07319 author:Shariq Iqbal, John Pearson category:q-bio.NC cs.LG stat.ML  published:2017-02-23 summary:We address the problem of designing artificial agents capable of reproducing human behavior in a competitive game involving dynamic control. Given data consisting of multiple realizations of inputs generated by pairs of interacting players, we model each agent's actions as governed by a time-varying latent goal state coupled to a control model. These goals, in turn, are described as stochastic processes evolving according to player-specific value functions depending on the current state of the game. We model these value functions using generative adversarial networks (GANs) and show that our GAN-based approach succeeds in producing sample gameplay that captures the rich dynamics of human agents. The latent goal dynamics inferred and generated by our model has applications to fields like neuroscience and animal behavior, where the underlying value functions themselves are of theoretical interest. version:1
arxiv-1702-07306 | Causal Discovery Using Proxy Variables | http://arxiv.org/abs/1702.07306 | id:1702.07306 author:Mateo Rojas-Carulla, Marco Baroni, David Lopez-Paz category:stat.ML cs.LG  published:2017-02-23 summary:Discovering causal relations is fundamental to reasoning and intelligence. In particular, observational causal discovery algorithms estimate the cause-effect relation between two random entities $X$ and $Y$, given $n$ samples from $P(X,Y)$. In this paper, we develop a framework to estimate the cause-effect relation between two static entities $x$ and $y$: for instance, an art masterpiece $x$ and its fraudulent copy $y$. To this end, we introduce the notion of proxy variables, which allow the construction of a pair of random entities $(A,B)$ from the pair of static entities $(x,y)$. Then, estimating the cause-effect relation between $A$ and $B$ using an observational causal discovery algorithm leads to an estimation of the cause-effect relation between $x$ and $y$. For example, our framework detects the causal relation between unprocessed photographs and their modifications, and orders in time a set of shuffled frames from a video. As our main case study, we introduce a human-elicited dataset of 10,000 pairs of casually-linked pairs of words from natural language. Our methods discover 75% of these causal relations. Finally, we discuss the role of proxy variables in machine learning, as a general tool to incorporate static knowledge into prediction tasks. version:1
arxiv-1702-07305 | Online Multiclass Boosting | http://arxiv.org/abs/1702.07305 | id:1702.07305 author:Young Hun Jung, Ambuj Tewari category:stat.ML cs.LG  published:2017-02-23 summary:Recent work has extended the theoretical analysis of boosting algorithms to multiclass problems and online settings. However, the multiclass extension is in the batch setting and the online extensions only consider binary classification. To the best of our knowledge, there exists no framework to analyze online boosting algorithms for multiclass classification. We fill this gap in the literature by defining, and justifying, a weak learning condition for online multiclass boosting. We also provide an algorithm called online multiclass boost-by-majority to optimally combine weak learners in our setting. version:1
arxiv-1702-07254 | Sobolev Norm Learning Rates for Regularized Least-Squares Algorithm | http://arxiv.org/abs/1702.07254 | id:1702.07254 author:Simon Fischer, Ingo Steinwart category:stat.ML  published:2017-02-23 summary:Learning rates for regularized least-squares algorithms are in most cases expressed with respect to the excess risk, or equivalently, the $L_2$-norm. For some applications, however, guarantees with respect to stronger norms such as the $L_\infty$-norm, are desirable. We address this problem by establishing learning rates for a continuous scale of norms between the $L_2$- and the RKHS norm. As a byproduct we derive $L_\infty$-norm learning rates, and in the case of Sobolev RKHSs we actually obtain Sobolev norm learning rates, which may also imply $L_\infty$-norm rates for some derivatives. In all cases, we do not need to assume the target function to be contained in the used RKHS. Finally, we show that in many cases the derived rates are minimax optimal. version:1
arxiv-1702-05970 | Automatic Liver and Tumor Segmentation of CT and MRI Volumes using Cascaded Fully Convolutional Neural Networks | http://arxiv.org/abs/1702.05970 | id:1702.05970 author:Patrick Ferdinand Christ, Florian Ettlinger, Felix Grün, Mohamed Ezzeldin A. Elshaera, Jana Lipkova, Sebastian Schlecht, Freba Ahmaddy, Sunil Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Felix Hofmann, Melvin D Anastasi, Seyed-Ahmad Ahmadi, Georgios Kaissis, Julian Holch, Wieland Sommer, Rickmer Braren, Volker Heinemann, Bjoern Menze category:cs.CV cs.AI  published:2017-02-20 summary:Automatic segmentation of the liver and hepatic lesions is an important step towards deriving quantitative biomarkers for accurate clinical diagnosis and computer-aided decision support systems. This paper presents a method to automatically segment liver and lesions in CT and MRI abdomen images using cascaded fully convolutional neural networks (CFCNs) enabling the segmentation of a large-scale medical trial or quantitative image analysis. We train and cascade two FCNs for a combined segmentation of the liver and its lesions. In the first step, we train a FCN to segment the liver as ROI input for a second FCN. The second FCN solely segments lesions within the predicted liver ROIs of step 1. CFCN models were trained on an abdominal CT dataset comprising 100 hepatic tumor volumes. Validations on further datasets show that CFCN-based semantic liver and lesion segmentation achieves Dice scores over 94% for liver with computation times below 100s per volume. We further experimentally demonstrate the robustness of the proposed method on an 38 MRI liver tumor volumes and the public 3DIRCAD dataset. version:2
arxiv-1702-07211 | A minimax and asymptotically optimal algorithm for stochastic bandits | http://arxiv.org/abs/1702.07211 | id:1702.07211 author:Pierre Ménard, Aurélien Garivier category:stat.ML cs.LG math.ST stat.TH  published:2017-02-23 summary:We propose the kl-UCB ++ algorithm for regret minimization in stochastic bandit models with exponential families of distributions. We prove that it is simultaneously asymptotically optimal (in the sense of Lai and Robbins' lower bound) and minimax optimal. This is the first algorithm proved to enjoy these two properties at the same time. This work thus merges two different lines of research, with simple proofs involving no complexity overhead. version:1
arxiv-1702-07203 | Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages | http://arxiv.org/abs/1702.07203 | id:1702.07203 author:Anoop Kunchukuttan, Maulik Shah, Pradyot Prakash, Pushpak Bhattacharyya category:cs.CL  published:2017-02-23 summary:We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting. version:1
arxiv-1702-07191 | ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection | http://arxiv.org/abs/1702.07191 | id:1702.07191 author:Yikang Li, Wanli Ouyang, Xiaogang Wang category:cs.CV  published:2017-02-23 summary:As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers' attention because of its descriptive power and clear structure. It localizes the objects and captures their interactions with a subject-predicate-object triplet, e.g. person-ride-horse. In this paper, the visual relationship is considered as a phrase with three components. So we formulate the visual relationship detection as three inter-connected recognition problems and propose a Visual Phrase reasoning Convolutional Neural Network (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a Visual Phrase Reasoning Structure (VPRS) to set up the connection among the relationship components and help the model consider the three problems jointly. Corresponding non-maximum suppression method and model training strategy are also proposed. Experimental results show that our ViP-CNN outperforms the state-of-art method both in speed and accuracy. We further pretrain our model on our cleansed Visual Genome Relationship dataset, which is found to perform better than the pretraining on the ImageNet for this task. version:1
arxiv-1702-07190 | Spectral Clustering using PCKID - A Probabilistic Cluster Kernel for Incomplete Data | http://arxiv.org/abs/1702.07190 | id:1702.07190 author:Sigurd Løkse, Filippo Maria Bianchi, Arnt-Børre Salberg, Robert Jenssen category:stat.ML  published:2017-02-23 summary:In this paper, we propose PCKID, a novel, robust, kernel function for spectral clustering, specifically designed to handle incomplete data. By combining posterior distributions of Gaussian Mixture Models for incomplete data on different scales, we are able to learn a kernel for incomplete data that does not depend on any critical hyperparameters, unlike the commonly used RBF kernel. To evaluate our method, we perform experiments on two real datasets. PCKID outperforms the baseline methods for all fractions of missing values and in some cases outperforms the baseline methods with up to 25 percentage points. version:1
arxiv-1702-07189 | Analyzing Learned Convnet Features with Dirichlet Process Gaussian Mixture Models | http://arxiv.org/abs/1702.07189 | id:1702.07189 author:David Malmgren-Hansen, Allan Aasbjerg Nielsen, Rasmus Engholm category:cs.CV  published:2017-02-23 summary:Convolutional Neural Networks (Convnets) have achieved good results in a range of computer vision tasks the recent years. Though given a lot of attention, visualizing the learned representations to interpret Convnets, still remains a challenging task. The high dimensionality of internal representations and the high abstractions of deep layers are the main challenges when visualizing Convnet functionality. We present in this paper a technique based on clustering internal Convnet representations with a Dirichlet Process Gaussian Mixture Model, for visualization of learned representations in Convnets. Our method copes with the high dimensionality of a Convnet by clustering representations across all nodes of each layer. We will discuss how this application is useful when considering transfer learning, i.e.\ transferring a model trained on one dataset to solve a task on a different one. version:1
arxiv-1702-07186 | Stability of Topic Modeling via Matrix Factorization | http://arxiv.org/abs/1702.07186 | id:1702.07186 author:Mark Belford, Brian Mac Namee, Derek Greene category:cs.IR cs.CL cs.LG stat.ML  published:2017-02-23 summary:Topic models can provide us with an insight into the underlying latent structure of a large corpus of documents. A range of methods have been proposed in the literature, including probabilistic topic models and techniques based on matrix factorization. However, in both cases, standard implementations rely on stochastic elements in their initialization phase, which can potentially lead to different results being generated on the same corpus when using the same parameter values. This corresponds to the concept of "instability" which has previously been studied in the context of $k$-means clustering. In many applications of topic modeling, this problem of instability is not considered and topic models are treated as being definitive, even though the results may change considerably if the initialization process is altered. In this paper we demonstrate the inherent instability of popular topic modeling approaches, using a number of new measures to assess stability. To address this issue in the context of matrix factorization for topic modeling, we propose the use of ensemble learning strategies. Based on experiments performed on annotated text corpora, we show that a K-Fold ensemble strategy, combining both ensembles and structured initialization, can significantly reduce instability, while simultaneously yielding more accurate topic models. version:1
arxiv-1703-02635 | A Computational Model of a Single-Photon Avalanche Diode Sensor for Transient Imaging | http://arxiv.org/abs/1703.02635 | id:1703.02635 author:Quercus Hernandez, Diego Gutierrez, Adrian Jarabo category:physics.ins-det cs.CV cs.GR  published:2017-02-23 summary:Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable to collect extremely fast low-energy events, due to their single-photon sensibility. This makes them very suitable for time-of-flight-based range imaging systems, allowing to reduce costs and power requirements, without sacrifizing much temporal resolution. In this work we describe a computational model to simulate the behaviour of SPAD sensors, aiming to provide a realistic camera model for time-resolved light transport simulation, with applications on prototyping new reconstructions techniques based on SPAD time-of-flight data. Our model accounts for the major effects of the sensor on the incoming signal. We compare our model against real-world measurements, and apply it to a variety of scenarios, including complex multiply-scattered light transport. version:1
arxiv-1105-3351 | Splitting method for spatio-temporal search efforts planning | http://arxiv.org/abs/1105.3351 | id:1105.3351 author:Chouchane Mathieu, Paris Sébastien, Le Gland François, Ouladsine Mustapha category:cs.NE cs.SY math.OC  published:2011-05-17 summary:This article deals with the spatio-temporal sensors deployment in order to maximize detection probability of an intelligent and randomly moving target in an area under surveillance. Our work is based on the rare events simulation framework. More precisely, we derive a novel stochastic optimization algorithm based on the generalized splitting method. This new approach offers promising results without any state-space discretization and can handle various types of constraints. version:2
arxiv-1702-07125 | Automatic Representation for Lifetime Value Recommender Systems | http://arxiv.org/abs/1702.07125 | id:1702.07125 author:Assaf Hallak, Yishay Mansour, Elad Yom-Tov category:stat.ML cs.LG  published:2017-02-23 summary:Many modern commercial sites employ recommender systems to propose relevant content to users. While most systems are focused on maximizing the immediate gain (clicks, purchases or ratings), a better notion of success would be the lifetime value (LTV) of the user-system interaction. The LTV approach considers the future implications of the item recommendation, and seeks to maximize the cumulative gain over time. The Reinforcement Learning (RL) framework is the standard formulation for optimizing cumulative successes over time. However, RL is rarely used in practice due to its associated representation, optimization and validation techniques which can be complex. In this paper we propose a new architecture for combining RL with recommendation systems which obviates the need for hand-tuned features, thus automating the state-space representation construction process. We analyze the practical difficulties in this formulation and test our solutions on batch off-line real-world recommendation data. version:1
arxiv-1702-07121 | Consistent On-Line Off-Policy Evaluation | http://arxiv.org/abs/1702.07121 | id:1702.07121 author:Assaf Hallak, Shie Mannor category:stat.ML cs.LG  published:2017-02-23 summary:The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme. However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied. In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD($\lambda$, $\beta$)) algorithm that addresses this issue and reduces this bias at some computational expense. We show that COP-TD($\lambda$, $\beta$) can be designed to converge to the same value that would have been obtained by using on-policy TD($\lambda$) with the target policy. Subsequently, the proposed scheme leads to a related and promising heuristic we call log-COP-TD($\lambda$, $\beta$). Both algorithms have favorable empirical results to the current state of the art on-line OPE algorithms. Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning. version:1
arxiv-1702-03118 | Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning | http://arxiv.org/abs/1702.03118 | id:1702.03118 author:Stefan Elfwing, Eiji Uchibe, Kenji Doya category:cs.LG  published:2017-02-10 summary:In recent years, neural networks have enjoyed a renaissance as function approximators in reinforcement learning. Two decades after Teasauro's TD-Gammon achieved near top-level human performance in backgammon, the deep reinforcement learning algorithm DQN (combining Q-learning with a deep neural network, experience replay, and a separate target network) achieved human-level performance in many Atari 2600 games. The purpose of this study is twofold. First, based on the expected energy restricted Boltzmann machine (EE-RBM), we propose two activation functions for neural network function approximation in reinforcement learning: the sigmoid-weighted linear (SiL) unit and its derivative function (SiLd1). The activation of the SiL unit is computed by the sigmoid function multiplied by its input, which is equal to the contribution to the output from one hidden unit in an EE-RBM. Second, we suggest that the more traditional approach of using on-policy learning with eligibility traces, instead of experience replay, and softmax action selection can be competitive with DQN, without the need for a separate target network. We validate our proposed approach by, first, achieving new state-of-the-art results in both stochastic SZ-Tetris and Tetris with a small 10x10 board, using TD($\lambda$) learning and shallow SiLd1 network agents, and, then, outperforming DQN in the Atari 2600 domain by using a deep Sarsa($\lambda$) agent with SiL and SiLd1 hidden units. version:2
arxiv-1702-07117 | LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations | http://arxiv.org/abs/1702.07117 | id:1702.07117 author:Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu Rong category:cs.CL  published:2017-02-23 summary:Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks. version:1
arxiv-1702-07103 | Discriminating Traces with Time | http://arxiv.org/abs/1702.07103 | id:1702.07103 author:Saeid Tizpaz-Niari, Pavol Cerny, Bor-Yuh Evan Chang, Sriram Sankaranarayanan, Ashutosh Trivedi category:cs.PL cs.CR cs.FL cs.LG cs.SE  published:2017-02-23 summary:What properties about the internals of a program explain the possible differences in its overall running time for different inputs? In this paper, we propose a formal framework for considering this question we dub trace-set discrimination. We show that even though the algorithmic problem of computing maximum likelihood discriminants is NP-hard, approaches based on integer linear programming (ILP) and decision tree learning can be useful in zeroing-in on the program internals. On a set of Java benchmarks, we find that compactly-represented decision trees scalably discriminate with high accuracy---more scalably than maximum likelihood discriminants and with comparable accuracy. We demonstrate on three larger case studies how decision-tree discriminants produced by our tool are useful for debugging timing side-channel vulnerabilities (i.e., where a malicious observer infers secrets simply from passively watching execution times) and availability vulnerabilities. version:1
arxiv-1702-07092 | A Neural Attention Model for Categorizing Patient Safety Events | http://arxiv.org/abs/1702.07092 | id:1702.07092 author:Arman Cohan, Allan Fong, Nazli Goharian, Raj Ratwani category:cs.CL cs.IR  published:2017-02-23 summary:Medical errors are leading causes of death in the US and as such, prevention of these errors is paramount to promoting health care. Patient Safety Event reports are narratives describing potential adverse events to the patients and are important in identifying and preventing medical errors. We present a neural network architecture for identifying the type of safety events which is the first step in understanding these narratives. Our proposed model is based on a soft neural attention model to improve the effectiveness of encoding long sequences. Empirical results on two large-scale real-world datasets of patient safety reports demonstrate the effectiveness of our method with significant improvements over existing methods. version:1
arxiv-1702-07083 | Scalable Inference for Nested Chinese Restaurant Process Topic Models | http://arxiv.org/abs/1702.07083 | id:1702.07083 author:Jianfei Chen, Jun Zhu, Jie Lu, Shixia Liu category:stat.ML cs.DC cs.IR cs.LG  published:2017-02-23 summary:Nested Chinese Restaurant Process (nCRP) topic models are powerful nonparametric Bayesian methods to extract a topic hierarchy from a given text corpus, where the hierarchical structure is automatically determined by the data. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of nCRP topic models. However, hLDA has only been evaluated at small scale, because the existing collapsed Gibbs sampling and instantiated weight variational inference algorithms either are not scalable or sacrifice inference quality with mean-field assumptions. Moreover, an efficient distributed implementation of the data structures, such as dynamically growing count matrices and trees, is challenging. In this paper, we propose a novel partially collapsed Gibbs sampling (PCGS) algorithm, which combines the advantages of collapsed and instantiated weight algorithms to achieve good scalability as well as high model quality. An initialization strategy is presented to further improve the model quality. Finally, we propose an efficient distributed implementation of PCGS through vectorization, pre-processing, and a careful design of the concurrent data structures and communication strategy. Empirical studies show that our algorithm is 111 times more efficient than the previous open-source implementation for hLDA, with comparable or even better model quality. Our distributed implementation can extract 1,722 topics from a 131-million-document corpus with 28 billion tokens, which is 4-5 orders of magnitude larger than the previous largest corpus, with 50 machines in 7 hours. version:1
arxiv-1702-06237 | Exact tensor completion with sum-of-squares | http://arxiv.org/abs/1702.06237 | id:1702.06237 author:Aaron Potechin, David Steurer category:cs.LG cs.CC cs.DS cs.IT math.IT stat.ML  published:2017-02-21 summary:We obtain the first polynomial-time algorithm for exact tensor completion that improves over the bound implied by reduction to matrix completion. The algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed entries of the tensor. This bound improves over the previous best one of $r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound also matches the best known results for the easier problem of approximate tensor completion (Barak & Moitra, 2015). Our algorithm and analysis extends seminal results for exact matrix completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares method. The main technical challenge is to show that a small number of randomly chosen monomials are enough to construct a degree-3 polynomial with precisely planted orthogonal global optima over the sphere and that this fact can be certified within the sum-of-squares proof system. version:2
arxiv-1702-07071 | Pronunciation recognition of English phonemes /\textipa{@}/, /æ/, /\textipa{A}:/ and /\textipa{2}/ using Formants and Mel Frequency Cepstral Coefficients | http://arxiv.org/abs/1702.07071 | id:1702.07071 author:Keith Y. Patarroyo, Vladimir Vargas-Calderón category:cs.CL cs.SD  published:2017-02-23 summary:The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /\textipa{@}/, /{\ae}/, /\textipa{A}:/ and /\textipa{2}/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70\% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 \% was achieved and an accuracy of 71\% when /\textipa{@}/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does. version:1
arxiv-1702-07066 | A Unified Parallel Algorithm for Regularized Group PLS Scalable to Big Data | http://arxiv.org/abs/1702.07066 | id:1702.07066 author:Pierre Lafaye de Micheaux, Benoit Liquet, Matthew Sutton category:stat.ML  published:2017-02-23 summary:Partial Least Squares (PLS) methods have been heavily exploited to analyse the association between two blocs of data. These powerful approaches can be applied to data sets where the number of variables is greater than the number of observations and in presence of high collinearity between variables. Different sparse versions of PLS have been developed to integrate multiple data sets while simultaneously selecting the contributing variables. Sparse modelling is a key factor in obtaining better estimators and identifying associations between multiple data sets. The cornerstone of the sparsity version of PLS methods is the link between the SVD of a matrix (constructed from deflated versions of the original matrices of data) and least squares minimisation in linear regression. We present here an accurate description of the most popular PLS methods, alongside their mathematical proofs. A unified algorithm is proposed to perform all four types of PLS including their regularised versions. Various approaches to decrease the computation time are offered, and we show how the whole procedure can be scalable to big data sets. version:1
arxiv-1702-07064 | Learning Model Predictive Control for Iterative Tasks: A Computationally Efficient Approach for Linear System | http://arxiv.org/abs/1702.07064 | id:1702.07064 author:Ugo Rosolia, Francesco Borrelli category:math.OC cs.LG  published:2017-02-23 summary:A Learning Model Predictive Controller (LMPC) for linear system in presented. The proposed controller is an extension of the LMPC [1] and it aims to decrease the computational burden. The control scheme is reference-free and is able to improve its performance by learning from previous iterations. A convex safe set and a terminal cost function are used in order to guarantee recursive feasibility and non- increasing performance at each iteration. The paper presents the control design approach, and shows how to recursively construct the convex terminal set and the terminal cost from state and input trajectories of previous iterations. Simulation results show the effectiveness of the proposed control logic. version:1
arxiv-1702-07059 | Robust and fully automated segmentation of mandible from CT scans | http://arxiv.org/abs/1702.07059 | id:1702.07059 author:Neslisah Torosdagli, Denise K. Liberton, Payal Verma, Murat Sincan Janice Lee, Sumanta Pattanaik, Ulas Bagci category:cs.CV  published:2017-02-23 summary:Mandible bone segmentation from computed tomography (CT) scans is challenging due to mandible's structural irregularities, complex shape patterns, and lack of contrast in joints. Furthermore, connections of teeth to mandible and mandible to remaining parts of the skull make it extremely difficult to identify mandible boundary automatically. This study addresses these challenges by proposing a novel framework where we define the segmentation as two complementary tasks: recognition and delineation. For recognition, we use random forest regression to localize mandible in 3D. For delineation, we propose to use 3D gradient-based fuzzy connectedness (FC) image segmentation algorithm, operating on the recognized mandible sub-volume. Despite heavy CT artifacts and dental fillings, consisting half of the CT image data in our experiments, we have achieved highly accurate detection and delineation results. Specifically, detection accuracy more than 96% (measured by union of intersection (UoI)), the delineation accuracy of 91% (measured by dice similarity coefficient), and less than 1 mm in shape mismatch (Hausdorff Distance) were found. version:1
arxiv-1702-07054 | Learning Chained Deep Features and Classifiers for Cascade in Object Detection | http://arxiv.org/abs/1702.07054 | id:1702.07054 author:Wanli Ouyang, Ku Wang, Xin Zhu, Xiaogang Wang category:cs.CV  published:2017-02-23 summary:Cascade is a widely used approach that rejects obvious negative samples at early stages for learning better classifier and faster inference. This paper presents chained cascade network (CC-Net). In this CC-Net, the cascaded classifier at a stage is aided by the classification scores in previous stages. Feature chaining is further proposed so that the feature learning for the current cascade stage uses the features in previous stages as the prior information. The chained ConvNet features and classifiers of multiple stages are jointly learned in an end-to-end network. In this way, features and classifiers at latter stages handle more difficult samples with the help of features and classifiers in previous stages. It yields consistent boost in detection performance on benchmarks like PASCAL VOC 2007 and ImageNet. Combined with better region proposal, CC-Net leads to state-of-the-art result of 81.1% mAP on PASCAL VOC 2007. version:1
arxiv-1702-07046 | Feature Generation for Robust Semantic Role Labeling | http://arxiv.org/abs/1702.07046 | id:1702.07046 author:Travis Wolfe, Mark Dredze, Benjamin Van Durme category:cs.CL  published:2017-02-22 summary:Hand-engineered feature sets are a well understood method for creating robust NLP models, but they require a lot of expertise and effort to create. In this work we describe how to automatically generate rich feature sets from simple units called featlets, requiring less engineering. Using information gain to guide the generation process, we train models which rival the state of the art on two standard Semantic Role Labeling datasets with almost no task or linguistic insight. version:1
arxiv-1702-07028 | On the ability of neural nets to express distributions | http://arxiv.org/abs/1702.07028 | id:1702.07028 author:Holden Lee, Rong Ge, Andrej Risteski, Tengyu Ma, Sanjeev Arora category:cs.LG  published:2017-02-22 summary:Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution---also theoretically not understood---concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with $n$ hidden layers. A key ingredient is Barron's Theorem \cite{Barron1993}, which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of $n$ functions which satisfy certain Fourier conditions ("Barron functions") can be approximated by a $n+1$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance---a natural metric on probability distributions---by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone. version:1
arxiv-1702-07019 | CT Image Denoising with Perceptive Deep Neural Networks | http://arxiv.org/abs/1702.07019 | id:1702.07019 author:Qingsong Yang, Pingkun Yan, Mannudeep K. Kalra, Ge Wang category:cs.CV  published:2017-02-22 summary:Increasing use of CT in modern medical practice has raised concerns over associated radiation dose. Reduction of radiation dose associated with CT can increase noise and artifacts, which can adversely affect diagnostic confidence. Denoising of low-dose CT images on the other hand can help improve diagnostic confidence, which however is a challenging problem due to its ill-posed nature, since one noisy image patch may correspond to many different output patches. In the past decade, machine learning based approaches have made quite impressive progress in this direction. However, most of those methods, including the recently popularized deep learning techniques, aim for minimizing mean-squared-error (MSE) between a denoised CT image and the ground truth, which results in losing important structural details due to over-smoothing, although the PSNR based performance measure looks great. In this work, we introduce a new perceptual similarity measure as the objective function for a deep convolutional neural network to facilitate CT image denoising. Instead of directly computing MSE for pixel-to-pixel intensity loss, we compare the perceptual features of a denoised output against those of the ground truth in a feature space. Therefore, our proposed method is capable of not only reducing the image noise levels, but also keeping the critical structural information at the same time. Promising results have been obtained in our experiments with a large number of CT images. version:1
arxiv-1702-07015 | Unsupervised Learning of Morphological Forests | http://arxiv.org/abs/1702.07015 | id:1702.07015 author:Jiaming Luo, Karthik Narasimhan, Regina Barzilay category:cs.CL  published:2017-02-22 summary:This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results. version:1
arxiv-1702-07013 | Learning Hawkes Processes from Short Doubly-Censored Event Sequences | http://arxiv.org/abs/1702.07013 | id:1702.07013 author:Hongteng Xu, Dixin Luo, Hongyuan Zha category:cs.LG math.PR stat.ML  published:2017-02-22 summary:Many real-world applications require robust algorithms to learn point process models based on a type of incomplete data --- the so-called short doubly-censored (SDC) event sequences. In this paper, we study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the general idea of data synthesis. In particular, given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method --- sampling predecessor and successor for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences. The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both time-invariant and time-varying Hawkes processes. version:1
arxiv-1702-07006 | Synthesising Dynamic Textures using Convolutional Neural Networks | http://arxiv.org/abs/1702.07006 | id:1702.07006 author:Christina M. Funke, Leon A. Gatys, Alexander S. Ecker, Matthias Bethge category:cs.CV  published:2017-02-22 summary:Here we present a parametric model for dynamic textures. The model is based on spatiotemporal summary statistics computed from the feature representations of a Convolutional Neural Network (CNN) trained on object recognition. We demonstrate how the model can be used to synthesise new samples of dynamic textures and to predict motion in simple movies. version:1
arxiv-1702-07005 | Large-Scale Stochastic Learning using GPUs | http://arxiv.org/abs/1702.07005 | id:1702.07005 author:Thomas Parnell, Celestine Dünner, Kubilay Atasu, Manolis Sifalakis, Haris Pozidis category:cs.LG cs.DC  published:2017-02-22 summary:In this work we propose an accelerated stochastic learning system for very large-scale applications. Acceleration is achieved by mapping the training algorithm onto massively parallel processors: we demonstrate a parallel, asynchronous GPU implementation of the widely used stochastic coordinate descent/ascent algorithm that can provide up to 35x speed-up over a sequential CPU implementation. In order to train on very large datasets that do not fit inside the memory of a single GPU, we then consider techniques for distributed stochastic learning. We propose a novel method for optimally aggregating model updates from worker nodes when the training data is distributed either by example or by feature. Using this technique, we demonstrate that one can scale out stochastic learning across up to 8 worker nodes without any significant loss of training time. Finally, we combine GPU acceleration with the optimized distributed method to train on a dataset consisting of 200 million training examples and 75 million features. We show by scaling out across 4 GPUs, one can attain a high degree of training accuracy in around 4 seconds: a 20x speed-up in training time compared to a multi-threaded, distributed implementation across 4 CPUs. version:1
arxiv-1702-08516 | Lensless computational imaging through deep learning | http://arxiv.org/abs/1702.08516 | id:1702.08516 author:Ayan Sinha, Justin Lee, Shuai Li, George Barbastathis category:cs.CV physics.optics  published:2017-02-22 summary:Deep learning has been proven to yield reliably generalizable answers to numerous classification and decision tasks. Here, we demonstrate for the first time, to our knowledge, that deep neural networks (DNNs) can be trained to solve inverse problems in computational imaging. We experimentally demonstrate a lens-less imaging system where a DNN was trained to recover a phase object given a raw intensity image recorded some distance away. version:1
arxiv-1702-06980 | On Polynomial Time Methods for Exact Low Rank Tensor Completion | http://arxiv.org/abs/1702.06980 | id:1702.06980 author:Dong Xia, Ming Yuan category:stat.ML cs.IT cs.LG math.IT  published:2017-02-22 summary:In this paper, we investigate the sample size requirement for exact recovery of a high order tensor of low rank from a subset of its entries. We show that a gradient descent algorithm with initial value obtained from a spectral method can, in particular, reconstruct a ${d\times d\times d}$ tensor of multilinear ranks $(r,r,r)$ with high probability from as few as $O(r^{7/2}d^{3/2}\log^{7/2}d+r^7d\log^6d)$ entries. In the case when the ranks $r=O(1)$, our sample size requirement matches those for nuclear norm minimization (Yuan and Zhang, 2016a), or alternating least squares assuming orthogonal decomposability (Jain and Oh, 2014). Unlike these earlier approaches, however, our method is efficient to compute, easy to implement, and does not impose extra structures on the tensor. Numerical results are presented to further demonstrate the merits of the proposed approach. version:1
arxiv-1702-06976 | Heavy-Tailed Analogues of the Covariance Matrix for ICA | http://arxiv.org/abs/1702.06976 | id:1702.06976 author:Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher category:cs.LG stat.ML  published:2017-02-22 summary:Independent Component Analysis (ICA) is the problem of learning a square matrix $A$, given samples of $X=AS$, where $S$ is a random vector with independent coordinates. Most existing algorithms are provably efficient only when each $S_i$ has finite and moderately valued fourth moment. However, there are practical applications where this assumption need not be true, such as speech and finance. Algorithms have been proposed for heavy-tailed ICA, but they are not practical, using random walks and the full power of the ellipsoid algorithm multiple times. The main contributions of this paper are: (1) A practical algorithm for heavy-tailed ICA that we call HTICA. We provide theoretical guarantees and show that it outperforms other algorithms in some heavy-tailed regimes, both on real and synthetic data. Like the current state-of-the-art, the new algorithm is based on the centroid body (a first moment analogue of the covariance matrix). Unlike the state-of-the-art, our algorithm is practically efficient. To achieve this, we use explicit analytic representations of the centroid body, which bypasses the use of the ellipsoid method and random walks. (2) We study how heavy tails affect different ICA algorithms, including HTICA. Somewhat surprisingly, we show that some algorithms that use the covariance matrix or higher moments can successfully solve a range of ICA instances with infinite second moment. We study this theoretically and experimentally, with both synthetic and real-world heavy-tailed data. version:1
arxiv-1702-06972 | Approximations of the Restless Bandit Problem | http://arxiv.org/abs/1702.06972 | id:1702.06972 author:Steffen Grunewalder, Azadeh Khaleghi category:math.ST cs.LG math.PR stat.ML stat.TH  published:2017-02-22 summary:The multi-armed restless bandit problem is studied in the case where the pay-offs are not necessarily independent over time nor across the arms. Even though this version of the problem provides a more realistic model for most real-world applications, it cannot be optimally solved in practice since it is known to be PSPACE-hard. The objective of this paper is to characterize special sub-classes of the problem where good approximate solutions can be found using tractable approaches. Specifically, it is shown that in the case where the joint distribution over the arms is $\varphi$-mixing, and under some conditions on the $\varphi$-mixing coefficients, a modified version of UCB can prove optimal. On the other hand, it is shown that when the pay-off distributions are strongly dependent, simple switching strategies may be devised which leverage the strong inter-dependencies. To this end, an example is provided using Gaussian Processes. The techniques developed in this paper apply, more generally, to the problem of online sampling under dependence. version:1
arxiv-1702-06941 | An Algebraic Formalization of Forward and Forward-backward Algorithms | http://arxiv.org/abs/1702.06941 | id:1702.06941 author:Ai Azuma, Masashi Shimbo, Yuji Matsumoto category:cs.LG  published:2017-02-22 summary:In this paper, we propose an algebraic formalization of the two important classes of dynamic programming algorithms called forward and forward-backward algorithms. They are generalized extensively in this study so that a wide range of other existing algorithms is subsumed. Forward algorithms generalized in this study subsume the ordinary forward algorithm on trellises for sequence labeling, the inside algorithm on derivation forests for CYK parsing, a unidirectional message passing on acyclic factor graphs, the forward mode of automatic differentiation on computation graphs with addition and multiplication, and so on. In addition, we reveal algebraic structures underlying complicated computation with forward algorithms. By the aid of the revealed algebraic structures, we also propose a systematic framework to design complicated variants of forward algorithms. Forward-backward algorithms generalized in this study subsume the ordinary forward-backward algorithm on trellises for sequence labeling, the inside-outside algorithm on derivation forests for CYK parsing, the sum-product algorithm on acyclic factor graphs, the reverse mode of automatic differentiation (a.k.a. back propagation) on computation graphs with addition and multiplication, and so on. We also propose an algebraic characterization of what can be computed by forward-backward algorithms and elucidate the relationship between forward and forward-backward algorithms. version:1
arxiv-1702-06925 | Transferring Face Verification Nets To Pain and Expression Regression | http://arxiv.org/abs/1702.06925 | id:1702.06925 author:Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter, Gregory D. Hager, Harry Quon, Jian Cheng, Alan L. Yuille category:cs.CV cs.AI cs.LG cs.MM  published:2017-02-22 summary:Limited annotated data is available for the research of estimating facial expression intensities, which makes the training of deep networks for automated expression assessment very challenging. Fortunately, fine-tuning from a data-extensive pre-trained domain such as face verification can alleviate the problem. In this paper, we propose a transferred network that fine-tunes a state-of-the-art face verification network using expression-intensity labeled data with a regression layer. In this way, the expression regression task can benefit from the rich feature representations trained on a huge amount of data for face verification. The proposed transferred deep regressor is applied in estimating the intensity of facial action units (2017 EmotionNet Challenge) and in particular pain intensity estimation (UNBS-McMaster Shoulder-Pain dataset). It wins the second place in the challenge and achieves the state-of-the-art performance on Shoulder-Pain dataset. Particularly for Shoulder-Pain with the imbalance issue of different pain levels, a new weighted evaluation metric is proposed. version:1
arxiv-1702-06921 | Distributed Representation of Subgraphs | http://arxiv.org/abs/1702.06921 | id:1702.06921 author:Bijaya Adhikari, Yao Zhang, Naren Ramakrishnan, B. Aditya Prakash category:cs.SI cs.LG stat.ML  published:2017-02-22 summary:Network embeddings have become very popular in learning effective feature representations of networks. Motivated by the recent successes of embeddings in natural language processing, researchers have tried to find network embeddings in order to exploit machine learning algorithms for mining tasks like node classification and edge prediction. However, most of the work focuses on finding distributed representations of nodes, which are inherently ill-suited to tasks such as community detection which are intuitively dependent on subgraphs. Here, we propose sub2vec, an unsupervised scalable algorithm to learn feature representations of arbitrary subgraphs. We provide means to characterize similarties between subgraphs and provide theoretical analysis of sub2vec and demonstrate that it preserves the so-called local proximity. We also highlight the usability of sub2vec by leveraging it for network mining tasks, like community detection. We show that sub2vec gets significant gains over state-of-the-art methods and node-embedding methods. In particular, sub2vec offers an approach to generate a richer vocabulary of features of subgraphs to support representation and reasoning. version:1
arxiv-1702-06917 | Bandit Optimization with Upper-Confidence Frank-Wolfe | http://arxiv.org/abs/1702.06917 | id:1702.06917 author:Quentin Berthet, Vianney Perchet category:cs.LG math.OC stat.ML  published:2017-02-22 summary:We consider the problem of bandit optimization, inspired by stochastic optimization and online learning problems with bandit feedback. In this problem, the objective is to minimize a global loss function of all the actions, not necessarily a cumulative loss. This framework allows us to study a very general class of problems, with applications in statistics, machine learning, and other fields. To solve this problem, we introduce the Upper-Confidence Frank-Wolfe algorithm, inspired by techniques for bandits and convex optimization. We show upper bounds on the optimization error of this algorithm over various classes of functions, and discuss the optimality of these results. version:1
arxiv-1702-06899 | liquidSVM: A Fast and Versatile SVM package | http://arxiv.org/abs/1702.06899 | id:1702.06899 author:Ingo Steinwart, Philipp Thomann category:stat.ML cs.LG  published:2017-02-22 summary:liquidSVM is a package written in C++ that provides SVM-type solvers for various classification and regression tasks. Because of a fully integrated hyper-parameter selection, very carefully implemented solvers, multi-threading and GPU support, and several built-in data decomposition strategies it provides unprecedented speed for small training sizes as well as for data sets of tens of millions of samples. Besides the C++ API and a command line interface, bindings to R, MATLAB, Java, Python, and Spark are available. We present a brief description of the package and report experimental comparisons to other SVM packages. version:1
arxiv-1702-06891 | EVE: Explainable Vector Based Embedding Technique Using Wikipedia | http://arxiv.org/abs/1702.06891 | id:1702.06891 author:M. Atif Qureshi, Derek Greene category:cs.CL  published:2017-02-22 summary:We present an unsupervised explainable word embedding technique, called EVE, which is built upon the structure of Wikipedia. The proposed model defines the dimensions of a semantic vector representing a word using human-readable labels, thereby it readily interpretable. Specifically, each vector is constructed using the Wikipedia category graph structure together with the Wikipedia article link structure. To test the effectiveness of the proposed word embedding model, we consider its usefulness in three fundamental tasks: 1) intruder detection - to evaluate its ability to identify a non-coherent vector from a list of coherent vectors, 2) ability to cluster - to evaluate its tendency to group related vectors together while keeping unrelated vectors in separate clusters, and 3) sorting relevant items first - to evaluate its ability to rank vectors (items) relevant to the query in the top order of the result. For each task, we also propose a strategy to generate a task-specific human-interpretable explanation from the model. These demonstrate the overall effectiveness of the explainable embeddings generated by EVE. Finally, we compare EVE with the Word2Vec, FastText, and GloVe embedding techniques across the three tasks, and report improvements over the state-of-the-art. version:1
arxiv-1702-06890 | Learning Deep Features via Congenerous Cosine Loss for Person Recognition | http://arxiv.org/abs/1702.06890 | id:1702.06890 author:Yu Liu, Hongyang Li, Xiaogang Wang category:cs.CV cs.LG stat.ML  published:2017-02-22 summary:Person recognition aims at recognizing the same identity across time and space with complicated scenes and similar appearance. In this paper, we propose a novel method to address this task by training a network to obtain robust and representative features. A key observation is that traditional cross entropy loss only enforces the inter-class variation among samples and ignores to narrow down the similarity within each category. We propose a congenerous cosine loss to enlarge the inter-class distinction as well as alleviate the inner-class variance. Such a design is achieved by minimizing the cosine distance between sample and its cluster centroid in a cooperative way. Our method differs from previous work in person recognition that we do not conduct a second training on the test subset and thus maintain a good generalization ability. The identity of a person is determined by measuring the similarity from several body regions in the reference set. Experimental results show that the proposed approach achieves better classification accuracy against previous state-of-the-arts. version:1
arxiv-1702-06879 | Knowledge Graph Completion via Complex Tensor Factorization | http://arxiv.org/abs/1702.06879 | id:1702.06879 author:Théo Trouillon, Christopher R. Dance, Johannes Welbl, Sebastian Riedel, Éric Gaussier, Guillaume Bouchard category:cs.AI cs.LG math.SP stat.ML  published:2017-02-22 summary:In statistical relational learning, knowledge graph completion deals with automatically understanding the structure of large knowledge graphs---labeled directed graphs---and predicting missing relationships---labeled edges. State-of-the-art embedding models propose different trade-offs between modeling expressiveness, and time and space complexity. We reconcile both expressiveness and complexity through the use of complex-valued embeddings and explore the link between such complex-valued embeddings and unitary diagonalization. We corroborate our approach theoretically and show that all real square matrices---thus all possible relation/adjacency matrices---are the real part of some unitarily diagonalizable matrix. This results opens the door to a lot of other applications of square matrices factorization. Our approach based on complex embeddings is arguably simple, as it only involves a Hermitian dot product, the complex counterpart of the standard dot product between real vectors, whereas other methods resort to more and more complicated composition functions to increase their expressiveness. The proposed complex embeddings are scalable to large data sets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks. version:1
arxiv-1702-06875 | Triaging Content Severity in Online Mental Health Forums | http://arxiv.org/abs/1702.06875 | id:1702.06875 author:Arman Cohan, Sydney Young, Andrew Yates, Nazli Goharian category:cs.CL cs.IR cs.SI  published:2017-02-22 summary:Mental health forums are online communities where people express their issues and seek help from moderators and other users. In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm. Moderators need to respond to these severe posts in a timely manner to prevent potential self-harm. However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts. We present a framework for triaging user content into four severity categories which are defined based on indications of self-harm ideation. Our models are based on a feature-rich classification framework which includes lexical, psycholinguistic, contextual and topic modeling features. Our approaches improve the state of the art in triaging the content severity in mental health forums by large margins (up to 17% improvement over the F-1 scores). Using the proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate a decreased severity of risk over time. Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in need. version:1
arxiv-1702-06861 | On the Power of Truncated SVD for General High-rank Matrix Estimation Problems | http://arxiv.org/abs/1702.06861 | id:1702.06861 author:Simon S. Du, Yining Wang, Aarti Singh category:stat.ML cs.LG math.NA  published:2017-02-22 summary:We show that given an estimate $\widehat{A}$ that is close to a general high-rank positive semi-definite (PSD) matrix $A$ in spectral norm (i.e., $\ \widehat{A}-A\ _2 \leq \delta$), the simple truncated SVD of $\widehat{A}$ produces a multiplicative approximation of $A$ in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems, which we briefly summarize below ($A$ is an $n\times n$ high-rank PSD matrix and $A_k$ is the best rank-$k$ approximation of $A$): (1) High-rank matrix completion: By observing $\Omega(\frac{n\max\{\epsilon^{-4},k^2\}\mu_0^2\ A\ _F^2\log n}{\sigma_{k+1}(A)^2})$ elements of $A$ where $\sigma_{k+1}\left(A\right)$ is the $\left(k+1\right)$-th singular value of $A$ and $\mu_0$ is the incoherence, the truncated SVD on a zero-filled matrix satisfies $\ \widehat{A}_k-A\ _F \leq (1+O(\epsilon))\ A-A_k\ _F$ with high probability. (2)High-rank matrix de-noising: Let $\widehat{A}=A+E$ where $E$ is a Gaussian random noise matrix with zero mean and $\nu^2/n$ variance on each entry. Then the truncated SVD of $\widehat{A}$ satisfies $\ \widehat{A}_k-A\ _F \leq (1+O(\sqrt{\nu/\sigma_{k+1}(A)}))\ A-A_k\ _F + O(\sqrt{k}\nu)$. (3) Low-rank Estimation of high-dimensional covariance: Given $N$ i.i.d.~samples $X_1,\cdots,X_N\sim\mathcal N_n(0,A)$, can we estimate $A$ with a relative-error Frobenius norm bound? We show that if $N = \Omega\left(n\max\{\epsilon^{-4},k^2\}\gamma_k(A)^2\log N\right)$ for $\gamma_k(A)=\sigma_1(A)/\sigma_{k+1}(A)$, then $\ \widehat{A}_k-A\ _F \leq (1+O(\epsilon))\ A-A_k\ _F$ with high probability, where $\widehat{A}=\frac{1}{N}\sum_{i=1}^N{X_iX_i^\top}$ is the sample covariance. version:1
arxiv-1702-06838 | Sketchy Decisions: Convex Low-Rank Matrix Optimization with Optimal Storage | http://arxiv.org/abs/1702.06838 | id:1702.06838 author:Alp Yurtsever, Madeleine Udell, Joel A. Tropp, Volkan Cevher category:math.OC stat.ML  published:2017-02-22 summary:This paper concerns a fundamental class of convex matrix optimization problems. It presents the first algorithm that uses optimal storage and provably computes a low-rank approximation of a solution. In particular, when all solutions have low rank, the algorithm converges to a solution. This algorithm, SketchyCGM, modifies a standard convex optimization scheme, the conditional gradient method, to store only a small randomized sketch of the matrix variable. After the optimization terminates, the algorithm extracts a low-rank approximation of the solution from the sketch. In contrast to nonconvex heuristics, the guarantees for SketchyCGM do not rely on statistical models for the problem data. Numerical work demonstrates the benefits of SketchyCGM over heuristics. version:1
arxiv-1702-06832 | Adversarial examples for generative models | http://arxiv.org/abs/1702.06832 | id:1702.06832 author:Jernej Kos, Ian Fischer, Dawn Song category:stat.ML cs.LG  published:2017-02-22 summary:We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network. version:1
arxiv-1702-06819 | SIGNet: Scalable Embeddings for Signed Networks | http://arxiv.org/abs/1702.06819 | id:1702.06819 author:Mohammad Raihanul Islam, B. Aditya Prakash, Naren Ramakrishnan category:stat.ML cs.LG cs.SI  published:2017-02-22 summary:Recent successes in word embedding and document embedding have motivated researchers to explore similar representations for networks and to use such representations for tasks such as edge prediction, node label prediction, and community detection. Existing methods are largely focused on finding distributed representations for unsigned networks and are unable to discover embeddings that respect polarities inherent in edges. We propose SIGNet, a fast scalable embedding method suitable for signed networks. Our proposed objective function aims to carefully model the social structure implicit in signed networks by reinforcing the principles of social balance theory. Our method builds upon the traditional word2vec family of embedding approaches but we propose a new targeted node sampling strategy to maintain structural balance in higher-order neighborhoods. We demonstrate the superiority of SIGNet over state-of-the-art methods proposed for both signed and unsigned networks on several real world datasets from different domains. In particular, SIGNet offers an approach to generate a richer vocabulary of features of signed networks to support representation and reasoning. version:1
arxiv-1702-06818 | Stochastic Approximation for Canonical Correlation Analysis | http://arxiv.org/abs/1702.06818 | id:1702.06818 author:Raman Arora, Teodor V. Marinov, Poorya Mianjy category:cs.LG stat.ML  published:2017-02-22 summary:We study canonical correlation analysis (CCA) as a stochastic optimization problem. We show that regularized CCA is efficiently PAC-learnable. We give stochastic approximation (SA) algorithms that are instances of stochastic mirror descent, which achieve $\epsilon$-suboptimality in the population objective in time $\operatorname{poly}(\frac{1}{\epsilon},\frac{1}{\delta},d)$ with probability $1-\delta$, where $d$ is the input dimensionality. version:1
arxiv-1702-06799 | Boosted Multiple Kernel Learning for First-Person Activity Recognition | http://arxiv.org/abs/1702.06799 | id:1702.06799 author:Fatih Ozkan, Mehmet Ali Arabaci, Elif Surer, Alptekin Temizel category:cs.CV  published:2017-02-22 summary:Activity recognition from first-person (ego-centric) videos has recently gained attention due to the increasing ubiquity of the wearable cameras. There has been a surge of efforts adapting existing feature descriptors and designing new descriptors for the first-person videos. An effective activity recognition system requires selection and use of complementary features and appropriate kernels for each feature. In this study, we propose a data-driven framework for first-person activity recognition which effectively selects and combines features and their respective kernels during the training. Our experimental results show that use of Multiple Kernel Learning (MKL) and Boosted MKL in first-person activity recognition problem exhibits improved results in comparison to the state-of-the-art. In addition, these techniques enable the expansion of the framework with new features in an efficient and convenient way. version:1
arxiv-1702-06794 | Tackling Error Propagation through Reinforcement Learning: A Case of Greedy Dependency Parsing | http://arxiv.org/abs/1702.06794 | id:1702.06794 author:Minh Le, Antske Fokkens category:cs.CL  published:2017-02-22 summary:Error propagation is a common problem in NLP. Reinforcement learning explores erroneous states during training and can therefore be more robust when mistakes are made early in a process. In this paper, we apply reinforcement learning to greedy dependency parsing which is known to suffer from error propagation. Reinforcement learning improves accuracy of both labeled and unlabeled dependencies of the Stanford Neural Dependency Parser, a high performance greedy parser, while maintaining its efficiency. We investigate the portion of errors which are the result of error propagation and confirm that reinforcement learning reduces the occurrence of error propagation. version:1
arxiv-1702-06777 | Dialectometric analysis of language variation in Twitter | http://arxiv.org/abs/1702.06777 | id:1702.06777 author:Gonzalo Donoso, David Sanchez category:cs.CL cs.IR cs.SI physics.soc-ph  published:2017-02-22 summary:In the last few years, microblogging platforms such as Twitter have given rise to a deluge of textual data that can be used for the analysis of informal communication between millions of individuals. In this work, we propose an information-theoretic approach to geographic language variation using a corpus based on Twitter. We test our models with tens of concepts and their associated keywords detected in Spanish tweets geolocated in Spain. We employ dialectometric measures (cosine similarity and Jensen-Shannon divergence) to quantify the linguistic distance on the lexical level between cells created in a uniform grid over the map. This can be done for a single concept or in the general case taking into account an average of the considered variants. The latter permits an analysis of the dialects that naturally emerge from the data. Interestingly, our results reveal the existence of two dialect macrovarieties. The first group includes a region-specific speech spoken in small towns and rural areas whereas the second cluster encompasses cities that tend to use a more uniform variety. Since the results obtained with the two different metrics qualitatively agree, our work suggests that social media corpora can be efficiently used for dialectometric analyses. version:1
arxiv-1702-06776 | Causal Inference by Stochastic Complexity | http://arxiv.org/abs/1702.06776 | id:1702.06776 author:Kailash Budhathoki, Jilles Vreeken category:cs.LG cs.AI  published:2017-02-22 summary:The algorithmic Markov condition states that the most likely causal direction between two random variables X and Y can be identified as that direction with the lowest Kolmogorov complexity. Due to the halting problem, however, this notion is not computable. We hence propose to do causal inference by stochastic complexity. That is, we propose to approximate Kolmogorov complexity via the Minimum Description Length (MDL) principle, using a score that is mini-max optimal with regard to the model class under consideration. This means that even in an adversarial setting, such as when the true distribution is not in this class, we still obtain the optimal encoding for the data relative to the class. We instantiate this framework, which we call CISC, for pairs of univariate discrete variables, using the class of multinomial distributions. Experiments show that CISC is highly accurate on synthetic, benchmark, as well as real-world data, outperforming the state of the art by a margin, and scales extremely well with regard to sample and domain sizes. version:1
arxiv-1702-06767 | MomentsNet: a simple learning-free method for binary image recognition | http://arxiv.org/abs/1702.06767 | id:1702.06767 author:Jiasong Wu, Shijie Qiu, Youyong Kong, Yang Chen, Lotfi Senhadji, Huazhong Shu category:cs.CV  published:2017-02-22 summary:In this paper, we propose a new simple and learning-free deep learning network named MomentsNet, whose convolution layer, nonlinear processing layer and pooling layer are constructed by Moments kernels, binary hashing and block-wise histogram, respectively. Twelve typical moments (including geometrical moment, Zernike moment, Tchebichef moment, etc.) are used to construct the MomentsNet whose recognition performance for binary image is studied. The results reveal that MomentsNet has better recognition performance than its corresponding moments in almost all cases and ZernikeNet achieves the best recognition performance among MomentsNet constructed by twelve moments. ZernikeNet also shows better recognition performance on binary image database than that of PCANet, which is a learning-based deep learning network. version:1
arxiv-1702-06762 | Style Transfer Generative Adversarial Networks: Learning to Play Chess Differently | http://arxiv.org/abs/1702.06762 | id:1702.06762 author:Muthuraman Chidambaram, Yanjun Qi category:cs.LG  published:2017-02-22 summary:The idea of style transfer has largely only been explored in image-based tasks, which we attribute in part to the specific nature of loss functions used for style transfer. We propose a general formulation of style transfer as an extension of generative adversarial networks, by using a discriminator to regularize a generator with an otherwise separate loss function. We apply our approach to the task of learning to play chess in the style of a specific player, and present empirical evidence for the viability of our approach. version:1
arxiv-1702-06760 | Memory Matching Networks for Genomic Sequence Classification | http://arxiv.org/abs/1702.06760 | id:1702.06760 author:Jack Lanchantin, Ritambhara Singh, Yanjun Qi category:cs.LG q-bio.GN stat.ML  published:2017-02-22 summary:When analyzing the genome, researchers have discovered that proteins bind to DNA based on certain patterns of the DNA sequence known as "motifs". However, it is difficult to manually construct motifs due to their complexity. Recently, externally learned memory models have proven to be effective methods for reasoning over inputs and supporting sets. In this work, we present memory matching networks (MMN) for classifying DNA sequences as protein binding sites. Our model learns a memory bank of encoded motifs, which are dynamic memory modules, and then matches a new test sequence to each of the motifs to classify the sequence as a binding or nonbinding site. version:1
arxiv-1702-06733 | Improving a Strong Neural Parser with Conjunction-Specific Features | http://arxiv.org/abs/1702.06733 | id:1702.06733 author:Jessica Ficler, Yoav Goldberg category:cs.CL  published:2017-02-22 summary:While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction (i.e., correctly attaching the "conj" relation). We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended parser yields an improvement in "conj" attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank. version:1
arxiv-1702-06722 | 3D Reconstruction of Temples in the Special Region of Yogyakarta By Using Close-Range Photogrammetry | http://arxiv.org/abs/1702.06722 | id:1702.06722 author:Adityo Priyandito Utomo, Canggih Puspo Wibowo category:cs.CV  published:2017-02-22 summary:Object reconstruction is one of the main problems in cultural heritage preservation. This problem is due to lack of data in documentation. Thus in this research we presented a method of 3D reconstruction using close-range photogrammetry. We collected 1319 photos from five temples in Yogyakarta. Using A-KAZE algorithm, keypoints of each image were obtained. Then we employed LIOP to create feature descriptor from it. After performing feature matching, L1RA was utilized to create sparse point clouds. In order to generate the geometry shape, MVS was used. Finally, FSSR and Large Scale Texturing were employed to deal with the surface and texture of the object. The quality of the reconstructed 3D model was measured by comparing the 3D images of the model with the original photos utilizing SSIM. The results showed that in terms of quality, our method was on par with other commercial method such as PhotoModeler and PhotoScan. version:1
arxiv-1702-06712 | Ensembles of Randomized Time Series Shapelets Provide Improved Accuracy while Reducing Computational Costs | http://arxiv.org/abs/1702.06712 | id:1702.06712 author:Atif Raza, Stefan Kramer category:cs.LG  published:2017-02-22 summary:Shapelets are discriminative time series subsequences that allow generation of interpretable classification models, which provide faster and generally better classification than the nearest neighbor approach. However, the shapelet discovery process requires the evaluation of all possible subsequences of all time series in the training set, making it extremely computation intensive. Consequently, shapelet discovery for large time series datasets quickly becomes intractable. A number of improvements have been proposed to reduce the training time. These techniques use approximation or discretization and often lead to reduced classification accuracy compared to the exact method. We are proposing the use of ensembles of shapelet-based classifiers obtained using random sampling of the shapelet candidates. Using random sampling reduces the number of evaluated candidates and consequently the required computational cost, while the classification accuracy of the resulting models is also not significantly different than that of the exact algorithm. The combination of randomized classifiers rectifies the inaccuracies of individual models because of the diversity of the solutions. Based on the experiments performed, it is shown that the proposed approach of using an ensemble of inexpensive classifiers provides better classification accuracy compared to the exact method at a significantly lesser computational cost. version:1
arxiv-1702-06709 | Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings | http://arxiv.org/abs/1702.06709 | id:1702.06709 author:Abhishek, Ashish Anand, Amit Awekar category:cs.CL  published:2017-02-22 summary:Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous state-of-the-art methods on two publicly available datasets, namely FIGER (GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models. version:1
arxiv-1701-06547 | Adversarial Learning for Neural Dialogue Generation | http://arxiv.org/abs/1701.06547 | id:1701.06547 author:Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, Dan Jurafsky category:cs.CL  published:2017-01-23 summary:In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator---analagous to the human evaluator in the Turing test--- to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial {\em evaluation} that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines. version:4
arxiv-1702-06703 | Data Distillation for Controlling Specificity in Dialogue Generation | http://arxiv.org/abs/1702.06703 | id:1702.06703 author:Jiwei Li, Will Monroe, Dan Jurafsky category:cs.CL  published:2017-02-22 summary:People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between \emph{data distillation} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity. We then train a reinforcement learning system for selecting among this pool of generation models, to choose the best level of specificity for a given input. Compared to the original generative model trained without distillation, the proposed system is capable of generating more interesting and higher-quality responses, in addition to appropriately adjusting specificity depending on the context. Our research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time. version:1
arxiv-1702-06700 | Task-driven Visual Saliency and Attention-based Visual Question Answering | http://arxiv.org/abs/1702.06700 | id:1702.06700 author:Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang category:cs.CV cs.AI cs.CL cs.NE  published:2017-02-22 summary:Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results. version:1
arxiv-1702-06696 | One Representation per Word - Does it make Sense for Composition? | http://arxiv.org/abs/1702.06696 | id:1702.06696 author:Thomas Kober, Julie Weeds, John Wilkie, Jeremy Reffin, David Weir category:cs.CL  published:2017-02-22 summary:In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf single-vector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remarkably well. version:1
arxiv-1702-05870 | Cosine Normalization: Using Cosine Similarity Instead of Dot Product in Neural Networks | http://arxiv.org/abs/1702.05870 | id:1702.05870 author:Luo Chunjie, Zhan jianfeng, Wang lei, Yang Qiang category:cs.LG cs.AI stat.ML  published:2017-02-20 summary:Traditionally, multi-layer neural networks use dot product between the output vector of previous layer and the incoming weight vector as the input to activation function. The result of dot product is unbounded, thus increases the risk of large variance. Large variance of neuron makes the model sensitive to the change of input distribution, thus results in poor generalization, and aggravates the internal covariate shift which slows down the training. To bound dot product and decrease the variance, we propose to use cosine similarity instead of dot product in neural networks, which we call cosine normalization. Our experiments show that cosine normalization in fully-connected neural networks notably reduces the test err with lower divergence, compared to other normalization techniques. Applied to convolutional networks, cosine normalization also significantly enhances the accuracy of classification and accelerates the training. version:2
arxiv-1702-06677 | Discussion quality diffuses in the digital public square | http://arxiv.org/abs/1702.06677 | id:1702.06677 author:George Berry, Sean J. Taylor category:cs.CY cs.CL cs.SI  published:2017-02-22 summary:Studies of online social influence have demonstrated that friends have important effects on many types of behavior in a wide variety of settings. However, we know much less about how influence works among relative strangers in digital public squares, despite important conversations happening in such spaces. We present the results of a study on large public Facebook pages where we randomly used two different methods--most recent and social feedback--to order comments on posts. We find that the social feedback condition results in higher quality viewed comments and response comments. After measuring the average quality of comments written by users before the study, we find that social feedback has a positive effect on response quality for both low and high quality commenters. We draw on a theoretical framework of social norms to explain this empirical result. In order to examine the influence mechanism further, we measure the similarity between comments viewed and written during the study, finding that similarity increases for the highest quality contributors under the social feedback condition. This suggests that, in addition to norms, some individuals may respond with increased relevance to high-quality comments. version:1
arxiv-1702-06675 | Context-Aware Prediction of Derivational Word-forms | http://arxiv.org/abs/1702.06675 | id:1702.06675 author:Ekaterina Vylomova, Ryan Cotterell, Timothy Baldwin, Trevor Cohn category:cs.CL  published:2017-02-22 summary:Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder--decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting. version:1
arxiv-1702-06674 | Unsupervised Diverse Colorization via Generative Adversarial Networks | http://arxiv.org/abs/1702.06674 | id:1702.06674 author:Yun Cao, Zhiming Zhou, Weinan Zhang, Yong Yu category:cs.CV cs.AI  published:2017-02-22 summary:Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible. version:1
arxiv-1702-06672 | Calculating Probabilities Simplifies Word Learning | http://arxiv.org/abs/1702.06672 | id:1702.06672 author:Aida Nematzadeh, Barend Beekhuizen, Shanshan Huang, Suzanne Stevenson category:cs.CL  published:2017-02-22 summary:Children can use the statistical regularities of their environment to learn word meanings, a mechanism known as cross-situational learning. We take a computational approach to investigate how the information present during each observation in a cross-situational framework can affect the overall acquisition of word meanings. We do so by formulating various in-the-moment learning mechanisms that are sensitive to different statistics of the environment, such as counts and conditional probabilities. Each mechanism introduces a unique source of competition or mutual exclusivity bias to the model; the mechanism that maximally uses the model's knowledge of word meanings performs the best. Moreover, the gap between this mechanism and others is amplified in more challenging learning scenarios, such as learning from few examples. version:1
arxiv-1702-06663 | Guided Deep List: Automating the Generation of Epidemiological Line Lists from Open Sources | http://arxiv.org/abs/1702.06663 | id:1702.06663 author:Saurav Ghosh, Prithwish Chakraborty, Bryan L. Lewis, Maimuna S. Majumder, Emily Cohn, John S. Brownstein, Madhav V. Marathe, Naren Ramakrishnan category:cs.CL cs.IR  published:2017-02-22 summary:Real-time monitoring and responses to emerging public health threats rely on the availability of timely surveillance data. During the early stages of an epidemic, the ready availability of line lists with detailed tabular information about laboratory-confirmed cases can assist epidemiologists in making reliable inferences and forecasts. Such inferences are crucial to understand the epidemiology of a specific disease early enough to stop or control the outbreak. However, construction of such line lists requires considerable human supervision and therefore, difficult to generate in real-time. In this paper, we motivate Guided Deep List, the first tool for building automated line lists (in near real-time) from open source reports of emerging disease outbreaks. Specifically, we focus on deriving epidemiological characteristics of an emerging disease and the affected population from reports of illness. Guided Deep List uses distributed vector representations (ala word2vec) to discover a set of indicators for each line list feature. This discovery of indicators is followed by the use of dependency parsing based techniques for final extraction in tabular form. We evaluate the performance of Guided Deep List against a human annotated line list provided by HealthMap corresponding to MERS outbreaks in Saudi Arabia. We demonstrate that Guided Deep List extracts line list features with increased accuracy compared to a baseline method. We further show how these automatically extracted line list features can be used for making epidemiological inferences, such as inferring demographics and symptoms-to-hospitalization period of affected individuals. version:1
arxiv-1702-06661 | Social Learning and Diffusion of Pervasive Goods: An Empirical Study of an African App Store | http://arxiv.org/abs/1702.06661 | id:1702.06661 author:Meisam Hejazi Nia, Brian T. Ratchford, Norris Bruce category:stat.ML cs.SI stat.AP  published:2017-02-22 summary:In this study, the authors develop a structural model that combines a macro diffusion model with a micro choice model to control for the effect of social influence on the mobile app choices of customers over app stores. Social influence refers to the density of adopters within the proximity of other customers. Using a large data set from an African app store and Bayesian estimation methods, the authors quantify the effect of social influence and investigate the impact of ignoring this process in estimating customer choices. The findings show that customer choices in the app store are explained better by offline than online density of adopters and that ignoring social influence in estimations results in biased estimates. Furthermore, the findings show that the mobile app adoption process is similar to adoption of music CDs, among all other classic economy goods. A counterfactual analysis shows that the app store can increase its revenue by 13.6% through a viral marketing policy (e.g., a sharing with friends and family button). version:1
arxiv-1702-05743 | DR$^{2}$-Net: Deep Residual Reconstruction Network for Image Compressive Sensing | http://arxiv.org/abs/1702.05743 | id:1702.05743 author:Hantao Yao, Feng Dai, Dongming Zhang, Yike Ma, Shiliang Zhang, Yongdong Zhang category:cs.CV  published:2017-02-19 summary:Most traditional algorithms for compressive sensing image reconstruction suffer from the intensive computation. Recently, deep learning-based reconstruction algorithms have been reported, which dramatically reduce the time complexity than iterative reconstruction algorithms. In this paper, we propose a novel Deep Residual Reconstruction Network (DR$^{2}$-Net) to reconstruct the image from its Compressively Sensed (CS) measurement. The DR$^{2}$-Net is proposed based on two observations: 1) linear mapping could reconstruct a high-quality preliminary image, and 2) residual learning could further improve the reconstruction quality. Accordingly, DR$^{2}$-Net consists of two components, i.e., linear mapping network and residual network, respectively. Specifically, the fully-connected layer in neural network implements the linear mapping network. We then expand the linear mapping network to DR$^{2}$-Net by adding several residual learning blocks to enhance the preliminary image. Extensive experiments demonstrate that the DR$^{2}$-Net outperforms traditional iterative methods and recent deep learning-based methods by large margins at measurement rates 0.01, 0.04, 0.1, and 0.25, respectively. version:2
arxiv-1702-06264 | Weighted Motion Averaging for the Registration of Multi-View Range Scans | http://arxiv.org/abs/1702.06264 | id:1702.06264 author:Rui Guo, Jihua Zhu, Yaochen Li, Dapeng Chen, Zhongyu Li, Yongqin Zhang category:cs.CV  published:2017-02-21 summary:Multi-view registration is a fundamental but challenging problem in 3D reconstruction and robot vision. Although the original motion averaging algorithm has been introduced as an effective means to solve the multi-view registration problem, it does not consider the reliability and accuracy of each relative motion. Accordingly, this paper proposes a novel motion averaging algorithm for multi-view registration. Firstly, it utilizes the pair-wise registration algorithm to estimate the relative motion and overlapping percentage of each scan pair with a certain degree of overlap. With the overlapping percentage available, it views the overlapping percentage as the corresponding weight of each scan pair and proposes the weight motion averaging algorithm, which can pay more attention to reliable and accurate relative motions. By treating each relative motion distinctively, more accurate registration can be achieved by applying the weighted motion averaging to multi-view range scans. Experimental results demonstrate the superiority of our proposed approach compared with the state-of-the-art methods in terms of accuracy, robustness and efficiency. version:2
arxiv-1702-06619 | Lensless Photography with only an image sensor | http://arxiv.org/abs/1702.06619 | id:1702.06619 author:Ganghun Kim, Kyle Isaacson, Racheal Palmer, Rajesh Menon category:cs.CV physics.optics  published:2017-02-21 summary:Photography usually requires optics in conjunction with a recording device (an image sensor). Eliminating the optics could lead to new form factors for cameras. Here, we report a simple demonstration of imaging using a bare CMOS sensor that utilizes computation. The technique relies on the space variant point-spread functions resulting from the interaction of a point source in the field of view with the image sensor. These space-variant point-spread functions are combined with a reconstruction algorithm in order to image simple objects displayed on a discrete LED array as well as on an LCD screen. We extended the approach to video imaging at the native frame rate of the sensor. Finally, we performed experiments to analyze the parametric impact of the object distance. Improving the sensor designs and reconstruction algorithms can lead to useful cameras without optics. version:1
arxiv-1702-06602 | Exemplar-Centered Supervised Shallow Parametric Data Embedding | http://arxiv.org/abs/1702.06602 | id:1702.06602 author:Martin Renqiang Min, Hongyu Guo, Dongjin Song category:cs.LG stat.ML  published:2017-02-21 summary:Metric learning methods for dimensionality reduction in combination with k-Nearest Neighbors (kNN) have been extensively deployed in many classification, data embedding, and information retrieval applications. However, most of these approaches involve pairwise training data comparisons, and thus have quadratic computational complexity with respect to the size of training set, preventing them from scaling to fairly big datasets. Moreover, during testing, comparing test data against all the training data points is also expensive in terms of both computational cost and resources required. Furthermore, previous metrics are either too constrained or too expressive to be well learned. To effectively solve these issues, we present an exemplar-centered supervised shallow parametric data embedding model, using a Maximally Collapsing Metric Learning (MCML) objective. Our strategy learns a shallow high-order parametric embedding function and compares training/test data only with learned or precomputed exemplars, resulting in a cost function with linear computational complexity for both training and testing. We also empirically demonstrate, using several benchmark datasets, that for classification in two-dimensional embedding space, our approach not only gains speedup of kNN by hundreds of times, but also outperforms state-of-the-art supervised embedding approaches. version:1
arxiv-1702-06594 | On the Complexity of CCG Parsing | http://arxiv.org/abs/1702.06594 | id:1702.06594 author:Marco Kuhlmann, Giorgio Satta, Peter Jonsson category:cs.CL  published:2017-02-21 summary:We study the parsing complexity of Combinatory Categorial Grammar (CCG) in the formalism of Vijay-Shanker and Weir (1994). As our main result, we prove that any parsing algorithm for this formalism will necessarily take exponential time when the size of the grammar, and not only the length of the input sentence, is included in the analysis. This result sets the formalism of Vijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as Tree-Adjoining Grammar (TAG), for which parsing can be performed in time polynomial in the combined size of grammar and input sentence. Our proof highlights important differences between the formalism of Vijay-Shanker and Weir (1994) and contemporary incarnations of CCG. version:1
arxiv-1702-06589 | Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables | http://arxiv.org/abs/1702.06589 | id:1702.06589 author:Till Haug, Octavian-Eugen Ganea, Paulina Grnarova category:cs.CL  published:2017-02-21 summary:Advances in natural language processing tasks have gained momentum in recent years due to the increasingly popular neural network methods. In this paper, we explore deep learning techniques for answering multi-step reasoning questions that operate on semi-structured tables. Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness. Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision. It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015]. Second, paraphrases of logical forms and questions are embedded in a jointly learned vector space using word and character convolutional neural networks. A neural scoring function is further used to rank and retrieve the most probable logical form (interpretation) of a question. Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016]. version:1
arxiv-1702-05471 | Maximally Correlated Principal Component Analysis | http://arxiv.org/abs/1702.05471 | id:1702.05471 author:Soheil Feizi, David Tse category:stat.ML cs.IT cs.LG math.IT  published:2017-02-17 summary:In the era of big data, reducing data dimensionality is critical in many areas of science. Widely used Principal Component Analysis (PCA) addresses this problem by computing a low dimensional data embedding that maximally explain variance of the data. However, PCA has two major weaknesses. Firstly, it only considers linear correlations among variables (features), and secondly it is not suitable for categorical data. We resolve these issues by proposing Maximally Correlated Principal Component Analysis (MCPCA). MCPCA computes transformations of variables whose covariance matrix has the largest Ky Fan norm. Variable transformations are unknown, can be nonlinear and are computed in an optimization. MCPCA can also be viewed as a multivariate extension of Maximal Correlation. For jointly Gaussian variables we show that the covariance matrix corresponding to the identity (or the negative of the identity) transformations majorizes covariance matrices of non-identity functions. Using this result we characterize global MCPCA optimizers for nonlinear functions of jointly Gaussian variables for every rank constraint. For categorical variables we characterize global MCPCA optimizers for the rank one constraint based on the leading eigenvector of a matrix computed using pairwise joint distributions. For a general rank constraint we propose a block coordinate descend algorithm and show its convergence to stationary points of the MCPCA optimization. We compare MCPCA with PCA and other state-of-the-art dimensionality reduction methods including Isomap, LLE, multilayer autoencoders (neural networks), kernel PCA, probabilistic PCA and diffusion maps on several synthetic and real datasets. We show that MCPCA consistently provides improved performance compared to other methods. version:2
arxiv-1702-06559 | Active One-shot Learning | http://arxiv.org/abs/1702.06559 | id:1702.06559 author:Mark Woodward, Chelsea Finn category:cs.LG  published:2017-02-21 summary:Recent advances in one-shot learning have produced models that can learn from a handful of labeled examples, for passive classification and regression tasks. This paper combines reinforcement learning with one-shot learning, allowing the model to decide, during classification, which examples are worth labeling. We introduce a classification task in which a stream of images are presented and, on each time step, a decision must be made to either predict a label or pay to receive the correct label. We present a recurrent neural network based action-value function, and demonstrate its ability to learn how and when to request labels. Through the choice of reward function, the model can achieve a higher prediction accuracy than a similar model on a purely supervised task, or trade prediction accuracy for fewer label requests. version:1
arxiv-1702-06525 | A Nonconvex Free Lunch for Low-Rank plus Sparse Matrix Recovery | http://arxiv.org/abs/1702.06525 | id:1702.06525 author:Xiao Zhang, Lingxiao Wang, Quanquan Gu category:stat.ML  published:2017-02-21 summary:We study the problem of low-rank plus sparse matrix recovery. We propose a generic and efficient nonconvex optimization algorithm based on projected gradient descent and double thresholding operator, with much lower computational complexity. Compared with existing convex-relaxation based methods, the proposed algorithm recovers the low-rank plus sparse matrices for free, without incurring any additional statistical cost. It not only enables exact recovery of the unknown low-rank and sparse matrices in the noiseless setting, and achieves minimax optimal statistical error rate in the noisy case, but also matches the best-known robustness guarantee (i.e., tolerance for sparse corruption). At the core of our theory is a novel structural Lipschitz gradient condition for low-rank plus sparse matrices, which is essential for proving the linear convergence rate of our algorithm, and we believe is of independent interest to prove fast rates for general superposition-structured models. We demonstrate the superiority of our generic algorithm, both theoretically and experimentally, through three concrete applications: robust matrix sensing, robust PCA and one-bit matrix decomposition. version:1
arxiv-1702-06521 | VidLoc: 6-DoF Video-Clip Relocalization | http://arxiv.org/abs/1702.06521 | id:1702.06521 author:Ronald Clark, Sen Wang, Andrew Markham, Niki Trigoni, Hongkai Wen category:cs.CV  published:2017-02-21 summary:Machine learning techniques, namely convolutional neural networks (CNN) and regression forests, have recently shown great promise in performing 6-DoF localization of monocular images. However, in most cases image-sequences, rather only single images, are readily available. To this extent, none of the proposed learning-based approaches exploit the valuable constraint of temporal smoothness, often leading to situations where the per-frame error is larger than the camera motion. In this paper we propose a recurrent model for performing 6-DoF localization of video-clips. We find that, even by considering only short sequences (20 frames), the pose estimates are smoothed and the localization error can be drastically reduced. Finally, we consider means of obtaining probabilistic pose estimates from our model. We evaluate our method on openly-available real-world autonomous driving and indoor localization datasets. version:1
arxiv-1702-06516 | A data-driven basis for direct estimation of functionals of distributions | http://arxiv.org/abs/1702.06516 | id:1702.06516 author:Alan Wisler, Visar Berisha, Andreas Spanias, Alfred O. Hero category:cs.IT math.IT stat.ML  published:2017-02-21 summary:A number of fundamental quantities in statistical signal processing and information theory can be expressed as integral functions of two probability density functions. Such quantities are called density functionals as they map density functions onto the real line. For example, information divergence functions measure the dissimilarity between two probability density functions and are particularly useful in a number of applications. Typically, estimating these quantities requires complete knowledge of the underlying distribution followed by multi-dimensional integration. Existing methods make parametric assumptions about the data distribution or use non-parametric density estimation followed by high-dimensional integration. In this paper, we propose a new alternative. We introduce the concept of "data-driven" basis functions - functions of distributions whose value we can estimate given only samples from the underlying distributions without requiring distribution fitting or direct integration. We derive a new data-driven complete basis that is similar to the deterministic Bernstein polynomial basis and develop two methods for performing basis expansions of functionals of two distributions. We also show that the new basis set allows us to approximate functions of distributions as closely as desired. Finally, we evaluate the methodology by developing data driven estimators for the Kullback-Leibler divergences and the Hellinger distance and by constructing tight data-driven bounds on the Bayes Error Rate. version:1
arxiv-1702-06510 | Algorithmes de classification et d'optimisation: participation du LIA/ADOC á DEFT'14 | http://arxiv.org/abs/1702.06510 | id:1702.06510 author:Luis Adrián Cabrera-Diego, Stéphane Huet, Bassam Jabaian, Alejandro Molina, Juan-Manuel Torres-Moreno, Marc El-Bèze, Barthélémy Durette category:cs.IR cs.CL  published:2017-02-21 summary:This year, the DEFT campaign (D\'efi Fouilles de Textes) incorporates a task which aims at identifying the session in which articles of previous TALN conferences were presented. We describe the three statistical systems developed at LIA/ADOC for this task. A fusion of these systems enables us to obtain interesting results (micro-precision score of 0.76 measured on the test corpus) version:1
arxiv-1702-06506 | PixelNet: Representation of the pixels, by the pixels, and for the pixels | http://arxiv.org/abs/1702.06506 | id:1702.06506 author:Aayush Bansal, Xinlei Chen, Bryan Russell, Abhinav Gupta, Deva Ramanan category:cs.CV cs.LG cs.RO  published:2017-02-21 summary:We explore design principles for general pixel-level prediction problems, from low-level edge detection to mid-level surface normal estimation to high-level semantic segmentation. Convolutional predictors, such as the fully-convolutional network (FCN), have achieved remarkable success by exploiting the spatial redundancy of neighboring pixels through convolutional processing. Though computationally efficient, we point out that such approaches are not statistically efficient during learning precisely because spatial redundancy limits the information learned from neighboring pixels. We demonstrate that stratified sampling of pixels allows one to (1) add diversity during batch updates, speeding up learning; (2) explore complex nonlinear predictors, improving accuracy; and (3) efficiently train state-of-the-art models tabula rasa (i.e., "from scratch") for diverse pixel-labeling tasks. Our single architecture produces state-of-the-art results for semantic segmentation on PASCAL-Context dataset, surface normal estimation on NYUDv2 depth dataset, and edge detection on BSDS. version:1
arxiv-1702-06478 | Systèmes du LIA à DEFT'13 | http://arxiv.org/abs/1702.06478 | id:1702.06478 author:Xavier Bost, Ilaria Brunetti, Luis Adrián Cabrera-Diego, Jean-Valère Cossu, Andréa Linhares, Mohamed Morchid, Juan-Manuel Torres-Moreno, Marc El-Bèze, Richard Dufour category:cs.CL cs.IR  published:2017-02-21 summary:The 2013 D\'efi de Fouille de Textes (DEFT) campaign is interested in two types of language analysis tasks, the document classification and the information extraction in the specialized domain of cuisine recipes. We present the systems that the LIA has used in DEFT 2013. Our systems show interesting results, even though the complexity of the proposed tasks. version:1
arxiv-1702-06467 | Efficient Social Network Multilingual Classification using Character, POS n-grams and Dynamic Normalization | http://arxiv.org/abs/1702.06467 | id:1702.06467 author:Carlos-Emiliano González-Gallardo, Juan-Manuel Torres-Moreno, Azucena Montes Rendón, Gerardo Sierra category:cs.IR cs.CL cs.SI  published:2017-02-21 summary:In this paper we describe a dynamic normalization process applied to social network multilingual documents (Facebook and Twitter) to improve the performance of the Author profiling task for short texts. After the normalization process, $n$-grams of characters and n-grams of POS tags are obtained to extract all the possible stylistic information encoded in the documents (emoticons, character flooding, capital letters, references to other users, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of performance. version:1
arxiv-1702-06463 | Predicting non-linear dynamics: a stable local learning scheme for recurrent spiking neural networks | http://arxiv.org/abs/1702.06463 | id:1702.06463 author:Aditya Gilra, Wulfram Gerstner category:q-bio.NC cs.LG cs.NE cs.SY  published:2017-02-21 summary:Brains need to predict how our muscles and body react to motor commands. How networks of spiking neurons can learn to reproduce these non-linear dynamics, using local, online and stable learning rules, is an important, open question. Here, we present a supervised learning scheme for the feedforward and recurrent connections in a network of heterogeneous spiking neurons. The error in the output is fed back through fixed random connections with a negative gain, causing the network to follow the desired dynamics, while an online and local rule changes the weights; hence we call the scheme FOLLOW (Feedback-based Online Local Learning Of Weights) The rule is local in the sense that weight changes depend on the presynaptic activity and the error signal projected onto the post-synaptic neuron. We provide examples of learning linear, non-linear and chaotic dynamics, as well as the dynamics of a two-link arm. Using the Lyapunov method, and under reasonable assumptions and approximations, we show that FOLLOW learning is uniformly stable, with the error going to zero asymptotically. version:1
arxiv-1702-06461 | Crowd Sourcing Image Segmentation with iaSTAPLE | http://arxiv.org/abs/1702.06461 | id:1702.06461 author:Dmitrij Schlesinger, Florian Jug, Gene Myers, Carsten Rother, Dagmar Kainmüller category:cs.CV  published:2017-02-21 summary:We propose a novel label fusion technique as well as a crowdsourcing protocol to efficiently obtain accurate epithelial cell segmentations from non-expert crowd workers. Our label fusion technique simultaneously estimates the true segmentation, the performance levels of individual crowd workers, and an image segmentation model in the form of a pairwise Markov random field. We term our approach image-aware STAPLE (iaSTAPLE) since our image segmentation model seamlessly integrates into the well-known and widely used STAPLE approach. In an evaluation on a light microscopy dataset containing more than 5000 membrane labeled epithelial cells of a fly wing, we show that iaSTAPLE outperforms STAPLE in terms of segmentation accuracy as well as in terms of the accuracy of estimated crowd worker performance levels, and is able to correctly segment 99% of all cells when compared to expert segmentations. These results show that iaSTAPLE is a highly useful tool for crowd sourcing image segmentation. version:1
arxiv-1702-06456 | Online Representation Learning with Multi-layer Hebbian Networks for Image Classification Tasks | http://arxiv.org/abs/1702.06456 | id:1702.06456 author:Yanis Bahroun, Andrea Soltoggio category:cs.NE cs.CV I.5.1  published:2017-02-21 summary:Unsupervised learning allows algorithms to adapt to different data thanks to the autonomous discovery of discriminating features during the training. When these algorithms are reducible to cost-function minimisation, better interpretations of their learning dynamics are possible. Recently, new Hebbian-like plasticity, bio-inspired, local and unsupervised learning rules for neural networks, have been shown to minimise a cost-function while performing online sparse representation learning. However, it is unclear to what degree such rules are effective to learn features from images. To investigate this point, this study introduces a novel multi-layer Hebbian network trained by a rule derived from a non-negative classical multidimensional scaling cost-function. The performance is compared to that of other fully unsupervised learning algorithms. version:1
arxiv-1702-06451 | Traffic Surveillance Camera Calibration by 3D Model Bounding Box Alignment for Accurate Vehicle Speed Measurement | http://arxiv.org/abs/1702.06451 | id:1702.06451 author:Jakub Sochor, Roman Juránek, Adam Herout category:cs.CV  published:2017-02-21 summary:In this paper, we focus on fully automatic traffic surveillance camera calibration which we use for speed measurement of passing vehicles. We improve over a recent state-of-the-art camera calibration method for traffic surveillance based on two detected vanishing points. More importantly, we propose a novel automatic scene scale inference based on matching bounding boxes of rendered 3D models of vehicles with detected bounding boxes in the image. The proposed method can be used from an arbitrary viewpoint and it has no constraints on camera placement. We evaluate our method on recent comprehensive dataset for speed measurement BrnoCompSpeed. Experiments show that our automatic camera calibration by detected two vanishing points method reduces the error by 50% compared to the previous state-of-the-art method. We also show that our scene scale inference method is much more precise (mean speed measurement error 1.10km/h) outperforming both state of the art automatic calibration method (error reduction by 86% -- mean error 7.98km/h) and manual calibration (error reduction by 19% -- mean error 1.35km/h). We also present qualitative results of automatic camera calibration method on video sequences obtained from real surveillance cameras on various places and under different lighting conditions (night, dawn, day). version:1
arxiv-1702-06441 | BrnoCompSpeed: Review of Traffic Camera Calibration and Comprehensive Dataset for Monocular Speed Measurement | http://arxiv.org/abs/1702.06441 | id:1702.06441 author:Jakub Sochor, Roman Juránek, Jakub Špaňhel, Lukáš Maršík, Adam Široký, Adam Herout, Pavel Zemčík category:cs.CV  published:2017-02-21 summary:In this paper, we focus on traffic camera calibration and visual speed measurement from a single monocular camera, which is an important task of visual traffic surveillance. Existing methods addressing this problem are hard to compare due to lack of a common dataset with reliable ground truth. Therefore, it is not clear how the methods compare in various aspects and what are the factors affecting their performance. We captured a new dataset of 18 full-HD videos, each around one hour long, captured at 6 different locations. Vehicles in videos (20,865 instances in total) are annotated with precise speed measurements from optical gates using LIDAR and verified with several reference GPS tracks. We provide the videos and metadata (calibration, lengths of features in image, annotations, etc.) for future comparison and evaluation. Camera calibration is the most crucial part of the speed measurement; therefore, we provide a review of the methods and analyze a recently published method for fully automatic camera calibration and vehicle speed measurement and report the results on this dataset in detail. version:1
arxiv-1702-06435 | Phase Transitions of Spectral Initialization for High-Dimensional Nonconvex Estimation | http://arxiv.org/abs/1702.06435 | id:1702.06435 author:Yue M. Lu, Gen Li category:cs.IT math.IT stat.ML  published:2017-02-21 summary:We study a spectral initialization method that serves as a key ingredient in recent work on using efficient iterative algorithms for estimating signals in nonconvex settings. Unlike previous analysis in the literature, which is restricted to the phase retrieval setting and which provides only performance bounds, we consider arbitrary generalized linear sensing models and present a precise asymptotic characterization of the performance of the spectral method in the high-dimensional regime. Our analysis reveals a phase transition phenomenon that depends on the sampling ratio. When the ratio is below a minimum threshold, the estimates given by the spectral method are no better than a random guess drawn uniformly from the hypersphere; above a maximum threshold, however, the estimates become increasingly aligned with the target signal. The computational complexity of the spectral method is also markedly different in the two phases. Worked examples and numerical results are provided to illustrate and verify the analytical predictions. In particular, simulations show that our asymptotic formulas provide accurate predictions even at moderate signal dimensions. version:1
arxiv-1702-06429 | Stochastic Composite Least-Squares Regression with convergence rate O(1/n) | http://arxiv.org/abs/1702.06429 | id:1702.06429 author:Nicolas Flammarion, Francis Bach category:math.OC stat.ML  published:2017-02-21 summary:We consider the minimization of composite objective functions composed of the expectation of quadratic functions and an arbitrary convex function. We study the stochastic dual averaging algorithm with a constant step-size, showing that it leads to a convergence rate of O(1/n) without strong convexity assumptions. This thus extends earlier results on least-squares regression with the Euclidean geometry to (a) all convex regularizers and constraints, and (b) all geome-tries represented by a Bregman divergence. This is achieved by a new proof technique that relates stochastic and deterministic recursions. version:1
arxiv-1702-06408 | A Discriminative Event Based Model for Alzheimer's Disease Progression Modeling | http://arxiv.org/abs/1702.06408 | id:1702.06408 author:Vikram Venkatraghavan, Esther Bron, Wiro Niessen, Stefan Klein category:cs.CV q-bio.QM  published:2017-02-21 summary:The event-based model (EBM) for data-driven disease progression modeling estimates the sequence in which biomarkers for a disease become abnormal. This helps in understanding the dynamics of disease progression and facilitates early diagnosis by staging patients on a disease progression timeline. Existing EBM methods are all generative in nature. In this work we propose a novel discriminative approach to EBM, which is shown to be more accurate as well as computationally more efficient than existing state-of-the art EBM methods. The method first estimates for each subject an approximate ordering of events, by ranking the posterior probabilities of individual biomarkers being abnormal. Subsequently, the central ordering over all subjects is estimated by fitting a generalized Mallows model to these approximate subject-specific orderings based on a novel probabilistic Kendall's Tau distance. To evaluate the accuracy, we performed extensive experiments on synthetic data simulating the progression of Alzheimer's disease. Subsequently, the method was applied to the Alzheimer's Disease Neuroimaging Initiative (ADNI) data to estimate the central event ordering in the dataset. The experiments benchmark the accuracy of the new model under various conditions and compare it with existing state-of-the-art EBM methods. The results indicate that discriminative EBM could be a simple and elegant approach to disease progression modeling. version:1
arxiv-1702-06385 | Causal Inference on Multivariate Mixed-Type Data by Minimum Description Length | http://arxiv.org/abs/1702.06385 | id:1702.06385 author:Alexander Marx, Jilles Vreeken category:stat.ML cs.LG  published:2017-02-21 summary:Given data over the joint distribution of two univariate or multivariate random variables $X$ and $Y$ of mixed or single type data, we consider the problem of inferring the most likely causal direction between $X$ and $Y$. We take an information theoretic approach, from which it follows that first describing the data over cause and then that of effect given cause is shorter than the reverse direction. For practical inference, we propose a score for causal models for mixed type data based on the Minimum Description Length (MDL) principle. In particular, we model dependencies between $X$ and $Y$ using classification and regression trees. Inferring the optimal model is NP-hard, and hence we propose Crack, a fast greedy algorithm to infer the most likely causal direction directly from the data. Empirical evaluation on synthetic, benchmark, and real world data shows that Crack reliably and with high accuracy infers the correct causal direction on both univariate and multivariate cause--effect pairs over both single and mixed type data. version:1
arxiv-1702-06383 | Deep Geometric Retrieval | http://arxiv.org/abs/1702.06383 | id:1702.06383 author:Y Qian, E Vazquez, B Sengupta category:cs.IR cs.CV  published:2017-02-21 summary:Comparing images in order to recommend items from an image-inventory is a subject of continued interest. Added with the scalability of deep-learning architectures the once `manual' job of hand-crafting features have been largely alleviated, and images can be compared according to features generated from a deep convolutional neural network. In this paper, we compare distance metrics (and divergences) to rank features generated from a neural network, for content-based image retrieval. Specifically, after modelling individual images using approximations of mixture models or sparse covariance estimators we resort to their information-theoretic and Riemann geometric comparisons. We show that using approximations of mixture models enable us to to compute a distance measure based on the Wasserstein metric that requires less effort than computationally intensive optimal transport plans; finally, an affine invariant metric is used to compare the optimal transport metric to its Riemann geometric counterpart -- we conclude that although expensive, retrieval metric based on Wasserstein geometry are more suitable than information theoretic comparison of images. In short, we combine GPU scalability in learning deep feature vectors with computationally efficient metrics that we foresee being utilized in a commercial setting. version:1
arxiv-1702-06376 | Mimicking Ensemble Learning with Deep Branched Networks | http://arxiv.org/abs/1702.06376 | id:1702.06376 author:Byungju Kim, Youngsoo Kim, Yeakang Lee, Junmo Kim category:cs.CV  published:2017-02-21 summary:This paper proposes a branched residual network for image classification. It is known that high-level features of deep neural network are more representative than lower-level features. By sharing the low-level features, the network can allocate more memory to high-level features. The upper layers of our proposed network are branched, so that it mimics the ensemble learning. By mimicking ensemble learning with single network, we have achieved better performance on ImageNet classification task. version:1
arxiv-1702-06355 | Object Detection in Videos with Tubelet Proposal Networks | http://arxiv.org/abs/1702.06355 | id:1702.06355 author:Kai Kang, Hongsheng Li, Tong Xiao, Wanli Ouyang, Junjie Yan, Xihui Liu, Xiaogang Wang category:cs.CV  published:2017-02-21 summary:Object detection in videos has drawn increasing attention recently with the introduction of the large-scale ImageNet VID dataset. Different from object detection in static images, temporal information in videos provides vital information for object detection. To fully utilize temporal information, state-of-the-art methods are therefore based on spatiotemporal tubelets, which are essentially sequences of associated bounding boxes across time. However, the existing methods have major limitations in generating tubelets in terms of quality and efficiency. Motion-based methods are able to obtain dense tubelets, but the lengths are generally only several frames, which is not optimal to incorporate long-term temporal information. Appearance-based methods, usually involving generic object tracking, could generate long tubelets, but are usually computational expensive. In this work, we propose a framework for object detection in videos, which consists of a novel tubelet proposal network to efficiently generate spatiotemporal proposals, and a Long Short-term Memory (LSTM) network that incorporates temporal information from tubelet proposals for achieving high object detection accuracy in videos. The experiments on the large-scale ImageNet VID dataset demonstrate the effectiveness of the proposed framework for object detection in videos. version:1
arxiv-1702-06354 | Interpreting Outliers: Localized Logistic Regression for Density Ratio Estimation | http://arxiv.org/abs/1702.06354 | id:1702.06354 author:Makoto Yamada, Song Liu, Samuel Kaski category:stat.ML cs.LG  published:2017-02-21 summary:We propose an inlier-based outlier detection method capable of both identifying the outliers and explaining why they are outliers, by identifying the outlier-specific features. Specifically, we employ an inlier-based outlier detection criterion, which uses the ratio of inlier and test probability densities as a measure of plausibility of being an outlier. For estimating the density ratio function, we propose a localized logistic regression algorithm. Thanks to the locality of the model, variable selection can be outlier-specific, and will help interpret why points are outliers in a high-dimensional space. Through synthetic experiments, we show that the proposed algorithm can successfully detect the important features for outliers. Moreover, we show that the proposed algorithm tends to outperform existing algorithms in benchmark datasets. version:1
arxiv-1702-06347 | Positive-Unlabeled Demand-Aware Recommendation | http://arxiv.org/abs/1702.06347 | id:1702.06347 author:Jinfeng Yi, Cho-Jui Hsieh, Kush Varshney, Lijun Zhang, Yao Li category:cs.LG  published:2017-02-21 summary:Recommendation for e-commerce with a mix of durable and nondurable goods has characteristics that distinguish it from the well-studied media recommendation problem. The demand for items is a combined effect of form utility and time utility, i.e., a product must both be intrinsically appealing to a consumer and the time must be right for purchase. In particular for durable goods, time utility is a function of inter-purchase duration within product category because consumers are unlikely to purchase two items in the same category in close temporal succession. Moreover, purchase data, in contrast to ratings data, is implicit with non-purchases not necessarily indicating dislike. Together, these issues give rise to the positive-unlabeled demand-aware recommendation problem that we pose via joint low-rank tensor completion and product category inter-purchase duration vector estimation. We further relax this problem and propose a highly scalable alternating minimization approach with which we can solve problems with millions of users and items. We also show superior prediction accuracies on multiple real-world data sets. version:1
arxiv-1702-06341 | Fast rates for online learning in Linearly Solvable Markov Decision Processes | http://arxiv.org/abs/1702.06341 | id:1702.06341 author:Gergely Neu, Vicenç Gómez category:cs.LG math.OC stat.ML  published:2017-02-21 summary:We study the problem of online learning in a class of Markov decision processes known as linearly solvable MDPs. In the stationary version of this problem, a learner interacts with its environment by directly controlling the state transitions, attempting to balance a fixed state-dependent cost and a certain smooth cost penalizing extreme control inputs. In the current paper, we consider an online setting where the state costs may change arbitrarily between consecutive rounds, and the learner only observes the costs at the end of each respective round. We are interested in constructing algorithms for the learner that guarantee small regret against the best stationary control policy chosen in full knowledge of the cost sequence. Our main result is showing that the smoothness of the control cost enables the simple algorithm of following the leader to achieve a regret of order $\log^2 T$ after $T$ rounds, vastly improving on the best known regret bound of order $T^{3/4}$ for this setting. version:1
