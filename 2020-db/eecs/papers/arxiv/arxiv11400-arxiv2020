arxiv-1508-01081 | Detection of Critical Number of People in Interlocked Doors for Security Access Control by Exploiting a Microwave Transceiver-Array | http://arxiv.org/abs/1508.01081 | id:1508.01081 author:Paolo Nesi, Gianni Pantaleo category:cs.CV  published:2015-08-05 summary:Counting the number of people is something many security application focus on, when dealing with controlling accesses in restricted areas, as it occurs with banks, airports, railway stations and governmental offices. This paper presents an automated solution for detecting the presence of more than one person into interlocked doors adopted in many accesses. In most cases, interlocked doors are small areas where other pieces of information and sensors are placed in order to detect the presence of guns, explosive, etc. The general goals and the required environmental condition, allowed us to implement a detection system at lower costs and complexity, with respect to other existing techniques. The system consists of a fixed array of microwave transceiver modules, whose received signals are processed to collect information related to a sort of volume occupied in the interlocked door cabin. The proposed solution has been statistically validated by using statistical analysis. The whole solution has been also implemented to be used in a real time environment and thus validated against real experimental measures. version:1
arxiv-1508-01071 | A MAP approach for $\ell_q$-norm regularized sparse parameter estimation using the EM algorithm | http://arxiv.org/abs/1508.01071 | id:1508.01071 author:Rodrigo Carvajal, Juan C. Agüero, Boris I. Godoy, Dimitrios Katselis category:cs.SY stat.ML  published:2015-08-05 summary:In this paper, Bayesian parameter estimation through the consideration of the Maximum A Posteriori (MAP) criterion is revisited under the prism of the Expectation-Maximization (EM) algorithm. By incorporating a sparsity-promoting penalty term in the cost function of the estimation problem through the use of an appropriate prior distribution, we show how the EM algorithm can be used to efficiently solve the corresponding optimization problem. To this end, we rely on variance-mean Gaussian mixtures (VMGM) to describe the prior distribution, while we incorporate many nice features of these mixtures to our estimation problem. The corresponding MAP estimation problem is completely expressed in terms of the EM algorithm, which allows for handling nonlinearities and hidden variables that cannot be easily handled with traditional methods. For comparison purposes, we also develop a Coordinate Descent algorithm for the $\ell_q$-norm penalized problem and present the performance results via simulations. version:1
arxiv-1508-01067 | Topic Stability over Noisy Sources | http://arxiv.org/abs/1508.01067 | id:1508.01067 author:Jing Su, Oisín Boydell, Derek Greene, Gerard Lynch category:cs.CL cs.IR  published:2015-08-05 summary:Topic modelling techniques such as LDA have recently been applied to speech transcripts and OCR output. These corpora may contain noisy or erroneous texts which may undermine topic stability. Therefore, it is important to know how well a topic modelling algorithm will perform when applied to noisy data. In this paper we show that different types of textual noise will have diverse effects on the stability of different topic models. From these observations, we propose guidelines for text corpus generation, with a focus on automatic speech transcription. We also suggest topic model selection methods for noisy corpora. version:1
arxiv-1508-01057 | On the convergence of the sparse possibilistic c-means algorithm | http://arxiv.org/abs/1508.01057 | id:1508.01057 author:Spyridoula D. Xenaki, Konstantinos D. Koutroumbas, Athanasios A. Rontogiannis category:cs.CV  published:2015-08-05 summary:In this paper, a convergence proof for the recently proposed sparse possibilistic c-means (SPCM) algorithm is provided, utilizing the celebrated Zangwill convergence theorem. It is shown that the iterative sequence generated by SPCM converges to a stationary point or there exists a subsequence of it that converges to a stationary point of the cost function of the algorithm. version:1
arxiv-1508-01055 | Estimating snow cover from publicly available images | http://arxiv.org/abs/1508.01055 | id:1508.01055 author:Roman Fedorov, Alessandro Camerada, Piero Fraternali, Marco Tagliasacchi category:cs.MM cs.CV  published:2015-08-05 summary:In this paper we study the problem of estimating snow cover in mountainous regions, that is, the spatial extent of the earth surface covered by snow. We argue that publicly available visual content, in the form of user generated photographs and image feeds from outdoor webcams, can both be leveraged as additional measurement sources, complementing existing ground, satellite and airborne sensor data. To this end, we describe two content acquisition and processing pipelines that are tailored to such sources, addressing the specific challenges posed by each of them, e.g., identifying the mountain peaks, filtering out images taken in bad weather conditions, handling varying illumination conditions. The final outcome is summarized in a snow cover index, which indicates for a specific mountain and day of the year, the fraction of visible area covered by snow, possibly at different elevations. We created a manually labelled dataset to assess the accuracy of the image snow covered area estimation, achieving 90.0% precision at 91.1% recall. In addition, we show that seasonal trends related to air temperature are captured by the snow cover index. version:1
arxiv-1508-01023 | A review of heterogeneous data mining for brain disorders | http://arxiv.org/abs/1508.01023 | id:1508.01023 author:Bokai Cao, Xiangnan Kong, Philip S. Yu category:cs.LG cs.CE cs.DB q-bio.NC stat.AP  published:2015-08-05 summary:With rapid advances in neuroimaging techniques, the research on brain disorder identification has become an emerging area in the data mining community. Brain disorder data poses many unique challenges for data mining research. For example, the raw data generated by neuroimaging experiments is in tensor representations, with typical characteristics of high dimensionality, structural complexity and nonlinear separability. Furthermore, brain connectivity networks can be constructed from the tensor data, embedding subtle interactions between brain regions. Other clinical measures are usually available reflecting the disease status from different perspectives. It is expected that integrating complementary information in the tensor data and the brain network data, and incorporating other clinical parameters will be potentially transformative for investigating disease mechanisms and for informing therapeutic interventions. Many research efforts have been devoted to this area. They have achieved great success in various applications, such as tensor-based modeling, subgraph pattern mining, multi-view feature analysis. In this paper, we review some recent data mining methods that are used for analyzing brain disorders. version:1
arxiv-1508-01019 | Direct Estimation of the Derivative of Quadratic Mutual Information with Application in Supervised Dimension Reduction | http://arxiv.org/abs/1508.01019 | id:1508.01019 author:Voot Tangkaratt, Hiroaki Sasaki, Masashi Sugiyama category:stat.ML  published:2015-08-05 summary:A typical goal of supervised dimension reduction is to find a low-dimensional subspace of the input space such that the projected input variables preserve maximal information about the output variables. The dependence maximization approach solves the supervised dimension reduction problem through maximizing a statistical dependence between projected input variables and output variables. A well-known statistical dependence measure is mutual information (MI) which is based on the Kullback-Leibler (KL) divergence. However, it is known that the KL divergence is sensitive to outliers. On the other hand, quadratic MI (QMI) is a variant of MI based on the $L_2$ distance which is more robust against outliers than the KL divergence, and a computationally efficient method to estimate QMI from data, called least-squares QMI (LSQMI), has been proposed recently. For these reasons, developing a supervised dimension reduction method based on LSQMI seems promising. However, not QMI itself, but the derivative of QMI is needed for subspace search in supervised dimension reduction, and the derivative of an accurate QMI estimator is not necessarily a good estimator of the derivative of QMI. In this paper, we propose to directly estimate the derivative of QMI without estimating QMI itself. We show that the direct estimation of the derivative of QMI is more accurate than the derivative of the estimated QMI. Finally, we develop a supervised dimension reduction algorithm which efficiently uses the proposed derivative estimator, and demonstrate through experiments that the proposed method is more robust against outliers than existing methods. version:1
arxiv-1508-01011 | Learning from LDA using Deep Neural Networks | http://arxiv.org/abs/1508.01011 | id:1508.01011 author:Dongxu Zhang, Tianyi Luo, Dong Wang, Rong Liu category:cs.LG cs.CL cs.IR cs.NE  published:2015-08-05 summary:Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian model for topic inference. In spite of its great success, inferring the latent topic distribution with LDA is time-consuming. Motivated by the transfer learning approach proposed by~\newcite{hinton2015distilling}, we present a novel method that uses LDA to supervise the training of a deep neural network (DNN), so that the DNN can approximate the costly LDA inference with less computation. Our experiments on a document classification task show that a simple DNN can learn the LDA behavior pretty well, while the inference is speeded up tens or hundreds of times. version:1
arxiv-1508-01008 | INsight: A Neuromorphic Computing System for Evaluation of Large Neural Networks | http://arxiv.org/abs/1508.01008 | id:1508.01008 author:Jaeyong Chung, Taehwan Shin, Yongshin Kang category:cs.NE  published:2015-08-05 summary:Deep neural networks have been demonstrated impressive results in various cognitive tasks such as object detection and image classification. In order to execute large networks, Von Neumann computers store the large number of weight parameters in external memories, and processing elements are timed-shared, which leads to power-hungry I/O operations and processing bottlenecks. This paper describes a neuromorphic computing system that is designed from the ground up for the energy-efficient evaluation of large-scale neural networks. The computing system consists of a non-conventional compiler, a neuromorphic architecture, and a space-efficient microarchitecture that leverages existing integrated circuit design methodologies. The compiler factorizes a trained, feedforward network into a sparsely connected network, compresses the weights linearly, and generates a time delay neural network reducing the number of connections. The connections and units in the simplified network are mapped to silicon synapses and neurons. We demonstrate an implementation of the neuromorphic computing system based on a field-programmable gate array that performs the MNIST hand-written digit classification with 97.64% accuracy. version:1
arxiv-1508-00984 | Dimension Reduction with Non-degrading Generalization | http://arxiv.org/abs/1508.00984 | id:1508.00984 author:Pitoyo Hartono category:cs.LG cs.NE  published:2015-08-05 summary:Visualizing high dimensional data by projecting them into two or three dimensional space is one of the most effective ways to intuitively understand the data's underlying characteristics, for example their class neighborhood structure. While data visualization in low dimensional space can be efficient for revealing the data's underlying characteristics, classifying a new sample in the reduced-dimensional space is not always beneficial because of the loss of information in expressing the data. It is possible to classify the data in the high dimensional space, while visualizing them in the low dimensional space, but in this case, the visualization is often meaningless because it fails to illustrate the underlying characteristics that are crucial for the classification process. In this paper, the performance-preserving property of the previously proposed Restricted Radial Basis Function Network in reducing the dimension of labeled data is explained. Here, it is argued through empirical experiments that the internal representation of the Restricted Radial Basis Function Network, which during the supervised learning process organizes a visualizable two dimensional map, does not only preserve the topographical structure of high dimensional data but also captures their class neighborhood structures that are important for classifying them. Hence, unlike many of the existing dimension reduction methods, the Restricted Radial Basis Function Network offers two dimensional visualization that is strongly correlated with the classification process. version:1
arxiv-1508-00973 | Progressive EM for Latent Tree Models and Hierarchical Topic Detection | http://arxiv.org/abs/1508.00973 | id:1508.00973 author:Peixian Chen, Nevin L. Zhang, Leonard K. M. Poon, Zhourong Chen category:cs.LG cs.CL cs.IR stat.ML  published:2015-08-05 summary:Hierarchical latent tree analysis (HLTA) is recently proposed as a new method for topic detection. It differs fundamentally from the LDA-based methods in terms of topic definition, topic-document relationship, and learning method. It has been shown to discover significantly more coherent topics and better topic hierarchies. However, HLTA relies on the Expectation-Maximization (EM) algorithm for parameter estimation and hence is not efficient enough to deal with large datasets. In this paper, we propose a method to drastically speed up HLTA using a technique inspired by recent advances in the moments method. Empirical experiments show that our method greatly improves the efficiency of HLTA. It is as efficient as the state-of-the-art LDA-based method for hierarchical topic detection and finds substantially better topics and topic hierarchies. version:1
arxiv-1508-00966 | 3D Automatic Segmentation Method for Retinal Optical Coherence Tomography Volume Data Using Boundary Surface Enhancement | http://arxiv.org/abs/1508.00966 | id:1508.00966 author:Yankui Sun, Tian Zhang, Yue Zhao, Yufan He category:cs.CV  published:2015-08-05 summary:With the introduction of spectral-domain optical coherence tomography (SDOCT), much larger image datasets are routinely acquired compared to what was possible using the previous generation of time-domain OCT. Thus, there is a critical need for the development of 3D segmentation methods for processing these data. We present here a novel 3D automatic segmentation method for retinal OCT volume data. Briefly, to segment a boundary surface, two OCT volume datasets are obtained by using a 3D smoothing filter and a 3D differential filter. Their linear combination is then calculated to generate new volume data with an enhanced boundary surface, where pixel intensity, boundary position information, and intensity changes on both sides of the boundary surface are used simultaneously. Next, preliminary discrete boundary points are detected from the A-Scans of the volume data. Finally, surface smoothness constraints and a dynamic threshold are applied to obtain a smoothed boundary surface by correcting a small number of error points. Our method can extract retinal layer boundary surfaces sequentially with a decreasing search region of volume data. We performed automatic segmentation on eight human OCT volume datasets acquired from a commercial Spectralis OCT system, where each volume of data consisted of 97 OCT images with a resolution of 496 512; experimental results show that this method can accurately segment seven layer boundary surfaces in normal as well as some abnormal eyes. version:1
arxiv-1505-03823 | Distant Supervision for Entity Linking | http://arxiv.org/abs/1505.03823 | id:1505.03823 author:Miao Fan, Qiang Zhou, Thomas Fang Zheng category:cs.CL cs.IR  published:2015-05-14 summary:Entity linking is an indispensable operation of populating knowledge repositories for information extraction. It studies on aligning a textual entity mention to its corresponding disambiguated entry in a knowledge repository. In this paper, we propose a new paradigm named distantly supervised entity linking (DSEL), in the sense that the disambiguated entities that belong to a huge knowledge repository (Freebase) are automatically aligned to the corresponding descriptive webpages (Wiki pages). In this way, a large scale of weakly labeled data can be generated without manual annotation and fed to a classifier for linking more newly discovered entities. Compared with traditional paradigms based on solo knowledge base, DSEL benefits more via jointly leveraging the respective advantages of Freebase and Wikipedia. Specifically, the proposed paradigm facilitates bridging the disambiguated labels (Freebase) of entities and their textual descriptions (Wikipedia) for Web-scale entities. Experiments conducted on a dataset of 140,000 items and 60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze the feature performance and improve the F1-measure to 0.545. version:3
arxiv-1508-00882 | Asynchronous stochastic convex optimization | http://arxiv.org/abs/1508.00882 | id:1508.00882 author:John C. Duchi, Sorathan Chaturapruek, Christopher Ré category:math.OC stat.ML  published:2015-08-04 summary:We show that asymptotically, completely asynchronous stochastic gradient procedures achieve optimal (even to constant factors) convergence rates for the solution of convex optimization problems under nearly the same conditions required for asymptotic optimality of standard stochastic gradient procedures. Roughly, the noise inherent to the stochastic approximation scheme dominates any noise from asynchrony. We also give empirical evidence demonstrating the strong performance of asynchronous, parallel stochastic optimization schemes, demonstrating that the robustness inherent to stochastic approximation problems allows substantially faster parallel and asynchronous solution methods. version:1
arxiv-1501-00752 | A Deep-structured Conditional Random Field Model for Object Silhouette Tracking | http://arxiv.org/abs/1501.00752 | id:1501.00752 author:Mohammad Shafiee, Zohreh Azimifar, Alexander Wong category:cs.CV cs.LG stat.ML  published:2015-01-05 summary:In this work, we introduce a deep-structured conditional random field (DS-CRF) model for the purpose of state-based object silhouette tracking. The proposed DS-CRF model consists of a series of state layers, where each state layer spatially characterizes the object silhouette at a particular point in time. The interactions between adjacent state layers are established by inter-layer connectivity dynamically determined based on inter-frame optical flow. By incorporate both spatial and temporal context in a dynamic fashion within such a deep-structured probabilistic graphical model, the proposed DS-CRF model allows us to develop a framework that can accurately and efficiently track object silhouettes that can change greatly over time, as well as under different situations such as occlusion and multiple targets within the scene. Experiment results using video surveillance datasets containing different scenarios such as occlusion and multiple targets showed that the proposed DS-CRF approach provides strong object silhouette tracking performance when compared to baseline methods such as mean-shift tracking, as well as state-of-the-art methods such as context tracking and boosted particle filtering. version:2
arxiv-1508-00835 | Semantic Pose using Deep Networks Trained on Synthetic RGB-D | http://arxiv.org/abs/1508.00835 | id:1508.00835 author:Jeremie Papon, Markus Schoeler category:cs.CV  published:2015-08-04 summary:In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel on a GPU in seconds. version:1
arxiv-1501-01181 | Object localization in ImageNet by looking out of the window | http://arxiv.org/abs/1501.01181 | id:1501.01181 author:Alexander Vezhnevets, Vittorio Ferrari category:cs.CV  published:2015-01-06 summary:We propose a method for annotating the location of objects in ImageNet. Traditionally, this is cast as an image window classification problem, where each window is considered independently and scored based on its appearance alone. Instead, we propose a method which scores each candidate window in the context of all other windows in the image, taking into account their similarity in appearance space as well as their spatial relations in the image plane. We devise a fast and exact procedure to optimize our scoring function over all candidate windows in an image, and we learn its parameters using structured output regression. We demonstrate on 92000 images from ImageNet that this significantly improves localization over recent techniques that score windows in isolation. version:2
arxiv-1508-00776 | Online Domain Adaptation for Multi-Object Tracking | http://arxiv.org/abs/1508.00776 | id:1508.00776 author:Adrien Gaidon, Eleonora Vig category:cs.CV  published:2015-08-04 summary:Automatically detecting, labeling, and tracking objects in videos depends first and foremost on accurate category-level object detectors. These might, however, not always be available in practice, as acquiring high-quality large scale labeled training datasets is either too costly or impractical for all possible real-world application scenarios. A scalable solution consists in re-using object detectors pre-trained on generic datasets. This work is the first to investigate the problem of on-line domain adaptation of object detectors for causal multi-object tracking (MOT). We propose to alleviate the dataset bias by adapting detectors from category to instances, and back: (i) we jointly learn all target models by adapting them from the pre-trained one, and (ii) we also adapt the pre-trained model on-line. We introduce an on-line multi-task learning algorithm to efficiently share parameters and reduce drift, while gradually improving recall. Our approach is applicable to any linear object detector, and we evaluate both cheap "mini-Fisher Vectors" and expensive "off-the-shelf" ConvNet features. We quantitatively measure the benefit of our domain adaptation strategy on the KITTI tracking benchmark and on a new dataset (PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT. version:1
arxiv-1508-00761 | Recognition of Emotions using Kinects | http://arxiv.org/abs/1508.00761 | id:1508.00761 author:Shun Li, Changye Zhu, Liqing Cui, Nan Zhao, Baobin Li, Tingshao Zhu category:cs.CY cs.CV cs.HC  published:2015-08-04 summary:Psychological studies indicate that emotional states are expressed in the way people walk and the human gait is investigated in terms of its ability to reveal a person's emotional state. And Microsoft Kinect is a rapidly developing, inexpensive, portable and no-marker motion capture system. This paper gives a new referable method to do emotion recognition, by using Microsoft Kinect to do gait pattern analysis, which has not been reported. $59$ subjects are recruited in this study and their gait patterns are record by two Kinect cameras. Significant joints selecting, Coordinate system transforming, Slider window gauss filter, Differential operation, and Data segmentation are used in data preprocessing. Feature extracting is based on Fourier transformation. By using the NaiveBayes, RandomForests, libSVM and SMO classification, the recognition rate of natural and unnatural emotions can reach above 70%.It is concluded that using the Kinect system can be a new method in recognition of emotions. version:1
arxiv-1508-00722 | Multi-Label Active Learning from Crowds | http://arxiv.org/abs/1508.00722 | id:1508.00722 author:Shao-Yuan Li, Yuan Jiang, Zhi-Hua Zhou category:cs.LG cs.SI  published:2015-08-04 summary:Multi-label active learning is a hot topic in reducing the label cost by optimally choosing the most valuable instance to query its label from an oracle. In this paper, we consider the poolbased multi-label active learning under the crowdsourcing setting, where during the active query process, instead of resorting to a high cost oracle for the ground-truth, multiple low cost imperfect annotators with various expertise are available for labeling. To deal with this problem, we propose the MAC (Multi-label Active learning from Crowds) approach which incorporate the local influence of label correlations to build a probabilistic model over the multi-label classifier and annotators. Based on this model, we can estimate the labels for instances as well as the expertise of each annotator. Then we propose the instance selection and annotator selection criteria that consider the uncertainty/diversity of instances and the reliability of annotators, such that the most reliable annotator will be queried for the most valuable instances. Experimental results demonstrate the effectiveness of the proposed approach. version:1
arxiv-1508-00430 | Kernelized Multiview Projection | http://arxiv.org/abs/1508.00430 | id:1508.00430 author:Mengyang Yu, Li Liu, Ling Shao category:cs.CV  published:2015-08-03 summary:Conventional vision algorithms adopt a single type of feature or a simple concatenation of multiple features, which is always represented in a high-dimensional space. In this paper, we propose a novel unsupervised spectral embedding algorithm called Kernelized Multiview Projection (KMP) to better fuse and embed different feature representations. Computing the kernel matrices from different features/views, KMP can encode them with the corresponding weights to achieve a low-dimensional and semantically meaningful subspace where the distribution of each view is sufficiently smooth and discriminative. More crucially, KMP is linear for the reproducing kernel Hilbert space (RKHS) and solves the out-of-sample problem, which allows it to be competent for various practical applications. Extensive experiments on three popular image datasets demonstrate the effectiveness of our multiview embedding algorithm. version:2
arxiv-1508-00703 | Parameter Database : Data-centric Synchronization for Scalable Machine Learning | http://arxiv.org/abs/1508.00703 | id:1508.00703 author:Naman Goel, Divyakant Agrawal, Sanjay Chawla, Ahmed Elmagarmid category:cs.DB cs.LG  published:2015-08-04 summary:We propose a new data-centric synchronization framework for carrying out of machine learning (ML) tasks in a distributed environment. Our framework exploits the iterative nature of ML algorithms and relaxes the application agnostic bulk synchronization parallel (BSP) paradigm that has previously been used for distributed machine learning. Data-centric synchronization complements function-centric synchronization based on using stale updates to increase the throughput of distributed ML computations. Experiments to validate our framework suggest that we can attain substantial improvement over BSP while guaranteeing sequential correctness of ML tasks. version:1
arxiv-1408-5418 | Hierarchical Saliency Detection on Extended CSSD | http://arxiv.org/abs/1408.5418 | id:1408.5418 author:Jianping Shi, Qiong Yan, Li Xu, Jiaya Jia category:cs.CV  published:2014-08-11 summary:Complex structures commonly exist in natural images. When an image contains small-scale high-contrast patterns either in the background or foreground, saliency detection could be adversely affected, resulting erroneous and non-uniform saliency assignment. The issue forms a fundamental challenge for prior methods. We tackle it from a scale point of view and propose a multi-layer approach to analyze saliency cues. Different from varying patch sizes or downsizing images, we measure region-based scales. The final saliency values are inferred optimally combining all the saliency cues in different scales using hierarchical inference. Through our inference model, single-scale information is selected to obtain a saliency map. Our method improves detection quality on many images that cannot be handled well traditionally. We also construct an extended Complex Scene Saliency Dataset (ECSSD) to include complex but general natural images. version:2
arxiv-1504-00983 | Temporal Localization of Fine-Grained Actions in Videos by Domain Transfer from Web Images | http://arxiv.org/abs/1504.00983 | id:1504.00983 author:Chen Sun, Sanketh Shetty, Rahul Sukthankar, Ram Nevatia category:cs.CV cs.MM I.2.10  published:2015-04-04 summary:We address the problem of fine-grained action localization from temporally untrimmed web videos. We assume that only weak video-level annotations are available for training. The goal is to use these weak labels to identify temporal segments corresponding to the actions, and learn models that generalize to unconstrained web videos. We find that web images queried by action names serve as well-localized highlights for many actions, but are noisily labeled. To solve this problem, we propose a simple yet effective method that takes weak video labels and noisy image labels as input, and generates localized action frames as output. This is achieved by cross-domain transfer between video frames and web images, using pre-trained deep convolutional neural networks. We then use the localized action frames to train action recognition models with long short-term memory networks. We collect a fine-grained sports action data set FGA-240 of more than 130,000 YouTube videos. It has 240 fine-grained actions under 85 sports activities. Convincing results are shown on the FGA-240 data set, as well as the THUMOS 2014 localization data set with untrimmed training videos. version:2
arxiv-1508-00655 | Adaptivity and Computation-Statistics Tradeoffs for Kernel and Distance based High Dimensional Two Sample Testing | http://arxiv.org/abs/1508.00655 | id:1508.00655 author:Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry Wasserman category:math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH  published:2015-08-04 summary:Nonparametric two sample testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. We refer to the most common settings as mean difference alternatives (MDA), for testing differences only in first moments, and general difference alternatives (GDA), which is about testing for any difference in distributions. A large number of test statistics have been proposed for both these settings. This paper connects three classes of statistics - high dimensional variants of Hotelling's t-test, statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics based on pairwise distances. We ask the question: how much statistical power do popular kernel and distance based tests for GDA have when the unknown distributions differ in their means, compared to specialized tests for MDA? We formally characterize the power of popular tests for GDA like the Maximum Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent variants of the Energy Distance with the Euclidean norm (eED) in the high-dimensional MDA regime. Some practically important properties include (a) eED and gMMD have asymptotically equal power; furthermore they enjoy a free lunch because, while they are additionally consistent for GDA, they also have the same power as specialized high-dimensional t-test variants for MDA. All these tests are asymptotically optimal (including matching constants) under MDA for spherical covariances, according to simple lower bounds, (b) The power of gMMD is independent of the kernel bandwidth, as long as it is larger than the choice made by the median heuristic, (c) There is a clear and smooth computation-statistics tradeoff for linear-time, subquadratic-time and quadratic-time versions of these tests, with more computation resulting in higher power. version:1
arxiv-1508-01168 | Particle Swarm Optimization for Weighted Sum Rate Maximization in MIMO Broadcast Channels | http://arxiv.org/abs/1508.01168 | id:1508.01168 author:Tung T. Vu, Ha Hoang Kha, Trung Q. Duong, Nguyen-Son Vo category:cs.IT cs.NE math.IT math.OC  published:2015-08-04 summary:In this paper, we investigate the downlink multiple-input-multipleoutput (MIMO) broadcast channels in which a base transceiver station (BTS) broadcasts multiple data streams to K MIMO mobile stations (MSs) simultaneously. In order to maximize the weighted sum-rate (WSR) of the system subject to the transmitted power constraint, the design problem is to find the pre-coding matrices at BTS and the decoding matrices at MSs. However, such a design problem is typically a nonlinear and nonconvex optimization and, thus, it is quite hard to obtain the analytical solutions. To tackle with the mathematical difficulties, we propose an efficient stochastic optimization algorithm to optimize the transceiver matrices. Specifically, we utilize the linear minimum mean square error (MMSE) Wiener filters at MSs. Then, we introduce the constrained particle swarm optimization (PSO) algorithm to jointly optimize the precoding and decoding matrices. Numerical experiments are exhibited to validate the effectiveness of the proposed algorithm in terms of convergence, computational complexity and total WSR. version:1
arxiv-1508-00217 | Indexing of CNN Features for Large Scale Image Search | http://arxiv.org/abs/1508.00217 | id:1508.00217 author:Ruoyu Liu, Yao Zhao, Shikui Wei, Zhenfeng Zhu, Lixin Liao, Shuang Qiu category:cs.CV  published:2015-08-02 summary:Convolutional neural network (CNN) feature that represents an image with a global and high-dimensional vector has shown highly discriminative capability in image search. Although CNN features are more compact than most of local representation schemes, it still cannot efficiently deal with large-scale image search issues due to its non-negligible computational cost and storage usage. In this paper, we propose a simple but effective image indexing framework to improve the computational and storage efficiency of CNN features. Instead of projecting each CNN feature vector into a global hashing code, the proposed framework adapts Bag-of-Word model and inverted table to global feature indexing. To this end, two strategies, which are based on semantic information associated with CNN features, are proposed to convert a global vector to one or several discrete words. In addition, several strategies for compensating quantization error are fully investigated under the indexing framework. Extensive experimental results on two public benchmarks show the superiority of our framework. version:2
arxiv-1508-00635 | Bayesian mixtures of spatial spline regressions | http://arxiv.org/abs/1508.00635 | id:1508.00635 author:Faicel Chamroukhi category:stat.ME cs.LG stat.CO stat.ML  published:2015-08-04 summary:This work relates the framework of model-based clustering for spatial functional data where the data are surfaces. We first introduce a Bayesian spatial spline regression model with mixed-effects (BSSR) for modeling spatial function data. The BSSR model is based on Nodal basis functions for spatial regression and accommodates both common mean behavior for the data through a fixed-effects part, and variability inter-individuals thanks to a random-effects part. Then, in order to model populations of spatial functional data issued from heterogeneous groups, we integrate the BSSR model into a mixture framework. The resulting model is a Bayesian mixture of spatial spline regressions with mixed-effects (BMSSR) used for density estimation and model-based surface clustering. The models, through their Bayesian formulation, allow to integrate possible prior knowledge on the data structure and constitute a good alternative to recent mixture of spatial spline regressions model estimated in a maximum likelihood framework via the expectation-maximization (EM) algorithm. The Bayesian model inference is performed by Markov Chain Monte Carlo (MCMC) sampling. We derive two Gibbs sampler to infer the BSSR and the BMSSR models and apply them on simulated surfaces and a real problem of handwritten digit recognition using the MNIST data set. The obtained results highlight the potential benefit of the proposed Bayesian approaches for modeling surfaces possibly dispersed in particular in clusters. version:1
arxiv-1508-00625 | Sparse PCA via Bipartite Matchings | http://arxiv.org/abs/1508.00625 | id:1508.00625 author:Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, Alexandros G. Dimakis category:stat.ML cs.DS cs.LG math.OC  published:2015-08-04 summary:We consider the following multi-component sparse PCA problem: given a set of data points, we seek to extract a small number of sparse components with disjoint supports that jointly capture the maximum possible variance. These components can be computed one by one, repeatedly solving the single-component problem and deflating the input data matrix, but as we show this greedy procedure is suboptimal. We present a novel algorithm for sparse PCA that jointly optimizes multiple disjoint components. The extracted features capture variance that lies within a multiplicative factor arbitrarily close to 1 from the optimal. Our algorithm is combinatorial and computes the desired components by solving multiple instances of the bipartite maximum weight matching problem. Its complexity grows as a low order polynomial in the ambient dimension of the input data matrix, but exponentially in its rank. However, it can be effectively applied on a low-dimensional sketch of the data; this allows us to obtain polynomial-time approximation guarantees via spectral bounds. We evaluate our algorithm on real data-sets and empirically demonstrate that in many cases it outperforms existing, deflation-based approaches. version:1
arxiv-1508-00537 | Evaluating software-based fingerprint liveness detection using Convolutional Networks and Local Binary Patterns | http://arxiv.org/abs/1508.00537 | id:1508.00537 author:Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, Rubens Campos Machado category:cs.CV  published:2015-08-03 summary:With the growing use of biometric authentication systems in the past years, spoof fingerprint detection has become increasingly important. In this work, we implement and evaluate two different feature extraction techniques for software-based fingerprint liveness detection: Convolutional Networks with random weights and Local Binary Patterns. Both techniques were used in conjunction with a Support Vector Machine (SVM) classifier. Dataset Augmentation was used to increase classifier's performance and a variety of preprocessing operations were tested, such as frequency filtering, contrast equalization, and region of interest filtering. The experiments were made on the datasets used in The Liveness Detection Competition of years 2009, 2011 and 2013, which comprise almost 50,000 real and fake fingerprints' images. Our best method achieves an overall rate of 95.2% of correctly classified samples - an improvement of 35% in test error when compared with the best previously published results. version:1
arxiv-1508-00507 | A Weakly Supervised Learning Approach based on Spectral Graph-Theoretic Grouping | http://arxiv.org/abs/1508.00507 | id:1508.00507 author:Tameem Adel, Alexander Wong, Daniel Stashuk category:cs.LG cs.AI  published:2015-08-03 summary:In this study, a spectral graph-theoretic grouping strategy for weakly supervised classification is introduced, where a limited number of labelled samples and a larger set of unlabelled samples are used to construct a larger annotated training set composed of strongly labelled and weakly labelled samples. The inherent relationship between the set of strongly labelled samples and the set of unlabelled samples is established via spectral grouping, with the unlabelled samples subsequently weakly annotated based on the strongly labelled samples within the associated spectral groups. A number of similarity graph models for spectral grouping, including two new similarity graph models introduced in this study, are explored to investigate their performance in the context of weakly supervised classification in handling different types of data. Experimental results using benchmark datasets as well as real EMG datasets demonstrate that the proposed approach to weakly supervised classification can provide noticeable improvements in classification performance, and that the proposed similarity graph models can lead to ultimate learning results that are either better than or on a par with existing similarity graph models in the context of spectral grouping for weakly supervised classification. version:1
arxiv-1508-00468 | Evolutionary Algorithms: Concepts, Designs, and Applications in Bioinformatics: Evolutionary Algorithms for Bioinformatics | http://arxiv.org/abs/1508.00468 | id:1508.00468 author:Ka-Chun Wong category:cs.NE q-bio.GN q-bio.QM stat.CO stat.ME  published:2015-08-03 summary:Since genetic algorithm was proposed by John Holland (Holland J. H., 1975) in the early 1970s, the study of evolutionary algorithm has emerged as a popular research field (Civicioglu & Besdok, 2013). Researchers from various scientific and engineering disciplines have been digging into this field, exploring the unique power of evolutionary algorithms (Hadka & Reed, 2013). Many applications have been successfully proposed in the past twenty years. For example, mechanical design (Lampinen & Zelinka, 1999), electromagnetic optimization (Rahmat-Samii & Michielssen, 1999), environmental protection (Bertini, Felice, Moretti, & Pizzuti, 2010), finance (Larkin & Ryan, 2010), musical orchestration (Esling, Carpentier, & Agon, 2010), pipe routing (Furuholmen, Glette, Hovin, & Torresen, 2010), and nuclear reactor core design (Sacco, Henderson, Rios-Coelho, Ali, & Pereira, 2009). In particular, its function optimization capability was highlighted (Goldberg & Richardson, 1987) because of its high adaptability to different function landscapes, to which we cannot apply traditional optimization techniques (Wong, Leung, & Wong, 2009). Here we review the applications of evolutionary algorithms in bioinformatics. version:1
arxiv-1508-00459 | Unsupervised Learning in Genome Informatics | http://arxiv.org/abs/1508.00459 | id:1508.00459 author:Ka-Chun Wong, Yue Li, Zhaolei Zhang category:q-bio.GN q-bio.QM stat.AP stat.ME stat.ML  published:2015-08-03 summary:With different genomes available, unsupervised learning algorithms are essential in learning genome-wide biological insights. Especially, the functional characterization of different genomes is essential for us to understand lives. In this book chapter, we review the state-of-the-art unsupervised learning algorithms for genome informatics from DNA to MicroRNA. DNA (DeoxyriboNucleic Acid) is the basic component of genomes. A significant fraction of DNA regions (transcription factor binding sites) are bound by proteins (transcription factors) to regulate gene expression at different development stages in different tissues. To fully understand genetics, it is necessary of us to apply unsupervised learning algorithms to learn and infer those DNA regions. Here we review several unsupervised learning methods for deciphering the genome-wide patterns of those DNA regions. MicroRNA (miRNA), a class of small endogenous non-coding RNA (RiboNucleic acid) species, regulate gene expression post-transcriptionally by forming imperfect base-pair with the target sites primarily at the 3$'$ untranslated regions of the messenger RNAs. Since the 1993 discovery of the first miRNA \emph{let-7} in worms, a vast amount of studies have been dedicated to functionally characterizing the functional impacts of miRNA in a network context to understand complex diseases such as cancer. Here we review several representative unsupervised learning frameworks on inferring miRNA regulatory network by exploiting the static sequence-based information pertinent to the prior knowledge of miRNA targeting and the dynamic information of miRNA activities implicated by the recently available large data compendia, which interrogate genome-wide expression profiles of miRNAs and/or mRNAs across various cell conditions. version:1
arxiv-1508-00457 | Evolutionary Multimodal Optimization: A Short Survey | http://arxiv.org/abs/1508.00457 | id:1508.00457 author:Ka-Chun Wong category:cs.NE cs.AI q-bio.QM  published:2015-08-03 summary:Real world problems always have different multiple solutions. For instance, optical engineers need to tune the recording parameters to get as many optimal solutions as possible for multiple trials in the varied-line-spacing holographic grating design problem. Unfortunately, most traditional optimization techniques focus on solving for a single optimal solution. They need to be applied several times; yet all solutions are not guaranteed to be found. Thus the multimodal optimization problem was proposed. In that problem, we are interested in not only a single optimal point, but also the others. With strong parallel search capability, evolutionary algorithms are shown to be particularly effective in solving this type of problem. In particular, the evolutionary algorithms for multimodal optimization usually not only locate multiple optima in a single run, but also preserve their population diversity throughout a run, resulting in their global optimization ability on multimodal functions. In addition, the techniques for multimodal optimization are borrowed as diversity maintenance techniques to other problems. In this chapter, we describe and review the state-of-the-arts evolutionary algorithms for multimodal optimization in terms of methodology, benchmarking, and application. version:1
arxiv-1507-01839 | Dependency-based Convolutional Neural Networks for Sentence Embedding | http://arxiv.org/abs/1507.01839 | id:1507.01839 author:Mingbo Ma, Liang Huang, Bing Xiang, Bowen Zhou category:cs.CL cs.AI cs.LG  published:2015-07-07 summary:In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC. version:2
arxiv-1508-00354 | Significance of Maximum Spectral Amplitude in Sub-bands for Spectral Envelope Estimation and Its Application to Statistical Parametric Speech Synthesis | http://arxiv.org/abs/1508.00354 | id:1508.00354 author:Sivanand Achanta, Anandaswarup Vadapalli, Sai Krishna R., Suryakanth V. Gangashetty category:cs.SD cs.CL  published:2015-08-03 summary:In this paper we propose a technique for spectral envelope estimation using maximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most other methods in the literature parametrize spectral envelope in cepstral domain such as Mel-generalized cepstrum etc. Such cepstral domain representations, although compact, are not readily interpretable. This difficulty is overcome by our method which parametrizes in the spectral domain itself. In our experiments, spectral envelope estimated using MSASB method was incorporated in the STRAIGHT vocoder. Both objective and subjective results of analysis-by-synthesis indicate that the proposed method is comparable to STRAIGHT. We also evaluate the effectiveness of the proposed parametrization in a statistical parametric speech synthesis framework using deep neural networks. version:1
arxiv-1508-00317 | Time-series modeling with undecimated fully convolutional neural networks | http://arxiv.org/abs/1508.00317 | id:1508.00317 author:Roni Mittelman category:stat.ML cs.LG  published:2015-08-03 summary:We present a new convolutional neural network-based time-series model. Typical convolutional neural network (CNN) architectures rely on the use of max-pooling operators in between layers, which leads to reduced resolution at the top layers. Instead, in this work we consider a fully convolutional network (FCN) architecture that uses causal filtering operations, and allows for the rate of the output signal to be the same as that of the input signal. We furthermore propose an undecimated version of the FCN, which we refer to as the undecimated fully convolutional neural network (UFCNN), and is motivated by the undecimated wavelet transform. Our experimental results verify that using the undecimated version of the FCN is necessary in order to allow for effective time-series modeling. The UFCNN has several advantages compared to other time-series models such as the recurrent neural network (RNN) and long short-term memory (LSTM), since it does not suffer from either the vanishing or exploding gradients problems, and is therefore easier to train. Convolution operations can also be implemented more efficiently compared to the recursion that is involved in RNN-based models. We evaluate the performance of our model in a synthetic target tracking task using bearing only measurements generated from a state-space model, a probabilistic modeling of polyphonic music sequences problem, and a high frequency trading task using a time-series of ask/bid quotes and their corresponding volumes. Our experimental results using synthetic and real datasets verify the significant advantages of the UFCNN compared to the RNN and LSTM baselines. version:1
arxiv-1508-00307 | Local Color Contrastive Descriptor for Image Classification | http://arxiv.org/abs/1508.00307 | id:1508.00307 author:Sheng Guo, Weilin Huang, Yu Qiao category:cs.CV  published:2015-08-03 summary:Image representation and classification are two fundamental tasks towards multimedia content retrieval and understanding. The idea that shape and texture information (e.g. edge or orientation) are the key features for visual representation is ingrained and dominated in current multimedia and computer vision communities. A number of low-level features have been proposed by computing local gradients (e.g. SIFT, LBP and HOG), and have achieved great successes on numerous multimedia applications. In this paper, we present a simple yet efficient local descriptor for image classification, referred as Local Color Contrastive Descriptor (LCCD), by leveraging the neural mechanisms of color contrast. The idea originates from the observation in neural science that color and shape information are linked inextricably in visual cortical processing. The color contrast yields key information for visual color perception and provides strong linkage between color and shape. We propose a novel contrastive mechanism to compute the color contrast in both spatial location and multiple channels. The color contrast is computed by measuring \emph{f}-divergence between the color distributions of two regions. Our descriptor enriches local image representation with both color and contrast information. We verified experimentally that it can compensate strongly for the shape based descriptor (e.g. SIFT), while keeping computationally simple. Extensive experimental results on image classification show that our descriptor improves the performance of SIFT substantially by combinations, and achieves the state-of-the-art performance on three challenging benchmark datasets. It improves recent Deep Learning model (DeCAF) [1] largely from the accuracy of 40.94% to 49.68% in the large scale SUN397 database. Codes for the LCCD will be available. version:1
arxiv-1508-00305 | Compositional Semantic Parsing on Semi-Structured Tables | http://arxiv.org/abs/1508.00305 | id:1508.00305 author:Panupong Pasupat, Percy Liang category:cs.CL  published:2015-08-03 summary:Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: answering complex questions on semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: the broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. version:1
arxiv-1508-00299 | When Crowdsourcing Meets Mobile Sensing: A Social Network Perspective | http://arxiv.org/abs/1508.00299 | id:1508.00299 author:Pin-Yu Chen, Shin-Ming Cheng, Pai-Shun Ting, Chia-Wei Lien, Fu-Jen Chu category:cs.SI stat.ML  published:2015-08-03 summary:Mobile sensing is an emerging technology that utilizes agent-participatory data for decision making or state estimation, including multimedia applications. This article investigates the structure of mobile sensing schemes and introduces crowdsourcing methods for mobile sensing. Inspired by social network, one can establish trust among participatory agents to leverage the wisdom of crowds for mobile sensing. A prototype of social network inspired mobile multimedia and sensing application is presented for illustrative purpose. Numerical experiments on real-world datasets show improved performance of mobile sensing via crowdsourcing. Challenges for mobile sensing with respect to Internet layers are discussed. version:1
arxiv-1504-03391 | Tight Bounds on Low-degree Spectral Concentration of Submodular and XOS functions | http://arxiv.org/abs/1504.03391 | id:1504.03391 author:Vitaly Feldman, Jan Vondrak category:cs.DS cs.LG  published:2015-04-13 summary:Submodular and fractionally subadditive (or equivalently XOS) functions play a fundamental role in combinatorial optimization, algorithmic game theory and machine learning. Motivated by learnability of these classes of functions from random examples, we consider the question of how well such functions can be approximated by low-degree polynomials in $\ell_2$ norm over the uniform distribution. This question is equivalent to understanding of the concentration of Fourier weight on low-degree coefficients, a central concept in Fourier analysis. We show that 1. For any submodular function $f:\{0,1\}^n \rightarrow [0,1]$, there is a polynomial of degree $O(\log (1/\epsilon) / \epsilon^{4/5})$ approximating $f$ within $\epsilon$ in $\ell_2$, and there is a submodular function that requires degree $\Omega(1/\epsilon^{4/5})$. 2. For any XOS function $f:\{0,1\}^n \rightarrow [0,1]$, there is a polynomial of degree $O(1/\epsilon)$ and there exists an XOS function that requires degree $\Omega(1/\epsilon)$. This improves on previous approaches that all showed an upper bound of $O(1/\epsilon^2)$ for submodular and XOS functions. The best previous lower bound was $\Omega(1/\epsilon^{2/3})$ for monotone submodular functions. Our techniques reveal new structural properties of submodular and XOS functions and the upper bounds lead to nearly optimal PAC learning algorithms for these classes of functions. version:2
arxiv-1508-00285 | Optimal Radio Frequency Energy Harvesting with Limited Energy Arrival Knowledge | http://arxiv.org/abs/1508.00285 | id:1508.00285 author:Zhenhua Zou, Anders Gidmark, Themistoklis Charalambous, Mikael Johansson category:cs.IT cs.LG math.IT  published:2015-08-02 summary:In this paper, we develop optimal policies for deciding when a wireless node with radio frequency (RF) energy harvesting (EH) capabilities should try and harvest ambient RF energy. While the idea of RF-EH is appealing, it is not always beneficial to attempt to harvest energy; in environments where the ambient energy is low, nodes could consume more energy being awake with their harvesting circuits turned on than what they can extract from the ambient radio signals; it is then better to enter a sleep mode until the ambient RF energy increases. Towards this end, we consider a scenario with intermittent energy arrivals and a wireless node that wakes up for a period of time (herein called the time-slot) and harvests energy. If enough energy is harvested during the time-slot, then the harvesting is successful and excess energy is stored; however, if there does not exist enough energy the harvesting is unsuccessful and energy is lost. We assume that the ambient energy level is constant during the time-slot, and changes at slot boundaries. The energy level dynamics are described by a two-state Gilbert-Elliott Markov chain model, where the state of the Markov chain can only be observed during the harvesting action, and not when in sleep mode. Two scenarios are studied under this model. In the first scenario, we assume that we have knowledge of the transition probabilities of the Markov chain and formulate the problem as a Partially Observable Markov Decision Process (POMDP), where we find a threshold-based optimal policy. In the second scenario, we assume that we don't have any knowledge about these parameters and formulate the problem as a Bayesian adaptive POMDP; to reduce the complexity of the computations we also propose a heuristic posterior sampling algorithm. The performance of our approaches is demonstrated via numerical examples. version:1
arxiv-1508-00282 | On Hyperspectral Classification in the Compressed Domain | http://arxiv.org/abs/1508.00282 | id:1508.00282 author:Mohammad Aghagolzadeh, Hayder Radha category:cs.CV  published:2015-08-02 summary:In this paper, we study the problem of hyperspectral pixel classification based on the recently proposed architectures for compressive whisk-broom hyperspectral imagers without the need to reconstruct the complete data cube. A clear advantage of classification in the compressed domain is its suitability for real-time on-site processing of the sensed data. Moreover, it is assumed that the training process also takes place in the compressed domain, thus, isolating the classification unit from the recovery unit at the receiver's side. We show that, perhaps surprisingly, using distinct measurement matrices for different pixels results in more accuracy of the learned classifier and consistent classification performance, supporting the role of information diversity in learning. version:1
arxiv-1403-4099 | High-speed detection of emergent market clustering via an unsupervised parallel genetic algorithm | http://arxiv.org/abs/1403.4099 | id:1403.4099 author:Dieter Hendricks, Diane Wilcox, Tim Gebbie category:q-fin.CP cs.DC cs.NE  published:2014-03-17 summary:We implement a master-slave parallel genetic algorithm (PGA) with a bespoke log-likelihood fitness function to identify emergent clusters within price evolutions. We use graphics processing units (GPUs) to implement a PGA and visualise the results using disjoint minimal spanning trees (MSTs). We demonstrate that our GPU PGA, implemented on a commercially available general purpose GPU, is able to recover stock clusters in sub-second speed, based on a subset of stocks in the South African market. This represents a pragmatic choice for low-cost, scalable parallel computing and is significantly faster than a prototype serial implementation in an optimised C-based fourth-generation programming language, although the results are not directly comparable due to compiler differences. Combined with fast online intraday correlation matrix estimation from high frequency data for cluster identification, the proposed implementation offers cost-effective, near-real-time risk assessment for financial practitioners. version:4
arxiv-1508-00278 | Dictionary and Image Recovery from Incomplete and Random Measurements | http://arxiv.org/abs/1508.00278 | id:1508.00278 author:Mohammad Aghagolzadeh, Hayder Radha category:cs.CV  published:2015-08-02 summary:This paper tackles algorithmic and theoretical aspects of dictionary learning from incomplete and random block-wise image measurements and the performance of the adaptive dictionary for sparse image recovery. This problem is related to blind compressed sensing in which the sparsifying dictionary or basis is viewed as an unknown variable and subject to estimation during sparse recovery. However, unlike existing guarantees for a successful blind compressed sensing, our results do not rely on additional structural constraints on the learned dictionary or the measured signal. In particular, we rely on the spatial diversity of compressive measurements to guarantee that the solution is unique with a high probability. Moreover, our distinguishing goal is to measure and reduce the estimation error with respect to the ideal dictionary that is based on the complete image. Using recent results from random matrix theory, we show that applying a slightly modified dictionary learning algorithm over compressive measurements results in accurate estimation of the ideal dictionary for large-scale images. Empirically, we experiment with both space-invariant and space-varying sensing matrices and demonstrate the critical role of spatial diversity in measurements. Simulation results confirm that the presented algorithm outperforms the typical non-adaptive sparse recovery based on offline-learned universal dictionaries. version:1
arxiv-1508-00239 | Partial matching face recognition method for rehabilitation nursing robots beds | http://arxiv.org/abs/1508.00239 | id:1508.00239 author:Dongmei Liang, Wushan Cheng category:cs.CV  published:2015-08-02 summary:In order to establish face recognition system in rehabilitation nursing robots beds and achieve real-time monitor the patient on the bed. We propose a face recognition method based on partial matching Hu moments which apply for rehabilitation nursing robots beds. Firstly we using Haar classifier to detect human faces automatically in dynamic video frames. Secondly we using Otsu threshold method to extract facial features (eyebrows, eyes, mouth) in the face image and its Hu moments. Finally, we using Hu moment feature set to achieve the automatic face recognition. Experimental results show that this method can efficiently identify face in a dynamic video and it has high practical value (the accuracy rate is 91% and the average recognition time is 4.3s). version:1
arxiv-1508-00230 | Toward a Robust Sparse Data Representation for Wireless Sensor Networks | http://arxiv.org/abs/1508.00230 | id:1508.00230 author:Mohammad Abu Alsheikh, Shaowei Lin, Hwee-Pink Tan, Dusit Niyato category:cs.NI cs.LG cs.NE  published:2015-08-02 summary:Compressive sensing has been successfully used for optimized operations in wireless sensor networks. However, raw data collected by sensors may be neither originally sparse nor easily transformed into a sparse data representation. This paper addresses the problem of transforming source data collected by sensor nodes into a sparse representation with a few nonzero elements. Our contributions that address three major issues include: 1) an effective method that extracts population sparsity of the data, 2) a sparsity ratio guarantee scheme, and 3) a customized learning algorithm of the sparsifying dictionary. We introduce an unsupervised neural network to extract an intrinsic sparse coding of the data. The sparse codes are generated at the activation of the hidden layer using a sparsity nomination constraint and a shrinking mechanism. Our analysis using real data samples shows that the proposed method outperforms conventional sparsity-inducing methods. version:1
arxiv-1508-00200 | PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks | http://arxiv.org/abs/1508.00200 | id:1508.00200 author:Jian Tang, Meng Qu, Qiaozhu Mei category:cs.CL cs.LG cs.NE I.2.6  published:2015-08-02 summary:Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the \textit{predictive text embedding} (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune. version:1
arxiv-1508-00189 | Class Vectors: Embedding representation of Document Classes | http://arxiv.org/abs/1508.00189 | id:1508.00189 author:Devendra Singh Sachan, Shailesh Kumar category:cs.CL cs.IR  published:2015-08-02 summary:Distributed representations of words and paragraphs as semantic embeddings in high dimensional data are used across a number of Natural Language Understanding tasks such as retrieval, translation, and classification. In this work, we propose "Class Vectors" - a framework for learning a vector per class in the same embedding space as the word and paragraph embeddings. Similarity between these class vectors and word vectors are used as features to classify a document to a class. In experiment on several sentiment analysis tasks such as Yelp reviews and Amazon electronic product reviews, class vectors have shown better or comparable results in classification while learning very meaningful class embeddings. version:1
arxiv-1508-00181 | An Analytic Framework for Maritime Situation Analysis | http://arxiv.org/abs/1508.00181 | id:1508.00181 author:Hamed Yaghoubi Shahir, Uwe Glässer, Amir Yaghoubi Shahir, Hans Wehn category:cs.LG  published:2015-08-02 summary:Maritime domain awareness is critical for protecting sea lanes, ports, harbors, offshore structures and critical infrastructures against common threats and illegal activities. Limited surveillance resources constrain maritime domain awareness and compromise full security coverage at all times. This situation calls for innovative intelligent systems for interactive situation analysis to assist marine authorities and security personal in their routine surveillance operations. In this article, we propose a novel situation analysis framework to analyze marine traffic data and differentiate various scenarios of vessel engagement for the purpose of detecting anomalies of interest for marine vessels that operate over some period of time in relative proximity to each other. The proposed framework views vessel behavior as probabilistic processes and uses machine learning to model common vessel interaction patterns. We represent patterns of interest as left-to-right Hidden Markov Models and classify such patterns using Support Vector Machines. version:1
arxiv-1502-05082 | What makes for effective detection proposals? | http://arxiv.org/abs/1502.05082 | id:1502.05082 author:Jan Hosang, Rodrigo Benenson, Piotr Dollár, Bernt Schiele category:cs.CV  published:2015-02-17 summary:Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL, ImageNet, and MS COCO, and their impact on DPM, R-CNN, and Fast R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detection performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods. version:3
arxiv-1508-00102 | Towards Distortion-Predictable Embedding of Neural Networks | http://arxiv.org/abs/1508.00102 | id:1508.00102 author:Axel Angel category:cs.CV  published:2015-08-01 summary:Current research in Computer Vision has shown that Convolutional Neural Networks (CNN) give state-of-the-art performance in many classification tasks and Computer Vision problems. The embedding of CNN, which is the internal representation produced by the last layer, can indirectly learn topological and relational properties. Moreover, by using a suitable loss function, CNN models can learn invariance to a wide range of non-linear distortions such as rotation, viewpoint angle or lighting condition. In this work, new insights are discovered about CNN embeddings and a new loss function is proposed, derived from the contrastive loss, that creates models with more predicable mappings and also quantifies distortions. In typical distortion-dependent methods, there is no simple relation between the features corresponding to one image and the features of this image distorted. Therefore, these methods require to feed-forward inputs under every distortions in order to find the corresponding features representations. Our contribution makes a step towards embeddings where features of distorted inputs are related and can be derived from each others by the intensity of the distortion. version:1
arxiv-1508-00097 | The Interactive Effects of Operators and Parameters to GA Performance Under Different Problem Sizes | http://arxiv.org/abs/1508.00097 | id:1508.00097 author:Jaderick P. Pabico, Elizer A. Albacea category:cs.NE  published:2015-08-01 summary:The complex effect of genetic algorithm's (GA) operators and parameters to its performance has been studied extensively by researchers in the past but none studied their interactive effects while the GA is under different problem sizes. In this paper, We present the use of experimental model (1)~to investigate whether the genetic operators and their parameters interact to affect the offline performance of GA, (2)~to find what combination of genetic operators and parameter settings will provide the optimum performance for GA, and (3)~to investigate whether these operator-parameter combination is dependent on the problem size. We designed a GA to optimize a family of traveling salesman problems (TSP), with their optimal solutions known for convenient benchmarking. Our GA was set to use different algorithms in simulating selection ($\Omega_s$), different algorithms ($\Omega_c$) and parameters ($p_c$) in simulating crossover, and different parameters ($p_m$) in simulating mutation. We used several $n$-city TSPs ($n=\{5, 7, 10, 100, 1000\}$) to represent the different problem sizes (i.e., size of the resulting search space as represented by GA schemata). Using analysis of variance of 3-factor factorial experiments, we found out that GA performance is affected by $\Omega_s$ at small problem size (5-city TSP) where the algorithm Partially Matched Crossover significantly outperforms Cycle Crossover at $95\%$ confidence level. version:1
arxiv-1508-00092 | Land Use Classification in Remote Sensing Images by Convolutional Neural Networks | http://arxiv.org/abs/1508.00092 | id:1508.00092 author:Marco Castelluccio, Giovanni Poggi, Carlo Sansone, Luisa Verdoliva category:cs.CV  published:2015-08-01 summary:We explore the use of convolutional neural networks for the semantic classification of remote sensing scenes. Two recently proposed architectures, CaffeNet and GoogLeNet, are adopted, with three different learning modalities. Besides conventional training from scratch, we resort to pre-trained networks that are only fine-tuned on the target data, so as to avoid overfitting problems and reduce design time. Experiments on two remote sensing datasets, with markedly different characteristics, testify on the effectiveness and wide applicability of the proposed solution, which guarantees a significant performance improvement over all state-of-the-art references. version:1
arxiv-1508-00088 | Turnover Prediction Of Shares using Data Mining techniques : A Case Study | http://arxiv.org/abs/1508.00088 | id:1508.00088 author:D. S. Shashaank, V. Sruthi, M. L. S Vijayalakshimi, Jacob Shomona Garcia category:cs.LG  published:2015-08-01 summary:Predicting the turnover of a company in the ever fluctuating Stock market has always proved to be a precarious situation and most certainly a difficult task in hand. Data mining is a well-known sphere of Computer Science that aims on extracting meaningful information from large databases. However, despite the existence of many algorithms for the purpose of predicting the future trends, their efficiency is questionable as their predictions suffer from a high error rate. The objective of this paper is to investigate various classification algorithms to predict the turnover of different companies based on the Stock price. The authorized dataset for predicting the turnover was taken from www.bsc.com and included the stock market values of various companies over the past 10 years. The algorithms were investigated using the "R" tool. The feature selection algorithm, Boruta, was run on this dataset to extract the important and influential features for classification. With these extracted features, the Total Turnover of the company was predicted using various classification algorithms like Random Forest, Decision Tree, SVM and Multinomial Regression. This prediction mechanism was implemented to predict the turnover of a company on an everyday basis and hence could help navigate through dubious stock market trades. An accuracy rate of 95% was achieved by the above prediction process. Moreover, the importance of stock market attributes was established as well. version:1
arxiv-1508-00085 | Regularized Multi-Task Learning for Multi-Dimensional Log-Density Gradient Estimation | http://arxiv.org/abs/1508.00085 | id:1508.00085 author:Ikko Yamane, Hiroaki Sasaki, Masashi Sugiyama category:stat.ML  published:2015-08-01 summary:Log-density gradient estimation is a fundamental statistical problem and possesses various practical applications such as clustering and measuring non-Gaussianity. A naive two-step approach of first estimating the density and then taking its log-gradient is unreliable because an accurate density estimate does not necessarily lead to an accurate log-density gradient estimate. To cope with this problem, a method to directly estimate the log-density gradient without density estimation has been explored, and demonstrated to work much better than the two-step method. The objective of this paper is to further improve the performance of this direct method in multi-dimensional cases. Our idea is to regard the problem of log-density gradient estimation in each dimension as a task, and apply regularized multi-task learning to the direct log-density gradient estimator. We experimentally demonstrate the usefulness of the proposed multi-task method in log-density gradient estimation and mode-seeking clustering. version:1
arxiv-1504-00948 | The Child is Father of the Man: Foresee the Success at the Early Stage | http://arxiv.org/abs/1504.00948 | id:1504.00948 author:Liangyue Li, Hanghang Tong category:cs.LG  published:2015-04-03 summary:Understanding the dynamic mechanisms that drive the high-impact scientific work (e.g., research papers, patents) is a long-debated research topic and has many important implications, ranging from personal career development and recruitment search, to the jurisdiction of research resources. Recent advances in characterizing and modeling scientific success have made it possible to forecast the long-term impact of scientific work, where data mining techniques, supervised learning in particular, play an essential role. Despite much progress, several key algorithmic challenges in relation to predicting long-term scientific impact have largely remained open. In this paper, we propose a joint predictive model to forecast the long-term scientific impact at the early stage, which simultaneously addresses a number of these open challenges, including the scholarly feature design, the non-linearity, the domain-heterogeneity and dynamics. In particular, we formulate it as a regularized optimization problem and propose effective and scalable algorithms to solve it. We perform extensive empirical evaluations on large, real scholarly data sets to validate the effectiveness and the efficiency of our method. version:3
arxiv-1303-1208 | Classification with Asymmetric Label Noise: Consistency and Maximal Denoising | http://arxiv.org/abs/1303.1208 | id:1303.1208 author:Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, Clayton Scott category:stat.ML cs.LG  published:2013-03-05 summary:In many real-world classification problems, the labels of training examples are randomly corrupted. Most previous theoretical work on classification with label noise assumes that the two classes are separable, that the label noise is independent of the true class label, or that the noise proportions for each class are known. In this work, we give conditions that are necessary and sufficient for the true class-conditional distributions to be identifiable. These conditions are weaker than those analyzed previously, and allow for the classes to be nonseparable and the noise levels to be asymmetric and unknown. The conditions essentially state that a majority of the observed labels are correct and that the true class-conditional distributions are "mutually irreducible," a concept we introduce that limits the similarity of the two distributions. For any label noise problem, there is a unique pair of true class-conditional distributions satisfying the proposed conditions, and we argue that this pair corresponds in a certain sense to maximal denoising of the observed distributions. Our results are facilitated by a connection to "mixture proportion estimation," which is the problem of estimating the maximal proportion of one distribution that is present in another. We establish a novel rate of convergence result for mixture proportion estimation, and apply this to obtain consistency of a discrimination rule based on surrogate loss minimization. Experimental results on benchmark data and a nuclear particle classification problem demonstrate the efficacy of our approach. version:2
arxiv-1507-05738 | Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos | http://arxiv.org/abs/1507.05738 | id:1507.05738 author:Serena Yeung, Olga Russakovsky, Ning Jin, Mykhaylo Andriluka, Greg Mori, Li Fei-Fei category:cs.CV  published:2015-07-21 summary:Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory (LSTM) deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction. version:2
arxiv-1305-2545 | Bandits with Knapsacks | http://arxiv.org/abs/1305.2545 | id:1305.2545 author:Ashwinkumar Badanidiyuru, Robert Kleinberg, Aleksandrs Slivkins category:cs.DS cs.LG  published:2013-05-11 summary:Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called "bandits with knapsacks", that combines aspects of stochastic integer programming with online learning. A distinctive feature of our problem, in comparison to the existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sublinear regret in the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems. We present two algorithms whose reward is close to the information-theoretic optimum: one is based on a novel "balanced exploration" paradigm, while the other is a primal-dual algorithm that uses multiplicative updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors. We illustrate the generality of the problem by presenting applications in a number of different domains including electronic commerce, routing, and scheduling. As one example of a concrete application, we consider the problem of dynamic posted pricing with limited supply and obtain the first algorithm whose regret, with respect to the optimal dynamic policy, is sublinear in the supply. version:6
arxiv-1402-6779 | Resourceful Contextual Bandits | http://arxiv.org/abs/1402.6779 | id:1402.6779 author:Ashwinkumar Badanidiyuru, John Langford, Aleksandrs Slivkins category:cs.LG cs.DS cs.GT  published:2014-02-27 summary:We study contextual bandits with ancillary constraints on resources, which are common in real-world applications such as choosing ads or dynamic pricing of items. We design the first algorithm for solving these problems that handles constrained resources other than time, and improves over a trivial reduction to the non-contextual case. We consider very general settings for both contextual bandits (arbitrary policy sets, e.g. Dudik et al. (UAI'11)) and bandits with resource constraints (bandits with knapsacks, Badanidiyuru et al. (FOCS'13)), and prove a regret guarantee with near-optimal statistical properties. version:6
arxiv-1507-08958 | SnowWatch: Snow Monitoring through Acquisition and Analysis of User-Generated Content | http://arxiv.org/abs/1507.08958 | id:1507.08958 author:Roman Fedorov, Piero Fraternali, Chiara Pasini, Marco Tagliasacchi category:cs.CV cs.CY  published:2015-07-31 summary:We present a system for complementing snow phenomena monitoring with virtual measurements extracted from public visual content. The proposed system integrates an automatic acquisition and analysis of photographs and webcam images depicting Alpine mountains. In particular, the technical demonstration consists in a web portal that interfaces the whole system with the population. It acts as an entertaining photo-sharing social web site, acquiring at the same time visual content necessary for environmental monitoring. version:1
arxiv-1507-08937 | Efficient and robust calibration of the Heston option pricing model for American options using an improved Cuckoo Search Algorithm | http://arxiv.org/abs/1507.08937 | id:1507.08937 author:Stefan Haring, Ronald Hochreiter category:cs.NE q-fin.PR  published:2015-07-31 summary:In this paper an improved Cuckoo Search Algorithm is developed to allow for an efficient and robust calibration of the Heston option pricing model for American options. Calibration of stochastic volatility models like the Heston is significantly harder than classical option pricing models as more parameters have to be estimated. The difficult task of calibrating one of these models to American Put options data is the main objective of this paper. Numerical results are shown to substantiate the suitability of the chosen method to tackle this problem. version:1
arxiv-1508-00504 | Spin Glass Models of Syntax and Language Evolution | http://arxiv.org/abs/1508.00504 | id:1508.00504 author:Karthik Siva, Jim Tao, Matilde Marcolli category:cs.CL cond-mat.dis-nn physics.soc-ph 91F20  82B20  published:2015-07-31 summary:Using the SSWL database of syntactic parameters of world languages, and the MIT Media Lab data on language interactions, we construct a spin glass model of language evolution. We treat binary syntactic parameters as spin states, with languages as vertices of a graph, and assigned interaction energies along the edges. We study a rough model of syntax evolution, under the assumption that a strong interaction energy tends to cause parameters to align, as in the case of ferromagnetic materials. We also study how the spin glass model needs to be modified to account for entailment relations between syntactic parameters. This modification leads naturally to a generalization of Potts models with external magnetic field, which consists of a coupling at the vertices of an Ising model and a Potts model with q=3, that have the same edge interactions. We describe the results of simulations of the dynamics of these models, in different temperature and energy regimes. We discuss the linguistic interpretation of the parameters of the physical model. version:1
arxiv-1507-08861 | Mobile Multi-View Object Image Search | http://arxiv.org/abs/1507.08861 | id:1507.08861 author:Fatih Çalışır, Özgür Ulusoy, Uğur Güdükbay, Muhammet Baştan category:cs.MM cs.CV  published:2015-07-31 summary:High user interaction capability of mobile devices can help improve the accuracy of mobile visual search systems. At query time, it is possible to capture multiple views of an object from different viewing angles and at different scales with the mobile device camera to obtain richer information about the object compared to a single view and hence return more accurate results. Motivated by this, we developed a mobile multi-view object image search system, using a client-server architecture. Multi-view images of objects acquired by the mobile clients are processed and local features are sent to the server, which combines the query image representations with early/late fusion methods based on bag-of-visual-words and sends back the query results. We performed a comprehensive analysis of early and late fusion approaches using various similarity functions, on an existing single view and a new multi-view object image database. The experimental results show that multi-view search provides significantly better retrieval accuracy compared to single view search. version:1
arxiv-1507-08847 | A novel multivariate performance optimization method based on sparse coding and hyper-predictor learning | http://arxiv.org/abs/1507.08847 | id:1507.08847 author:Jiachen Yanga, Zhiyong Dinga, Fei Guoa, Huogen Wanga, Nick Hughesb category:cs.LG cs.CV cs.NA  published:2015-07-31 summary:In this paper, we investigate the problem of optimization multivariate performance measures, and propose a novel algorithm for it. Different from traditional machine learning methods which optimize simple loss functions to learn prediction function, the problem studied in this paper is how to learn effective hyper-predictor for a tuple of data points, so that a complex loss function corresponding to a multivariate performance measure can be minimized. We propose to present the tuple of data points to a tuple of sparse codes via a dictionary, and then apply a linear function to compare a sparse code against a give candidate class label. To learn the dictionary, sparse codes, and parameter of the linear function, we propose a joint optimization problem. In this problem, the both the reconstruction error and sparsity of sparse code, and the upper bound of the complex loss function are minimized. Moreover, the upper bound of the loss function is approximated by the sparse codes and the linear function parameter. To optimize this problem, we develop an iterative algorithm based on descent gradient methods to learn the sparse codes and hyper-predictor parameter alternately. Experiment results on some benchmark data sets show the advantage of the proposed methods over other state-of-the-art algorithms. version:1
arxiv-1501-00092 | Image Super-Resolution Using Deep Convolutional Networks | http://arxiv.org/abs/1501.00092 | id:1501.00092 author:Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang category:cs.CV cs.NE I.4.5; I.2.6  published:2014-12-31 summary:We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality. version:3
arxiv-1507-08788 | Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity | http://arxiv.org/abs/1507.08788 | id:1507.08788 author:Ohad Shamir category:cs.LG cs.NA math.NA math.OC stat.ML  published:2015-07-31 summary:We study the convergence properties of the VR-PCA algorithm introduced by \cite{shamir2015stochastic} for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the runtime of stochastic methods, and what are the convexity and non-convexity properties of the underlying optimization problem. version:1
arxiv-1502-01097 | Dense v.s. Sparse: A Comparative Study of Sampling Analysis in Scene Classification of High-Resolution Remote Sensing Imagery | http://arxiv.org/abs/1502.01097 | id:1502.01097 author:Jingwen Hu, Gui-Song Xia, Fan Hu, Liangpei Zhang category:cs.CV  published:2015-02-04 summary:Scene classification is a key problem in the interpretation of high-resolution remote sensing imagery. Many state-of-the-art methods, e.g. bag-of-visual-words model and its variants, the topic models as well as deep learning-based approaches, share similar procedures: patch sampling, feature description/learning and classification. Patch sampling is the first and a key procedure which has a great influence on the results. In the literature, many different sampling strategies have been used, {e.g. dense sampling, random sampling, keypoint-based sampling and saliency-based sampling, etc. However, it is still not clear which sampling strategy is suitable for the scene classification of high-resolution remote sensing images. In this paper, we comparatively study the effects of different sampling strategies under the scenario of scene classification of high-resolution remote sensing images. We divide the existing sampling methods into two types: dense sampling and sparse sampling, the later of which includes random sampling, keypoint-based sampling and various saliency-based sampling proposed recently. In order to compare their performances, we rely on a standard bag-of-visual-words model to construct our testing scheme, owing to their simplicity, robustness and efficiency. The experimental results on two commonly used datasets show that dense sampling has the best performance among all the strategies but with high spatial and computational complexity, random sampling gives better or comparable results than other sparse sampling methods, like the sophisticated multi-scale key-point operators and the saliency-based methods which are intensively studied and commonly used recently. version:2
arxiv-1507-08761 | Multimodal Multipart Learning for Action Recognition in Depth Videos | http://arxiv.org/abs/1507.08761 | id:1507.08761 author:Amir Shahroudy, Gang Wang, Tian-Tsong Ng, Qingxiong Yang category:cs.CV  published:2015-07-31 summary:The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy. version:1
arxiv-1507-08754 | Flip-Rotate-Pooling Convolution and Split Dropout on Convolution Neural Networks for Image Classification | http://arxiv.org/abs/1507.08754 | id:1507.08754 author:Fa Wu, Peijun Hu, Dexing Kong category:cs.CV  published:2015-07-31 summary:This paper presents a new version of Dropout called Split Dropout (sDropout) and rotational convolution techniques to improve CNNs' performance on image classification. The widely used standard Dropout has advantage of preventing deep neural networks from overfitting by randomly dropping units during training. Our sDropout randomly splits the data into two subsets and keeps both rather than discards one subset. We also introduce two rotational convolution techniques, i.e. rotate-pooling convolution (RPC) and flip-rotate-pooling convolution (FRPC) to boost CNNs' performance on the robustness for rotation transformation. These two techniques encode rotation invariance into the network without adding extra parameters. Experimental evaluations on ImageNet2012 classification task demonstrate that sDropout not only enhances the performance but also converges faster. Additionally, RPC and FRPC make CNNs more robust for rotation transformations. Overall, FRPC together with sDropout bring $1.18\%$ (model of Zeiler and Fergus~\cite{zeiler2013visualizing}, 10-view, top-1) accuracy increase in ImageNet 2012 classification task compared to the original network. version:1
arxiv-1507-08752 | An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback | http://arxiv.org/abs/1507.08752 | id:1507.08752 author:Ohad Shamir category:cs.LG math.OC stat.ML  published:2015-07-31 summary:We consider the closely related problems of bandit convex optimization with two-point feedback, and zero-order stochastic convex optimization with two function evaluations per round. We provide a simple algorithm and analysis which is optimal for convex Lipschitz functions. This improves on \cite{dujww13}, which only provides an optimal result for smooth functions; Moreover, the algorithm and analysis are simpler, and readily extend to non-Euclidean problems. The algorithm is based on a small but surprisingly powerful modification of the gradient estimator. version:1
arxiv-1409-2848 | A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate | http://arxiv.org/abs/1409.2848 | id:1409.2848 author:Ohad Shamir category:cs.LG cs.NA math.OC stat.ML  published:2014-09-09 summary:We describe and analyze a simple algorithm for principal component analysis and singular value decomposition, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to an inherently non-convex problem, using a very different analysis. version:5
arxiv-1507-08736 | A Sinc Wavelet Describes the Receptive Fields of Neurons in the Motion Cortex | http://arxiv.org/abs/1507.08736 | id:1507.08736 author:Stephen G. Odaibo category:q-bio.NC cs.CV cs.IT math.IT physics.bio-ph  published:2015-07-31 summary:Visual perception results from a systematic transformation of the information flowing through the visual system. In the neuronal hierarchy, the response properties of single neurons are determined by neurons located one level below, and in turn, determine the responses of neurons located one level above. Therefore in modeling receptive fields, it is essential to ensure that the response properties of neurons in a given level can be generated by combining the response models of neurons in its input levels. However, existing response models of neurons in the motion cortex do not inherently yield the temporal frequency filtering gradient (TFFG) property that is known to emerge along the primary visual cortex (V1) to middle temporal (MT) motion processing stream. TFFG is the change from predominantly lowpass to predominantly bandpass temporal frequency filtering character along the V1 to MT pathway (Foster et al 1985; DeAngelis et al 1993; Hawken et al 1996). We devised a new model, the sinc wavelet model (Odaibo, 2014), which logically and efficiently generates the TFFG. The model replaces the Gabor function's sine wave carrier with a sinc (sin(x)/x) function, and has the same or fewer number of parameters as existing models. Because of its logical consistency with the emergent network property of TFFG, we conclude that the sinc wavelet is a better model for the receptive fields of motion cortex neurons. This model will provide new physiological insights into how the brain represents visual information. version:1
arxiv-1507-08711 | Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs | http://arxiv.org/abs/1507.08711 | id:1507.08711 author:Mehrtash Harandi, Mathieu Salzmann, Mahsa Baktashmotlagh category:cs.CV  published:2015-07-31 summary:State-of-the-art image-set matching techniques typically implicitly model each image-set with a Gaussian distribution. Here, we propose to go beyond these representations and model image-sets as probability distribution functions (PDFs) using kernel density estimators. To compare and match image-sets, we exploit Csiszar f-divergences, which bear strong connections to the geodesic distance defined on the space of PDFs, i.e., the statistical manifold. Furthermore, we introduce valid positive definite kernels on the statistical manifolds, which let us make use of more powerful classification schemes to match image-sets. Finally, we introduce a supervised dimensionality reduction technique that learns a latent space where f-divergences reflect the class labels of the data. Our experiments on diverse problems, such as video-based face recognition and dynamic texture classification, evidence the benefits of our approach over the state-of-the-art image-set matching methods. version:1
arxiv-1502-05680 | Finding One Community in a Sparse Graph | http://arxiv.org/abs/1502.05680 | id:1502.05680 author:Andrea Montanari category:stat.ML cond-mat.stat-mech cs.SI  published:2015-02-19 summary:We consider a random sparse graph with bounded average degree, in which a subset of vertices has higher connectivity than the background. In particular, the average degree inside this subset of vertices is larger than outside (but still bounded). Given a realization of such graph, we aim at identifying the hidden subset of vertices. This can be regarded as a model for the problem of finding a tightly knitted community in a social network, or a cluster in a relational dataset. In this paper we present two sets of contributions: $(i)$ We use the cavity method from spin glass theory to derive an exact phase diagram for the reconstruction problem. In particular, as the difference in edge probability increases, the problem undergoes two phase transitions, a static phase transition and a dynamic one. $(ii)$ We establish rigorous bounds on the dynamic phase transition and prove that, above a certain threshold, a local algorithm (belief propagation) correctly identify most of the hidden set. Below the same threshold \emph{no local algorithm} can achieve this goal. However, in this regime the subset can be identified by exhaustive search. For small hidden sets and large average degree, the phase transition for local algorithms takes an intriguingly simple form. Local algorithms succeed with high probability for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} > \sqrt{{\rm deg}_{\rm out}/e}$ and fail for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} < \sqrt{{\rm deg}_{\rm out}/e}$ (with ${\rm deg}_{\rm in}$, ${\rm deg}_{\rm out}$ the average degrees inside and outside the community). We argue that spectral algorithms are also ineffective in the latter regime. It is an open problem whether any polynomial time algorithms might succeed for ${\rm deg}_{\rm in} - {\rm deg}_{\rm out} < \sqrt{{\rm deg}_{\rm out}/e}$. version:2
arxiv-1409-3821 | Computational Implications of Reducing Data to Sufficient Statistics | http://arxiv.org/abs/1409.3821 | id:1409.3821 author:Andrea Montanari category:stat.CO cs.IT cs.LG math.IT  published:2014-09-12 summary:Given a large dataset and an estimation task, it is common to pre-process the data by reducing them to a set of sufficient statistics. This step is often regarded as straightforward and advantageous (in that it simplifies statistical analysis). I show that -on the contrary- reducing data to sufficient statistics can change a computationally tractable estimation problem into an intractable one. I discuss connections with recent work in theoretical computer science, and implications for some techniques to estimate graphical models. version:3
arxiv-1507-08577 | Orthogonal parallel MCMC methods for sampling and optimization | http://arxiv.org/abs/1507.08577 | id:1507.08577 author:L. Martino, V. Elvira, D. Luengo, J. Corander, F. Louzada category:stat.CO stat.ML  published:2015-07-30 summary:Monte Carlo (MC) methods are widely used in statistics, signal processing and machine learning. A well-known class of MC methods are Markov Chain Monte Carlo (MCMC) algorithms. In order to foster better exploration of the state space, specially in high-dimensional applications, several schemes employing multiple parallel MCMC chains have been recently introduced. In this work, we describe a novel parallel interacting MCMC scheme, called orthogonal MCMC (O-MCMC), where a set of vertical parallel MCMC chains share information using some horizontal MCMC techniques working on the entire population of current states. More specifically, the vertical chains are led by random-walk proposals, whereas the horizontal MCMC techniques employ independent proposals, thus allowing an efficient combination of global exploration and local approximation. The interaction is contained in these horizontal iterations. Within the analysis of different implementations of O-MCMC, novel schemes for reducing the overall computational cost of parallel multiple try Metropolis (MTM) chains are also presented. Furthermore, a modified version of O-MCMC for optimization is provided by considering parallel simulated annealing (SA) algorithms. Finally, we also discuss the application of O-MCMC in a big bata framework. Numerical results show the advantages of the proposed sampling scheme in terms of efficiency in the estimation, as well as robustness in terms of independence with respect to initial values and the choice of the parameters. version:1
arxiv-1507-08539 | Multilayer Network of Language: a Unified Framework for Structural Analysis of Linguistic Subsystems | http://arxiv.org/abs/1507.08539 | id:1507.08539 author:Domagoj Margan, Ana Meštrović, Sanda Martinčić-Ipšić category:cs.CL  published:2015-07-30 summary:Recently, the focus of complex networks research has shifted from the analysis of isolated properties of a system toward a more realistic modeling of multiple phenomena - multilayer networks. Motivated by the prosperity of multilayer approach in social, transport or trade systems, we propose the introduction of multilayer networks for language. The multilayer network of language is a unified framework for modeling linguistic subsystems and their structural properties enabling the exploration of their mutual interactions. Various aspects of natural language systems can be represented as complex networks, whose vertices depict linguistic units, while links model their relations. The multilayer network of language is defined by three aspects: the network construction principle, the linguistic subsystem and the language of interest. More precisely, we construct a word-level (syntax, co-occurrence and its shuffled counterpart) and a subword level (syllables and graphemes) network layers, from five variations of original text (in the modeled language). The obtained results suggest that there are substantial differences between the networks structures of different language subsystems, which are hidden during the exploration of an isolated layer. The word-level layers share structural properties regardless of the language (e.g. Croatian or English), while the syllabic subword level expresses more language dependent structural properties. The preserved weighted overlap quantifies the similarity of word-level layers in weighted and directed networks. Moreover, the analysis of motifs reveals a close topological structure of the syntactic and syllabic layers for both languages. The findings corroborate that the multilayer network framework is a powerful, consistent and systematic approach to model several linguistic subsystems simultaneously and hence to provide a more unified view on language. version:1
arxiv-1507-08482 | Framework for learning agents in quantum environments | http://arxiv.org/abs/1507.08482 | id:1507.08482 author:Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel category:quant-ph cs.AI cs.LG  published:2015-07-30 summary:In this paper we provide a broad framework for describing learning agents in general quantum environments. We analyze the types of classically specified environments which allow for quantum enhancements in learning, by contrasting environments to quantum oracles. We show that whether or not quantum improvements are at all possible depends on the internal structure of the quantum environment. If the environments are constructed and the internal structure is appropriately chosen, or if the agent has limited capacities to influence the internal states of the environment, we show that improvements in learning times are possible in a broad range of scenarios. Such scenarios we call luck-favoring settings. The case of constructed environments is particularly relevant for the class of model-based learning agents, where our results imply a near-generic improvement. version:1
arxiv-1507-08452 | Unsupervised Sentence Simplification Using Deep Semantics | http://arxiv.org/abs/1507.08452 | id:1507.08452 author:Shashi Narayan, Claire Gardent category:cs.CL  published:2015-07-30 summary:We present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. Second, sentence splitting operates on deep semantic structure. We show (i) that the unsupervised framework we propose is competitive with four state-of-the-art supervised systems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting. version:1
arxiv-1507-08445 | People Counting in High Density Crowds from Still Images | http://arxiv.org/abs/1507.08445 | id:1507.08445 author:Ankan Bansal, K. S. Venkatesh category:cs.CV  published:2015-07-30 summary:We present a method of estimating the number of people in high density crowds from still images. The method estimates counts by fusing information from multiple sources. Most of the existing work on crowd counting deals with very small crowds (tens of individuals) and use temporal information from videos. Our method uses only still images to estimate the counts in high density images (hundreds to thousands of individuals). At this scale, we cannot rely on only one set of features for count estimation. We, therefore, use multiple sources, viz. interest points (SIFT), Fourier analysis, wavelet decomposition, GLCM features and low confidence head detections, to estimate the counts. Each of these sources gives a separate estimate of the count along with confidences and other statistical measures which are then combined to obtain the final estimate. We test our method on an existing dataset of fifty images containing over 64000 individuals. Further, we added another fifty annotated images of crowds and tested on the complete dataset of hundred images containing over 87000 individuals. The counts per image range from 81 to 4633. We report the performance in terms of mean absolute error, which is a measure of accuracy of the method, and mean normalised absolute error, which is a measure of the robustness. version:1
arxiv-1507-08429 | Multilinear Map Layer: Prediction Regularization by Structural Constraint | http://arxiv.org/abs/1507.08429 | id:1507.08429 author:Shuchang Zhou, Yuxin Wu category:cs.CV  published:2015-07-30 summary:In this paper we propose and study a technique to impose structural constraints on the output of a neural network, which can reduce amount of computation and number of parameters besides improving prediction accuracy when the output is known to approximately conform to the low-rankness prior. The technique proceeds by replacing the output layer of neural network with the so-called MLM layers, which forces the output to be the result of some Multilinear Map, like a hybrid-Kronecker-dot product or Kronecker Tensor Product. In particular, given an "autoencoder" model trained on SVHN dataset, we can construct a new model with MLM layer achieving 62\% reduction in total number of parameters and reduction of $\ell_2$ reconstruction error from 0.088 to 0.004. Further experiments on other autoencoder model variants trained on SVHN datasets also demonstrate the efficacy of MLM layers. version:1
arxiv-1507-08396 | Tag-Weighted Topic Model For Large-scale Semi-Structured Documents | http://arxiv.org/abs/1507.08396 | id:1507.08396 author:Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, Rong Pan category:cs.CL cs.IR cs.LG stat.ML  published:2015-07-30 summary:To date, there have been massive Semi-Structured Documents (SSDs) during the evolution of the Internet. These SSDs contain both unstructured features (e.g., plain text) and metadata (e.g., tags). Most previous works focused on modeling the unstructured text, and recently, some other methods have been proposed to model the unstructured text with specific tags. To build a general model for SSDs remains an important problem in terms of both model fitness and efficiency. We propose a novel method to model the SSDs by a so-called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining tasks. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. Meanwhile, we propose three large-scale solutions for our model under the MapReduce distributed computing platform for modeling large-scale SSDs. The experimental results show the effectiveness, efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling, tags prediction and text classification. We also show the performance of the three distributed solutions in terms of time and accuracy on document modeling. version:1
arxiv-1507-08379 | VMF-SNE: Embedding for Spherical Data | http://arxiv.org/abs/1507.08379 | id:1507.08379 author:Mian Wang, Dong Wang category:cs.LG  published:2015-07-30 summary:T-SNE is a well-known approach to embedding high-dimensional data and has been widely used in data visualization. The basic assumption of t-SNE is that the data are non-constrained in the Euclidean space and the local proximity can be modelled by Gaussian distributions. This assumption does not hold for a wide range of data types in practical applications, for instance spherical data for which the local proximity is better modelled by the von Mises-Fisher (vMF) distribution instead of the Gaussian. This paper presents a vMF-SNE embedding algorithm to embed spherical data. An iterative process is derived to produce an efficient embedding. The results on a simulation data set demonstrated that vMF-SNE produces better embeddings than t-SNE for spherical data. version:1
arxiv-1507-08373 | When VLAD met Hilbert | http://arxiv.org/abs/1507.08373 | id:1507.08373 author:Mehrtash Harandi, Mathieu Salzmann, Fatih Porikli category:cs.CV  published:2015-07-30 summary:Vectors of Locally Aggregated Descriptors (VLAD) have emerged as powerful image/video representations that compete with or even outperform state-of-the-art approaches on many challenging visual recognition tasks. In this paper, we address two fundamental limitations of VLAD: its requirement for the local descriptors to have vector form and its restriction to linear classifiers due to its high-dimensionality. To this end, we introduce a kernelized version of VLAD. This not only lets us inherently exploit more sophisticated classification schemes, but also enables us to efficiently aggregate non-vector descriptors (e.g., tensors) in the VLAD framework. Furthermore, we propose three approximate formulations that allow us to accelerate the coding process while still benefiting from the properties of kernel VLAD. Our experiments demonstrate the effectiveness of our approach at handling manifold-valued data, such as covariance descriptors, on several classification tasks. Our results also evidence the benefits of our nonlinear VLAD descriptors against the linear ones in Euclidean space using several standard benchmark datasets. version:1
arxiv-1508-03530 | Information-theoretical analysis of the statistical dependencies among three variables: Applications to written language | http://arxiv.org/abs/1508.03530 | id:1508.03530 author:Damián G. Hernández, Damián H. Zanette, Inés Samengo category:cs.CL physics.data-an physics.soc-ph  published:2015-07-30 summary:We develop the information-theoretical concepts required to study the statistical dependencies among three variables. Some of such dependencies are pure triple interactions, in the sense that they cannot be explained in terms of a combination of pairwise correlations. We derive bounds for triple dependencies, and characterize the shape of the joint probability distribution of three binary variables with high triple interaction. The analysis also allows us to quantify the amount of redundancy in the mutual information between pairs of variables, and to assess whether the information between two variables is or is not mediated by a third variable. These concepts are applied to the analysis of written texts. We find that the probability that a given word is found in a particular location within the text is not only modulated by the presence or absence of other nearby words, but also, on the presence or absence of nearby pairs of words. We identify the words enclosing the key semantic concepts of the text, the triplets of words with high pairwise and triple interactions, and the words that mediate the pairwise interactions between other words. version:1
arxiv-1507-08363 | Action recognition in still images by latent superpixel classification | http://arxiv.org/abs/1507.08363 | id:1507.08363 author:Shaukat Abidi, Massimo Piccardi, Mary-Anne Williams category:cs.CV  published:2015-07-30 summary:Action recognition from still images is an important task of computer vision applications such as image annotation, robotic navigation, video surveillance and several others. Existing approaches mainly rely on either bag-of-feature representations or articulated body-part models. However, the relationship between the action and the image segments is still substantially unexplored. For this reason, in this paper we propose to approach action recognition by leveraging an intermediate layer of "superpixels" whose latent classes can act as attributes of the action. In the proposed approach, the action class is predicted by a structural model(learnt by Latent Structural SVM) based on measurements from the image superpixels and their latent classes. Experimental results over the challenging Stanford 40 Actions dataset report a significant average accuracy of 74.06% for the positive class and 88.50% for the negative class, giving evidence to the performance of the proposed approach. version:1
arxiv-1507-08322 | Distributed Mini-Batch SDCA | http://arxiv.org/abs/1507.08322 | id:1507.08322 author:Martin Takáč, Peter Richtárik, Nathan Srebro category:cs.LG math.OC  published:2015-07-29 summary:We present an improved analysis of mini-batched stochastic dual coordinate ascent for regularized empirical loss minimization (i.e. SVM and SVM-type objectives). Our analysis allows for flexible sampling schemes, including where data is distribute across machines, and combines a dependence on the smoothness of the loss and/or the data spread (measured through the spectral norm). version:1
arxiv-1507-08286 | Deep Learning for Single-View Instance Recognition | http://arxiv.org/abs/1507.08286 | id:1507.08286 author:David Held, Sebastian Thrun, Silvio Savarese category:cs.CV cs.LG cs.NE cs.RO  published:2015-07-29 summary:Deep learning methods have typically been trained on large datasets in which many training examples are available. However, many real-world product datasets have only a small number of images available for each product. We explore the use of deep learning methods for recognizing object instances when we have only a single training example per class. We show that feedforward neural networks outperform state-of-the-art methods for recognizing objects from novel viewpoints even when trained from just a single image per object. To further improve our performance on this task, we propose to take advantage of a supplementary dataset in which we observe a separate set of objects from multiple viewpoints. We introduce a new approach for training deep learning methods for instance recognition with limited training data, in which we use an auxiliary multi-view dataset to train our network to be robust to viewpoint changes. We find that this approach leads to a more robust classifier for recognizing objects from novel viewpoints, outperforming previous state-of-the-art approaches including keypoint-matching, template-based techniques, and sparse coding. version:1
arxiv-1507-08272 | Context-aware learning for finite mixture models | http://arxiv.org/abs/1507.08272 | id:1507.08272 author:Serafeim Perdikis, Robert Leeb, Ricardo Chavarriaga, José del R. Millán category:stat.ML  published:2015-07-29 summary:This work introduces algorithms able to exploit contextual information in order to improve maximum-likelihood (ML) parameter estimation in finite mixture models (FMM), demonstrating their benefits and properties in several scenarios. The proposed algorithms are derived in a probabilistic framework with regard to situations where the regular FMM graphs can be extended with context-related variables, respecting the standard expectation-maximization (EM) methodology and, thus, rendering explicit supervision completely redundant. We show that, by direct application of the missing information principle, the compared algorithms' learning behaviour operates between the extremities of supervised and unsupervised learning, proportionally to the information content of contextual assistance. Our simulation results demonstrate the superiority of context-aware FMM training as compared to conventional unsupervised training in terms of estimation precision, standard errors, convergence rates and classification accuracy or regression fitness in various scenarios, while also highlighting important differences among the outlined situations. Finally, the improved classification outcome of contextually enhanced FMMs is showcased in a brain-computer interface application scenario. version:1
arxiv-1507-07830 | Zero-Shot Domain Adaptation via Kernel Regression on the Grassmannian | http://arxiv.org/abs/1507.07830 | id:1507.07830 author:Yongxin Yang, Timothy Hospedales category:cs.LG cs.CV  published:2015-07-28 summary:Most visual recognition methods implicitly assume the data distribution remains unchanged from training to testing. However, in practice domain shift often exists, where real-world factors such as lighting and sensor type change between train and test, and classifiers do not generalise from source to target domains. It is impractical to train separate models for all possible situations because collecting and labelling the data is expensive. Domain adaptation algorithms aim to ameliorate domain shift, allowing a model trained on a source to perform well on a different target domain. However, even for the setting of unsupervised domain adaptation, where the target domain is unlabelled, collecting data for every possible target domain is still costly. In this paper, we propose a new domain adaptation method that has no need to access either data or labels of the target domain when it can be described by a parametrised vector and there exits several related source domains within the same parametric space. It greatly reduces the burden of data collection and annotation, and our experiments show some promising results. version:2
arxiv-1507-06711 | The SYSU System for the Interspeech 2015 Automatic Speaker Verification Spoofing and Countermeasures Challenge | http://arxiv.org/abs/1507.06711 | id:1507.06711 author:Shitao Weng, Shushan Chen, Lei Yu, Xuewei Wu, Weicheng Cai, Zhi Liu, Ming Li category:cs.SD cs.CL  published:2015-07-24 summary:Many existing speaker verification systems are reported to be vulnerable against different spoofing attacks, for example speaker-adapted speech synthesis, voice conversion, play back, etc. In order to detect these spoofed speech signals as a countermeasure, we propose a score level fusion approach with several different i-vector subsystems. We show that the acoustic level Mel-frequency cepstral coefficients (MFCC) features, the phase level modified group delay cepstral coefficients (MGDCC) and the phonetic level phoneme posterior probability (PPP) tandem features are effective for the countermeasure. Furthermore, feature level fusion of these features before i-vector modeling also enhance the performance. A polynomial kernel support vector machine is adopted as the supervised classifier. In order to enhance the generalizability of the countermeasure, we also adopted the cosine similarity and PLDA scoring as one-class classifications methods. By combining the proposed i-vector subsystems with the OpenSMILE baseline which covers the acoustic and prosodic information further improves the final performance. The proposed fusion system achieves 0.29% and 3.26% EER on the development and test set of the database provided by the INTERSPEECH 2015 automatic speaker verification spoofing and countermeasures challenge. version:2
arxiv-1506-04723 | Layered Interpretation of Street View Images | http://arxiv.org/abs/1506.04723 | id:1506.04723 author:Ming-Yu Liu, Shuoxin Lin, Srikumar Ramalingam, Oncel Tuzel category:cs.CV  published:2015-06-15 summary:We propose a layered street view model to encode both depth and semantic information on street view images for autonomous driving. Recently, stixels, stix-mantics, and tiered scene labeling methods have been proposed to model street view images. We propose a 4-layer street view model, a compact representation over the recently proposed stix-mantics model. Our layers encode semantic classes like ground, pedestrians, vehicles, buildings, and sky in addition to the depths. The only input to our algorithm is a pair of stereo images. We use a deep neural network to extract the appearance features for semantic classes. We use a simple and an efficient inference algorithm to jointly estimate both semantic classes and layered depth values. Our method outperforms other competing approaches in Daimler urban scene segmentation dataset. Our algorithm is massively parallelizable, allowing a GPU implementation with a processing speed about 9 fps. version:2
arxiv-1507-08155 | IT-Dendrogram: A New Member of the In-Tree (IT) Clustering Family | http://arxiv.org/abs/1507.08155 | id:1507.08155 author:Teng Qiu, Yongjie Li category:stat.ML cs.CV cs.LG stat.ME  published:2015-07-29 summary:Previously, we proposed a physically-inspired method to construct data points into an effective in-tree (IT) structure, in which the underlying cluster structure in the dataset is well revealed. Although there are some edges in the IT structure requiring to be removed, such undesired edges are generally distinguishable from other edges and thus are easy to be determined. For instance, when the IT structures for the 2-dimensional (2D) datasets are graphically presented, those undesired edges can be easily spotted and interactively determined. However, in practice, there are many datasets that do not lie in the 2D Euclidean space, thus their IT structures cannot be graphically presented. But if we can effectively map those IT structures into a visualized space in which the salient features of those undesired edges are preserved, then the undesired edges in the IT structures can still be visually determined in a visualization environment. Previously, this purpose was reached by our method called IT-map. The outstanding advantage of IT-map is that clusters can still be found even with the so-called crowding problem in the embedding. In this paper, we propose another method, called IT-Dendrogram, to achieve the same goal through an effective combination of the IT structure and the single link hierarchical clustering (SLHC) method. Like IT-map, IT-Dendrogram can also effectively represent the IT structures in a visualization environment, whereas using another form, called the Dendrogram. IT-Dendrogram can serve as another visualization method to determine the undesired edges in the IT structures and thus benefit the IT-based clustering analysis. This was demonstrated on several datasets with different shapes, dimensions, and attributes. Unlike IT-map, IT-Dendrogram can always avoid the crowding problem, which could help users make more reliable cluster analysis in certain problems. version:1
arxiv-1507-08104 | Learning Representations for Outlier Detection on a Budget | http://arxiv.org/abs/1507.08104 | id:1507.08104 author:Barbora Micenková, Brian McWilliams, Ira Assent category:cs.LG  published:2015-07-29 summary:The problem of detecting a small number of outliers in a large dataset is an important task in many fields from fraud detection to high-energy physics. Two approaches have emerged to tackle this problem: unsupervised and supervised. Supervised approaches require a sufficient amount of labeled data and are challenged by novel types of outliers and inherent class imbalance, whereas unsupervised methods do not take advantage of available labeled training examples and often exhibit poorer predictive performance. We propose BORE (a Bagged Outlier Representation Ensemble) which uses unsupervised outlier scoring functions (OSFs) as features in a supervised learning framework. BORE is able to adapt to arbitrary OSF feature representations, to the imbalance in labeled data as well as to prediction-time constraints on computational cost. We demonstrate the good performance of BORE compared to a variety of competing methods in the non-budgeted and the budgeted outlier detection problem on 12 real-world datasets. version:1
arxiv-1507-08076 | Cross-pose Face Recognition by Canonical Correlation Analysis | http://arxiv.org/abs/1507.08076 | id:1507.08076 author:Annan Li, Shiguang Shan, Xilin Chen, Bingpeng Ma, Shuicheng Yan, Wen Gao category:cs.CV  published:2015-07-29 summary:The pose problem is one of the bottlenecks in automatic face recognition. We argue that one of the diffculties in this problem is the severe misalignment in face images or feature vectors with different poses. In this paper, we propose that this problem can be statistically solved or at least mitigated by maximizing the intra-subject across-pose correlations via canonical correlation analysis (CCA). In our method, based on the data set with coupled face images of the same identities and across two different poses, CCA learns simultaneously two linear transforms, each for one pose. In the transformed subspace, the intra-subject correlations between the different poses are maximized, which implies pose-invariance or pose-robustness is achieved. The experimental results show that our approach could considerably improve the recognition performance. And if further enhanced with holistic+local feature representation, the performance could be comparable to the state-of-the-art. version:1
arxiv-1507-08074 | STC Anti-spoofing Systems for the ASVspoof 2015 Challenge | http://arxiv.org/abs/1507.08074 | id:1507.08074 author:Sergey Novoselov, Alexandr Kozlov, Galina Lavrentyeva, Konstantin Simonchik, Vadim Shchemelinin category:cs.SD cs.LG stat.ML  published:2015-07-29 summary:This paper presents the Speech Technology Center (STC) systems submitted to Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof) Challenge 2015. In this work we investigate different acoustic feature spaces to determine reliable and robust countermeasures against spoofing attacks. In addition to the commonly used front-end MFCC features we explored features derived from phase spectrum and features based on applying the multiresolution wavelet transform. Similar to state-of-the-art ASV systems, we used the standard TV-JFA approach for probability modelling in spoofing detection systems. Experiments performed on the development and evaluation datasets of the Challenge demonstrate that the use of phase-related and wavelet-based features provides a substantial input into the efficiency of the resulting STC systems. In our research we also focused on the comparison of the linear (SVM) and nonlinear (DBN) classifiers. version:1
arxiv-1504-01025 | Preprint Extending Touch-less Interaction on Vision Based Wearable Device | http://arxiv.org/abs/1504.01025 | id:1504.01025 author:Zhihan Lv, Liangbing Feng, Shengzhong Feng, Haibo Li category:cs.HC cs.CV cs.GR H.1.2; H.5.1  published:2015-04-04 summary:This is the preprint version of our paper on IEEE Virtual Reality Conference 2015. A touch-less interaction technology on vision based wearable device is designed and evaluated. Users interact with the application with dynamic hands/feet gestures in front of the camera. Several proof-of-concept prototypes with eleven dynamic gestures are developed based on the touch-less interaction. At last, a comparing user study evaluation is proposed to demonstrate the usability of the touch-less approach, as well as the impact on user's emotion, running on a wearable framework or Google Glass. version:2
arxiv-1507-08064 | Collaborative Representation Classification Ensemble for Face Recognition | http://arxiv.org/abs/1507.08064 | id:1507.08064 author:Xiaochao Qu, Suah Kim, Run Cui, Hyoung Joong Kim category:cs.CV  published:2015-07-29 summary:Collaborative Representation Classification (CRC) for face recognition attracts a lot attention recently due to its good recognition performance and fast speed. Compared to Sparse Representation Classification (SRC), CRC achieves a comparable recognition performance with 10-1000 times faster speed. In this paper, we propose to ensemble several CRC models to promote the recognition rate, where each CRC model uses different and divergent randomly generated biologically-inspired features as the face representation. The proposed ensemble algorithm calculates an ensemble weight for each CRC model that guided by the underlying classification rule of CRC. The obtained weights reflect the confidences of those CRC models where the more confident CRC models have larger weights. The proposed weighted ensemble method proves to be very effective and improves the performance of each CRC model significantly. Extensive experiments are conducted to show the superior performance of the proposed method. version:1
arxiv-1507-08030 | Adapted sampling for 3D X-ray computed tomography | http://arxiv.org/abs/1507.08030 | id:1507.08030 author:Anthony Cazasnoves, Fanny Buyens, Sylvie Sevestre category:cs.CV  published:2015-07-29 summary:In this paper, we introduce a method to build an adapted mesh representation of a 3D object for X-Ray tomography reconstruction. Using this representation, we provide means to reduce the computational cost of reconstruction by way of iterative algorithms. The adapted sampling of the reconstruction space is directly obtained from the projection dataset and prior to any reconstruction. It is built following two stages : firstly, 2D structural information is extracted from the projection images and is secondly merged in 3D to obtain a 3D pointcloud sampling the interfaces of the object. A relevant mesh is then built from this cloud by way of tetrahedralization. Critical parameters selections have been automatized through a statistical framework, thus avoiding dependence on users expertise. Applying this approach on geometrical shapes and on a 3D Shepp-Logan phantom, we show the relevance of such a sampling - obtained in a few seconds - and the drastic decrease in cells number to be estimated during reconstruction when compared to the usual regular voxel lattice. A first iterative reconstruction of the Shepp-Logan using this kind of sampling shows the relevant advantages in terms of low dose or sparse acquisition sampling contexts. The method can also prove useful for other applications such as finite element method computations. version:1
arxiv-1502-06108 | Don't Just Listen, Use Your Imagination: Leveraging Visual Common Sense for Non-Visual Tasks | http://arxiv.org/abs/1502.06108 | id:1502.06108 author:Xiao Lin, Devi Parikh category:cs.CV  published:2015-02-21 summary:Artificial agents today can answer factual questions. But they fall short on questions that require common sense reasoning. Perhaps this is because most existing common sense databases rely on text to learn and represent knowledge. But much of common sense knowledge is unwritten - partly because it tends not to be interesting enough to talk about, and partly because some common sense is unnatural to articulate in text. While unwritten, it is not unseen. In this paper we leverage semantic common sense knowledge learned from images - i.e. visual common sense - in two textual tasks: fill-in-the-blank and visual paraphrasing. We propose to "imagine" the scene behind the text, and leverage visual cues from the "imagined" scenes in addition to textual cues while answering these questions. We imagine the scenes as a visual abstraction. Our approach outperforms a strong text-only baseline on these tasks. Our proposed tasks can serve as benchmarks to quantitatively evaluate progress in solving tasks that go "beyond recognition". Our code and datasets are publicly available. version:3
arxiv-1411-5371 | Unification of field theory and maximum entropy methods for learning probability densities | http://arxiv.org/abs/1411.5371 | id:1411.5371 author:Justin B. Kinney category:physics.data-an cs.LG q-bio.QM stat.ML  published:2014-11-19 summary:The need to estimate smooth probability distributions (a.k.a. probability densities) from finite sampled data is ubiquitous in science. Many approaches to this problem have been described, but none is yet regarded as providing a definitive solution. Maximum entropy estimation and Bayesian field theory are two such approaches. Both have origins in statistical physics, but the relationship between them has remained unclear. Here I unify these two methods by showing that every maximum entropy density estimate can be recovered in the infinite smoothness limit of an appropriate Bayesian field theory. I also show that Bayesian field theory estimation can be performed without imposing any boundary conditions on candidate densities, and that the infinite smoothness limit of these theories recovers the most common types of maximum entropy estimates. Bayesian field theory is thus seen to provide a natural test of the validity of the maximum entropy null hypothesis. Bayesian field theory also returns a lower entropy density estimate when the maximum entropy hypothesis is falsified. The computations necessary for this approach can be performed rapidly for one-dimensional data, and software for doing this is provided. Based on these results, I argue that Bayesian field theory is poised to provide a definitive solution to the density estimation problem in one dimension. version:5
arxiv-1507-08007 | On Proportions of Fit Individuals in Population of Genetic Algorithm with Tournament Selection | http://arxiv.org/abs/1507.08007 | id:1507.08007 author:Anton Eremeev category:cs.NE  published:2015-07-29 summary:In this paper, we consider a fitness-level model of a non-elitist mutation-only genetic algorithm (GA) with tournament selection. The model provides upper and lower bounds for the expected proportion of the individuals with fitness above given thresholds. In the case of GA with bitwise mutation and OneMax fitness function, the lower bounds are tight when population size equals one, while the upper bounds are asymptotically tight when population size tends to infinity. The lower bounds on expected proportions of sufficiently fit individuals may be obtained from the probability distribution of an appropriate Markov chain. This approach yields polynomial upper bounds on the runtime of an Iterated version of the GA on 2-SAT problem and on a family of Set Cover problems proposed by E. Balas. version:1
arxiv-1501-04158 | On the Performance of ConvNet Features for Place Recognition | http://arxiv.org/abs/1501.04158 | id:1501.04158 author:Niko Sünderhauf, Feras Dayoub, Sareh Shirazi, Ben Upcroft, Michael Milford category:cs.RO cs.CV  published:2015-01-17 summary:After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem. version:3
arxiv-1503-02120 | Identifying missing dictionary entries with frequency-conserving context models | http://arxiv.org/abs/1503.02120 | id:1503.02120 author:Jake Ryland Williams, Eric M. Clark, James P. Bagrow, Christopher M. Danforth, Peter Sheridan Dodds category:cs.CL cs.IT math.IT stat.ML  published:2015-03-07 summary:In an effort to better understand meaning from natural language texts, we explore methods aimed at organizing lexical objects into contexts. A number of these methods for organization fall into a family defined by word ordering. Unlike demographic or spatial partitions of data, these collocation models are of special importance for their universal applicability. While we are interested here in text and have framed our treatment appropriately, our work is potentially applicable to other areas of research (e.g., speech, genomics, and mobility patterns) where one has ordered categorical data, (e.g., sounds, genes, and locations). Our approach focuses on the phrase (whether word or larger) as the primary meaning-bearing lexical unit and object of study. To do so, we employ our previously developed framework for generating word-conserving phrase-frequency data. Upon training our model with the Wiktionary---an extensive, online, collaborative, and open-source dictionary that contains over 100,000 phrasal-definitions---we develop highly effective filters for the identification of meaningful, missing phrase-entries. With our predictions we then engage the editorial community of the Wiktionary and propose short lists of potential missing entries for definition, developing a breakthrough, lexical extraction technique, and expanding our knowledge of the defined English lexicon of phrases. version:3
arxiv-1507-07998 | Document Embedding with Paragraph Vectors | http://arxiv.org/abs/1507.07998 | id:1507.07998 author:Andrew M. Dai, Christopher Olah, Quoc V. Le category:cs.CL cs.AI cs.LG  published:2015-07-29 summary:Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results. version:1
arxiv-1507-07984 | A constrained optimization perspective on actor critic algorithms and application to network routing | http://arxiv.org/abs/1507.07984 | id:1507.07984 author:Prashanth L. A., H. L. Prasad, Shalabh Bhatnagar, Prakash Chandra category:cs.LG math.OC  published:2015-07-28 summary:We propose a novel actor-critic algorithm with guaranteed convergence to an optimal policy for a discounted reward Markov decision process. The actor incorporates a descent direction that is motivated by the solution of a certain non-linear optimization problem. We also discuss an extension to incorporate function approximation and demonstrate the practicality of our algorithms on a network routing application. version:1
arxiv-1507-07974 | An algorithm for online tensor prediction | http://arxiv.org/abs/1507.07974 | id:1507.07974 author:John Pothier, Josh Girson, Shuchin Aeron category:stat.ML cs.IT cs.LG math.IT  published:2015-07-28 summary:We present a new method for online prediction and learning of tensors ($N$-way arrays, $N >2$) from sequential measurements. We focus on the specific case of 3-D tensors and exploit a recently developed framework of structured tensor decompositions proposed in [1]. In this framework it is possible to treat 3-D tensors as linear operators and appropriately generalize notions of rank and positive definiteness to tensors in a natural way. Using these notions we propose a generalization of the matrix exponentiated gradient descent algorithm [2] to a tensor exponentiated gradient descent algorithm using an extension of the notion of von-Neumann divergence to tensors. Then following a similar construction as in [3], we exploit this algorithm to propose an online algorithm for learning and prediction of tensors with provable regret guarantees. Simulations results are presented on semi-synthetic data sets of ratings evolving in time under local influence over a social network. The result indicate superior performance compared to other (online) convex tensor completion methods. version:1
arxiv-1507-07242 | Face Search at Scale: 80 Million Gallery | http://arxiv.org/abs/1507.07242 | id:1507.07242 author:Dayong Wang, Charles Otto, Anil K. Jain category:cs.CV  published:2015-07-26 summary:Due to the prevalence of social media websites, one challenge facing computer vision researchers is to devise methods to process and search for persons of interest among the billions of shared photos on these websites. Facebook revealed in a 2013 white paper that its users have uploaded more than 250 billion photos, and are uploading 350 million new photos each day. Due to this humongous amount of data, large-scale face search for mining web images is both important and challenging. Despite significant progress in face recognition, searching a large collection of unconstrained face images has not been adequately addressed. To address this challenge, we propose a face search system which combines a fast search procedure, coupled with a state-of-the-art commercial off the shelf (COTS) matcher, in a cascaded framework. Given a probe face, we first filter the large gallery of photos to find the top-k most similar faces using deep features generated from a convolutional neural network. The k candidates are re-ranked by combining similarities from deep features and the COTS matcher. We evaluate the proposed face search system on a gallery containing 80 million web-downloaded face images. Experimental results demonstrate that the deep features are competitive with state-of-the-art methods on unconstrained face recognition benchmarks (LFW and IJB-A). Further, the proposed face search system offers an excellent trade-off between accuracy and scalability on datasets consisting of millions of images. Additionally, in an experiment involving searching for face images of the Tsarnaev brothers, convicted of the Boston Marathon bombing, the proposed face search system could find the younger brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a 5M gallery and at rank 8 in 7 seconds on an 80M gallery. version:2
arxiv-1507-07955 | Sparse Multidimensional Patient Modeling using Auxiliary Confidence Labels | http://arxiv.org/abs/1507.07955 | id:1507.07955 author:Eric Heim, Milos Hauskrecht category:cs.LG  published:2015-07-28 summary:In this work, we focus on the problem of learning a classification model that performs inference on patient Electronic Health Records (EHRs). Often, a large amount of costly expert supervision is required to learn such a model. To reduce this cost, we obtain confidence labels that indicate how sure an expert is in the class labels she provides. If meaningful confidence information can be incorporated into a learning method, fewer patient instances may need to be labeled to learn an accurate model. In addition, while accuracy of predictions is important for any inference model, a model of patients must be interpretable so that clinicians can understand how the model is making decisions. To these ends, we develop a novel metric learning method called Confidence bAsed MEtric Learning (CAMEL) that supports inclusion of confidence labels, but also emphasizes interpretability in three ways. First, our method induces sparsity, thus producing simple models that use only a few features from patient EHRs. Second, CAMEL naturally produces confidence scores that can be taken into consideration when clinicians make treatment decisions. Third, the metrics learned by CAMEL induce multidimensional spaces where each dimension represents a different "factor" that clinicians can use to assess patients. In our experimental evaluation, we show on a real-world clinical data set that our CAMEL methods are able to learn models that are as or more accurate as other methods that use the same supervision. Furthermore, we show that when CAMEL uses confidence scores it is able to learn models as or more accurate as others we tested while using only 10% of the training instances. Finally, we perform qualitative assessments on the metrics learned by CAMEL and show that they identify and clearly articulate important factors in how the model performs inference. version:1
arxiv-1503-08329 | Risk Bounds for the Majority Vote: From a PAC-Bayesian Analysis to a Learning Algorithm | http://arxiv.org/abs/1503.08329 | id:1503.08329 author:Pascal Germain, Alexandre Lacasse, François Laviolette, Mario Marchand, Jean-Francis Roy category:stat.ML cs.LG  published:2015-03-28 summary:We propose an extensive analysis of the behavior of majority votes in binary classification. In particular, we introduce a risk bound for majority votes, called the C-bound, that takes into account the average quality of the voters and their average disagreement. We also propose an extensive PAC-Bayesian analysis that shows how the C-bound can be estimated from various observations contained in the training data. The analysis intends to be self-contained and can be used as introductory material to PAC-Bayesian statistical learning theory. It starts from a general PAC-Bayesian perspective and ends with uncommon PAC-Bayesian bounds. Some of these bounds contain no Kullback-Leibler divergence and others allow kernel functions to be used as voters (via the sample compression setting). Finally, out of the analysis, we propose the MinCq learning algorithm that basically minimizes the C-bound. MinCq reduces to a simple quadratic program. Aside from being theoretically grounded, MinCq achieves state-of-the-art performance, as shown in our extensive empirical comparison with both AdaBoost and the Support Vector Machine. version:2
arxiv-1507-07826 | Classifying informative and imaginative prose using complex networks | http://arxiv.org/abs/1507.07826 | id:1507.07826 author:Henrique F. de Arruda, Luciano da F. Costa, Diego R. Amancio category:cs.CL  published:2015-07-28 summary:Statistical methods have been widely employed in recent years to grasp many language properties. The application of such techniques have allowed an improvement of several linguistic applications, which encompasses machine translation, automatic summarization and document classification. In the latter, many approaches have emphasized the semantical content of texts, as it is the case of bag-of-word language models. This approach has certainly yielded reasonable performance. However, some potential features such as the structural organization of texts have been used only on a few studies. In this context, we probe how features derived from textual structure analysis can be effectively employed in a classification task. More specifically, we performed a supervised classification aiming at discriminating informative from imaginative documents. Using a networked model that describes the local topological/dynamical properties of function words, we achieved an accuracy rate of up to 95%, which is much higher than similar networked approaches. A systematic analysis of feature relevance revealed that symmetry and accessibility measurements are among the most prominent network measurements. Our results suggest that these measurements could be used in related language applications, as they play a complementary role in characterizing texts. version:1
arxiv-1506-05702 | Comparing the writing style of real and artificial papers | http://arxiv.org/abs/1506.05702 | id:1506.05702 author:Diego R. Amancio category:cs.CL  published:2015-06-18 summary:Recent years have witnessed the increase of competition in science. While promoting the quality of research in many cases, an intense competition among scientists can also trigger unethical scientific behaviors. To increase the total number of published papers, some authors even resort to software tools that are able to produce grammatical, but meaningless scientific manuscripts. Because automatically generated papers can be misunderstood as real papers, it becomes of paramount importance to develop means to identify these scientific frauds. In this paper, I devise a methodology to distinguish real manuscripts from those generated with SCIGen, an automatic paper generator. Upon modeling texts as complex networks (CN), it was possible to discriminate real from fake papers with at least 89\% of accuracy. A systematic analysis of features relevance revealed that the accessibility and betweenness were useful in particular cases, even though the relevance depended upon the dataset. The successful application of the methods described here show, as a proof of principle, that network features can be used to identify scientific gibberish papers. In addition, the CN-based approach can be combined in a straightforward fashion with traditional statistical language processing methods to improve the performance in identifying artificially generated papers. version:2
arxiv-1507-07815 | A Multi-Camera Image Processing and Visualization System for Train Safety Assessment | http://arxiv.org/abs/1507.07815 | id:1507.07815 author:Giuseppe Lisanti, Svebor Karaman, Daniele Pezzatini, Alberto Del Bimbo category:cs.CV  published:2015-07-28 summary:In this paper we present a machine vision system to efficiently monitor, analyze and present visual data acquired with a railway overhead gantry equipped with multiple cameras. This solution aims to improve the safety of daily life railway transportation in a two- fold manner: (1) by providing automatic algorithms that can process large imagery of trains (2) by helping train operators to keep attention on any possible malfunction. The system is designed with the latest cutting edge, high-rate visible and thermal cameras that ob- serve a train passing under an railway overhead gantry. The machine vision system is composed of three principal modules: (1) an automatic wagon identification system, recognizing the wagon ID according to the UIC classification of railway coaches; (2) a temperature monitoring system; (3) a system for the detection, localization and visualization of the pantograph of the train. These three machine vision modules process batch trains sequences and their resulting analysis are presented to an operator using a multitouch user interface. We detail all technical aspects of our multi-camera portal: the hardware requirements, the software developed to deal with the high-frame rate cameras and ensure reliable acquisition, the algorithms proposed to solve each computer vision task, and the multitouch interaction and visualization interface. We evaluate each component of our system on a dataset recorded in an ad-hoc railway test-bed, showing the potential of our proposed portal for train safety assessment. version:1
arxiv-1507-07813 | An Analytically Tractable Bayesian Approximation to Optimal Point Process Filtering | http://arxiv.org/abs/1507.07813 | id:1507.07813 author:Yuval Harel, Ron Meir, Manfred Opper category:stat.ML q-bio.NC  published:2015-07-28 summary:The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitates the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance. version:1
arxiv-1502-05742 | Application of Independent Component Analysis Techniques in Speckle Noise Reduction of Retinal OCT Images | http://arxiv.org/abs/1502.05742 | id:1502.05742 author:Ahmadreza Baghaie, Roshan M. D'souza, Zeyun Yu category:cs.CV  published:2015-02-19 summary:Optical Coherence Tomography (OCT) is an emerging technique in the field of biomedical imaging, with applications in ophthalmology, dermatology, coronary imaging etc. OCT images usually suffer from a granular pattern, called speckle noise, which restricts the process of interpretation. Therefore the need for speckle noise reduction techniques is of high importance. To the best of our knowledge, use of Independent Component Analysis (ICA) techniques has never been explored for speckle reduction of OCT images. Here, a comparative study of several ICA techniques (InfoMax, JADE, FastICA and SOBI) is provided for noise reduction of retinal OCT images. Having multiple B-scans of the same location, the eye movements are compensated using a rigid registration technique. Then, different ICA techniques are applied to the aggregated set of B-scans for extracting the noise-free image. Signal-to-Noise-Ratio (SNR), Contrast-to-Noise-Ratio (CNR) and Equivalent-Number-of-Looks (ENL), as well as analysis on the computational complexity of the methods, are considered as metrics for comparison. The results show that use of ICA can be beneficial, especially in case of having fewer number of B-scans. version:3
arxiv-1507-07800 | SynapCountJ --- a Tool for Analyzing Synaptic Densities in Neurons | http://arxiv.org/abs/1507.07800 | id:1507.07800 author:Gadea Mata, Jónathan Heras, Miguel Morales, Ana Romero, Julio Rubio category:cs.CV q-bio.NC  published:2015-07-28 summary:The quantification of synapses is instrumental to measure the evolution of synaptic densities of neurons under the effect of some physiological conditions, neuronal diseases or even drug treatments. However, the manual quantification of synapses is a tedious, error-prone, time-consuming and subjective task; therefore, tools that might automate this process are desirable. In this paper, we present SynapCountJ, an ImageJ plugin, that can measure synaptic density of individual neurons obtained by immunofluorescence techniques, and also can be applied for batch processing of neurons that have been obtained in the same experiment or using the same setting. The procedure to quantify synapses implemented in SynapCountJ is based on the colocalization of three images of the same neuron (the neuron marked with two antibody markers and the structure of the neuron) and is inspired by methods coming from Computational Algebraic Topology. SynapCountJ provides a procedure to semi-automatically quantify the number of synapses of neuron cultures; as a result, the time required for such an analysis is greatly reduced. version:1
arxiv-1507-07508 | Fast Segmentation of Left Ventricle in CT Images by Explicit Shape Regression using Random Pixel Difference Features | http://arxiv.org/abs/1507.07508 | id:1507.07508 author:Peng Sun, Haoyin Zhou, Devon Lundine, James K. Min, Guanglei Xiong category:cs.CV  published:2015-07-27 summary:Recently, machine learning has been successfully applied to model-based left ventricle (LV) segmentation. The general framework involves two stages, which starts with LV localization and is followed by boundary delineation. Both are driven by supervised learning techniques. When compared to previous non-learning-based methods, several advantages have been shown, including full automation and improved accuracy. However, the speed is still slow, in the order of several seconds, for applications involving a large number of cases or case loads requiring real-time performance. In this paper, we propose a fast LV segmentation algorithm by joint localization and boundary delineation via training explicit shape regressor with random pixel difference features. Tested on 3D cardiac computed tomography (CT) image volumes, the average running time of the proposed algorithm is 1.2 milliseconds per case. On a dataset consisting of 139 CT volumes, a 5-fold cross validation shows the segmentation error is $1.21 \pm 0.11$ for LV endocardium and $1.23 \pm 0.11$ millimeters for epicardium. Compared with previous work, the proposed method is more stable (lower standard deviation) without significant compromise to the accuracy. version:2
arxiv-1507-07760 | A Hyperelastic Two-Scale Optimization Model for Shape Matching | http://arxiv.org/abs/1507.07760 | id:1507.07760 author:Konrad Simon, Sameer Sheorey, David Jacobs, Ronen Basri category:cs.CG cs.CV cs.GR  published:2015-07-28 summary:We suggest a novel shape matching algorithm for three-dimensional surface meshes of disk or sphere topology. The method is based on the physical theory of nonlinear elasticity and can hence handle large rotations and deformations. Deformation boundary conditions that supplement the underlying equations are usually unknown. Given an initial guess, these are optimized such that the mechanical boundary forces that are responsible for the deformation are of a simple nature. We show a heuristic way to approximate the nonlinear optimization problem by a sequence of convex problems using finite elements. The deformation cost, i.e, the forces, is measured on a coarse scale while ICP-like matching is done on the fine scale. We demonstrate the plausibility of our algorithm on examples taken from different datasets. version:1
arxiv-1506-05244 | Detection of Epigenomic Network Community Oncomarkers | http://arxiv.org/abs/1506.05244 | id:1506.05244 author:Thomas E. Bartlett, Alexey Zaikin category:stat.AP q-bio.GN q-bio.MN stat.ML  published:2015-06-17 summary:In this paper we propose network methodology to infer prognostic cancer biomarkers, based on the epigenetic pattern DNA methylation. Epigenetic processes such as DNA methylation reflect environmental risk factors, and are increasingly recognised for their fundamental role in diseases such as cancer. DNA methylation is a gene-regulatory pattern, and hence provides a means by which to assess genomic regulatory interactions. Network models are a natural way to represent and analyse groups of such interactions. The utility of network models also increases as the quantity of data and number of variables increase, making them increasingly relevant to large-scale genomic studies. We propose methodology to infer prognostic genomic networks from a DNA methylation-based measure of genomic interaction and association. We then show how to identify prognostic biomarkers from such networks, which we term `network community oncomarkers'. We illustrate the power of our proposed methodology in the context of a large publicly available breast cancer data-set. version:2
arxiv-1507-07636 | Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds | http://arxiv.org/abs/1507.07636 | id:1507.07636 author:Sridhar Mahadevan, Sarath Chandar category:cs.CL  published:2015-07-28 summary:Recent work has explored methods for learning continuous vector space word representations reflecting the underlying semantics of words. Simple vector space arithmetic using cosine distances has been shown to capture certain types of analogies, such as reasoning about plurals from singulars, past tense from present tense, etc. In this paper, we introduce a new approach to capture analogies in continuous word representations, based on modeling not just individual word vectors, but rather the subspaces spanned by groups of words. We exploit the property that the set of subspaces in n-dimensional Euclidean space form a curved manifold space called the Grassmannian, a quotient subgroup of the Lie group of rotations in n- dimensions. Based on this mathematical model, we develop a modified cosine distance model based on geodesic kernels that captures relation-specific distances across word categories. Our experiments on analogy tasks show that our approach performs significantly better than the previous approaches for the given task. version:1
arxiv-1308-3282 | Complete stability analysis of a heuristic ADP control design | http://arxiv.org/abs/1308.3282 | id:1308.3282 author:Yury Sokolov, Robert Kozma, Ludmilla D. Werbos, Paul J. Werbos category:cs.NE cs.SY  published:2013-08-15 summary:This paper provides new stability results for Action-Dependent Heuristic Dynamic Programming (ADHDP), using a control algorithm that iteratively improves an internal model of the external world in the autonomous system based on its continuous interaction with the environment. We extend previous results by ADHDP control to the case of general multi-layer neural networks with deep learning across all layers. In particular, we show that the introduced control approach is uniformly ultimately bounded (UUB) under specific conditions on the learning rates, without explicit constraints on the temporal discount factor. We demonstrate the benefit of our results to the control of linear and nonlinear systems, including the cart-pole balancing problem. Our results show significantly improved learning and control performance as compared to the state-of-art. version:2
arxiv-1411-0254 | Variational Inference for Gaussian Process Modulated Poisson Processes | http://arxiv.org/abs/1411.0254 | id:1411.0254 author:Chris Lloyd, Tom Gunter, Michael A. Osborne, Stephen J. Roberts category:stat.ML  published:2014-11-02 summary:We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya. version:3
arxiv-1507-07536 | Online Censoring for Large-Scale Regressions with Application to Streaming Big Data | http://arxiv.org/abs/1507.07536 | id:1507.07536 author:Dimitris Berberidis, Vassilis Kekatos, Georgios B. Giannakis category:stat.AP stat.ML  published:2015-07-27 summary:Linear regression is arguably the most prominent among statistical inference methods, popular both for its simplicity as well as its broad applicability. On par with data-intensive applications, the sheer size of linear regression problems creates an ever growing demand for quick and cost efficient solvers. Fortunately, a significant percentage of the data accrued can be omitted while maintaining a certain quality of statistical inference with an affordable computational budget. The present paper introduces means of identifying and omitting "less informative" observations in an online and data-adaptive fashion, built on principles of stochastic approximation and data censoring. First- and second-order stochastic approximation maximum likelihood-based algorithms for censored observations are developed for estimating the regression coefficients. Online algorithms are also put forth to reduce the overall complexity by adaptively performing censoring along with estimation. The novel algorithms entail simple closed-form updates, and have provable (non)asymptotic convergence guarantees. Furthermore, specific rules are investigated for tuning to desired censoring patterns and levels of dimensionality reduction. Simulated tests on real and synthetic datasets corroborate the efficacy of the proposed data-adaptive methods compared to data-agnostic random projection-based alternatives. version:1
arxiv-1507-07882 | Occlusion-Aware Object Localization, Segmentation and Pose Estimation | http://arxiv.org/abs/1507.07882 | id:1507.07882 author:Samarth Brahmbhatt, Heni Ben Amor, Henrik Christensen category:cs.CV  published:2015-07-27 summary:We present a learning approach for localization and segmentation of objects in an image in a manner that is robust to partial occlusion. Our algorithm produces a bounding box around the full extent of the object and labels pixels in the interior that belong to the object. Like existing segmentation aware detection approaches, we learn an appearance model of the object and consider regions that do not fit this model as potential occlusions. However, in addition to the established use of pairwise potentials for encouraging local consistency, we use higher order potentials which capture information at the level of im- age segments. We also propose an efficient loss function that targets both localization and segmentation performance. Our algorithm achieves 13.52% segmentation error and 0.81 area under the false-positive per image vs. recall curve on average over the challenging CMU Kitchen Occlusion Dataset. This is a 42.44% decrease in segmentation error and a 16.13% increase in localization performance compared to the state-of-the-art. Finally, we show that the visibility labelling produced by our algorithm can make full 3D pose estimation from a single image robust to occlusion. version:1
arxiv-1507-05455 | AMP: a new time-frequency feature extraction method for intermittent time-series data | http://arxiv.org/abs/1507.05455 | id:1507.05455 author:Duncan Barrack, James Goulding, Keith Hopcraft, Simon Preston, Gavin Smith category:cs.LG G.3  published:2015-07-20 summary:The characterisation of time-series data via their most salient features is extremely important in a range of machine learning task, not least of all with regards to classification and clustering. While there exist many feature extraction techniques suitable for non-intermittent time-series data, these approaches are not always appropriate for intermittent time-series data, where intermittency is characterized by constant values for large periods of time punctuated by sharp and transient increases or decreases in value. Motivated by this, we present aggregation, mode decomposition and projection (AMP) a feature extraction technique particularly suited to intermittent time-series data which contain time-frequency patterns. For our method all individual time-series within a set are combined to form a non-intermittent aggregate. This is decomposed into a set of components which represent the intrinsic time-frequency signals within the data set. Individual time-series can then be fit to these components to obtain a set of numerical features that represent their intrinsic time-frequency patterns. To demonstrate the effectiveness of AMP, we evaluate against the real word task of clustering intermittent time-series data. Using synthetically generated data we show that a clustering approach which uses the features derived from AMP significantly outperforms traditional clustering methods. Our technique is further exemplified on a real world data set where AMP can be used to discover groupings of individuals which correspond to real world sub-populations. version:2
arxiv-1507-07495 | Estimating an Activity Driven Hidden Markov Model | http://arxiv.org/abs/1507.07495 | id:1507.07495 author:David A. Meyer, Asif Shakeel category:stat.ML cs.DS cs.LG cs.SI math.ST stat.TH  published:2015-07-27 summary:We define a Hidden Markov Model (HMM) in which each hidden state has time-dependent $\textit{activity levels}$ that drive transitions and emissions, and show how to estimate its parameters. Our construction is motivated by the problem of inferring human mobility on sub-daily time scales from, for example, mobile phone records. version:1
arxiv-1507-07458 | Discovery of Shared Semantic Spaces for Multi-Scene Video Query and Summarization | http://arxiv.org/abs/1507.07458 | id:1507.07458 author:Xun Xu, Timothy Hospedales, Shaogang Gong category:cs.CV  published:2015-07-27 summary:The growing rate of public space CCTV installations has generated a need for automated methods for exploiting video surveillance data including scene understanding, query, behaviour annotation and summarization. For this reason, extensive research has been performed on surveillance scene understanding and analysis. However, most studies have considered single scenes, or groups of adjacent scenes. The semantic similarity between different but related scenes (e.g., many different traffic scenes of similar layout) is not generally exploited to improve any automated surveillance tasks and reduce manual effort. Exploiting commonality, and sharing any supervised annotations, between different scenes is however challenging due to: Some scenes are totally un-related -- and thus any information sharing between them would be detrimental; while others may only share a subset of common activities -- and thus information sharing is only useful if it is selective. Moreover, semantically similar activities which should be modelled together and shared across scenes may have quite different pixel-level appearance in each scene. To address these issues we develop a new framework for distributed multiple-scene global understanding that clusters surveillance scenes by their ability to explain each other's behaviours; and further discovers which subset of activities are shared versus scene-specific within each cluster. We show how to use this structured representation of multiple scenes to improve common surveillance tasks including scene activity understanding, cross-scene query-by-example, behaviour classification with reduced supervised labelling requirements, and video summarization. In each case we demonstrate how our multi-scene model improves on a collection of standard single scene models and a flat model of all scenes. version:1
arxiv-1501-06202 | Robust Subjective Visual Property Prediction from Crowdsourced Pairwise Labels | http://arxiv.org/abs/1501.06202 | id:1501.06202 author:Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Jiechao Xiong, Shaogang Gong, Yizhou Wang, Yuan Yao category:cs.CV cs.LG cs.MM cs.SI math.ST stat.TH  published:2015-01-25 summary:The problem of estimating subjective visual properties from image and video has attracted increasing interest. A subjective visual property is useful either on its own (e.g. image and video interestingness) or as an intermediate representation for visual recognition (e.g. a relative attribute). Due to its ambiguous nature, annotating the value of a subjective visual property for learning a prediction model is challenging. To make the annotation more reliable, recent studies employ crowdsourcing tools to collect pairwise comparison labels because human annotators are much better at ranking two images/videos (e.g. which one is more interesting) than giving an absolute value to each of them separately. However, using crowdsourced data also introduces outliers. Existing methods rely on majority voting to prune the annotation outliers/errors. They thus require large amount of pairwise labels to be collected. More importantly as a local outlier detection method, majority voting is ineffective in identifying outliers that can cause global ranking inconsistencies. In this paper, we propose a more principled way to identify annotation outliers by formulating the subjective visual property prediction task as a unified robust learning to rank problem, tackling both the outlier detection and learning to rank jointly. Differing from existing methods, the proposed method integrates local pairwise comparison labels together to minimise a cost that corresponds to global inconsistency of ranking order. This not only leads to better detection of annotation outliers but also enables learning with extremely sparse annotations. Extensive experiments on various benchmark datasets demonstrate that our new approach significantly outperforms state-of-the-arts alternatives. version:4
arxiv-1507-07403 | Requirements for Open-Ended Evolution in Natural and Artificial Systems | http://arxiv.org/abs/1507.07403 | id:1507.07403 author:Tim Taylor category:cs.NE q-bio.PE  published:2015-07-27 summary:Open-ended evolutionary dynamics remains an elusive goal for artificial evolutionary systems. Many ideas exist in the biological literature beyond the basic Darwinian requirements of variation, differential reproduction and inheritance. I argue that these ideas can be seen as aspects of five fundamental requirements for open-ended evolution: (1) robustly reproductive individuals, (2) a medium allowing the possible existence of a practically unlimited diversity of individuals and interactions, (3) individuals capable of producing more complex offspring, (4) mutational pathways to other viable individuals, and (5) drive for continued evolution. I briefly discuss implications of this view for the design of artificial systems with greater evolutionary potential. version:1
arxiv-1507-07374 | A genetic algorithm for autonomous navigation in partially observable domain | http://arxiv.org/abs/1507.07374 | id:1507.07374 author:Maxim Borisyak, Andrey Ustyuzhanin category:cs.LG cs.AI cs.NE 68T05  published:2015-07-27 summary:The problem of autonomous navigation is one of the basic problems for robotics. Although, in general, it may be challenging when an autonomous vehicle is placed into partially observable domain. In this paper we consider simplistic environment model and introduce a navigation algorithm based on Learning Classifier System. version:1
arxiv-1507-06759 | Variational Bayesian strategies for high-dimensional, stochastic design problems | http://arxiv.org/abs/1507.06759 | id:1507.06759 author:Phaedon-Stelios Koutsourelakis category:stat.CO math.NA stat.ML  published:2015-07-24 summary:This paper is concerned with a lesser-studied problem in the context of model-based, uncertainty quantification (UQ), that of optimization/design/control under uncertainty. The solution of such problems is hindered not only by the usual difficulties encountered in UQ tasks (e.g. the high computational cost of each forward simulation, the large number of random variables) but also by the need to solve a nonlinear optimization problem involving large numbers of design variables and potentially constraints. We propose a framework that is suitable for a large class of such problems and is based on the idea of recasting them as probabilistic inference tasks. To that end, we propose a Variational Bayesian (VB) formulation and an iterative VB-Expectation-Maximization scheme that is also capable of identifying a low-dimensional set of directions in the design space, along which, the objective exhibits the largest sensitivity. We demonstrate the validity of the proposed approach in the context of two numerical examples involving $\mathcal{O}(10^3)$ random and design variables. In all cases considered the cost of the computations in terms of calls to the forward model was of the order $\mathcal{O}(10^2)$. The accuracy of the approximations provided is assessed by appropriate information-theoretic metrics. version:2
arxiv-1502-03032 | Implementing Randomized Matrix Algorithms in Parallel and Distributed Environments | http://arxiv.org/abs/1502.03032 | id:1502.03032 author:Jiyan Yang, Xiangrui Meng, Michael W. Mahoney category:cs.DC cs.DS math.NA stat.ML  published:2015-02-10 summary:In this era of large-scale data, distributed systems built on top of clusters of commodity hardware provide cheap and reliable storage and scalable processing of massive data. Here, we review recent work on developing and implementing randomized matrix algorithms in large-scale parallel and distributed environments. Randomized algorithms for matrix problems have received a great deal of attention in recent years, thus far typically either in theory or in machine learning applications or with implementations on a single machine. Our main focus is on the underlying theory and practical implementation of random projection and random sampling algorithms for very large very overdetermined (i.e., overconstrained) $\ell_1$ and $\ell_2$ regression problems. Randomization can be used in one of two related ways: either to construct sub-sampled problems that can be solved, exactly or approximately, with traditional numerical methods; or to construct preconditioned versions of the original full problem that are easier to solve with traditional iterative algorithms. Theoretical results demonstrate that in near input-sparsity time and with only a few passes through the data one can obtain very strong relative-error approximate solutions, with high probability. Empirical results highlight the importance of various trade-offs (e.g., between the time to construct an embedding and the conditioning quality of the embedding, between the relative importance of computation versus communication, etc.) and demonstrate that $\ell_1$ and $\ell_2$ regression problems can be solved to low, medium, or high precision in existing distributed systems on up to terabyte-sized data. version:2
arxiv-1503-07689 | Likelihood-free Model Choice | http://arxiv.org/abs/1503.07689 | id:1503.07689 author:Jean-Michel Marin, Pierre Pudlo, Christian P. Robert category:stat.ME stat.CO stat.ML  published:2015-03-26 summary:This document is an invited chapter covering the specificities of ABC model choice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont (2015). Beyond exposing the potential pitfalls of ABC based posterior probabilities, the review emphasizes mostly the solution proposed by Pudlo et al. (2014) on the use of random forests for aggregating summary statistics and and for estimating the posterior probability of the most likely model via a secondary random fores. version:2
arxiv-1507-07301 | A Social Spider Algorithm for Solving the Non-convex Economic Load Dispatch Problem | http://arxiv.org/abs/1507.07301 | id:1507.07301 author:James J. Q. Yu, Victor O. K. Li category:cs.NE  published:2015-07-27 summary:Economic Load Dispatch (ELD) is one of the essential components in power system control and operation. Although conventional ELD formulation can be solved using mathematical programming techniques, modern power system introduces new models of the power units which are non-convex, non-differentiable, and sometimes non-continuous. In order to solve such non-convex ELD problems, in this paper we propose a new approach based on the Social Spider Algorithm (SSA). The classical SSA is modified and enhanced to adapt to the unique characteristics of ELD problems, e.g., valve-point effects, multi-fuel operations, prohibited operating zones, and line losses. To demonstrate the superiority of our proposed approach, five widely-adopted test systems are employed and the simulation results are compared with the state-of-the-art algorithms. In addition, the parameter sensitivity is illustrated by a series of simulations. The simulation results show that SSA can solve ELD problems effectively and efficiently. version:1
arxiv-1507-06763 | Differentially Private Analysis of Outliers | http://arxiv.org/abs/1507.06763 | id:1507.06763 author:Rina Okada, Kazuto Fukuchi, Kazuya Kakizaki, Jun Sakuma category:stat.ML cs.CR cs.LG  published:2015-07-24 summary:This paper investigates differentially private analysis of distance-based outliers. The problem of outlier detection is to find a small number of instances that are apparently distant from the remaining instances. On the other hand, the objective of differential privacy is to conceal presence (or absence) of any particular instance. Outlier detection and privacy protection are thus intrinsically conflicting tasks. In this paper, instead of reporting outliers detected, we present two types of differentially private queries that help to understand behavior of outliers. One is the query to count outliers, which reports the number of outliers that appear in a given subspace. Our formal analysis on the exact global sensitivity of outlier counts reveals that regular global sensitivity based method can make the outputs too noisy, particularly when the dimensionality of the given subspace is high. Noting that the counts of outliers are typically expected to be relatively small compared to the number of data, we introduce a mechanism based on the smooth upper bound of the local sensitivity. The other is the query to discovery top-$h$ subspaces containing a large number of outliers. This task can be naively achieved by issuing count queries to each subspace in turn. However, the variation of subspaces can grow exponentially in the data dimensionality. This can cause serious consumption of the privacy budget. For this task, we propose an exponential mechanism with a customized score function for subspace discovery. To the best of our knowledge, this study is the first trial to ensure differential privacy for distance-based outlier analysis. We demonstrated our methods with synthesized datasets and real datasets. The experimental results show that out method achieve better utility compared to the global sensitivity based methods. version:2
arxiv-1507-07260 | Reduced-Set Kernel Principal Components Analysis for Improving the Training and Execution Speed of Kernel Machines | http://arxiv.org/abs/1507.07260 | id:1507.07260 author:Hassan A. Kingravi, Patricio A. Vela, Alexandar Gray category:stat.ML cs.LG  published:2015-07-26 summary:This paper presents a practical, and theoretically well-founded, approach to improve the speed of kernel manifold learning algorithms relying on spectral decomposition. Utilizing recent insights in kernel smoothing and learning with integral operators, we propose Reduced Set KPCA (RSKPCA), which also suggests an easy-to-implement method to remove or replace samples with minimal effect on the empirical operator. A simple data point selection procedure is given to generate a substitute density for the data, with accuracy that is governed by a user-tunable parameter . The effect of the approximation on the quality of the KPCA solution, in terms of spectral and operator errors, can be shown directly in terms of the density estimate error and as a function of the parameter . We show in experiments that RSKPCA can improve both training and evaluation time of KPCA by up to an order of magnitude, and compares favorably to the widely-used Nystrom and density-weighted Nystrom methods. version:1
arxiv-1507-07238 | Estimator Selection: End-Performance Metric Aspects | http://arxiv.org/abs/1507.07238 | id:1507.07238 author:Dimitrios Katselis, Cristian R. Rojas, Carolyn L. Beck category:cs.IT math.IT stat.ML  published:2015-07-26 summary:Recently, a framework for application-oriented optimal experiment design has been introduced. In this context, the distance of the estimated system from the true one is measured in terms of a particular end-performance metric. This treatment leads to superior unknown system estimates to classical experiment designs based on usual pointwise functional distances of the estimated system from the true one. The separation of the system estimator from the experiment design is done within this new framework by choosing and fixing the estimation method to either a maximum likelihood (ML) approach or a Bayesian estimator such as the minimum mean square error (MMSE). Since the MMSE estimator delivers a system estimate with lower mean square error (MSE) than the ML estimator for finite-length experiments, it is usually considered the best choice in practice in signal processing and control applications. Within the application-oriented framework a related meaningful question is: Are there end-performance metrics for which the ML estimator outperforms the MMSE when the experiment is finite-length? In this paper, we affirmatively answer this question based on a simple linear Gaussian regression example. version:1
arxiv-1507-07204 | Modeling Website Workload Using Neural Networks | http://arxiv.org/abs/1507.07204 | id:1507.07204 author:Yasir Shoaib, Olivia Das category:cs.DC cs.NE  published:2015-07-26 summary:In this article, artificial neural networks (ANN) are used for modeling the number of requests received by 1998 FIFA World Cup website. Modeling is done by means of time-series forecasting. The log traces of the website, available through the Internet Traffic Archive (ITA), are processed to obtain two time-series data sets that are used for finding the following measurements: requests/day and requests/second. These are modeled by training and simulating ANN. The method followed to collect and process the data, and perform the experiments have been detailed in this article. In total, 13 cases have been tried and their results have been presented, discussed, compared and summarized. Lastly, future works have also been mentioned. version:1
arxiv-1507-07203 | Capturing the Dynamics of Pedestrian Traffic Using a Machine Vision System | http://arxiv.org/abs/1507.07203 | id:1507.07203 author:Louie Vincent A. Ngoho, Jaderick P. Pabico category:cs.CV  published:2015-07-26 summary:We developed a machine vision system to automatically capture the dynamics of pedestrians under four different traffic scenarios. By considering the overhead view of each pedestrian as a digital object, the system processes the image sequences to track the pedestrians. Considering the perspective effect of the camera lens and the projected area of the hallway at the top-view scene, the distance of each tracked object from its original position to its current position is approximated every video frame. Using the approximated distance and the video frame rate (30 frames per second), the respective velocity and acceleration of each tracked object are later derived. The quantified motion characteristics of the pedestrians are displayed by the system through 2-dimensional graphs of the kinematics of motion. The system also outputs video images of the pedestrians with superimposed markers for tracking. These visual markers were used to visually describe and quantify the behavior of the pedestrians under different traffic scenarios. version:1
arxiv-1507-07200 | A Neural Prototype for a Virtual Chemical Spectrophotometer | http://arxiv.org/abs/1507.07200 | id:1507.07200 author:Jaderick P. Pabico, Jose Rene L. Micor, Elmer Rico E. Mojica category:cs.NE  published:2015-07-26 summary:A virtual chemical spectrophotometer for the simultaneous analysis of nickel (Ni) and cobalt (Co) was developed based on an artificial neural network (ANN). The developed ANN correlates the respective concentrations of Co and Ni given the absorbance profile of a Co-Ni mixture based on the Beer's Law. The virtual chemical spectrometer was trained using a 3-layer jump connection neural network model (NNM) with 126 input nodes corresponding to the 126 absorbance readings from 350 nm to 600 nm, 70 nodes in the hidden layer using a logistic activation function, and 2 nodes in the output layer with a logistic function. Test result shows that the NNM has correlation coefficients of 0.9953 and 0.9922 when predicting [Co] and [Ni], respectively. We observed, however, that the NNM has a duality property and that there exists a real-world practical application in solving the dual problem: Predict the Co-Ni mixture's absorbance profile given [Co] and [Ni]. It turns out that the dual problem is much harder to solve because the intended output has a much bigger cardinality than that of the input. Thus, we trained the dual ANN, a 3-layer jump connection nets with 2 input nodes corresponding to [Co] and [Ni], 70-logistic-activated nodes in the hidden layer, and 126 output nodes corresponding to the 126 absorbance readings from 250 nm to 600 nm. Test result shows that the dual NNM has correlation coefficients that range from 0.9050 through 0.9980 at 356 nm through 578 nm with the maximum coefficient observed at 480 nm. This means that the dual ANN can be used to predict the absorbance profile given the respective Co-Ni concentrations which can be of importance in creating academic models for a virtual chemical spectrophotometer. version:1
arxiv-1507-07199 | Task Selection for Bandit-Based Task Assignment in Heterogeneous Crowdsourcing | http://arxiv.org/abs/1507.07199 | id:1507.07199 author:Hao Zhang, Masashi Sugiyama category:cs.LG  published:2015-07-26 summary:Task selection (picking an appropriate labeling task) and worker selection (assigning the labeling task to a suitable worker) are two major challenges in task assignment for crowdsourcing. Recently, worker selection has been successfully addressed by the bandit-based task assignment (BBTA) method, while task selection has not been thoroughly investigated yet. In this paper, we experimentally compare several task selection strategies borrowed from active learning literature, and show that the least confidence strategy significantly improves the performance of task assignment in crowdsourcing. version:1
arxiv-1507-07147 | True Online Emphatic TD($λ$): Quick Reference and Implementation Guide | http://arxiv.org/abs/1507.07147 | id:1507.07147 author:Richard S. Sutton category:cs.LG  published:2015-07-25 summary:This document is a guide to the implementation of true online emphatic TD($\lambda$), a model-free temporal-difference algorithm for learning to make long-term predictions which combines the emphasis idea (Sutton, Mahmood & White 2015) and the true-online idea (van Seijen & Sutton 2014). The setting used here includes linear function approximation, the possibility of off-policy training, and all the generality of general value functions, as well as the emphasis algorithm's notion of "interest". version:1
arxiv-1507-07146 | A Framework of Sparse Online Learning and Its Applications | http://arxiv.org/abs/1507.07146 | id:1507.07146 author:Dayong Wang, Pengcheng Wu, Peilin Zhao, Steven C. H. Hoi category:cs.LG  published:2015-07-25 summary:The amount of data in our society has been exploding in the era of big data today. In this paper, we address several open challenges of big data stream classification, including high volume, high velocity, high dimensionality, high sparsity, and high class-imbalance. Many existing studies in data mining literature solve data stream classification tasks in a batch learning setting, which suffers from poor efficiency and scalability when dealing with big data. To overcome the limitations, this paper investigates an online learning framework for big data stream classification tasks. Unlike some existing online data stream classification techniques that are often based on first-order online learning, we propose a framework of Sparse Online Classification (SOC) for data stream classification, which includes some state-of-the-art first-order sparse online learning algorithms as special cases and allows us to derive a new effective second-order online learning algorithm for data stream classification. In addition, we also propose a new cost-sensitive sparse online learning algorithm by extending the framework with application to tackle online anomaly detection tasks where class distribution of data could be very imbalanced. We also analyze the theoretical bounds of the proposed method, and finally conduct an extensive set of experiments, in which encouraging results validate the efficacy of the proposed algorithms in comparison to a family of state-of-the-art techniques on a variety of data stream classification tasks. version:1
arxiv-1507-07870 | Detect & Describe: Deep learning of bank stress in the news | http://arxiv.org/abs/1507.07870 | id:1507.07870 author:Samuel Rönnqvist, Peter Sarlin category:q-fin.CP cs.AI cs.LG cs.NE q-fin.RM  published:2015-07-25 summary:News is a pertinent source of information on financial risks and stress factors, which nevertheless is challenging to harness due to the sparse and unstructured nature of natural text. We propose an approach based on distributional semantics and deep learning with neural networks to model and link text to a scarce set of bank distress events. Through unsupervised training, we learn semantic vector representations of news articles as predictors of distress events. The predictive model that we learn can signal coinciding stress with an aggregated index at bank or European level, while crucially allowing for automatic extraction of text descriptions of the events, based on passages with high stress levels. The method offers insight that models based on other types of data cannot provide, while offering a general means for interpreting this type of semantic-predictive model. We model bank distress with data on 243 events and 6.6M news articles for 101 large European banks. version:1
arxiv-1602-07335 | Robust Detection of Intensity Variant Clones in Forged and JPEG Compressed Images | http://arxiv.org/abs/1602.07335 | id:1602.07335 author:Minati Mishra, M. C. Adhikary category:cs.CV  published:2015-07-25 summary:Digitization of images has made image editing easier. Ease of image editing tempted users and professionals to manipulate digital images leading to digital image forgeries. Today digital image forgery has posed a great threat to the authenticity of the popular digital media, the digital images. A lot of research is going on worldwide to detect image forgery and to separate the forged images from their authentic counterparts. This paper provides a novel intensity invariant detection model (IIDM) for detection of intensity variant clones that is robust against JPEG compression, noise attacks and blurring. version:1
arxiv-1507-07096 | Thinning Algorithm Using Hypergraph Based Morphological Operators | http://arxiv.org/abs/1507.07096 | id:1507.07096 author:R. P. Prakash, Keerthana S. Prakash, V. P. Binu category:cs.CV  published:2015-07-25 summary:The object recognition is a complex problem in the image processing. Mathematical morphology is Shape oriented operations, that simplify image data, preserving their essential shape characteristics and eliminating irrelevancies. This paper briefly describes morphological operators using hypergraph and its applications for thinning algorithms. The morphological operators using hypergraph method is used to preventing errors and irregularities in skeleton, and is an important step recognizing line objects. The morphological operators using hypergraph such as dilation, erosion, opening, closing is a novel approach in image processing and it act as a filter remove the noise and errors in the images. version:1
arxiv-1501-04378 | Instance Significance Guided Multiple Instance Boosting for Robust Visual Tracking | http://arxiv.org/abs/1501.04378 | id:1501.04378 author:Jinwu Liu, Yao Lu, Tianfei Zhou category:cs.CV  published:2015-01-19 summary:Multiple Instance Learning (MIL) recently provides an appealing way to alleviate the drifting problem in visual tracking. Following the tracking-by-detection framework, an online MILBoost approach is developed that sequentially chooses weak classifiers by maximizing the bag likelihood. In this paper, we extend this idea towards incorporating the instance significance estimation into the online MILBoost framework. First, instead of treating all instances equally, with each instance we associate a significance-coefficient that represents its contribution to the bag likelihood. The coefficients are estimated by a simple Bayesian formula that jointly considers the predictions from several standard MILBoost classifiers. Next, we follow the online boosting framework, and propose a new criterion for the selection of weak classifiers. Experiments with challenging public datasets show that the proposed method outperforms both existing MIL based and boosting based trackers. version:4
arxiv-1507-07077 | Making sense of randomness: an approach for fast recovery of compressively sensed signals | http://arxiv.org/abs/1507.07077 | id:1507.07077 author:V. Abrol, P. Sharma, A. K Sao category:cs.IT cs.CV math.IT  published:2015-07-25 summary:In compressed sensing (CS) framework, a signal is sampled below Nyquist rate, and the acquired compressed samples are generally random in nature. However, for efficient estimation of the actual signal, the sensing matrix must preserve the relative distances among the acquired compressed samples. Provided this condition is fulfilled, we show that CS samples will preserve the envelope of the actual signal even at different compression ratios. Exploiting this envelope preserving property of CS samples, we propose a new fast dictionary learning (DL) algorithm which is able to extract prototype signals from compressive samples for efficient sparse representation and recovery of signals. These prototype signals are orthogonal intrinsic mode functions (IMFs) extracted using empirical mode decomposition (EMD), which is one of the popular methods to capture the envelope of a signal. The extracted IMFs are used to build the dictionary without even comprehending the original signal or the sensing matrix. Moreover, one can build the dictionary on-line as new CS samples are available. In particularly, to recover first $L$ signals ($\in\mathbb{R}^n$) at the decoder, one can build the dictionary in just $\mathcal{O}(nL\log n)$ operations, that is far less as compared to existing approaches. The efficiency of the proposed approach is demonstrated experimentally for recovery of speech signals. version:1
arxiv-1507-07075 | A Study of Morphological Filtering Using Graph and Hypergraphs | http://arxiv.org/abs/1507.07075 | id:1507.07075 author:Keerthana S. Prakash, R. P. Prakash, V. P. Binu category:cs.CV  published:2015-07-25 summary:Mathematical morphology (MM) helps to describe and analyze shapes using set theory. MM can be effectively applied to binary images which are treated as sets. Basic morphological operators defined can be used as an effective tool in image processing. Morphological operators are also developed based on graph and hypergraph. These operators have found better performance and applications in image processing. Bino et al. [8], [9] developed the theory of morphological operators on hypergraph. A hypergraph structure is considered and basic morphological operation erosion/dilation is defined. Several new operators opening/closing and filtering are also defined on the hypergraphs. Hypergraph based filtering have found comparatively better performance with morphological filters based on graph. In this paper we evaluate the effectiveness of hypergraph based ASF on binary images. Experimental results shows that hypergraph based ASF filters have outperformed graph based ASF. version:1
arxiv-1502-00478 | Structured Occlusion Coding for Robust Face Recognition | http://arxiv.org/abs/1502.00478 | id:1502.00478 author:Yandong Wen, Weiyang Liu, Meng Yang, Yuli Fu, Youjun Xiang, Rui Hu category:cs.CV  published:2015-02-02 summary:Occlusion in face recognition is a common yet challenging problem. While sparse representation based classification (SRC) has been shown promising performance in laboratory conditions (i.e. noiseless or random pixel corrupted), it performs much worse in practical scenarios. In this paper, we consider the practical face recognition problem, where the occlusions are predictable and available for sampling. We propose the structured occlusion coding (SOC) to address occlusion problems. The structured coding here lies in two folds. On one hand, we employ a structured dictionary for recognition. On the other hand, we propose to use the structured sparsity in this formulation. Specifically, SOC simultaneously separates the occlusion and classifies the image. In this way, the problem of recognizing an occluded image is turned into seeking a structured sparse solution on occlusion-appended dictionary. In order to construct a well-performing occlusion dictionary, we propose an occlusion mask estimating technique via locality constrained dictionary (LCD), showing striking improvement in occlusion sample. On a category-specific occlusion dictionary, we replace norm sparsity with the structured sparsity which is shown more robust, further enhancing the robustness of our approach. Moreover, SOC achieves significant improvement in handling large occlusion in real world. Extensive experiments are conducted on public data sets to validate the superiority of the proposed algorithm. version:2
arxiv-1506-00354 | Learning with hidden variables | http://arxiv.org/abs/1506.00354 | id:1506.00354 author:Yasser Roudi, Graham Taylor category:q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML  published:2015-06-01 summary:Learning and inferring features that generate sensory input is a task continuously performed by cortex. In recent years, novel algorithms and learning rules have been proposed that allow neural network models to learn such features from natural images, written text, audio signals, etc. These networks usually involve deep architectures with many layers of hidden neurons. Here we review recent advancements in this area emphasizing, amongst other things, the processing of dynamical inputs by networks with hidden nodes and the role of single neuron models. These points and the questions they arise can provide conceptual advancements in understanding of learning in the cortex and the relationship between machine learning approaches to learning with hidden nodes and those in cortical circuits. version:2
arxiv-1507-06947 | Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition | http://arxiv.org/abs/1507.06947 | id:1507.06947 author:Haşim Sak, Andrew Senior, Kanishka Rao, Françoise Beaufays category:cs.CL cs.LG cs.NE stat.ML  published:2015-07-24 summary:We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly. version:1
arxiv-1507-06923 | A Reinforcement Learning Approach to Online Learning of Decision Trees | http://arxiv.org/abs/1507.06923 | id:1507.06923 author:Abhinav Garlapati, Aditi Raghunathan, Vaishnavh Nagarajan, Balaraman Ravindran category:cs.LG  published:2015-07-24 summary:Online decision tree learning algorithms typically examine all features of a new data point to update model parameters. We propose a novel alternative, Reinforcement Learning- based Decision Trees (RLDT), that uses Reinforcement Learning (RL) to actively examine a minimal number of features of a data point to classify it with high accuracy. Furthermore, RLDT optimizes a long term return, providing a better alternative to the traditional myopic greedy approach to growing decision trees. We demonstrate that this approach performs as well as batch learning algorithms and other online decision tree learning algorithms, while making significantly fewer queries about the features of the data points. We also show that RLDT can effectively handle concept drift. version:1
arxiv-1504-07295 | Document Classification by Inversion of Distributed Language Representations | http://arxiv.org/abs/1504.07295 | id:1504.07295 author:Matt Taddy category:cs.CL cs.IR stat.AP  published:2015-04-27 summary:There have been many recent advances in the structure and measurement of distributed language models: those that map from words to a vector-space that is rich in information about word choice and composition. This vector-space is the distributed language representation. The goal of this note is to point out that any distributed representation can be turned into a classifier through inversion via Bayes rule. The approach is simple and modular, in that it will work with any language representation whose training can be formulated as optimizing a probability model. In our application to 2 million sentences from Yelp reviews, we also find that it performs as well as or better than complex purpose-built algorithms. version:3
arxiv-1507-06877 | Multi-objective analysis of computational models | http://arxiv.org/abs/1507.06877 | id:1507.06877 author:Stéphane Doncieux, Jean Liénard, Benoît Girard, Mohamed Hamdaoui, Joël Chaskalovic category:cs.NE  published:2015-07-24 summary:Computational models are of increasing complexity and their behavior may in particular emerge from the interaction of different parts. Studying such models becomes then more and more difficult and there is a need for methods and tools supporting this process. Multi-objective evolutionary algorithms generate a set of trade-off solutions instead of a single optimal solution. The availability of a set of solutions that have the specificity to be optimal relative to carefully chosen objectives allows to perform data mining in order to better understand model features and regularities. We review the corresponding work, propose a unifying framework, and highlight its potential use. Typical questions that such a methodology allows to address are the following: what are the most critical parameters of the model? What are the relations between the parameters and the objectives? What are the typical behaviors of the model? Two examples are provided to illustrate the capabilities of the methodology. The features of a flapping-wing robot are thus evaluated to find out its speed-energy relation, together with the criticality of its parameters. A neurocomputational model of the Basal Ganglia brain nuclei is then considered and its most salient features according to this methodology are presented and discussed. version:1
arxiv-1507-06837 | YARBUS : Yet Another Rule Based belief Update System | http://arxiv.org/abs/1507.06837 | id:1507.06837 author:Jeremy Fix, Herve Frezza-buet category:cs.CL cs.AI  published:2015-07-24 summary:We introduce a new rule based system for belief tracking in dialog systems. Despite the simplicity of the rules being considered, the proposed belief tracker ranks favourably compared to the previous submissions on the second and third Dialog State Tracking challenges. The results of this simple tracker allows to reconsider the performances of previous submissions using more elaborate techniques. version:1
arxiv-1507-06829 | The Polylingual Labeled Topic Model | http://arxiv.org/abs/1507.06829 | id:1507.06829 author:Lisa Posch, Arnim Bleier, Philipp Schaer, Markus Strohmaier category:cs.CL cs.IR cs.LG G.3; I.2.7  published:2015-07-24 summary:In this paper, we present the Polylingual Labeled Topic Model, a model which combines the characteristics of the existing Polylingual Topic Model and Labeled LDA. The model accounts for multiple languages with separate topic distributions for each language while restricting the permitted topics of a document to a set of predefined labels. We explore the properties of the model in a two-language setting on a dataset from the social science domain. Our experiments show that our model outperforms LDA and Labeled LDA in terms of their held-out perplexity and that it produces semantically coherent topics which are well interpretable by human subjects. version:1
arxiv-1507-06803 | A Neighbourhood-Based Stopping Criterion for Contrastive Divergence Learning | http://arxiv.org/abs/1507.06803 | id:1507.06803 author:E. Romero, F. Mazzanti, J. Delgado category:cs.NE cs.LG  published:2015-07-24 summary:Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used as a stopping criterion for CD, although several authors \cite{schulz-et-al-Convergence-Contrastive-Divergence-2010-NIPSw, fischer-igel-Divergence-Contrastive-Divergence-2010-ICANN} have raised doubts concerning the feasibility of this procedure. In many cases the evolution curve of the reconstruction error is monotonic while the log-likelihood is not, thus indicating that the former is not a good estimator of the optimal stopping point for learning. However, not many alternatives to the reconstruction error have been discussed in the literature. In this manuscript we investigate simple alternatives to the reconstruction error, based on the inclusion of information contained in neighboring states to the training set, as a stopping criterion for CD learning. version:1
arxiv-1507-06802 | Implicitly Constrained Semi-Supervised Least Squares Classification | http://arxiv.org/abs/1507.06802 | id:1507.06802 author:Jesse H. Krijthe, Marco Loog category:stat.ML cs.LG  published:2015-07-24 summary:We introduce a novel semi-supervised version of the least squares classifier. This implicitly constrained least squares (ICLS) classifier minimizes the squared loss on the labeled data among the set of parameters implied by all possible labelings of the unlabeled data. Unlike other discriminative semi-supervised methods, our approach does not introduce explicit additional assumptions into the objective function, but leverages implicit assumptions already present in the choice of the supervised least squares classifier. We show this approach can be formulated as a quadratic programming problem and its solution can be found using a simple gradient descent procedure. We prove that, in a certain way, our method never leads to performance worse than the supervised classifier. Experimental results corroborate this theoretical result in the multidimensional case on benchmark datasets, also in terms of the error rate. version:1
arxiv-1507-06738 | Linear Contextual Bandits with Global Constraints and Objective | http://arxiv.org/abs/1507.06738 | id:1507.06738 author:Shipra Agrawal, Nikhil R. Devanur category:cs.LG math.OC stat.ML  published:2015-07-24 summary:We consider the linear contextual bandit problem with global convex constraints and a concave objective function. In each round, the outcome of pulling an arm is a vector, that depends linearly on the context of that arm. The global constraints require the average of these vectors to lie in a certain convex set. The objective is a concave function of this average vector. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual) [Auer 2003], bandits with concave rewards and convex knapsacks (BwCR) [Agrawal, Devanur 2014], and the online stochastic convex programming (OSCP) problem [Agrawal, Devanur 2015]. We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstructured version of the problem [Agrawal et al. 2015, Badanidiyuru et al. 2014] where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a fixed set of policies. version:1
arxiv-1507-06683 | Clustering of Modal Valued Symbolic Data | http://arxiv.org/abs/1507.06683 | id:1507.06683 author:Vladimir Batagelj, Nataša Kejžar, Simona Korenjak-Černe category:stat.ML  published:2015-07-23 summary:Symbolic Data Analysis is based on special descriptions of data - symbolic objects (SO). Such descriptions preserve more detailed information about units and their clusters than the usual representations with mean values. A special kind of symbolic object is a representation with frequency or probability distributions (modal values). This representation enables us to consider in the clustering process the variables of all measurement types at the same time. In the paper a clustering criterion function for SOs is proposed such that the representative of each cluster is again composed of distributions of variables' values over the cluster. The corresponding leaders clustering method is based on this result. It is also shown that for the corresponding agglomerative hierarchical method a generalized Ward's formula holds. Both methods are compatible - they are solving the same clustering optimization problem. The leaders method efficiently solves clustering problems with large number of units; while the agglomerative method can be applied alone on the smaller data set, or it could be applied on leaders, obtained with compatible nonhierarchical clustering method. Such a combination of two compatible methods enables us to decide upon the right number of clusters on the basis of the corresponding dendrogram. The proposed methods were applied on different data sets. In the paper, some results of clustering of ESS data are presented. version:1
arxiv-1412-6980 | Adam: A Method for Stochastic Optimization | http://arxiv.org/abs/1412.6980 | id:1412.6980 author:Diederik Kingma, Jimmy Ba category:cs.LG  published:2014-12-22 summary:We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. version:8
arxiv-1506-04855 | PCA with Gaussian perturbations | http://arxiv.org/abs/1506.04855 | id:1506.04855 author:Wojciech Kotłowski, Manfred K. Warmuth category:cs.LG stat.ML  published:2015-06-16 summary:Most of machine learning deals with vector parameters. Ideally we would like to take higher order information into account and make use of matrix or even tensor parameters. However the resulting algorithms are usually inefficient. Here we address on-line learning with matrix parameters. It is often easy to obtain online algorithm with good generalization performance if you eigendecompose the current parameter matrix in each trial (at a cost of $O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend $O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is a core trade-off between the running time and the generalization performance, here measured by the regret of the on-line algorithm (total gain of the best off-line predictor minus the total gain of the on-line algorithm). We focus on the key matrix problem of rank $k$ Principal Component Analysis in $\mathbb{R}^n$ where $k \ll n$. There are $O(n^3)$ algorithms that achieve the optimum regret but require eigendecompositions. We develop a simple algorithm that needs $O(kn^2)$ per trial whose regret is off by a small factor of $O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leader paradigm. It replaces full eigendecompositions at each trial by the problem finding $k$ principal components of the current covariance matrix that is perturbed by Gaussian noise. version:2
arxiv-1507-06615 | Optimal Learning Rates for Localized SVMs | http://arxiv.org/abs/1507.06615 | id:1507.06615 author:Mona Eberts, Ingo Steinwart category:stat.ML  published:2015-07-23 summary:One of the limiting factors of using support vector machines (SVMs) in large scale applications are their super-linear computational requirements in terms of the number of training samples. To address this issue, several approaches that train SVMs on many small chunks of large data sets separately have been proposed in the literature. So far, however, almost all these approaches have only been empirically investigated. In addition, their motivation was always based on computational requirements. In this work, we consider a localized SVM approach based upon a partition of the input space. For this local SVM, we derive a general oracle inequality. Then we apply this oracle inequality to least squares regression using Gaussian kernels and deduce local learning rates that are essentially minimax optimal under some standard smoothness assumptions on the regression function. This gives the first motivation for using local SVMs that is not based on computational requirements but on theoretical predictions on the generalization performance. We further introduce a data-dependent parameter selection method for our local SVM approach and show that this method achieves the same learning rates as before. Finally, we present some larger scale experiments for our localized SVM showing that it achieves essentially the same test performance as a global SVM for a fraction of the computational requirements. In addition, it turns out that the computational requirements for the local SVMs are similar to those of a vanilla random chunk approach, while the achieved test errors are significantly better. version:1
arxiv-1507-06580 | Multi-scale exploration of convex functions and bandit convex optimization | http://arxiv.org/abs/1507.06580 | id:1507.06580 author:Sébastien Bubeck, Ronen Eldan category:math.MG cs.LG math.OC math.PR stat.ML  published:2015-07-23 summary:We construct a new map from a convex function to a distribution on its domain, with the property that this distribution is a multi-scale exploration of the function. We use this map to solve a decade-old open problem in adversarial bandit convex optimization by showing that the minimax regret for this problem is $\tilde{O}(\mathrm{poly}(n) \sqrt{T})$, where $n$ is the dimension and $T$ the number of rounds. This bound is obtained by studying the dual Bayesian maximin regret via the information ratio analysis of Russo and Van Roy, and then using the multi-scale exploration to solve the Bayesian problem. version:1
arxiv-1505-00393 | ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks | http://arxiv.org/abs/1505.00393 | id:1505.00393 author:Francesco Visin, Kyle Kastner, Kyunghyun Cho, Matteo Matteucci, Aaron Courville, Yoshua Bengio category:cs.CV  published:2015-05-03 summary:In this paper, we propose a deep neural network architecture for object recognition based on recurrent neural networks. The proposed network, called ReNet, replaces the ubiquitous convolution+pooling layer of the deep convolutional neural network with four recurrent neural networks that sweep horizontally and vertically in both directions across the image. We evaluate the proposed ReNet on three widely-used benchmark datasets; MNIST, CIFAR-10 and SVHN. The result suggests that ReNet is a viable alternative to the deep convolutional neural network, and that further investigation is needed. version:3
arxiv-1507-06535 | Manitest: Are classifiers really invariant? | http://arxiv.org/abs/1507.06535 | id:1507.06535 author:Alhussein Fawzi, Pascal Frossard category:cs.CV cs.LG stat.ML  published:2015-07-23 summary:Invariance to geometric transformations is a highly desirable property of automatic classifiers in many image recognition tasks. Nevertheless, it is unclear to which extent state-of-the-art classifiers are invariant to basic transformations such as rotations and translations. This is mainly due to the lack of general methods that properly measure such an invariance. In this paper, we propose a rigorous and systematic approach for quantifying the invariance to geometric transformations of any classifier. Our key idea is to cast the problem of assessing a classifier's invariance as the computation of geodesics along the manifold of transformed images. We propose the Manitest method, built on the efficient Fast Marching algorithm to compute the invariance of classifiers. Our new method quantifies in particular the importance of data augmentation for learning invariance from data, and the increased invariance of convolutional neural networks with depth. We foresee that the proposed generic tool for measuring invariance to a large class of geometric transformations and arbitrary classifiers will have many applications for evaluating and comparing classifiers based on their invariance, and help improving the invariance of existing classifiers. version:1
arxiv-1507-06452 | Dynamic Matrix Factorization with Priors on Unknown Values | http://arxiv.org/abs/1507.06452 | id:1507.06452 author:Robin Devooght, Nicolas Kourtellis, Amin Mantrach category:stat.ML cs.IR cs.LG  published:2015-07-23 summary:Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones (\emph{not missing at random}). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings. version:1
arxiv-1507-06429 | Deep Fishing: Gradient Features from Deep Nets | http://arxiv.org/abs/1507.06429 | id:1507.06429 author:Albert Gordo, Adrien Gaidon, Florent Perronnin category:cs.CV  published:2015-07-23 summary:Convolutional Networks (ConvNets) have recently improved image recognition performance thanks to end-to-end learning of deep feed-forward models from raw pixels. Deep learning is a marked departure from the previous state of the art, the Fisher Vector (FV), which relied on gradient-based encoding of local hand-crafted features. In this paper, we discuss a novel connection between these two approaches. First, we show that one can derive gradient representations from ConvNets in a similar fashion to the FV. Second, we show that this gradient representation actually corresponds to a structured matrix that allows for efficient similarity computation. We experimentally study the benefits of transferring this representation over the outputs of ConvNet layers, and find consistent improvements on the Pascal VOC 2007 and 2012 datasets. version:1
arxiv-1502-04383 | A Comprehensive Survey on Pose-Invariant Face Recognition | http://arxiv.org/abs/1502.04383 | id:1502.04383 author:Changxing Ding, Dacheng Tao category:cs.CV  published:2015-02-15 summary:The capacity to recognize faces under varied poses is a fundamental human ability that presents a unique challenge for computer vision systems. Compared to frontal face recognition, which has been intensively studied and has gradually matured in the past few decades, pose-invariant face recognition (PIFR) remains a largely unsolved problem. However, PIFR is crucial to realizing the full potential of face recognition for real-world applications, since face recognition is intrinsically a passive biometric technology for recognizing uncooperative subjects. In this paper, we discuss the inherent difficulties in PIFR and present a comprehensive review of established techniques. Existing PIFR methods can be grouped into four categories, i.e., pose-robust feature extraction approaches, multi-view subspace learning approaches, face synthesis approaches, and hybrid approaches. The motivations, strategies, pros/cons, and performance of representative approaches are described and compared. Moreover, promising directions for future research are discussed. version:2
arxiv-1507-06411 | Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment | http://arxiv.org/abs/1507.06411 | id:1507.06411 author:Olivier Francois category:stat.OT cs.DL stat.ML  published:2015-07-23 summary:The principle of peer review is central to the evaluation of research, by ensuring that only high-quality items are funded or published. But peer review has also received criticism, as the selection of reviewers may introduce biases in the system. In 2014, the organizers of the ``Neural Information Processing Systems\rq\rq{} conference conducted an experiment in which $10\%$ of submitted manuscripts (166 items) went through the review process twice. Arbitrariness was measured as the conditional probability for an accepted submission to get rejected if examined by the second committee. This number was equal to $60\%$, for a total acceptance rate equal to $22.5\%$. Here we present a Bayesian analysis of those two numbers, by introducing a hidden parameter which measures the probability that a submission meets basic quality criteria. The standard quality criteria usually include novelty, clarity, reproducibility, correctness and no form of misconduct, and are met by a large proportions of submitted items. The Bayesian estimate for the hidden parameter was equal to $56\%$ ($95\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result suggested the total acceptance rate should be increased in order to decrease arbitrariness estimates in future review processes. version:1
arxiv-1507-06397 | Multi-Target Tracking with Time-Varying Clutter Rate and Detection Profile: Application to Time-lapse Cell Microscopy Sequences | http://arxiv.org/abs/1507.06397 | id:1507.06397 author:Seyed Hamid Rezatofighi, Stephen Gould, Ba Tuong Vo, Ba-Ngu Vo, Katarina Mele, Richard Hartley category:cs.CV  published:2015-07-23 summary:Quantitative analysis of the dynamics of tiny cellular and sub-cellular structures, known as particles, in time-lapse cell microscopy sequences requires the development of a reliable multi-target tracking method capable of tracking numerous similar targets in the presence of high levels of noise, high target density, complex motion patterns and intricate interactions. In this paper, we propose a framework for tracking these structures based on the random finite set Bayesian filtering framework. We focus on challenging biological applications where image characteristics such as noise and background intensity change during the acquisition process. Under these conditions, detection methods usually fail to detect all particles and are often followed by missed detections and many spurious measurements with unknown and time-varying rates. To deal with this, we propose a bootstrap filter composed of an estimator and a tracker. The estimator adaptively estimates the required meta parameters for the tracker such as clutter rate and the detection probability of the targets, while the tracker estimates the state of the targets. Our results show that the proposed approach can outperform state-of-the-art particle trackers on both synthetic and real data in this regime. version:1
arxiv-1507-01215 | Combining Models of Approximation with Partial Learning | http://arxiv.org/abs/1507.01215 | id:1507.01215 author:Ziyuan Gao, Frank Stephan, Sandra Zilles category:cs.LG 68Q32  published:2015-07-05 summary:In Gold's framework of inductive inference, the model of partial learning requires the learner to output exactly one correct index for the target object and only the target object infinitely often. Since infinitely many of the learner's hypotheses may be incorrect, it is not obvious whether a partial learner can be modifed to "approximate" the target object. Fulk and Jain (Approximate inference and scientific method. Information and Computation 114(2):179--191, 1994) introduced a model of approximate learning of recursive functions. The present work extends their research and solves an open problem of Fulk and Jain by showing that there is a learner which approximates and partially identifies every recursive function by outputting a sequence of hypotheses which, in addition, are also almost all finite variants of the target function. The subsequent study is dedicated to the question how these findings generalise to the learning of r.e. languages from positive data. Here three variants of approximate learning will be introduced and investigated with respect to the question whether they can be combined with partial learning. Following the line of Fulk and Jain's research, further investigations provide conditions under which partial language learners can eventually output only finite variants of the target language. The combinabilities of other partial learning criteria will also be briefly studied. version:2
arxiv-1506-07310 | Targeting Ultimate Accuracy: Face Recognition via Deep Embedding | http://arxiv.org/abs/1506.07310 | id:1506.07310 author:Jingtuo Liu, Yafeng Deng, Tao Bai, Zhengping Wei, Chang Huang category:cs.CV  published:2015-06-24 summary:Face Recognition has been studied for many decades. As opposed to traditional hand-crafted features such as LBP and HOG, much more sophisticated features can be learned automatically by deep learning methods in a data-driven way. In this paper, we propose a two-stage approach that combines a multi-patch deep CNN and deep metric learning, which extracts low dimensional but very discriminative features for face verification and recognition. Experiments show that this method outperforms other state-of-the-art methods on LFW dataset, achieving 99.77% pair-wise verification accuracy and significantly better accuracy under other two more practical protocols. This paper also discusses the importance of data size and the number of patches, showing a clear path to practical high-performance face recognition systems in real world. version:4
arxiv-1404-5692 | Forward - Backward Greedy Algorithms for Atomic Norm Regularization | http://arxiv.org/abs/1404.5692 | id:1404.5692 author:Nikhil Rao, Parikshit Shah, Stephen Wright category:cs.DS cs.LG math.OC stat.ML  published:2014-04-23 summary:In many signal processing applications, the aim is to reconstruct a signal that has a simple representation with respect to a certain basis or frame. Fundamental elements of the basis known as "atoms" allow us to define "atomic norms" that can be used to formulate convex regularizations for the reconstruction problem. Efficient algorithms are available to solve these formulations in certain special cases, but an approach that works well for general atomic norms, both in terms of speed and reconstruction accuracy, remains to be found. This paper describes an optimization algorithm called CoGEnT that produces solutions with succinct atomic representations for reconstruction problems, generally formulated with atomic-norm constraints. CoGEnT combines a greedy selection scheme based on the conditional gradient approach with a backward (or "truncation") step that exploits the quadratic nature of the objective to reduce the basis size. We establish convergence properties and validate the algorithm via extensive numerical experiments on a suite of signal processing applications. Our algorithm and analysis also allow for inexact forward steps and for occasional enhancements of the current representation to be performed. CoGEnT can outperform the basic conditional gradient method, and indeed many methods that are tailored to specific applications, when the enhancement and truncation steps are defined appropriately. We also introduce several novel applications that are enabled by the atomic-norm framework, including tensor completion, moment problems in signal processing, and graph deconvolution. version:2
arxiv-1507-06346 | Evaluation of Spectral Learning for the Identification of Hidden Markov Models | http://arxiv.org/abs/1507.06346 | id:1507.06346 author:Robert Mattila, Cristian R. Rojas, Bo Wahlberg category:stat.ML cs.LG math.OC  published:2015-07-22 summary:Hidden Markov models have successfully been applied as models of discrete time series in many fields. Often, when applied in practice, the parameters of these models have to be estimated. The currently predominating identification methods, such as maximum-likelihood estimation and especially expectation-maximization, are iterative and prone to have problems with local minima. A non-iterative method employing a spectral subspace-like approach has recently been proposed in the machine learning literature. This paper evaluates the performance of this algorithm, and compares it to the performance of the expectation-maximization algorithm, on a number of numerical examples. We find that the performance is mixed; it successfully identifies some systems with relatively few available observations, but fails completely for some systems even when a large amount of observations is available. An open question is how this discrepancy can be explained. We provide some indications that it could be related to how well-conditioned some system parameters are. version:1
arxiv-1507-06332 | Part Localization using Multi-Proposal Consensus for Fine-Grained Categorization | http://arxiv.org/abs/1507.06332 | id:1507.06332 author:Kevin J. Shih, Arun Mallya, Saurabh Singh, Derek Hoiem category:cs.CV  published:2015-07-22 summary:We present a simple deep learning framework to simultaneously predict keypoint locations and their respective visibilities and use those to achieve state-of-the-art performance for fine-grained classification. We show that by conditioning the predictions on object proposals with sufficient image support, our method can do well without complicated spatial reasoning. Instead, inference methods with robustness to outliers, yield state-of-the-art for keypoint localization. We demonstrate the effectiveness of our accurate keypoint localization and visibility prediction on the fine-grained bird recognition task with and without ground truth bird bounding boxes, and outperform existing state-of-the-art methods by over 2%. version:1
arxiv-1506-05070 | Reservoir Characterization: A Machine Learning Approach | http://arxiv.org/abs/1506.05070 | id:1506.05070 author:Soumi Chaki category:cs.CE cs.LG  published:2015-06-15 summary:Reservoir Characterization (RC) can be defined as the act of building a reservoir model that incorporates all the characteristics of the reservoir that are pertinent to its ability to store hydrocarbons and also to produce them.It is a difficult problem due to non-linear and heterogeneous subsurface properties and associated with a number of complex tasks such as data fusion, data mining, formulation of the knowledge base, and handling of the uncertainty.This present work describes the development of algorithms to obtain the functional relationships between predictor seismic attributes and target lithological properties. Seismic attributes are available over a study area with lower vertical resolution. Conversely, well logs and lithological properties are available only at specific well locations in a study area with high vertical resolution.Sand fraction, which represents per unit sand volume within the rock, has a balanced distribution between zero to unity.The thesis addresses the issues of handling the information content mismatch between predictor and target variables and proposes regularization of target property prior to building a prediction model.In this thesis, two Artificial Neural Network (ANN) based frameworks are proposed to model sand fraction from multiple seismic attributes without and with well tops information respectively. The performances of the frameworks are quantified in terms of Correlation Coefficient, Root Mean Square Error, Absolute Error Mean, etc. version:2
arxiv-1507-05016 | Incremental Variational Inference for Latent Dirichlet Allocation | http://arxiv.org/abs/1507.05016 | id:1507.05016 author:Cedric Archambeau, Beyza Ermis category:stat.ML  published:2015-07-17 summary:We introduce incremental variational inference and apply it to latent Dirichlet allocation (LDA). Incremental variational inference is inspired by incremental EM and provides an alternative to stochastic variational inference. Incremental LDA can process massive document collections, does not require to set a learning rate, converges faster to a local optimum of the variational bound and enjoys the attractive property of monotonically increasing it. We study the performance of incremental LDA on large benchmark data sets. We further introduce a stochastic approximation of incremental variational inference which extends to the asynchronous distributed setting. The resulting distributed algorithm achieves comparable performance as single host incremental variational inference, but with a significant speed-up. version:2
arxiv-1507-06222 | STICK: Spike Time Interval Computational Kernel, A Framework for General Purpose Computation using Neurons, Precise Timing, Delays, and Synchrony | http://arxiv.org/abs/1507.06222 | id:1507.06222 author:Xavier Lagorce, Ryad Benosman category:cs.NE  published:2015-07-22 summary:There has been significant research over the past two decades in developing new platforms for spiking neural computation. Current neural computers are primarily developed to mimick biology. They use neural networks which can be trained to perform specific tasks to mainly solve pattern recognition problems. These machines can do more than simulate biology, they allow us to re-think our current paradigm of computation. The ultimate goal is to develop brain inspired general purpose computation architectures that can breach the current bottleneck introduced by the Von Neumann architecture. This work proposes a new framework for such a machine. We show that the use of neuron like units with precise timing representation, synaptic diversity, and temporal delays allows us to set a complete, scalable compact computation framework. The presented framework provides both linear and non linear operations, allowing us to represent and solve any function. We show usability in solving real use cases from simple differential equations to sets of non-linear differential equations leading to chaotic attractors. version:1
arxiv-1507-06173 | Bayesian Time-of-Flight for Realtime Shape, Illumination and Albedo | http://arxiv.org/abs/1507.06173 | id:1507.06173 author:Amit Adam, Christoph Dann, Omer Yair, Shai Mazor, Sebastian Nowozin category:cs.CV  published:2015-07-22 summary:We propose a computational model for shape, illumination and albedo inference in a pulsed time-of-flight (TOF) camera. In contrast to TOF cameras based on phase modulation, our camera enables general exposure profiles. This results in added flexibility and requires novel computational approaches. To address this challenge we propose a generative probabilistic model that accurately relates latent imaging conditions to observed camera responses. While principled, realtime inference in the model turns out to be infeasible, and we propose to employ efficient non-parametric regression trees to approximate the model outputs. As a result we are able to provide, for each pixel, at video frame rate, estimates and uncertainty for depth, effective albedo, and ambient light intensity. These results we present are state-of-the-art in depth imaging. The flexibility of our approach allows us to easily enrich our generative model. We demonstrate that by extending the original single-path model to a two-path model, capable of describing some multipath effects. The new model is seamlessly integrated in the system at no additional computational cost. Our work also addresses the important question of optimal exposure design in pulsed TOF systems. Finally, for benchmark purposes and to obtain realistic empirical priors of multipath and insights into this phenomena, we propose a physically accurate simulation of multipath phenomena. version:1
arxiv-1507-06149 | Data-free parameter pruning for Deep Neural Networks | http://arxiv.org/abs/1507.06149 | id:1507.06149 author:Suraj Srinivas, R. Venkatesh Babu category:cs.CV  published:2015-07-22 summary:Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\% of the total parameters in an MNIST-trained network, and about 35\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network. version:1
arxiv-1507-05775 | Compression of Fully-Connected Layer in Neural Network by Kronecker Product | http://arxiv.org/abs/1507.05775 | id:1507.05775 author:Shuchang Zhou, Jia-Nan Wu category:cs.NE cs.CV cs.LG  published:2015-07-21 summary:In this paper we propose and study a technique to reduce the number of parameters and computation time in fully-connected layers of neural networks using Kronecker product, at a mild cost of the prediction quality. The technique proceeds by replacing Fully-Connected layers with so-called Kronecker Fully-Connected layers, where the weight matrices of the FC layers are approximated by linear combinations of multiple Kronecker products of smaller matrices. In particular, given a model trained on SVHN dataset, we are able to construct a new KFC model with 73\% reduction in total number of parameters, while the error only rises mildly. In contrast, using low-rank method can only achieve 35\% reduction in total number of parameters given similar quality degradation allowance. If we only compare the KFC layer with its counterpart fully-connected layer, the reduction in the number of parameters exceeds 99\%. The amount of computation is also reduced as we replace matrix product of the large matrices in FC layers with matrix products of a few smaller matrices in KFC layers. Further experiments on MNIST, SVHN and some Chinese Character recognition models also demonstrate effectiveness of our technique. version:2
arxiv-1507-06105 | Banzhaf Random Forests | http://arxiv.org/abs/1507.06105 | id:1507.06105 author:Jianyuan Sun, Guoqiang Zhong, Junyu Dong, Yajuan Cai category:cs.LG cs.CV stat.ML  published:2015-07-22 summary:Random forests are a type of ensemble method which makes predictions by combining the results of several independent trees. However, the theory of random forests has long been outpaced by their application. In this paper, we propose a novel random forests algorithm based on cooperative game theory. Banzhaf power index is employed to evaluate the power of each feature by traversing possible feature coalitions. Unlike the previously used information gain rate of information theory, which simply chooses the most informative feature, the Banzhaf power index can be considered as a metric of the importance of each feature on the dependency among a group of features. More importantly, we have proved the consistency of the proposed algorithm, named Banzhaf random forests (BRF). This theoretical analysis takes a step towards narrowing the gap between the theory and practice of random forests for classification problems. Experiments on several UCI benchmark data sets show that BRF is competitive with state-of-the-art classifiers and dramatically outperforms previous consistent random forests. Particularly, it is much more efficient than previous consistent random forests. version:1
arxiv-1412-8307 | Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm | http://arxiv.org/abs/1412.8307 | id:1412.8307 author:Mark D. McDonnell, Migel D. Tissera, Tony Vladusich, André van Schaik, Jonathan Tapson category:cs.NE cs.CV cs.LG  published:2014-12-29 summary:Recent advances in training deep (multi-layer) architectures have inspired a renaissance in neural network use. For example, deep convolutional networks are becoming the default option for difficult tasks on large datasets, such as image and speech recognition. However, here we show that error rates below 1% on the MNIST handwritten digit benchmark can be replicated with shallow non-convolutional neural networks. This is achieved by training such networks using the 'Extreme Learning Machine' (ELM) approach, which also enables a very rapid training time (~10 minutes). Adding distortions, as is common practise for MNIST, reduces error rates even further. Our methods are also shown to be capable of achieving less than 5.5% error rates on the NORB image database. To achieve these results, we introduce several enhancements to the standard ELM algorithm, which individually and in combination can significantly improve performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random `receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90% of weights equal to zero. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems. version:2
arxiv-1507-06073 | Discriminative Segmental Cascades for Feature-Rich Phone Recognition | http://arxiv.org/abs/1507.06073 | id:1507.06073 author:Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu category:cs.CL  published:2015-07-22 summary:Discriminative segmental models, such as segmental conditional random fields (SCRFs) and segmental structured support vector machines (SSVMs), have had success in speech recognition via both lattice rescoring and first-pass decoding. However, such models suffer from slow decoding, hampering the use of computationally expensive features, such as segment neural networks or other high-order features. A typical solution is to use approximate decoding, either by beam pruning in a single pass or by beam pruning to generate a lattice followed by a second pass. In this work, we study discriminative segmental models trained with a hinge loss (i.e., segmental structured SVMs). We show that beam search is not suitable for learning rescoring models in this approach, though it gives good approximate decoding performance when the model is already well-trained. Instead, we consider an approach inspired by structured prediction cascades, which use max-marginal pruning to generate lattices. We obtain a high-accuracy phonetic recognition system with several expensive feature types: a segment neural network, a second-order language model, and second-order phone boundary features. version:1
arxiv-1507-06065 | MixEst: An Estimation Toolbox for Mixture Models | http://arxiv.org/abs/1507.06065 | id:1507.06065 author:Reshad Hosseini, Mohamadreza Mash'al category:stat.ML cs.LG  published:2015-07-22 summary:Mixture models are powerful statistical models used in many applications ranging from density estimation to clustering and classification. When dealing with mixture models, there are many issues that the experimenter should be aware of and needs to solve. The MixEst toolbox is a powerful and user-friendly package for MATLAB that implements several state-of-the-art approaches to address these problems. Additionally, MixEst gives the possibility of using manifold optimization for fitting the density model, a feature specific to this toolbox. MixEst simplifies using and integration of mixture models in statistical models and applications. For developing mixture models of new densities, the user just needs to provide a few functions for that statistical distribution and the toolbox takes care of all the issues regarding mixture models. MixEst is available at visionlab.ut.ac.ir/mixest and is fully documented and is licensed under GPL. version:1
arxiv-1506-05869 | A Neural Conversational Model | http://arxiv.org/abs/1506.05869 | id:1506.05869 author:Oriol Vinyals, Quoc Le category:cs.CL  published:2015-06-19 summary:Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model. version:3
arxiv-1410-4639 | Dependent Types for Pragmatics | http://arxiv.org/abs/1410.4639 | id:1410.4639 author:Darryl McAdams, Jonathan Sterling category:cs.CL  published:2014-10-17 summary:This paper proposes the use of dependent types for pragmatic phenomena such as pronoun binding and presupposition resolution as a type-theoretic alternative to formalisms such as Discourse Representation Theory and Dynamic Semantics. version:3
arxiv-1507-06032 | Elastic Net Procedure for Partially Linear Models | http://arxiv.org/abs/1507.06032 | id:1507.06032 author:Chunhong Li, Dengxiang Huang, Hongshuai Dai, Xinxing Wei category:stat.ME math.PR stat.ML  published:2015-07-22 summary:Variable selection plays an important role in the high-dimensional data analysis. However the high-dimensional data often induces the strongly correlated variables problem. In this paper, we propose Elastic Net procedure for partially linear models and prove the group effect of its estimate. By a simulation study, we show that the strongly correlated variables problem can be better handled by the Elastic Net procedure than Lasso, ALasso and Ridge. Based on an empirical analysis, we can get that the Elastic Net procedure is particularly useful when the number of predictors $p$ is much bigger than the sample size $n$. version:1
arxiv-1507-06028 | The challenges of SVM optimization using Adaboost on a phoneme recognition problem | http://arxiv.org/abs/1507.06028 | id:1507.06028 author:Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG  published:2015-07-22 summary:The use of digital technology is growing at a very fast pace which led to the emergence of systems based on the cognitive infocommunications. The expansion of this sector impose the use of combining methods in order to ensure the robustness in cognitive systems. version:1
arxiv-1507-06025 | Incorporating Belief Function in SVM for Phoneme Recognition | http://arxiv.org/abs/1507.06025 | id:1507.06025 author:Rimah Amami, Dorra Ben Ayed, Nouerddine Ellouze category:cs.CL cs.LG  published:2015-07-22 summary:The Support Vector Machine (SVM) method has been widely used in numerous classification tasks. The main idea of this algorithm is based on the principle of the margin maximization to find an hyperplane which separates the data into two different classes.In this paper, SVM is applied to phoneme recognition task. However, in many real-world problems, each phoneme in the data set for recognition problems may differ in the degree of significance due to noise, inaccuracies, or abnormal characteristics; All those problems can lead to the inaccuracies in the prediction phase. Unfortunately, the standard formulation of SVM does not take into account all those problems and, in particular, the variation in the speech input. This paper presents a new formulation of SVM (B-SVM) that attributes to each phoneme a confidence degree computed based on its geometric position in the space. Then, this degree is used in order to strengthen the class membership of the tested phoneme. Hence, we introduce a reformulation of the standard SVM that incorporates the degree of belief. Experimental performance on TIMIT database shows the effectiveness of the proposed method B-SVM on a phoneme recognition problem. version:1
arxiv-1507-06023 | Robust speech recognition using consensus function based on multi-layer networks | http://arxiv.org/abs/1507.06023 | id:1507.06023 author:Rimah Amami, Ghaith Manita, Abir Smiti category:cs.CL cs.LG  published:2015-07-22 summary:The clustering ensembles mingle numerous partitions of a specified data into a single clustering solution. Clustering ensemble has emerged as a potent approach for ameliorating both the forcefulness and the stability of unsupervised classification results. One of the major problems in clustering ensembles is to find the best consensus function. Finding final partition from different clustering results requires skillfulness and robustness of the classification algorithm. In addition, the major problem with the consensus function is its sensitivity to the used data sets quality. This limitation is due to the existence of noisy, silence or redundant data. This paper proposes a novel consensus function of cluster ensembles based on Multilayer networks technique and a maintenance database method. This maintenance database approach is used in order to handle any given noisy speech and, thus, to guarantee the quality of databases. This can generates good results and efficient data partitions. To show its effectiveness, we support our strategy with empirical evaluation using distorted speech from Aurora speech databases. version:1
arxiv-1507-06021 | An Empirical Comparison of SVM and Some Supervised Learning Algorithms for Vowel recognition | http://arxiv.org/abs/1507.06021 | id:1507.06021 author:Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG  published:2015-07-22 summary:In this article, we conduct a study on the performance of some supervised learning algorithms for vowel recognition. This study aims to compare the accuracy of each algorithm. Thus, we present an empirical comparison between five supervised learning classifiers and two combined classifiers: SVM, KNN, Naive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms were tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral coefficients (MFCCs). version:1
arxiv-1507-06020 | Practical Selection of SVM Supervised Parameters with Different Feature Representations for Vowel Recognition | http://arxiv.org/abs/1507.06020 | id:1507.06020 author:Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze category:cs.CL cs.LG  published:2015-07-22 summary:It is known that the classification performance of Support Vector Machine (SVM) can be conveniently affected by the different parameters of the kernel tricks and the regularization parameter, C. Thus, in this article, we propose a study in order to find the suitable kernel with which SVM may achieve good generalization performance as well as the parameters to use. We need to analyze the behavior of the SVM classifier when these parameters take very small or very large values. The study is conducted for a multi-class vowel recognition using the TIMIT corpus. Furthermore, for the experiments, we used different feature representations such as MFCC and PLP. Finally, a comparative study was done to point out the impact of the choice of the parameters, kernel trick and feature representations on the performance of the SVM classifier version:1
arxiv-1507-05253 | The Population Posterior and Bayesian Inference on Streams | http://arxiv.org/abs/1507.05253 | id:1507.05253 author:James McInerney, Rajesh Ranganath, David M. Blei category:stat.ML  published:2015-07-19 summary:Many modern data analysis problems involve inferences from streaming data. However, streaming data is not easily amenable to the standard probabilistic modeling approaches, which assume that we condition on finite data. We develop population variational Bayes, a new approach for using Bayesian modeling to analyze streams of data. It approximates a new type of distribution, the population posterior, which combines the notion of a population distribution of the data with Bayesian inference in a probabilistic model. We study our method with latent Dirichlet allocation and Dirichlet process mixtures on several large-scale data sets. version:2
arxiv-1507-05950 | On the Worst-Case Approximability of Sparse PCA | http://arxiv.org/abs/1507.05950 | id:1507.05950 author:Siu On Chan, Dimitris Papailiopoulos, Aviad Rubinstein category:stat.ML cs.CC cs.DS cs.LG  published:2015-07-21 summary:It is well known that Sparse PCA (Sparse Principal Component Analysis) is NP-hard to solve exactly on worst-case instances. What is the complexity of solving Sparse PCA approximately? Our contributions include: 1) a simple and efficient algorithm that achieves an $n^{-1/3}$-approximation; 2) NP-hardness of approximation to within $(1-\varepsilon)$, for some small constant $\varepsilon > 0$; 3) SSE-hardness of approximation to within any constant factor; and 4) an $\exp\exp\left(\Omega\left(\sqrt{\log \log n}\right)\right)$ ("quasi-quasi-polynomial") gap for the standard semidefinite program. version:1
arxiv-1507-05936 | The Cumulative Distribution Transform and Linear Pattern Classification | http://arxiv.org/abs/1507.05936 | id:1507.05936 author:Se Rim Park, Soheil Kolouri, Shinjini Kundu, Gustavo Rohde category:cs.CV  published:2015-07-21 summary:Classifying (determining the label) of data emanating from sensors is an important problem with many applications in science and technology. We describe a new transform for patterns that can be interpreted as a probability density function, that has special properties with regards to classification. The transform, which we denote as the Cumulative Distribution Transform (CDT) is invertible, with well defined forward and inverse operations. We show that it can be useful in 'parsing out' variations (confounds) that are 'Lagrangian' (displacement or transport) by converting these to 'Eulerian' variations in transform domain. This conversion is the basis for our main result that describes when the CDT can allow for linear classification to be possible in signal domain. We also describe several properties of the transform and show, with computational experiments that used both real and simulated data, that the CDT can help render a variety of real world problems simpler to solve. version:1
arxiv-1501-07873 | Sketch-a-Net that Beats Humans | http://arxiv.org/abs/1501.07873 | id:1501.07873 author:Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales category:cs.CV cs.NE  published:2015-01-30 summary:We propose a multi-scale multi-channel deep neural network framework that, for the first time, yields sketch recognition performance surpassing that of humans. Our superior performance is a result of explicitly embedding the unique characteristics of sketches in our model: (i) a network architecture designed for sketch rather than natural photo statistics, (ii) a multi-channel generalisation that encodes sequential ordering in the sketching process, and (iii) a multi-scale network ensemble with joint Bayesian fusion that accounts for the different levels of abstraction exhibited in free-hand sketches. We show that state-of-the-art deep networks specifically engineered for photos of natural objects fail to perform well on sketch recognition, regardless whether they are trained using photo or sketch. Our network on the other hand not only delivers the best performance on the largest human sketch dataset to date, but also is small in size making efficient training possible using just CPUs. version:3
arxiv-1507-05880 | A study of the classification of low-dimensional data with supervised manifold learning | http://arxiv.org/abs/1507.05880 | id:1507.05880 author:Elif Vural, Christine Guillemot category:cs.LG  published:2015-07-21 summary:Supervised manifold learning methods learn data representations by preserving the geometric structure of data while enhancing the separation between data samples from different classes. In this paper, we propose a theoretical study of supervised manifold learning for classification. We first focus on the supervised Laplacian eigenmaps algorithm and study the conditions under which this method computes low-dimensional embeddings where different classes become linearly separable. We then consider arbitrary supervised manifold learning algorithms that compute a linearly separable embedding and study the accuracy of the classifiers given by the out-of-sample extensions of these embeddings. We characterize the classification accuracy in terms of several parameters of the classifier such as the separation between different classes in the embedding, the regularity of the interpolation function and the number of training samples. The proposed analysis is supported by experiments on synthetic and real data and has potential for guiding the design of classifiers for intrinsically low-dimensional data. version:1
arxiv-1507-05869 | Kernel convolution model for decoding sounds from time-varying neural responses | http://arxiv.org/abs/1507.05869 | id:1507.05869 author:Ali Faisal, Anni Nora, Jaeho Seol, Hanna Renvall, Riitta Salmelin category:stat.ML q-bio.NC  published:2015-07-21 summary:In this study we present a kernel based convolution model to characterize neural responses to natural sounds by decoding their time-varying acoustic features. The model allows to decode natural sounds from high-dimensional neural recordings, such as magnetoencephalography (MEG), that track timing and location of human cortical signalling noninvasively across multiple channels. We used the MEG responses recorded from subjects listening to acoustically different environmental sounds. By decoding the stimulus frequencies from the responses, our model was able to accurately distinguish between two different sounds that it had never encountered before with 70% accuracy. Convolution models typically decode frequencies that appear at a certain time point in the sound signal by using neural responses from that time point until a certain fixed duration of the response. Using our model, we evaluated several fixed durations (time-lags) of the neural responses and observed auditory MEG responses to be most sensitive to spectral content of the sounds at time-lags of 250 ms to 500 ms. The proposed model should be useful for determining what aspects of natural sounds are represented by high-dimensional neural responses and may reveal novel properties of neural signals. version:1
arxiv-1412-6134 | Data Representation using the Weyl Transform | http://arxiv.org/abs/1412.6134 | id:1412.6134 author:Qiang Qiu, Andrew Thompson, Robert Calderbank, Guillermo Sapiro category:cs.CV stat.ML  published:2014-12-18 summary:The Weyl transform is introduced as a rich framework for data representation. Transform coefficients are connected to the Walsh-Hadamard transform of multiscale autocorrelations, and different forms of dyadic periodicity in a signal are shown to appear as different features in its Weyl coefficients. The Weyl transform has a high degree of symmetry with respect to a large group of multiscale transformations, which allows compact yet discriminative representations to be obtained by pooling coefficients. The effectiveness of the Weyl transform is demonstrated through the example of textured image classification. version:5
arxiv-1507-05819 | Detecting Internet Filtering from Geographic Time Series | http://arxiv.org/abs/1507.05819 | id:1507.05819 author:Joss Wright, Alexander Darer, Oliver Farnan category:cs.CY cs.LG cs.NI  published:2015-07-21 summary:We propose an approach based on principle component analysis to identify per-country anomalous periods in traffic usage as a means to detect internet filtering, and demonstrate the applicability of this approach with global usage statistics from the Tor Project. In contrast to previous country-specific investigations, our techniques use deviation from global patterns of usage to identify countries straying from predicted behaviour, allowing the identification of periods of filtering and related events in any country for which usage statistics exist. To our knowledge the work presented here is the first automated approach to detecting internet filtering at a global scale. We demonstrate the applicability of our approach by identifying known historical filtering events as well as events injected synthetically into a dataset, and evaluate the sensitivity of this technique against different classes of censorship events. Importantly, our results show that usage of circumvention tools, such as those provided by the Tor Project, act not only as direct indicators of network censorship but also as a meaningful proxy variable for related events such as protests in which internet use is restricted. version:1
arxiv-1507-05800 | Bandit-Based Task Assignment for Heterogeneous Crowdsourcing | http://arxiv.org/abs/1507.05800 | id:1507.05800 author:Hao Zhang, Yao Ma, Masashi Sugiyama category:cs.LG  published:2015-07-21 summary:We consider a task assignment problem in crowdsourcing, which is aimed at collecting as many reliable labels as possible within a limited budget. A challenge in this scenario is how to cope with the diversity of tasks and the task-dependent reliability of workers, e.g., a worker may be good at recognizing the name of sports teams, but not be familiar with cosmetics brands. We refer to this practical setting as heterogeneous crowdsourcing. In this paper, we propose a contextual bandit formulation for task assignment in heterogeneous crowdsourcing, which is able to deal with the exploration-exploitation trade-off in worker selection. We also theoretically investigate the regret bounds for the proposed method, and demonstrate its practical usefulness experimentally. version:1
arxiv-1501-02555 | Tri-Subject Kinship Verification: Understanding the Core of A Family | http://arxiv.org/abs/1501.02555 | id:1501.02555 author:Xiaoqian Qin, Xiaoyang Tan, Songcan Chen category:cs.CV  published:2015-01-12 summary:One major challenge in computer vision is to go beyond the modeling of individual objects and to investigate the bi- (one-versus-one) or tri- (one-versus-two) relationship among multiple visual entities, answering such questions as whether a child in a photo belongs to given parents. The child-parents relationship plays a core role in a family and understanding such kin relationship would have fundamental impact on the behavior of an artificial intelligent agent working in the human world. In this work, we tackle the problem of one-versus-two (tri-subject) kinship verification and our contributions are three folds: 1) a novel relative symmetric bilinear model (RSBM) introduced to model the similarity between the child and the parents, by incorporating the prior knowledge that a child may resemble a particular parent more than the other; 2) a spatially voted method for feature selection, which jointly selects the most discriminative features for the child-parents pair, while taking local spatial information into account; 3) a large scale tri-subject kinship database characterized by over 1,000 child-parents families. Extensive experiments on KinFaceW, Family101 and our newly released kinship database show that the proposed method outperforms several previous state of the art methods, while could also be used to significantly boost the performance of one-versus-one kinship verification when the information about both parents are available. version:3
arxiv-1507-05781 | Gradient Importance Sampling | http://arxiv.org/abs/1507.05781 | id:1507.05781 author:Ingmar Schuster category:stat.ML  published:2015-07-21 summary:Adaptive Monte Carlo schemes developed over the last years usually seek to ensure ergodicity of the sampling process in line with MCMC tradition. This poses constraints on what is possible in terms of adaptation. In the general case ergodicity can only be guaranteed if adaptation is diminished at a certain rate. Importance Sampling approaches offer a way to circumvent this limitation and design sampling algorithms that keep adapting. Here I present a gradient informed variant of SMC (and its special case Population Monte Carlo) for static problems. version:1
arxiv-1507-02379 | Understanding Intra-Class Knowledge Inside CNN | http://arxiv.org/abs/1507.02379 | id:1507.02379 author:Donglai Wei, Bolei Zhou, Antonio Torrabla, William Freeman category:cs.CV  published:2015-07-09 summary:Convolutional Neural Network (CNN) has been successful in image recognition tasks, and recent works shed lights on how CNN separates different classes with the learned inter-class knowledge through visualization. In this work, we instead visualize the intra-class knowledge inside CNN to better understand how an object class is represented in the fully-connected layers. To invert the intra-class knowledge into more interpretable images, we propose a non-parametric patch prior upon previous CNN visualization models. With it, we show how different "styles" of templates for an object class are organized by CNN in terms of location and content, and represented in a hierarchical and ensemble way. Moreover, such intra-class knowledge can be used in many interesting applications, e.g. style-based image retrieval and style-based object completion. version:2
arxiv-1507-05737 | Online Metric-Weighted Linear Representations for Robust Visual Tracking | http://arxiv.org/abs/1507.05737 | id:1507.05737 author:Xi Li, Chunhua Shen, Anthony Dick, Zhongfei Zhang, Yueting Zhuang category:cs.CV  published:2015-07-21 summary:In this paper, we propose a visual tracker based on a metric-weighted linear representation of appearance. In order to capture the interdependence of different feature dimensions, we develop two online distance metric learning methods using proximity comparison information and structured output learning. The learned metric is then incorporated into a linear representation of appearance. We show that online distance metric learning significantly improves the robustness of the tracker, especially on those sequences exhibiting drastic appearance changes. In order to bound growth in the number of training samples, we design a time-weighted reservoir sampling method. Moreover, we enable our tracker to automatically perform object identification during the process of object tracking, by introducing a collection of static template samples belonging to several object classes of interest. Object identification results for an entire video sequence are achieved by systematically combining the tracking information and visual recognition at each frame. Experimental results on challenging video sequences demonstrate the effectiveness of the method for both inter-frame tracking and object identification. version:1
arxiv-1507-05726 | Rule Of Thumb: Deep derotation for improved fingertip detection | http://arxiv.org/abs/1507.05726 | id:1507.05726 author:Aaron Wetzler, Ron Slossberg, Ron Kimmel category:cs.CV  published:2015-07-21 summary:We investigate a novel global orientation regression approach for articulated objects using a deep convolutional neural network. This is integrated with an in-plane image derotation scheme, DeROT, to tackle the problem of per-frame fingertip detection in depth images. The method reduces the complexity of learning in the space of articulated poses which is demonstrated by using two distinct state-of-the-art learning based hand pose estimation methods applied to fingertip detection. Significant classification improvements are shown over the baseline implementation. Our framework involves no tracking, kinematic constraints or explicit prior model of the articulated object in hand. To support our approach we also describe a new pipeline for high accuracy magnetic annotation and labeling of objects imaged by a depth camera. version:1
arxiv-1507-05720 | Gene expression modelling across multiple cell-lines with MapReduce | http://arxiv.org/abs/1507.05720 | id:1507.05720 author:David M. Budden, Edmund J. Crampin category:q-bio.QM cs.DC q-bio.GN stat.ML  published:2015-07-21 summary:With the wealth of high-throughput sequencing data generated by recent large-scale consortia, predictive gene expression modelling has become an important tool for integrative analysis of transcriptomic and epigenetic data. However, sequencing data-sets are characteristically large, and previously modelling frameworks are typically inefficient and unable to leverage multi-core or distributed processing architectures. In this study, we detail an efficient and parallelised MapReduce implementation of gene expression modelling. We leverage the computational efficiency of this framework to provide an integrative analysis of over fifty histone modification data-sets across a variety of cancerous and non-cancerous cell-lines. Our results demonstrate that the genome-wide relationships between histone modifications and mRNA transcription are lineage, tissue and karyotype-invariant, and that models trained on matched epigenetic/transcriptomic data from non-cancerous cell-lines are able to predict cancerous expression with equivalent genome-wide fidelity. version:1
arxiv-1507-05717 | An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition | http://arxiv.org/abs/1507.05717 | id:1507.05717 author:Baoguang Shi, Xiang Bai, Cong Yao category:cs.CV  published:2015-07-21 summary:Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it. version:1
arxiv-1502-00727 | Macrostate mixture models for probabilistic multiscale nonparametric kernelized spectral clustering | http://arxiv.org/abs/1502.00727 | id:1502.00727 author:Daniel Korenblum category:stat.ML  published:2015-02-03 summary:Automating the discovery of meaningful structures in large complex datasets is an important problem in many application areas including machine learning, source separation, and dimensionality reduction. Mixture models are one category of methods for discovering structure using convex sums of probability distributions to represent structures or clusters in data. Spectral clustering is another category of methods where eigenspaces of Laplacian matrices are used prior to or as part of the clustering process. Macrostate theory defines nonparametric mixture models directly from Laplacian eigensystems, providing a connection between nonhierarchical spectral clustering and nonparametric mixture modeling. Unlike other spectral clustering methods, macrostates are self-contained and predict both the appropriate number of mixture components and the cluster assignment distributions directly from Laplacian eigensystems. Macrostates reduce the number of input parameters and steps required compared to other spectral clustering methods and avoid issues of explicit density estimation in higher dimensional input data spaces. Previous formulations used customized algorithms to compute macrostate clustering solutions, limiting their practical accessibility. The new formulation presented here depends only on standardized linear programming solvers and is very easily parallelized, improving the practicality and performance compared to previous formulations. Numerical examples compare the performance of other finite mixture modeling and spectral clustering methods to macrostate clustering. version:3
arxiv-1507-05695 | A neuromorphic hardware architecture using the Neural Engineering Framework for pattern recognition | http://arxiv.org/abs/1507.05695 | id:1507.05695 author:Runchun Wang, Chetan Singh Thakur, Tara Julia Hamilton, Jonathan Tapson, Andre van Schaik category:cs.NE  published:2015-07-21 summary:We present a hardware architecture that uses the Neural Engineering Framework (NEF) to implement large-scale neural networks on Field Programmable Gate Arrays (FPGAs) for performing pattern recognition in real time. NEF is a framework that is capable of synthesising large-scale cognitive systems from subnetworks. We will first present the architecture of the proposed neural network implemented using fixed-point numbers and demonstrate a routine that computes the decoding weights by using the online pseudoinverse update method (OPIUM) in a parallel and distributed manner. The proposed system is efficiently implemented on a compact digital neural core. This neural core consists of 64 neurons that are instantiated by a single physical neuron using a time-multiplexing approach. As a proof of concept, we combined 128 identical neural cores together to build a handwritten digit recognition system using the MNIST database and achieved a recognition rate of 96.55%. The system is implemented on a state-of-the-art FPGA and can process 5.12 million digits per second. The architecture is not limited to handwriting recognition, but is generally applicable as an extremely fast pattern recognition processor for various kinds of patterns such as speech and images. version:1
arxiv-1506-01113 | Multi-Objective Optimization for Self-Adjusting Weighted Gradient in Machine Learning Tasks | http://arxiv.org/abs/1506.01113 | id:1506.01113 author:Conrado Silva Miranda, Fernando José Von Zuben category:stat.ML cs.LG  published:2015-06-03 summary:Much of the focus in machine learning research is placed in creating new architectures and optimization methods, but the overall loss function is seldom questioned. This paper interprets machine learning from a multi-objective optimization perspective, showing the limitations of the default linear combination of loss functions over a data set and introducing the hypervolume indicator as an alternative. It is shown that the gradient of the hypervolume is defined by a self-adjusting weighted mean of the individual loss gradients, making it similar to the gradient of a weighted mean loss but without requiring the weights to be defined a priori. This enables an inner boosting-like behavior, where the current model is used to automatically place higher weights on samples with higher losses but without requiring the use of multiple models. Results on a denoising autoencoder show that the new formulation is able to achieve better mean loss than the direct optimization of the mean loss, providing evidence to the conjecture that self-adjusting the weights creates a smoother loss surface. version:2
arxiv-1507-05630 | Notes About a More Aware Dependency Parser | http://arxiv.org/abs/1507.05630 | id:1507.05630 author:Matteo Grella category:cs.CL  published:2015-07-20 summary:In this paper I explain the reasons that led me to research and conceive a novel technology for dependency parsing, mixing together the strengths of data-driven transition-based and constraint-based approaches. In particular I highlight the problem to infer the reliability of the results of a data-driven transition-based parser, which is extremely important for high-level processes that expect to use correct parsing results. I then briefly introduce a number of notes about a new parser model I'm working on, capable to proceed with the analysis in a "more aware" way, with a more "robust" concept of robustness. version:1
arxiv-1507-05605 | A semidefinite program for unbalanced multisection in the stochastic block model | http://arxiv.org/abs/1507.05605 | id:1507.05605 author:William Perry, Alexander S. Wein category:cs.DS math.PR stat.ML 68  published:2015-07-20 summary:We analyze semidefinite programming (SDP) algorithms that exactly recover community structure in graphs generated from the stochastic block model. In this model, a graph is randomly generated on a vertex set that is partitioned into multiple communities of potentially different sizes, where edges are more probable within communities than between communities. We achieve exact recovery of the community structure, up to the information-theoretic limits determined by Abbe and Sandon. By virtue of a semidefinite approach, our algorithms succeed against a semirandom form of the stochastic block model, guaranteeing generalization to scenarios with radically different noise structure. version:1
arxiv-1507-05086 | Parallel Correlation Clustering on Big Graphs | http://arxiv.org/abs/1507.05086 | id:1507.05086 author:Xinghao Pan, Dimitris Papailiopoulos, Samet Oymak, Benjamin Recht, Kannan Ramchandran, Michael I. Jordan category:cs.DC cs.DS stat.ML  published:2015-07-17 summary:Given a similarity graph between items, correlation clustering (CC) groups similar items together and dissimilar ones apart. One of the most popular CC algorithms is KwikCluster: an algorithm that serially clusters neighborhoods of vertices, and obtains a 3-approximation ratio. Unfortunately, KwikCluster in practice requires a large number of clustering rounds, a potential bottleneck for large graphs. We present C4 and ClusterWild!, two algorithms for parallel correlation clustering that run in a polylogarithmic number of rounds and achieve nearly linear speedups, provably. C4 uses concurrency control to enforce serializability of a parallel clustering process, and guarantees a 3-approximation ratio. ClusterWild! is a coordination free algorithm that abandons consistency for the benefit of better scaling; this leads to a provably small loss in the 3-approximation ratio. We provide extensive experimental results for both algorithms, where we outperform the state of the art, both in terms of clustering accuracy and running time. We show that our algorithms can cluster billion-edge graphs in under 5 seconds on 32 cores, while achieving a 15x speedup. version:2
arxiv-1502-04754 | 3D Pose from Detections | http://arxiv.org/abs/1502.04754 | id:1502.04754 author:Cosimo Rubino, Marco Crocco, Alessandro Perina, Vittorio Murino, Alessio Del Bue category:cs.CV  published:2015-02-17 summary:We present a novel method to infer, in closed-form, a general 3D spatial occupancy and orientation of a collection of rigid objects given 2D image detections from a sequence of images. In particular, starting from 2D ellipses fitted to bounding boxes, this novel multi-view problem can be reformulated as the estimation of a quadric (ellipsoid) in 3D. We show that an efficient solution exists in the dual-space using a minimum of three views while a solution with two views is possible through the use of regularization. However, this algebraic solution can be negatively affected in the presence of gross inaccuracies in the bounding boxes estimation. To this end, we also propose a robust ellipse fitting algorithm able to improve performance in the presence of errors in the detected objects. Results on synthetic tests and on different real datasets, involving real challenging scenarios, demonstrate the applicability and potential of our method. version:3
arxiv-1507-05578 | Subspace Alignment Based Domain Adaptation for RCNN Detector | http://arxiv.org/abs/1507.05578 | id:1507.05578 author:Anant Raj, Vinay P. Namboodiri, Tinne Tuytelaars category:cs.CV  published:2015-07-20 summary:In this paper, we propose subspace alignment based domain adaptation of the state of the art RCNN based object detector. The aim is to be able to achieve high quality object detection in novel, real world target scenarios without requiring labels from the target domain. While, unsupervised domain adaptation has been studied in the case of object classification, for object detection it has been relatively unexplored. In subspace based domain adaptation for objects, we need access to source and target subspaces for the bounding box features. The absence of supervision (labels and bounding boxes are absent) makes the task challenging. In this paper, we show that we can still adapt sub- spaces that are localized to the object by obtaining detections from the RCNN detector trained on source and applied on target. Then we form localized subspaces from the detections and show that subspace alignment based adaptation between these subspaces yields improved object detection. This evaluation is done by considering challenging real world datasets of PASCAL VOC as source and validation set of Microsoft COCO dataset as target for various categories. version:1
arxiv-1507-05523 | How to Generate a Good Word Embedding? | http://arxiv.org/abs/1507.05523 | id:1507.05523 author:Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao category:cs.CL  published:2015-07-20 summary:We analyze three critical components of word embedding training: the model, the corpus, and the training parameters. We systematize existing neural-network-based word embedding algorithms and compare them using the same corpus. We evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks and using it to initialize neural networks. We also provide several simple guidelines for training word embeddings. First, we discover that corpus domain is more important than corpus size. We recommend choosing a corpus in a suitable domain for the desired task, after that, using a larger corpus yields better results. Second, we find that faster models provide sufficient performance in most cases, and more complex models can be used if the training corpus is sufficiently large. Third, the early stopping metric for iterating should rely on the development set of the desired task rather than the validation loss of training embedding. version:1
arxiv-1507-01890 | Community detection in multiplex networks using locally adaptive random walks | http://arxiv.org/abs/1507.01890 | id:1507.01890 author:Zhana Kuncheva, Giovanni Montana category:cs.SI physics.soc-ph stat.ML  published:2015-07-06 summary:Multiplex networks, a special type of multilayer networks, are increasingly applied in many domains ranging from social media analytics to biology. A common task in these applications concerns the detection of community structures. Many existing algorithms for community detection in multiplexes attempt to detect communities which are shared by all layers. In this article we propose a community detection algorithm, LART (Locally Adaptive Random Transitions), for the detection of communities that are shared by either some or all the layers in the multiplex. The algorithm is based on a random walk on the multiplex, and the transition probabilities defining the random walk are allowed to depend on the local topological similarity between layers at any given node so as to facilitate the exploration of communities across layers. Based on this random walk, a node dissimilarity measure is derived and nodes are clustered based on this distance in a hierarchical fashion. We present experimental results using networks simulated under various scenarios to showcase the performance of LART in comparison to related community detection algorithms. version:2
arxiv-1507-05498 | On the Minimax Risk of Dictionary Learning | http://arxiv.org/abs/1507.05498 | id:1507.05498 author:Alexander Jung, Yonina C. Eldar, Norbert Görtz category:stat.ML cs.IT cs.LG math.IT  published:2015-07-20 summary:We consider the problem of learning a dictionary matrix from a number of observed signals, which are assumed to be generated via a linear model with a common underlying dictionary. In particular, we derive lower bounds on the minimum achievable worst case mean squared error (MSE), regardless of computational complexity of the dictionary learning (DL) schemes. By casting DL as a classical (or frequentist) estimation problem, the lower bounds on the worst case MSE are derived by following an established information-theoretic approach to minimax estimation. The main conceptual contribution of this paper is the adaption of the information-theoretic approach to minimax estimation for the DL problem in order to derive lower bounds on the worst case MSE of any DL scheme. We derive three different lower bounds applying to different generative models for the observed signals. The first bound applies to a wide range of models, it only requires the existence of a covariance matrix of the (unknown) underlying coefficient vector. By specializing this bound to the case of sparse coefficient distributions, and assuming the true dictionary satisfies the restricted isometry property, we obtain a lower bound on the worst case MSE of DL schemes in terms of a signal to noise ratio (SNR). The third bound applies to a more restrictive subclass of coefficient distributions by requiring the non-zero coefficients to be Gaussian. While, compared with the previous two bounds, the applicability of this final bound is the most limited it is the tightest of the three bounds in the low SNR regime. version:1
arxiv-1507-05489 | Efficient moving point handling for incremental 3D manifold reconstruction | http://arxiv.org/abs/1507.05489 | id:1507.05489 author:Andrea Romanoni, Matteo Matteucci category:cs.CV  published:2015-07-20 summary:As incremental Structure from Motion algorithms become effective, a good sparse point cloud representing the map of the scene becomes available frame-by-frame. From the 3D Delaunay triangulation of these points, state-of-the-art algorithms build a manifold rough model of the scene. These algorithms integrate incrementally new points to the 3D reconstruction only if their position estimate does not change. Indeed, whenever a point moves in a 3D Delaunay triangulation, for instance because its estimation gets refined, a set of tetrahedra have to be removed and replaced with new ones to maintain the Delaunay property; the management of the manifold reconstruction becomes thus complex and it entails a potentially big overhead. In this paper we investigate different approaches and we propose an efficient policy to deal with moving points in the manifold estimation process. We tested our approach with four sequences of the KITTI dataset and we show the effectiveness of our proposal in comparison with state-of-the-art approaches. version:1
arxiv-1502-03529 | Scalable Stochastic Alternating Direction Method of Multipliers | http://arxiv.org/abs/1502.03529 | id:1502.03529 author:Shen-Yi Zhao, Wu-Jun Li, Zhi-Hua Zhou category:cs.LG  published:2015-02-12 summary:Stochastic alternating direction method of multipliers (ADMM), which visits only one sample or a mini-batch of samples each time, has recently been proved to achieve better performance than batch ADMM. However, most stochastic methods can only achieve a convergence rate $O(1/\sqrt T)$ on general convex problems,where T is the number of iterations. Hence, these methods are not scalable with respect to convergence rate (computation cost). There exists only one stochastic method, called SA-ADMM, which can achieve convergence rate $O(1/T)$ on general convex problems. However, an extra memory is needed for SA-ADMM to store the historic gradients on all samples, and thus it is not scalable with respect to storage cost. In this paper, we propose a novel method, called scalable stochastic ADMM(SCAS-ADMM), for large-scale optimization and learning problems. Without the need to store the historic gradients, SCAS-ADMM can achieve the same convergence rate $O(1/T)$ as the best stochastic method SA-ADMM and batch ADMM on general convex problems. Experiments on graph-guided fused lasso show that SCAS-ADMM can achieve state-of-the-art performance in real applications version:3
arxiv-1504-02723 | Bayesian Inference of Graphical Model Structures Using Trees | http://arxiv.org/abs/1504.02723 | id:1504.02723 author:Loïc Schwaller, Stéphane Robin, Michael Stumpf category:stat.ML  published:2015-04-10 summary:We propose to learn the structure of an undirected graphical model by computing exact posterior probabilities for local structures in a Bayesian framework. This task would be untractable without any restriction on the considered graphs. We limit our exploration to the spanning trees and define priors on tree structures and parameters that allow fast and exact computation of the posterior probability for an edge to belong to the random tree thanks to an algebraic result called the Matrix-Tree theorem. We show that the assumption we have made does not prevent our approach to perform well on synthetic and flow cytometry data. version:3
arxiv-1507-02385 | Towards Effective Codebookless Model for Image Classification | http://arxiv.org/abs/1507.02385 | id:1507.02385 author:Qilong Wang, Peihua Li, Lei Zhang, Wangmeng Zuo category:cs.CV  published:2015-07-09 summary:The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a high dimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency of our CLM method. version:2
arxiv-1507-05370 | Linear Inverse Problems with Norm and Sparsity Constraints | http://arxiv.org/abs/1507.05370 | id:1507.05370 author:Volkan Cevher, Sina Jafarpour, Anastasios Kyrillidis category:cs.IT math.IT math.OC stat.ML  published:2015-07-20 summary:We describe two nonconventional algorithms for linear regression, called GAME and CLASH. The salient characteristics of these approaches is that they exploit the convex $\ell_1$-ball and non-convex $\ell_0$-sparsity constraints jointly in sparse recovery. To establish the theoretical approximation guarantees of GAME and CLASH, we cover an interesting range of topics from game theory, convex and combinatorial optimization. We illustrate that these approaches lead to improved theoretical guarantees and empirical performance beyond convex and non-convex solvers alone. version:1
arxiv-1507-05367 | Structured Sparsity: Discrete and Convex approaches | http://arxiv.org/abs/1507.05367 | id:1507.05367 author:Anastasios Kyrillidis, Luca Baldassarre, Marwa El-Halabi, Quoc Tran-Dinh, Volkan Cevher category:cs.IT math.IT math.OC stat.ML  published:2015-07-20 summary:Compressive sensing (CS) exploits sparsity to recover sparse or compressible signals from dimensionality reducing, non-adaptive sensing mechanisms. Sparsity is also used to enhance interpretability in machine learning and statistics applications: While the ambient dimension is vast in modern data analysis problems, the relevant information therein typically resides in a much lower dimensional space. However, many solutions proposed nowadays do not leverage the true underlying structure. Recent results in CS extend the simple sparsity idea to more sophisticated {\em structured} sparsity models, which describe the interdependency between the nonzero components of a signal, allowing to increase the interpretability of the results and lead to better recovery performance. In order to better understand the impact of structured sparsity, in this chapter we analyze the connections between the discrete models and their convex relaxations, highlighting their relative advantages. We start with the general group sparse model and then elaborate on two important special cases: the dispersive and the hierarchical models. For each, we present the models in their discrete nature, discuss how to solve the ensuing discrete problems and then describe convex relaxations. We also consider more general structures as defined by set functions and present their convex proxies. Further, we discuss efficient optimization solutions for structured sparsity problems and illustrate structured sparsity in action via three applications. version:1
arxiv-1507-05348 | Learning Complexity-Aware Cascades for Deep Pedestrian Detection | http://arxiv.org/abs/1507.05348 | id:1507.05348 author:Zhaowei Cai, Mohammad Saberian, Nuno Vasconcelos category:cs.CV  published:2015-07-19 summary:The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds. version:1
arxiv-1507-05331 | Fast Adaptive Weight Noise | http://arxiv.org/abs/1507.05331 | id:1507.05331 author:Justin Bayer, Maximilian Karl, Daniela Korhammer, Patrick van der Smagt category:stat.ML cs.LG  published:2015-07-19 summary:Marginalising out uncertain quantities within the internal representations or parameters of neural networks is of central importance for a wide range of learning techniques, such as empirical, variational or full Bayesian methods. We set out to generalise fast dropout (Wang & Manning, 2013) to cover a wider variety of noise processes in neural networks. This leads to an efficient calculation of the marginal likelihood and predictive distribution which evades sampling and the consequential increase in training time due to highly variant gradient estimates. This allows us to approximate variational Bayes for the parameters of feed-forward neural networks. Inspired by the minimum description length principle, we also propose and experimentally verify the direct optimisation of the regularised predictive distribution. The methods yield results competitive with previous neural network based approaches and Gaussian processes on a wide range of regression tasks. version:1
arxiv-1507-05307 | 2 Notes on Classes with Vapnik-Chervonenkis Dimension 1 | http://arxiv.org/abs/1507.05307 | id:1507.05307 author:Shai Ben-David category:cs.LG G.2; G.3  published:2015-07-19 summary:The Vapnik-Chervonenkis dimension is a combinatorial parameter that reflects the "complexity" of a set of sets (a.k.a. concept classes). It has been introduced by Vapnik and Chervonenkis in their seminal 1971 paper and has since found many applications, most notably in machine learning theory and in computational geometry. Arguably the most influential consequence of the VC analysis is the fundamental theorem of statistical machine learning, stating that a concept class is learnable (in some precise sense) if and only if its VC-dimension is finite. Furthermore, for such classes a most simple learning rule - empirical risk minimization (ERM) - is guaranteed to succeed. The simplest non-trivial structures, in terms of the VC-dimension, are the classes (i.e., sets of subsets) for which that dimension is 1. In this note we show a couple of curious results concerning such classes. The first result shows that such classes share a very simple structure, and, as a corollary, the labeling information contained in any sample labeled by such a class can be compressed into a single instance. The second result shows that due to some subtle measurability issues, in spite of the above mentioned fundamental theorem, there are classes of dimension 1 for which an ERM learning rule fails miserably. version:1
arxiv-1507-05244 | Handwriting Recognition | http://arxiv.org/abs/1507.05244 | id:1507.05244 author:Jayati Ghosh Dastidar, Surabhi Sarkar, Rick Punyadyuti Sinha, Kasturi Basu category:cs.CV  published:2015-07-19 summary:This paper describes the method to recognize offline handwritten characters. A robust algorithm for handwriting segmentation is described here with the help of which individual characters can be segmented from a selected word from a paragraph of handwritten text image which is given as input. version:1
arxiv-1507-05243 | Hand Gesture Recognition Library | http://arxiv.org/abs/1507.05243 | id:1507.05243 author:Jonathan Fidelis Paul, Dibyabiva Seth, Cijo Paul, Jayati Ghosh Dastidar category:cs.CV  published:2015-07-19 summary:In this paper we have presented a hand gesture recognition library. Various functions include detecting cluster count, cluster orientation, finger pointing direction, etc. To use these functions first the input image needs to be processed into a logical array for which a function has been developed. The library has been developed keeping flexibility in mind and thus provides application developers a wide range of options to develop custom gestures. version:1
arxiv-1504-03991 | Theory of Dual-sparse Regularized Randomized Reduction | http://arxiv.org/abs/1504.03991 | id:1504.03991 author:Tianbao Yang, Lijun Zhang, Rong Jin, Shenghuo Zhu category:cs.LG stat.ML  published:2015-04-15 summary:In this paper, we study randomized reduction methods, which reduce high-dimensional features into low-dimensional space by randomized methods (e.g., random projection, random hashing), for large-scale high-dimensional classification. Previous theoretical results on randomized reduction methods hinge on strong assumptions about the data, e.g., low rank of the data matrix or a large separable margin of classification, which hinder their applications in broad domains. To address these limitations, we propose dual-sparse regularized randomized reduction methods that introduce a sparse regularizer into the reduced dual problem. Under a mild condition that the original dual solution is a (nearly) sparse vector, we show that the resulting dual solution is close to the original dual solution and concentrates on its support set. In numerical experiments, we present an empirical study to support the analysis and we also present a novel application of the dual-sparse regularized randomized reduction methods to reducing the communication cost of distributed learning from large-scale high-dimensional data. version:4
arxiv-1507-05185 | Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees | http://arxiv.org/abs/1507.05185 | id:1507.05185 author:Tianbao Yang, Lijun Zhang, Qihang Lin, Rong Jin category:math.ST cs.CC stat.ML stat.TH  published:2015-07-18 summary:In this paper, we study a fast approximation method for {\it large-scale high-dimensional} sparse least-squares regression problem by exploiting the Johnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional vectors into a low-dimensional space. In particular, we propose to apply the JL transforms to the data matrix and the target vector and then to solve a sparse least-squares problem on the compressed data with a {\it slightly larger regularization parameter}. Theoretically, we establish the optimization error bound of the learned model for two different sparsity-inducing regularizers, i.e., the elastic net and the $\ell_1$ norm. Compared with previous relevant work, our analysis is {\it non-asymptotic and exhibits more insights} on the bound, the sample complexity and the regularization. As an illustration, we also provide an error bound of the {\it Dantzig selector} under JL transforms. version:1
arxiv-1507-05181 | The Mondrian Process for Machine Learning | http://arxiv.org/abs/1507.05181 | id:1507.05181 author:Matej Balog, Yee Whye Teh category:stat.ML cs.LG  published:2015-07-18 summary:This report is concerned with the Mondrian process and its applications in machine learning. The Mondrian process is a guillotine-partition-valued stochastic process that possesses an elegant self-consistency property. The first part of the report uses simple concepts from applied probability to define the Mondrian process and explore its properties. The Mondrian process has been used as the main building block of a clever online random forest classification algorithm that turns out to be equivalent to its batch counterpart. We outline a slight adaptation of this algorithm to regression, as the remainder of the report uses regression as a case study of how Mondrian processes can be utilized in machine learning. In particular, the Mondrian process will be used to construct a fast approximation to the computationally expensive kernel ridge regression problem with a Laplace kernel. The complexity of random guillotine partitions generated by a Mondrian process and hence the complexity of the resulting regression models is controlled by a lifetime hyperparameter. It turns out that these models can be efficiently trained and evaluated for all lifetimes in a given range at once, without needing to retrain them from scratch for each lifetime value. This leads to an efficient procedure for determining the right model complexity for a dataset at hand. The limitation of having a single lifetime hyperparameter will motivate the final Mondrian grid model, in which each input dimension is endowed with its own lifetime parameter. In this model we preserve the property that its hyperparameters can be tweaked without needing to retrain the modified model from scratch. version:1
arxiv-1507-03148 | Face Alignment Assisted by Head Pose Estimation | http://arxiv.org/abs/1507.03148 | id:1507.03148 author:Heng Yang, Wenxuan Mou, Yichi Zhang, Ioannis Patras, Hatice Gunes, Peter Robinson category:cs.CV  published:2015-07-11 summary:In this paper we propose a supervised initialization scheme for cascaded face alignment based on explicit head pose estimation. We first investigate the failure cases of most state of the art face alignment approaches and observe that these failures often share one common global property, i.e. the head pose variation is usually large. Inspired by this, we propose a deep convolutional network model for reliable and accurate head pose estimation. Instead of using a mean face shape, or randomly selected shapes for cascaded face alignment initialisation, we propose two schemes for generating initialisation: the first one relies on projecting a mean 3D face shape (represented by 3D facial landmarks) onto 2D image under the estimated head pose; the second one searches nearest neighbour shapes from the training set according to head pose distance. By doing so, the initialisation gets closer to the actual shape, which enhances the possibility of convergence and in turn improves the face alignment performance. We demonstrate the proposed method on the benchmark 300W dataset and show very competitive performance in both head pose estimation and face alignment. version:2
arxiv-1411-7718 | Classification with Noisy Labels by Importance Reweighting | http://arxiv.org/abs/1411.7718 | id:1411.7718 author:Tongliang Liu, Dacheng Tao category:stat.ML cs.LG  published:2014-11-27 summary:In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability $\rho\in[0,0.5)$, and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate $\rho$. We show that the rate is upper bounded by the conditional probability $P(y x)$ of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods. version:2
arxiv-1507-05134 | Persistent Topology of Syntax | http://arxiv.org/abs/1507.05134 | id:1507.05134 author:Alexander Port, Iulia Gheorghita, Daniel Guth, John M. Clark, Crystal Liang, Shival Dasu, Matilde Marcolli category:cs.CL math.AT 91F20  published:2015-07-18 summary:We study the persistent homology of the data set of syntactic parameters of the world languages. We show that, while homology generators behave erratically over the whole data set, non-trivial persistent homology appears when one restricts to specific language families. Different families exhibit different persistent homology. We focus on the cases of the Indo-European and the Niger-Congo families, for which we compare persistent homology over different cluster filtering values. We investigate the possible significance, in historical linguistic terms, of the presence of persistent generators of the first homology. In particular, we show that the persistent first homology generator we find in the Indo-European family is not due (as one might guess) to the Anglo-Norman bridge in the Indo-European phylogenetic network, but is related to the position of Ancient Greek and the Hellenic branch within the network. version:1
arxiv-1507-05117 | Fast Approximate Bayesian Computation for Estimating Parameters in Differential Equations | http://arxiv.org/abs/1507.05117 | id:1507.05117 author:Sanmitra Ghosh, Srinandan Dasmahapatra, Koushik Maharatna category:stat.ML  published:2015-07-17 summary:Approximate Bayesian computation (ABC) using a sequential Monte Carlo method provides a comprehensive platform for parameter estimation, model selection and sensitivity analysis in differential equations. However, this method, like other Monte Carlo methods, incurs a significant computational cost as it requires explicit numerical integration of differential equations to carry out inference. In this paper we propose a novel method for circumventing the requirement of explicit integration by using derivatives of Gaussian processes to smooth the observations from which parameters are estimated. We evaluate our methods using synthetic data generated from model biological systems described by ordinary and delay differential equations. Upon comparing the performance of our method to existing ABC techniques, we demonstrate that it produces comparably reliable parameter estimates at a significantly reduced execution time. version:1
arxiv-1412-7149 | Deep Fried Convnets | http://arxiv.org/abs/1412.7149 | id:1412.7149 author:Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang category:cs.LG cs.NE stat.ML  published:2014-12-22 summary:The fully connected layers of a deep convolutional neural network typically contain over 90% of the network parameters, and consume the majority of the memory required to store the network parameters. Reducing the number of parameters while preserving essentially the same predictive performance is critically important for operating deep neural networks in memory constrained environments such as GPUs or embedded devices. In this paper we show how kernel methods, in particular a single Fastfood layer, can be used to replace all fully connected layers in a deep convolutional neural network. This novel Fastfood layer is also end-to-end trainable in conjunction with convolutional layers, allowing us to combine them into a new architecture, named deep fried convolutional networks, which substantially reduces the memory footprint of convolutional networks trained on MNIST and ImageNet with no drop in predictive performance. version:4
arxiv-1507-05087 | Type I and Type II Bayesian Methods for Sparse Signal Recovery using Scale Mixtures | http://arxiv.org/abs/1507.05087 | id:1507.05087 author:Ritwik Giri, Bhaskar D. Rao category:cs.LG stat.ML  published:2015-07-17 summary:In this paper, we propose a generalized scale mixture family of distributions, namely the Power Exponential Scale Mixture (PESM) family, to model the sparsity inducing priors currently in use for sparse signal recovery (SSR). We show that the successful and popular methods such as LASSO, Reweighted $\ell_1$ and Reweighted $\ell_2$ methods can be formulated in an unified manner in a maximum a posteriori (MAP) or Type I Bayesian framework using an appropriate member of the PESM family as the sparsity inducing prior. In addition, exploiting the natural hierarchical framework induced by the PESM family, we utilize these priors in a Type II framework and develop the corresponding EM based estimation algorithms. Some insight into the differences between Type I and Type II methods is provided and of particular interest in the algorithmic development is the Type II variant of the popular and successful reweighted $\ell_1$ method. Extensive empirical results are provided and they show that the Type II methods exhibit better support recovery than the corresponding Type I methods. version:1
arxiv-1507-05073 | Sequential Quantiles via Hermite Series Density Estimation | http://arxiv.org/abs/1507.05073 | id:1507.05073 author:Michael Stephanou, Melvin Varughese, Iain Macdonald category:stat.CO stat.ML  published:2015-07-17 summary:Sequential quantile estimation refers to incorporating observations into quantile estimates in an incremental fashion thus furnishing an online estimate of one or more quantiles at any given point in time. Sequential quantile estimation is also known as online quantile estimation. This area is relevant to the analysis of data streams and to the one-pass analysis of massive data sets. Applications include network traffic and latency analysis, real time fraud detection and high frequency trading. We introduce new techniques for sequential quantile estimation based on Hermite series estimators in the settings of static quantile estimation and dynamic quantile estimation. In the static quantile estimation setting we apply the existing Gauss-Hermite expansion in a novel manner. In particular, we exploit the fact that Gauss-Hermite coefficients can be updated in a sequential manner. To treat dynamic quantile estimation we introduce a novel expansion with an exponentially weighted estimator for the Gauss-Hermite coefficients which we term the Exponentially Weighted Gauss-Hermite (EWGH) expansion. These algorithms go beyond existing sequential quantile estimation algorithms in that they allow arbitrary quantiles (as opposed to pre-specified quantiles) to be estimated at any point in time. In doing so we solve the more general problem of estimating probability densities and cumulative distribution functions on data streams. In particular we derive an analytical expression for the CDF and prove consistency results for the PDF and CDF under certain conditions. Simulation studies and tests on real data reveal the Gauss-Hermite based algorithms to be competitive with a leading existing algorithm. version:1
arxiv-1507-05053 | Massively Deep Artificial Neural Networks for Handwritten Digit Recognition | http://arxiv.org/abs/1507.05053 | id:1507.05053 author:Keiron O'Shea category:cs.CV cs.LG cs.NE  published:2015-07-17 summary:Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on the famous MNIST database of handwritten digits. All that was required to achieve this result was a high number of hidden layers consisting of many neurons, and a graphics card to greatly speed up the rate of learning. version:1
arxiv-1412-1619 | Fast Rates by Transferring from Auxiliary Hypotheses | http://arxiv.org/abs/1412.1619 | id:1412.1619 author:Ilja Kuzborskij, Francesco Orabona category:cs.LG  published:2014-12-04 summary:In this work we consider the learning setting where, in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks. We focus on a broad class of ERM-based linear algorithms that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer. We establish generalization and excess risk bounds, showing that, if the algorithm is fed with a good combination of source hypotheses, generalization happens at the fast rate $\mathcal{O}(1/m)$ instead of the usual $\mathcal{O}(1/\sqrt{m})$. On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate. As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works. version:3
arxiv-1507-05033 | Classification of Complex Wishart Matrices with a Diffusion-Reaction System guided by Stochastic Distances | http://arxiv.org/abs/1507.05033 | id:1507.05033 author:Luis Gomez, Luis Alvarez, Luis Mazorra, Alejandro C. Frery category:cs.CV  published:2015-07-17 summary:We propose a new method for PolSAR (Polarimetric Synthetic Aperture Radar) imagery classification based on stochastic distances in the space of random matrices obeying complex Wishart distributions. Given a collection of prototypes $\{Z_m\}_{m=1}^M$ and a stochastic distance $d(.,.)$, we classify any random matrix $X$ using two criteria in an iterative setup. Firstly, we associate $X$ to the class which minimizes the weighted stochastic distance $w_md(X,Z_m)$, where the positive weights $w_m$ are computed to maximize the class discrimination power. Secondly, we improve the result by embedding the classification problem into a diffusion-reaction partial differential system where the diffusion term smooths the patches within the image, and the reaction term tends to move the pixel values towards the closest class prototype. In particular, the method inherits the benefits of speckle reduction by diffusion-like methods. Results on synthetic and real PolSAR data show the performance of the method. version:1
arxiv-1507-04997 | FRULER: Fuzzy Rule Learning through Evolution for Regression | http://arxiv.org/abs/1507.04997 | id:1507.04997 author:I. Rodríguez-Fdez, M. Mucientes, A. Bugarín category:cs.LG cs.AI stat.ML  published:2015-07-17 summary:In regression problems, the use of TSK fuzzy systems is widely extended due to the precision of the obtained models. Moreover, the use of simple linear TSK models is a good choice in many real problems due to the easy understanding of the relationship between the output and input variables. In this paper we present FRULER, a new genetic fuzzy system for automatically learning accurate and simple linguistic TSK fuzzy rule bases for regression problems. In order to reduce the complexity of the learned models while keeping a high accuracy, the algorithm consists of three stages: instance selection, multi-granularity fuzzy discretization of the input variables, and the evolutionary learning of the rule base that uses the Elastic Net regularization to obtain the consequents of the rules. Each stage was validated using 28 real-world datasets and FRULER was compared with three state of the art enetic fuzzy systems. Experimental results show that FRULER achieves the most accurate and simple models compared even with approximative approaches. version:1
arxiv-1507-04913 | Tree-based Visualization and Optimization for Image Collection | http://arxiv.org/abs/1507.04913 | id:1507.04913 author:Xintong Han, Chongyang Zhang, Weiyao Lin, Mingliang Xu, Bin Sheng, Tao Mei category:cs.MM cs.AI cs.CV  published:2015-07-17 summary:The visualization of an image collection is the process of displaying a collection of images on a screen under some specific layout requirements. This paper focuses on an important problem that is not well addressed by the previous methods: visualizing image collections into arbitrary layout shapes while arranging images according to user-defined semantic or visual correlations (e.g., color or object category). To this end, we first propose a property-based tree construction scheme to organize images of a collection into a tree structure according to user-defined properties. In this way, images can be adaptively placed with the desired semantic or visual correlations in the final visualization layout. Then, we design a two-step visualization optimization scheme to further optimize image layouts. As a result, multiple layout effects including layout shape and image overlap ratio can be effectively controlled to guarantee a satisfactory visualization. Finally, we also propose a tree-transfer scheme such that visualization layouts can be adaptively changed when users select different "images of interest". We demonstrate the effectiveness of our proposed approach through the comparisons with state-of-the-art visualization techniques. version:1
arxiv-1507-04910 | Lower Bounds for Multi-armed Bandit with Non-equivalent Multiple Plays | http://arxiv.org/abs/1507.04910 | id:1507.04910 author:Aleksandr Vorobev, Gleb Gusev category:cs.LG  published:2015-07-17 summary:We study the stochastic multi-armed bandit problem with non-equivalent multiple plays where, at each step, an agent chooses not only a set of arms, but also their order, which influences reward distribution. In several problem formulations with different assumptions, we provide lower bounds for regret with standard asymptotics $O(\log{t})$ but novel coefficients and provide optimal algorithms, thus proving that these bounds cannot be improved. version:1
arxiv-1507-04908 | Analysis of the South Slavic Scripts by Run-Length Features of the Image Texture | http://arxiv.org/abs/1507.04908 | id:1507.04908 author:Darko Brodic, Zoran N. Milivojevic, Alessia Amelio category:cs.CV cs.CL  published:2015-07-17 summary:The paper proposes an algorithm for the script recognition based on the texture characteristics. The image texture is achieved by coding each letter with the equivalent script type (number code) according to its position in the text line. Each code is transformed into equivalent gray level pixel creating an 1-D image. Then, the image texture is subjected to the run-length analysis. This analysis extracts the run-length features, which are classified to make a distinction between the scripts under consideration. In the experiment, a custom oriented database is subject to the proposed algorithm. The database consists of some text documents written in Cyrillic, Latin and Glagolitic scripts. Furthermore, it is divided into training and test parts. The results of the experiment show that 3 out of 5 run-length features can be used for effective differentiation between the analyzed South Slavic scripts. version:1
arxiv-1507-04844 | Learning Robust Deep Face Representation | http://arxiv.org/abs/1507.04844 | id:1507.04844 author:Xiang Wu category:cs.CV  published:2015-07-17 summary:With the development of convolution neural network, more and more researchers focus their attention on the advantage of CNN for face recognition task. In this paper, we propose a deep convolution network for learning a robust face representation. The deep convolution net is constructed by 4 convolution layers, 4 max pooling layers and 2 fully connected layers, which totally contains about 4M parameters. The Max-Feature-Map activation function is used instead of ReLU because the ReLU might lead to the loss of information due to the sparsity while the Max-Feature-Map can get the compact and discriminative feature vectors. The model is trained on CASIA-WebFace dataset and evaluated on LFW dataset. The result on LFW achieves 97.77% on unsupervised setting for single net. version:1
arxiv-1501-06209 | Parallel Magnetic Resonance Imaging | http://arxiv.org/abs/1501.06209 | id:1501.06209 author:Martin Uecker category:cs.NA cs.CV math.NA math.OC physics.med-ph  published:2015-01-25 summary:The main disadvantage of Magnetic Resonance Imaging (MRI) are its long scan times and, in consequence, its sensitivity to motion. Exploiting the complementary information from multiple receive coils, parallel imaging is able to recover images from under-sampled k-space data and to accelerate the measurement. Because parallel magnetic resonance imaging can be used to accelerate basically any imaging sequence it has many important applications. Parallel imaging brought a fundamental shift in image reconstruction: Image reconstruction changed from a simple direct Fourier transform to the solution of an ill-conditioned inverse problem. This work gives an overview of image reconstruction from the perspective of inverse problems. After introducing basic concepts such as regularization, discretization, and iterative reconstruction, advanced topics are discussed including algorithms for auto-calibration, the connection to approximation theory, and the combination with compressed sensing. version:2
arxiv-1507-04835 | Multiscale Adaptive Representation of Signals: I. The Basic Framework | http://arxiv.org/abs/1507.04835 | id:1507.04835 author:Cheng Tai, Weinan E category:cs.CV  published:2015-07-17 summary:We introduce a framework for designing multi-scale, adaptive, shift-invariant frames and bi-frames for representing signals. The new framework, called AdaFrame, improves over dictionary learning-based techniques in terms of computational efficiency at inference time. It improves classical multi-scale basis such as wavelet frames in terms of coding efficiency. It provides an attractive alternative to dictionary learning-based techniques for low level signal processing tasks, such as compression and denoising, as well as high level tasks, such as feature extraction for object recognition. Connections with deep convolutional networks are also discussed. In particular, the proposed framework reveals a drawback in the commonly used approach for visualizing the activations of the intermediate layers in convolutional networks, and suggests a natural alternative. version:1
arxiv-1507-04831 | Deep Multimodal Speaker Naming | http://arxiv.org/abs/1507.04831 | id:1507.04831 author:Yongtao Hu, Jimmy Ren, Jingwen Dai, Chang Yuan, Li Xu, Wenping Wang category:cs.CV cs.LG cs.MM cs.SD H.3  published:2015-07-17 summary:Automatic speaker naming is the problem of localizing as well as identifying each speaking character in a TV/movie/live show video. This is a challenging problem mainly attributes to its multimodal nature, namely face cue alone is insufficient to achieve good performance. Previous multimodal approaches to this problem usually process the data of different modalities individually and merge them using handcrafted heuristics. Such approaches work well for simple scenes, but fail to achieve high performance for speakers with large appearance variations. In this paper, we propose a novel convolutional neural networks (CNN) based learning framework to automatically learn the fusion function of both face and audio cues. We show that without using face tracking, facial landmark localization or subtitle/transcript, our system with robust multimodal feature extraction is able to achieve state-of-the-art speaker naming performance evaluated on two diverse TV series. The dataset and implementation of our algorithm are publicly available online. version:1
arxiv-1507-04816 | RBIR Based on Signature Graph | http://arxiv.org/abs/1507.04816 | id:1507.04816 author:Thanh The Van, Thanh Manh Le category:cs.CV  published:2015-07-17 summary:This paper approaches the image retrieval system on the base of visual features local region RBIR (region-based image retrieval). First of all, the paper presents a method for extracting the interest points based on Harris-Laplace to create the feature region of the image. Next, in order to reduce the storage space and speed up query image, the paper builds the binary signature structure to describe the visual content of image. Based on the image's binary signature, the paper builds the SG (signature graph) to classify and store image's binary signatures. Since then, the paper builds the image retrieval algorithm on SG through the similar measure EMD (earth mover's distance) between the image's binary signatures. Last but not least, the paper gives an image retrieval model RBIR, experiments and assesses the image retrieval method on Corel image database over 10,000 images. version:1
arxiv-1506-08956 | Lens Factory: Automatic Lens Generation Using Off-the-shelf Components | http://arxiv.org/abs/1506.08956 | id:1506.08956 author:Libin Sun, Brian Guenter, Neel Joshi, Patrick Therien, James Hays category:cs.GR cs.CV  published:2015-06-30 summary:Custom optics is a necessity for many imaging applications. Unfortunately, custom lens design is costly (thousands to tens of thousands of dollars), time consuming (10-12 weeks typical lead time), and requires specialized optics design expertise. By using only inexpensive, off-the-shelf lens components the Lens Factory automatic design system greatly reduces cost and time. Design, ordering of parts, delivery, and assembly can be completed in a few days, at a cost in the low hundreds of dollars. Lens design constraints, such as focal length and field of view, are specified in terms familiar to the graphics community so no optics expertise is necessary. Unlike conventional lens design systems, which only use continuous optimization methods, Lens Factory adds a discrete optimization stage. This stage searches the combinatorial space of possible combinations of lens elements to find novel designs, evolving simple canonical lens designs into more complex, better designs. Intelligent pruning rules make the combinatorial search feasible. We have designed and built several high performance optical systems which demonstrate the practicality of the system. version:2
arxiv-1507-04798 | Exploratory topic modeling with distributional semantics | http://arxiv.org/abs/1507.04798 | id:1507.04798 author:Samuel Rönnqvist category:cs.IR cs.CL cs.LG  published:2015-07-16 summary:As we continue to collect and store textual data in a multitude of domains, we are regularly confronted with material whose largely unknown thematic structure we want to uncover. With unsupervised, exploratory analysis, no prior knowledge about the content is required and highly open-ended tasks can be supported. In the past few years, probabilistic topic modeling has emerged as a popular approach to this problem. Nevertheless, the representation of the latent topics as aggregations of semi-coherent terms limits their interpretability and level of detail. This paper presents an alternative approach to topic modeling that maps topics as a network for exploration, based on distributional semantics using learned word vectors. From the granular level of terms and their semantic similarity relations global topic structures emerge as clustered regions and gradients of concepts. Moreover, the paper discusses the visual interactive representation of the topic map, which plays an important role in supporting its exploration. version:1
arxiv-1507-04761 | Deep Learning and Music Adversaries | http://arxiv.org/abs/1507.04761 | id:1507.04761 author:Corey Kereliuk, Bob L. Sturm, Jan Larsen category:cs.LG cs.NE cs.SD  published:2015-07-16 summary:An adversary is essentially an algorithm intent on making a classification system perform in some particular way given an input, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, which exploits the parameters of the system to find the minimal perturbation of the input image such that the network misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the input to the systems is magnitude spectral frames, which requires special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different deep architectures, we find that this adversary is very effective in defeating the resulting systems. We find the convolutional networks are more robust, however, compared with systems based on a majority vote over individually classified audio frames. Furthermore, we integrate the adversary into the training of new deep systems, but do not find that this improves their resilience against the same adversary. version:1
arxiv-1507-04734 | Variational Gram Functions: Convex Analysis and Optimization | http://arxiv.org/abs/1507.04734 | id:1507.04734 author:Amin Jalali, Lin Xiao, Maryam Fazel category:math.OC cs.LG stat.ML  published:2015-07-16 summary:We propose a new class of convex penalty functions, called variational Gram functions (VGFs), that can promote pairwise relations, such as orthogonality, among a set of vectors in a vector space. When used as regularizers in convex optimization problems, these functions find application in hierarchical classification, multitask learning, and estimation of vectors with disjoint supports, among other applications. We describe a general condition for convexity, which is then used to prove the convexity of a few known functions as well as some new ones. We give a characterization of the associated subdifferential and the proximal operator, and discuss efficient optimization algorithms for some structured regularized loss-minimization problems using VGFs. Numerical experiments on a hierarchical classification problem are also presented that demonstrate the effectiveness of VGFs and the associated optimization algorithms in practice. version:1
arxiv-1507-04727 | Recursive Sparse Point Process Regression with Application to Spectrotemporal Receptive Field Plasticity Analysis | http://arxiv.org/abs/1507.04727 | id:1507.04727 author:Alireza Sheikhattar, Jonathan B. Fritz, Shihab A. Shamma, Behtash Babadi category:cs.NE cs.SY math.OC stat.AP stat.CO  published:2015-07-16 summary:We consider the problem of estimating the sparse time-varying parameter vectors of a point process model in an online fashion, where the observations and inputs respectively consist of binary and continuous time series. We construct a novel objective function by incorporating a forgetting factor mechanism into the point process log-likelihood to enforce adaptivity and employ $\ell_1$-regularization to capture the sparsity. We provide a rigorous analysis of the maximizers of the objective function, which extends the guarantees of compressed sensing to our setting. We construct two recursive filters for online estimation of the parameter vectors based on proximal optimization techniques, as well as a novel filter for recursive computation of statistical confidence regions. Simulation studies reveal that our algorithms outperform several existing point process filters in terms of trackability, goodness-of-fit and mean square error. We finally apply our filtering algorithms to experimentally recorded spiking data from the ferret primary auditory cortex during attentive behavior in a click rate discrimination task. Our analysis provides new insights into the time-course of the spectrotemporal receptive field plasticity of the auditory neurons. version:1
arxiv-1412-1559 | Iterative Subsampling in Solution Path Clustering of Noisy Big Data | http://arxiv.org/abs/1412.1559 | id:1412.1559 author:Yuliya Marchetti, Qing Zhou category:stat.ME stat.ML  published:2014-12-04 summary:We develop an iterative subsampling approach to improve the computational efficiency of our previous work on solution path clustering (SPC). The SPC method achieves clustering by concave regularization on the pairwise distances between cluster centers. This clustering method has the important capability to recognize noise and to provide a short path of clustering solutions; however, it is not sufficiently fast for big datasets. Thus, we propose a method that iterates between clustering a small subsample of the full data and sequentially assigning the other data points to attain orders of magnitude of computational savings. The new method preserves the ability to isolate noise, includes a solution selection mechanism that ultimately provides one clustering solution with an estimated number of clusters, and is shown to be able to extract small tight clusters from noisy data. The method's relatively minor losses in accuracy are demonstrated through simulation studies, and its ability to handle large datasets is illustrated through applications to gene expression datasets. An R package, SPClustering, for the SPC method with iterative subsampling is available at http://www.stat.ucla.edu/~zhou/Software.html. version:2
arxiv-1507-04646 | A Dependency-Based Neural Network for Relation Classification | http://arxiv.org/abs/1507.04646 | id:1507.04646 author:Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, Houfeng Wang category:cs.CL cs.LG cs.NE  published:2015-07-16 summary:Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results. version:1
arxiv-1507-03040 | Tight Risk Bounds for Multi-Class Margin Classifiers | http://arxiv.org/abs/1507.03040 | id:1507.03040 author:Yury Maximov, Daria Reshetova category:stat.ML cs.LG  published:2015-07-10 summary:We consider a problem of risk estimation for large-margin multi-class classifiers. We propose a novel risk bound for the multi-class classification problem. The bound involves the marginal distribution of the classifier and the Rademacher complexity of the hypothesis class. We prove that our bound is tight in the number of classes. Finally, we compare our bound with the related ones and provide a simplified version of the bound for the multi-class classification with kernel based hypotheses. version:2
arxiv-1501-04782 | Constructing Binary Descriptors with a Stochastic Hill Climbing Search | http://arxiv.org/abs/1501.04782 | id:1501.04782 author:Nenad Markuš, Igor S. Pandžić, Jörgen Ahlberg category:cs.CV  published:2015-01-20 summary:Binary descriptors of image patches provide processing speed advantages and require less storage than methods that encode the patch appearance with a vector of real numbers. We provide evidence that, despite its simplicity, a stochastic hill climbing bit selection procedure for descriptor construction defeats recently proposed alternatives on a standard discriminative power benchmark. The method is easy to implement and understand, has no free parameters that need fine tuning, and runs fast. version:2
arxiv-1507-04564 | Ordinal optimization - empirical large deviations rate estimators, and stochastic multi-armed bandits | http://arxiv.org/abs/1507.04564 | id:1507.04564 author:Peter Glynn, Sandeep Juneja category:math.PR stat.ML 65C05  60-08  published:2015-07-16 summary:Consider the ordinal optimization problem of finding a population amongst many with the smallest mean when these means are unknown but population samples can be generated via simulation. Typically, by selecting a population with the smallest sample mean, it can be shown that the false selection probability decays at an exponential rate. Lately researchers have sought algorithms that guarantee that this probability is restricted to a small $\delta$ in order $\log(1/\delta)$ computational time by estimating the associated large deviations rate function via simulation. We show that such guarantees are misleading. Enroute, we identify the large deviations principle followed by the empirically estimated large deviations rate function that may also be of independent interest. Further, we show a negative result that when populations have unbounded support, any policy that asymptotically identifies the correct population with probability at least $1-\delta$ for each problem instance requires more than $O(\log(1/\delta))$ samples in making such a determination in any problem instance. This suggests that some restrictions are essential on populations to devise $O(\log(1/\delta))$ algorithms with $1 - \delta$ correctness guarantees. We note that under restriction on population moments, such methods are easily designed. We also observe that sequential methods from stochastic multi-armed bandit literature can be adapted to devise such algorithms. version:1
arxiv-1507-04523 | Upper-Confidence-Bound Algorithms for Active Learning in Multi-Armed Bandits | http://arxiv.org/abs/1507.04523 | id:1507.04523 author:Alexandra Carpentier, Alessandro Lazaric, Mohammad Ghavamzadeh, Rémi Munos, Peter Auer, András Antos category:cs.LG G.3  published:2015-07-16 summary:In this paper, we study the problem of estimating uniformly well the mean values of several distributions given a finite budget of samples. If the variance of the distributions were known, one could design an optimal sampling strategy by collecting a number of independent samples per distribution that is proportional to their variance. However, in the more realistic case where the distributions are not known in advance, one needs to design adaptive sampling strategies in order to select which distribution to sample from according to the previously observed samples. We describe two strategies based on pulling the distributions a number of times that is proportional to a high-probability upper-confidence-bound on their variance (built from previous observed samples) and report a finite-sample performance analysis on the excess estimation error compared to the optimal allocation. We show that the performance of these allocation strategies depends not only on the variances but also on the full shape of the distributions. version:1
arxiv-1507-04513 | Scalable Gaussian Process Classification via Expectation Propagation | http://arxiv.org/abs/1507.04513 | id:1507.04513 author:Daniel Hernández-Lobato, José Miguel Hernández-Lobato category:stat.ML  published:2015-07-16 summary:Variational methods have been recently considered for scaling the training process of Gaussian process classifiers to large datasets. As an alternative, we describe here how to train these classifiers efficiently using expectation propagation. The proposed method allows for handling datasets with millions of data instances. More precisely, it can be used for (i) training in a distributed fashion where the data instances are sent to different nodes in which the required computations are carried out, and for (ii) maximizing an estimate of the marginal likelihood using a stochastic approximation of the gradient. Several experiments indicate that the method described is competitive with the variational approach. version:1
arxiv-1507-04512 | Diagnosing State-Of-The-Art Object Proposal Methods | http://arxiv.org/abs/1507.04512 | id:1507.04512 author:Hongyuan Zhu, Shijian Lu, Jianfei Cai, Quangqing Lee category:cs.CV  published:2015-07-16 summary:Object proposal has become a popular paradigm to replace exhaustive sliding window search in current top-performing methods in PASCAL VOC and ImageNet. Recently, Hosang et al. conduct the first unified study of existing methods' in terms of various image-level degradations. On the other hand, the vital question "what object-level characteristics really affect existing methods' performance?" is not yet answered. Inspired by Hoiem et al.'s work in categorical object detection, this paper conducts the first meta-analysis of various object-level characteristics' impact on state-of-the-art object proposal methods. Specifically, we examine the effects of object size, aspect ratio, iconic view, color contrast, shape regularity and texture. We also analyse existing methods' localization accuracy and latency for various PASCAL VOC object classes. Our study reveals the limitations of existing methods in terms of non-iconic view, small object size, low color contrast, shape regularity etc. Based on our observations, lessons are also learned and shared with respect to the selection of existing object proposal technologies as well as the design of the future ones. version:1
arxiv-1507-04505 | On the Convergence of Stochastic Variational Inference in Bayesian Networks | http://arxiv.org/abs/1507.04505 | id:1507.04505 author:Ulrich Paquet category:stat.ML  published:2015-07-16 summary:We highlight a pitfall when applying stochastic variational inference to general Bayesian networks. For global random variables approximated by an exponential family distribution, natural gradient steps, commonly starting from a unit length step size, are averaged to convergence. This useful insight into the scaling of initial step sizes is lost when the approximation factorizes across a general Bayesian network, and care must be taken to ensure practical convergence. We experimentally investigate how much of the baby (well-scaled steps) is thrown out with the bath water (exact gradients). version:1
arxiv-1507-04502 | Towards Predicting First Daily Departure Times: a Gaussian Modeling Approach for Load Shift Forecasting | http://arxiv.org/abs/1507.04502 | id:1507.04502 author:Nicholas H. Kirk, Ilya Dianov category:cs.LG  published:2015-07-16 summary:This work provides two statistical Gaussian forecasting methods for predicting First Daily Departure Times (FDDTs) of everyday use electric vehicles. This is important in smart grid applications to understand disconnection times of such mobile storage units, for instance to forecast storage of non dispatchable loads (e.g. wind and solar power). We provide a review of the relevant state-of-the-art driving behavior features towards FDDT prediction, to then propose an approximated Gaussian method which qualitatively forecasts how many vehicles will depart within a given time frame, by assuming that departure times follow a normal distribution. This method considers sampling sessions as Poisson distributions which are superimposed to obtain a single approximated Gaussian model. Given the Gaussian distribution assumption of the departure times, we also model the problem with Gaussian Mixture Models (GMM), in which the priorly set number of clusters represents the desired time granularity. Evaluation has proven that for the dataset tested, low error and high confidence ($\approx 95\%$) is possible for 15 and 10 minute intervals, and that GMM outperforms traditional modeling but is less generalizable across datasets, as it is a closer fit to the sampling data. Conclusively we discuss future possibilities and practical applications of the discussed model. version:1
arxiv-1507-04296 | Massively Parallel Methods for Deep Reinforcement Learning | http://arxiv.org/abs/1507.04296 | id:1507.04296 author:Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, David Silver category:cs.LG cs.AI cs.DC cs.NE  published:2015-07-15 summary:We present the first massively distributed architecture for deep reinforcement learning. This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience. We used our architecture to implement the Deep Q-Network algorithm (DQN). Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters. Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games. version:2
arxiv-1311-1354 | How to Center Binary Deep Boltzmann Machines | http://arxiv.org/abs/1311.1354 | id:1311.1354 author:Jan Melchior, Asja Fischer, Laurenz Wiskott category:stat.ML cs.LG  published:2013-11-06 summary:This work analyzes centered binary Restricted Boltzmann Machines (RBMs) and binary Deep Boltzmann Machines (DBMs), where centering is done by subtracting offset values from visible and hidden variables. We show analytically that (i) centering results in a different but equivalent parameterization for artificial neural networks in general, (ii) the expected performance of centered binary RBMs/DBMs is invariant under simultaneous flip of data and offsets, for any offset value in the range of zero to one, (iii) centering can be reformulated as a different update rule for normal binary RBMs/DBMs, and (iv) using the enhanced gradient is equivalent to setting the offset values to the average over model and data mean. Furthermore, numerical simulations suggest that (i) optimal generative performance is achieved by subtracting mean values from visible as well as hidden variables, (ii) centered RBMs/DBMs reach significantly higher log-likelihood values than normal binary RBMs/DBMs, (iii) centering variants whose offsets depend on the model mean, like the enhanced gradient, suffer from severe divergence problems, (iv) learning is stabilized if an exponentially moving average over the batch means is used for the offset values instead of the current batch mean, which also prevents the enhanced gradient from diverging, (v) centered RBMs/DBMs reach higher LL values than normal RBMs/DBMs while having a smaller norm of the weight matrix, (vi) centering leads to an update direction that is closer to the natural gradient and that the natural gradient is extremly efficient for training RBMs, (vii) centering dispense the need for greedy layer-wise pre-training of DBMs, (viii) furthermore we show that pre-training often even worsen the results independently whether centering is used or not, and (ix) centering is also beneficial for auto encoders. version:3
arxiv-1507-04457 | Preference Completion: Large-scale Collaborative Ranking from Pairwise Comparisons | http://arxiv.org/abs/1507.04457 | id:1507.04457 author:Dohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, Inderjit S. Dhillon category:stat.ML cs.LG  published:2015-07-16 summary:In this paper we consider the collaborative ranking setting: a pool of users each provides a small number of pairwise preferences between $d$ possible items; from these we need to predict preferences of the users for items they have not yet seen. We do so by fitting a rank $r$ score matrix to the pairwise data, and provide two main contributions: (a) we show that an algorithm based on convex optimization provides good generalization guarantees once each user provides as few as $O(r\log^2 d)$ pairwise comparisons -- essentially matching the sample complexity required in the related matrix completion setting (which uses actual numerical as opposed to pairwise information), and (b) we develop a large-scale non-convex implementation, which we call AltSVM, that trains a factored form of the matrix via alternating minimization (which we show reduces to alternating SVM problems), and scales and parallelizes very well to large problem settings. It also outperforms common baselines on many moderately large popular collaborative filtering datasets in both NDCG and in other measures of ranking performance. version:1
arxiv-1507-04437 | A Deep Hashing Learning Network | http://arxiv.org/abs/1507.04437 | id:1507.04437 author:Guoqiang Zhong, Pan Yang, Sijiang Wang, Junyu Dong category:cs.CV  published:2015-07-16 summary:Hashing-based methods seek compact and efficient binary codes that preserve the neighborhood structure in the original data space. For most existing hashing methods, an image is first encoded as a vector of hand-crafted visual feature, followed by a hash projection and quantization step to get the compact binary vector. Most of the hand-crafted features just encode the low-level information of the input, the feature may not preserve the semantic similarities of images pairs. Meanwhile, the hashing function learning process is independent with the feature representation, so the feature may not be optimal for the hashing projection. In this paper, we propose a supervised hashing method based on a well designed deep convolutional neural network, which tries to learn hashing code and compact representations of data simultaneously. The proposed model learn the binary codes by adding a compact sigmoid layer before the loss layer. Experiments on several image data sets show that the proposed model outperforms other state-of-the-art methods. version:1
arxiv-1507-04436 | Joint Tensor Factorization and Outlying Slab Suppression with Applications | http://arxiv.org/abs/1507.04436 | id:1507.04436 author:Xiao Fu, Kejun Huang, Wing-Kin Ma, Nicholas D. Sidiropoulos, Rasmus Bro category:stat.ML  published:2015-07-16 summary:We consider factoring low-rank tensors in the presence of outlying slabs. This problem is important in practice, because data collected in many real-world applications, such as speech, fluorescence, and some social network data, fit this paradigm. Prior work tackles this problem by iteratively selecting a fixed number of slabs and fitting, a procedure which may not converge. We formulate this problem from a group-sparsity promoting point of view, and propose an alternating optimization framework to handle the corresponding $\ell_p$ ($0<p\leq 1$) minimization-based low-rank tensor factorization problem. The proposed algorithm features a similar per-iteration complexity as the plain trilinear alternating least squares (TALS) algorithm. Convergence of the proposed algorithm is also easy to analyze under the framework of alternating optimization and its variants. In addition, regularization and constraints can be easily incorporated to make use of \emph{a priori} information on the latent loading factors. Simulations and real data experiments on blind speech separation, fluorescence data analysis, and social network mining are used to showcase the effectiveness of the proposed algorithm. version:1
arxiv-1412-3756 | Certifying and removing disparate impact | http://arxiv.org/abs/1412.3756 | id:1412.3756 author:Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, Suresh Venkatasubramanian category:stat.ML cs.CY  published:2014-12-11 summary:What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender, religious practice) and an explicit description of the process. When the process is implemented using computers, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the algorithm, we propose making inferences based on the data the algorithm uses. We make four contributions to this problem. First, we link the legal notion of disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on analyzing the information leakage of the protected class from the other data attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny. version:3
arxiv-1507-04420 | Bias and population structure in the actuation of sound change | http://arxiv.org/abs/1507.04420 | id:1507.04420 author:James Kirby, Morgan Sonderegger category:cs.CL physics.soc-ph  published:2015-07-16 summary:Why do human languages change at some times, and not others? We address this longstanding question from a computational perspective, focusing on the case of sound change. Sound change arises from the pronunciation variability ubiquitous in every speech community, but most such variability does not lead to change. Hence, an adequate model must allow for stability as well as change. Existing theories of sound change tend to emphasize factors at the level of individual learners promoting one outcome or the other, such as channel bias (which favors change) or inductive bias (which favors stability). Here, we consider how the interaction of these biases can lead to both stability and change in a population setting. We find that population structure itself can act as a source of stability, but that both stability and change are possible only when both types of bias are active, suggesting that it is possible to understand why sound change occurs at some times and not others as the population-level result of the interplay between forces promoting each outcome in individual speakers. In addition, if it is assumed that learners learn from two or more teachers, the transition from stability to change is marked by a phase transition, consistent with the abrupt transitions seen in many empirical cases of sound change. The predictions of multiple-teacher models thus match empirical cases of sound change better than the predictions of single-teacher models, underscoring the importance of modeling language change in a population setting. version:1
arxiv-1507-04396 | Parallel MMF: a Multiresolution Approach to Matrix Computation | http://arxiv.org/abs/1507.04396 | id:1507.04396 author:Risi Kondor, Nedelina Teneva, Pramod K. Mudrakarta category:cs.NA cs.LG stat.ML  published:2015-07-15 summary:Multiresolution Matrix Factorization (MMF) was recently introduced as a method for finding multiscale structure and defining wavelets on graphs/matrices. In this paper we derive pMMF, a parallel algorithm for computing the MMF factorization. Empirically, the running time of pMMF scales linearly in the dimension for sparse matrices. We argue that this makes pMMF a valuable new computational primitive in its own right, and present experiments on using pMMF for two distinct purposes: compressing matrices and preconditioning large sparse linear systems. version:1
arxiv-1205-2171 | A Generalized Kernel Approach to Structured Output Learning | http://arxiv.org/abs/1205.2171 | id:1205.2171 author:Hachem Kadri, Mohammad Ghavamzadeh, Philippe Preux category:stat.ML cs.LG  published:2012-05-10 summary:We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) problem using operator-valued kernels. We show that some of the existing formulations of this problem are special cases of our framework. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on two structured output problems, and compare it to the state-of-the-art kernel-based structured output regression methods. version:2
arxiv-1507-04319 | Learning Boolean functions with concentrated spectra | http://arxiv.org/abs/1507.04319 | id:1507.04319 author:Dustin G. Mixon, Jesse Peterson category:cs.LG cs.IT math.FA math.IT  published:2015-07-15 summary:This paper discusses the theory and application of learning Boolean functions that are concentrated in the Fourier domain. We first estimate the VC dimension of this function class in order to establish a small sample complexity of learning in this case. Next, we propose a computationally efficient method of empirical risk minimization, and we apply this method to the MNIST database of handwritten digits. These results demonstrate the effectiveness of our model for modern classification tasks. We conclude with a list of open problems for future investigation. version:1
arxiv-1507-04285 | Learning Action Models: Qualitative Approach | http://arxiv.org/abs/1507.04285 | id:1507.04285 author:Thomas Bolander, Nina Gierasimczuk category:cs.LG cs.AI cs.LO  published:2015-07-15 summary:In dynamic epistemic logic, actions are described using action models. In this paper we introduce a framework for studying learnability of action models from observations. We present first results concerning propositional action models. First we check two basic learnability criteria: finite identifiability (conclusively inferring the appropriate action model in finite time) and identifiability in the limit (inconclusive convergence to the right action model). We show that deterministic actions are finitely identifiable, while non-deterministic actions require more learning power-they are identifiable in the limit. We then move on to a particular learning method, which proceeds via restriction of a space of events within a learning-specific action model. This way of learning closely resembles the well-known update method from dynamic epistemic logic. We introduce several different learning methods suited for finite identifiability of particular types of deterministic actions. version:1
arxiv-1507-04230 | The Role of Principal Angles in Subspace Classification | http://arxiv.org/abs/1507.04230 | id:1507.04230 author:Jiaji Huang, Qiang Qiu, Robert Calderbank category:stat.ML cs.LG  published:2015-07-15 summary:Subspace models play an important role in a wide range of signal processing tasks, and this paper explores how the pairwise geometry of subspaces influences the probability of misclassification. When the mismatch between the signal and the model is vanishingly small, the probability of misclassification is determined by the product of the sines of the principal angles between subspaces. When the mismatch is more significant, the probability of misclassification is determined by the sum of the squares of the sines of the principal angles. Reliability of classification is derived in terms of the distribution of signal energy across principal vectors. Larger principal angles lead to smaller classification error, motivating a linear transform that optimizes principal angles. The transform presented here (TRAIT) preserves some specific characteristic of each individual class, and this approach is shown to be complementary to a previously developed transform (LRT) that enlarges inter-class distance while suppressing intra-class dispersion. Theoretical results are supported by demonstration of superior classification accuracy on synthetic and measured data even in the presence of significant model mismatch. version:1
arxiv-1507-04214 | Associative Measures and Multi-word Unit Extraction in Turkish | http://arxiv.org/abs/1507.04214 | id:1507.04214 author:Umit Mersinli category:cs.CL  published:2015-07-15 summary:Associative measures are "mathematical formulas determining the strength of association between two or more words based on their occurrences and cooccurrences in a text corpus" (Pecina, 2010, p. 138). The purpose of this paper is to test the 12 associative measures that Text-NSP (Banerjee & Pedersen, 2003) contains on a 10-million-word subcorpus of Turkish National Corpus (TNC) (Aksan et.al., 2012). A statistical comparison of those measures is out of the scope of the study, and the measures will be evaluated according to the linguistic relevance of the rankings they provide. The focus of the study is basically on optimizing the corpus data, before applying the measures and then, evaluating the rankings produced by these measures as a whole, not on the linguistic relevance of individual n-grams. The findings include intra-linguistically relevant associative measures for a comma delimited, sentence splitted, lower-cased, well-balanced, representative, 10-million-word corpus of Turkish. version:1
arxiv-1206-5754 | Bayesian Modeling with Gaussian Processes using the GPstuff Toolbox | http://arxiv.org/abs/1206.5754 | id:1206.5754 author:Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari category:stat.ML cs.AI cs.MS  published:2012-06-25 summary:Gaussian processes (GP) are powerful tools for probabilistic modeling purposes. They can be used to define prior distributions over latent functions in hierarchical Bayesian models. The prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. The inference can then be conducted directly in the function space by evaluating or approximating the posterior process. Despite their attractive theoretical properties GPs provide practical challenges in their implementation. GPstuff is a versatile collection of computational tools for GP models compatible with Linux and Windows MATLAB and Octave. It includes, among others, various inference methods, sparse approximations and tools for model assessment. In this work, we review these tools and demonstrate the use of GPstuff in several models. version:6
arxiv-1502-05312 | Predictive Entropy Search for Bayesian Optimization with Unknown Constraints | http://arxiv.org/abs/1502.05312 | id:1502.05312 author:José Miguel Hernández-Lobato, Michael A. Gelbart, Matthew W. Hoffman, Ryan P. Adams, Zoubin Ghahramani category:stat.ML  published:2015-02-18 summary:Unknown constraints arise in many types of expensive black-box optimization problems. Several methods have been proposed recently for performing Bayesian optimization with constraints, based on the expected improvement (EI) heuristic. However, EI can lead to pathologies when used with constraints. For example, in the case of decoupled constraints---i.e., when one can independently evaluate the objective or the constraints---EI can encounter a pathology that prevents exploration. Additionally, computing EI requires a current best solution, which may not exist if none of the data collected so far satisfy the constraints. By contrast, information-based approaches do not suffer from these failure modes. In this paper, we present a new information-based method called Predictive Entropy Search with Constraints (PESC). We analyze the performance of PESC and show that it compares favorably to EI-based approaches on synthetic and benchmark problems, as well as several real-world examples. We demonstrate that PESC is an effective algorithm that provides a promising direction towards a unified solution for constrained Bayesian optimization. version:2
arxiv-1507-04155 | ALEVS: Active Learning by Statistical Leverage Sampling | http://arxiv.org/abs/1507.04155 | id:1507.04155 author:Cem Orhan, Öznur Taştan category:cs.LG stat.ML  published:2015-07-15 summary:Active learning aims to obtain a classifier of high accuracy by using fewer label requests in comparison to passive learning by selecting effective queries. Many active learning methods have been developed in the past two decades, which sample queries based on informativeness or representativeness of unlabeled data points. In this work, we explore a novel querying criterion based on statistical leverage scores. The statistical leverage scores of a row in a matrix are the squared row-norms of the matrix containing its (top) left singular vectors and is a measure of influence of the row on the matrix. Leverage scores have been used for detecting high influential points in regression diagnostics and have been recently shown to be useful for data analysis and randomized low-rank matrix approximation algorithms. We explore how sampling data instances with high statistical leverage scores perform in active learning. Our empirical comparison on several binary classification datasets indicate that querying high leverage points is an effective strategy. version:1
arxiv-1502-05336 | Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks | http://arxiv.org/abs/1502.05336 | id:1502.05336 author:José Miguel Hernández-Lobato, Ryan P. Adams category:stat.ML  published:2015-02-18 summary:Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights. version:2
arxiv-1406-5774 | Factors of Transferability for a Generic ConvNet Representation | http://arxiv.org/abs/1406.5774 | id:1406.5774 author:Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto Maki, Stefan Carlsson category:cs.CV  published:2014-06-22 summary:Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their distance from the source task such that a correlation between the performance of tasks and their distance from the source task w.r.t. the proposed factors is observed. version:3
arxiv-1507-04126 | Revisiting AdaBoost for Cost-Sensitive Classification. Part II: Empirical Analysis | http://arxiv.org/abs/1507.04126 | id:1507.04126 author:Iago Landesa-Vázquez, José Luis Alba-Castro category:cs.CV cs.AI cs.LG  published:2015-07-15 summary:A lot of approaches, each following a different strategy, have been proposed in the literature to provide AdaBoost with cost-sensitive properties. In the first part of this series of two papers, we have presented these algorithms in a homogeneous notational framework, proposed a clustering scheme for them and performed a thorough theoretical analysis of those approaches with a fully theoretical foundation. The present paper, in order to complete our analysis, is focused on the empirical study of all the algorithms previously presented over a wide range of heterogeneous classification problems. The results of our experiments, confirming the theoretical conclusions, seem to reveal that the simplest approach, just based on cost-sensitive weight initialization, is the one showing the best and soundest results, despite having been recurrently overlooked in the literature. version:1
arxiv-1507-04125 | Revisiting AdaBoost for Cost-Sensitive Classification. Part I: Theoretical Perspective | http://arxiv.org/abs/1507.04125 | id:1507.04125 author:Iago Landesa-Vázquez, José Luis Alba-Castro category:cs.CV cs.AI cs.LG  published:2015-07-15 summary:Boosting algorithms have been widely used to tackle a plethora of problems. In the last few years, a lot of approaches have been proposed to provide standard AdaBoost with cost-sensitive capabilities, each with a different focus. However, for the researcher, these algorithms shape a tangled set with diffuse differences and properties, lacking a unifying analysis to jointly compare, classify, evaluate and discuss those approaches on a common basis. In this series of two papers we aim to revisit the various proposals, both from theoretical (Part I) and practical (Part II) perspectives, in order to analyze their specific properties and behavior, with the final goal of identifying the algorithm providing the best and soundest results. version:1
arxiv-1507-04124 | On the Computability of Solomonoff Induction and Knowledge-Seeking | http://arxiv.org/abs/1507.04124 | id:1507.04124 author:Jan Leike, Marcus Hutter category:cs.AI cs.LG  published:2015-07-15 summary:Solomonoff induction is held as a gold standard for learning, but it is known to be incomputable. We quantify its incomputability by placing various flavors of Solomonoff's prior M in the arithmetical hierarchy. We also derive computability bounds for knowledge-seeking agents, and give a limit-computable weakly asymptotically optimal reinforcement learning agent. version:1
arxiv-1507-04121 | Solomonoff Induction Violates Nicod's Criterion | http://arxiv.org/abs/1507.04121 | id:1507.04121 author:Jan Leike, Marcus Hutter category:cs.LG cs.AI math.ST stat.TH  published:2015-07-15 summary:Nicod's criterion states that observing a black raven is evidence for the hypothesis H that all ravens are black. We show that Solomonoff induction does not satisfy Nicod's criterion: there are time steps in which observing black ravens decreases the belief in H. Moreover, while observing any computable infinite string compatible with H, the belief in H decreases infinitely often when using the unnormalized Solomonoff prior, but only finitely often when using the normalized Solomonoff prior. We argue that the fault is not with Solomonoff induction; instead we should reject Nicod's criterion. version:1
arxiv-1507-04116 | Language discrimination and clustering via a neural network approach | http://arxiv.org/abs/1507.04116 | id:1507.04116 author:Angelo Mariano, Giorgio Parisi, Saverio Pascazio category:cond-mat.dis-nn cs.CL cs.NE physics.soc-ph  published:2015-07-15 summary:We classify twenty-one Indo-European languages starting from written text. We use neural networks in order to define a distance among different languages, construct a dendrogram and analyze the ultrametric structure that emerges. Four or five subgroups of languages are identified, according to the "cut" of the dendrogram, drawn with an entropic criterion. The results and the method are discussed. version:1
arxiv-1507-04060 | Unsupervised Decision Forest for Data Clustering and Density Estimation | http://arxiv.org/abs/1507.04060 | id:1507.04060 author:Hayder Albehadili, Naz Islam category:cs.CV  published:2015-07-15 summary:An algorithm to improve performance parameter for unsupervised decision forest clustering and density estimation is presented. Specifically, a dual assignment parameter is introduced as a density estimator by combining Random Forest and Gaussian Mixture Model. The Random Forest method has been specifically applied to construct a robust affinity graph that provides information on the underlying structure of data objects used in clustering. The proposed algorithm differs from the commonly used spectral clustering methods where the computed distance metric is used to find similarities between data points. Experiments were conducted using five datasets. A comparison with six other state-of-the-art methods shows that our model is superior to existing approaches. Efficiency of the proposed model is in capturing the underlying structure for a given set of data points. The proposed method is also robust, and can discriminate between the complex features of data points among different clusters. version:1
arxiv-1507-00436 | Online Transfer Learning in Reinforcement Learning Domains | http://arxiv.org/abs/1507.00436 | id:1507.00436 author:Yusen Zhan, Matthew E. Taylor category:cs.AI cs.LG I.2.11; I.2.6  published:2015-07-02 summary:This paper proposes an online transfer framework to capture the interaction among agents and shows that current transfer learning in reinforcement learning is a special case of online transfer. Furthermore, this paper re-characterizes existing agents-teaching-agents methods as online transfer and analyze one such teaching method in three ways. First, the convergence of Q-learning and Sarsa with tabular representation with a finite budget is proven. Second, the convergence of Q-learning and Sarsa with linear function approximation is established. Third, the we show the asymptotic performance cannot be hurt through teaching. Additionally, all theoretical results are empirically validated. version:2
arxiv-1507-04029 | Training artificial neural networks to learn a nondeterministic game | http://arxiv.org/abs/1507.04029 | id:1507.04029 author:Thomas E. Portegys category:cs.LG  published:2015-07-14 summary:It is well known that artificial neural networks (ANNs) can learn deterministic automata. Learning nondeterministic automata is another matter. This is important because much of the world is nondeterministic, taking the form of unpredictable or probabilistic events that must be acted upon. If ANNs are to engage such phenomena, then they must be able to learn how to deal with nondeterminism. In this project the game of Pong poses a nondeterministic environment. The learner is given an incomplete view of the game state and underlying deterministic physics, resulting in a nondeterministic game. Three models were trained and tested on the game: Mona, Elman, and Numenta's NuPIC. version:1
arxiv-1507-04019 | Feature Normalisation for Robust Speech Recognition | http://arxiv.org/abs/1507.04019 | id:1507.04019 author:D. S. Pavan Kumar category:cs.CL cs.SD  published:2015-07-14 summary:Speech recognition system performance degrades in noisy environments. If the acoustic models are built using features of clean utterances, the features of a noisy test utterance would be acoustically mismatched with the trained model. This gives poor likelihoods and poor recognition accuracy. Model adaptation and feature normalisation are two broad areas that address this problem. While the former often gives better performance, the latter involves estimation of lesser number of parameters, making the system feasible for practical implementations. This research focuses on the efficacies of various subspace, statistical and stereo based feature normalisation techniques. A subspace projection based method has been investigated as a standalone and adjunct technique involving reconstruction of noisy speech features from a precomputed set of clean speech building-blocks. The building blocks are learned using non-negative matrix factorisation (NMF) on log-Mel filter bank coefficients, which form a basis for the clean speech subspace. The work provides a detailed study on how the method can be incorporated into the extraction process of Mel-frequency cepstral coefficients. Experimental results show that the new features are robust to noise, and achieve better results when combined with the existing techniques. The work also proposes a modification to the training process of SPLICE algorithm for noise robust speech recognition. It is based on feature correlations, and enables this stereo-based algorithm to improve the performance in all noise conditions, especially in unseen cases. Further, the modified framework is extended to work for non-stereo datasets where clean and noisy training utterances, but not stereo counterparts, are required. An MLLR-based computationally efficient run-time noise adaptation method in SPLICE framework has been proposed. version:1
arxiv-1507-04001 | Structure and inference in annotated networks | http://arxiv.org/abs/1507.04001 | id:1507.04001 author:M. E. J. Newman, Aaron Clauset category:cs.SI physics.data-an physics.soc-ph stat.ML  published:2015-07-14 summary:For many networks of scientific interest we know both the connections of the network and information about the network nodes, such as the age or gender of individuals in a social network, geographic location of nodes in the Internet, or cellular function of nodes in a gene regulatory network. Here we demonstrate how this "metadata" can be used to improve our analysis and understanding of network structure. We focus in particular on the problem of community detection in networks and develop a mathematically principled approach that combines a network and its metadata to detect communities more accurately than can be done with either alone. Crucially, the method does not assume that the metadata are correlated with the communities we are trying to find. Instead the method learns whether a correlation exists and correctly uses or ignores the metadata depending on whether they contain useful information. The learned correlations are also of interest in their own right, allowing us to make predictions about the community membership of nodes whose network connections are unknown. We demonstrate our method on synthetic networks with known structure and on real-world networks, large and small, drawn from social, biological, and technological domains. version:1
