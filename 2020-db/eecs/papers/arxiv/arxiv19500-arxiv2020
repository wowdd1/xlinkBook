arxiv-1607-02810 | The benefits of word embeddings features for active learning in clinical information extraction | http://arxiv.org/abs/1607.02810 | id:1607.02810 author:Mahnoosh Kholghi, Lance De Vine, Laurianne Sitbon, Guido Zuccon, Anthony Nguyen category:cs.CL 68U15  68T50  published:2016-07-11 summary:Objective This study investigates the use of word embeddings and sequence features for sample representation in an active learning framework built to extract clinical concepts from clinical free text. The objective is to further reduce the manual annotation effort while achieving higher effectiveness compared to a set of baseline features. Materials and methods The comparative performance of unsupervised features and baseline hand-crafted features in an active learning framework were investigated. Unsupervised features were derived from skip-gram word embeddings and a sequence representation approach. Least confidence, information diversity, information density and diversity, and domain knowledge informativeness were used as selection criteria for active learning framework. Two clinical datasets were used for evaluation: the i2b2/VA 2010 NLP challenge and the ShARe/CLEF 2013 eHealth Evaluation Lab. Results Our results demonstrated significant improvement by adding unsupervised word and sequence level features in the active learning framework in terms of effectiveness as well as annotation effort across both datasets. Using unsupervised features along with baseline features for sample representation led to further savings of up to 10% and 6% of the token and concept annotation rates, respectively. Conclusion This study shows that the manual annotation of clinical free text for automated analysis can be accelerated by using unsupervised features for sample representation in an active learning framework. To the best of our knowledge, this is the first study to analyze the effect of unsupervised features on active learning performance in clinical information extraction. version:1
arxiv-1607-02802 | Mapping distributional to model-theoretic semantic spaces: a baseline | http://arxiv.org/abs/1607.02802 | id:1607.02802 author:Franck Dernoncourt category:cs.CL cs.AI stat.ML  published:2016-07-11 summary:Word embeddings have been shown to be useful across state-of-the-art systems in many natural language processing tasks, ranging from question answering systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word embeddings and their utility for modeling language semantics. In particular, they presented an approach to automatically map a standard distributional semantic space onto a set-theoretic model using partial least squares regression. We show in this paper that a simple baseline achieves a +51% relative improvement compared to their model on one of the two datasets they used, and yields competitive results on the second dataset. version:1
arxiv-1607-02801 | Bounds on the Number of Measurements for Reliable Compressive Classification | http://arxiv.org/abs/1607.02801 | id:1607.02801 author:Hugo Reboredo, Francesco Renna, Robert Calderbank, Miguel R. D. Rodrigues category:cs.IT cs.CV math.IT stat.ML  published:2016-07-11 summary:This paper studies the classification of high-dimensional Gaussian signals from low-dimensional noisy, linear measurements. In particular, it provides upper bounds (sufficient conditions) on the number of measurements required to drive the probability of misclassification to zero in the low-noise regime, both for random measurements and designed ones. Such bounds reveal two important operational regimes that are a function of the characteristics of the source: i) when the number of classes is less than or equal to the dimension of the space spanned by signals in each class, reliable classification is possible in the low-noise regime by using a one-vs-all measurement design; ii) when the dimension of the spaces spanned by signals in each class is lower than the number of classes, reliable classification is guaranteed in the low-noise regime by using a simple random measurement design. Simulation results both with synthetic and real data show that our analysis is sharp, in the sense that it is able to gauge the number of measurements required to drive the misclassification probability to zero in the low-noise regime. version:1
arxiv-1607-02793 | An Improved Convergence Analysis of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization | http://arxiv.org/abs/1607.02793 | id:1607.02793 author:Xingguo Li, Tuo Zhao, Raman Arora, Han Liu, Mingyi Hong category:math.OC cs.LG stat.ML  published:2016-07-10 summary:The cyclic block coordinate descent-type (CBCD-type) methods have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that the CBCD-type methods attain iteration complexity of $O(p\cdot\log(1/\epsilon))$, where $\epsilon$ is a pre-specified accuracy of the objective value, and $p$ is the number of blocks. However, such iteration complexity explicitly depends on $p$, and therefore is at least $p$ times worse than those of gradient descent methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity of the CBCD-type methods matches that of the gradient descent methods in term of dependency on $p$ (up to a $\log^2 p$ factor). Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\log^2p$. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a $\log^2 p$ factor) if the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones version:1
arxiv-1607-02791 | Syntactic Phylogenetic Trees | http://arxiv.org/abs/1607.02791 | id:1607.02791 author:Kevin Shu, Sharjeel Aziz, Vy-Luan Huynh, David Warrick, Matilde Marcolli category:cs.CL 91F20  13P10  published:2016-07-10 summary:In this paper we identify several serious problems that arise in the use of syntactic data from the SSWL database for the purpose of computational phylogenetic reconstruction. We show that the most naive approach fails to produce reliable linguistic phylogenetic trees. We identify some of the sources of the observed problems and we discuss how they may be, at least partly, corrected by using additional information, such as prior subdivision into language families and subfamilies, and a better use of the information about ancient languages. We also describe how the use of phylogenetic algebraic geometry can help in estimating to what extent the probability distribution at the leaves of the phylogenetic tree obtained from the SSWL data can be considered reliable, by testing it on phylogenetic trees established by other forms of linguistic analysis. In simple examples, we find that, after restricting to smaller language subfamilies and considering only those SSWL parameters that are fully mapped for the whole subfamily, the SSWL data match extremely well reliable phylogenetic trees, according to the evaluation of phylogenetic invariants. This is a promising sign for the use of SSWL data for linguistic phylogenetics. version:1
arxiv-1607-02789 | Charagram: Embedding Words and Sentences via Character n-grams | http://arxiv.org/abs/1607.02789 | id:1607.02789 author:John Wieting, Mohit Bansal, Kevin Gimpel, Karen Livescu category:cs.CL  published:2016-07-10 summary:We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks. version:1
arxiv-1607-02784 | Open Information Extraction | http://arxiv.org/abs/1607.02784 | id:1607.02784 author:Duc-Thuan Vo, Ebrahim Bagheri category:cs.CL cs.AI  published:2016-07-10 summary:Open Information Extraction (Open IE) systems aim to obtain relation tuples with highly scalable extraction in portable across domain by identifying a variety of relation phrases and their arguments in arbitrary sentences. The first generation of Open IE learns linear chain models based on unlexicalized features such as Part-of-Speech (POS) or shallow tags to label the intermediate words between pair of potential arguments for identifying extractable relations. Open IE currently is developed in the second generation that is able to extract instances of the most frequently observed relation types such as Verb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic analysis. They expose simple yet principled ways in which verbs express relationships in linguistics such as verb phrase-based extraction or clause-based extraction. They obtain a significantly higher performance over previous systems in the first generation. In this paper, we describe an overview of two Open IE generations including strengths, weaknesses and application areas. version:1
arxiv-1607-02769 | Annotation Methodologies for Vision and Language Dataset Creation | http://arxiv.org/abs/1607.02769 | id:1607.02769 author:Gitit Kehat, James Pustejovsky category:cs.CV cs.CL  published:2016-07-10 summary:Annotated datasets are commonly used in the training and evaluation of tasks involving natural language and vision (image description generation, action recognition and visual question answering). However, many of the existing datasets reflect problems that emerge in the process of data selection and annotation. Here we point out some of the difficulties and problems one confronts when creating and validating annotated vision and language datasets. version:1
arxiv-1607-02748 | Adversarial Training For Sketch Retrieval | http://arxiv.org/abs/1607.02748 | id:1607.02748 author:Antonia Creswell, Anil Anthony Bharath category:cs.CV  published:2016-07-10 summary:Generative Adversarial Networks (GAN) can learn excellent representations for unlabelled data which have been applied to image generation and scene classification. The representations have not yet - to the best of our knowledge - been applied to visual search. In this paper, we show that representations learned by GANs can be applied to visual search within heritage documents that contain Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that makes it suitable for sketch understanding. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation. version:1
arxiv-1607-02738 | Magnetic Hamiltonian Monte Carlo | http://arxiv.org/abs/1607.02738 | id:1607.02738 author:Nilesh Tripuraneni, Mark Rowland, Zoubin Ghahramani, Richard Turner category:stat.ML  published:2016-07-10 summary:Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits \textit{non-canonical} Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC. version:1
arxiv-1607-02737 | Transition Forests: Learning Discriminative Temporal Transitions for Action Recognition | http://arxiv.org/abs/1607.02737 | id:1607.02737 author:Guillermo Garcia-Hernando, Tae-Kyun Kim category:cs.CV  published:2016-07-10 summary:A human action can be seen as transitions between one's body poses over time, where the transition depicts a temporal relation between two poses. Recognizing actions thus involves learning a classifier sensitive to these pose transitions from given high-dimensional frame representations. In this paper, we introduce transitions forests, an ensemble of decision trees that learn transitions between pairs of two independent frames in a discriminative fashion. During training, node splitting is driven by alternating two different criteria: the standard classification entropy that maximizes the discrimination power in individual frames, and the proposed one in pairwise frame transitions. Growing the trees tends to group frames that have similar associated transitions and share same action label. Unlike conventional classification trees where node-wise the best split is determined, the transition forests try to find the best split of nodes jointly (within a layer) for incorporating distant node transitions. When inferring the class label of a new video, frames are independently passed down the trees (thus highly efficient) then a prediction for a certain time-frame is made based on the transitions between previous observed frames and the current one in an efficient manner. We apply our method on varied action recognition datasets showing its suitability over several baselines and state-of-the-art approaches. version:1
arxiv-1607-02104 | Zero-Shot Visual Recognition via Bidirectional Latent Embedding | http://arxiv.org/abs/1607.02104 | id:1607.02104 author:Qian Wang, Ke Chen category:cs.CV  published:2016-07-07 summary:Zero-shot learning for visual recognition, e.g., object and action recognition, has recently attracted a lot of attention. However, it still remains challenging in bridging the semantic gap between visual features and their underlying semantics and transferring knowledge to semantic categories unseen during learning. Unlike most of the existing methods that learn either a direct mapping from visual features to their semantic representations or a common latent space by the joint use of visual features and their semantic representations, we propose a stagewise bidirectional latent embedding framework for zero-shot visual recognition. In the bottom-up stage, a latent embedding space is first created by exploring the topological and labeling information underlying training data of known classes via supervised locality preserving projection and the latent representations of training data are used to form landmarks that guide embedding semantics underlying unseen classes onto this latent space. In the top-down stage, semantic representations for unseen classes are then projected to the latent embedding space to preserve the semantic relatedness via the semi-supervised Sammon mapping with landmarks. As a result, the resultant latent embedding space allows for predicting the label of a test instance with a simple nearest neighbor algorithm. To evaluate the effectiveness of the proposed framework, we have conducted experiments on four benchmark datasets in object and action recognition, i.e., AwA, CUB-200-2011, UCF101 and HMDB51. The experimental results under comparative studies demonstrate that our proposed approach yields the state-of-the-art performance. version:2
arxiv-1607-02720 | Memory Efficient Nonuniform Quantization for Deep Convolutional Neural Network | http://arxiv.org/abs/1607.02720 | id:1607.02720 author:Fangxuan Sun, Jun Lin category:cs.CV  published:2016-07-10 summary:Convolutional neural network (CNN) is one of the most famous algorithms for deep learning. It has been applied in various applications due to its remarkable performance. The real-time hardware implement of CNN is highly demanded due to its excellent performance in computer vision. However, the cost of memory of a deep CNN is very huge which increases the area of hardware implementation. In this paper, we apply several methods in the quantization of CNN and use about 5 bits for convolutional layers. The accuracy lost is less than $2\%$ without fine tuning. Our experiment is depending on the VGG-16 net and Alex net. In VGG-16 net, the total memory needed after uniform quantization is 16.85 MB per image and the total memory needed after our quantization is only about 8.42 MB. Our quantization method has saved $50.0\%$ of the memory needed in VGG-16 and Alex net compared with the state-of-art quantization method. version:1
arxiv-1607-02717 | The BioDynaMo Project | http://arxiv.org/abs/1607.02717 | id:1607.02717 author:Roman Bauer, Lukas Breitwieser, Alberto Di Meglio, Leonard Johard, Marcus Kaiser, Marco Manca, Manuel Mazzara, Max Talanov category:cs.NE  published:2016-07-10 summary:Computer simulations have become a very powerful tool for scientific research. Given the vast complexity that comes with many open scientific questions, a purely analytical or experimental approach is often not viable. For example, biological systems (such as the human brain) comprise an extremely complex organization and heterogeneous interactions across different spatial and temporal scales. In order to facilitate research on such problems, the BioDynaMo project (\url{https://biodynamo.web.cern.ch/}) aims at a general platform for computer simulations for biological research. Since the scientific investigations require extensive computer resources, this platform should be executable on hybrid cloud computing systems, allowing for the efficient use of state-of-the-art computing technology. This paper describes challenges during the early stages of the software development process. In particular, we describe issues regarding the implementation and the highly interdisciplinary as well as international nature of the collaboration. Moreover, we explain the methodologies, the approach, and the lessons learnt by the team during these first stages. version:1
arxiv-1607-02715 | Learning to Sketch Human Facial Portraits using Personal Styles by Case-Based Reasoning | http://arxiv.org/abs/1607.02715 | id:1607.02715 author:Bingwen Jin, Songhua Xu, Weidong Geng category:cs.CV  published:2016-07-10 summary:This paper employs case-based reasoning (CBR) to capture the personal styles of individual artists and generate the human facial portraits from photos accordingly. For each human artist to be mimicked, a series of cases are firstly built-up from her/his exemplars of source facial photo and hand-drawn sketch, and then its stylization for facial photo is transformed as a style-transferring process of iterative refinement by looking-for and applying best-fit cases in a sense of style optimization. Two models, fitness evaluation model and parameter estimation model, are learned for case retrieval and adaptation respectively from these cases. The fitness evaluation model is to decide which case is best-fitted to the sketching of current interest, and the parameter estimation model is to automate case adaptation. The resultant sketch is synthesized progressively with an iterative loop of retrieval and adaptation of candidate cases until the desired aesthetic style is achieved. To explore the effectiveness and advantages of the novel approach, we experimentally compare the sketch portraits generated by the proposed method with that of a state-of-the-art example-based facial sketch generation algorithm as well as a couple commercial software packages. The comparisons reveal that our CBR based synthesis method for facial portraits is superior both in capturing and reproducing artists' personal illustration styles to the peer methods. version:1
arxiv-1607-02705 | Dealing with Class Imbalance using Thresholding | http://arxiv.org/abs/1607.02705 | id:1607.02705 author:Charmgil Hong, Rumi Ghosh, Soundar Srinivasan category:cs.LG  published:2016-07-10 summary:We propose thresholding as an approach to deal with class imbalance. We define the concept of thresholding as a process of determining a decision boundary in the presence of a tunable parameter. The threshold is the maximum value of this tunable parameter where the conditions of a certain decision are satisfied. We show that thresholding is applicable not only for linear classifiers but also for non-linear classifiers. We show that this is the implicit assumption for many approaches to deal with class imbalance in linear classifiers. We then extend this paradigm beyond linear classification and show how non-linear classification can be dealt with under this umbrella framework of thresholding. The proposed method can be used for outlier detection in many real-life scenarios like in manufacturing. In advanced manufacturing units, where the manufacturing process has matured over time, the number of instances (or parts) of the product that need to be rejected (based on a strict regime of quality tests) becomes relatively rare and are defined as outliers. How to detect these rare parts or outliers beforehand? How to detect combination of conditions leading to these outliers? These are the questions motivating our research. This paper focuses on prediction of outliers and conditions leading to outliers using classification. We address the problem of outlier detection using classification. The classes are good parts (those passing the quality tests) and bad parts (those failing the quality tests and can be considered as outliers). The rarity of outliers transforms this problem into a class-imbalanced classification problem. version:1
arxiv-1607-02678 | Towards an "In-the-Wild" Emotion Dataset Using a Game-based Framework | http://arxiv.org/abs/1607.02678 | id:1607.02678 author:Wei Li, Farnaz Abtahi, Christina Tsangouri, Zhigang Zhu category:cs.CV  published:2016-07-10 summary:In order to create an "in-the-wild" dataset of facial emotions with large number of balanced samples, this paper proposes a game-based data collection framework. The framework mainly include three components: a game engine, a game interface, and a data collection and evaluation module. We use a deep learning approach to build an emotion classifier as the game engine. Then a emotion web game to allow gamers to enjoy the games, while the data collection module obtains automatically-labelled emotion images. Using our game, we have collected more than 15,000 images within a month of the test run and built an emotion dataset "GaMo". To evaluate the dataset, we compared the performance of two deep learning models trained on both GaMo and CIFE. The results of our experiments show that because of being large and balanced, GaMo can be used to build a more robust emotion detector than the emotion detector trained on CIFE, which was used in the game engine to collect the face images. version:1
arxiv-1607-02676 | Bayesian quantile additive regression trees | http://arxiv.org/abs/1607.02676 | id:1607.02676 author:Bereket P. Kindo, Hao Wang, Timothy Hanson, Edsel A. Peña category:stat.ML  published:2016-07-10 summary:Ensemble of regression trees have become popular statistical tools for the estimation of conditional mean given a set of predictors. However, quantile regression trees and their ensembles have not yet garnered much attention despite the increasing popularity of the linear quantile regression model. This work proposes a Bayesian quantile additive regression trees model that shows very good predictive performance illustrated using simulation studies and real data applications. Further extension to tackle binary classification problems is also considered. version:1
arxiv-1607-02675 | Convex Relaxation for Community Detection with Covariates | http://arxiv.org/abs/1607.02675 | id:1607.02675 author:Bowei Yan, Purnamrita Sarkar category:stat.ME stat.ML  published:2016-07-10 summary:Community detection in networks is an important problem in many applied areas. In this paper, we investigate this in the presence of node covariates. Recently, an emerging body of theoretical work has been focused on leveraging information from both the edges in the network and the node covariates to infer community memberships. However, so far the role of the network and that of the covariates have not been examined closely. In essence, in most parameter regimes, one of the sources of information provides enough information to infer the hidden clusters, thereby making the other source redundant. To our knowledge, this is the first work which shows that when the network and the covariates carry "orthogonal" pieces of information about the cluster memberships, one can get asymptotically consistent clustering by using them both, while each of them fails individually. version:1
arxiv-1607-02670 | Sparse additive Gaussian process with soft interactions | http://arxiv.org/abs/1607.02670 | id:1607.02670 author:Garret Vo, Debdeep Pati category:stat.ML  published:2016-07-09 summary:Additive nonparametric regression models provide an attractive tool for variable selection in high dimensions when the relationship between the response and predictors is complex. They offer greater flexibility compared to parametric non-linear regression models and better interpretability and scalability than the non-parametric regression models. However, achieving sparsity simultaneously in the number of nonparametric components as well as in the variables within each nonparametric component poses a stiff computational challenge. In this article, we develop a novel Bayesian additive regression model using a combination of hard and soft shrinkages to separately control the number of additive components and the variables within each component. An efficient algorithm is developed to select the importance variables and estimate the interaction network. Excellent performance is obtained in simulated and real data examples. version:1
arxiv-1607-02665 | Classifier Risk Estimation under Limited Labeling Resources | http://arxiv.org/abs/1607.02665 | id:1607.02665 author:Anurag Kumar, Bhiksha Raj category:cs.LG stat.AP stat.ML  published:2016-07-09 summary:In this paper we propose strategies for estimating performance of a classifier when labels cannot be obtained for the whole test set. The number of test instances which can be labeled is very small compared to the whole test data size. The goal then is to obtain a precise estimate of classifier performance using as little labeling resource as possible. Specifically, we try to answer, how to select a subset of the large test set for labeling such that the performance of a classifier estimated on this subset is as close as possible to the one on the whole test set. We propose strategies based on stratified sampling for selecting this subset. We show that these strategies can reduce the variance in estimation of classifier accuracy by a significant amount compared to simple random sampling (over 65% in several cases). Hence, our proposed methods are much more precise compared to random sampling for accuracy estimation under restricted labeling resources. The reduction in number of samples required (compared to random sampling) to estimate the classifier accuracy with only 1% error is high as 60% in some cases. version:1
arxiv-1607-02660 | Augmenting Supervised Emotion Recognition with Rule-Based Decision Model | http://arxiv.org/abs/1607.02660 | id:1607.02660 author:Amol Patwardhan, Gerald Knapp category:cs.HC cs.AI cs.CV  published:2016-07-09 summary:The aim of this research is development of rule based decision model for emotion recognition. This research also proposes using the rules for augmenting inter-corporal recognition accuracy in multimodal systems that use supervised learning techniques. The classifiers for such learning based recognition systems are susceptible to over fitting and only perform well on intra-corporal data. To overcome the limitation this research proposes using rule based model as an additional modality. The rules were developed using raw feature data from visual channel, based on human annotator agreement and existing studies that have attributed movement and postures to emotions. The outcome of the rule evaluations was combined during the decision phase of emotion recognition system. The results indicate rule based emotion recognition augment recognition accuracy of learning based systems and also provide better recognition rate across inter corpus emotion test data. version:1
arxiv-1607-02654 | Combining multiple resolutions into hierarchical representations for kernel-based image classification | http://arxiv.org/abs/1607.02654 | id:1607.02654 author:Yanwei Cui, Sébastien Lefevre, Laetitia Chapel, Anne Puissant category:cs.CV stat.ML  published:2016-07-09 summary:Geographic object-based image analysis (GEOBIA) framework has gained increasing interest recently. Following this popular paradigm, we propose a novel multiscale classification approach operating on a hierarchical image representation built from two images at different resolutions. They capture the same scene with different sensors and are naturally fused together through the hierarchical representation, where coarser levels are built from a Low Spatial Resolution (LSR) or Medium Spatial Resolution (MSR) image while finer levels are generated from a High Spatial Resolution (HSR) or Very High Spatial Resolution (VHSR) image. Such a representation allows one to benefit from the context information thanks to the coarser levels, and subregions spatial arrangement information thanks to the finer levels. Two dedicated structured kernels are then used to perform machine learning directly on the constructed hierarchical representation. This strategy overcomes the limits of conventional GEOBIA classification procedures that can handle only one or very few pre-selected scales. Experiments run on an urban classification task show that the proposed approach can highly improve the classification accuracy w.r.t. conventional approaches working on a single scale. version:2
arxiv-1607-02652 | Multimodal Affect Recognition using Kinect | http://arxiv.org/abs/1607.02652 | id:1607.02652 author:Amol Patwardhan, Gerald Knapp category:cs.HC cs.CV  published:2016-07-09 summary:Affect (emotion) recognition has gained significant attention from researchers in the past decade. Emotion-aware computer systems and devices have many applications ranging from interactive robots, intelligent online tutor to emotion based navigation assistant. In this research data from multiple modalities such as face, head, hand, body and speech was utilized for affect recognition. The research used color and depth sensing device such as Kinect for facial feature extraction and tracking human body joints. Temporal features across multiple frames were used for affect recognition. Event driven decision level fusion was used to combine the results from each individual modality using majority voting to recognize the emotions. The study also implemented affect recognition by matching the features to the rule based emotion templates per modality. Experiments showed that multimodal affect recognition rates using combination of emotion templates and supervised learning were better compared to recognition rates based on supervised learning alone. Recognition rates obtained using temporal feature were higher compared to recognition rates obtained using position based features only. version:1
arxiv-1607-02643 | Hierarchical Deep Temporal Models for Group Activity Recognition | http://arxiv.org/abs/1607.02643 | id:1607.02643 author:Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat, Greg Mori category:cs.CV  published:2016-07-09 summary:In this paper we present an approach for classifying the activity performed by a group of people in a video sequence. This problem of group activity recognition can be addressed by examining individual person actions and their relations. Temporal dynamics exist both at the level of individual person actions as well as at the level of group activity. Given a video sequence as input, methods can be developed to capture these dynamics at both person-level and group-level detail. We build a deep model to capture these dynamics based on LSTM (long short-term memory) models. In order to model both person-level and group-level dynamics, we present a 2-stage deep temporal model for the group activity recognition problem. In our approach, one LSTM model is designed to represent action dynamics of individual people in a video sequence and another LSTM model is designed to aggregate person-level information for group activity recognition. We collected a new dataset consisting of volleyball videos labeled with individual and group activities in order to evaluate our method. Experimental results on this new Volleyball Dataset and the standard benchmark Collective Activity Dataset demonstrate the efficacy of the proposed models. version:1
arxiv-1607-02624 | Beating level-set methods for 3D seismic data interpolation: a primal-dual alternating approach | http://arxiv.org/abs/1607.02624 | id:1607.02624 author:Rajiv Kumar, Oscar López, Damek Davis, Aleksandr Y. Aravkin, Felix J. Herrmann category:math.OC stat.ML 62F35  65K10  published:2016-07-09 summary:Acquisition cost is a crucial bottleneck for seismic workflows, and low-rank formulations for data interpolation allow practitioners to `fill in' data volumes from critically subsampled data acquired in the field. Tremendous size of seismic data volumes required for seismic processing remains a major challenge for these techniques. We propose a new approach to solve residual constrained formulations for interpolation. We represent the data volume using matrix factors, and build a block-coordinate algorithm with constrained convex subproblems that are solved with a primal-dual splitting scheme. The new approach is competitive with state of the art level-set algorithms that interchange the role of objectives with constraints. We use the new algorithm to successfully interpolate a large scale 5D seismic data volume, generated from the geologically complex synthetic 3D Compass velocity model, where 80% of the data has been removed. version:1
arxiv-1607-02586 | Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks | http://arxiv.org/abs/1607.02586 | id:1607.02586 author:Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman category:cs.CV cs.LG  published:2016-07-09 summary:We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose a novel approach that models future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. Future frame synthesis is challenging, as it involves low- and high-level image and motion understanding. We propose a novel network structure, namely a Cross Convolutional Network to aid in synthesizing future frames; this network structure encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-wold videos. We also show that our model can be applied to tasks such as visual analogy-making, and present an analysis of the learned network representations. version:1
arxiv-1607-02584 | A Unified Alternating Direction Method of Multipliers by Majorization Minimization | http://arxiv.org/abs/1607.02584 | id:1607.02584 author:Canyi Lu, Jiashi Feng, Shuicheng Yan, Zhouchen Lin category:cs.NA  published:2016-07-09 summary:Accompanied with the rising popularity of compressed sensing, the Alternating Direction Method of Multipliers (ADMM) has become the most widely used solver for linearly constrained convex problems with separable objectives. In this work, we observe that many previous variants of ADMM update the primal variable by minimizing different majorant functions with their convergence proofs given case by case. Inspired by the principle of majorization minimization, we respectively present the unified frameworks and convergence analysis for the Gauss-Seidel ADMMs and Jacobian ADMMs, which use different historical information for the current updating. Our frameworks further generalize previous ADMMs to the ones capable of solving the problems with non-separable objectives by minimizing their separable majorant surrogates. We also show that the bound which measures the convergence speed of ADMMs depends on the tightness of the used majorant function. Then several techniques are introduced to improve the efficiency of ADMMs by tightening the majorant functions. In particular, we propose the Mixed Gauss-Seidel and Jacobian ADMM (M-ADMM) which alleviates the slow convergence issue of Jacobian ADMMs by absorbing merits of the Gauss-Seidel ADMMs. M-ADMM can be further improved by using backtracking, wise variable partition and fully exploiting the structure of the constraint. Beyond the guarantee in theory, numerical experiments on both synthesized and real-world data further demonstrate the superiority of our new ADMMs in practice. Finally, we release a toolbox at https://github.com/canyilu/LibADMM that implements efficient ADMMs for many problems in compressed sensing. version:1
arxiv-1607-02576 | Analysis of opinionated text for opinion mining | http://arxiv.org/abs/1607.02576 | id:1607.02576 author:K Paramesha, K C Ravishankar category:cs.CL cs.AI cs.IR  published:2016-07-09 summary:In sentiment analysis, the polarities of the opinions expressed on an object/feature are determined to assess the sentiment of a sentence or document whether it is positive/negative/neutral. Naturally, the object/feature is a noun representation which refers to a product or a component of a product, let us say, the "lens" in a camera and opinions emanating on it are captured in adjectives, verbs, adverbs and noun words themselves. Apart from such words, other meta-information and diverse effective features are also going to play an important role in influencing the sentiment polarity and contribute significantly to the performance of the system. In this paper, some of the associated information/meta-data are explored and investigated in the sentiment text. Based on the analysis results presented here, there is scope for further assessment and utilization of the meta-information as features in text categorization, ranking text document, identification of spam documents and polarity classification problems. version:1
arxiv-1607-00847 | Confidence-Weighted Bipartite Ranking | http://arxiv.org/abs/1607.00847 | id:1607.00847 author:Majdi Khaled, Indrakshi Ray, Hamidreza Chitsaz category:cs.LG  published:2016-07-04 summary:Bipartite ranking is a fundamental machine learning and data mining problem. It commonly concerns the maximization of the AUC metric. Recently, a number of studies have proposed online bipartite ranking for maximizing the AUC to learn from massive streams of class-imbalanced data. These methods suggest both linear and kernel-based bipartite ranking based on first and second-order online learning. Unlike kernelized ranker, linear ranker is more scalable learning algorithm. The existing linear online bipartite ranking frameworks lack either handling non-separable data or constructing adaptive large margin. In this work, we propose a linear online confidence-weighted bipartite ranking algorithm (CBR) that adopts soft confidence-weighted learning. The proposed algorithm leverages the same properties of soft confidence-weighted learning in a framework for bipartite ranking. We also propose a diagonal variation of the soft confidence-weighted algorithm to deal with high-dimensional data. We empirically evaluate the effectiveness of the proposed algorithms on several benchmark and high-dimensional datasets. The results validate the reliability of the proposed algorithms. The experimental results also show that our algorithms outperform or are at least comparable to the competing online AUC maximization methods. version:2
arxiv-1607-02568 | Deep Learning of Appearance Models for Online Object Tracking | http://arxiv.org/abs/1607.02568 | id:1607.02568 author:Mengyao Zhai, Mehrsan Javan Roshtkhari, Greg Mori category:cs.CV  published:2016-07-09 summary:This paper introduces a novel deep learning based approach for vision based single target tracking. We address this problem by proposing a network architecture which takes the input video frames and directly computes the tracking score for any candidate target location by estimating the probability distributions of the positive and negative examples. This is achieved by combining a deep convolutional neural network with a Bayesian loss layer in a unified framework. In order to deal with the limited number of positive training examples, the network is pre-trained offline for a generic image feature representation and then is fine-tuned in multiple steps. An online fine-tuning step is carried out at every frame to learn the appearance of the target. We adopt a two-stage iterative algorithm to adaptively update the network parameters and maintain a probability density for target/non-target regions. The tracker has been tested on the standard tracking benchmark and the results indicate that the proposed solution achieves state-of-the-art tracking results. version:1
arxiv-1607-02565 | Direct Sparse Odometry | http://arxiv.org/abs/1607.02565 | id:1607.02565 author:Jakob Engel, Vladlen Koltun, Daniel Cremers category:cs.CV  published:2016-07-09 summary:We propose a novel direct sparse visual odometry formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry -- represented as inverse depth in a reference frame -- and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on mostly white walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non-linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness. version:1
arxiv-1607-02559 | Uncovering Locally Discriminative Structure for Feature Analysis | http://arxiv.org/abs/1607.02559 | id:1607.02559 author:Sen Wang, Feiping Nie, Xiaojun Chang, Xue Li, Quan Z. Sheng, Lina Yao category:cs.LG  published:2016-07-09 summary:Manifold structure learning is often used to exploit geometric information among data in semi-supervised feature learning algorithms. In this paper, we find that local discriminative information is also of importance for semi-supervised feature learning. We propose a method that utilizes both the manifold structure of data and local discriminant information. Specifically, we define a local clique for each data point. The k-Nearest Neighbors (kNN) is used to determine the structural information within each clique. We then employ a variant of Fisher criterion model to each clique for local discriminant evaluation and sum all cliques as global integration into the framework. In this way, local discriminant information is embedded. Labels are also utilized to minimize distances between data from the same class. In addition, we use the kernel method to extend our proposed model and facilitate feature learning in a high-dimensional space after feature mapping. Experimental results show that our method is superior to all other compared methods over a number of datasets. version:1
arxiv-1607-02556 | Action Recognition with Joint Attention on Multi-Level Deep Features | http://arxiv.org/abs/1607.02556 | id:1607.02556 author:Jialin Wu, Gu Wang, Wukui Yang, Xiangyang Ji category:cs.CV  published:2016-07-09 summary:We propose a novel deep supervised neural network for the task of action recognition in videos, which implicitly takes advantage of visual tracking and shares the robustness of both deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). In our method, a multi-branch model is proposed to suppress noise from background jitters. Specifically, we firstly extract multi-level deep features from deep CNNs and feed them into 3d-convolutional network. After that we feed those feature cubes into our novel joint LSTM module to predict labels and to generate attention regularization. We evaluate our model on two challenging datasets: UCF101 and HMDB51. The results show that our model achieves the state-of-art by only using convolutional features. version:1
arxiv-1607-02555 | A Photometrically Calibrated Benchmark For Monocular Visual Odometry | http://arxiv.org/abs/1607.02555 | id:1607.02555 author:Jakob Engel, Vladyslav Usenko, Daniel Cremers category:cs.CV  published:2016-07-09 summary:We present a dataset for evaluating the tracking accuracy of monocular visual odometry and SLAM methods. It contains 50 real-world sequences comprising more than 100 minutes of video, recorded across dozens of different environments -- ranging from narrow indoor corridors to wide outdoor scenes. All sequences contain mostly exploring camera motion, starting and ending at the same position. This allows to evaluate tracking accuracy via the accumulated drift from start to end, without requiring ground truth for the full sequence. In contrast to existing datasets, all sequences are photometrically calibrated. We provide exposure times for each frame as reported by the sensor, the camera response function, and dense lens attenuation factors. We also propose a novel, simple approach to non-parametric vignette calibration, which requires minimal set-up and is easy to reproduce. Finally, we thoroughly evaluate two existing methods (ORB-SLAM and DSO) on the dataset, including an analysis of the effect of image resolution, camera field of view, and the camera motion direction. version:1
arxiv-1607-02552 | Online Learning Schemes for Power Allocation in Energy Harvesting Communications | http://arxiv.org/abs/1607.02552 | id:1607.02552 author:Pranav Sakulkar, Bhaskar Krishnamachari category:cs.LG  published:2016-07-08 summary:We consider the problem of power allocation over a time-varying channel with unknown distribution in energy harvesting communication systems. In this problem, the transmitter has to choose the transmit power based on the amount of stored energy in its battery with the goal of maximizing the average rate obtained over time. We model this problem as a Markov decision process (MDP) with the transmitter as the agent, the battery status as the state, the transmit power as the action and the rate obtained as the reward. The average reward maximization problem over the MDP can be solved by a linear program (LP) that uses the transition probabilities for the state-action pairs and their reward values to choose a power allocation policy. Since the rewards associated the state-action pairs are unknown, we propose two online learning algorithms: UCLP and Epoch-UCLP that learn these rewards and adapt their policies along the way. The UCLP algorithm solves the LP at each step to decide its current policy using the upper confidence bounds on the rewards, while the Epoch-UCLP algorithm divides the time into epochs, solves the LP only at the beginning of the epochs and follows the obtained policy in that epoch. We prove that the reward losses or regrets incurred by both these algorithms are upper bounded by constants. Epoch-UCLP incurs a higher regret compared to UCLP, but reduces the computational requirements substantially. We also show that the presented algorithms work for online learning in cost minimization problems like the packet scheduling with power-delay tradeoff with minor changes. version:1
arxiv-1607-02547 | Screen Content Image Segmentation Using Robust Regression and Sparse Decomposition | http://arxiv.org/abs/1607.02547 | id:1607.02547 author:Shervin Minaee, Yao Wang category:cs.CV  published:2016-07-08 summary:This paper considers how to separate text and/or graphics from smooth background in screen content and mixed document images and proposes two approaches to perform this segmentation task. The proposed methods make use of the fact that the background in each block is usually smoothly varying and can be modeled well by a linear combination of a few smoothly varying basis functions, while the foreground text and graphics create sharp discontinuity. The algorithms separate the background and foreground pixels by trying to fit background pixel values in the block into a smooth function using two different schemes. One is based on robust regression, where the inlier pixels will be considered as background, while remaining outlier pixels will be considered foreground. The second approach uses a sparse decomposition framework where the background and foreground layers are modeled with a smooth and sparse components respectively. These algorithms have been tested on images extracted from HEVC standard test sequences for screen content coding, and are shown to have superior performance over previous approaches. The proposed methods can be used in different applications such as text extraction, separate coding of background and foreground for compression of screen content, and medical image segmentation. version:1
arxiv-1607-02539 | Graph Construction with Label Information for Semi-Supervised Learning | http://arxiv.org/abs/1607.02539 | id:1607.02539 author:Liansheng Zhuang, Zihan Zhou, Jingwen Yin, Shenghua Gao, Zhouchen Lin, Yi Ma, Nenghai Yu category:cs.CV  published:2016-07-08 summary:In the literature, most existing graph-based semi-supervised learning (SSL) methods only use the label information of observed samples in the label propagation stage, while ignoring such valuable information when learning the graph. In this paper, we argue that it is beneficial to consider the label information in the graph learning stage. Specifically, by enforcing the weight of edges between labeled samples of different classes to be zero, we explicitly incorporate the label information into the state-of-the-art graph learning methods, such as the Low-Rank Representation (LRR), and propose a novel semi-supervised graph learning method called Semi-Supervised Low-Rank Representation (SSLRR). This results in a convex optimization problem with linear constraints, which can be solved by the linearized alternating direction method. Though we take LRR as an example, our proposed method is in fact very general and can be applied to any self-representation graph learning methods. Experiment results on both synthetic and real datasets demonstrate that the proposed graph learning method can better capture the global geometric structure of the data, and therefore is more effective for semi-supervised learning tasks. version:2
arxiv-1607-02537 | Multi-level Contextual RNNs with Attention Model for Scene Labeling | http://arxiv.org/abs/1607.02537 | id:1607.02537 author:Heng Fan, Xue Mei, Danil Prokhorov, Haibin Ling category:cs.CV  published:2016-07-08 summary:Context in image is crucial for scene labeling while existing methods only exploit local context generated from a small surrounding area of an image patch or a pixel, by contrast long-range and global contextual information is ignored. To handle this issue, we in this work propose a novel approach for scene labeling by exploring multi-level contextual recurrent neural networks (ML-CRNNs). Specifically, we encode three kinds of contextual cues, i.e., local context, global context and image topic context in structural recurrent neural networks (RNNs) to model long-range local and global dependencies in image. In this way, our method is able to `see' the image in terms of both long-range local and holistic views, and make a more reliable inference for image labeling. Besides, we integrate the proposed contextual RNNs into hierarchical convolutional neural networks (CNNs), and exploit dependence relationships in multiple levels to provide rich spatial and semantic information. Moreover, we novelly adopt an attention model to effectively merge multiple levels and show that it outperforms average- or max-pooling fusion strategies. Extensive experiments demonstrate that the proposed approach achieves new state-of-the-art results on the CamVid, SiftFlow and Stanford-background datasets. version:1
arxiv-1607-02535 | Learning from Multiway Data: Simple and Efficient Tensor Regression | http://arxiv.org/abs/1607.02535 | id:1607.02535 author:Rose Yu, Yan Liu category:cs.LG  published:2016-07-08 summary:Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications. version:1
arxiv-1607-02533 | Adversarial examples in the physical world | http://arxiv.org/abs/1607.02533 | id:1607.02533 author:Alexey Kurakin, Ian Goodfellow, Samy Bengio category:cs.CV cs.CR cs.LG stat.ML  published:2016-07-08 summary:Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera. version:1
arxiv-1607-02531 | Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016) | http://arxiv.org/abs/1607.02531 | id:1607.02531 author:Been Kim, Dmitry M. Malioutov, Kush R. Varshney category:stat.ML cs.LG  published:2016-07-08 summary:This is the Proceedings of the 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), which was held in New York, NY, June 23, 2016. Invited speakers were Susan Athey, Rich Caruana, Jacob Feldman, Percy Liang, and Hanna Wallach. version:1
arxiv-1607-02516 | Pseudo-Marginal Hamiltonian Monte Carlo | http://arxiv.org/abs/1607.02516 | id:1607.02516 author:Fredrik Lindsten, Arnaud Doucet category:stat.ME stat.ML  published:2016-07-08 summary:Bayesian inference in the presence of an intractable likelihood function is computationally challenging. When following a Markov chain Monte Carlo (MCMC) approach to approximate the posterior distribution in this context, one typically either uses MCMC schemes which target the joint posterior of the parameters and some auxiliary latent variables or pseudo-marginal Metropolis-Hastings (MH) schemes which mimic a MH algorithm targeting the marginal posterior of the parameters by approximating unbiasedly the intractable likelihood. In scenarios where the parameters and auxiliary variables are strongly correlated under the posterior and/or this posterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will perform poorly and the pseudo-marginal MH algorithm, as any other MH scheme, will be inefficient for high dimensional parameters. We propose here an original MCMC algorithm, termed pseudo-marginal HMC, which approximates the HMC algorithm targeting the marginal posterior of the parameters. We demonstrate through experiments that pseudo-marginal HMC can outperform significantly both standard HMC and pseudo-marginal MH schemes. version:1
arxiv-1607-02504 | Fast Predictive Image Registration | http://arxiv.org/abs/1607.02504 | id:1607.02504 author:Xiao Yang, Roland Kwitt, Marc Niethammer category:cs.CV  published:2016-07-08 summary:We present a method to predict image deformations based on patch-wise image appearance. Specifically, we design a patch-based deep encoder-decoder network which learns the pixel/voxel-wise mapping between image appearance and registration parameters. Our approach can predict general deformation parameterizations, however, we focus on the large deformation diffeomorphic metric mapping (LDDMM) registration model. By predicting the LDDMM momentum-parameterization we retain the desirable theoretical properties of LDDMM, while reducing computation time by orders of magnitude: combined with patch pruning, we achieve a 1500x/66x speed up compared to GPU-based optimization for 2D/3D image registration. Our approach has better prediction accuracy than predicting deformation or velocity fields and results in diffeomorphic transformations. Additionally, we create a Bayesian probabilistic version of our network, which allows evaluation of deformation field uncertainty through Monte Carlo sampling using dropout at test time. We show that deformation uncertainty highlights areas of ambiguous deformations. We test our method on the OASIS brain image dataset in 2D and 3D. version:1
arxiv-1607-02501 | Actionable and Political Text Classification using Word Embeddings and LSTM | http://arxiv.org/abs/1607.02501 | id:1607.02501 author:Adithya Rao, Nemanja Spasojevic category:cs.CL cs.IR 68T50  92B20  published:2016-07-08 summary:In this work, we apply word embeddings and neural networks with Long Short-Term Memory (LSTM) to text classification problems, where the classification criteria are decided by the context of the application. We examine two applications in particular. The first is that of Actionability, where we build models to classify social media messages from customers of service providers as Actionable or Non-Actionable. We build models for over 30 different languages for actionability, and most of the models achieve accuracy around 85%, with some reaching over 90% accuracy. We also show that using LSTM neural networks with word embeddings vastly outperform traditional techniques. Second, we explore classification of messages with respect to political leaning, where social media messages are classified as Democratic or Republican. The model is able to classify messages with a high accuracy of 87.57%. As part of our experiments, we vary different hyperparameters of the neural networks, and report the effect of such variation on the accuracy. These actionability models have been deployed to production and help company agents provide customer support by prioritizing which messages to respond to. The model for political leaning has been opened and made available for wider use. version:1
arxiv-1607-02488 | Generalizing and Improving Weight Initialization | http://arxiv.org/abs/1607.02488 | id:1607.02488 author:Dan Hendrycks, Kevin Gimpel category:cs.LG cs.NE  published:2016-07-08 summary:We propose a new weight initialization suited for arbitrary nonlinearities by generalizing previous weight initializations. The initialization corrects for the influence of dropout rates and an arbitrary nonlinearity's influence on variance through simple corrective scalars. Consequently, this initialization does not require computing mini-batch statistics nor weight pre-initialization. This simple method enables improved accuracy over previous initializations, and it allows for training highly regularized neural networks where previous initializations lead to poor convergence. version:1
arxiv-1607-02467 | Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior Knowledge | http://arxiv.org/abs/1607.02467 | id:1607.02467 author:Marc Dymetman, Chunyang Xiao category:cs.AI cs.CL cs.LG cs.NE  published:2016-07-08 summary:We introduce \emph{LL-RNNs} (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiring that each combination is directly observed in the training data (as the softmax does). Second, it permits the inclusion of flexible prior knowledge in the form of \emph{a priori} specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features. We provide some motivating illustrations, and argue that the log-linear and the neural-network components contribute complementary strengths to the LL-RNN: the LL aspect allows the model to incorporate rich prior knowledge, while the NN aspect, according to the "representation learning" paradigm, allows the model to discover novel combination of characteristics. version:1
arxiv-1607-02450 | Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications | http://arxiv.org/abs/1607.02450 | id:1607.02450 author:Kush R. Varshney category:stat.ML cs.CY  published:2016-07-08 summary:This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York. version:1
arxiv-1607-02444 | Explaining Deep Convolutional Neural Networks on Music Classification | http://arxiv.org/abs/1607.02444 | id:1607.02444 author:Keunwoo Choi, George Fazekas, Mark Sandler category:cs.LG cs.AI cs.MM cs.SD  published:2016-07-08 summary:Deep convolutional neural networks (CNNs) have been actively adopted in the field of music information retrieval, e.g. genre classification, mood detection, and chord recognition. However, the process of learning and prediction is little understood, particularly when it is applied to spectrograms. We introduce auralisation of a CNN to understand its underlying mechanism, which is based on a deconvolution procedure introduced in [2]. Auralisation of a CNN is converting the learned convolutional features that are obtained from deconvolution into audio signals. In the experiments and discussions, we explain trained features of a 5-layer CNN based on the deconvolved spectrograms and auralised signals. The pairwise correlations per layers with varying different musical attributes are also investigated to understand the evolution of the learnt features. It is shown that in the deep layers, the features are learnt to capture textures, the patterns of continuous distributions, rather than shapes of lines. version:1
arxiv-1607-02436 | Document Clustering Games in Static and Dynamic Scenarios | http://arxiv.org/abs/1607.02436 | id:1607.02436 author:Rocco Tripodi, Marcello Pelillo category:cs.AI cs.CL cs.GT  published:2016-07-08 summary:In this work we propose a game theoretic model for document clustering. Each document to be clustered is represented as a player and each cluster as a strategy. The players receive a reward interacting with other players that they try to maximize choosing their best strategies. The geometry of the data is modeled with a weighted graph that encodes the pairwise similarity among documents, so that similar players are constrained to choose similar strategies, updating their strategy preferences at each iteration of the games. We used different approaches to find the prototypical elements of the clusters and with this information we divided the players into two disjoint sets, one collecting players with a definite strategy and the other one collecting players that try to learn from others the correct strategy to play. The latter set of players can be considered as new data points that have to be clustered according to previous information. This representation is useful in scenarios in which the data are streamed continuously. The evaluation of the system was conducted on 13 document datasets using different settings. It shows that the proposed method performs well compared to different document clustering algorithms. version:1
arxiv-1607-02435 | Optimal rates of Statistical Seriation | http://arxiv.org/abs/1607.02435 | id:1607.02435 author:Nicolas Flammarion, Cheng Mao, Philippe Rigollet category:math.ST stat.ML stat.TH 62G08  published:2016-07-08 summary:Given a matrix the seriation problem consists in permuting its rows in such way that all its columns have the same shape, for example, they are monotone increasing. We propose a statistical approach to this problem where the matrix of interest is observed with noise and study the corresponding minimax rate of estimation of the matrices. Specifically, when the columns are either unimodal or monotone, we show that the least squares estimator is optimal up to logarithmic factors and adapts to matrices with a certain natural structure. Finally, we propose a computationally efficient estimator in the monotonic case and study its performance both theoretically and experimentally. Our work is at the intersection of shape constrained estimation and recent work that involves permutation learning, such as graph denoising and ranking. version:1
arxiv-1607-02413 | Lower Bounds on Active Learning for Graphical Model Selection | http://arxiv.org/abs/1607.02413 | id:1607.02413 author:Jonathan Scarlett, Volkan Cevher category:cs.IT cs.LG cs.SI math.IT stat.ML  published:2016-07-08 summary:We consider the problem of estimating the underlying graph associated with a Markov random field, with the added twist that the decoding algorithm can iteratively choose which subsets of nodes to sample based on the previous samples, i.e., active learning. Considering both Ising and Gaussian models, we provide algorithm-independent lower bounds for high-probability recovery within the class of degree-bounded graphs. Our main results are minimax lower bounds for the active setting that match the best known lower bounds for the passive setting, which are known to be tight in several cases. Our analysis is based on a novel variation of Fano's inequality for the active learning setting. While we consider graph ensembles that are similar or identical to those considered for the passive setting, we require different techniques to analyze them, with the key challenge being bounding a mutual information quantity associated with observed subsets of the nodes, as opposed to full observations. version:1
arxiv-1607-02397 | Enlightening Deep Neural Networks with Knowledge of Confounding Factors | http://arxiv.org/abs/1607.02397 | id:1607.02397 author:Yu Zhong, Gil Ettinger category:cs.CV  published:2016-07-08 summary:Deep learning techniques have demonstrated significant capacity in modeling some of the most challenging real world problems of high complexity. Despite the popularity of deep models, we still strive to better understand the underlying mechanism that drives their success. Motivated by observations that neurons in trained deep nets predict attributes indirectly related to the training tasks, we recognize that a deep network learns representations more general than the task at hand to disentangle impacts of multiple confounding factors governing the data, in order to isolate the effects of the concerning factors and optimize a given objective. Consequently, we propose a general framework to augment training of deep models with information on auxiliary explanatory data variables, in an effort to boost this disentanglement and train deep networks that comprehend the data interactions and distributions more accurately, and thus improve their generalizability. We incorporate information on prominent auxiliary explanatory factors of the data population into existing architectures as secondary objective/loss blocks that take inputs from hidden layers during training. Once trained, these secondary circuits can be removed to leave a model with the same architecture as the original, but more generalizable and discerning thanks to its comprehension of data interactions. Since pose is one of the most dominant confounding factors for object recognition, we apply this principle to instantiate a pose-aware deep convolutional neural network and demonstrate that auxiliary pose information indeed improves the classification accuracy in our experiments on SAR target classification tasks. version:1
arxiv-1607-02387 | Convergence rates of Kernel Conjugate Gradient for random design regression | http://arxiv.org/abs/1607.02387 | id:1607.02387 author:Gilles Blanchard, Nicole Krämer category:math.ST stat.ML stat.TH  published:2016-07-08 summary:We prove statistical rates of convergence for kernel-based least squares regression from i.i.d. data using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping. This method is related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. Following the setting introduced in earlier related literature, we study so-called "fast convergence rates" depending on the regularity of the target regression function (measured by a source condition in terms of the kernel integral operator) and on the effective dimensionality of the data mapped into the kernel space. We obtain upper bounds, essentially matching known minimax lower bounds, for the $\mathcal{L}^2$ (prediction) norm as well as for the stronger Hilbert norm, if the true regression function belongs to the reproducing kernel Hilbert space. If the latter assumption is not fulfilled, we obtain similar convergence rates for appropriate norms, provided additional unlabeled data are available. version:1
arxiv-1607-02355 | Lexical Based Semantic Orientation of Online Customer Reviews and Blogs | http://arxiv.org/abs/1607.02355 | id:1607.02355 author:Aurangzeb khan, Khairullah khan, Shakeel Ahmad, Fazal Masood Kundi, Irum Tareen, Muhammad Zubair Asghar category:cs.CL cs.IR  published:2016-07-08 summary:Rapid increase in internet users along with growing power of online review sites and social media has given birth to sentiment analysis or opinion mining, which aims at determining what other people think and comment. Sentiments or Opinions contain public generated content about products, services, policies and politics. People are usually interested to seek positive and negative opinions containing likes and dislikes, shared by users for features of particular product or service. This paper proposed sentence-level lexical based domain independent sentiment classification method for different types of data such as reviews and blogs. The proposed method is based on general lexicons i.e. WordNet, SentiWordNet and user defined lexical dictionaries for semantic orientation. The relations and glosses of these dictionaries provide solution to the domain portability problem. The method performs better than word and text level corpus based machine learning methods for semantic orientation. The results show the proposed method performs better as it shows precision of 87% and83% at document and sentence levels respectively for online comments. version:1
arxiv-1607-02329 | Watch This: Scalable Cost-Function Learning for Path Planning in Urban Environments | http://arxiv.org/abs/1607.02329 | id:1607.02329 author:Markus Wulfmeier, Dominic Zeng Wang, Ingmar Posner category:cs.RO cs.LG  published:2016-07-08 summary:In this work, we present an approach to learn cost maps for driving in complex urban environments from a very large number of demonstrations of driving behaviour by human experts. The learned cost maps are constructed directly from raw sensor measurements, bypassing the effort of manually designing cost maps as well as features. When deploying the learned cost maps, the trajectories generated not only replicate human-like driving behaviour but are also demonstrably robust against systematic errors in putative robot configuration. To achieve this we deploy a Maximum Entropy based, non-linear IRL framework which uses Fully Convolutional Neural Networks (FCNs) to represent the cost model underlying expert driving behaviour. Using a deep, parametric approach enables us to scale efficiently to large datasets and complex behaviours by being run-time independent of dataset extent during deployment. We demonstrate the scalability and the performance of the proposed approach on an ambitious dataset collected over the course of one year including more than 25k demonstration trajectories extracted from over 120km of driving around pedestrianised areas in the city of Milton Keynes, UK. We evaluate the resulting cost representations by showing the advantages over a carefully manually designed cost map and, in addition, demonstrate its robustness to systematic errors by learning precise cost-maps even in the presence of system calibration perturbations. version:1
arxiv-1607-02310 | Collaborative Training of Tensors for Compositional Distributional Semantics | http://arxiv.org/abs/1607.02310 | id:1607.02310 author:Tamara Polajnar category:cs.CL cs.LG  published:2016-07-08 summary:Type-based compositional distributional semantic models present an interesting line of research into functional representations of linguistic meaning. One of the drawbacks of such models, however, is the lack of training data required to train each word-type combination. In this paper we address this by introducing training methods that share parameters between similar words. We show that these methods enable zero-shot learning for words that have no training data at all, as well as enabling construction of high-quality tensors from very few training examples per word. version:1
arxiv-1607-02306 | CaR-FOREST: Joint Classification-Regression Decision Forests for Overlapping Audio Event Detection | http://arxiv.org/abs/1607.02306 | id:1607.02306 author:Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins category:cs.SD cs.AI cs.LG cs.MM  published:2016-07-08 summary:This report describes our submissions to Task2 and Task3 of the DCASE 2016 challenge. The systems aim at dealing with the detection of overlapping audio events in continuous streams, where the detectors are based on random decision forests. The proposed forests are jointly trained for classification and regression simultaneously. Initially, the training is classification-oriented to encourage the trees to select discriminative features from overlapping mixtures to separate positive audio segments from the negative ones. The regression phase is then carried out to let the positive audio segments vote for the event onsets and offsets, and therefore model the temporal structure of audio events. One random decision forest is specifically trained for each event category of interest. Experimental results on the development data show that our systems outperform the DCASE 2016 challenge baselines with absolute gains of 64.4% and 8.0% on Task2 and Task3, respectively. version:1
arxiv-1607-02303 | CNN-LTE: a Class of 1-X Pooling Convolutional Neural Networks on Label Tree Embeddings for Audio Scene Recognition | http://arxiv.org/abs/1607.02303 | id:1607.02303 author:Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins category:cs.NE cs.CV cs.LG cs.MM cs.SD  published:2016-07-08 summary:We describe in this report our audio scene recognition system submitted to the DCASE 2016 challenge. Firstly, given the label set of the scenes, a label tree is automatically constructed. This category taxonomy is then used in the feature extraction step in which an audio scene instance is represented by a label tree embedding image. Different convolutional neural networks, which are tailored for the task at hand, are finally learned on top of the image features for scene recognition. Our system reaches an overall recognition accuracy of 81.2% and outperforms the DCASE 2016 baseline with an absolute improvement of 8.7% on the development data. version:1
arxiv-1607-00318 | The Evolution of Sex through the Baldwin Effect | http://arxiv.org/abs/1607.00318 | id:1607.00318 author:Larry Bull category:cs.NE q-bio.PE  published:2016-07-01 summary:This paper suggests that the fundamental haploid-diploid cycle of eukaryotic sex exploits a rudimentary form of the Baldwin effect. With this explanation for the basic cycle, the other associated phenomena can be explained as evolution tuning the amount and frequency of learning experienced by an organism. Using the well-known NK model of fitness landscapes it is shown that varying landscape ruggedness varies the benefit of the haploid-diploid cycle, whether based upon endomitosis or syngamy. The utility of mechanisms such as pre-meiotic doubling and recombination during the cycle are also shown to vary with landscape ruggedness. This view is suggested as underpinning, rather than contradicting, many existing explanations for sex. version:3
arxiv-1607-02290 | Non-Central Catadioptric Cameras Pose Estimation using 3D Lines | http://arxiv.org/abs/1607.02290 | id:1607.02290 author:Andre Mateus, Pedro Miraldo, Pedro U. Lima category:cs.RO cs.CV  published:2016-07-08 summary:In this article we purpose a novel method for planar pose estimation of mobile robots. This method is based on an analytic solution (which we derived) for the projection of 3D straight lines, onto the mirror of Non-Central Catadioptric Cameras (NCCS). The resulting solution is rewritten as a function of the rotation and translation parameters, which is then used as an error function for a set of mirror points. Those should be the result of the projection of a set of points incident with the respective 3D lines. The camera's pose is given by minimizing the error function, with the associated constraints. The method is validated by experiments both with synthetic and real data. The latter was collected from a mobile robot equipped with a NCCS. version:1
arxiv-1607-02257 | Siamese Regression Networks with Efficient mid-level Feature Extraction for 3D Object Pose Estimation | http://arxiv.org/abs/1607.02257 | id:1607.02257 author:Andreas Doumanoglou, Vassileios Balntas, Rigas Kouskouridas, Tae-Kyun Kim category:cs.CV  published:2016-07-08 summary:In this paper we tackle the problem of estimating the 3D pose of object instances, using convolutional neural networks. State of the art methods usually solve the challenging problem of regression in angle space indirectly, focusing on learning discriminative features that are later fed into a separate architecture for 3D pose estimation. In contrast, we propose an end-to-end learning framework for directly regressing object poses by exploiting Siamese Networks. For a given image pair, we enforce a similarity measure between the representation of the sample images in the feature and pose space respectively, that is shown to boost regression performance. Furthermore, we argue that our pose-guided feature learning using our Siamese Regression Network generates more discriminative features that outperform the state of the art. Last, our feature learning formulation provides the ability of learning features that can perform under severe occlusions, demonstrating high performance on our novel hand-object dataset. version:1
arxiv-1607-02250 | Consensus Attention-based Neural Networks for Chinese Reading Comprehension | http://arxiv.org/abs/1607.02250 | id:1607.02250 author:Yiming Cui, Ting Liu, Zhipeng Chen, Shijin Wang, Guoping Hu category:cs.CL cs.NE  published:2016-07-08 summary:Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading comprehension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network significantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research. version:1
arxiv-1607-02241 | Overcoming Challenges in Fixed Point Training of Deep Convolutional Networks | http://arxiv.org/abs/1607.02241 | id:1607.02241 author:Darryl D. Lin, Sachin S. Talathi category:cs.LG cs.CV  published:2016-07-08 summary:It is known that training deep neural networks, in particular, deep convolutional networks, with aggressively reduced numerical precision is challenging. The stochastic gradient descent algorithm becomes unstable in the presence of noisy gradient updates resulting from arithmetic with limited numeric precision. One of the well-accepted solutions facilitating the training of low precision fixed point networks is stochastic rounding. However, to the best of our knowledge, the source of the instability in training neural networks with noisy gradient updates has not been well investigated. This work is an attempt to draw a theoretical connection between low numerical precision and training algorithm stability. In doing so, we will also propose and verify through experiments methods that are able to improve the training performance of deep convolutional networks in fixed point. version:1
arxiv-1607-02235 | From Collective Adaptive Systems to Human Centric Computation and Back: Spatial Model Checking for Medical Imaging | http://arxiv.org/abs/1607.02235 | id:1607.02235 author:Gina Belmonte, Vincenzo Ciancia, Diego Latella, Mieke Massink category:cs.LO cs.CV  published:2016-07-08 summary:Recent research on formal verification for Collective Adaptive Systems (CAS) pushed advancements in spatial and spatio-temporal model checking, and as a side result provided novel image analysis methodologies, rooted in logical methods for topological spaces. Medical Imaging (MI) is a field where such technologies show potential for ground-breaking innovation. In this position paper, we present a preliminary investigation centred on applications of spatial model checking to MI. The focus is shifted from pure logics to a mixture of logical, statistical and algorithmic approaches, driven by the logical nature intrinsic to the specification of the properties of interest in the field. As a result, novel operators are introduced, that could as well be brought back to the setting of CAS. version:1
arxiv-1607-02204 | Multi Channel-Kernel Canonical Correlation Analysis for Cross-View Person Re-Identification | http://arxiv.org/abs/1607.02204 | id:1607.02204 author:Giuseppe Lisanti, Svebor Karaman, Iacopo Masi, Alberto Del Bimbo category:cs.CV  published:2016-07-08 summary:In this paper we introduce a method to overcome one of the main challenges of person re-identification in multi-camera networks, namely cross-view appearance changes. The proposed solution addresses the extreme variability of person appearance in different camera views by exploiting multiple feature representations. For each feature, Kernel Canonical Correlation Analysis (KCCA) with different kernels is exploited to learn several projection spaces in which the appearance correlation between samples of the same person observed from different cameras is maximized. An iterative logistic regression is finally used to select and weigh the contributions of each feature projections and perform the matching between the two views. Experimental evaluation shows that the proposed solution obtains comparable performance on VIPeR and PRID 450s datasets and improves on PRID and CUHK01 datasets with respect to the state of the art. version:1
arxiv-1607-02188 | Whole-brain substitute CT generation using Markov random field mixture models | http://arxiv.org/abs/1607.02188 | id:1607.02188 author:Anders Hildeman, David Bolin, Jonas Wallin, Adam Johansson, Tufve Nyholm, Thomas Asklund, Jun Yu category:stat.AP stat.CO stat.ML  published:2016-07-07 summary:Computed tomography (CT) equivalent information is needed for attenuation correction in PET imaging and for dose planning in radiotherapy. Prior work has shown that Gaussian mixture models can be used to generate a substitute CT (s-CT) image from a specific set of MRI modalities. This work introduces a more flexible class of mixture models for s-CT generation, that incorporates spatial dependency in the data through a Markov random field prior on the latent class probabilities. Furthermore, the mixture distributions are extended from Gaussian to normal inverse Gaussian, allowing heavier tails and skewness. The amount of data needed to train a model for s-CT generation is of the order of $10^8$ voxels. The computational efficiency of the parameter estimation and prediction methods are hence paramount, especially when spatial dependency is included in the models. A stochastic EM gradient algorithm is proposed in order to tackle this challenge. The advantages of the spatial model and normal inverse Gaussian distributions are evaluated with a cross-validation study based on data from 14 patients. The study shows that the proposed model enhances the predictive quality of the s-CT image by reducing the mean absolute error with $17.9\%$. version:1
arxiv-1607-02177 | Applying Deep Learning to the Newsvendor Problem | http://arxiv.org/abs/1607.02177 | id:1607.02177 author:Afshin Oroojlooyjadid, Lawrence Snyder, Martin Takáč category:cs.LG  published:2016-07-07 summary:The newsvendor problem is one of the most basic and widely applied inventory models. There are numerous extensions of this problem. One important extension is the multi-item newsvendor problem, in which the demand of each item may be correlated with that of other items. If the joint probability distribution of the demand is known, the problem can be solved analytically. However, approximating the probability distribution is not easy and is prone to error; therefore, the resulting solution to the newsvendor problem may be not optimal. To address this issue, we propose an algorithm based on deep learning that optimizes the order quantities for all products based on features of the demand data. Our algorithm integrates the forecasting and inventory-optimization steps, rather than solving them separately as is typically done. The algorithm does not require the knowledge of the probability distributions of the demand. Numerical experiments on real-world data suggest that our algorithm outperforms other approaches, including data-driven and SVM approaches, especially for demands with high volatility. version:1
arxiv-1607-02173 | Single-Channel Multi-Speaker Separation using Deep Clustering | http://arxiv.org/abs/1607.02173 | id:1607.02173 author:Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, John R. Hershey category:cs.LG cs.SD stat.ML  published:2016-07-07 summary:Deep clustering is a recently introduced deep learning architecture that uses discriminatively trained embeddings as the basis for clustering. It was recently applied to spectrogram segmentation, resulting in impressive results on speaker-independent multi-speaker separation. In this paper we extend the baseline system with an end-to-end signal approximation objective that greatly improves performance on a challenging speech separation. We first significantly improve upon the baseline system performance by incorporating better regularization, larger temporal context, and a deeper architecture, culminating in an overall improvement in signal to distortion ratio (SDR) of 10.3 dB compared to the baseline of 6.0 dB for two-speaker separation, as well as a 7.1 dB SDR improvement for three-speaker separation. We then extend the model to incorporate an enhancement layer to refine the signal estimates, and perform end-to-end training through both the clustering and enhancement stages to maximize signal fidelity. We evaluate the results using automatic speech recognition. The new signal approximation objective, combined with end-to-end training, produces unprecedented performance, reducing the word error rate (WER) from 89.1% down to 30.8%. This represents a major advancement towards solving the cocktail party problem. version:1
arxiv-1607-01759 | Bag of Tricks for Efficient Text Classification | http://arxiv.org/abs/1607.01759 | id:1607.01759 author:Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov category:cs.CL  published:2016-07-06 summary:This paper proposes a simple and efficient approach for text classification and representation learning. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute. version:2
arxiv-1607-02109 | Predicting and Understanding Law-Making with Machine Learning | http://arxiv.org/abs/1607.02109 | id:1607.02109 author:John J. Nay category:cs.CL physics.soc-ph stat.AP stat.ML  published:2016-07-07 summary:Out of nearly 70,000 bills introduced in the U.S. Congress from 2001 to 2015, only 2,513 were enacted. We developed a machine learning approach to forecasting the probability that any bill will become law. Starting in 2001 with the 107th Congress, we trained models on data from previous Congresses, predicted all bills in the current Congress, and repeated until the 113th Congress served as the test. For prediction we scored each sentence of a bill with a language model that embeds legislative vocabulary into a semantic-laden vector space. This language representation enables our investigation into which words increase the probability of enactment for any topic. To test the relative importance of text and context, we compared the text model to a context-only model that uses variables such as whether the bill's sponsor is in the majority party. To test the effect of changes to bills after their introduction on our ability to predict their final outcome, we compared using the bill text and meta-data available at the time of introduction with using the most recent data. At the time of introduction context-only predictions outperform text-only, and with the newest data text-only outperforms context-only. Combining text and context always performs best. We conducted a global sensitivity analysis on the combined model to determine important factors predicting enactment. version:1
arxiv-1607-00331 | Machine-based Multimodal Pain Assessment Tool for Infants: A Review | http://arxiv.org/abs/1607.00331 | id:1607.00331 author:Ghada Zamzmi, Chih-Yun Pai, Dmitry Goldgof, Rangachar Kasturi, Yu Sun, Terri Ashmeade category:cs.CV  published:2016-07-01 summary:The current practice of assessing infants' pain depends on using subjective tools that fail to meet rigorous psychometric standards and requires continuous monitoring by health professionals. Therefore, pain may be misinterpreted or totally missed leading to misdiagnosis and over/under treatment. To address these shortcomings, the current practice can be augmented with a machine-based assessment tool that continuously monitors various pain cues and provides a consistent and minimally biased evaluation of pain. Several machine-based approaches have been proposed to assess infants' pain based on analysis of whether behavioral or physiological pain indictors (i.e., single modality). The aim of this review paper is to provide the reader with the current machine-based approaches in assessing infants' pain. It also proposes the development of a multimodal machine-based pain assessment tool and presents preliminary implementation results. version:2
arxiv-1607-02085 | A Classification Framework for Partially Observed Dynamical Systems | http://arxiv.org/abs/1607.02085 | id:1607.02085 author:Yuan Shen, Peter Tino, Krasimira Tsaneva-Atanasova category:stat.ML  published:2016-07-07 summary:We present a general framework for classifying partially observed dynamical systems based on the idea of learning in the model space. In contrast to the existing approaches using model point estimates to represent individual data items, we employ posterior distributions over models, thus taking into account in a principled manner the uncertainty due to both the generative (observational and/or dynamic noise) and observation (sampling in time) processes. We evaluate the framework on two testbeds - a biological pathway model and a stochastic double-well system. Crucially, we show that the classifier performance is not impaired when the model class used for inferring posterior distributions is much more simple than the observation-generating model class, provided the reduced complexity inferential model class captures the essential characteristics needed for the given classification task. version:1
arxiv-1607-02078 | DeepChrome: Deep-learning for predicting gene expression from histone modifications | http://arxiv.org/abs/1607.02078 | id:1607.02078 author:Ritambhara Singh, Jack Lanchantin, Gabriel Robins, Yanjun Qi category:cs.LG q-bio.GN  published:2016-07-07 summary:Motivation: Histone modifications are among the most important factors that control gene regulation. Computational methods that predict gene expression from histone modification signals are highly desirable for understanding their combinatorial effects in gene regulation. This knowledge can help in developing 'epigenetic drugs' for diseases like cancer. Previous studies for quantifying the relationship between histone modifications and gene expression levels either failed to capture combinatorial effects or relied on multiple methods that separate predictions and combinatorial analysis. This paper develops a unified discriminative framework using a deep convolutional neural network to classify gene expression using histone modification data as input. Our system, called DeepChrome, allows automatic extraction of complex interactions among important features. To simultaneously visualize the combinatorial interactions among histone modifications, we propose a novel optimization-based technique that generates feature pattern maps from the learnt deep model. This provides an intuitive description of underlying epigenetic mechanisms that regulate genes. Results: We show that DeepChrome outperforms state-of-the-art models like Support Vector Machines and Random Forests for gene expression classification task on 56 different cell-types from REMC database. The output of our visualization technique not only validates the previous observations but also allows novel insights about combinatorial interactions among histone modification marks, some of which have recently been observed by experimental studies. version:1
arxiv-1607-02066 | A characterization of product-form exchangeable feature probability functions | http://arxiv.org/abs/1607.02066 | id:1607.02066 author:Marco Battiston, Stefano Favaro, Daniel M. Roy, Yee Whye Teh category:math.PR math.ST stat.ML stat.TH 60K99  60C99  published:2016-07-07 summary:We characterize the class of exchangeable feature allocations assigning probability $V_{n,k}\prod_{l=1}^{k}W_{m_{l}}U_{n-m_{l}}$ to a feature allocation of $n$ individuals, displaying $k$ features with counts $(m_{1},\ldots,m_{k})$ for these features. Each element of this class is parametrized by a countable matrix $V$ and two sequences $U$ and $W$ of non-negative weights. Moreover, a consistency condition is imposed to guarantee that the distribution for feature allocations of $n-1$ individuals is recovered from that of $n$ individuals, when the last individual is integrated out. In Theorem 1.1, we prove that the only members of this class satisfying the consistency condition are mixtures of the Indian Buffet Process over its mass parameter $\gamma$ and mixtures of the Beta--Bernoulli model over its dimensionality parameter $N$. Hence, we provide a characterization of these two models as the only, up to randomization of the parameters, consistent exchangeable feature allocations having the required product form. version:1
arxiv-1607-02061 | Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity | http://arxiv.org/abs/1607.02061 | id:1607.02061 author:Emmanuele Chersoni, Enrico Santus, Alessandro Lenci, Philippe Blache, Chu-Ren Huang category:cs.CL cs.AI  published:2016-07-07 summary:Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, which causes the loss of important information for word expectations, such as word order and interrelations. In this paper, we present a DSM which addresses the issue by defining verb contexts as joint dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts are more efficient than single dependencies, even with a relatively small amount of training data. version:1
arxiv-1607-02046 | MoCap-guided Data Augmentation for 3D Pose Estimation in the Wild | http://arxiv.org/abs/1607.02046 | id:1607.02046 author:Grégory Rogez, Cordelia Schmid category:cs.CV  published:2016-07-07 summary:In this paper, we address the problem of 3D human pose understanding in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D pose. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images and 2D human pose annotations using 3D Motion Capture (MoCap) data. Given a candidate 3D pose, our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a K-way classification problem. Such approach is viable only with large training sets such as ours. Our method outperforms state-of-the-art in terms of 3D pose estimation in controlled environments (Human3.6M), showing promising results for in-the-wild images (LSP). version:1
arxiv-1607-02024 | Mini-Batch Spectral Clustering | http://arxiv.org/abs/1607.02024 | id:1607.02024 author:Yufei Han, Yun Shen, Maurizio Filippone category:stat.ML cs.LG  published:2016-07-07 summary:The cost of computing the spectrum of Laplacian matrices hinders the application of spectral clustering to large data sets. While approximations recover computational tractability, they can potentially affect clustering performance. This paper proposes a practical approach to learn spectral clustering based on adaptive stochastic gradient optimization. Crucially, the proposed approach recovers the exact spectrum of Laplacian matrices in the limit of the iterations, and the cost of each iteration is linear in the number of samples. Extensive experimental validation on data sets with up to half a million samples demonstrate its scalability and its ability to outperform state-of-the-art approximate methods to learn spectral clustering for a given computational budget. version:1
arxiv-1607-02011 | Kernel Bayesian Inference with Posterior Regularization | http://arxiv.org/abs/1607.02011 | id:1607.02011 author:Yang Song, Jun Zhu, Yong Ren category:stat.ML  published:2016-07-07 summary:We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines. version:1
arxiv-1607-02003 | Tubelets: Unsupervised action proposals from spatiotemporal super-voxels | http://arxiv.org/abs/1607.02003 | id:1607.02003 author:Mihir Jain, Jan van Gemert, Hervé Jégou, Patrick Bouthemy, Cees G. M. Snoek category:cs.CV  published:2016-07-07 summary:This paper considers the problem of localizing actions in videos as a sequences of bounding boxes. The objective is to generate action proposals that are likely to include the action of interest, ideally achieving high recall with few proposals. Our contributions are threefold. First, inspired by selective search for object proposals, we introduce an approach to generate action proposals from spatiotemporal super-voxels in an unsupervised manner, we call them Tubelets. Second, along with the static features from individual frames our approach advantageously exploits motion. We introduce independent motion evidence as a feature to characterize how the action deviates from the background and explicitly incorporate such motion information in various stages of the proposal generation. Finally, we introduce spatiotemporal refinement of Tubelets, for more precise localization of actions, and pruning to keep the number of Tubelets limited. We demonstrate the suitability of our approach by extensive experiments for action proposal quality and action localization on three public datasets: UCF Sports, MSR-II and UCF101. For action proposal quality, our unsupervised proposals beat all other existing approaches on the three datasets. For action localization, we show top performance on both the trimmed videos of UCF Sports and UCF101 as well as the untrimmed videos of MSR-II. version:1
arxiv-1607-01990 | A Maturity Model for Public Administration as Open Translation Data Providers | http://arxiv.org/abs/1607.01990 | id:1607.01990 author:Núria Bel, Mikel L. Forcada, Asunción Gómez-Pérez category:cs.CY cs.CL  published:2016-07-07 summary:Any public administration that produces translation data can be a provider of useful reusable data to meet its own translation needs and the ones of other public organizations and private companies that work with texts of the same domain. These data can also be crucial to produce domain-tuned Machine Translation systems. The organization's management of the translation process, the characteristics of the archives of the generated resources and of the infrastructure available to support them determine the efficiency and the effectiveness with which the materials produced can be converted into reusable data. However, it is of utmost importance that the organizations themselves first become aware of the goods they are producing and, second, adapt their internal processes to become optimal providers. In this article, we propose a Maturity Model to help these organizations to achieve it by identifying the different stages of the management of translation data that determine the path to the aforementioned goal. version:1
arxiv-1607-01979 | Untrimmed Video Classification for Activity Detection: submission to ActivityNet Challenge | http://arxiv.org/abs/1607.01979 | id:1607.01979 author:Gurkirt Singh, Fabio Cuzzolin category:cs.CV  published:2016-07-07 summary:Current state-of-the-art human activity recognition is focused on the classification of temporally trimmed videos in which only one action occurs per frame. We propose a simple, yet effective, method for the temporal detection of activities in temporally untrimmed videos with the help of untrimmed classification. Firstly, our model predicts the top k labels for each untrimmed video by analysing global video-level features. Secondly, frame-level binary classification is combined with dynamic programming to generate the temporally trimmed activity proposals. Finally, each proposal is assigned a label based on the global label, and scored with the score of the temporal activity proposal and the global score. Ultimately, we show that untrimmed video classification models can be used as stepping stone for temporal detection. version:1
arxiv-1607-01977 | Deep Depth Super-Resolution : Learning Depth Super-Resolution using Deep Convolutional Neural Network | http://arxiv.org/abs/1607.01977 | id:1607.01977 author:Xibin Song, Yuchao Dai, Xueying Qin category:cs.CV  published:2016-07-07 summary:Depth image super-resolution is an extremely challenging task due to the information loss in sub-sampling. Deep convolutional neural network have been widely applied to color image super-resolution. Quite surprisingly, this success has not been matched to depth super-resolution. This is mainly due to the inherent difference between color and depth images. In this paper, we bridge up the gap and extend the success of deep convolutional neural network to depth super-resolution. The proposed deep depth super-resolution method learns the mapping from a low-resolution depth image to a high resolution one in an end-to-end style. Furthermore, to better regularize the learned depth map, we propose to exploit the depth field statistics and the local correlation between depth image and color image. These priors are integrated in an energy minimization formulation, where the deep neural network learns the unary term, the depth field statistics works as global model constraint and the color-depth correlation is utilized to enforce the local structure in depth images. Extensive experiments on various depth super-resolution benchmark datasets show that our method outperforms the state-of-the-art depth image super-resolution methods with a margin. version:1
arxiv-1607-01971 | Superimposition of eye fundus images for longitudinal analysis from large public health databases | http://arxiv.org/abs/1607.01971 | id:1607.01971 author:Guillaume Noyel, Rebecca Thomas, Gavin Bhakta, Andrew Crowder, David Owens, Peter Boyle category:cs.CV  published:2016-07-07 summary:In this paper, we present a method for superimposition (i.e. registration) of eye fundus images from persons with diabetes screened over many years for Diabetic Retinopathy. The method is fully automatic and robust to camera changes and colour variations across the images both in space and time. All the stages of the process are designed for longitudinal analysis of cohort public health databases. The method relies on a model correcting two radial distortions and an affine transformation between pairs of images which is robustly fitted on salient points. Each stage involves linear estimators followed by non-linear optimisation. The model of image warping is also invertible for fast computation. The method has been validated 1. on a simulated montage with an average error of 0.81 pixels for one distortion (respectively 1.08 pixels for two distortions) and a standard deviation of 1.36 pixels (resp. 3.09) in images of 1568 x 2352 pixels in both directions and 2 on public health databases with 69 patients with high quality images (with 271 pairs and 268 pairs) with a success rates of 96 % and 97 % and 5 patients (with 20 pairs) with low quality images with a success rate of 100%. version:1
arxiv-1607-01963 | Sequence Training and Adaptation of Highway Deep Neural Networks | http://arxiv.org/abs/1607.01963 | id:1607.01963 author:Liang Lu category:cs.CL cs.LG cs.NE  published:2016-07-07 summary:Highway deep neural network (HDNN) is a type of depth-gated feedforward neural network, which has shown to be easier to train with more hidden layers and also generalise better compared to conventional plain deep neural networks (DNNs). Previously, we investigated a structured HDNN architecture for speech recognition, in which the two gate functions are tied across all the hidden layers, and we were able to train a much smaller model without sacrificing the recognition accuracy. In this paper, we carry on the study of this architecture with sequence-discriminative training criterion and speaker adaptation techniques on the AMI meeting speech recognition corpus. We show that these two techniques improve speech recognition accuracy on top of the model trained with the cross entropy criterion. Furthermore, we demonstrate that the two gate functions that are tied across all the hidden layers are able to control the information flow over the whole network, and we achieved considerable improvements by only updating these gate functions in both sequence training and adaptation experiments. version:1
arxiv-1607-01958 | Stock trend prediction using news sentiment analysis | http://arxiv.org/abs/1607.01958 | id:1607.01958 author:Joshi Kalyani, Prof. H. N. Bharathi, Prof. Rao Jyothi category:cs.CL cs.IR cs.LG  published:2016-07-07 summary:Efficient Market Hypothesis is the popular theory about stock prediction. With its failure much research has been carried in the area of prediction of stocks. This project is about taking non quantifiable data such as financial news articles about a company and predicting its future stock trend with news sentiment classification. Assuming that news articles have impact on stock market, this is an attempt to study relationship between news and stock trend. To show this, we created three different classification models which depict polarity of news articles being positive or negative. Observations show that RF and SVM perform well in all types of testing. Na\"ive Bayes gives good result but not compared to the other two. Experiments are conducted to evaluate various aspects of the proposed model and encouraging results are obtained in all of the experiments. The accuracy of the prediction model is more than 80% and in comparison with news random labeling with 50% of accuracy; the model has increased the accuracy by 30%. version:1
arxiv-1607-01895 | Random Walk Graph Laplacian based Smoothness Prior for Soft Decoding of JPEG Images | http://arxiv.org/abs/1607.01895 | id:1607.01895 author:Xianming Liu, Gene Cheung, Xiaolin Wu, Debin Zhao category:cs.CV  published:2016-07-07 summary:Given the prevalence of JPEG compressed images, optimizing image reconstruction from the compressed format remains an important problem. Instead of simply reconstructing a pixel block from the centers of indexed DCT coefficient quantization bins (hard decoding), soft decoding reconstructs a block by selecting appropriate coefficient values within the indexed bins with the help of signal priors. The challenge thus lies in how to define suitable priors and apply them effectively. In this paper, we combine three image priors---Laplacian prior for DCT coefficients, sparsity prior and graph-signal smoothness prior for image patches---to construct an efficient JPEG soft decoding algorithm. Specifically, we first use the Laplacian prior to compute a minimum mean square error (MMSE) initial solution for each code block. Next, we show that while the sparsity prior can reduce block artifacts, limiting the size of the over-complete dictionary (to lower computation) would lead to poor recovery of high DCT frequencies. To alleviate this problem, we design a new graph-signal smoothness prior (desired signal has mainly low graph frequencies) based on the left eigenvectors of the random walk graph Laplacian matrix (LERaG). Compared to previous graph-signal smoothness priors, LERaG has desirable image filtering properties with low computation overhead. We demonstrate how LERaG can facilitate recovery of high DCT frequencies of a piecewise smooth (PWS) signal via an interpretation of low graph frequency components as relaxed solutions to normalized cut in spectral clustering. Finally, we construct a soft decoding algorithm using the three signal priors with appropriate prior weights. Experimental results show that our proposal outperforms state-of-the-art soft decoding algorithms in both objective and subjective evaluations noticeably. version:1
arxiv-1607-01869 | Scalable Semantic Matching of Queries to Ads in Sponsored Search Advertising | http://arxiv.org/abs/1607.01869 | id:1607.01869 author:Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio Silvestri, Ricardo Baeza-Yates, Andrew Feng, Erik Ordentlich, Lee Yang, Gavin Owens category:cs.IR cs.AI cs.CL  published:2016-07-07 summary:Sponsored search represents a major source of revenue for web search engines. This popular advertising model brings a unique possibility for advertisers to target users' immediate intent communicated through a search query, usually by displaying their ads alongside organic search results for queries deemed relevant to their products or services. However, due to a large number of unique queries it is challenging for advertisers to identify all such relevant queries. For this reason search engines often provide a service of advanced matching, which automatically finds additional relevant queries for advertisers to bid on. We present a novel advanced matching approach based on the idea of semantic embeddings of queries and ads. The embeddings were learned using a large data set of user search sessions, consisting of search queries, clicked ads and search links, while utilizing contextual information such as dwell time and skipped ads. To address the large-scale nature of our problem, both in terms of data and vocabulary size, we propose a novel distributed algorithm for training of the embeddings. Finally, we present an approach for overcoming a cold-start problem associated with new ads and queries. We report results of editorial evaluation and online tests on actual search traffic. The results show that our approach significantly outperforms baselines in terms of relevance, coverage, and incremental revenue. Lastly, we open-source learned query embeddings to be used by researchers in computational advertising and related fields. version:1
arxiv-1607-01434 | Risk Bounds for High-dimensional Ridge Function Combinations Including Neural Networks | http://arxiv.org/abs/1607.01434 | id:1607.01434 author:Jason M. Klusowski, Andrew R. Barron category:math.ST stat.ML stat.TH 62J02  62G08  68T05  published:2016-07-05 summary:Let $ f^{\star} $ be a function on $ \mathbb{R}^d $ satisfying a spectral norm condition. For various noise settings, we show that $ \mathbb{E}\ \hat{f} - f^{\star} \ ^2 \leq v_{f^{\star}}\left(\frac{\log d}{n}\right)^{1/4} $, where $ n $ is the sample size and $ \hat{f} $ is either a penalized least squares estimator or a greedily obtained version of such using linear combinations of ramp, sinusoidal, sigmoidal or other bounded Lipschitz ridge functions. Our risk bound is effective even when the dimension $ d $ is much larger than the available sample size. For settings where the dimension is larger than the square root of the sample size this quantity is seen to improve the more familiar risk bound of $ v_{f^{\star}}\left(\frac{d\log (n/d)}{n}\right)^{1/2} $, also investigated here. version:2
arxiv-1607-01856 | Neural Name Translation Improves Neural Machine Translation | http://arxiv.org/abs/1607.01856 | id:1607.01856 author:Xiaoqing Li, Jiajun Zhang, Chengqing Zong category:cs.CL  published:2016-07-07 summary:In order to control computational complexity, neural machine translation (NMT) systems convert all rare words outside the vocabulary into a single unk symbol. Previous solution (Luong et al., 2015) resorts to use multiple numbered unks to learn the correspondence between source and target rare words. However, testing words unseen in the training corpus cannot be handled by this method. And it also suffers from the noisy word alignment. In this paper, we focus on a major type of rare words -- named entity (NE), and propose to translate them with character level sequence to sequence model. The NE translation model is further used to derive high quality NE alignment in the bilingual training corpus. With the integration of NE translation and alignment modules, our NMT system is able to surpass the baseline system by 2.9 BLEU points on the Chinese to English task. version:1
arxiv-1607-01855 | Iterative Multi-domain Regularized Deep Learning for Anatomical Structure Detection and Segmentation from Ultrasound Images | http://arxiv.org/abs/1607.01855 | id:1607.01855 author:Hao Chen, Yefeng Zheng, Jin-Hyeong Park, Pheng-Ann Heng, S. Kevin Zhou category:cs.CV  published:2016-07-07 summary:Accurate detection and segmentation of anatomical structures from ultrasound images are crucial for clinical diagnosis and biometric measurements. Although ultrasound imaging has been widely used with superiorities such as low cost and portability, the fuzzy border definition and existence of abounding artifacts pose great challenges for automatically detecting and segmenting the complex anatomical structures. In this paper, we propose a multi-domain regularized deep learning method to address this challenging problem. By leveraging the transfer learning from cross domains, the feature representations are effectively enhanced. The results are further improved by the iterative refinement. Moreover, our method is quite efficient by taking advantage of a fully convolutional network, which is formulated as an end-to-end learning framework of detection and segmentation. Extensive experimental results on a large-scale database corroborated that our method achieved a superior detection and segmentation accuracy, outperforming other methods by a significant margin and demonstrating competitive capability even compared to human performance. version:1
arxiv-1607-01842 | Finding Significant Fourier Coefficients: Clarifications, Simplifications, Applications and Limitations | http://arxiv.org/abs/1607.01842 | id:1607.01842 author:Steven D. Galbraith, Joel Laity, Barak Shani category:cs.CR cs.DS cs.LG  published:2016-07-06 summary:Ideas from Fourier analysis have been used in cryptography for three decades. Akavia, Goldwasser and Safra unified some of these ideas to give a complete algorithm that finds significant Fourier coefficients of functions on any finite abelian group. Their algorithm stimulated a lot of interest in the cryptography community, especially in the context of "bit security". This paper attempts to be a friendly and comprehensive guide to the tools and results in this field. The intended readership is cryptographers who have heard about these tools and seek an understanding of their mechanics, and their usefulness and limitations. A compact overview of the algorithm is presented with emphasis on the ideas behind it. We survey some applications of this algorithm, and explain that several results should be taken in the right context. We point out that some of the most important bit security problems are still open. Our original contributions include: an approach to the subject based on modulus switching; a discussion of the limitations on the usefulness of these tools; an answer to an open question about the modular inversion hidden number problem. version:1
arxiv-1607-01794 | VideoLSTM Convolves, Attends and Flows for Action Recognition | http://arxiv.org/abs/1607.01794 | id:1607.01794 author:Zhenyang Li, Efstratios Gavves, Mihir Jain, Cees G. M. Snoek category:cs.CV  published:2016-07-06 summary:We present a new architecture for end-to-end sequence learning of actions in video, we call VideoLSTM. Rather than adapting the video to the peculiarities of established recurrent or convolutional architectures, we adapt the architecture to fit the requirements of the video medium. Starting from the soft-Attention LSTM, VideoLSTM makes three novel contributions. First, video has a spatial layout. To exploit the spatial correlation we hardwire convolutions in the soft-Attention LSTM architecture. Second, motion not only informs us about the action content, but also guides better the attention towards the relevant spatio-temporal locations. We introduce motion-based attention. And finally, we demonstrate how the attention from VideoLSTM can be used for action localization by relying on just the action class label. Experiments and comparisons on challenging datasets for action classification and localization support our claims. version:1
arxiv-1607-01750 | Formal Definitions of Unbounded Evolution and Innovation Reveal Universal Mechanisms for Open-Ended Evolution in Dynamical Systems | http://arxiv.org/abs/1607.01750 | id:1607.01750 author:Alyssa M Adams, Hector Zenil, Paul CW Davies, Sara I Walker category:cs.NE nlin.CG  published:2016-07-06 summary:One of the most remarkable features of the > 3.5 billion year history of life on Earth is the apparent trend of innovation and open-ended growth of complexity. Similar trends are apparent in artificial and technological systems. However, a general framework for understanding open-ended evolution as it might occur in biological or technological systems has not yet been achieved. Here, we cast the problem within the broader context of dynamical systems theory to uncover and characterize mechanisms for producing open-ended evolution (OEE). We present formal definitions of two hallmark features of OEE: unbounded evolution and innovation. We define unbounded evolution as patterns that are non-repeating within the expected Poincare\'e recurrence time of an equivalent isolated system, and innovation as trajectories not observed in isolated systems. As a case study, we test three new variants of cellular automata (CA) that implement time-dependent update rules against these two definitions. We find that each is capable of generating conditions for OEE, but vary in their ability to do so. Our results demonstrate that state-dependent dynamics, widely regarded as a hallmark feature of life, statistically out-perform other candidate mechanisms. It is also the only mechanism to produce OEE in a scalable manner, consistent with notions of OEE as ongoing production of complexity. Our results thereby suggest a new framework for unifying the mechanisms for generating OEE with features distinctive to life and its artifacts, with wide applicability to both biological and artificial systems. version:1
arxiv-1607-01730 | Rolling Horizon Coevolutionary Planning for Two-Player Video Games | http://arxiv.org/abs/1607.01730 | id:1607.01730 author:Jialin Liu, Diego Pérez-Liébana, Simon M. Lucas category:cs.AI cs.NE  published:2016-07-06 summary:This paper describes a new algorithm for decision making in two-player real-time video games. As with Monte Carlo Tree Search, the algorithm can be used without heuristics and has been developed for use in general video game AI. The approach is to extend recent work on rolling horizon evolutionary planning, which has been shown to work well for single-player games, to two (or in principle many) player games. To select an action the algorithm co-evolves two (or in the general case N) populations, one for each player, where each individual is a sequence of actions for the respective player. The fitness of each individual is evaluated by playing it against a selection of action-sequences from the opposing population. When choosing an action to take in the game, the first action is chosen from the fittest member of the population for that player. The new algorithm is compared with a number of general video game AI algorithms on three variations of a two-player space battle game, with promising results. version:1
arxiv-1607-01718 | Graphons, mergeons, and so on! | http://arxiv.org/abs/1607.01718 | id:1607.01718 author:Justin Eldridge, Mikhail Belkin, Yusu Wang category:stat.ML cs.DS math.ST stat.TH  published:2016-07-06 summary:In this work we develop a theory of hierarchical clustering for graphs. Our modelling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the "correct" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties. version:1
arxiv-1607-01719 | Deep CORAL: Correlation Alignment for Deep Domain Adaptation | http://arxiv.org/abs/1607.01719 | id:1607.01719 author:Baochen Sun, Kate Saenko category:cs.CV cs.AI cs.LG cs.NE  published:2016-07-06 summary:Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance. version:1
arxiv-1607-01691 | A Modified Activation Function with Improved Run-Times For Neural Networks | http://arxiv.org/abs/1607.01691 | id:1607.01691 author:Vincent Ike Anireh, Emmanuel Ndidi Osegi category:cs.NE  published:2016-07-06 summary:In this paper we present a modified version of the Hyperbolic Tangent Activation Function as a learning unit generator for neural networks. The function uses an integer calibration constant as an approximation to the Euler number, e, based on a quadratic Real Number Formula (RNF) algorithm and an adaptive normalization constraint on the input activations to avoid the vanishing gradient. We demonstrate the effectiveness of the proposed modification using a hypothetical and real world dataset and show that lower run-times can be achieved by learning algorithms using this function leading to improved speed-ups and learning accuracies during training. version:1
arxiv-1607-01690 | A New Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes Classifier for Coping with Gene Ontology-based Features | http://arxiv.org/abs/1607.01690 | id:1607.01690 author:Cen Wan, Alex A. Freitas category:cs.LG cs.AI H.2.8; I.5.1; I.5.2  published:2016-07-06 summary:The Tree Augmented Naive Bayes classifier is a type of probabilistic graphical model that can represent some feature dependencies. In this work, we propose a Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes (HRE-TAN) algorithm, which considers removing the hierarchical redundancy during the classifier learning process, when coping with data containing hierarchically structured features. The experiments showed that HRE-TAN obtains significantly better predictive performance than the conventional Tree Augmented Naive Bayes classifier, and enhanced the robustness against imbalanced class distributions, in aging-related gene datasets with Gene Ontology terms used as features. version:1
arxiv-1607-01679 | Rock Texture Classification Using Spectral Analysis And Genetically Optimized Texture Features | http://arxiv.org/abs/1607.01679 | id:1607.01679 author:Manuel Blanco Valentin, Clecio Roque De Bom, Marcio Portes de Albuquerque, Marcelo Portes de Albuquerque, Elisangela Faria, Maury Duarte Correia, Rodrigo Surmas category:cs.CV  published:2016-07-06 summary:In this work we present a method to classify a set of rock textures based on a Spectral Analysis and the extraction of the texture Features of the resulted images. Up to 520 features were tested using 4 different filters and all 31 different combinations were verified. The classification process relies on a Naive Bayes classifier. We performed two kinds of optimizations: statistical optimization with Principal Component Analysis (PCA) and a genetic optimization, for 10,000 randomly defined samples, achieving a final maximum classification success of 91% against the original ~70% success ratio. After the optimization 9 types of features emerged as most relevant. version:1
arxiv-1607-01668 | Tensor Decomposition for Signal Processing and Machine Learning | http://arxiv.org/abs/1607.01668 | id:1607.01668 author:Nicholas D. Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E. Papalexakis, Christos Faloutsos category:stat.ML cs.LG  published:2016-07-06 summary:Tensors or multi-way arrays are functions of three or more indices $(i,j,k,\cdots)$ -- similar to matrices (two-way arrays), which are functions of two indices $(r,c)$ for (row,column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning. version:1
arxiv-1607-01628 | Guided Alignment Training for Topic-Aware Neural Machine Translation | http://arxiv.org/abs/1607.01628 | id:1607.01628 author:Wenhu Chen, Evgeny Matusov, Shahram Khadivi, Jan-Thorsten Peter category:cs.CL cs.NE  published:2016-07-06 summary:In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3%. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1% BLEU absolute. version:1
arxiv-1607-01624 | Bayesian nonparametrics for Sparse Dynamic Networks | http://arxiv.org/abs/1607.01624 | id:1607.01624 author:Konstantina Palla, Francois Caron, Yee Whye Teh category:stat.ML  published:2016-07-06 summary:We propose a Bayesian nonparametric prior for time-varying networks. To each node of the network is associated a positive parameter, modeling the sociability of that node. Sociabilities are assumed to evolve over time, and are modeled via a dynamic point process model. The model is able to (a) capture smooth evolution of the interaction between nodes, allowing edges to appear/disappear over time (b) capture long term evolution of the sociabilities of the nodes (c) and yield sparse graphs, where the number of edges grows subquadratically with the number of nodes. The evolution of the sociabilities is described by a tractable time-varying gamma process. We provide some theoretical insights into the model and apply it to three real world datasets. version:1
arxiv-1607-02028 | Artificial neural networks and fuzzy logic for recognizing alphabet characters and mathematical symbols | http://arxiv.org/abs/1607.02028 | id:1607.02028 author:Giuseppe Airò Farulla, Tiziana Armano, Anna Capietto, Nadir Murru, Rosaria Rossini category:cs.NE cs.LG  published:2016-07-06 summary:Optical Character Recognition software (OCR) are important tools for obtaining accessible texts. We propose the use of artificial neural networks (ANN) in order to develop pattern recognition algorithms capable of recognizing both normal texts and formulae. We present an original improvement of the backpropagation algorithm. Moreover, we describe a novel image segmentation algorithm that exploits fuzzy logic for separating touching characters. version:1
arxiv-1607-01582 | Bagged Boosted Trees for Classification of Ecological Momentary Assessment Data | http://arxiv.org/abs/1607.01582 | id:1607.01582 author:Gerasimos Spanakis, Gerhard Weiss, Anne Roefs category:cs.LG  published:2016-07-06 summary:Ecological Momentary Assessment (EMA) data is organized in multiple levels (per-subject, per-day, etc.) and this particular structure should be taken into account in machine learning algorithms used in EMA like decision trees and its variants. We propose a new algorithm called BBT (standing for Bagged Boosted Trees) that is enhanced by a over/under sampling method and can provide better estimates for the conditional class probability function. Experimental results on a real-world dataset show that BBT can benefit EMA data classification and performance. version:1
arxiv-1607-01577 | CUNet: A Compact Unsupervised Network for Image Classification | http://arxiv.org/abs/1607.01577 | id:1607.01577 author:Le Dong, Ling He, Gaipeng Kong, Qianni Zhang, Xiaochun Cao, Ebroul Izquierdo category:cs.CV  published:2016-07-06 summary:In this paper, we propose a compact network called CUNet (compact unsupervised network) to counter the image classification challenge. Different from the traditional convolutional neural networks learning filters by the time-consuming stochastic gradient descent, CUNet learns the filter bank from diverse image patches with the simple K-means, which significantly avoids the requirement of scarce labeled training images, reduces the training consumption, and maintains the high discriminative ability. Besides, we propose a new pooling method named weighted pooling considering the different weight values of adjacent neurons, which helps to improve the robustness to small image distortions. In the output layer, CUNet integrates the feature maps gained in the last hidden layer, and straightforwardly computes histograms in non-overlapped blocks. To reduce feature redundancy, we implement the max-pooling operation on adjacent blocks to select the most competitive features. Comprehensive experiments are conducted to demonstrate the state-of-the-art classification performances with CUNet on CIFAR-10, STL-10, MNIST and Caltech101 benchmark datasets. version:1
arxiv-1607-01551 | On Sampling and Greedy MAP Inference of Constrained Determinantal Point Processes | http://arxiv.org/abs/1607.01551 | id:1607.01551 author:Tarun Kathuria, Amit Deshpande category:cs.DS cs.LG math.PR  published:2016-07-06 summary:Subset selection problems ask for a small, diverse yet representative subset of the given data. When pairwise similarities are captured by a kernel, the determinants of submatrices provide a measure of diversity or independence of items within a subset. Matroid theory gives another notion of independence, thus giving rise to optimization and sampling questions about Determinantal Point Processes (DPPs) under matroid constraints. Partition constraints, as a special case, arise naturally when incorporating additional labeling or clustering information, besides the kernel, in DPPs. Finding the maximum determinant submatrix under matroid constraints on its row/column indices has been previously studied. However, the corresponding question of sampling from DPPs under matroid constraints has been unresolved, beyond the simple cardinality constrained k-DPPs. We give the first polynomial time algorithm to sample exactly from DPPs under partition constraints, for any constant number of partitions. We complement this by a complexity theoretic barrier that rules out such a result under general matroid constraints. Our experiments indicate that partition-constrained DPPs offer more flexibility and more diversity than k-DPPs and their naive extensions, while being reasonably efficient in running time. We also show that a simple greedy initialization followed by local search gives improved approximation guarantees for the problem of MAP inference from k- DPPs on well-conditioned kernels. Our experiments show that this improvement is significant for larger values of k, supporting our theoretical result. version:1
arxiv-1607-01327 | Report: Feature Selection Techniques for Classification | http://arxiv.org/abs/1607.01327 | id:1607.01327 author:Giorgio Roffo category:cs.CV  published:2016-07-05 summary:In an era where accumulating data is easy and storing it inexpensive, feature selection plays a central role in helping to reduce the high-dimensionality of huge amounts of otherwise meaningless data. This report overviews concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification with a categorizing framework based on the complexity: filter, embedded, and wrappers methods. Some real-world applications are included to demonstrate the use of feature selection in data mining. As a result, the report proposes extensive tests on diverse datasets. We conclude this work by identifying trends and challenges of feature selection research and development while providing a toolbox of 10 methods selected from recent literature. version:2
arxiv-1607-01490 | Towards Self-explanatory Ontology Visualization with Contextual Verbalization | http://arxiv.org/abs/1607.01490 | id:1607.01490 author:Renārs Liepiņš, Uldis Bojārs, Normunds Grūzītis, Kārlis Čerāns, Edgars Celms category:cs.AI cs.CL  published:2016-07-06 summary:Ontologies are one of the core foundations of the Semantic Web. To participate in Semantic Web projects, domain experts need to be able to understand the ontologies involved. Visual notations can provide an overview of the ontology and help users to understand the connections among entities. However, the users first need to learn the visual notation before they can interpret it correctly. Controlled natural language representation would be readable right away and might be preferred in case of complex axioms, however, the structure of the ontology would remain less apparent. We propose to combine ontology visualizations with contextual ontology verbalizations of selected ontology (diagram) elements, displaying controlled natural language (CNL) explanations of OWL axioms corresponding to the selected visual notation elements. Thus, the domain experts will benefit from both the high-level overview provided by the graphical notation and the detailed textual explanations of particular elements in the diagram. version:1
arxiv-1607-01485 | Extracting Formal Models from Normative Texts | http://arxiv.org/abs/1607.01485 | id:1607.01485 author:John J. Camilleri, Normunds Gruzitis, Gerardo Schneider category:cs.CL  published:2016-07-06 summary:Normative texts are documents based on the deontic notions of obligation, permission, and prohibition. Our goal is to model such texts using the C-O Diagram formalism, making them amenable to formal analysis, in particular verifying that a text satisfies properties concerning causality of actions and timing constraints. We present an experimental, semi-automatic aid to bridge the gap between a normative text and its formal representation. Our approach uses dependency trees combined with our own rules and heuristics for extracting the relevant components. The resulting tabular data can then be converted into a C-O Diagram. version:1
arxiv-1607-01462 | An optimal learning method for developing personalized treatment regimes | http://arxiv.org/abs/1607.01462 | id:1607.01462 author:Yingfei Wang, Warren Powell category:stat.ML cs.LG  published:2016-07-06 summary:A treatment regime is a function that maps individual patient information to a recommended treatment, hence explicitly incorporating the heterogeneity in need for treatment across individuals. Patient responses are dichotomous and can be predicted through an unknown relationship that depends on the patient information and the selected treatment. The goal is to find the treatments that lead to the best patient responses on average. Each experiment is expensive, forcing us to learn the most from each experiment. We adopt a Bayesian approach both to incorporate possible prior information and to update our treatment regime continuously as information accrues, with the potential to allow smaller yet more informative trials and for patients to receive better treatment. By formulating the problem as contextual bandits, we introduce a knowledge gradient policy to guide the treatment assignment by maximizing the expected value of information, for which an approximation method is used to overcome computational challenges. We provide a detailed study on how to make sequential medical decisions under uncertainty to reduce health care costs on a real world knee replacement dataset. We use clustering and LASSO to deal with the intrinsic sparsity in health datasets. We show experimentally that even though the problem is sparse, through careful selection of physicians (versus picking them at random), we can significantly improve the success rates. version:1
arxiv-1607-01450 | Pooling Faces: Template based Face Recognition with Pooled Face Images | http://arxiv.org/abs/1607.01450 | id:1607.01450 author:Tal Hassner, Iacopo Masi, Jungyeon Kim, Jongmoo Choi, Shai Harel, Prem Natarajan, Gerard Medioni category:cs.CV  published:2016-07-06 summary:We propose a novel approach to template based face recognition. Our dual goal is to both increase recognition accuracy and reduce the computational and storage costs of template matching. To do this, we leverage on an approach which was proven effective in many other domains, but, to our knowledge, never fully explored for face images: average pooling of face photos. We show how (and why!) the space of a template's images can be partitioned and then pooled based on image quality and head pose and the effect this has on accuracy and template size. We perform extensive tests on the IJB-A and Janus CS2 template based face identification and verification benchmarks. These show that not only does our approach outperform published state of the art despite requiring far fewer cross template comparisons, but also, surprisingly, that image pooling performs on par with deep feature pooling. version:1
arxiv-1607-01437 | Attribute Recognition from Adaptive Parts | http://arxiv.org/abs/1607.01437 | id:1607.01437 author:Luwei Yang, Ligen Zhu, Yichen Wei, Shuang Liang, Ping Tan category:cs.CV  published:2016-07-05 summary:Previous part-based attribute recognition approaches perform part detection and attribute recognition in separate steps. The parts are not optimized for attribute recognition and therefore could be sub-optimal. We present an end-to-end deep learning approach to overcome the limitation. It generates object parts from key points and perform attribute recognition accordingly, allowing adaptive spatial transform of the parts. Both key point estimation and attribute recognition are learnt jointly in a multi-task setting. Extensive experiments on two datasets verify the efficacy of proposed end-to-end approach. version:1
arxiv-1607-00559 | Sub-sampled Newton Methods with Non-uniform Sampling | http://arxiv.org/abs/1607.00559 | id:1607.00559 author:Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Ré, Michael W. Mahoney category:math.OC stat.ML  published:2016-07-02 summary:We consider the problem of finding the minimizer of a convex function $F: \mathbb R^d \rightarrow \mathbb R$ of the form $F(w) := \sum_{i=1}^n f_i(w) + R(w)$ where a low-rank factorization of $\nabla^2 f_i(w)$ is readily available. We consider the regime where $n \gg d$. As second-order methods prove to be effective in finding the minimizer to a high-precision, in this work, we propose randomized Newton-type algorithms that exploit \textit{non-uniform} sub-sampling of $\{\nabla^2 f_i(w)\}_{i=1}^{n}$, as well as inexact updates, as means to reduce the computational complexity. Two non-uniform sampling distributions based on {\it block norm squares} and {\it block partial leverage scores} are considered in order to capture important terms among $\{\nabla^2 f_i(w)\}_{i=1}^{n}$. We show that at each iteration non-uniformly sampling at most $\mathcal O(d \log d)$ terms from $\{\nabla^2 f_i(w)\}_{i=1}^{n}$ is sufficient to achieve a linear-quadratic convergence rate in $w$ when a suitable initial point is provided. In addition, we show that our algorithms achieve a lower computational complexity and exhibit more robustness and better dependence on problem specific quantities, such as the condition number, compared to similar existing methods, especially the ones based on uniform sampling. Finally, we empirically demonstrate that our methods are at least twice as fast as Newton's methods with ridge logistic regression on several real datasets. version:2
arxiv-1607-01432 | Global Neural CCG Parsing with Optimality Guarantees | http://arxiv.org/abs/1607.01432 | id:1607.01432 author:Kenton Lee, Mike Lewis, Luke Zettlemoyer category:cs.CL  published:2016-07-05 summary:We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a new objective that encourages the parser to explore a tiny fraction of the search space. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees. version:1
arxiv-1607-01426 | Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks | http://arxiv.org/abs/1607.01426 | id:1607.01426 author:Rajarshi Das, Arvind Neelakantan, David Belanger, Andrew McCallum category:cs.CL  published:2016-07-05 summary:Our goal is to combine the rich multi-step inference of symbolic logical reasoning together with the generalization capabilities of vector embeddings and neural networks. We are particularly interested in complex reasoning about the entities and relations in knowledge bases. Recently Neelakantan et al. (2015) presented a compelling methodology using recurrent neural networks (RNNs) to compose the meaning of relations in a Horn clause consisting of a connected chain. However, this work has multiple weaknesses: it accounts for relations but not entities; it limits generalization by training many separate models; it does not combine evidence over multiple paths. In this paper we address all these weaknesses, making key strides towards our goal of rich logical reasoning with neural networks: our RNN leverages and jointly trains both relation and entity type embeddings, we train a single high-capacity RNN to compose Horn clause chains across all predicted relation types; we demonstrate that pooling evidence across multiple chains can dramatically improve both speed of training and final accuracy. We also explore multi-task training of entity and relation types. On a large dataset from ClueWeb and Freebase our approach provides a significant increase in mean average precision from 55.3% to 73.2% version:1
arxiv-1607-01417 | Algorithms for Generalized Cluster-wise Linear Regression | http://arxiv.org/abs/1607.01417 | id:1607.01417 author:Young Woong Park, Yan Jiang, Diego Klabjan, Loren Williams category:stat.ML cs.LG  published:2016-07-05 summary:Cluster-wise linear regression (CLR), a clustering problem intertwined with regression, is to find clusters of entities such that the overall sum of squared errors from regressions performed over these clusters is minimized, where each cluster may have different variances. We generalize the CLR problem by allowing each entity to have more than one observation, and refer to it as generalized CLR. We propose an exact mathematical programming based approach relying on column generation, a column generation based heuristic algorithm that clusters predefined groups of entities, a metaheuristic genetic algorithm with adapted Lloyd's algorithm for K-means clustering, a two-stage approach, and a modified algorithm of Sp{\"a}th \cite{Spath1979} for solving generalized CLR. We examine the performance of our algorithms on a stock keeping unit (SKU) clustering problem employed in forecasting halo and cannibalization effects in promotions using real-world retail data from a large supermarket chain. In the SKU clustering problem, the retailer needs to cluster SKUs based on their seasonal effects in response to promotions. The seasonal effects are the results of regressions with predictors being promotion mechanisms and seasonal dummies performed over clusters generated. We compare the performance of all proposed algorithms for the SKU problem with real-world and synthetic data. version:2
arxiv-1607-01400 | An Aggregate and Iterative Disaggregate Algorithm with Proven Optimality in Machine Learning | http://arxiv.org/abs/1607.01400 | id:1607.01400 author:Young Woong Park, Diego Klabjan category:stat.ML cs.LG  published:2016-07-05 summary:We propose a clustering-based iterative algorithm to solve certain optimization problems in machine learning, where we start the algorithm by aggregating the original data, solving the problem on aggregated data, and then in subsequent steps gradually disaggregate the aggregated data. We apply the algorithm to common machine learning problems such as the least absolute deviation regression problem, support vector machines, and semi-supervised support vector machines. We derive model-specific data aggregation and disaggregation procedures. We also show optimality, convergence, and the optimality gap of the approximated solution in each iteration. A computational study is provided. version:1
arxiv-1607-01381 | One-Shot Session Recommendation Systems with Combinatorial Items | http://arxiv.org/abs/1607.01381 | id:1607.01381 author:Yahel David, Dotan Di Castro, Zohar Karnin category:stat.ML cs.AI cs.IR  published:2016-07-05 summary:In recent years, content recommendation systems in large websites (or \emph{content providers}) capture an increased focus. While the type of content varies, e.g.\ movies, articles, music, advertisements, etc., the high level problem remains the same. Based on knowledge obtained so far on the user, recommend the most desired content. In this paper we present a method to handle the well known user-cold-start problem in recommendation systems. In this scenario, a recommendation system encounters a new user and the objective is to present items as relevant as possible with the hope of keeping the user's session as long as possible. We formulate an optimization problem aimed to maximize the length of this initial session, as this is believed to be the key to have the user come back and perhaps register to the system. In particular, our model captures the fact that a single round with low quality recommendation is likely to terminate the session. In such a case, we do not proceed to the next round as the user leaves the system, possibly never to seen again. We denote this phenomenon a \emph{One-Shot Session}. Our optimization problem is formulated as an MDP where the action space is of a combinatorial nature as we recommend in each round, multiple items. This huge action space presents a computational challenge making the straightforward solution intractable. We analyze the structure of the MDP to prove monotone and submodular like properties that allow a computationally efficient solution via a method denoted by \emph{Greedy Value Iteration} (G-VI). version:1
arxiv-1607-01375 | Efficient Estimation in the Tails of Gaussian Copulas | http://arxiv.org/abs/1607.01375 | id:1607.01375 author:Kalyani Nagaraj, Jie Xu, Raghu Pasupathy, Soumyadip Ghosh category:stat.CO stat.ML  published:2016-07-05 summary:We consider the question of efficient estimation in the tails of Gaussian copulas. Our special focus is estimating expectations over multi-dimensional constrained sets that have a small implied measure under the Gaussian copula. We propose three estimators, all of which rely on a simple idea: identify certain \emph{dominating} point(s) of the feasible set, and appropriately shift and scale an exponential distribution for subsequent use within an importance sampling measure. As we show, the efficiency of such estimators depends crucially on the local structure of the feasible set around the dominating points. The first of our proposed estimators $\estOpt$ is the "full-information" estimator that actively exploits such local structure to achieve bounded relative error in Gaussian settings. The second and third estimators $\estExp$, $\estLap$ are "partial-information" estimators, for use when complete information about the constraint set is not available, they do not exhibit bounded relative error but are shown to achieve polynomial efficiency. We provide sharp asymptotics for all three estimators. For the NORTA setting where no ready information about the dominating points or the feasible set structure is assumed, we construct a multinomial mixture of the partial-information estimator $\estLap$ resulting in a fourth estimator $\estNt$ with polynomial efficiency, and implementable through the ecoNORTA algorithm. Numerical results on various example problems are remarkable, and consistent with theory. version:1
arxiv-1607-01369 | On the Consistency of the Likelihood Maximization Vertex Nomination Scheme: Bridging the Gap Between Maximum Likelihood Estimation and Graph Matching | http://arxiv.org/abs/1607.01369 | id:1607.01369 author:Vince Lyzinski, Keith Levin, Donniell E. Fishkind, Carey E. Priebe category:stat.ML  published:2016-07-05 summary:Given a graph with some block structure in which one of the blocks is deemed interesting, the task of vertex nomination is to order the vertices in such a way that vertices from the interesting block tend to appear earlier. Previous work has yielded several approaches to this problem, with theoretical results proven in the setting where the graph is drawn from a stochastic block model (SBM), including a canonical method that is the vertex nomination analogue of the Bayes optimal classifier. In this paper, we prove the consistency of maximum likelihood (ML)-based vertex nomination, in the sense that the performance of the ML-based scheme asymptotically matches that of the canonical scheme. We prove theorems of this form both in the setting where model parameters are known and where model parameters are unknown. Additionally, we introduce restricted-focus maximum-likelihood vertex nomination, in which an expensive graph-matching subproblem required for the ML-based scheme is replaced with a less expensive linear assignment problem. This change makes ML-based vertex nomination tractable for graphs on the order of tens of thousands of vertices, at the expense of using less information than the standard ML-based scheme. Nonetheless, we prove consistency of the restricted-focus ML-based scheme and show that it is empirically competitive with other standard approaches. Finally, we introduce a way to incorporate vertex features into ML-based vertex nomination and briefly explore the empirical effectiveness of this approach. version:1
arxiv-1607-00024 | Review-Based Rating Prediction | http://arxiv.org/abs/1607.00024 | id:1607.00024 author:Tal Hadad category:cs.IR cs.LG  published:2016-06-30 summary:Recommendation systems are an important units in today's e-commerce applications, such as targeted advertising, personalized marketing and information retrieval. In recent years, the importance of contextual information has motivated generation of personalized recommendations according to the available contextual information of users. Compared to the traditional systems which mainly utilize users' rating history, review-based recommendation hopefully provide more relevant results to users. We introduce a review-based recommendation approach that obtains contextual information by mining user reviews. The proposed approach relate to features obtained by analyzing textual reviews using methods developed in Natural Language Processing (NLP) and information retrieval discipline to compute a utility function over a given item. An item utility is a measure that shows how much it is preferred according to user's current context. In our system, the context inference is modeled as similarity between the users reviews history and the item reviews history. As an example application, we used our method to mine contextual data from customers' reviews of movies and use it to produce review-based rating prediction. The predicted ratings can generate recommendations that are item-based and should appear at the recommended items list in the product page. Our evaluations (surprisingly) suggest that our system can help produce better prediction rating scores in comparison to the standard prediction methods. version:2
arxiv-1607-01346 | Resource Allocation in a MAC with and without security via Game Theoretic Learning | http://arxiv.org/abs/1607.01346 | id:1607.01346 author:Shahid Mehraj Shah, Krishna Chaitanya A, Vinod Sharma category:cs.IT cs.LG math.IT  published:2016-07-05 summary:In this paper a $K$-user fading multiple access channel with and without security constraints is studied. First we consider a F-MAC without the security constraints. Under the assumption of individual CSI of users, we propose the problem of power allocation as a stochastic game when the receiver sends an ACK or a NACK depending on whether it was able to decode the message or not. We have used Multiplicative weight no-regret algorithm to obtain a Coarse Correlated Equilibrium (CCE). Then we consider the case when the users can decode ACK/NACK of each other. In this scenario we provide an algorithm to maximize the weighted sum-utility of all the users and obtain a Pareto optimal point. PP is socially optimal but may be unfair to individual users. Next we consider the case where the users can cooperate with each other so as to disagree with the policy which will be unfair to individual user. We then obtain a Nash bargaining solution, which in addition to being Pareto optimal, is also fair to each user. Next we study a $K$-user fading multiple access wiretap Channel with CSI of Eve available to the users. We use the previous algorithms to obtain a CCE, PP and a NBS. Next we consider the case where each user does not know the CSI of Eve but only its distribution. In that case we use secrecy outage as the criterion for the receiver to send an ACK or a NACK. Here also we use the previous algorithms to obtain a CCE, PP or a NBS. Finally we show that our algorithms can be extended to the case where a user can transmit at different rates. At the end we provide a few examples to compute different solutions and compare them under different CSI scenarios. version:1
arxiv-1607-01312 | Minimum Message Length based Mixture Modelling using Bivariate von Mises Distributions with Applications to Bioinformatics | http://arxiv.org/abs/1607.01312 | id:1607.01312 author:Parthan Kasarapu category:stat.ML q-bio.QM  published:2016-07-05 summary:The modelling of empirically observed data is commonly done using mixtures of probability distributions. In order to model angular data, directional probability distributions such as the bivariate von Mises (BVM) is typically used. The critical task involved in mixture modelling is to determine the optimal number of component probability distributions. We employ the Bayesian information-theoretic principle of minimum message length (MML) to distinguish mixture models by balancing the trade-off between the model's complexity and its goodness-of-fit to the data. We consider the problem of modelling angular data resulting from the spatial arrangement of protein structures using BVM distributions. The main contributions of the paper include the development of the mixture modelling apparatus along with the MML estimation of the parameters of the BVM distribution. We demonstrate that statistical inference using the MML framework supersedes the traditional methods and offers a mechanism to objectively determine models that are of practical significance. version:1
arxiv-1607-01232 | A probabilistic tour of visual attention and gaze shift computational models | http://arxiv.org/abs/1607.01232 | id:1607.01232 author:Giuseppe Boccignone category:cs.CV  published:2016-07-05 summary:In this paper a number of problems are considered which are related to the modelling of eye guidance under visual attention in a natural setting. From a crude discussion of a variety of available models spelled in probabilistic terms, it appears that current approaches in computational vision are hitherto far from achieving the goal of an active observer relying upon eye guidance to accomplish real-world tasks. We argue that this challenging goal not only requires to embody, in a principled way, the problem of eye guidance within the action/perception loop, but to face the inextricable link tying up visual attention, emotion and executive control, in so far as recent neurobiological findings are weighed up. version:1
arxiv-1607-01224 | Machine Learning for Antimicrobial Resistance | http://arxiv.org/abs/1607.01224 | id:1607.01224 author:John W. Santerre, James J. Davis, Fangfang Xia, Rick Stevens category:stat.ML q-bio.QM  published:2016-07-05 summary:Biological datasets amenable to applied machine learning are more available today than ever before, yet they lack adequate representation in the Data-for-Good community. Here we present a work in progress case study performing analysis on antimicrobial resistance (AMR) using standard ensemble machine learning techniques and note the successes and pitfalls such work entails. Broadly, applied machine learning (AML) techniques are well suited to AMR, with classification accuracies ranging from mid-90% to low- 80% depending on sample size. Additionally, these techniques prove successful at identifying gene regions known to be associated with the AMR phenotype. We believe that the extensive amount of biological data available, the plethora of problems presented, and the global impact of such work merits the consideration of the Data- for-Good community. version:1
arxiv-1607-01205 | Learning the semantic structure of objects from Web supervision | http://arxiv.org/abs/1607.01205 | id:1607.01205 author:David Novotny, Diane Larlus, Andrea Vedaldi category:cs.CV  published:2016-07-05 summary:While recent research in image understanding has often focused on recognizing more types of objects, understanding more about the objects is just as important. Recognizing object parts and attributes has been extensively studied before, yet learning large space of such concepts remains elusive due to the high cost of providing detailed object annotations for supervision. The key contribution of this paper is an algorithm to learn the nameable parts of objects automatically, from images obtained by querying Web search engines. The key challenge is the high level of noise in the annotations; to address it, we propose a new unified embedding space where the appearance and geometry of objects and their semantic parts are represented uniformly. Geometric relationships are induced in a soft manner by a rich set of nonsemantic mid-level anchors, bridging the gap between semantic and non-semantic parts. We also show that the resulting embedding provides a visually-intuitive mechanism to navigate the learned concepts and their corresponding images. version:1
arxiv-1606-09002 | Scene Text Detection via Holistic, Multi-Channel Prediction | http://arxiv.org/abs/1606.09002 | id:1606.09002 author:Cong Yao, Xiang Bai, Nong Sang, Xinyu Zhou, Shuchang Zhou, Zhimin Cao category:cs.CV  published:2016-06-29 summary:Recently, scene text detection has become an active research topic in computer vision and document analysis, because of its great importance and significant challenge. However, vast majority of the existing methods detect text within local regions, typically through extracting character, word or line level candidates followed by candidate aggregation and false positive elimination, which potentially exclude the effect of wide-scope and long-range contextual cues in the scene. To take full advantage of the rich information available in the whole natural image, we propose to localize text in a holistic manner, by casting scene text detection as a semantic segmentation problem. The proposed algorithm directly runs on full images and produces global, pixel-wise prediction maps, in which detections are subsequently formed. To better make use of the properties of text, three types of information regarding text region, individual characters and their relationship are estimated, with a single Fully Convolutional Network (FCN) model. With such predictions of text properties, the proposed algorithm can simultaneously handle horizontal, multi-oriented and curved text in real-world natural images. The experiments on standard benchmarks, including ICDAR 2013, ICDAR 2015 and MSRA-TD500, demonstrate that the proposed algorithm substantially outperforms previous state-of-the-art approaches. Moreover, we report the first baseline result on the recently-released, large-scale dataset COCO-Text. version:2
arxiv-1607-01152 | How to Evaluate the Quality of Unsupervised Anomaly Detection Algorithms? | http://arxiv.org/abs/1607.01152 | id:1607.01152 author:Nicolas Goix category:stat.ML cs.LG  published:2016-07-05 summary:When sufficient labeled data are available, classical criteria based on Receiver Operating Characteristic (ROC) or Precision-Recall (PR) curves can be used to compare the performance of un-supervised anomaly detection algorithms. However , in many situations, few or no data are labeled. This calls for alternative criteria one can compute on non-labeled data. In this paper, two criteria that do not require labels are empirically shown to discriminate accurately (w.r.t. ROC or PR based criteria) between algorithms. These criteria are based on existing Excess-Mass (EM) and Mass-Volume (MV) curves, which generally cannot be well estimated in large dimension. A methodology based on feature sub-sampling and aggregating is also described and tested, extending the use of these criteria to high-dimensional datasets and solving major drawbacks inherent to standard EM and MV curves. version:1
arxiv-1607-01149 | Target-Side Context for Discriminative Models in Statistical Machine Translation | http://arxiv.org/abs/1607.01149 | id:1607.01149 author:Aleš Tamchyna, Alexander Fraser, Ondřej Bojar, Marcin Junczys-Dowmunt category:cs.CL  published:2016-07-05 summary:Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses. version:1
arxiv-1607-01136 | Minimalist Regression Network with Reinforced Gradients and Weighted Estimates: a Case Study on Parameters Estimation in Automated Welding | http://arxiv.org/abs/1607.01136 | id:1607.01136 author:Soheil Keshmiri category:cs.LG cs.RO  published:2016-07-05 summary:This paper presents a minimalist neural regression network as an aggregate of independent identical regression blocks that are trained simultaneously. Moreover, it introduces a new multiplicative parameter, shared by all the neural units of a given layer, to maintain the quality of its gradients. Furthermore, it increases its estimation accuracy via learning a weight factor whose quantity captures the redundancy between the estimated and actual values at each training iteration. We choose the estimation of the direct weld parameters of different welding techniques to show a significant improvement in calculation of these parameters by our model in contrast to state-of-the-arts techniques in the literature. Furthermore, we demonstrate the ability of our model to retain its performance when presented with combined data of different welding techniques. This is a nontrivial result in attaining an scalable model whose quality of estimation is independent of adopted welding techniques. version:1
arxiv-1607-01133 | Learning when to trust distant supervision: An application to low-resource POS tagging using cross-lingual projection | http://arxiv.org/abs/1607.01133 | id:1607.01133 author:Meng Fang, Trevor Cohn category:cs.CL  published:2016-07-05 summary:Cross lingual projection of linguistic annotation suffers from many sources of bias and noise, leading to unreliable annotations that cannot be used directly. In this paper, we introduce a novel approach to sequence tagging that learns to correct the errors from cross-lingual projection using an explicit debiasing layer. This is framed as joint learning over two corpora, one tagged with gold standard and the other with projected tags. We evaluated with only 1,000 tokens tagged with gold standard tags, along with more plentiful parallel data. Our system equals or exceeds the state-of-the-art on eight simulated low-resource settings, as well as two real low-resource languages, Malagasy and Kinyarwanda. version:1
arxiv-1607-01115 | Click Carving: Segmenting Objects in Video with Point Clicks | http://arxiv.org/abs/1607.01115 | id:1607.01115 author:Suyog Dutt Jain, Kristen Grauman category:cs.CV cs.AI cs.HC  published:2016-07-05 summary:We present a novel form of interactive video object segmentation where a few clicks by the user helps the system produce a full spatio-temporal segmentation of the object of interest. Whereas conventional interactive pipelines take the user's initialization as a starting point, we show the value in the system taking the lead even in initialization. In particular, for a given video frame, the system precomputes a ranked list of thousands of possible segmentation hypotheses (also referred to as object region proposals) using image and motion cues. Then, the user looks at the top ranked proposals, and clicks on the object boundary to carve away erroneous ones. This process iterates (typically 2-3 times), and each time the system revises the top ranked proposal set, until the user is satisfied with a resulting segmentation mask. Finally, the mask is propagated across the video to produce a spatio-temporal object tube. On three challenging datasets, we provide extensive comparisons with both existing work and simpler alternative methods. In all, the proposed Click Carving approach strikes an excellent balance of accuracy and human effort. It outperforms all similarly fast methods, and is competitive or better than those requiring 2 to 12 times the effort. version:1
arxiv-1607-01097 | AdaNet: Adaptive Structural Learning of Artificial Neural Networks | http://arxiv.org/abs/1607.01097 | id:1607.01097 author:Corinna Cortes, Xavi Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, Scott Yang category:cs.LG  published:2016-07-05 summary:We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accom- panied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem. version:1
arxiv-1607-00976 | Modelling Context with User Embeddings for Sarcasm Detection in Social Media | http://arxiv.org/abs/1607.00976 | id:1607.00976 author:Silvio Amir, Byron C. Wallace, Hao Lyu, Paula Carvalho Mário J. Silva category:cs.CL cs.AI  published:2016-07-04 summary:We introduce a deep neural network for automated sarcasm detection. Recent work has emphasized the need for models to capitalize on contextual features, beyond lexical and syntactic cues present in utterances. For example, different speakers will tend to employ sarcasm regarding different subjects and, thus, sarcasm detection models ought to encode such speaker information. Current methods have achieved this by way of laborious feature engineering. By contrast, we propose to automatically learn and then exploit user embeddings, to be used in concert with lexical signals to recognize sarcasm. Our approach does not require elaborate feature engineering (and concomitant data scraping); fitting user embeddings requires only the text from their previous posts. The experimental results show that our model outperforms a state-of-the-art approach leveraging an extensive set of carefully crafted features. version:2
arxiv-1607-01092 | Incorporating prior knowledge in medical image segmentation: a survey | http://arxiv.org/abs/1607.01092 | id:1607.01092 author:Masoud S. Nosrati, Ghassan Hamarneh category:cs.CV  published:2016-07-05 summary:Medical image segmentation, the task of partitioning an image into meaningful parts, is an important step toward automating medical image analysis and is at the crux of a variety of medical imaging applications, such as computer aided diagnosis, therapy planning and delivery, and computer aided interventions. However, the existence of noise, low contrast and objects' complexity in medical images are critical obstacles that stand in the way of achieving an ideal segmentation system. Incorporating prior knowledge into image segmentation algorithms has proven useful for obtaining more accurate and plausible results. This paper surveys the different types of prior knowledge that have been utilized in different segmentation frameworks. We focus our survey on optimization-based methods that incorporate prior information into their frameworks. We review and compare these methods in terms of the types of prior employed, the domain of formulation (continuous vs. discrete), and the optimization techniques (global vs. local). We also created an interactive online database of existing works and categorized them based on the type of prior knowledge they use. Our website is interactive so that researchers can contribute to keep the database up to date. We conclude the survey by discussing different aspects of designing an energy functional for image segmentation, open problems, and future perspectives. version:1
arxiv-1607-01077 | EmoFit: Affect Monitoring System for Sedentary Jobs | http://arxiv.org/abs/1607.01077 | id:1607.01077 author:Amol Patwardhan, Gerald Knapp category:cs.HC cs.CV  published:2016-07-05 summary:Emotional and physical well-being at workplace is important for a positive work environment and higher productivity. Jobs such as software programming lead to a sedentary lifestyle and require high interaction with computers. Working at the same job for years can cause a feeling of intellectual stagnation and lack of drive. Many employees experience lack of motivation, mild to extreme depression due to reasons such as aversion towards job responsibilities and incompatibility with coworkers or boss. This research proposed an affect monitoring system EmoFit that would play the role of psychological and physical health trainer. The day to day computer activity and body language was analyzed to detect the physical and emotional well-being of the user. Keystrokes, activity interruptions, eye tracking, facial expressions, body posture and speech were monitored to gauge the users health. The system also provided activities such as at-desk exercise and stress relief game and motivational quotes in an attempt to promote users well-being. The experimental results and positive feedback from test subjects showed that EmoFit would help improve emotional and physical well-being at jobs that involve significant computer usage. version:1
arxiv-1607-01076 | Aggressive actions and anger detection from multiple modalities using Kinect | http://arxiv.org/abs/1607.01076 | id:1607.01076 author:Amol Patwardhan, Gerald Knapp category:cs.HC cs.CV  published:2016-07-05 summary:Prison facilities, mental correctional institutions, sports bars and places of public protest are prone to sudden violence and conflicts. Surveillance systems play an important role in mitigation of hostile behavior and improvement of security by detecting such provocative and aggressive activities. This research proposed using automatic aggressive behavior and anger detection to improve the effectiveness of the surveillance systems. An emotion and aggression aware component will make the surveillance system highly responsive and capable of alerting the security guards in real time. This research proposed facial expression, head, hand and body movement and speech tracking for detecting anger and aggressive actions. Recognition was achieved using support vector machines and rule based features. The multimodal affect recognition precision rate for anger improved by 15.2% and recall rate improved by 11.7% when behavioral rule based features were used in aggressive action detection. version:1
arxiv-1607-01059 | Improving Sparse Representation-Based Classification Using Local Principal Component Analysis | http://arxiv.org/abs/1607.01059 | id:1607.01059 author:Chelsea Weaver, Naoki Saito category:cs.CV  published:2016-07-04 summary:Sparse representation-based classification (SRC), proposed by Wright et al., seeks the sparsest decomposition of a test sample over the dictionary of training samples, with classification to the most-contributing class. Because it assumes test samples can be written as linear combinations of their same-class training samples, the success of SRC depends on the size and representativeness of the training set. Our proposed classification algorithm enlarges the training set by using local principal component analysis to approximate the basis vectors of the tangent hyperplane of the class manifold at each training sample. The dictionary in SRC is replaced by a local dictionary that adapts to the test sample and includes training samples and their corresponding tangent basis vectors. We use a synthetic data set and three face databases to demonstrate that this method can achieve higher classification accuracy than SRC in cases of sparse sampling, nonlinear class manifolds, and stringent dimension reduction. version:1
arxiv-1607-01050 | Application of Statistical Relational Learning to Hybrid Recommendation Systems | http://arxiv.org/abs/1607.01050 | id:1607.01050 author:Shuo Yang, Mohammed Korayem, Khalifeh AlJadda, Trey Grainger, Sriraam Natarajan category:cs.AI cs.IR cs.LG  published:2016-07-04 summary:Recommendation systems usually involve exploiting the relations among known features and content that describe items (content-based filtering) or the overlap of similar users who interacted with or rated the target item (collaborative filtering). To combine these two filtering approaches, current model-based hybrid recommendation systems typically require extensive feature engineering to construct a user profile. Statistical Relational Learning (SRL) provides a straightforward way to combine the two approaches. However, due to the large scale of the data used in real world recommendation systems, little research exists on applying SRL models to hybrid recommendation systems, and essentially none of that research has been applied on real big-data-scale systems. In this paper, we proposed a way to adapt the state-of-the-art in SRL learning approaches to construct a real hybrid recommendation system. Furthermore, in order to satisfy a common requirement in recommendation systems (i.e. that false positives are more undesirable and therefore penalized more harshly than false negatives), our approach can also allow tuning the trade-off between the precision and recall of the system in a principled way. Our experimental results demonstrate the efficiency of our proposed approach as well as its improved performance on recommendation precision. version:1
arxiv-1607-01036 | Bootstrap Model Aggregation for Distributed Statistical Learning | http://arxiv.org/abs/1607.01036 | id:1607.01036 author:Jun Han, Qiang Liu category:stat.ML cs.AI cs.LG  published:2016-07-04 summary:In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods. version:1
arxiv-1607-01027 | Accelerate Stochastic Subgradient Method by Leveraging Local Error Bound | http://arxiv.org/abs/1607.01027 | id:1607.01027 author:Yi Xu, Qihang Lin, Tianbao Yang category:math.OC cs.LG cs.NA stat.ML  published:2016-07-04 summary:In this paper, we propose two accelerated stochastic {\bf subgradient} methods for stochastic non-strongly convex optimization problems by leveraging a generic local error bound condition. The novelty of the proposed methods lies at smartly leveraging the recent historical solution to tackle the variance in the stochastic subgradient. The key idea of both methods is to iteratively solve the original problem approximately in a local region around a recent historical solution with size of the local region gradually decreasing as the solution approaches the optimal set. The difference of the two methods lies at how to construct the local region. The first method uses an explicit ball constraint and the second method uses an implicit regularization approach. For both methods, we establish the improved iteration complexity in a high probability for achieving an $\epsilon$-optimal solution. Besides the improved order of iteration complexity with a high probability, the proposed algorithms also enjoy a logarithmic dependence on the distance of the initial solution to the optimal set. When applied to the $\ell_1$ regularized polyhedral loss minimization (e.g., hinge loss, absolute loss), the proposed stochastic methods have a logarithmic iteration complexity. version:1
arxiv-1607-00992 | Generic Statistical Relational Entity Resolution in Knowledge Graphs | http://arxiv.org/abs/1607.00992 | id:1607.00992 author:Jay Pujara, Lise Getoor category:cs.AI cs.CL  published:2016-07-04 summary:Entity resolution, the problem of identifying the underlying entity of references found in data, has been researched for many decades in many communities. A common theme in this research has been the importance of incorporating relational features into the resolution process. Relational entity resolution is particularly important in knowledge graphs (KGs), which have a regular structure capturing entities and their interrelationships. We identify three major problems in KG entity resolution: (1) intra-KG reference ambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending KGs with new facts. We implement a framework that generalizes across these three settings and exploits this regular structure of KGs. Our framework has many advantages over custom solutions widely deployed in industry, including collective inference, scalability, and interpretability. We apply our framework to two real-world KG entity resolution problems, ambiguity in NELL and merging data from Freebase and MusicBrainz, demonstrating the importance of relational features. version:1
arxiv-1607-00971 | Can we unify monocular detectors for autonomous driving by using the pixel-wise semantic segmentation of CNNs? | http://arxiv.org/abs/1607.00971 | id:1607.00971 author:Eduardo Romera, Luis M. Bergasa, Roberto Arroyo category:cs.CV  published:2016-07-04 summary:Autonomous driving is a challenging topic that requires complex solutions in perception tasks such as recognition of road, lanes, traffic signs or lights, vehicles and pedestrians. Through years of research, computer vision has grown capable of tackling these tasks with monocular detectors that can provide remarkable detection rates with relatively low processing times. However, the recent appearance of Convolutional Neural Networks (CNNs) has revolutionized the computer vision field and has made possible approaches to perform full pixel-wise semantic segmentation in times close to real time (even on hardware that can be carried on a vehicle). In this paper, we propose to use full image segmentation as an approach to simplify and unify most of the detection tasks required in the perception module of an autonomous vehicle, analyzing major concerns such as computation time and detection performance. version:1
arxiv-1607-00970 | Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation | http://arxiv.org/abs/1607.00970 | id:1607.00970 author:Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, Zhi Jin category:cs.CL cs.LG  published:2016-07-04 summary:Using neural networks to generate replies in human-computer dialogue systems is attracting increasing attention over the past few years. However, the performance is not satisfactory: the neural network tends to generate short, safe universal replies which carry little meaning. In this paper, we propose a content-introducing approach to neural network-based generative dialogue systems. We first use pointwise mutial information (PMI) to predict a noun as a keyword, reflecting the main topic of the reply. We then propose seq2BF, a "sequence to backward and forward sequences" model, which generates a reply containing the given keyword. Experimental results show that our approach significantly outperforms traditional sequence-to-sequence models in terms of human evaluation and the entropy measure. version:1
arxiv-1607-00969 | Cell assemblies at multiple time scales with arbitrary lag distributions | http://arxiv.org/abs/1607.00969 | id:1607.00969 author:Eleonora Russo, Daniel Durstewitz category:q-bio.NC cs.CV  published:2016-07-04 summary:Hebb's idea of a cell assembly as the fundamental unit of neural information processing has dominated neuroscience like no other theoretical concept within the past 60 years. A range of different physiological phenomena, from precisely synchronized spiking to broadly simultaneous rate increases, has been subsumed under this term. Yet progress in this area is hampered by the lack of statistical tools that would enable to extract assemblies with arbitrary constellations of time lags, and at multiple temporal scales, partly due to the severe computational burden. Here we present such a unifying methodological and conceptual framework which detects assembly structure at many different time scales, levels of precision, and with arbitrary internal organization. Applying this methodology to multiple single unit recordings from various cortical areas, we find that there is no universal cortical coding scheme, but that assembly structure and precision significantly depends on brain area recorded and ongoing task demands. version:1
arxiv-1606-09187 | Zero Shot Learning for Semantic Boundary Detection - How Far Can We Get? | http://arxiv.org/abs/1606.09187 | id:1606.09187 author:Jing Yu Koh, Wojciech Samek, Klaus-Robert Müller, Alexander Binder category:cs.CV cs.NE stat.ML  published:2016-06-29 summary:Semantic boundary and edge detection aims at simultaneously detecting object edge pixels in images and assigning class labels to them. Systematic training of predictors for this task requires the labeling of edges in images which is a particularly tedious task. We propose a novel strategy for solving this task in an almost zero-shot manner by relying on conventional whole image neural net classifiers that were trained using large bounding boxes. Our method performs the following two steps at test time. First it predicts the class labels by applying the trained whole image network to the test images. Second it computes pixel-wise scores from the obtained predictions by applying backprop gradients as well as recent visualization algorithms such as deconvolution and layer-wise relevance propagation. We show that high pixel-wise scores are indicative for the location of semantic boundaries, which suggests that the semantic boundary problem can be approached without using edge labels during the training phase. version:2
arxiv-1607-00932 | Optimal Quantum Sample Complexity of Learning Algorithms | http://arxiv.org/abs/1607.00932 | id:1607.00932 author:Srinivasan Arunachalam, Ronald de Wolf category:quant-ph cs.CC cs.LG  published:2016-07-04 summary:$ \newcommand{\eps}{\varepsilon} $In learning theory, the VC dimension of a concept class $C$ is the most common way to measure its "richness." In the PAC model $$ \Theta\Big(\frac{d}{\eps} + \frac{\log(1/\delta)}{\eps}\Big) $$ examples are necessary and sufficient for a learner to output, with probability $1-\delta$, a hypothesis $h\in C$ that is $\eps$-close to the target concept $c$. In the related agnostic model, where the samples need not come from a $c\in C$, we know that $$ \Theta\Big(\frac{d}{\eps^2} + \frac{\log(1/\delta)}{\eps^2}\Big) $$ examples are necessary and sufficient to output an hypothesis $h\in C$ that is $\eps$-close to the best concept in $C$. Here we analyze quantum sample complexity, where each example is a coherent quantum state. This model was introduced by Bshouty and Jackson, who showed that quantum examples are more powerful than classical examples in some fixed-distribution settings. However, Atici and Servedio, improved by Zhang, showed that in the PAC setting, quantum examples cannot be much more powerful: the required number of quantum examples is $$ \Omega\Big(\frac{d^{1-\eta}}{\eps} + d + \frac{\log(1/\delta)}{\eps}\Big)\mbox{ for all }\eta> 0. $$ Our main result is that quantum and classical sample complexity are in fact equal up to constant factors in both the PAC and agnostic models. We give two approaches. The first is a fairly simple information-theoretic argument that yields the above two classical bounds and yields the same bounds for quantum sample complexity up to a $\log(d/\eps)$ factor. We then give a second approach that avoids the log-factor loss, based on analyzing the behavior of the "Pretty Good Measurement" on the quantum state identification problems that correspond to learning. This shows classical and quantum sample complexity are equal up to constant factors. version:1
arxiv-1607-00139 | TensiStrength: Stress and relaxation magnitude detection for social media texts | http://arxiv.org/abs/1607.00139 | id:1607.00139 author:Mike Thelwall category:cs.CL  published:2016-07-01 summary:Computer systems need to be able to react to stress in order to perform optimally on some tasks. This article describes TensiStrength, a system to detect the strength of stress and relaxation expressed in social media text messages. TensiStrength uses a lexical approach and a set of rules to detect direct and indirect expressions of stress or relaxation, particularly in the context of transportation. It is slightly more effective than a comparable sentiment analysis program, although their similar performances occur despite differences on almost half of the tweets gathered. The effectiveness of TensiStrength depends on the nature of the tweets classified, with tweets that are rich in stress-related terms being particularly problematic. Although generic machine learning methods can give better performance than TensiStrength overall, they exploit topic-related terms in a way that may be undesirable in practical applications and that may not work as well in more focused contexts. In conclusion, TensiStrength and generic machine learning approaches work well enough to be practical choices for intelligent applications that need to take advantage of stress information, and the decision about which to use depends on the nature of the texts analysed and the purpose of the task. version:2
arxiv-1607-00872 | Neighborhood Features Help Detecting Electricity Theft in Big Data Sets | http://arxiv.org/abs/1607.00872 | id:1607.00872 author:Patrick Glauner, Jorge Meira, Lautaro Dolberg, Radu State, Franck Bettinger, Yves Rangoni, Diogo Duarte category:cs.LG cs.AI  published:2016-07-04 summary:Electricity theft is a major problem around the world in both developed and developing countries and may range up to 40% of the total electricity distributed. More generally, electricity theft belongs to non-technical losses (NTL), which are losses that occur during the distribution of electricity in power grids. In this paper, we build features from the neighborhood of customers. We first split the area in which the customers are located into grids of different sizes. For each grid cell we then compute the proportion of inspected customers and the proportion of NTL found among the inspected customers. We then analyze the distributions of features generated and show why they are useful to predict NTL. In addition, we compute features from the consumption time series of customers. We also use master data features of customers, such as their customer class and voltage of their connection. We compute these features for a Big Data base of 31M meter readings, 700K customers and 400K inspection results. We then use these features to train four machine learning algorithms that are particularly suitable for Big Data sets because of their parallelizable structure: logistic regression, k-nearest neighbors, linear support vector machine and random forest. Using the neighborhood features instead of only analyzing the time series has resulted in appreciable results for Big Data sets for varying NTL proportions of 1%-90%. This work can therefore be deployed to a wide range of different regions around the world. version:1
arxiv-1607-00765 | Multi-Objective Design of State Feedback Controllers Using Reinforced Quantum-Behaved Particle Swarm Optimization | http://arxiv.org/abs/1607.00765 | id:1607.00765 author:Kaveh Hassani, Won-Sook Lee category:cs.NE cs.RO cs.SY  published:2016-07-04 summary:In this paper, a novel and generic multi-objective design paradigm is proposed which utilizes quantum-behaved PSO(QPSO) for deciding the optimal configuration of the LQR controller for a given problem considering a set of competing objectives. There are three main contributions introduced in this paper as follows. (1) The standard QPSO algorithm is reinforced with an informed initialization scheme based on the simulated annealing algorithm and Gaussian neighborhood selection mechanism. (2) It is also augmented with a local search strategy which integrates the advantages of memetic algorithm into conventional QPSO. (3) An aggregated dynamic weighting criterion is introduced that dynamically combines the soft and hard constraints with control objectives to provide the designer with a set of Pareto optimal solutions and lets her to decide the target solution based on practical preferences. The proposed method is compared against a gradient-based method, seven meta-heuristics, and the trial-and-error method on two control benchmarks using sensitivity analysis and full factorial parameter selection and the results are validated using one-tailed T-test. The experimental results suggest that the proposed method outperforms opponent methods in terms of controller effort, measures associated with transient response and criteria related to steady-state. version:1
arxiv-1607-00743 | A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank Designs | http://arxiv.org/abs/1607.00743 | id:1607.00743 author:Miles E. Lopes category:math.ST stat.ME stat.ML stat.TH  published:2016-07-04 summary:We study the residual bootstrap (RB) method in the context of high-dimensional linear regression. Specifically, we analyze the distributional approximation of linear contrasts $c^{\top} (\hat{\beta}_{\rho}-\beta)$, where $\hat{\beta}_{\rho}$ is a ridge-regression estimator. When regression coefficients are estimated via least squares, classical results show that RB consistently approximates the laws of contrasts, provided that $p\ll n$, where the design matrix is of size $n\times p$. Up to now, relatively little work has considered how additional structure in the linear model may extend the validity of RB to the setting where $p/n\asymp 1$. In this setting, we propose a version of RB that resamples residuals obtained from ridge regression. Our main structural assumption on the design matrix is that it is nearly low rank --- in the sense that its singular values decay according to a power-law profile. Under a few extra technical assumptions, we derive a simple criterion for ensuring that RB consistently approximates the law of a given contrast. We then specialize this result to study confidence intervals for mean response values $X_i^{\top} \beta$, where $X_i^{\top}$ is the $i$th row of the design. More precisely, we show that conditionally on a Gaussian design with near low-rank structure, RB simultaneously approximates all of the laws $X_i^{\top}(\hat{\beta}_{\rho}-\beta)$, $i=1,\dots,n$. This result is also notable as it imposes no sparsity assumptions on $\beta$. Furthermore, since our consistency results are formulated in terms of the Mallows (Kantorovich) metric, the existence of a limiting distribution is not required. version:1
arxiv-1606-08998 | LCrowdV: Generating Labeled Videos for Simulation-based Crowd Behavior Learning | http://arxiv.org/abs/1606.08998 | id:1606.08998 author:Ernest Cheung, Tsan Kwong Wong, Aniket Bera, Xiaogang Wang, Dinesh Manocha category:cs.CV  published:2016-06-29 summary:We present a novel procedural framework to generate an arbitrary number of labeled crowd videos (LCrowdV). The resulting crowd video datasets are used to design accurate algorithms or training models for crowded scene understanding. Our overall approach is composed of two components: a procedural simulation framework for generating crowd movements and behaviors, and a procedural rendering framework to generate different videos or images. Each video or image is automatically labeled based on the environment, number of pedestrians, density, behavior, flow, lighting conditions, viewpoint, noise, etc. Furthermore, we can increase the realism by combining synthetically-generated behaviors with real-world background videos. We demonstrate the benefits of LCrowdV over prior lableled crowd datasets by improving the accuracy of pedestrian detection and crowd behavior classification algorithms. LCrowdV would be released on the WWW. version:2
arxiv-1607-00730 | Learning Fine-Scaled Depth Maps from Single RGB Images | http://arxiv.org/abs/1607.00730 | id:1607.00730 author:Jun Li, Reinhard Klein, Angela Yao category:cs.CV  published:2016-07-04 summary:Inferring the underlying depth map from a single image is an ill-posed and inherently ambiguous problem. State-of-the-art deep learning methods can now estimate accurate depth maps, but when projected into 3D, still lack local detail and are often highly distorted. We propose a multi-scale convolution neural network to learn from single RGB images fine-scaled depth maps that result in realistic 3D reconstructions. To encourage spatial coherency, we introduce spatial coordinate feature maps and a local relative depth constraint. In our network, the three scales are closely integrated with skip fusion layers, making it highly efficient to train with large-scale data. Experiments on the NYU Depth v2 dataset shows that our depth predictions are not only competitive with state-of-the-art but also leading to 3D reconstructions that are accurate and rich with detail. version:1
arxiv-1607-00719 | Coarse2Fine: Two-Layer Fusion For Image Retrieval | http://arxiv.org/abs/1607.00719 | id:1607.00719 author:Gaipeng Kong, Le Dong, Wenpu Dong, Liang Zheng, Qi Tian category:cs.MM cs.CV cs.IR  published:2016-07-04 summary:This paper addresses the problem of large-scale image retrieval. We propose a two-layer fusion method which takes advantage of global and local cues and ranks database images from coarse to fine (C2F). Departing from the previous methods fusing multiple image descriptors simultaneously, C2F is featured by a layered procedure composed by filtering and refining. In particular, C2F consists of three components. 1) Distractor filtering. With holistic representations, noise images are filtered out from the database, so the number of candidate images to be used for comparison with the query can be greatly reduced. 2) Adaptive weighting. For a certain query, the similarity of candidate images can be estimated by holistic similarity scores in complementary to the local ones. 3) Candidate refining. Accurate retrieval is conducted via local features, combining the pre-computed adaptive weights. Experiments are presented on two benchmarks, \emph{i.e.,} Holidays and Ukbench datasets. We show that our method outperforms recent fusion methods in terms of storage consumption and computation complexity, and that the accuracy is competitive to the state-of-the-arts. version:1
arxiv-1607-00718 | Towards Abstraction from Extraction: Multiple Timescale Gated Recurrent Unit for Summarization | http://arxiv.org/abs/1607.00718 | id:1607.00718 author:Minsoo Kim, Moirangthem Dennis Singh, Minho Lee category:cs.CL  published:2016-07-04 summary:In this work, we introduce temporal hierarchies to the sequence to sequence (seq2seq) model to tackle the problem of abstractive summarization of scientific articles. The proposed Multiple Timescale model of the Gated Recurrent Unit (MTGRU) is implemented in the encoder-decoder setting to better deal with the presence of multiple compositionalities in larger texts. The proposed model is compared to the conventional RNN encoder-decoder, and the results demonstrate that our model trains faster and shows significant performance gains. The results also show that the temporal hierarchies help improve the ability of seq2seq models to capture compositionalities better without the presence of highly complex architectural hierarchies. version:1
arxiv-1607-01274 | Temporal Topic Analysis with Endogenous and Exogenous Processes | http://arxiv.org/abs/1607.01274 | id:1607.01274 author:Baiyang Wang, Diego Klabjan category:cs.CL cs.IR cs.LG  published:2016-07-04 summary:We consider the problem of modeling temporal textual data taking endogenous and exogenous processes into account. Such text documents arise in real world applications, including job advertisements and economic news articles, which are influenced by the fluctuations of the general economy. We propose a hierarchical Bayesian topic model which imposes a "group-correlated" hierarchical structure on the evolution of topics over time incorporating both processes, and show that this model can be estimated from Markov chain Monte Carlo sampling methods. We further demonstrate that this model captures the intrinsic relationships between the topic distribution and the time-dependent factors, and compare its performance with latent Dirichlet allocation (LDA) and two other related models. The model is applied to two collections of documents to illustrate its empirical performance: online job advertisements from DirectEmployers Association and journalists' postings on BusinessInsider.com. version:1
arxiv-1607-00710 | Automatic Generation of Probabilistic Programming from Time Series Data | http://arxiv.org/abs/1607.00710 | id:1607.00710 author:Anh Tong, Jaesik Choi category:stat.ML  published:2016-07-04 summary:Probabilistic programming languages present complex data with a few lines of code. Efficient inference in the compact representation makes probabilistic programming handle complex data in a unified framework. Also, probabilistic programming is a flexible tool which make easier to combine several heterogeneous models. Recently, the Automatic Bayesian Covariance Discovery (ABCD) system discovers a descriptive model to explain time series data and write an easy understandable report. Here we present a novel method to generate probabilistic programming codes from time series data. version:1
arxiv-1607-00706 | A Semi-supervised learning approach to enhance health care Community-based Question Answering: A case study in alcoholism | http://arxiv.org/abs/1607.00706 | id:1607.00706 author:Papis Wongchaisuwat, Diego Klabjan, Siddhartha R. Jonnalagadda category:stat.ML  published:2016-07-04 summary:Community-based Question Answering (CQA) sites play an important role in addressing health information needs. However, a significant number of posted questions remain unanswered. Automatically answering the posted questions can provide a useful source of information for online health communities. In this study, we developed an algorithm to automatically answer health-related questions based on past questions and answers (QA). We also aimed to understand information embedded within online health content that are good features in identifying valid answers. Our proposed algorithm uses information retrieval techniques to identify candidate answers from resolved QA. In order to rank these candidates, we implemented a semi-supervised leaning algorithm that extracts the best answer to a question. We assessed this approach on a curated corpus from Yahoo! Answers and compared against a rule-based string similarity baseline. On our dataset, the semi-supervised learning algorithm has an accuracy of 86.2%. UMLS-based (health-related) features used in the model enhance the algorithm's performance by proximately 8 %. A reasonably high rate of accuracy is obtained given that the data is considerably noisy. Important features distinguishing a valid answer from an invalid answer include text length, number of stop words contained in a test question, a distance between the test question and other questions in the corpus as well as a number of overlapping health-related terms between questions. Overall, our automated QA system based on historical QA pairs is shown to be effective according to the data set in this case study. It is developed for general use in the health care domain which can also be applied to other CQA sites. version:1
arxiv-1607-00696 | Variational limits of k-NN graph based functionals on data clouds | http://arxiv.org/abs/1607.00696 | id:1607.00696 author:Nicolas Garcia Trillos category:math.ST math.AP math.OC stat.ML stat.TH  published:2016-07-03 summary:We consider i.i.d. samples $x_1, \dots, x_n$ from a measure $\nu$ with density supported on a bounded Euclidean domain $D \subseteq R^d $ where $d\geq3$. A graph on the point cloud is obtained by connecting two points if one of them is among the $k$-nearest neighbors of the other. Our goal is to study consistency of graph based procedures to clustering, classification and dimensionality reduction by studying the variational convergence of the graph total variation associated to such $k$-NN graph. We prove that provided $k:=k_n$ scales like $n \gg k_n \gg \log(n)$, then the $\Gamma$-convergence of the graph total variation towards an appropriate weighted total variation is guaranteed. version:1
arxiv-1607-00669 | Understanding the Energy and Precision Requirements for Online Learning | http://arxiv.org/abs/1607.00669 | id:1607.00669 author:Charbel Sakr, Ameya Patil, Sai Zhang, Naresh Shanbhag category:stat.ML cs.LG  published:2016-07-03 summary:It is well-known that the precision of data, hyperparameters, and internal representations employed in learning systems directly impacts its energy, throughput, and latency. The precision requirements for the training algorithm are also important for systems that learn on-the-fly. Prior work has shown that the data and hyperparameters can be quantized heavily without incurring much penalty in classification accuracy when compared to floating point implementations. These works suffer from two key limitations. First, they assume uniform precision for the classifier and for the training algorithm and thus miss out on the opportunity to further reduce precision. Second, prior works are empirical studies. In this article, we overcome both these limitations by deriving analytical lower bounds on the precision requirements of the commonly employed stochastic gradient descent (SGD) on-line learning algorithm in the specific context of a support vector machine (SVM). Lower bounds on the data precision are derived in terms of the the desired classification accuracy and precision of the hyperparameters used in the classifier. Additionally, lower bounds on the hyperparameter precision in the SGD training algorithm are obtained. These bounds are validated using both synthetic and the UCI breast cancer dataset. Additionally, the impact of these precisions on the energy consumption of a fixed-point SVM with on-line training is studied. version:1
arxiv-1606-08698 | Reviving Threshold-Moving: a Simple Plug-in Bagging Ensemble for Binary and Multiclass Imbalanced Data | http://arxiv.org/abs/1606.08698 | id:1606.08698 author:Guillem Collell, Drazen Prelec, Kaustubh Patil category:cs.LG stat.AP stat.ML  published:2016-06-28 summary:Class imbalance presents a major hurdle in the application of data mining methods. A common practice to deal with it is to create ensembles of classifiers that learn from resampled balanced data. For example, bagged decision trees combined with random undersampling (RUS) or the synthetic minority oversampling technique (SMOTE). However, most of the resampling methods entail asymmetric changes to the examples of different classes, which in turn can introduce its own biases in the model. Furthermore, those methods require a performance measure to be specified a priori before learning. An alternative is to use a so-called threshold-moving method that a posteriori changes the decision threshold of a model to counteract the imbalance, thus has a potential to adapt to the performance measure of interest. Surprisingly, little attention has been paid to the potential of combining bagging ensemble with threshold-moving. In this paper, we present probability thresholding bagging (PT-bagging), a versatile plug-in method that fills this gap. Contrary to usual rebalancing practice, our method preserves the natural class distribution of the data resulting in well calibrated posterior probabilities. We also extend the proposed method to handle multiclass data. The method is validated on binary and multiclass benchmark data sets. We perform analyses that provide insights into the proposed method. version:2
arxiv-1607-00662 | Unsupervised Learning of 3D Structure from Images | http://arxiv.org/abs/1607.00662 | id:1607.00662 author:Danilo Jimenez Rezende, S. M. Ali Eslami, Shakir Mohamed, Peter Battaglia, Max Jaderberg, Nicolas Heess category:cs.CV cs.LG stat.ML  published:2016-07-03 summary:A key goal of computer vision is to recover the underlying 3D structure from 2D observations of the world. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 3D and 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained end-to-end from 2D images. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner. version:1
arxiv-1607-00659 | Robust Deep Appearance Models | http://arxiv.org/abs/1607.00659 | id:1607.00659 author:Kha Gia Quach, Chi Nhan Duong, Khoa Luu, Tien D. Bui category:cs.CV  published:2016-07-03 summary:This paper presents a novel Robust Deep Appearance Models to learn the non-linear correlation between shape and texture of face images. In this approach, two crucial components of face images, i.e. shape and texture, are represented by Deep Boltzmann Machines and Robust Deep Boltzmann Machines (RDBM), respectively. The RDBM, an alternative form of Robust Boltzmann Machines, can separate corrupted/occluded pixels in the texture modeling to achieve better reconstruction results. The two models are connected by Restricted Boltzmann Machines at the top layer to jointly learn and capture the variations of both facial shapes and appearances. This paper also introduces new fitting algorithms with occlusion awareness through the mask obtained from the RDBM reconstruction. The proposed approach is evaluated in various applications by using challenging face datasets, i.e. Labeled Face Parts in the Wild (LFPW), Helen, EURECOM and AR databases, to demonstrate its robustness and capabilities. version:1
arxiv-1607-00653 | node2vec: Scalable Feature Learning for Networks | http://arxiv.org/abs/1607.00653 | id:1607.00653 author:Aditya Grover, Jure Leskovec category:cs.SI cs.LG stat.ML  published:2016-07-03 summary:Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks. version:1
arxiv-1607-00623 | Visualizing Natural Language Descriptions: A Survey | http://arxiv.org/abs/1607.00623 | id:1607.00623 author:Kaveh Hassani, Won-Sook Lee category:cs.CL cs.AI cs.CV cs.GR cs.HC  published:2016-07-03 summary:A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation. This survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects. This work serves as a frame of reference to researchers and to enable further advances in the field. version:1
arxiv-1607-00598 | A Coarse-to-Fine Indoor Layout Estimation (CFILE) Method | http://arxiv.org/abs/1607.00598 | id:1607.00598 author:Yuzhuo Ren, Chen Chen, Shangwen Li, C. -C. Jay Kuo category:cs.CV  published:2016-07-03 summary:The task of estimating the spatial layout of cluttered indoor scenes from a single RGB image is addressed in this work. Existing solutions to this problems largely rely on hand-craft features and vanishing lines, and they often fail in highly cluttered indoor rooms. The proposed coarse-to-fine indoor layout estimation (CFILE) method consists of two stages: 1) coarse layout estimation; and 2) fine layout localization. In the first stage, we adopt a fully convolutional neural network (FCN) to obtain a coarse-scale room layout estimate that is close to the ground truth globally. The proposed FCN considers combines the layout contour property and the surface property so as to provide a robust estimate in the presence of cluttered objects. In the second stage, we formulate an optimization framework that enforces several constraints such as layout contour straightness, surface smoothness and geometric constraints for layout detail refinement. Our proposed system offers the state-of-the-art performance on two commonly used benchmark datasets. version:1
arxiv-1607-00592 | Automatic Techniques for Gridding cDNA Microarray Images | http://arxiv.org/abs/1607.00592 | id:1607.00592 author:Naima Kaabouch, Hamid Shahbazkia category:cs.CV  published:2016-07-03 summary:Microarray is considered an important instrument and powerful new technology for large-scale gene sequence and gene expression analysis. One of the major challenges of this technique is the image processing phase. The accuracy of this phase has an important impact on the accuracy and effectiveness of the subsequent gene expression and identification analysis. The processing can be organized mainly into four steps: gridding, spot isolation, segmentation, and quantification. Although several commercial software packages are now available, microarray image analysis still requires some intervention by the user, and thus a certain level of image processing expertise. This paper describes and compares four techniques that perform automatic gridding and spot isolation. The proposed techniques are based on template matching technique, standard deviation, sum, and derivative of these profiles. Experimental results show that the accuracy of the derivative of the sum profile is highly accurate compared to other techniques for good and poor quality microarray images. version:1
arxiv-1607-00589 | An Analysis System for DNA Gel Electrophoresis Images Based on Automatic Thresholding an Enhancement | http://arxiv.org/abs/1607.00589 | id:1607.00589 author:Naima Kaabouch, Richard R. Schultz, Barry Milavetz, Lata Balakrishnan category:cs.CV  published:2016-07-03 summary:Gel electrophoresis, a widely used technique to separate DNA according to their size and weight, generates images that can be analyzed automatically. Manual or semiautomatic image processing presents a bottleneck for further development and leads to reproducibility issues. In this paper, we present a fully automated system with high accuracy for analyzing DNA and proteins. The proposed algorithm consists of four main steps: automatic thresholding, shifting, filtering, and data processing. Automatic thresholding, used to equalize the gray values of the gel electrophoresis image background, is one of the novel operations in this algorithm. Enhancement is also used to improve poor quality images that have faint DNA bands. Experimental results show that the proposed technique eliminates defects due to noise for average quality gel electrophoresis images, while it also improves the quality of poor images. version:1
arxiv-1607-00582 | 3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes | http://arxiv.org/abs/1607.00582 | id:1607.00582 author:Qi Dou, Hao Chen, Yueming Jin, Lequan Yu, Jing Qin, Pheng-Ann Heng category:cs.CV  published:2016-07-03 summary:Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper, we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly, we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties, and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN, a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed. version:1
arxiv-1607-00578 | Context-Dependent Word Representation for Neural Machine Translation | http://arxiv.org/abs/1607.00578 | id:1607.00578 author:Heeyoul Choi, Kyunghyun Cho, Yoshua Bengio category:cs.CL  published:2016-07-03 summary:We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly. version:1
arxiv-1607-00577 | A Hierarchical Distributed Processing Framework for Big Image Data | http://arxiv.org/abs/1607.00577 | id:1607.00577 author:Le Dong, Zhiyu Lin, Yan Liang, Ling He, Ning Zhang, Qi Chen, Xiaochun Cao, Ebroul lzquierdo category:cs.CV  published:2016-07-03 summary:This paper introduces an effective processing framework nominated ICP (Image Cloud Processing) to powerfully cope with the data explosion in image processing field. While most previous researches focus on optimizing the image processing algorithms to gain higher efficiency, our work dedicates to providing a general framework for those image processing algorithms, which can be implemented in parallel so as to achieve a boost in time efficiency without compromising the results performance along with the increasing image scale. The proposed ICP framework consists of two mechanisms, i.e. SICP (Static ICP) and DICP (Dynamic ICP). Specifically, SICP is aimed at processing the big image data pre-stored in the distributed system, while DICP is proposed for dynamic input. To accomplish SICP, two novel data representations named P-Image and Big-Image are designed to cooperate with MapReduce to achieve more optimized configuration and higher efficiency. DICP is implemented through a parallel processing procedure working with the traditional processing mechanism of the distributed system. Representative results of comprehensive experiments on the challenging ImageNet dataset are selected to validate the capacity of our proposed ICP framework over the traditional state-of-the-art methods, both in time efficiency and quality of results. version:1
arxiv-1607-00570 | Representation learning for very short texts using weighted word embedding aggregation | http://arxiv.org/abs/1607.00570 | id:1607.00570 author:Cedric De Boom, Steven Van Canneyt, Thomas Demeester, Bart Dhoedt category:cs.IR cs.CL  published:2016-07-02 summary:Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box. version:1
arxiv-1607-00567 | Rademacher Complexity Bounds for a Penalized Multiclass Semi-supervised Algorithm | http://arxiv.org/abs/1607.00567 | id:1607.00567 author:Yury Maximov, Massih-Reza Amini, Zaid Harchaoui category:stat.ML cs.LG  published:2016-07-02 summary:We propose Rademacher complexity bounds for multiclass classifiers trained with a two-step semi-supervised model. In the first step, the algorithm partitions the partially labeled data and then identifies dense clusters containing $\kappa$ predominant classes using the labeled training examples such that the proportion of their non-predominant classes is below a fixed threshold. In the second step, a classifier is trained by minimizing a margin empirical loss over the labeled training set and a penalization term measuring the disability of the learner to predict the $\kappa$ predominant classes of the identified clusters. The resulting data-dependent generalization error bound involves the margin distribution of the classifier, the stability of the clustering technique used in the first step and Rademacher complexity terms corresponding to partially labeled training data. Our theoretical result exhibit convergence rates extending those proposed in the literature for the binary case, and experimental results on different multiclass classification problems show empirical evidence that supports the theory. version:1
arxiv-1607-00556 | Alzheimer's Disease Diagnostics by a Deeply Supervised Adaptable 3D Convolutional Network | http://arxiv.org/abs/1607.00556 | id:1607.00556 author:Ehsan Hosseini-Asl, Georgy Gimel'farb, Ayman El-Baz category:cs.LG q-bio.NC stat.ML  published:2016-07-02 summary:Early diagnosis, playing an important role in preventing progress and treating the Alzheimer's disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposes to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the \emph{ADNI} MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy and robustness. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the \emph{CADDementia} dataset. version:1
arxiv-1607-00548 | Active Object Localization in Visual Situations | http://arxiv.org/abs/1607.00548 | id:1607.00548 author:Max H. Quinn, Anthony D. Rhodes, Melanie Mitchell category:cs.CV  published:2016-07-02 summary:We describe a method for performing active localization of objects in instances of visual situations. A visual situation is an abstract concept---e.g., "a boxing match", "a birthday party", "walking the dog", "waiting for a bus"---whose image instantiations are linked more by their common spatial and semantic structure than by low-level visual similarity. Our system combines given and learned knowledge of the structure of a particular situation, and adapts that knowledge to a new situation instance as it actively searches for objects. More specifically, the system learns a set of probability distributions describing spatial and other relationships among relevant objects. The system uses those distributions to iteratively sample object proposals on a test image, but also continually uses information from those object proposals to adaptively modify the distributions based on what the system has detected. We test our approach's ability to efficiently localize objects, using a situation-specific image dataset created by our group. We compare the results with several baselines and variations on our method, and demonstrate the strong benefit of using situation knowledge and active context-driven localization. Finally, we contrast our method with several other approaches that use context as well as active search for object localization in images. version:1
arxiv-1607-00534 | Text comparison using word vector representations and dimensionality reduction | http://arxiv.org/abs/1607.00534 | id:1607.00534 author:Hendrik Heuer category:cs.CL  published:2016-07-02 summary:This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (t-SNE) and how it can be implemented using Python. The technique provides a bird's-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like "capital city of". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn. version:1
arxiv-1606-08571 | Learning Generative ConvNet with Continuous Latent Factors by Alternating Back-Propagation | http://arxiv.org/abs/1606.08571 | id:1606.08571 author:Tian Han, Yang Lu, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.CV cs.LG cs.NE  published:2016-06-28 summary:The supervised learning of the discriminative convolutional neural network (ConvNet) is powered by back-propagation on the parameters. In this paper, we show that the unsupervised learning of a popular top-down generative ConvNet model with latent continuous factors can be accomplished by a learning algorithm that consists of alternatively performing back-propagation on both the latent factors and the parameters. The model is a non-linear generalization of factor analysis, where the high-dimensional observed data, is assumed to be the noisy version of a vector generated by a non-linear transformation of a low-dimensional vector of continuous latent factors. Furthermore, it is assumed that these latent factors follow known independent distributions, such as standard normal distributions, and the non-linear transformation is assumed to be parametrized by a top-down ConvNet, which is capable of approximating the highly non-linear mapping from the latent factors to the image. We explore a simple and natural learning algorithm for this model that alternates between the following two steps: (1) inferring the latent factors by Langevin dynamics or gradient descent, and (2) updating the parameters of the ConvNet by gradient descent. Step (1) is based on the gradient of the reconstruction error with respect to the latent factors, which is available by back-propagation. We call this step inferential back-propagation. Step (2) is based on the gradient of the reconstruction error with respect to the parameters, and is also obtained by back-propagation. We refer to this step as learning back-propagation. The code for inferential back-propagation is actually part of the code for learning back-propagation, and thus the inferential back-propagation is actually a by-product of the learning back-propagation. We show that our algorithm can learn realistic generative models of images and sounds. version:2
arxiv-1607-00514 | Approximate Joint Matrix Triangularization | http://arxiv.org/abs/1607.00514 | id:1607.00514 author:Nicolo Colombo, Nikos Vlassis category:cs.NA cs.LG math.NA stat.ML  published:2016-07-02 summary:We consider the problem of approximate joint triangularization of a set of noisy jointly diagonalizable real matrices. Approximate joint triangularizers are commonly used in the estimation of the joint eigenstructure of a set of matrices, with applications in signal processing, linear algebra, and tensor decomposition. By assuming the input matrices to be perturbations of noise-free, simultaneously diagonalizable ground-truth matrices, the approximate joint triangularizers are expected to be perturbations of the exact joint triangularizers of the ground-truth matrices. We provide a priori and a posteriori perturbation bounds on the `distance' between an approximate joint triangularizer and its exact counterpart. The a priori bounds are theoretical inequalities that involve functions of the ground-truth matrices and noise matrices, whereas the a posteriori bounds are given in terms of observable quantities that can be computed from the input matrices. From a practical perspective, the problem of finding the best approximate joint triangularizer of a set of noisy matrices amounts to solving a nonconvex optimization problem. We show that, under a condition on the noise level of the input matrices, it is possible to find a good initial triangularizer such that the solution obtained by any local descent-type algorithm has certain global guarantees. Finally, we discuss the application of approximate joint matrix triangularization to canonical tensor decomposition and we derive novel estimation error bounds. version:1
arxiv-1607-00509 | Big IoT and social networking data for smart cities: Algorithmic improvements on Big Data Analysis in the context of RADICAL city applications | http://arxiv.org/abs/1607.00509 | id:1607.00509 author:Evangelos Psomakelis, Fotis Aisopos, Antonios Litke, Konstantinos Tserpes, Magdalini Kardara, Pablo Martínez Campo category:cs.CY cs.LG cs.SI  published:2016-07-02 summary:In this paper we present a SOA (Service Oriented Architecture)-based platform, enabling the retrieval and analysis of big datasets stemming from social networking (SN) sites and Internet of Things (IoT) devices, collected by smart city applications and socially-aware data aggregation services. A large set of city applications in the areas of Participating Urbanism, Augmented Reality and Sound-Mapping throughout participating cities is being applied, resulting into produced sets of millions of user-generated events and online SN reports fed into the RADICAL platform. Moreover, we study the application of data analytics such as sentiment analysis to the combined IoT and SN data saved into an SQL database, further investigating algorithmic and configurations to minimize delays in dataset processing and results retrieval. version:1
arxiv-1607-00501 | A Distributed Deep Representation Learning Model for Big Image Data Classification | http://arxiv.org/abs/1607.00501 | id:1607.00501 author:Le Dong, Na Lv, Qianni Zhang, Shanshan Xie, Ling He, Mengdie Mao category:cs.CV  published:2016-07-02 summary:This paper describes an effective and efficient image classification framework nominated distributed deep representation learning model (DDRL). The aim is to strike the balance between the computational intensive deep learning approaches (tuned parameters) which are intended for distributed computing, and the approaches that focused on the designed parameters but often limited by sequential computing and cannot scale up. In the evaluation of our approach, it is shown that DDRL is able to achieve state-of-art classification accuracy efficiently on both medium and large datasets. The result implies that our approach is more efficient than the conventional deep learning approaches, and can be applied to big data that is too complex for parameter designing focused approaches. More specifically, DDRL contains two main components, i.e., feature extraction and selection. A hierarchical distributed deep representation learning algorithm is designed to extract image statistics and a nonlinear mapping algorithm is used to map the inherent statistics into abstract features. Both algorithms are carefully designed to avoid millions of parameters tuning. This leads to a more compact solution for image classification of big data. We note that the proposed approach is designed to be friendly with parallel computing. It is generic and easy to be deployed to different distributed computing resources. In the experiments, the largescale image datasets are classified with a DDRM implementation on Hadoop MapReduce, which shows high scalability and resilience. version:1
arxiv-1607-00494 | Double-detector for Sparse Signal Detection from One Bit Compressed Sensing Measurements | http://arxiv.org/abs/1607.00494 | id:1607.00494 author:Hadi Zayyani, Farzan Haddadi, Mehdi Korki category:cs.IT math.IT stat.ML  published:2016-07-02 summary:This letter presents the sparse vector signal detection from one bit compressed sensing measurements, in contrast to the previous works which deal with scalar signal detection. In this letter, available results are extended to the vector case and the GLRT detector and the optimal quantizer design are obtained. Also, a double-detector scheme is introduced in which a sensor level threshold detector is integrated into network level GLRT to improve the performance. The detection criteria of oracle and clairvoyant detectors are also derived. Simulation results show that with careful design of the threshold detector, the overall detection performance of double-detector scheme would be better than the sign-GLRT proposed in [1] and close to oracle and clairvoyant detectors. Also, the proposed detector is applied to spectrum sensing and the results are near the well known energy detector which uses the real valued data while the proposed detector only uses the sign of the data. version:1
arxiv-1607-00485 | Group Sparse Regularization for Deep Neural Networks | http://arxiv.org/abs/1607.00485 | id:1607.00485 author:Simone Scardapane, Danilo Comminiello, Amir Hussain, Aurelio Uncini category:stat.ML cs.LG  published:2016-07-02 summary:In this paper, we consider the joint task of simultaneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are generally dealt with separately, we present a simple regularized formulation allowing to solve all three of them in parallel, using standard optimization routines. Specifically, we extend the group Lasso penalty (originated in the linear regression literature) in order to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We perform an extensive experimental evaluation, by comparing with classical weight decay and Lasso penalties. We show that a sparse version of the group Lasso penalty is able to achieve competitive performances, while at the same time resulting in extremely compact networks with a smaller number of input features. We evaluate both on a toy dataset for handwritten digit recognition, and on multiple realistic large-scale classification problems. version:1
arxiv-1607-00474 | Adaptive Neighborhood Graph Construction for Inference in Multi-Relational Networks | http://arxiv.org/abs/1607.00474 | id:1607.00474 author:Shobeir Fakhraei, Dhanya Sridhar, Jay Pujara, Lise Getoor category:cs.SI cs.AI cs.LG  published:2016-07-02 summary:A neighborhood graph, which represents the instances as vertices and their relations as weighted edges, is the basis of many semi-supervised and relational models for node labeling and link prediction. Most methods employ a sequential process to construct the neighborhood graph. This process often consists of generating a candidate graph, pruning the candidate graph to make a neighborhood graph, and then performing inference on the variables (i.e., nodes) in the neighborhood graph. In this paper, we propose a framework that can dynamically adapt the neighborhood graph based on the states of variables from intermediate inference results, as well as structural properties of the relations connecting them. A key strength of our framework is its ability to handle multi-relational data and employ varying amounts of relations for each instance based on the intermediate inference results. We formulate the link prediction task as inference on neighborhood graphs, and include preliminary results illustrating the effects of different strategies in our proposed framework. version:1
arxiv-1607-00470 | A survey on non-filter-based monocular Visual SLAM systems | http://arxiv.org/abs/1607.00470 | id:1607.00470 author:Georges Younes, Daniel Asmar, Elie Shammas category:cs.CV cs.RO  published:2016-07-02 summary:Extensive research in the field of Visual SLAM for the past fifteen years has yielded workable systems that found their way into various applications, such as robotics and augmented reality. Although filter-based (e.g., Kalman Filter, Particle Filter) Visual SLAM systems were common at some time, non-filter based (i.e., akin to SfM solutions), which are more efficient, are becoming the de facto methodology for building a Visual SLAM system. This paper presents a survey that covers the various non-filter based Visual SLAM systems in the literature, detailing the various components of their implementation, while critically assessing the specific strategies made by each proposed system in implementing its components. version:1
arxiv-1607-00466 | Outlier absorbing based on a Bayesian approach | http://arxiv.org/abs/1607.00466 | id:1607.00466 author:Parsa Bagherzadeh, Hadi Sadoghi Yazdi category:cs.LG  published:2016-07-02 summary:The presence of outliers is prevalent in machine learning applications and may produce misleading results. In this paper a new method for dealing with outliers and anomal samples is proposed. To overcome the outlier issue, the proposed method combines the global and local views of the samples. By combination of these views, our algorithm performs in a robust manner. The experimental results show the capabilities of the proposed method. version:1
arxiv-1607-00464 | NIST: An Image Classification Network to Image Semantic Retrieval | http://arxiv.org/abs/1607.00464 | id:1607.00464 author:Le Dong, Xiuyuan Chen, Mengdie Mao, Qianni Zhang category:cs.CV  published:2016-07-02 summary:This paper proposes a classification network to image semantic retrieval (NIST) framework to counter the image retrieval challenge. Our approach leverages the successful classification network GoogleNet based on Convolutional Neural Networks to obtain the semantic feature matrix which contains the serial number of classes and corresponding probabilities. Compared with traditional image retrieval using feature matching to compute the similarity between two images, NIST leverages the semantic information to construct semantic feature matrix and uses the semantic distance algorithm to compute the similarity. Besides, the fusion strategy can significantly reduce storage and time consumption due to less classes participating in the last semantic distance computation. Experiments demonstrate that our NIST framework produces state-of-the-art results in retrieval experiments on MIRFLICKR-25K dataset. version:1
arxiv-1607-00455 | Alzheimer's Disease Diagnostics by Adaptation of 3D Convolutional Network | http://arxiv.org/abs/1607.00455 | id:1607.00455 author:Ehsan Hosseini-Asl, Robert Keynto, Ayman El-Baz category:cs.LG q-bio.NC stat.ML  published:2016-07-02 summary:Early diagnosis, playing an important role in preventing progress and treating the Alzheimer\{'}s disease (AD), is based on classification of features extracted from brain images. The features have to accurately capture main AD-related variations of anatomical brain structures, such as, e.g., ventricles size, hippocampus shape, cortical thickness, and brain volume. This paper proposed to predict the AD with a deep 3D convolutional neural network (3D-CNN), which can learn generic features capturing AD biomarkers and adapt to different domain datasets. The 3D-CNN is built upon a 3D convolutional autoencoder, which is pre-trained to capture anatomical shape variations in structural brain MRI scans. Fully connected upper layers of the 3D-CNN are then fine-tuned for each task-specific AD classification. Experiments on the CADDementia MRI dataset with no skull-stripping preprocessing have shown our 3D-CNN outperforms several conventional classifiers by accuracy. Abilities of the 3D-CNN to generalize the features learnt and adapt to other domains have been validated on the ADNI dataset. version:1
arxiv-1607-00446 | A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning | http://arxiv.org/abs/1607.00446 | id:1607.00446 author:Martha White, Adam White category:cs.AI cs.LG stat.ML  published:2016-07-02 summary:One of the main obstacles to broad application of reinforcement learning methods is the parameter sensitivity of our core learning algorithms. In many large-scale applications, online computation and function approximation represent key strategies in scaling up reinforcement learning algorithms. In this setting, we have effective and reasonably well understood algorithms for adapting the learning-rate parameter, online during learning. Such meta-learning approaches can improve robustness of learning and enable specialization to current task, improving learning speed. For temporal-difference learning algorithms which we study here, there is yet another parameter, $\lambda$, that similarly impacts learning speed and stability in practice. Unfortunately, unlike the learning-rate parameter, $\lambda$ parametrizes the objective function that temporal-difference methods optimize. Different choices of $\lambda$ produce different fixed-point solutions, and thus adapting $\lambda$ online and characterizing the optimization is substantially more complex than adapting the learning-rate parameter. There are no meta-learning method for $\lambda$ that can achieve (1) incremental updating, (2) compatibility with function approximation, and (3) maintain stability of learning under both on and off-policy sampling. In this paper we contribute a novel objective function for optimizing $\lambda$ as a function of state rather than time. We derive a new incremental, linear complexity $\lambda$-adaption algorithm that does not require offline batch updating or access to a model of the world, and present a suite of experiments illustrating the practicality of our new algorithm in three different settings. Taken together, our contributions represent a concrete step towards black-box application of temporal-difference learning methods in real world problems. version:1
arxiv-1607-00435 | Decoding the Encoding of Functional Brain Networks: an fMRI Classification Comparison of Non-negative Matrix Factorization (NMF), Independent Component Analysis (ICA), and Sparse Coding Algorithms | http://arxiv.org/abs/1607.00435 | id:1607.00435 author:Jianwen Xie, Pamela K. Douglas, Ying Nian Wu, Arthur L. Brody, Ariana E. Anderson category:q-bio.NC cs.LG stat.ML  published:2016-07-01 summary:Brain networks in fMRI are typically identified using spatial independent component analysis (ICA), yet mathematical constraints such as sparse coding and positivity both provide alternate biologically-plausible frameworks for generating brain networks. Non-negative Matrix Factorization (NMF) would suppress negative BOLD signal by enforcing positivity. Spatial sparse coding algorithms ($L1$ Regularized Learning and K-SVD) would impose local specialization and a discouragement of multitasking, where the total observed activity in a single voxel originates from a restricted number of possible brain networks. The assumptions of independence, positivity, and sparsity to encode task-related brain networks are compared; the resulting brain networks for different constraints are used as basis functions to encode the observed functional activity at a given time point. These encodings are decoded using machine learning to compare both the algorithms and their assumptions, using the time series weights to predict whether a subject is viewing a video, listening to an audio cue, or at rest, in 304 fMRI scans from 51 subjects. For classifying cognitive activity, the sparse coding algorithm of $L1$ Regularized Learning consistently outperformed 4 variations of ICA across different numbers of networks and noise levels (p$<$0.001). The NMF algorithms, which suppressed negative BOLD signal, had the poorest accuracy. Within each algorithm, encodings using sparser spatial networks (containing more zero-valued voxels) had higher classification accuracy (p$<$0.001). The success of sparse coding algorithms may suggest that algorithms which enforce sparse coding, discourage multitasking, and promote local specialization may capture better the underlying source processes than those which allow inexhaustible local processes such as ICA. version:1
arxiv-1607-00424 | Learning Relational Dependency Networks for Relation Extraction | http://arxiv.org/abs/1607.00424 | id:1607.00424 author:Dileep Viswanathan, Ameet Soni, Jude Shavlik, Sriraam Natarajan category:cs.AI cs.CL cs.LG  published:2016-07-01 summary:We consider the task of KBP slot filling -- extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction. version:1
arxiv-1607-00417 | Continuous Adaptation of Multi-Camera Person Identification Models through Sparse Non-redundant Representative Selection | http://arxiv.org/abs/1607.00417 | id:1607.00417 author:Abir Das, Rameswar Panda, Amit K. Roy-Chowdhury category:cs.CV  published:2016-07-01 summary:The problem of image-base person identification/recognition is to provide an identity to the image of an individual based on learned models that describe his/her appearance. Most traditional person identification systems rely on learning a static model on tediously labeled training data. Though labeling manually is an indispensable part of a supervised framework, for a large scale identification system labeling huge amount of data is a significant overhead. For large multi-sensor data as typically encountered in camera networks, labeling a lot of samples does not always mean more information, as redundant images are labeled several times. In this work, we propose a convex optimization based iterative framework that progressively and judiciously chooses a sparse but informative set of samples for labeling, with minimal overlap with previously labeled images. We also use a structure preserving sparse reconstruction based classifier to reduce the training burden typically seen in discriminative classifiers. The two stage approach leads to a novel framework for online update of the classifiers involving only the incorporation of new labeled data rather than any expensive training phase. We demonstrate the effectiveness of our approach on multi-camera person re-identification datasets, to demonstrate the feasibility of learning online classification models in multi-camera big data applications. Using three benchmark datasets, we validate our approach and demonstrate that our framework achieves superior performance with significantly less amount of manual labeling. version:1
arxiv-1607-00410 | Domain Adaptation for Neural Networks by Parameter Augmentation | http://arxiv.org/abs/1607.00410 | id:1607.00410 author:Yusuke Watanabe, Kazuma Hashimoto, Yoshimasa Tsuruoka category:cs.CL cs.AI cs.LG  published:2016-07-01 summary:We propose a simple domain adaptation method for neural networks in a supervised setting. Supervised domain adaptation is a way of improving the generalization performance on the target domain by using the source domain dataset, assuming that both of the datasets are labeled. Recently, recurrent neural networks have been shown to be successful on a variety of NLP tasks such as caption generation; however, the existing domain adaptation techniques are limited to (1) tune the model parameters by the target dataset after the training by the source dataset, or (2) design the network to have dual output, one for the source domain and the other for the target domain. Reformulating the idea of the domain adaptation technique proposed by Daume (2007), we propose a simple domain adaptation method, which can be applied to neural networks trained with a cross-entropy loss. On captioning datasets, we show performance improvements over other domain adaptation methods. version:1
arxiv-1607-00360 | A scaled Bregman theorem with applications | http://arxiv.org/abs/1607.00360 | id:1607.00360 author:Richard Nock, Aditya Krishna Menon, Cheng Soon Ong category:cs.LG stat.ML  published:2016-07-01 summary:Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain "Bregman distortions'" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices. Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning. version:1
arxiv-1607-00359 | Moving Toward High Precision Dynamical Modelling in Hidden Markov Models | http://arxiv.org/abs/1607.00359 | id:1607.00359 author:Sébastien Gagnon, Jean Rouat category:cs.CL  published:2016-07-01 summary:Hidden Markov Model (HMM) is often regarded as the dynamical model of choice in many fields and applications. It is also at the heart of most state-of-the-art speech recognition systems since the 70's. However, from Gaussian mixture models HMMs (GMM-HMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of state-of-the-art models did not changed much. The "left-to-right" topology is mostly always employed because very few other alternatives exist. In this paper, we propose that finely-tuned HMM topologies are essential for precise temporal modelling and that this approach should be investigated in state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by pruning down complex generic models. Speech recognition experiments that were conducted indicate that complex time dependencies can be better learned by this approach than with classical "left-to-right" models. version:1
arxiv-1607-00345 | Convergence Rate of Frank-Wolfe for Non-Convex Objectives | http://arxiv.org/abs/1607.00345 | id:1607.00345 author:Simon Lacoste-Julien category:math.OC cs.LG cs.NA stat.ML 90C52  90C90  68T05 G.1.6; I.2.6  published:2016-07-01 summary:We give a simple proof that the Frank-Wolfe algorithm obtains a stationary point at a rate of $O(1/\sqrt{t})$ on non-convex objectives with a Lipschitz continuous gradient. Our analysis is affine invariant and is the first, to the best of our knowledge, giving a similar rate to what was already proven for projected gradient methods (though on slightly different measures of stationarity). version:1
arxiv-1607-00325 | Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation | http://arxiv.org/abs/1607.00325 | id:1607.00325 author:Dong Yu, Morten Kolbæk, Zheng-Hua Tan, Jesper Jensen category:cs.CL cs.LG cs.SD  published:2016-07-01 summary:We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common. version:1
arxiv-1607-00315 | A multilevel framework for sparse optimization with application to inverse covariance estimation and logistic regression | http://arxiv.org/abs/1607.00315 | id:1607.00315 author:Eran Treister, Javier S. Turek, Irad Yavneh category:cs.NA math.OC stat.ML I.2.6; G.1.6  published:2016-07-01 summary:Solving l1 regularized optimization problems is common in the fields of computational biology, signal processing and machine learning. Such l1 regularization is utilized to find sparse minimizers of convex functions. A well-known example is the LASSO problem, where the l1 norm regularizes a quadratic function. A multilevel framework is presented for solving such l1 regularized sparse optimization problems efficiently. We take advantage of the expected sparseness of the solution, and create a hierarchy of problems of similar type, which is traversed in order to accelerate the optimization process. This framework is applied for solving two problems: (1) the sparse inverse covariance estimation problem, and (2) l1-regularized logistic regression. In the first problem, the inverse of an unknown covariance matrix of a multivariate normal distribution is estimated, under the assumption that it is sparse. To this end, an l1 regularized log-determinant optimization problem needs to be solved. This task is challenging especially for large-scale datasets, due to time and memory limitations. In the second problem, the l1-regularization is added to the logistic regression classification objective to reduce overfitting to the data and obtain a sparse model. Numerical experiments demonstrate the efficiency of the multilevel framework in accelerating existing iterative solvers for both of these problems. version:1
arxiv-1607-00279 | Meaningful Models: Utilizing Conceptual Structure to Improve Machine Learning Interpretability | http://arxiv.org/abs/1607.00279 | id:1607.00279 author:Nick Condry category:stat.ML cs.AI  published:2016-07-01 summary:The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind's construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of 'form' and 'function', we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users. version:1
arxiv-1607-00274 | A new analytical approach to consistency and overfitting in regularized empirical risk minimization | http://arxiv.org/abs/1607.00274 | id:1607.00274 author:Nicolas Garcia Trillos, Ryan Murray category:math.ST stat.ML stat.TH  published:2016-07-01 summary:This work considers the problem of binary classification: given training data $x_1, \dots, x_n$ from a certain population, together with associated labels $y_1,\dots, y_n \in \left\{0,1 \right\}$, determine the best label for an element $x$ not among the training data. More specifically, this work considers a variant of the regularized empirical risk functional which is defined intrinsically to the observed data and does not depend on the underlying population. Tools from modern analysis are used to obtain a concise proof of asymptotic consistency as regularization parameters are taken to zero at rates related to the size of the sample. These analytical tools give a new framework for understanding overfitting and underfitting, and rigorously connect the notion of overfitting with a loss of compactness. version:1
arxiv-1607-00273 | Noise Models in Feature-based Stereo Visual Odometry | http://arxiv.org/abs/1607.00273 | id:1607.00273 author:Pablo F. Alcantarilla, Oliver J. Woodford category:cs.RO cs.CV  published:2016-07-01 summary:Feature-based visual structure and motion reconstruction pipelines, common in visual odometry and large-scale reconstruction from photos, use the location of corresponding features in different images to determine the 3D structure of the scene, as well as the camera parameters associated with each image. The noise model, which defines the likelihood of the location of each feature in each image, is a key factor in the accuracy of such pipelines, alongside optimization strategy. Many different noise models have been proposed in the literature; in this paper we investigate the performance of several. We evaluate these models specifically w.r.t. stereo visual odometry, as this task is both simple (camera intrinsics are constant and known; geometry can be initialized reliably) and has datasets with ground truth readily available (KITTI Odometry and New Tsukuba Stereo Dataset). Our evaluation shows that noise models which are more adaptable to the varying nature of noise generally perform better. version:1
arxiv-1607-00267 | Automated 5-year Mortality Prediction using Deep Learning and Radiomics Features from Chest Computed Tomography | http://arxiv.org/abs/1607.00267 | id:1607.00267 author:Gustavo Carneiro, Luke Oakden-Rayner, Andrew P. Bradley, Jacinto Nascimento, Lyle Palmer category:cs.CV  published:2016-07-01 summary:We propose new methods for the prediction of 5-year mortality in elderly individuals using chest computed tomography (CT). The methods consist of a classifi?er that performs this prediction using a set of features extracted from the CT image and segmentation maps of multiple anatomic structures. We explore two approaches: 1) a uni?ed framework based on deep learning, where features and classifier are automatically learned in a single optimisation process; and 2) a multi-stage framework based on the design and selection/extraction of hand-crafted radiomics features, followed by the classifier learning process. Experimental results, based on a dataset of 48 annotated chest CTs, show that the deep learning model produces a mean 5-year mortality prediction accuracy of 68.5%, while radiomics produces a mean accuracy that varies between 56% to 66% (depending on the feature selection/extraction method and classifier). The successful development of the proposed models has the potential to make a profound impact in preventive and personalised healthcare. version:1
arxiv-1607-00225 | Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource | http://arxiv.org/abs/1607.00225 | id:1607.00225 author:Stéphan Tulkens, Chris Emmery, Walter Daelemans category:cs.CL  published:2016-07-01 summary:Word embeddings have recently seen a strong increase in interest as a result of strong performance gains on a variety of tasks. However, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language-specific tasks. Still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing for unique observations into language use and variability. In this paper we demonstrate the performance of multiple types of embeddings, created with both count and prediction-based architectures on a variety of corpora, in two language-specific tasks: relation evaluation, and dialect identification. For the latter, we compare unsupervised methods with a traditional, hand-crafted dictionary. With this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task. version:1
arxiv-1607-00215 | Why is Posterior Sampling Better than Optimism for Reinforcement Learning | http://arxiv.org/abs/1607.00215 | id:1607.00215 author:Ian Osband, Benjamin Van Roy category:stat.ML cs.AI cs.LG  published:2016-07-01 summary:Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where $H$ is the horizon, $S$ is the number of states, $A$ is the number of actions and $T$ is the time elapsed. This improves upon the best previous bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. version:1
arxiv-1607-00198 | Sharing Network Parameters for Crosslingual Named Entity Recognition | http://arxiv.org/abs/1607.00198 | id:1607.00198 author:Rudra Murthy V, Mitesh Khapra, Pushpak Bhattacharyya category:cs.CL  published:2016-07-01 summary:Most state of the art approaches for Named Entity Recognition rely on hand crafted features and annotated corpora. Recently Neural network based models have been proposed which do not require handcrafted features but still require annotated corpora. However, such annotated corpora may not be available for many languages. In this paper, we propose a neural network based model which allows sharing the decoder as well as word and character level parameters between two languages thereby allowing a resource fortunate language to aid a resource deprived language. Specifically, we focus on the case when limited annotated corpora is available in one language ($L_1$) and abundant annotated corpora is available in another language ($L_2$). Sharing the network architecture and parameters between $L_1$ and $L_2$ leads to improved performance in $L_1$. Further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. We experiment with 4 language pairs and show that indeed in a resource constrained setup (lesser annotated corpora), a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language. version:1
arxiv-1607-00186 | Throwing fuel on the embers: Probability or Dichotomy, Cognitive or Linguistic? | http://arxiv.org/abs/1607.00186 | id:1607.00186 author:David M. W. Powers category:cs.CL cs.AI  published:2016-07-01 summary:Prof. Robert Berwick's abstract for his forthcoming invited talk at the ACL2016 workshop on Cognitive Aspects of Computational Language Learning revives an ancient debate. Entitled "Why take a chance?", Berwick seems to refer implicitly to Chomsky's critique of the statistical approach of Harris as well as the currently dominant paradigms in CoNLL. Berwick avoids Chomsky's use of "innate" but states that "the debate over the existence of sophisticated mental grammars was settled with Chomsky's Logical Structure of Linguistic Theory (1957/1975)", acknowledging that "this debate has often been revived". This paper agrees with the view that this debate has long since been settled, but with the opposite outcome! Given the embers have not yet died away, and the questions remain fundamental, perhaps it is appropriate to refuel the debate, so I would like to join Bob in throwing fuel on this fire by reviewing the evidence against the Chomskian position! version:1
arxiv-1607-00167 | SentiBubbles: Topic Modeling and Sentiment Visualization of Entity-centric Tweets | http://arxiv.org/abs/1607.00167 | id:1607.00167 author:João Oliveira, Mike Pinto, Pedro Saleiro, Jorge Teixeira category:cs.SI cs.CL cs.IR  published:2016-07-01 summary:Social Media users tend to mention entities when reacting to news events. The main purpose of this work is to create entity-centric aggregations of tweets on a daily basis. By applying topic modeling and sentiment analysis, we create data visualization insights about current events and people reactions to those events from an entity-centric perspective. version:1
arxiv-1607-00146 | Efficient and Consistent Robust Time Series Analysis | http://arxiv.org/abs/1607.00146 | id:1607.00146 author:Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, Purushottam Kar category:cs.LG stat.ML  published:2016-07-01 summary:We study the problem of robust time series analysis under the standard auto-regressive (AR) time series model in the presence of arbitrary outliers. We devise an efficient hard thresholding based algorithm which can obtain a consistent estimate of the optimal AR model despite a large fraction of the time series points being corrupted. Our algorithm alternately estimates the corrupted set of points and the model parameters, and is inspired by recent advances in robust regression and hard-thresholding methods. However, a direct application of existing techniques is hindered by a critical difference in the time-series domain: each point is correlated with all previous points rendering existing tools inapplicable directly. We show how to overcome this hurdle using novel proof techniques. Using our techniques, we are also able to provide the first efficient and provably consistent estimator for the robust regression problem where a standard linear observation model with white additive noise is corrupted arbitrarily. We illustrate our methods on synthetic datasets and show that our methods indeed are able to consistently recover the optimal parameters despite a large fraction of points being corrupted. version:1
arxiv-1607-00137 | Sparse Graphical Representation based Discriminant Analysis for Heterogeneous Face Recognition | http://arxiv.org/abs/1607.00137 | id:1607.00137 author:Chunlei Peng, Xinbo Gao, Nannan Wang, Jie Li category:cs.CV  published:2016-07-01 summary:Face images captured in heterogeneous environments, e.g., sketches generated by the artists or composite-generation software, photos taken by common cameras and infrared images captured by corresponding infrared imaging devices, usually subject to large texture (i.e., style) differences. This results in heavily degraded performance of conventional face recognition methods in comparison with the performance on images captured in homogeneous environments. In this paper, we propose a novel sparse graphical representation based discriminant analysis (SGR-DA) approach to address aforementioned face recognition in heterogeneous scenarios. An adaptive sparse graphical representation scheme is designed to represent heterogeneous face images, where a Markov networks model is constructed to generate adaptive sparse vectors. To handle the complex facial structure and further improve the discriminability, a spatial partition-based discriminant analysis framework is presented to refine the adaptive sparse vectors for face matching. We conducted experiments on six commonly used heterogeneous face datasets and experimental results illustrate that our proposed SGR-DA approach achieves superior performance in comparison with state-of-the-art methods. version:1
arxiv-1607-00136 | Missing Data Estimation in High-Dimensional Datasets: A Swarm Intelligence-Deep Neural Network Approach | http://arxiv.org/abs/1607.00136 | id:1607.00136 author:Collins Leke, Tshilidzi Marwala category:cs.AI cs.LG stat.ML  published:2016-07-01 summary:In this paper, we examine the problem of missing data in high-dimensional datasets by taking into consideration the Missing Completely at Random and Missing at Random mechanisms, as well as theArbitrary missing pattern. Additionally, this paper employs a methodology based on Deep Learning and Swarm Intelligence algorithms in order to provide reliable estimates for missing data. The deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input. This deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset. The investigated methodology in this paper therefore has longer running times, however, the promising potential outcomes justify the trade-off. Also, basic knowledge of statistics is presumed. version:1
arxiv-1607-00133 | Deep Learning with Differential Privacy | http://arxiv.org/abs/1607.00133 | id:1607.00133 author:Martín Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang category:stat.ML cs.CR cs.LG  published:2016-07-01 summary:Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality. version:1
arxiv-1607-00122 | Less-forgetting Learning in Deep Neural Networks | http://arxiv.org/abs/1607.00122 | id:1607.00122 author:Heechul Jung, Jeongwoo Ju, Minju Jung, Junmo Kim category:cs.LG  published:2016-07-01 summary:A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we show the effectiveness of our method using several experiments. Furthermore, we observed that the forgetting problem occurs between mini-batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. We also try to solve this problem using the proposed method. Finally, we show our less-forgetting learning method is also helpful to improve the performance of deep neural networks in terms of recognition rates. version:1
arxiv-1607-00110 | Combining Gradient Boosting Machines with Collective Inference to Predict Continuous Values | http://arxiv.org/abs/1607.00110 | id:1607.00110 author:Iman Alodah, Jennifer Neville category:cs.LG stat.ML  published:2016-07-01 summary:Gradient boosting of regression trees is a competitive procedure for learning predictive models of continuous data that fits the data with an additive non-parametric model. The classic version of gradient boosting assumes that the data is independent and identically distributed. However, relational data with interdependent, linked instances is now common and the dependencies in such data can be exploited to improve predictive performance. Collective inference is one approach to exploit relational correlation patterns and significantly reduce classification error. However, much of the work on collective learning and inference has focused on discrete prediction tasks rather than continuous. %target values has not got that attention in terms of collective inference. In this work, we investigate how to combine these two paradigms together to improve regression in relational domains. Specifically, we propose a boosting algorithm for learning a collective inference model that predicts a continuous target variable. In the algorithm, we learn a basic relational model, collectively infer the target values, and then iteratively learn relational models to predict the residuals. We evaluate our proposed algorithm on a real network dataset and show that it outperforms alternative boosting methods. However, our investigation also revealed that the relational features interact together to produce better predictions. version:1
arxiv-1606-09066 | Making Tree Ensembles Interpretable: A Bayesian Model Selection Approach | http://arxiv.org/abs/1606.09066 | id:1606.09066 author:Satoshi Hara, Kohei Hayashi category:stat.ML  published:2016-06-29 summary:Tree ensembles such as random forests and boosted trees are renowned for their high prediction performance; however, their interpretability is critically limited. One way of interpreting a complex tree ensemble is to obtain its simplified representation, which is formalized as a model selection problem: Given a complex tree ensemble, we want to obtain the simplest representation that is essentially equivalent to the original one. To this end, we derive a Bayesian model selection algorithm. Our approach has three appealing features: the prediction performance is maintained, the coverage is sufficiently large, and the computation is reasonably feasible. Our synthetic data experiment and real world data applications show that complicated tree ensembles are approximated reasonably as interpretable. version:2
arxiv-1607-00101 | Randomized block proximal damped Newton method for composite self-concordant minimization | http://arxiv.org/abs/1607.00101 | id:1607.00101 author:Zhaosong Lu category:math.OC cs.LG cs.NA math.NA stat.CO stat.ML  published:2016-07-01 summary:In this paper we consider the composite self-concordant (CSC) minimization problem, which minimizes the sum of a self-concordant function $f$ and a (possibly nonsmooth) proper closed convex function $g$. The CSC minimization is the cornerstone of the path-following interior point methods for solving a broad class of convex optimization problems. It has also found numerous applications in machine learning. The proximal damped Newton (PDN) methods have been well studied in the literature for solving this problem that enjoy a nice iteration complexity. Given that at each iteration these methods typically require evaluating or accessing the Hessian of $f$ and also need to solve a proximal Newton subproblem, the cost per iteration can be prohibitively high when applied to large-scale problems. Inspired by the recent success of block coordinate descent methods, we propose a randomized block proximal damped Newton (RBPDN) method for solving the CSC minimization. Compared to the PDN methods, the computational cost per iteration of RBPDN is usually significantly lower. The computational experiment on a class of regularized logistic regression problems demonstrate that RBPDN is indeed promising in solving large-scale CSC minimization problems. The convergence of RBPDN is also analyzed in the paper. In particular, we show that RBPDN is globally convergent when $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local linear convergence. Moreover, we show that for a class of $g$ including the case where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear convergence. As a striking consequence, it shows that the classical damped Newton methods [22,40] and the PDN [31] for such $g$ are globally linearly convergent, which was previously unknown in the literature. Moreover, this result can be used to sharpen the existing iteration complexity of these methods. version:1
arxiv-1607-00087 | Fractal Dimension Pattern Based Multiresolution Analysis for Rough Estimator of Person-Dependent Audio Emotion Recognition | http://arxiv.org/abs/1607.00087 | id:1607.00087 author:Miao Cheng category:cs.AI cs.LG cs.SD  published:2016-07-01 summary:As a general means of expression, audio analysis and recognition has attracted much attentions for its wide applications in real-life world. Audio emotion recognition (AER) attempts to understand emotional states of human with the given utterance signals, and has been studied abroad for its further development on friendly human-machine interfaces. Distinguish from other existing works, the person-dependent patterns of audio emotions are conducted, and fractal dimension features are calculated for acoustic feature extraction. Furthermore, it is able to efficiently learn intrinsic characteristics of auditory emotions, while the utterance features are learned from fractal dimensions of each sub-bands. Experimental results show the proposed method is able to provide comparative performance for audio emotion recognition. version:1
arxiv-1607-00084 | Provable Symmetric Nonnegative Matrix Factorization for Overlapping Clustering | http://arxiv.org/abs/1607.00084 | id:1607.00084 author:Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti category:stat.ML  published:2016-07-01 summary:The problem of finding overlapping communities in networks has gained much attention recently. Algorithmic approaches often employ non-negative matrix factorization (NMF) or variants, while model-based approaches (such as the widely used mixed-membership stochastic blockmodel, or MMSB) assume a distribution over communities for each node and run standard inference techniques to recover these parameters. However, few of these approaches have provable consistency guarantees. We investigate the use of the symmetric NMF (or SNMF) for the MMSB model, and provide conditions under which an optimal SNMF algorithm can recover the MMSB parameters consistently. Since we are unaware of general-purpose optimal SNMF algorithms, we develop an SNMF variant, called GeoNMF, designed specifically for the MMSB model. GeoNMF is provably consistent, and experiments on both simulated and real-world datasets show its accuracy. version:1
arxiv-1607-00076 | Multi-class classification: mirror descent approach | http://arxiv.org/abs/1607.00076 | id:1607.00076 author:Daria Reshetova category:math.OC cs.LG stat.ML  published:2016-06-30 summary:We consider the problem of multi-class classification and a stochastic opti- mization approach to it. We derive risk bounds for stochastic mirror descent algorithm and provide examples of set geometries that make the use of the algorithm efficient in terms of error in k. version:1
arxiv-1607-00071 | An Operator Theoretic Approach to Nonparametric Mixture Models | http://arxiv.org/abs/1607.00071 | id:1607.00071 author:Robert A. Vandermeulen, Clayton D. Scott category:stat.ML math.ST stat.TH  published:2016-06-30 summary:When estimating finite mixture models, it is common to make assumptions on the mixture components, such as parametric assumptions. In this work, we make no distributional assumptions on the mixture components and instead assume that observations from the mixture model are grouped, such that observations in the same group are known to be drawn from the same mixture component. We precisely characterize the number of observations $n$ per group needed for the mixture model to be identifiable, as a function of the number $m$ of mixture components. In addition to our assumption-free analysis, we also study the settings where the mixture components are either linearly independent or jointly irreducible. Furthermore, our analysis considers two kinds of identifiability -- where the mixture model is the simplest one explaining the data, and where it is the only one. As an application of these results, we precisely characterize identifiability of multinomial mixture models. Our analysis relies on an operator-theoretic framework that associates mixture models in the grouped-sample setting with certain infinite-dimensional tensors. Based on this framework, we introduce general spectral algorithms for recovering the mixture components and illustrate their use on a synthetic data set. version:1
arxiv-1607-00070 | A Sequence-to-Sequence Model for User Simulation in Spoken Dialogue Systems | http://arxiv.org/abs/1607.00070 | id:1607.00070 author:Layla El Asri, Jing He, Kaheer Suleman category:cs.CL  published:2016-06-30 summary:User simulation is essential for generating enough data to train a statistical spoken dialogue system. Previous models for user simulation suffer from several drawbacks, such as the inability to take dialogue history into account, the need of rigid structure to ensure coherent user behaviour, heavy dependence on a specific domain, the inability to output several user intentions during one dialogue turn, or the requirement of a summarized action space for tractability. This paper introduces a data-driven user simulator based on an encoder-decoder recurrent neural network. The model takes as input a sequence of dialogue contexts and outputs a sequence of dialogue acts corresponding to user intentions. The dialogue contexts include information about the machine acts and the status of the user goal. We show on the Dialogue State Tracking Challenge 2 (DSTC2) dataset that the sequence-to-sequence model outperforms an agenda-based simulator and an n-gram simulator, according to F-score. Furthermore, we show how this model can be used on the original action space and thereby models user behaviour with finer granularity. version:1
arxiv-1607-00067 | Unsupervised Learning with Imbalanced Data via Structure Consolidation Latent Variable Model | http://arxiv.org/abs/1607.00067 | id:1607.00067 author:Fariba Yousefi, Zhenwen Dai, Carl Henrik Ek, Neil Lawrence category:cs.LG stat.ML  published:2016-06-30 summary:Unsupervised learning on imbalanced data is challenging because, when given imbalanced data, current model is often dominated by the major category and ignores the categories with small amount of data. We develop a latent variable model that can cope with imbalanced data by dividing the latent space into a shared space and a private space. Based on Gaussian Process Latent Variable Models, we propose a new kernel formulation that enables the separation of latent space and derives an efficient variational inference method. The performance of our model is demonstrated with an imbalanced medical image dataset. version:1
arxiv-1607-00051 | Geometric Learning and Topological Inference with Biobotic Networks: Convergence Analysis | http://arxiv.org/abs/1607.00051 | id:1607.00051 author:Alireza Dirafzoon, Alper Bozkurt, Edgar Lobaton category:cs.RO math.AT stat.ML  published:2016-06-30 summary:In this study, we present and analyze a framework for geometric and topological estimation for mapping of unknown environments. We consider agents mimicking motion behaviors of cyborg insects, known as biobots, and exploit coordinate-free local interactions among them to infer geometric and topological information about the environment, under minimal sensing and localization constraints. Local interactions are used to create a graphical representation referred to as the encounter graph. A metric is estimated over the encounter graph of the agents in order to construct a geometric point cloud using manifold learning techniques. Topological data analysis (TDA), in particular persistent homology, is used in order to extract topological features of the space and a classification method is proposed to infer robust features of interest (e.g. existence of obstacles). We examine the asymptotic behavior of the proposed metric in terms of the convergence to the geodesic distances in the underlying manifold of the domain, and provide stability analysis results for the topological persistence. The proposed framework and its convergences and stability analysis are demonstrated through numerical simulations and experiments. version:1
arxiv-1607-00036 | Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes | http://arxiv.org/abs/1607.00036 | id:1607.00036 author:Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio category:cs.LG cs.NE  published:2016-06-30 summary:In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both soft, differentiable and hard, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of the Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. version:1
arxiv-1607-00034 | Ballpark Learning: Estimating Labels from Rough Group Comparisons | http://arxiv.org/abs/1607.00034 | id:1607.00034 author:Tom Hope, Dafna Shahaf category:stat.ML cs.LG  published:2016-06-30 summary:We are interested in estimating individual labels given only coarse, aggregated signal over the data points. In our setting, we receive sets ("bags") of unlabeled instances with constraints on label proportions. We relax the unrealistic assumption of known label proportions, made in previous work; instead, we assume only to have upper and lower bounds, and constraints on bag differences. We motivate the problem, propose an intuitive formulation and algorithm, and apply our methods to real-world scenarios. Across several domains, we show how using only proportion constraints and no labeled examples, we can achieve surprisingly high accuracy. In particular, we demonstrate how to predict income level using rough stereotypes and how to perform sentiment analysis using very little information. We also apply our method to guide exploratory analysis, recovering geographical differences in twitter dialect. version:1
arxiv-1607-00030 | HUME: Human UCCA-Based Evaluation of Machine Translation | http://arxiv.org/abs/1607.00030 | id:1607.00030 author:Alexandra Birch, Barry Haddow, Ondrej Bojar, Omri Abend category:cs.CL  published:2016-06-30 summary:Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, providing a more fine-grained analysis of translation quality, and enables the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME's broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores. version:1
arxiv-1606-08793 | Modeling Industrial ADMET Data with Multitask Networks | http://arxiv.org/abs/1606.08793 | id:1606.08793 author:Steven Kearnes, Brian Goldman, Vijay Pande category:stat.ML  published:2016-06-28 summary:Deep learning methods such as multitask neural networks have recently been applied to ligand-based virtual screening and other drug discovery applications. Using a set of industrial ADMET datasets, we compare neural networks to standard baseline models and analyze multitask learning effects with both random cross-validation and a more relevant temporal validation scheme. We confirm that multitask learning can provide modest benefits over single-task models and show that smaller datasets tend to benefit more than larger datasets from multitask learning. Additionally, we find that adding massive amounts of side information is not guaranteed to improve performance relative to simpler multitask learning. Our results emphasize that multitask effects are highly dataset-dependent, suggesting the use of dataset-specific models to maximize overall performance. version:2
arxiv-1607-00021 | The Simulator: An Engine to Streamline Simulations | http://arxiv.org/abs/1607.00021 | id:1607.00021 author:Jacob Bien category:stat.CO stat.ME stat.ML stat.OT  published:2016-06-30 summary:The simulator is an R package that streamlines the process of performing simulations by creating a common infrastructure that can be easily used and reused across projects. Methodological statisticians routinely write simulations to compare their methods to preexisting ones. While developing ideas, there is a temptation to write "quick and dirty" simulations to try out ideas. This approach of rapid prototyping is useful but can sometimes backfire if bugs are introduced. Using the simulator allows one to remove the "dirty" without sacrificing the "quick." Coding is quick because the statistician focuses exclusively on those aspects of the simulation that are specific to the particular paper being written. Code written with the simulator is succinct, highly readable, and easily shared with others. The modular nature of simulations written with the simulator promotes code reusability, which saves time and facilitates reproducibility. The syntax of the simulator leads to simulation code that is easily human-readable. Other benefits of using the simulator include the ability to "step in" to a simulation and change one aspect without having to rerun the entire simulation from scratch, the straightforward integration of parallel computing into simulations, and the ability to rapidly generate plots, tables, and reports with minimal effort. version:1
arxiv-1606-09636 | Mesoscopic representation of texts as complex networks | http://arxiv.org/abs/1606.09636 | id:1606.09636 author:Henrique F. de Arruda, Filipi N. Silva, Vanessa Q. Marinho, Diego R. Amancio, Luciano da F. Costa category:cs.CL  published:2016-06-30 summary:Texts are complex structures emerging from an intricate system consisting of syntactical constraints and semantical relationships. While the complete modeling of such structures is impractical owing to the high level of complexity inherent to linguistic constructions, under a limited domain, certain tasks can still be performed. Recently, statistical techniques aiming at analysis of texts, referred to as text analytics, have departed from the use of simple word count statistics towards a new paradigm. Text mining now hinges on a more sophisticate set of methods, including the representation of texts as complex networks. In this perspective, networks represent a set of textual elements, typically words; and links are established via adjacency relationships. While current word-adjacency (co-occurrence) methods successfully grasp syntactical and stylistic features of written texts, they are unable to represent important aspects of textual data, such as its topical structure. As a consequence, the mesoscopic structure of texts is often overlooked by current methodologies. In order to grasp mesoscopic characteristics of semantical content in written texts, we devised a network approach which is able to analyze documents in a multi-scale, mesoscopic fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content. To illustrate the capabilities of our model, we present, as a use case, a qualitative analysis of "Alice's Adventures in Wonderland", a novel by Lewis Carroll. We show that the mesoscopic structure of documents modeled as networks reveals many semantic traits of texts, a feature that could be explored in a myriad of semantic-based applications. version:1
arxiv-1606-09632 | A Permutation-based Model for Crowd Labeling: Optimal Estimation and Robustness | http://arxiv.org/abs/1606.09632 | id:1606.09632 author:Nihar B. Shah, Sivaraman Balakrishnan, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML  published:2016-06-30 summary:The aggregation and denoising of crowd labeled data is a task that has gained increased significance with the advent of crowdsourcing platforms and massive datasets. In this paper, we propose a permutation-based model for crowd labeled data that is a significant generalization of the common Dawid-Skene model, and introduce a new error metric by which to compare different estimators. Working in a high-dimensional non-asymptotic framework that allows both the number of workers and tasks to scale, we derive optimal rates of convergence for the permutation-based model. We show that the permutation-based model offers significant robustness in estimation due to its richness, while surprisingly incurring only a small additional statistical penalty as compared to the Dawid-Skene model. Finally, we propose a computationally-efficient method, called the OBI-WAN estimator, that is uniformly optimal over a class intermediate between the permutation-based and the Dawid-Skene models, and is uniformly consistent over the entire permutation-based model class. In contrast, the guarantees for estimators available in prior literature are sub-optimal over the original Dawid-Skene model. version:1
arxiv-1606-09604 | SnapToGrid: From Statistical to Interpretable Models for Biomedical Information Extraction | http://arxiv.org/abs/1606.09604 | id:1606.09604 author:Marco A. Valenzuela-Escarcega, Gus Hahn-Powell, Dane Bell, Mihai Surdeanu category:cs.CL  published:2016-06-30 summary:We propose an approach for biomedical information extraction that marries the advantages of machine learning models, e.g., learning directly from data, with the benefits of rule-based approaches, e.g., interpretability. Our approach starts by training a feature-based statistical model, then converts this model to a rule-based variant by converting its features to rules, and "snapping to grid" the feature weights to discrete votes. In doing so, our proposal takes advantage of the large body of work in machine learning, but it produces an interpretable model, which can be directly edited by experts. We evaluate our approach on the BioNLP 2009 event extraction task. Our results show that there is a small performance penalty when converting the statistical model to rules, but the gain in interpretability compensates for that: with minimal effort, human experts improve this model to have similar performance to the statistical model that served as starting point. version:1
arxiv-1606-09600 | Exploring Prediction Uncertainty in Machine Translation Quality Estimation | http://arxiv.org/abs/1606.09600 | id:1606.09600 author:Daniel Beck, Lucia Specia, Trevor Cohn category:cs.CL  published:2016-06-30 summary:Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows. version:1
arxiv-1606-09560 | Neural Network-based Word Alignment through Score Aggregation | http://arxiv.org/abs/1606.09560 | id:1606.09560 author:Joel Legrand, Michael Auli, Ronan Collobert category:cs.CL  published:2016-06-30 summary:We present a simple neural network for word alignment that builds source and target word window representations to compute alignment scores for sentence pairs. To enable unsupervised training, we use an aggregation operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on English-Czech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment. version:1
arxiv-1606-09549 | Fully-Convolutional Siamese Networks for Object Tracking | http://arxiv.org/abs/1606.09549 | id:1606.09549 author:Luca Bertinetto, Jack Valmadre, João F. Henriques, Andrea Vedaldi, Philip H. S. Torr category:cs.CV  published:2016-06-30 summary:The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object's appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 video object detection dataset. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in the VOT2015 benchmark. version:1
arxiv-1606-09518 | SLIC in a defined mask with applications to medical imaging | http://arxiv.org/abs/1606.09518 | id:1606.09518 author:Benjamin Irving category:cs.CV  published:2016-06-30 summary:Supervoxel methods are effective for reducing an image or volume into a set of locally similar regions which has a number of advantages to pixel based methods for segmentation and graph based methods. Simple linear iterative clustering (SLIC) is an effective supervoxel method but is limited to rectangular volumes. In this paper we reformulate the SLIC algorithm to work more effectively in predefined regions-of-interest. The key contribution is the reformulation of the seed point initialisation. This method is applied to an example image with source code and a live demo available. There are a number of applications to computing SLIC inside an mask region including assessment of pathological subregions. version:1
arxiv-1606-09517 | A Model Explanation System: Latest Updates and Extensions | http://arxiv.org/abs/1606.09517 | id:1606.09517 author:Ryan Turner category:stat.ML cs.LG  published:2016-06-30 summary:We propose a general model explanation system (MES) for "explaining" the output of black box classifiers. This paper describes extensions to Turner (2015), which is referred to frequently in the text. We use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. We focus on explaining positive predictions (alerts). However, the presented methodology is symmetrically applicable to negative predictions. version:1
arxiv-1606-09470 | Programming Patterns in Dataflow Matrix Machines and Generalized Recurrent Neural Nets | http://arxiv.org/abs/1606.09470 | id:1606.09470 author:Michael Bukatin, Steve Matthews, Andrey Radul category:cs.PL cs.NE  published:2016-06-30 summary:Dataflow matrix machines arise naturally in the context of synchronous dataflow programming with linear streams. They can be viewed as a rather powerful generalization of recurrent neural networks. Similarly to recurrent neural networks, large classes of dataflow matrix machines are described by matrices of numbers, and therefore dataflow matrix machines can be synthesized by computing their matrices. At the same time, the evidence is fairly strong that dataflow matrix machines have sufficient expressive power to be a convenient general-purpose programming platform. Because of the network nature of this platform, programming patterns often correspond to patterns of connectivity in the generalized recurrent neural networks understood as programs. This paper explores a variety of such programming patterns. version:1
arxiv-1606-09458 | Vote-boosting ensembles | http://arxiv.org/abs/1606.09458 | id:1606.09458 author:Maryam Sabzevari, Gonzalo Martínez-Muñoz, Alberto Suárez category:cs.LG stat.ML  published:2016-06-30 summary:Vote-boosting is a sequential ensemble learning method in which individual classifiers are built on different weighted versions of the training data. To build a new classifier, the weight of each training instance is determined as a function of the disagreement rate of the current ensemble predictions for that particular instance. Experiments using the symmetric beta distribution as the emphasis function and different base learners are used to illustrate the properties and to analyze the performance of these types of ensembles. In classification problems with low or no class-label noise, when simple base learners are used, vote-boosting behaves as if it were an interpolation between bagging and standard boosting (e.g. AdaBoost), depending on the value of the shape parameter of the beta distribution. In terms of predictive accuracy the best results, which are comparable or better than random forests, are obtained with vote-boosting ensembles of random trees. version:1
arxiv-1606-09433 | Steering a Predator Robot using a Mixed Frame/Event-Driven Convolutional Neural Network | http://arxiv.org/abs/1606.09433 | id:1606.09433 author:Diederik Paul Moeys, Federico Corradi, Emmett Kerr, Philip Vance, Gautham Das, Daniel Neil, Dermot Kerr, Tobi Delbruck category:cs.RO cs.CV  published:2016-06-30 summary:This paper describes the application of a Convolutional Neural Network (CNN) in the context of a predator/prey scenario. The CNN is trained and run on data from a Dynamic and Active Pixel Sensor (DAVIS) mounted on a Summit XL robot (the predator), which follows another one (the prey). The CNN is driven by both conventional image frames and dynamic vision sensor "frames" that consist of a constant number of DAVIS ON and OFF events. The network is thus "data driven" at a sample rate proportional to the scene activity, so the effective sample rate varies from 15 Hz to 240 Hz depending on the robot speeds. The network generates four outputs: steer right, left, center and non-visible. After off-line training on labeled data, the network is imported on the on-board Summit XL robot which runs jAER and receives steering directions in real time. Successful results on closed-loop trials, with accuracies up to 87% or 92% (depending on evaluation criteria) are reported. Although the proposed approach discards the precise DAVIS event timing, it offers the significant advantage of compatibility with conventional deep learning technology without giving up the advantage of data-driven computing. version:1
arxiv-1606-09403 | Learning Crosslingual Word Embeddings without Bilingual Corpora | http://arxiv.org/abs/1606.09403 | id:1606.09403 author:Long Duong, Hiroshi Kanayama, Tengfei Ma, Steven Bird, Trevor Cohn category:cs.CL cs.AI  published:2016-06-30 summary:Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools. However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-the-art performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. version:1
arxiv-1606-09388 | Asymptotically Optimal Algorithms for Multiple Play Bandits with Partial Feedback | http://arxiv.org/abs/1606.09388 | id:1606.09388 author:Alexander Luedtke, Emilie Kaufmann, Antoine Chambaz category:stat.ML cs.LG  published:2016-06-30 summary:We study a variant of the multi-armed bandit problem with multiple plays in which the user wishes to sample the m out of k arms with the highest expected rewards, but at any given time can only sample l $\le$ m arms. When l = m, Thompson sampling was recently shown to be asymptotically efficient. We derive an asymptotic regret lower bound for any uniformly efficient algorithm in our new setting where may be less than m. We then establish the asymptotic optimality of Thompson sampling for Bernoulli rewards, where our proof technique differs from earlier methods even when l = m. We also prove the asymptotic optimality of an algorithm based on upper confidence bounds, KL-CUCB, for single-parameter exponential families and bounded, finitely supported rewards, a result which is new for all values of l. version:1
arxiv-1606-09383 | On Approximate Dynamic Programming with Multivariate Splines for Adaptive Control | http://arxiv.org/abs/1606.09383 | id:1606.09383 author:Willem Eerland, Coen de Visser, Erik-Jan van Kampen category:cs.LG cs.SY  published:2016-06-30 summary:We define a SDP framework based on the RLSTD algorithm and multivariate simplex B-splines. We introduce a local forget factor capable of preserving the continuity of the simplex splines. This local forget factor is integrated with the RLSTD algorithm, resulting in a modified RLSTD algorithm that is capable of tracking time-varying systems. We present the results of two numerical experiments, one validating SDP and comparing it with NDP and another to show the advantages of the modified RLSTD algorithm over the original. While SDP requires more computations per time-step, the experiment shows that for the same amount of function approximator parameters, there is an increase in performance in terms of stability and learning rate compared to NDP. The second experiment shows that SDP in combination with the modified RLSTD algorithm allows for faster recovery compared to the original RLSTD algorithm when system parameters are altered, paving the way for an adaptive high-performance non-linear control method. version:1
arxiv-1606-09375 | Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering | http://arxiv.org/abs/1606.09375 | id:1606.09375 author:Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst category:cs.LG stat.ML  published:2016-06-30 summary:Convolutional neural networks (CNNs) have greatly improved state-of-the-art performances in a number of fields, notably computer vision and natural language processing. In this work, we are interested in generalizing the formulation of CNNs from low-dimensional regular Euclidean domains, where images (2D), videos (3D) and audios (1D) are represented, to high-dimensional irregular domains such as social networks or biological networks represented by graphs. This paper introduces a formulation of CNNs on graphs in the context of spectral graph theory. We borrow the fundamental tools from the emerging field of signal processing on graphs, which provides the necessary mathematical background and efficient numerical schemes to design localized graph filters efficient to learn and evaluate. As a matter of fact, we introduce the first technique that offers the same computational complexity than standard CNNs, while being universal to any graph structure. Numerical experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs, as long as the graph is well-constructed. version:1
arxiv-1606-09371 | Recurrent neural network models for disease name recognition using domain invariant features | http://arxiv.org/abs/1606.09371 | id:1606.09371 author:Sunil Kumar Sahu, Ashish Anand category:cs.CL  published:2016-06-30 summary:Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names. version:1
arxiv-1606-09370 | Relation extraction from clinical texts using domain invariant convolutional neural network | http://arxiv.org/abs/1606.09370 | id:1606.09370 author:Sunil Kumar Sahu, Ashish Anand, Krishnadev Oruganty, Mahanandeeshwar Gattu category:cs.CL  published:2016-06-30 summary:In recent years extracting relevant information from biomedical and clinical texts such as research articles, discharge summaries, or electronic health records have been a subject of many research efforts and shared challenges. Relation extraction is the process of detecting and classifying the semantic relation among entities in a given piece of texts. Existing models for this task in biomedical domain use either manually engineered features or kernel methods to create feature vector. These features are then fed to classifier for the prediction of the correct class. It turns out that the results of these methods are highly dependent on quality of user designed features and also suffer from curse of dimensionality. In this work we focus on extracting relations from clinical discharge summaries. Our main objective is to exploit the power of convolution neural network (CNN) to learn features automatically and thus reduce the dependency on manual feature engineering. We evaluate performance of the proposed model on i2b2-2010 clinical relation extraction challenge dataset. Our results indicate that convolution neural network can be a good model for relation exaction in clinical text without being dependent on expert's knowledge on defining quality features. version:1
arxiv-1606-09367 | Parking Stall Vacancy Indicator System Based on Deep Convolutional Neural Networks | http://arxiv.org/abs/1606.09367 | id:1606.09367 author:Sepehr Valipour, Mennatullah Siam, Eleni Stroulia, Martin Jagersand category:cs.CV  published:2016-06-30 summary:Parking management systems, and vacancy-indication services in particular, can play a valuable role in reducing traffic and energy waste in large cities. Visual detection methods represent a cost-effective option, since they can take advantage of hardware usually already available in many parking lots, namely cameras. However, visual detection methods can be fragile and not easily generalizable. In this paper, we present a robust detection algorithm based on deep convolutional neural networks. We implemented and tested our algorithm on a large baseline dataset, and also on a set of image feeds from actual cameras already installed in parking lots. We have developed a fully functional system, from server-side image analysis to front-end user interface, to demonstrate the practicality of our method. version:1
arxiv-1606-09349 | Zero-Shot Learning with Multi-Battery Factor Analysis | http://arxiv.org/abs/1606.09349 | id:1606.09349 author:Zhong Ji, Yuzhong Xie, Yanwei Pang, Lei Chen, Zhongfei Zhang category:cs.CV  published:2016-06-30 summary:Zero-shot learning (ZSL) extends the conventional image classification technique to a more challenging situation where the test image categories are not seen in the training samples. Most studies on ZSL utilize side information such as attributes or word vectors to bridge the relations between the seen classes and the unseen classes. However, existing approaches on ZSL typically exploit a shared space for each type of side information independently, which cannot make full use of the complementary knowledge of different types of side information. To this end, this paper presents an MBFA-ZSL approach to embed different types of side information as well as the visual feature into one shared space. Specifically, we first develop an algorithm named Multi-Battery Factor Analysis (MBFA) to build a unified semantic space, and then employ multiple types of side information in it to achieve the ZSL. The close-form solution makes MBFA-ZSL simple to implement and efficient to run on large datasets. Extensive experiments on the popular AwA, CUB, and SUN datasets show its significant superiority over the state-of-the-art approaches. version:1
arxiv-1606-09333 | Dimension-Free Iteration Complexity of Finite Sum Optimization Problems | http://arxiv.org/abs/1606.09333 | id:1606.09333 author:Yossi Arjevani, Ohad Shamir category:math.OC cs.LG math.NA  published:2016-06-30 summary:Many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure. However, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than $\mathcal{O}(d/n)$ (where $d$ is the dimension and $n$ is the number of samples). In this work, we extend the framework of (Arjevani et al., 2015) to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA. version:1
arxiv-1606-09282 | Learning without Forgetting | http://arxiv.org/abs/1606.09282 | id:1606.09282 author:Zhizhong Li, Derek Hoiem category:cs.CV cs.LG stat.ML  published:2016-06-29 summary:When building a unified vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning as standard practice for improved new task performance. version:1
arxiv-1606-09281 | Multiphase Segmentation For Simultaneously Homogeneous and Textural Images | http://arxiv.org/abs/1606.09281 | id:1606.09281 author:Duy Hoang Thai, Lucas Mentch category:cs.CV  published:2016-06-29 summary:Segmentation remains an important problem in image processing. For homogeneous (piecewise smooth) images, a number of important models have been developed and refined over the past several decades. However, these models often fail when applied to the substantially larger class of natural images that simultaneously contain regions of both texture and homogeneity. This work introduces a bi-level constrained minimization model for simultaneous multiphase segmentation of images containing both homogeneous and textural regions. We develop novel norms defined in different functional Banach spaces for the segmentation which results in a non-convex minimization. Finally, we develop a generalized notion of segmentation delving into approximation theory and demonstrating that a more refined decomposition of these images results in multiple meaningful components. Both theoretical results and demonstrations on natural images are provided. version:1
arxiv-1606-09274 | Compression of Neural Machine Translation Models via Pruning | http://arxiv.org/abs/1606.09274 | id:1606.09274 author:Abigail See, Minh-Thang Luong, Christopher D. Manning category:cs.AI cs.CL cs.NE  published:2016-06-29 summary:Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model. version:1
arxiv-1606-09264 | How smart does your profile image look? Intelligence estimation from social network profile images | http://arxiv.org/abs/1606.09264 | id:1606.09264 author:Xingjie Wei, David Stillwell category:cs.CV cs.MM cs.SI  published:2016-06-29 summary:Profile pictures on social networks are users' opportunity to present themselves and to affect how others judge them. In most social networks, profile pictures are public by default. 1,122 Facebook users completed a matrices intelligence test and shared their current Facebook profile picture. Strangers also rated the images for perceived intelligence. We use automatically extracted features from profile pictures to predict both measured and perceived intelligence. Intelligence estimation from images is a difficult task even for humans, but experimental results show that human accuracy can be equalled using computing methods. We report the image features that predict both measured and perceived intelligence, and highlight misleading features such as "smiling" and "wearing glasses" that are correlated with perceived but not measured intelligence. Our results give insights into inaccurate stereotyping from profile pictures and also have implications for privacy. version:1
arxiv-1606-09239 | Learning Concept Taxonomies from Multi-modal Data | http://arxiv.org/abs/1606.09239 | id:1606.09239 author:Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan, Eric P. Xing category:cs.CL cs.CV cs.LG  published:2016-06-29 summary:We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap. version:1
arxiv-1606-09222 | Penambahan emosi menggunakan metode manipulasi prosodi untuk sistem text to speech bahasa Indonesia | http://arxiv.org/abs/1606.09222 | id:1606.09222 author:Salita Ulitia Prini, Ary Setijadi Prihatmanto category:cs.SD cs.CL cs.RO I.2.7; H.5.2  published:2016-06-29 summary:Adding an emotions using prosody manipulation method for Indonesian text to speech system. Text To Speech (TTS) is a system that can convert text in one language into speech, accordance with the reading of the text in the language used. The focus of this research is a natural sounding concept, the make "humanize" for the pronunciation of voice synthesis system Text To Speech. Humans have emotions / intonation that may affect the sound produced. The main requirement for the system used Text To Speech in this research is eSpeak, the database MBROLA using id1, Human Speech Corpus database from a website that summarizes the words with the highest frequency (Most Common Words) used in a country. And there are 3 types of emotional / intonation designed base. There is a happy, angry and sad emotion. Method for develop the emotional filter is manipulate the relevant features of prosody (especially pitch and duration value) using a predetermined rate factor that has been established by analyzing the differences between the standard output Text To Speech and voice recording with emotional prosody / a particular intonation. The test results for the perception tests of Human Speech Corpus for happy emotion is 95 %, 96.25 % for angry emotion and 98.75 % for sad emotions. For perception test system carried by intelligibility and naturalness test. Intelligibility test for the accuracy of sound with the original sentence is 93.3%, and for clarity rate for each sentence is 62.8%. For naturalness, accuracy emotional election amounted to 75.6 % for happy emotion, 73.3 % for angry emotion, and 60 % for sad emotions. ----- Text To Speech (TTS) merupakan suatu sistem yang dapat mengonversi teks dalam format suatu bahasa menjadi ucapan sesuai dengan pembacaan teks dalam bahasa yang digunakan. version:1
arxiv-1606-09202 | Tighter bounds lead to improved classifiers | http://arxiv.org/abs/1606.09202 | id:1606.09202 author:Nicolas Le Roux category:cs.LG stat.ML  published:2016-06-29 summary:The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints. version:1
arxiv-1606-09197 | Model-free Trajectory Optimization for Reinforcement Learning | http://arxiv.org/abs/1606.09197 | id:1606.09197 author:Riad Akrour, Abbas Abdolmaleki, Hany Abdulsamad, Gerhard Neumann category:cs.LG cs.RO  published:2016-06-29 summary:Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics. version:1
arxiv-1606-09193 | Small coherence implies the weak Null Space Property | http://arxiv.org/abs/1606.09193 | id:1606.09193 author:Stéphane Chrétien, Zhen Wai Olivier Ho category:math.ST stat.ML stat.TH  published:2016-06-29 summary:In the Compressed Sensing community, it is well known that given a matrix $X \in \mathbb R^{n\times p}$ with $\ell_2$ normalized columns, the Restricted Isometry Property (RIP) implies the Null Space Property (NSP). It is also well known that a small Coherence $\mu$ implies a weak RIP, i.e. the singular values of $X_T$ lie between $1-\delta$ and $1+\delta$ for "most" index subsets $T \subset \{1,\ldots,p\}$ with size governed by $\mu$ and $\delta$. In this short note, we show that a small Coherence implies a weak Null Space Property, i.e. $\Vert h_T\Vert_2 \le C \ \Vert h_{T^c}\Vert_1/\sqrt{s}$ for most $T \subset \{1,\ldots,p\}$ with cardinality $ T \le s$. We moreover prove some singular value perturbation bounds that may also prove useful for other applications. version:1
arxiv-1606-09190 | A Semi-Definite Programming approach to low dimensional embedding for unsupervised clustering | http://arxiv.org/abs/1606.09190 | id:1606.09190 author:Stéphane Chrétien, Clément Dombry, Adrien Faivre category:stat.ML cs.LG  published:2016-06-29 summary:This paper proposes a variant of the method of Gu\'edon and Verhynin for estimating the cluster matrix in the Mixture of Gaussians framework via Semi-Definite Programming. A clustering oriented embedding is deduced from this estimate. The procedure is suitable for very high dimensional data because it is based on pairwise distances only. Theoretical garantees are provided and an eigenvalue optimisation approach is proposed for computing the embedding. The performance of the method is illustrated via Monte Carlo experiements and comparisons with other embeddings from the literature. version:1
arxiv-1606-09184 | Disease Trajectory Maps | http://arxiv.org/abs/1606.09184 | id:1606.09184 author:Peter Schulam, Raman Arora category:stat.ML cs.LG stat.AP  published:2016-06-29 summary:Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Time series data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of time series. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled time series. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes. version:1
arxiv-1606-09163 | Optimising The Input Window Alignment in CD-DNN Based Phoneme Recognition for Low Latency Processing | http://arxiv.org/abs/1606.09163 | id:1606.09163 author:Akash Kumar Dhaka, Giampiero Salvi category:cs.CL cs.CV cs.NE stat.ML  published:2016-06-29 summary:We present a systematic analysis on the performance of a phonetic recogniser when the window of input features is not symmetric with respect to the current frame. The recogniser is based on Context Dependent Deep Neural Networks (CD-DNNs) and Hidden Markov Models (HMMs). The objective is to reduce the latency of the system by reducing the number of future feature frames required to estimate the current output. Our tests performed on the TIMIT database show that the performance does not degrade when the input window is shifted up to 5 frames in the past compared to common practice (no future frame). This corresponds to improving the latency by 50 ms in our settings. Our tests also show that the best results are not obtained with the symmetric window commonly employed, but with an asymmetric window with eight past and two future context frames, although this observation should be confirmed on other data sets. The reduction in latency suggested by our results is critical for specific applications such as real-time lip synchronisation for tele-presence, but may also be beneficial in general applications to improve the lag in human-machine spoken interaction. version:1
arxiv-1606-09155 | Accelerated first-order primal-dual proximal methods for linearly constrained composite convex programming | http://arxiv.org/abs/1606.09155 | id:1606.09155 author:Yangyang Xu category:math.OC cs.NA stat.ML  published:2016-06-29 summary:Motivated by big data applications, first-order methods have been extremely popular in recent years. However, naive gradient methods generally converge slowly. Hence, much efforts have been made to accelerate various first-order methods. This paper proposes two accelerated methods towards solving structured linearly constrained convex programming, for which we assume composite convex objective. The first method is the accelerated linearized augmented Lagrangian method (LALM). At each update to the primal variable, it allows linearization to the differentiable function and also the augmented term, and thus it enables easy subproblems. Assuming merely weak convexity, we show that LALM owns $O(1/t)$ convergence if parameters are kept fixed during all the iterations and can be accelerated to $O(1/t^2)$ if the parameters are adapted, where $t$ is the number of total iterations. The second method is the accelerated linearized alternating direction method of multipliers (LADMM). In addition to the composite convexity, it further assumes two-block structure on the objective. Different from classic ADMM, our method allows linearization to the objective and also augmented term to make the update simple. Assuming strong convexity on one block variable, we show that LADMM also enjoys $O(1/t^2)$ convergence with adaptive parameters. This result is a significant improvement over that in [Goldstein et. al, SIIMS'14], which requires strong convexity on both block variables and no linearization to the objective or augmented term. Numerical experiments are performed on quadratic programming, image denoising, and support vector machine. The proposed accelerated methods are compared to nonaccelerated ones and also existing accelerated methods. The results demonstrate the validness of acceleration and superior performance of the proposed methods over existing ones. version:1
arxiv-1606-09152 | Actor-critic versus direct policy search: a comparison based on sample complexity | http://arxiv.org/abs/1606.09152 | id:1606.09152 author:Arnaud de Froissard de Broissia, Olivier Sigaud category:cs.LG  published:2016-06-29 summary:Sample efficiency is a critical property when optimizing policy parameters for the controller of a robot. In this paper, we evaluate two state-of-the-art policy optimization algorithms. One is a recent deep reinforcement learning method based on an actor-critic algorithm, Deep Deterministic Policy Gradient (DDPG), that has been shown to perform well on various control benchmarks. The other one is a direct policy search method, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a black-box optimization method that is widely used for robot learning. The algorithms are evaluated on a continuous version of the mountain car benchmark problem, so as to compare their sample complexity. From a preliminary analysis, we expect DDPG to be more sample efficient than CMA-ES, which is confirmed by our experimental results. version:1
arxiv-1606-09118 | A spectral-spatial fusion model for robust blood pulse waveform extraction in photoplethysmographic imaging | http://arxiv.org/abs/1606.09118 | id:1606.09118 author:Robert Amelard, David A Clausi, Alexander Wong category:cs.CV physics.optics  published:2016-06-29 summary:Photoplethysmographic imaging is a camera-based solution for non-contact cardiovascular monitoring from a distance. This technology enables monitoring in situations where contact-based devices may be problematic or infeasible, such as ambulatory, sleep, and multi-individual monitoring. However, extracting the blood pulse waveform signal is challenging due to the unknown mixture of relevant (pulsatile) and irrelevant pixels in the scene. Here, we design and implement a signal fusion framework, FusionPPG, for extracting a blood pulse waveform signal with strong temporal fidelity from a scene without requiring anatomical priors (e.g., facial tracking). The extraction problem is posed as a Bayesian least squares fusion problem, and solved using a novel probabilistic pulsatility model that incorporates both physiologically derived spectral and spatial waveform priors to identify pulsatility characteristics in the scene. Experimental results show statistically significantly improvements compared to the FaceMeanPPG method ($p<0.001$) and DistancePPG ($p<0.001$) methods. Heart rates predicted using FusionPPG correlated strongly with ground truth measurements ($r^2=0.9952$). FusionPPG was the only method able to assess cardiac arrhythmia via temporal analysis. version:1
arxiv-1606-09072 | Resolution- and throughput-enhanced spectroscopy using high-throughput computational slit | http://arxiv.org/abs/1606.09072 | id:1606.09072 author:Farnoud Kazemzadeh, Alexander Wong category:physics.optics cs.CV  published:2016-06-29 summary:There exists a fundamental tradeoff between spectral resolution and the efficiency or throughput for all optical spectrometers. The primary factors affecting the spectral resolution and throughput of an optical spectrometer are the size of the entrance aperture and the optical power of the focusing element. Thus far collective optimization of the above mentioned has proven difficult. Here, we introduce the concept of high-throughput computational slits (HTCS) for improving both the effective spectral resolution and efficiency of a spectrometer. The proposed HTCS approach was experimentally validated using an optical spectrometer configured with a 200 um entrance aperture, test, and a 50 um entrance aperture, control, demonstrating improvements in spectral resolution of the spectrum by ~20% over the control spectral resolution and improvements in efficiency of > 2 times the efficiency of the largest entrance aperture used in the study while producing highly accurate spectra. version:1
arxiv-1606-09058 | A Distributional Semantics Approach to Implicit Language Learning | http://arxiv.org/abs/1606.09058 | id:1606.09058 author:Dimitrios Alikaniotis, John N. Williams category:cs.CL cs.LG I.5.1; I.2.6; I.2.7  published:2016-06-29 summary:In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a Vector-Space model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans. version:1
arxiv-1606-08660 | Theory reconstruction: a representation learning view on predicate invention | http://arxiv.org/abs/1606.08660 | id:1606.08660 author:Sebastijan Dumancic, Wannes Meert, Hendrik Blockeel category:stat.ML cs.LG cs.LO  published:2016-06-28 summary:With this positional paper we present a representation learning view on predicate invention. The intention of this proposal is to bridge the relational and deep learning communities on the problem of predicate invention. We propose a theory reconstruction approach, a formalism that extends autoencoder approach to representation learning to the relational settings. Our intention is to start a discussion to define a unifying framework for predicate invention and theory revision. version:2
arxiv-1606-09029 | Geometry in Active Learning for Binary and Multi-class Image Segmentation | http://arxiv.org/abs/1606.09029 | id:1606.09029 author:Ksenia Konyushkova, Raphael Sznitman, Pascal Fua category:cs.CV  published:2016-06-29 summary:We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in both background-foreground and multi-class segmentation tasks that work in 2D and 3D image volumes. To this end, we use smoothness priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. We evaluate our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images of horses and faces. We demonstrate a marked performance increase over state-of-the-art approaches. version:1
arxiv-1606-08805 | Theta-RBM: Unfactored Gated Restricted Boltzmann Machine for Rotation-Invariant Representations | http://arxiv.org/abs/1606.08805 | id:1606.08805 author:Mario Valerio Giuffrida, Sotirios A. Tsaftaris category:cs.CV  published:2016-06-28 summary:Learning invariant representations is a critical task in computer vision. In this paper, we propose the Theta-Restricted Boltzmann Machine ({\theta}-RBM in short), which builds upon the original RBM formulation and injects the notion of rotation-invariance during the learning procedure. In contrast to previous approaches, we do not transform the training set with all possible rotations. Instead, we rotate the gradient filters when they are computed during the Contrastive Divergence algorithm. We formulate our model as an unfactored gated Boltzmann machine, where another input layer is used to modulate the input visible layer to drive the optimisation procedure. Among our contributions is a mathematical proof that demonstrates that {\theta}-RBM is able to learn rotation-invariant features according to a recently proposed invariance measure. Our method reaches an invariance score of ~90% on mnist-rot dataset, which is the highest result compared with the baseline methods and the current state of the art in transformation-invariant feature learning in RBM. Using an SVM classifier, we also showed that our network learns discriminative features as well, obtaining ~10% of testing error. version:2
arxiv-1606-09022 | Decision making via semi-supervised machine learning techniques | http://arxiv.org/abs/1606.09022 | id:1606.09022 author:Eftychios Protopapadakis category:cs.LG  published:2016-06-29 summary:Semi-supervised learning (SSL) is a class of supervised learning tasks and techniques that also exploits the unlabeled data for training. SSL significantly reduces labeling related costs and is able to handle large data sets. The primary objective is the extraction of robust inference rules. Decision support systems (DSSs) who utilize SSL have significant advantages. Only a small amount of labelled data is required for the initialization. Then, new (unlabeled) data can be utilized and improve system's performance. Thus, the DSS is continuously adopted to new conditions, with minimum effort. Techniques which are cost effective and easily adopted to dynamic systems, can be beneficial for many practical applications. Such applications fields are: (a) industrial assembly lines monitoring, (b) sea border surveillance, (c) elders' falls detection, (d) transportation tunnels inspection, (e) concrete foundation piles defect recognition, (f) commercial sector companies financial assessment and (g) image advanced filtering for cultural heritage applications. version:1
arxiv-1606-08999 | De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile Visual Search | http://arxiv.org/abs/1606.08999 | id:1606.08999 author:Yin-Hsi Kuo, Winston H. Hsu category:cs.MM cs.CV  published:2016-06-29 summary:Due to the prevalence of mobile devices, mobile search becomes a more convenient way than desktop search. Different from the traditional desktop search, mobile visual search needs more consideration for the limited resources on mobile devices (e.g., bandwidth, computing power, and memory consumption). The state-of-the-art approaches show that bag-of-words (BoW) model is robust for image and video retrieval; however, the large vocabulary tree might not be able to be loaded on the mobile device. We observe that recent works mainly focus on designing compact feature representations on mobile devices for bandwidth-limited network (e.g., 3G) and directly adopt feature matching on remote servers (cloud). However, the compact (binary) representation might fail to retrieve target objects (images, videos). Based on the hashed binary codes, we propose a de-hashing process that reconstructs BoW by leveraging the computing power of remote servers. To mitigate the information loss from binary codes, we further utilize contextual information (e.g., GPS) to reconstruct a context-aware BoW for better retrieval results. Experiment results show that the proposed method can achieve competitive retrieval accuracy as BoW while only transmitting few bits from mobile devices. version:1
arxiv-1606-08658 | Unsupervised Relational Representation Learning via Clustering: Preliminary Results | http://arxiv.org/abs/1606.08658 | id:1606.08658 author:Sebastijan Dumancic, Hendrik Blockeel category:stat.ML cs.LG  published:2016-06-28 summary:The goal of unsupervised representation learning methods is to learn a new representation of the original data, such that it makes a certain classification task easier to solve. Since their introduction in late 2000s, these methods initiated a revolution within machine learning. In this paper we present an unsupervised representation learning method for relational data. The proposed approach uses a clustering procedure to learn a new representation. Moreover, we introduce an adaptive clustering method, capable of addressing multiple interpretations of similarity in relational context. Finally, we experimentally evaluate the proposed approach. The preliminary results show the promise of the approach, as the models learned on the new representation often achieve better performance and are less complex than the ones learned on the original data representation. version:2
arxiv-1606-08963 | Non-linear Label Ranking for Large-scale Prediction of Long-Term User Interests | http://arxiv.org/abs/1606.08963 | id:1606.08963 author:Nemanja Djuric, Mihajlo Grbovic, Vladan Radosavljevic, Narayan Bhamidipati, Slobodan Vucetic category:cs.AI cs.LG stat.ML  published:2016-06-29 summary:We consider the problem of personalization of online services from the viewpoint of ad targeting, where we seek to find the best ad categories to be shown to each user, resulting in improved user experience and increased advertisers' revenue. We propose to address this problem as a task of ranking the ad categories depending on a user's preference, and introduce a novel label ranking approach capable of efficiently learning non-linear, highly accurate models in large-scale settings. Experiments on a real-world advertising data set with more than 3.2 million users show that the proposed algorithm outperforms the existing solutions in terms of both rank loss and top-K retrieval performance, strongly suggesting the benefit of using the proposed model on large-scale ranking problems. version:1
arxiv-1606-08957 | Alternating Estimation for Structured High-Dimensional Multi-Response Models | http://arxiv.org/abs/1606.08957 | id:1606.08957 author:Sheng Chen, Arindam Banerjee category:stat.ML  published:2016-06-29 summary:We consider learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among responses, we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector. Under suitable sample size and resampling assumptions, we show that the error of the estimates generated by AltEst, with high probability, converges linearly to certain minimum achievable level, which can be tersely expressed by a few geometric measures, such as Gaussian width of sets related to the parameter structure. To the best of our knowledge, this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation problem with general structures. version:1
arxiv-1606-08954 | Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs | http://arxiv.org/abs/1606.08954 | id:1606.08954 author:Swabha Swayamdipta, Miguel Ballesteros, Chris Dyer, Noah A. Smith category:cs.CL cs.AI  published:2016-06-29 summary:We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics. version:1
arxiv-1606-08928 | subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs | http://arxiv.org/abs/1606.08928 | id:1606.08928 author:Annamalai Narayanan, Mahinthan Chandramohan, Lihui Chen, Yang Liu, Santhoshkumar Saminathan category:cs.LG cs.AI cs.CR cs.SE  published:2016-06-29 summary:In this paper, we present subgraph2vec, a novel approach for learning latent representations of rooted subgraphs from large graphs inspired by recent advancements in Deep Learning and Graph Kernels. These latent representations encode semantic substructure dependencies in a continuous vector space, which is easily exploited by statistical models for tasks such as graph classification, clustering, link prediction and community detection. subgraph2vec leverages on local information obtained from neighbourhoods of nodes to learn their latent representations in an unsupervised fashion. We demonstrate that subgraph vectors learnt by our approach could be used in conjunction with classifiers such as CNNs, SVMs and relational data clustering algorithms to achieve significantly superior accuracies. Also, we show that the subgraph vectors could be used for building a deep learning variant of Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and large-scale real-world datasets reveal that subgraph2vec achieves significant improvements in accuracies over existing graph kernels on both supervised and unsupervised learning tasks. Specifically, on two realworld program analysis tasks, namely, code clone and malware detection, subgraph2vec outperforms state-of-the-art kernels by more than 17% and 4%, respectively. version:1
arxiv-1606-08921 | Image Restoration Using Convolutional Auto-encoders with Symmetric Skip Connections | http://arxiv.org/abs/1606.08921 | id:1606.08921 author:Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang category:cs.CV  published:2016-06-29 summary:Image restoration, including image denoising, super resolution, inpainting, and so on, is a well-studied problem in computer vision and image processing, as well as a test bed for low-level image modeling algorithms. In this work, we propose a very deep fully convolutional auto-encoder network for image restoration, which is a encoding-decoding framework with symmetric convolutional-deconvolutional layers. In other words, the network is composed of multiple layers of convolution and de-convolution operators, learning end-to-end mappings from corrupted images to the original ones. The convolutional layers capture the abstraction of image contents while eliminating corruptions. Deconvolutional layers have the capability to upsample the feature maps and recover the image details. To deal with the problem that deeper networks tend to be more difficult to train, we propose to symmetrically link convolutional and deconvolutional layers with skip-layer connections, with which the training converges much faster and attains better results. version:1
arxiv-1606-08920 | Exact Lower Bounds for the Agnostic Probably-Approximately-Correct (PAC) Machine Learning Model | http://arxiv.org/abs/1606.08920 | id:1606.08920 author:Aryeh Kontorovich, Iosif Pinelis category:cs.LG math.PR math.ST stat.TH  published:2016-06-29 summary:We provide an exact asymptotic lower bound on the minimax expected excess risk (EER) in the agnostic probably-approximately-correct (PAC) machine learning classification model. This bound is of the simple form $c_\infty/\sqrt{\nu}$ as $\nu\to\infty$, where $c_\infty=0.16997\dots$ is a universal constant, $\nu=m/d$, $m$ is the size of the training sample, and $d$ is the Vapnik--Chervonenkis dimension of the hypothesis class. In the case when randomization of learning algorithms is allowed, we also provide an exact non-asymptotic lower bound on the minimax EER and identify minimax learning algorithms as certain maximally symmetric and minimally randomized "voting" procedures. It is shown that the differences between these asymptotic and non-asymptotic bounds, as well as the differences between these two bounds and the maximum EER of any learning algorithms that minimize the empirical risk, are asymptotically negligible, and all these differences are due to ties in the mentioned "voting" procedures. A few easy to compute non-asymptotic lower bounds on the minimax EER are also obtained, which are shown to be close to the exact asymptotic lower bound $c_\infty/\sqrt{\nu}$ even for rather small values of the ratio $\nu=m/d$. As an application of these results, we substantially improve existing lower bounds on the tail probability of the excess risk. Among the tools used are Bayes estimation and apparently new identities and inequalities for binomial distributions. version:1
arxiv-1606-08883 | Defending Non-Bayesian Learning against Adversarial Attacks | http://arxiv.org/abs/1606.08883 | id:1606.08883 author:Lili Su, Nitin H. Vaidya category:cs.DC cs.LG  published:2016-06-28 summary:This paper addresses the problem of non-Bayesian learning over multi-agent networks, where agents repeatedly collect partially informative observations about an unknown state of the world, and try to collaboratively learn the true state. We focus on the impact of the adversarial agents on the performance of consensus-based non-Bayesian learning, where non-faulty agents combine local learning updates with consensus primitives. In particular, we consider the scenario where an unknown subset of agents suffer Byzantine faults -- agents suffering Byzantine faults behave arbitrarily. Two different learning rules are proposed. version:1
arxiv-1606-08882 | Tracking Switched Dynamic Network Topologies from Information Cascades | http://arxiv.org/abs/1606.08882 | id:1606.08882 author:Brian Baingana, Georgios B. Giannakis category:stat.ML  published:2016-06-28 summary:Contagions such as the spread of popular news stories, or infectious diseases, propagate in cascades over dynamic networks with unobservable topologies. However, "social signals" such as product purchase time, or blog entry timestamps are measurable, and implicitly depend on the underlying topology, making it possible to track it over time. Interestingly, network topologies often "jump" between discrete states that may account for sudden changes in the observed signals. The present paper advocates a switched dynamic structural equation model to capture the topology-dependent cascade evolution, as well as the discrete states driving the underlying topologies. Conditions under which the proposed switched model is identifiable are established. Leveraging the edge sparsity inherent to social networks, a recursive $\ell_1$-norm regularized least-squares estimator is put forth to jointly track the states and network topologies. An efficient first-order proximal-gradient algorithm is developed to solve the resulting optimization problem. Numerical experiments on both synthetic data and real cascades measured over the span of one year are conducted, and test results corroborate the efficacy of the advocated approach. version:1
arxiv-1606-08866 | Technical Report: Towards a Universal Code Formatter through Machine Learning | http://arxiv.org/abs/1606.08866 | id:1606.08866 author:Terence Parr, Jurgin Vinju category:cs.PL cs.AI cs.LG  published:2016-06-28 summary:There are many declarative frameworks that allow us to implement code formatters relatively easily for any specific language, but constructing them is cumbersome. The first problem is that "everybody" wants to format their code differently, leading to either many formatter variants or a ridiculous number of configuration options. Second, the size of each implementation scales with a language's grammar size, leading to hundreds of rules. In this paper, we solve the formatter construction problem using a novel approach, one that automatically derives formatters for any given language without intervention from a language expert. We introduce a code formatter called CodeBuff that uses machine learning to abstract formatting rules from a representative corpus, using a carefully designed feature set. Our experiments on Java, SQL, and ANTLR grammars show that CodeBuff is efficient, has excellent accuracy, and is grammar invariant for a given language. It also generalizes to a 4th language tested during manuscript preparation. version:1
arxiv-1606-08842 | Active Ranking from Pairwise Comparisons and the Futility of Parametric Assumptions | http://arxiv.org/abs/1606.08842 | id:1606.08842 author:Reinhard Heckel, Nihar B. Shah, Kannan Ramchandran, Martin J. Wainwright category:cs.LG cs.AI cs.IT math.IT stat.ML  published:2016-06-28 summary:We consider sequential or active ranking of a set of n items based on noisy pairwise comparisons. Items are ranked according to the probability that a given item beats a randomly chosen item, and ranking refers to partitioning the items into sets of pre-specified sizes according to their scores. This notion of ranking includes as special cases the identification of the top-k items and the total ordering of the items. We first analyze a sequential ranking algorithm that counts the number of comparisons won, and uses these counts to decide whether to stop, or to compare another pair of items, chosen based on confidence intervals specified by the data collected up to that point. We prove that this algorithm succeeds in recovering the ranking using a number of comparisons that is optimal up to logarithmic factors. This guarantee does not require any structural properties of the underlying pairwise probability matrix, unlike a significant body of past work on pairwise ranking based on parametric models such as the Thurstone or Bradley-Terry-Luce models. It has been a long-standing open question as to whether or not imposing these parametric assumptions allow for improved ranking algorithms. Our second contribution settles this issue in the context of the problem of active ranking from pairwise comparisons: by means of tight lower bounds, we prove that perhaps surprisingly, these popular parametric modeling choices offer little statistical advantage. version:1
arxiv-1606-08821 | Generation and Pruning of Pronunciation Variants to Improve ASR Accuracy | http://arxiv.org/abs/1606.08821 | id:1606.08821 author:Zhenhao Ge, Aravind Ganapathiraju, Ananth N. Iyer, Scott A. Randal, Felix I. Wyss category:cs.CL  published:2016-06-28 summary:Speech recognition, especially name recognition, is widely used in phone services such as company directory dialers, stock quote providers or location finders. It is usually challenging due to pronunciation variations. This paper proposes an efficient and robust data-driven technique which automatically learns acceptable word pronunciations and updates the pronunciation dictionary to build a better lexicon without affecting recognition of other words similar to the target word. It generalizes well on datasets with various sizes, and reduces the error rate on a database with 13000+ human names by 42%, compared to a baseline with regular dictionaries already covering canonical pronunciations of 97%+ words in names, plus a well-trained spelling-to-pronunciation (STP) engine. version:1
arxiv-1606-08819 | Multi-View Kernel Consensus For Data Analysis and Signal Processing | http://arxiv.org/abs/1606.08819 | id:1606.08819 author:Moshe Salhov, Ofir Lindenbaum, Avi Silberschatz, Yoel Shkolnisky, Amir Averbuch category:cs.LG stat.ML  published:2016-06-28 summary:The input data features set for many data driven tasks is high-dimensional while the intrinsic dimension of the data is low. Data analysis methods aim to uncover the underlying low dimensional structure imposed by the low dimensional hidden parameters by utilizing distance metrics that consider the set of attributes as a single monolithic set. However, the transformation of the low dimensional phenomena into the measured high dimensional observations might distort the distance metric, This distortion can effect the desired estimated low dimensional geometric structure. In this paper, we suggest to utilize the redundancy in the attribute domain by partitioning the attributes into multiple subsets we call views. The proposed methods utilize the agreement also called consensus between different views to extract valuable geometric information that unifies multiple views about the intrinsic relationships among several different observations. This unification enhances the information that a single view or a simple concatenations of views provides. version:1
arxiv-1606-08813 | European Union regulations on algorithmic decision-making and a "right to explanation" | http://arxiv.org/abs/1606.08813 | id:1606.08813 author:Bryce Goodman, Seth Flaxman category:stat.ML cs.CY cs.LG  published:2016-06-28 summary:We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination. version:2
arxiv-1606-08808 | Adaptive Training of Random Mapping for Data Quantization | http://arxiv.org/abs/1606.08808 | id:1606.08808 author:Miao Cheng, Ah Chung Tsoi category:cs.LG cs.AI  published:2016-06-28 summary:Data quantization learns encoding results of data with certain requirements, and provides a broad perspective of many real-world applications to data handling. Nevertheless, the results of encoder is usually limited to multivariate inputs with the random mapping, and side information of binary codes are hardly to mostly depict the original data patterns as possible. In the literature, cosine based random quantization has attracted much attentions due to its intrinsic bounded results. Nevertheless, it usually suffers from the uncertain outputs, and information of original data fails to be fully preserved in the reduced codes. In this work, a novel binary embedding method, termed adaptive training quantization (ATQ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. As an adaptive learning idea, the reduced mapping is adaptively calculated with idea of data group, while the bias of random transform is to be improved to hold most matching information. Experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods. version:1
arxiv-1606-08777 | "Show me the cup": Reference with Continuous Representations | http://arxiv.org/abs/1606.08777 | id:1606.08777 author:Gemma Boleda, Sebastian Padó, Marco Baroni category:cs.CL cs.AI cs.LG  published:2016-06-28 summary:One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties. version:1
arxiv-1606-08733 | Recurrent Neural Networks for Dialogue State Tracking | http://arxiv.org/abs/1606.08733 | id:1606.08733 author:Ondřej Plátek, Petr Bělohlávek, Vojtěch Hudeček, Filip Jurčíček category:cs.CL  published:2016-06-28 summary:This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2 [6]. On the one hand, RNN models became state of the art in DST, on the other hand, most state-of-the-art models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve state-of-the-art results. We implemented two architectures which can be used in incremental settings and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-of-the-art results. version:1
arxiv-1606-08694 | Scalable image coding based on epitomes | http://arxiv.org/abs/1606.08694 | id:1606.08694 author:Martin Alain, Christine Guillemot, Dominique Thoreau, Philippe Guillotel category:cs.CV  published:2016-06-28 summary:In this paper, we propose a novel scheme for scalable image coding based on the concept of epitome. An epitome can be seen as a factorized representation of an image. Focusing on spatial scalability, the enhancement layer of the proposed scheme contains only the epitome of the input image. The pixels of the enhancement layer not contained in the epitome are then restored using two approaches inspired from local learning-based super-resolution methods. In the first method, a locally linear embedding model is learned on base layer patches and then applied to the corresponding epitome patches to reconstruct the enhancement layer. The second approach learns linear mappings between pairs of co-located base layer and epitome patches. Experiments have shown that significant improvement of the rate-distortion performances can be achieved compared to an SHVC reference. version:1
arxiv-1606-08689 | Hierarchical Neural Language Models for Joint Representation of Streaming Documents and their Content | http://arxiv.org/abs/1606.08689 | id:1606.08689 author:Nemanja Djuric, Hao Wu, Vladan Radosavljevic, Mihajlo Grbovic, Narayan Bhamidipati category:cs.CL cs.IR I.2.7; I.5.4; I.7.m  published:2016-06-28 summary:We consider the problem of learning distributed representations for documents in data streams. The documents are represented as low-dimensional vectors and are jointly learned with distributed vector representations of word tokens using a hierarchical framework with two embedded neural language models. In particular, we exploit the context of documents in streams and use one of the language models to model the document sequences, and the other to model word sequences within them. The models learn continuous vector representations for both word tokens and documents such that semantically similar documents and words are close in a common vector space. We discuss extensions to our model, which can be applied to personalized recommendation and social relationship mining by adding further user layers to the hierarchy, thus learning user-specific vectors to represent individual preferences. We validated the learned representations on a public movie rating data set from MovieLens, as well as on a large-scale Yahoo News data comprising three months of user activity logs collected on Yahoo servers. The results indicate that the proposed model can learn useful representations of both documents and word tokens, outperforming the current state-of-the-art by a large margin. version:1
arxiv-1607-01040 | Facial Expression Classification Using Rotation Slepian-based Moment Invariants | http://arxiv.org/abs/1607.01040 | id:1607.01040 author:Cuiming Zou, Kit Ian Kou category:cs.CV 30E05  33E10  14L24  published:2016-06-28 summary:Rotation moment invariants have been of great interest in image processing and pattern recognition. This paper presents a novel kind of rotation moment invariants based on the Slepian functions, which were originally introduced in the method of separation of variables for Helmholtz equations. They were first proposed for time series by Slepian and his coworkers in the 1960s. Recent studies have shown that these functions have an good performance in local approximation compared to other approximation basis. Motivated by the good approximation performance, we construct the Slepian-based moments and derive the rotation invariant. We not only theoretically prove the invariance, but also discuss the experiments on real data. The proposed rotation invariants are robust to noise and yield decent performance in facial expression classification. version:1
arxiv-1606-09581 | Performance Based Evaluation of Various Machine Learning Classification Techniques for Chronic Kidney Disease Diagnosis | http://arxiv.org/abs/1606.09581 | id:1606.09581 author:Sahil Sharma, Vinod Sharma, Atul Sharma category:cs.LG cs.AI cs.CY  published:2016-06-28 summary:Areas where Artificial Intelligence (AI) & related fields are finding their applications are increasing day by day, moving from core areas of computer science they are finding their applications in various other domains.In recent times Machine Learning i.e. a sub-domain of AI has been widely used in order to assist medical experts and doctors in the prediction, diagnosis and prognosis of various diseases and other medical disorders. In this manuscript the authors applied various machine learning algorithms to a problem in the domain of medical diagnosis and analyzed their efficiency in predicting the results. The problem selected for the study is the diagnosis of the Chronic Kidney Disease.The dataset used for the study consists of 400 instances and 24 attributes. The authors evaluated 12 classification techniques by applying them to the Chronic Kidney Disease data. In order to calculate efficiency, results of the prediction by candidate methods were compared with the actual medical results of the subject.The various metrics used for performance evaluation are predictive accuracy, precision, sensitivity and specificity. The results indicate that decision-tree performed best with nearly the accuracy of 98.6%, sensitivity of 0.9720, precision of 1 and specificity of 1. version:1
arxiv-1606-08572 | Diversified Visual Attention Networks for Fine-Grained Object Classification | http://arxiv.org/abs/1606.08572 | id:1606.08572 author:Bo Zhao, Xiao Wu, Jiashi Feng, Qiang Peng, Shuicheng Yan category:cs.CV  published:2016-06-28 summary:Fine-grained object classification is a challenging task due to the subtle inter-class difference and large intra-class variation. Recently, visual attention models have been applied to automatically localize the discriminative regions of an image for better capturing critical difference and demonstrated promising performance. However, without consideration of the diversity in attention process, most of existing attention models perform poorly in classifying fine-grained objects. In this paper, we propose a diversified visual attention network (DVAN) to address the problems of fine-grained object classification, which substan- tially relieves the dependency on strongly-supervised information for learning to localize discriminative regions compared with attentionless models. More importantly, DVAN explicitly pursues the diversity of attention and is able to gather discriminative information to the maximal extent. Multiple attention canvases are generated to extract convolutional features for attention. An LSTM recurrent unit is employed to learn the attentiveness and discrimination of attention canvases. The proposed DVAN has the ability to attend the object from coarse to fine granularity, and a dynamic internal representation for classification is built up by incrementally combining the information from different locations and scales of the image. Extensive experiments con- ducted on CUB-2011, Stanford Dogs and Stanford Cars datasets have demonstrated that the proposed diversified visual attention networks achieve competitive performance compared to the state- of-the-art approaches, without using any prior knowledge, user interaction or external resource in training or testing. version:1
arxiv-1606-08561 | Estimating the class prior and posterior from noisy positives and unlabeled data | http://arxiv.org/abs/1606.08561 | id:1606.08561 author:Shantanu Jain, Martha White, Predrag Radivojac category:stat.ML cs.LG  published:2016-06-28 summary:We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and both parametric and nonparametric algorithms proposed here constitutes an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data. version:1
arxiv-1606-08549 | Automatic Variational ABC | http://arxiv.org/abs/1606.08549 | id:1606.08549 author:Alexander Moreno, Tameem Adel, Edward Meeds, James M. Rehg, Max Welling category:stat.ML  published:2016-06-28 summary:Approximate Bayesian Computation (ABC) is a framework for performing likelihood-free posterior inference for simulation models. Stochastic Variational inference (SVI) is an appealing alternative to the inefficient sampling approaches commonly used in ABC. However, SVI is highly sensitive to the variance of the gradient estimators, and this problem is exacerbated by approximating the likelihood. We draw upon recent advances in variance reduction for SV and likelihood-free inference using deterministic simulations to produce low variance gradient estimators of the variational lower-bound. By then exploiting automatic differentiation libraries we can avoid nearly all model-specific derivations. We demonstrate performance on three problems and compare to existing SVI algorithms. Our results demonstrate the correctness and efficiency of our algorithm. version:1
arxiv-1606-08538 | A Local Density-Based Approach for Local Outlier Detection | http://arxiv.org/abs/1606.08538 | id:1606.08538 author:Bo Tang, Haibo He category:cs.AI cs.LG stat.ML  published:2016-06-28 summary:This paper presents a simple but effective density-based outlier detection approach with the local kernel density estimation (KDE). A Relative Density-based Outlier Score (RDOS) is introduced to measure the local outlierness of objects, in which the density distribution at the location of an object is estimated with a local KDE method based on extended nearest neighbors of the object. Instead of using only $k$ nearest neighbors, we further consider reverse nearest neighbors and shared nearest neighbors of an object for density distribution estimation. Some theoretical properties of the proposed RDOS including its expected value and false alarm probability are derived. A comprehensive experimental study on both synthetic and real-life data sets demonstrates that our approach is more effective than state-of-the-art outlier detection methods. version:1
arxiv-1606-08531 | A Learning Algorithm for Relational Logistic Regression: Preliminary Results | http://arxiv.org/abs/1606.08531 | id:1606.08531 author:Bahare Fatemi, Seyed Mehran Kazemi, David Poole category:cs.AI cs.LG stat.ML  published:2016-06-28 summary:Relational logistic regression (RLR) is a representation of conditional probability in terms of weighted formulae for modelling multi-relational data. In this paper, we develop a learning algorithm for RLR models. Learning an RLR model from data consists of two steps: 1- learning the set of formulae to be used in the model (a.k.a. structure learning) and learning the weight of each formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt and Murphy's hierarchical assumption: first we learn a model with simple formulae, then more complex formulae are added iteratively only if all their sub-formulae have proven effective in previous learned models. For parameter learning, we convert the problem into a non-relational learning problem and use an off-the-shelf logistic regression learning algorithm from Weka, an open-source machine learning tool, to learn the weights. We also indicate how hidden features about the individuals can be incorporated into RLR to boost the learning performance. We compare our learning algorithm to other structure and parameter learning algorithms in the literature, and compare the performance of RLR models to standard logistic regression and RDN-Boost on a modified version of the MovieLens data-set. version:1
arxiv-1606-08513 | SelQA: A New Benchmark for Selection-based Question Answering | http://arxiv.org/abs/1606.08513 | id:1606.08513 author:Tomasz Jurczyk, Michael Zhai, Jinho D. Choi category:cs.CL  published:2016-06-27 summary:This paper presents a new dataset to benchmark selection-based question answering. Our dataset contains contexts drawn from the ten most prevalent topics in the English Wikipedia. For the generation of a large, diverse, and challenging dataset, a new annotation scheme is proposed. Our annotation scheme involves a series of crowdsourcing tasks that can be easily followed by any researcher. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon. We hope that providing a large corpus will enable researchers to work towards more effective open-domain question answering. version:1
arxiv-1606-08501 | Symmetric and antisymmetric properties of solutions to kernel-based machine learning problems | http://arxiv.org/abs/1606.08501 | id:1606.08501 author:Giorgio Gnecco category:cs.LG  published:2016-06-27 summary:A particularly interesting instance of supervised learning with kernels is when each training example is associated with two objects, as in pairwise classification (Brunner et al., 2012), and in supervised learning of preference relations (Herbrich et al., 1998). In these cases, one may want to embed additional prior knowledge into the optimization problem associated with the training of the learning machine, modeled, respectively, by the symmetry of its optimal solution with respect to an exchange of order between the two objects, and by its antisymmetry. Extending the approach proposed in (Brunner et al., 2012) (where the only symmetric case was considered), we show, focusing on support vector binary classification, how such embedding is possible through the choice of a suitable pairwise kernel, which takes as inputs the individual feature vectors and also the group feature vectors associated with the two objects. We also prove that the symmetry/antisymmetry constraints still hold when considering the sequence of suboptimal solutions generated by one version of the Sequential Minimal Optimization (SMO) algorithm, and we present numerical results supporting the theoretical findings. We conclude discussing extensions of the main results to support vector regression, to transductive support vector machines, and to several kinds of graph kernels, including diffusion kernels. version:1
arxiv-1606-08495 | Network-Efficient Distributed Word2vec Training System for Large Vocabularies | http://arxiv.org/abs/1606.08495 | id:1606.08495 author:Erik Ordentlich, Lee Yang, Andy Feng, Peter Cnudde, Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Gavin Owens category:cs.CL  published:2016-06-27 summary:Word2vec is a popular family of algorithms for unsupervised training of dense vector representations of words on large text corpuses. The resulting vectors have been shown to capture semantic relationships among their corresponding words, and have shown promise in reducing a number of natural language processing (NLP) tasks to mathematical operations on these vectors. While heretofore applications of word2vec have centered around vocabularies with a few million words, wherein the vocabulary is the set of words for which vectors are simultaneously trained, novel applications are emerging in areas outside of NLP with vocabularies comprising several 100 million words. Existing word2vec training systems are impractical for training such large vocabularies as they either require that the vectors of all vocabulary words be stored in the memory of a single server or suffer unacceptable training latency due to massive network data transfer. In this paper, we present a novel distributed, parallel training system that enables unprecedented practical training of vectors for vocabularies with several 100 million words on a shared cluster of commodity servers, using far less network traffic than the existing solutions. We evaluate the proposed system on a benchmark dataset, showing that the quality of vectors does not degrade relative to non-distributed training. Finally, for several quarters, the system has been deployed for the purpose of matching queries to ads in Gemini, the sponsored search advertising platform at Yahoo, resulting in significant improvement of business metrics. version:1
arxiv-1606-08476 | Dynamic Hierarchical Dirichlet Process for Abnormal Behaviour Detection in Video | http://arxiv.org/abs/1606.08476 | id:1606.08476 author:Olga Isupova, Danil Kuzin, Lyudmila Mihaylova category:stat.ML  published:2016-06-27 summary:This paper proposes a novel dynamic Hierarchical Dirichlet Process topic model that considers the dependence between successive observations. Conventional posterior inference algorithms for this kind of models require processing of the whole data through several passes. It is computationally intractable for massive or sequential data. We design the batch and online inference algorithms, based on the Gibbs sampling, for the proposed model. It allows to process sequential data, incrementally updating the model by a new observation. The model is applied to abnormal behaviour detection in video sequences. A new abnormality measure is proposed for decision making. The proposed method is compared with the method based on the non- dynamic Hierarchical Dirichlet Process, for which we also derive the online Gibbs sampler and the abnormality measure. The results with synthetic and real data show that the consideration of the dynamics in a topic model improves the classification performance for abnormal behaviour detection. version:1
arxiv-1606-08455 | Anomaly detection in video with Bayesian nonparametrics | http://arxiv.org/abs/1606.08455 | id:1606.08455 author:Olga Isupova, Danil Kuzin, Lyudmila Mihaylova category:stat.ML  published:2016-06-27 summary:A novel dynamic Bayesian nonparametric topic model for anomaly detection in video is proposed in this paper. Batch and online Gibbs samplers are developed for inference. The paper introduces a new abnormality measure for decision making. The proposed method is evaluated on both synthetic and real data. The comparison with a non-dynamic model shows the superiority of the proposed dynamic one in terms of the classification performance for anomaly detection. version:1
