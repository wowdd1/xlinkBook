arxiv-1310-4210 | Demystifying Information-Theoretic Clustering | http://arxiv.org/abs/1310.4210 | id:1310.4210 author:Greg Ver Steeg, Aram Galstyan, Fei Sha, Simon DeDeo category:cs.LG cs.IT math.IT physics.data-an stat.ML  published:2013-10-15 summary:We propose a novel method for clustering data which is grounded in information-theoretic principles and requires no parametric assumptions. Previous attempts to use information theory to define clusters in an assumption-free way are based on maximizing mutual information between data and cluster labels. We demonstrate that this intuition suffers from a fundamental conceptual flaw that causes clustering performance to deteriorate as the amount of data increases. Instead, we return to the axiomatic foundations of information theory to define a meaningful clustering measure based on the notion of consistency under coarse-graining for finite data. version:2
arxiv-1402-1151 | Image Acquisition in an Underwater Vision System with NIR and VIS Illumination | http://arxiv.org/abs/1402.1151 | id:1402.1151 author:Wojciech Biegański, Andrzej Kasiński category:cs.CV  published:2014-02-05 summary:The paper describes the image acquisition system able to capture images in two separated bands of light, used to underwater autonomous navigation. The channels are: the visible light spectrum and near infrared spectrum. The characteristics of natural, underwater environment were also described together with the process of the underwater image creation. The results of an experiment with comparison of selected images acquired in these channels are discussed. version:1
arxiv-1405-6137 | An enhanced neural network based approach towards object extraction | http://arxiv.org/abs/1405.6137 | id:1405.6137 author:S. K. Katiyar, P. V. Arun category:cs.CV cs.LG cs.NE  published:2014-02-05 summary:The improvements in spectral and spatial resolution of the satellite images have facilitated the automatic extraction and identification of the features from satellite images and aerial photographs. An automatic object extraction method is presented for extracting and identifying the various objects from satellite images and the accuracy of the system is verified with regard to IRS satellite images. The system is based on neural network and simulates the process of visual interpretation from remote sensing images and hence increases the efficiency of image analysis. This approach obtains the basic characteristics of the various features and the performance is enhanced by the automatic learning approach, intelligent interpretation, and intelligent interpolation. The major advantage of the method is its simplicity and that the system identifies the features not only based on pixel value but also based on the shape, haralick features etc of the objects. Further the system allows flexibility for identifying the features within the same category based on size and shape. The successful application of the system verified its effectiveness and the accuracy of the system were assessed by ground truth verification. version:1
arxiv-1402-1141 | Quantum Cybernetics and Complex Quantum Systems Science - A Quantum Connectionist Exploration | http://arxiv.org/abs/1402.1141 | id:1402.1141 author:Carlos Pedro Gonçalves category:cs.NE cond-mat.dis-nn quant-ph 92B20  81P68  82C32  published:2014-02-05 summary:Quantum cybernetics and its connections to complex quantum systems science is addressed from the perspective of complex quantum computing systems. In this way, the notion of an autonomous quantum computing system is introduced in regards to quantum artificial intelligence, and applied to quantum artificial neural networks, considered as autonomous quantum computing systems, which leads to a quantum connectionist framework within quantum cybernetics for complex quantum computing systems. Several examples of quantum feedforward neural networks are addressed in regards to Boolean functions' computation, multilayer quantum computation dynamics, entanglement and quantum complementarity. The examples provide a framework for a reflection on the role of quantum artificial neural networks as a general framework for addressing complex quantum systems that perform network-based quantum computation, possible consequences are drawn regarding quantum technologies, as well as fundamental research in complex quantum systems science and quantum biology. version:1
arxiv-1402-1128 | Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition | http://arxiv.org/abs/1402.1128 | id:1402.1128 author:Haşim Sak, Andrew Senior, Françoise Beaufays category:cs.NE cs.CL cs.LG stat.ML  published:2014-02-05 summary:Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. version:1
arxiv-1308-6342 | Linear and Parallel Learning of Markov Random Fields | http://arxiv.org/abs/1308.6342 | id:1308.6342 author:Yariv Dror Mizrahi, Misha Denil, Nando de Freitas category:stat.ML cs.LG  published:2013-08-29 summary:We introduce a new embarrassingly parallel parameter learning algorithm for Markov random fields with untied parameters which is efficient for a large class of practical models. Our algorithm parallelizes naturally over cliques and, for graphs of bounded degree, its complexity is linear in the number of cliques. Unlike its competitors, our algorithm is fully parallel and for log-linear models it is also data efficient, requiring only the local sufficient statistics of the data to estimate parameters. version:4
arxiv-1405-6136 | An evolutionary computational based approach towards automatic image registration | http://arxiv.org/abs/1405.6136 | id:1405.6136 author:P. V. Arun, S. K. Katiyar category:cs.CV cs.NE  published:2014-02-05 summary:Image registration is a key component of various image processing operations which involve the analysis of different image data sets. Automatic image registration domains have witnessed the application of many intelligent methodologies over the past decade; however inability to properly model object shape as well as contextual information had limited the attainable accuracy. In this paper, we propose a framework for accurate feature shape modeling and adaptive resampling using advanced techniques such as Vector Machines, Cellular Neural Network (CNN), SIFT, coreset, and Cellular Automata. CNN has found to be effective in improving feature matching as well as resampling stages of registration and complexity of the approach has been considerably reduced using corset optimization The salient features of this work are cellular neural network approach based SIFT feature point optimisation, adaptive resampling and intelligent object modelling. Developed methodology has been compared with contemporary methods using different statistical measures. Investigations over various satellite images revealed that considerable success was achieved with the approach. System has dynamically used spectral and spatial information for representing contextual knowledge using CNN-prolog approach. Methodology also illustrated to be effective in providing intelligent interpretation and adaptive resampling. version:1
arxiv-1405-6135 | Cellular Automata based adaptive resampling technique for the processing of remotely sensed imagery | http://arxiv.org/abs/1405.6135 | id:1405.6135 author:S. K. Katiyar, P. V. Arun category:cs.CV  published:2014-02-05 summary:Resampling techniques are being widely used at different stages of satellite image processing. The existing methodologies cannot perfectly recover features from a completely under sampled image and hence an intelligent adaptive resampling methodology is required. We address these issues and adopt an error metric from the available literature to define interpolation quality. We also propose a new resampling scheme that adapts itself with regard to the pixel and texture variation in the image. The proposed CNN based hybrid method has been found to perform better than the existing methods as it adapts itself with reference to the image features. version:1
arxiv-1405-6133 | A review over the applicability of image entropy in analyses of remote sensing datasets | http://arxiv.org/abs/1405.6133 | id:1405.6133 author:S. K. Katiyar, P. V. Arun category:cs.CV  published:2014-02-05 summary:Entropy is the measure of uncertainty in any data and is adopted for maximisation of mutual information in many remote sensing operations. The availability of wide entropy variations motivated us for an investigation over the suitability preference of these versions to specific operations. version:1
arxiv-1312-2565 | An SIR Graph Growth Model for the Epidemics of Communicable Diseases | http://arxiv.org/abs/1312.2565 | id:1312.2565 author:Charanpal Dhanjal, Stéphan Clémençon category:stat.AP q-bio.PE stat.ML  published:2013-12-09 summary:It is the main purpose of this paper to introduce a graph-valued stochastic process in order to model the spread of a communicable infectious disease. The major novelty of the SIR model we promote lies in the fact that the social network on which the epidemics is taking place is not specified in advance but evolves through time, accounting for the temporal evolution of the interactions involving infective individuals. Without assuming the existence of a fixed underlying network model, the stochastic process introduced describes, in a flexible and realistic manner, epidemic spread in non-uniformly mixing and possibly heterogeneous populations. It is shown how to fit such a (parametrised) model by means of Approximate Bayesian Computation methods based on graph-valued statistics. The concepts and statistical methods described in this paper are finally applied to a real epidemic dataset, related to the spread of HIV in Cuba in presence of a contact tracing system, which permits one to reconstruct partly the evolution of the graph of sexual partners diagnosed HIV positive between 1986 and 2006. version:2
arxiv-1301-6199 | Sample Complexity of Bayesian Optimal Dictionary Learning | http://arxiv.org/abs/1301.6199 | id:1301.6199 author:Ayaka Sakata, Yoshiyuki Kabashima category:cs.LG cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT  published:2013-01-26 summary:We consider a learning problem of identifying a dictionary matrix D (M times N dimension) from a sample set of M dimensional vectors Y = N^{-1/2} DX, where X is a sparse matrix (N times P dimension) in which the density of non-zero entries is 0<rho< 1. In particular, we focus on the minimum sample size P_c (sample complexity) necessary for perfectly identifying D of the optimal learning scheme when D and X are independently generated from certain distributions. By using the replica method of statistical mechanics, we show that P_c=O(N) holds as long as alpha = M/N >rho is satisfied in the limit of N to infinity. Our analysis also implies that the posterior distribution given Y is condensed only at the correct dictionary D when the compression rate alpha is greater than a certain critical value alpha_M(rho). This suggests that belief propagation may allow us to learn D with a low computational complexity using O(N) samples. version:2
arxiv-1402-0978 | Patchwise Joint Sparse Tracking with Occlusion Detection | http://arxiv.org/abs/1402.0978 | id:1402.0978 author:Ali Zarezade, Hamid R. Rabiee, Ali Soltani-Farani, Ahmad Khajenezhad category:cs.CV  published:2014-02-05 summary:This paper presents a robust tracking approach to handle challenges such as occlusion and appearance change. Here, the target is partitioned into a number of patches. Then, the appearance of each patch is modeled using a dictionary composed of corresponding target patches in previous frames. In each frame, the target is found among a set of candidates generated by a particle filter, via a likelihood measure that is shown to be proportional to the sum of patch-reconstruction errors of each candidate. Since the target's appearance often changes slowly in a video sequence, it is assumed that the target in the current frame and the best candidates of a small number of previous frames, belong to a common subspace. This is imposed using joint sparse representation to enforce the target and previous best candidates to have a common sparsity pattern. Moreover, an occlusion detection scheme is proposed that uses patch-reconstruction errors and a prior probability of occlusion, extracted from an adaptive Markov chain, to calculate the probability of occlusion per patch. In each frame, occluded patches are excluded when updating the dictionary. Extensive experimental results on several challenging sequences shows that the proposed method outperforms state-of-the-art trackers. version:1
arxiv-1312-5770 | Consistency of Causal Inference under the Additive Noise Model | http://arxiv.org/abs/1312.5770 | id:1312.5770 author:Samory Kpotufe, Eleni Sgouritsa, Dominik Janzing, Bernhard Schölkopf category:cs.LG stat.ML  published:2013-12-19 summary:We analyze a family of methods for statistical causal inference from sample under the so-called Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting. version:3
arxiv-1405-6132 | Comparative analysis of common edge detection techniques in context of object extraction | http://arxiv.org/abs/1405.6132 | id:1405.6132 author:S. K. Katiyar, P. V. Arun category:cs.CV  published:2014-02-05 summary:Edges characterize boundaries and are therefore a problem of practical importance in remote sensing.In this paper a comparative study of various edge detection techniques and band wise analysis of these algorithms in the context of object extraction with regard to remote sensing satellite images from the Indian Remote Sensing Satellite (IRS) sensors LISS 3, LISS 4 and Cartosat1 as well as Google Earth is presented. version:1
arxiv-1402-0915 | Learning Ordered Representations with Nested Dropout | http://arxiv.org/abs/1402.0915 | id:1402.0915 author:Oren Rippel, Michael A. Gelbart, Ryan P. Adams category:stat.ML cs.LG  published:2014-02-05 summary:In this paper, we study ordered representations of data in which different dimensions have different degrees of importance. To learn these representations we introduce nested dropout, a procedure for stochastically removing coherent nested sets of hidden units in a neural network. We first present a sequence of theoretical results in the simple case of a semi-linear autoencoder. We rigorously show that the application of nested dropout enforces identifiability of the units, which leads to an exact equivalence with PCA. We then extend the algorithm to deep models and demonstrate the relevance of ordered representations to a number of applications. Specifically, we use the ordered property of the learned codes to construct hash-based data structures that permit very fast retrieval, achieving retrieval in time logarithmic in the database size and independent of the dimensionality of the representation. This allows codes that are hundreds of times longer than currently feasible for retrieval. We therefore avoid the diminished quality associated with short codes, while still performing retrieval that is competitive in speed with existing methods. We also show that ordered representations are a promising way to learn adaptive compression for efficient online data reconstruction. version:1
arxiv-1402-0914 | Discovering Latent Network Structure in Point Process Data | http://arxiv.org/abs/1402.0914 | id:1402.0914 author:Scott W. Linderman, Ryan P. Adams category:stat.ML cs.LG  published:2014-02-04 summary:Networks play a central role in modern data analysis, enabling us to reason about systems by studying the relationships between their parts. Most often in network analysis, the edges are given. However, in many systems it is difficult or impossible to measure the network directly. Examples of latent networks include economic interactions linking financial instruments and patterns of reciprocity in gang violence. In these cases, we are limited to noisy observations of events associated with each node. To enable analysis of these implicit networks, we develop a probabilistic model that combines mutually-exciting point processes with random graph models. We show how the Poisson superposition principle enables an elegant auxiliary variable formulation and a fully-Bayesian, parallel inference algorithm. We evaluate this new model empirically on several datasets. version:1
arxiv-1310-0512 | Jointly Clustering Rows and Columns of Binary Matrices: Algorithms and Trade-offs | http://arxiv.org/abs/1310.0512 | id:1310.0512 author:Jiaming Xu, Rui Wu, Kai Zhu, Bruce Hajek, R. Srikant, Lei Ying category:stat.ML  published:2013-10-01 summary:In standard clustering problems, data points are represented by vectors, and by stacking them together, one forms a data matrix with row or column cluster structure. In this paper, we consider a class of binary matrices, arising in many applications, which exhibit both row and column cluster structure, and our goal is to exactly recover the underlying row and column clusters by observing only a small fraction of noisy entries. We first derive a lower bound on the minimum number of observations needed for exact cluster recovery. Then, we propose three algorithms with different running time and compare the number of observations needed by them for successful cluster recovery. Our analytical results show smooth time-data trade-offs: one can gradually reduce the computational complexity when increasingly more observations are available. version:2
arxiv-1204-5540 | Learning Loosely Connected Markov Random Fields | http://arxiv.org/abs/1204.5540 | id:1204.5540 author:Rui Wu, R. Srikant, Jian Ni category:stat.ML  published:2012-04-25 summary:We consider the structure learning problem for graphical models that we call loosely connected Markov random fields, in which the number of short paths between any pair of nodes is small, and present a new conditional independence test based algorithm for learning the underlying graph structure. The novel maximization step in our algorithm ensures that the true edges are detected correctly even when there are short cycles in the graph. The number of samples required by our algorithm is C*log p, where p is the size of the graph and the constant C depends on the parameters of the model. We show that several previously studied models are examples of loosely connected Markov random fields, and our algorithm achieves the same or lower computational complexity than the previously designed algorithms for individual cases. We also get new results for more general graphical models, in particular, our algorithm learns general Ising models on the Erdos-Renyi random graph G(p, c/p) correctly with running time O(np^5). version:3
arxiv-1402-0836 | Cognitive Aging as Interplay between Hebbian Learning and Criticality | http://arxiv.org/abs/1402.0836 | id:1402.0836 author:Sakyasingha Dasgupta category:nlin.AO cs.NE q-bio.NC  published:2014-02-04 summary:Cognitive ageing seems to be a story of global degradation. As one ages there are a number of physical, chemical and biological changes that take place. Therefore it is logical to assume that the brain is no exception to this phenomenon. The principle purpose of this project is to use models of neural dynamics and learning based on the underlying principle of self-organised criticality, to account for the age related cognitive effects. In this regard learning in neural networks can serve as a model for the acquisition of skills and knowledge in early development stages i.e. the ageing process and criticality in the network serves as the optimum state of cognitive abilities. Possible candidate mechanisms for ageing in a neural network are loss of connectivity and neurons, increase in the level of noise, reduction in white matter or more interestingly longer learning history and the competition among several optimization objectives. In this paper we are primarily interested in the affect of the longer learning history on memory and thus the optimality in the brain. Hence it is hypothesized that prolonged learning in the form of associative memory patterns can destroy the state of criticality in the network. We base our model on Tsodyks and Markrams [49] model of dynamic synapses, in the process to explore the effect of combining standard Hebbian learning with the phenomenon of Self-organised criticality. The project mainly consists of evaluations and simulations of networks of integrate and fire-neurons that have been subjected to various combinations of neural-level ageing effects, with the aim of establishing the primary hypothesis and understanding the decline of cognitive abilities due to ageing, using one of its important characteristics, a longer learning history. version:1
arxiv-1305-2524 | Corrupted Sensing: Novel Guarantees for Separating Structured Signals | http://arxiv.org/abs/1305.2524 | id:1305.2524 author:Rina Foygel, Lester Mackey category:cs.IT math.IT math.OC stat.ML  published:2013-05-11 summary:We study the problem of corrupted sensing, a generalization of compressed sensing in which one aims to recover a signal from a collection of corrupted or unreliable measurements. While an arbitrary signal cannot be recovered in the face of arbitrary corruption, tractable recovery is possible when both signal and corruption are suitably structured. We quantify the relationship between signal recovery and two geometric measures of structure, the Gaussian complexity of a tangent cone and the Gaussian distance to a subdifferential. We take a convex programming approach to disentangling signal and corruption, analyzing both penalized programs that trade off between signal and corruption complexity, and constrained programs that bound the complexity of signal or corruption when prior information is available. In each case, we provide conditions for exact signal recovery from structured corruption and stable signal recovery from structured corruption with added unstructured noise. Our simulations demonstrate close agreement between our theoretical recovery bounds and the sharp phase transitions observed in practice. In addition, we provide new interpretable bounds for the Gaussian complexity of sparse vectors, block-sparse vectors, and low-rank matrices, which lead to sharper guarantees of recovery when combined with our results and those in the literature. version:2
arxiv-1402-0796 | Sequential Model-Based Ensemble Optimization | http://arxiv.org/abs/1402.0796 | id:1402.0796 author:Alexandre Lacoste, Hugo Larochelle, François Laviolette, Mario Marchand category:cs.LG stat.ML  published:2014-02-04 summary:One of the most tedious tasks in the application of machine learning is model selection, i.e. hyperparameter selection. Fortunately, recent progress has been made in the automation of this process, through the use of sequential model-based optimization (SMBO) methods. This can be used to optimize a cross-validation performance of a learning algorithm over the value of its hyperparameters. However, it is well known that ensembles of learned models almost consistently outperform a single model, even if properly selected. In this paper, we thus propose an extension of SMBO methods that automatically constructs such ensembles. This method builds on a recently proposed ensemble construction paradigm known as agnostic Bayesian learning. In experiments on 22 regression and 39 classification data sets, we confirm the success of this proposed approach, which is able to outperform model selection with SMBO. version:1
arxiv-0610010 | One-Pass, One-Hash n-Gram Statistics Estimation | http://arxiv.org/abs/cs/0610010 | id:0610010 author:Daniel Lemire, Owen Kaser category:cs.DB cs.CL  published:2006-10-03 summary:In multimedia, text or bioinformatics databases, applications query sequences of n consecutive symbols called n-grams. Estimating the number of distinct n-grams is a view-size estimation problem. While view sizes can be estimated by sampling under statistical assumptions, we desire an unassuming algorithm with universally valid accuracy bounds. Most related work has focused on repeatedly hashing the data, which is prohibitive for large data sources. We prove that a one-pass one-hash algorithm is sufficient for accurate estimates if the hashing is sufficiently independent. To reduce costs further, we investigate recursive random hashing algorithms and show that they are sufficiently independent in practice. We compare our running times with exact counts using suffix arrays and show that, while we use hardly any storage, we are an order of magnitude faster. The approach further is extended to a one-pass/one-hash computation of n-gram entropy and iceberg counts. The experiments use a large collection of English text from the Gutenberg Project as well as synthetic data. version:4
arxiv-1402-0785 | Signal to Noise Ratio in Lensless Compressive Imaging | http://arxiv.org/abs/1402.0785 | id:1402.0785 author:Hong Jiang, Gang Huang, Paul Wilford category:cs.CV  published:2014-02-04 summary:We analyze the signal to noise ratio (SNR) in a lensless compressive imaging (LCI) architecture. The architecture consists of a sensor of a single detecting element and an aperture assembly of an array of programmable elements. LCI can be used in conjunction with compressive sensing to capture images in a compressed form of compressive measurements. In this paper, we perform SNR analysis of the LCI and compare it with imaging with a pinhole or a lens. We will show that the SNR in the LCI is independent of the image resolution, while the SNR in either pinhole aperture imaging or lens aperture imaging decreases as the image resolution increases. Consequently, the SNR in the LCI is much higher if the image resolution is large enough. version:1
arxiv-1402-0708 | Microstrip Coupler Design Using Bat Algorithm | http://arxiv.org/abs/1402.0708 | id:1402.0708 author:Ezgi Deniz Ulker, Sadik Ulker category:cs.NE  published:2014-02-04 summary:Evolutionary and swarm algorithms have found many applications in design problems since todays computing power enables these algorithms to find solutions to complicated design problems very fast. Newly proposed hybrid algorithm, bat algorithm, has been applied for the design of microwave microstrip couplers for the first time. Simulation results indicate that the bat algorithm is a very fast algorithm and it produces very reliable results. version:1
arxiv-1405-6130 | A Study of Local Binary Pattern Method for Facial Expression Detection | http://arxiv.org/abs/1405.6130 | id:1405.6130 author:Ms. Drashti H. Bhatt, Mr. Kirit R. Rathod, Mr. Shardul J. Agravat category:cs.CV  published:2014-02-04 summary:Face detection is a basic task for expression recognition. The reliability of face detection & face recognition approach has a major role on the performance and usability of the entire system. There are several ways to undergo face detection & recognition. We can use Image Processing Operations, various classifiers, filters or virtual machines for the former. Various strategies are being available for Facial Expression Detection. The field of facial expression detection can have various applications along with its importance & can be interacted between human being & computer. Many few options are available to identify a face in an image in accurate & efficient manner. Local Binary Pattern (LBP) based texture algorithms have gained popularity in these years. LBP is an effective approach to have facial expression recognition & is a feature-based approach. version:1
arxiv-1402-0645 | Local Gaussian Regression | http://arxiv.org/abs/1402.0645 | id:1402.0645 author:Franziska Meier, Philipp Hennig, Stefan Schaal category:cs.LG cs.RO  published:2014-02-04 summary:Locally weighted regression was created as a nonparametric learning method that is computationally efficient, can learn from very large amounts of data and add data incrementally. An interesting feature of locally weighted regression is that it can work with spatially varying length scales, a beneficial property, for instance, in control problems. However, it does not provide a generative model for function values and requires training and test data to be generated identically, independently. Gaussian (process) regression, on the other hand, provides a fully generative model without significant formal requirements on the distribution of training data, but has much higher computational cost and usually works with one global scale per input dimension. Using a localising function basis and approximate inference techniques, we take Gaussian (process) regression to increasingly localised properties and toward the same computational complexity class as locally weighted regression. version:1
arxiv-1301-4240 | Hypothesis Testing in High-Dimensional Regression under the Gaussian Random Design Model: Asymptotic Theory | http://arxiv.org/abs/1301.4240 | id:1301.4240 author:Adel Javanmard, Andrea Montanari category:stat.ME cs.IT math.IT math.ST stat.ML stat.TH  published:2013-01-17 summary:We consider linear regression in the high-dimensional regime where the number of observations $n$ is smaller than the number of parameters $p$. A very successful approach in this setting uses $\ell_1$-penalized least squares (a.k.a. the Lasso) to search for a subset of $s_0< n$ parameters that best explain the data, while setting the other parameters to zero. Considerable amount of work has been devoted to characterizing the estimation and model selection problems within this approach. In this paper we consider instead the fundamental, but far less understood, question of \emph{statistical significance}. More precisely, we address the problem of computing p-values for single regression coefficients. On one hand, we develop a general upper bound on the minimax power of tests with a given significance level. On the other, we prove that this upper bound is (nearly) achievable through a practical procedure in the case of random design matrices with independent entries. Our approach is based on a debiasing of the Lasso estimator. The analysis builds on a rigorous characterization of the asymptotic distribution of the Lasso estimator and its debiased version. Our result holds for optimal sample size, i.e., when $n$ is at least on the order of $s_0 \log(p/s_0)$. We generalize our approach to random design matrices with i.i.d. Gaussian rows $x_i\sim N(0,\Sigma)$. In this case we prove that a similar distributional characterization (termed `standard distributional limit') holds for $n$ much larger than $s_0(\log p)^2$. Finally, we show that for optimal sample size, $n$ being at least of order $s_0 \log(p/s_0)$, the standard distributional limit for general Gaussian designs can be derived from the replica heuristics in statistical physics. version:3
arxiv-1402-0595 | Scene Labeling with Contextual Hierarchical Models | http://arxiv.org/abs/1402.0595 | id:1402.0595 author:Mojtaba Seyedhosseini, Tolga Tasdizen category:cs.CV  published:2014-02-04 summary:Scene labeling is the problem of assigning an object label to each pixel. It unifies the image segmentation and object recognition problems. The importance of using contextual information in scene labeling frameworks has been widely realized in the field. We propose a contextual framework, called contextual hierarchical model (CHM), which learns contextual information in a hierarchical framework for scene labeling. At each level of the hierarchy, a classifier is trained based on downsampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. This training strategy allows for optimization of a joint posterior probability at multiple resolutions through the hierarchy. Contextual hierarchical model is purely based on the input image patches and does not make use of any fragments or shape examples. Hence, it is applicable to a variety of problems such as object segmentation and edge detection. We demonstrate that CHM outperforms state-of-the-art on Stanford background and Weizmann horse datasets. It also outperforms state-of-the-art edge detection methods on NYU depth dataset and achieves state-of-the-art on Berkeley segmentation dataset (BSDS 500). version:1
arxiv-1402-0586 | Topic Segmentation and Labeling in Asynchronous Conversations | http://arxiv.org/abs/1402.0586 | id:1402.0586 author:Shafiq Rayhan Joty, Giuseppe Carenini, Raymond T Ng category:cs.CL  published:2014-02-04 summary:Topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many Natural Language Processing (NLP) applications. We present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. Our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for NLP. For topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. For topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations. version:1
arxiv-1402-0578 | Natural Language Inference for Arabic Using Extended Tree Edit Distance with Subtrees | http://arxiv.org/abs/1402.0578 | id:1402.0578 author:Maytham Alabbas, Allan Ramsay category:cs.CL  published:2014-02-04 summary:Many natural language processing (NLP) applications require the computation of similarities between pairs of syntactic or semantic trees. Many researchers have used tree edit distance for this task, but this technique suffers from the drawback that it deals with single node operations only. We have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes. The extended algorithm with subtree operations, TED+ST, is more effective and flexible than the standard algorithm, especially for applications that pay attention to relations among nodes (e.g. in linguistic trees, deleting a modifier subtree should be cheaper than the sum of deleting its components individually). We describe the use of TED+ST for checking entailment between two Arabic text snippets. The preliminary results of using TED+ST were encouraging when compared with two string-based approaches and with the standard algorithm. version:1
arxiv-1402-0577 | A Survey on Latent Tree Models and Applications | http://arxiv.org/abs/1402.0577 | id:1402.0577 author:Raphaël Mourad, Christine Sinoquet, Nevin L. Zhang, Tengfei Liu, Philippe Leray category:cs.LG  published:2014-02-04 summary:In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field. version:1
arxiv-1402-0574 | Learning to Predict from Textual Data | http://arxiv.org/abs/1402.0574 | id:1402.0574 author:Kira Radinsky, Sagie Davidovich, Shaul Markovitch category:cs.CL cs.AI cs.IR  published:2014-02-04 summary:Given a current news event, we tackle the problem of generating plausible predictions of future events it might cause. We present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. Our Pundit algorithm generalizes examples of causality pairs to infer a causality predictor. To obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns. For generalization, the model uses a vast number of world knowledge ontologies. Empirical evaluation on real news articles shows that our Pundit algorithm performs as well as non-expert humans. version:1
arxiv-1402-0570 | A Feature Subset Selection Algorithm Automatic Recommendation Method | http://arxiv.org/abs/1402.0570 | id:1402.0570 author:Guangtao Wang, Qinbao Song, Heli Sun, Xueying Zhang, Baowen Xu, Yuming Zhou category:cs.LG  published:2014-02-04 summary:Many feature subset selection (FSS) algorithms have been proposed, but not all of them are appropriate for a given feature selection problem. At the same time, so far there is rarely a good way to choose appropriate FSS algorithms for the problem at hand. Thus, FSS algorithm automatic recommendation is very important and practically useful. In this paper, a meta learning based FSS algorithm automatic recommendation method is presented. The proposed method first identifies the data sets that are most similar to the one at hand by the k-nearest neighbor classification algorithm, and the distances among these data sets are calculated based on the commonly-used data set characteristics. Then, it ranks all the candidate FSS algorithms according to their performance on these similar data sets, and chooses the algorithms with best performance as the appropriate ones. The performance of the candidate FSS algorithms is evaluated by a multi-criteria metric that takes into account not only the classification accuracy over the selected features, but also the runtime of feature selection and the number of selected features. The proposed recommendation method is extensively tested on 115 real world data sets with 22 well-known and frequently-used different FSS algorithms for five representative classifiers. The results show the effectiveness of our proposed FSS algorithm recommendation method. version:1
arxiv-1402-0563 | Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine Translation | http://arxiv.org/abs/1402.0563 | id:1402.0563 author:Marta R. Costa-jussà, Carlos A. Henríquez, Rafael E. Banchs category:cs.CL  published:2014-02-04 summary:Although, Chinese and Spanish are two of the most spoken languages in the world, not much research has been done in machine translation for this language pair. This paper focuses on investigating the state-of-the-art of Chinese-to-Spanish statistical machine translation (SMT), which nowadays is one of the most popular approaches to machine translation. For this purpose, we report details of the available parallel corpus which are Basic Traveller Expressions Corpus (BTEC), Holy Bible and United Nations (UN). Additionally, we conduct experimental work with the largest of these three corpora to explore alternative SMT strategies by means of using a pivot language. Three alternatives are considered for pivoting: cascading, pseudo-corpus and triangulation. As pivot language, we use either English, Arabic or French. Results show that, for a phrase-based SMT system, English is the best pivot language between Chinese and Spanish. We propose a system output combination using the pivot strategies which is capable of outperforming the direct translation strategy. The main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact. version:1
arxiv-1402-0560 | Safe Exploration of State and Action Spaces in Reinforcement Learning | http://arxiv.org/abs/1402.0560 | id:1402.0560 author:Javier Garcia, Fernando Fernandez category:cs.LG cs.AI  published:2014-02-04 summary:In this paper, we consider the important problem of safe exploration in reinforcement learning. While reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. Traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). Consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. We introduce the PI-SRL algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. We evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management. version:1
arxiv-1402-0558 | Parameterized Complexity Results for Exact Bayesian Network Structure Learning | http://arxiv.org/abs/1402.0558 | id:1402.0558 author:Sebastian Ordyniak, Stefan Szeider category:cs.AI cs.LG  published:2014-02-04 summary:Bayesian network structure learning is the notoriously difficult problem of discovering a Bayesian network that optimally represents a given set of training data. In this paper we study the computational worst-case complexity of exact Bayesian network structure learning under graph theoretic restrictions on the (directed) super-structure. The super-structure is an undirected graph that contains as subgraphs the skeletons of solution networks. We introduce the directed super-structure as a natural generalization of its undirected counterpart. Our results apply to several variants of score-based Bayesian network structure learning where the score of a network decomposes into local scores of its nodes. Results: We show that exact Bayesian network structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth, and in linear time if in addition the super-structure has bounded maximum degree. Furthermore, we show that if the directed super-structure is acyclic, then exact Bayesian network structure learning can be carried out in quadratic time. We complement these positive results with a number of hardness results. We show that both restrictions (treewidth and degree) are essential and cannot be dropped without loosing uniform polynomial time tractability (subject to a complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard for "almost acyclic" directed super-structures. Furthermore, we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions, arc deletions, or arc reversals (k-neighborhood local search). version:1
arxiv-1402-0556 | Generating Extractive Summaries of Scientific Paradigms | http://arxiv.org/abs/1402.0556 | id:1402.0556 author:Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, Taesun Moon category:cs.IR cs.CL  published:2014-02-04 summary:Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary. version:1
arxiv-1402-0543 | How Does Latent Semantic Analysis Work? A Visualisation Approach | http://arxiv.org/abs/1402.0543 | id:1402.0543 author:Jan Koeman, William Rea category:cs.CL cs.IR  published:2014-02-03 summary:By using a small example, an analogy to photographic compression, and a simple visualization using heatmaps, we show that latent semantic analysis (LSA) is able to extract what appears to be semantic meaning of words from a set of documents by blurring the distinctions between the words. version:1
arxiv-1402-0459 | Applying Supervised Learning Algorithms and a New Feature Selection Method to Predict Coronary Artery Disease | http://arxiv.org/abs/1402.0459 | id:1402.0459 author:Hubert Haoyang Duan category:cs.LG stat.ML  published:2014-02-03 summary:From a fresh data science perspective, this thesis discusses the prediction of coronary artery disease based on genetic variations at the DNA base pair level, called Single-Nucleotide Polymorphisms (SNPs), collected from the Ontario Heart Genomics Study (OHGS). First, the thesis explains two commonly used supervised learning algorithms, the k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes a complete proof that the k-NN classifier is universally consistent in any finite dimensional normed vector space. Second, the thesis introduces two dimensionality reduction steps, Random Projections, a known feature extraction technique based on the Johnson-Lindenstrauss lemma, and a new method termed Mass Transportation Distance (MTD) Feature Selection for discrete domains. Then, this thesis compares the performance of Random Projections with the k-NN classifier against MTD Feature Selection and Random Forest, for predicting artery disease based on accuracy, the F-Measure, and area under the Receiver Operating Characteristic (ROC) curve. The comparative results demonstrate that MTD Feature Selection with Random Forest is vastly superior to Random Projections and k-NN. The Random Forest classifier is able to obtain an accuracy of 0.6660 and an area under the ROC curve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTD Feature Selection for classification. This area is considerably better than the previous high score of 0.608 obtained by Davies et al. in 2010 on the same dataset. version:1
arxiv-1402-0452 | A Lower Bound for the Variance of Estimators for Nakagami m Distribution | http://arxiv.org/abs/1402.0452 | id:1402.0452 author:Rangeet Mitra, Amit Kumar Mishra, Tarun Choubisa category:cs.LG  published:2014-02-03 summary:Recently, we have proposed a maximum likelihood iterative algorithm for estimation of the parameters of the Nakagami-m distribution. This technique performs better than state of art estimation techniques for this distribution. This could be of particular use in low data or block based estimation problems. In these scenarios, the estimator should be able to give accurate estimates in the mean square sense with less amounts of data. Also, the estimates should improve with the increase in number of blocks received. In this paper, we see through our simulations, that our proposal is well designed for such requirements. Further, it is well known in the literature that an efficient estimator does not exist for Nakagami-m distribution. In this paper, we derive a theoretical expression for the variance of our proposed estimator. We find that this expression clearly fits the experimental curve for the variance of the proposed estimator. This expression is pretty close to the cramer-rao lower bound(CRLB). version:1
arxiv-1402-0422 | A high-reproducibility and high-accuracy method for automated topic classification | http://arxiv.org/abs/1402.0422 | id:1402.0422 author:Andrea Lancichinetti, M. Irmak Sirer, Jane X. Wang, Daniel Acuna, Konrad Körding, Luís A. Nunes Amaral category:stat.ML cs.IR cs.LG physics.soc-ph  published:2014-02-03 summary:Much of human knowledge sits in large databases of unstructured text. Leveraging this knowledge requires algorithms that extract and record metadata on unstructured text documents. Assigning topics to documents will enable intelligent search, statistical characterization, and meaningful classification. Latent Dirichlet allocation (LDA) is the state-of-the-art in topic classification. Here, we perform a systematic theoretical and numerical analysis that demonstrates that current optimization techniques for LDA often yield results which are not accurate in inferring the most suitable model parameters. Adapting approaches for community detection in networks, we propose a new algorithm which displays high-reproducibility and high-accuracy, and also has high computational efficiency. We apply it to a large set of documents in the English Wikipedia and reveal its hierarchical structure. Our algorithm promises to make "big data" text analysis systems more reliable. version:1
arxiv-1402-0420 | Multidiscipinary Optimization For Gas Turbines Design | http://arxiv.org/abs/1402.0420 | id:1402.0420 author:Francesco Bertini, Lorenzo Dal Mas, Luca Vassio, Enrico Ampellio category:math.OC cs.NE  published:2014-02-03 summary:State-of-the-art aeronautic Low Pressure gas Turbines (LPTs) are already characterized by high quality standards, thus they offer very narrow margins of improvement. Typical design process starts with a Concept Design (CD) phase, defined using mean-line 1D and other low-order tools, and evolves through a Preliminary Design (PD) phase, which allows the geometric definition in details. In this framework, multidisciplinary optimization is the only way to properly handle the complicated peculiarities of the design. The authors present different strategies and algorithms that have been implemented exploiting the PD phase as a real-like design benchmark to illustrate results. The purpose of this work is to describe the optimization techniques, their settings and how to implement them effectively in a multidisciplinary environment. Starting from a basic gradient method and a semi-random second order method, the authors have introduced an Artificial Bee Colony-like optimizer, a multi-objective Genetic Diversity Evolutionary Algorithm [1] and a multi-objective response surface approach based on Artificial Neural Network, parallelizing and customizing them for the gas turbine study. Moreover, speedup and improvement arrangements are embedded in different hybrid strategies with the aim at finding the best solutions for different kind of problems that arise in this field. version:1
arxiv-1402-0808 | Associative Memories Based on Multiple-Valued Sparse Clustered Networks | http://arxiv.org/abs/1402.0808 | id:1402.0808 author:Hooman Jarollahi, Naoya Onizawa, Takahiro Hanyu, Warren J. Gross category:cs.NE  published:2014-02-03 summary:Associative memories are structures that store data patterns and retrieve them given partial inputs. Sparse Clustered Networks (SCNs) are recently-introduced binary-weighted associative memories that significantly improve the storage and retrieval capabilities over the prior state-of-the art. However, deleting or updating the data patterns result in a significant increase in the data retrieval error probability. In this paper, we propose an algorithm to address this problem by incorporating multiple-valued weights for the interconnections used in the network. The proposed algorithm lowers the error rate by an order of magnitude for our sample network with 60% deleted contents. We then investigate the advantages of the proposed algorithm for hardware implementations. version:1
arxiv-1209-3352 | Thompson Sampling for Contextual Bandits with Linear Payoffs | http://arxiv.org/abs/1209.3352 | id:1209.3352 author:Shipra Agrawal, Navin Goyal category:cs.LG cs.DS stat.ML 68W40  68Q25 F.2.0  published:2012-09-15 summary:Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state-of-the-art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze a generalization of Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is among the most important and widely studied versions of the contextual bandits problem. We provide the first theoretical guarantees for the contextual version of Thompson Sampling. We prove a high probability regret bound of $\tilde{O}(d^{3/2}\sqrt{T})$ (or $\tilde{O}(d\sqrt{T \log(N)})$), which is the best regret bound achieved by any computationally efficient algorithm available for this problem in the current literature, and is within a factor of $\sqrt{d}$ (or $\sqrt{\log(N)}$) of the information-theoretic lower bound for this problem. version:4
arxiv-1301-2609 | Learning to Optimize Via Posterior Sampling | http://arxiv.org/abs/1301.2609 | id:1301.2609 author:Daniel Russo, Benjamin Van Roy category:cs.LG  published:2013-01-11 summary:This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multi-armed bandit problems. The algorithm, also known as Thompson Sampling, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms. version:5
arxiv-1402-0289 | A Robust Framework for Moving-Object Detection and Vehicular Traffic Density Estimation | http://arxiv.org/abs/1402.0289 | id:1402.0289 author:Pranam Janney, Glenn Geers category:cs.CV cs.RO cs.SY  published:2014-02-03 summary:Intelligent machines require basic information such as moving-object detection from videos in order to deduce higher-level semantic information. In this paper, we propose a methodology that uses a texture measure to detect moving objects in video. The methodology is computationally inexpensive, requires minimal parameter fine-tuning and also is resilient to noise, illumination changes, dynamic background and low frame rate. Experimental results show that performance of the proposed approach is higher than those of state-of-the-art approaches. We also present a framework for vehicular traffic density estimation using the foreground object detection technique and present a comparison between the foreground object detection-based framework and the classical density state modelling-based framework for vehicular traffic density estimation. version:1
arxiv-1402-0288 | Transductive Learning with Multi-class Volume Approximation | http://arxiv.org/abs/1402.0288 | id:1402.0288 author:Gang Niu, Bo Dai, Marthinus Christoffel du Plessis, Masashi Sugiyama category:cs.LG stat.ML  published:2014-02-03 summary:Given a hypothesis space, the large volume principle by Vladimir Vapnik prioritizes equivalence classes according to their volume in the hypothesis space. The volume approximation has hitherto been successfully applied to binary learning problems. In this paper, we extend it naturally to a more general definition which can be applied to several transductive problem settings, such as multi-class, multi-label and serendipitous learning. Even though the resultant learning method involves a non-convex optimization problem, the globally optimal solution is almost surely unique and can be obtained in O(n^3) time. We theoretically provide stability and error analyses for the proposed method, and then experimentally show that it is promising. version:1
arxiv-1402-0282 | Principled Graph Matching Algorithms for Integrating Multiple Data Sources | http://arxiv.org/abs/1402.0282 | id:1402.0282 author:Duo Zhang, Benjamin I. P. Rubinstein, Jim Gemmell category:cs.DB cs.LG stat.ML  published:2014-02-03 summary:This paper explores combinatorial optimization for problems of max-weight graph matching on multi-partite graphs, which arise in integrating multiple data sources. Entity resolution-the data integration problem of performing noisy joins on structured data-typically proceeds by first hashing each record into zero or more blocks, scoring pairs of records that are co-blocked for similarity, and then matching pairs of sufficient similarity. In the most common case of matching two sources, it is often desirable for the final matching to be one-to-one (a record may be matched with at most one other); members of the database and statistical record linkage communities accomplish such matchings in the final stage by weighted bipartite graph matching on similarity scores. Such matchings are intuitively appealing: they leverage a natural global property of many real-world entity stores-that of being nearly deduped-and are known to provide significant improvements to precision and recall. Unfortunately unlike the bipartite case, exact max-weight matching on multi-partite graphs is known to be NP-hard. Our two-fold algorithmic contributions approximate multi-partite max-weight matching: our first algorithm borrows optimization techniques common to Bayesian probabilistic inference; our second is a greedy approximation algorithm. In addition to a theoretical guarantee on the latter, we present comparisons on a real-world ER problem from Bing significantly larger than typically found in the literature, publication data, and on a series of synthetic problems. Our results quantify significant improvements due to exploiting multiple sources, which are made possible by global one-to-one constraints linking otherwise independent matching sub-problems. We also discover that our algorithms are complementary: one being much more robust under noise, and the other being simple to implement and very fast to run. version:1
arxiv-1309-3256 | Recovery guarantees for exemplar-based clustering | http://arxiv.org/abs/1309.3256 | id:1309.3256 author:Abhinav Nellore, Rachel Ward category:stat.ML cs.CV cs.LG  published:2013-09-12 summary:For a certain class of distributions, we prove that the linear programming relaxation of $k$-medoids clustering---a variant of $k$-means clustering where means are replaced by exemplars from within the dataset---distinguishes points drawn from nonoverlapping balls with high probability once the number of points drawn and the separation distance between any two balls are sufficiently large. Our results hold in the nontrivial regime where the separation distance is small enough that points drawn from different balls may be closer to each other than points drawn from the same ball; in this case, clustering by thresholding pairwise distances between points can fail. We also exhibit numerical evidence of high-probability recovery in a substantially more permissive regime. version:2
arxiv-1312-6169 | Learning Information Spread in Content Networks | http://arxiv.org/abs/1312.6169 | id:1312.6169 author:Cédric Lagnier, Simon Bourigault, Sylvain Lamprier, Ludovic Denoyer, Patrick Gallinari category:cs.LG cs.SI physics.soc-ph  published:2013-12-20 summary:We introduce a model for predicting the diffusion of content information on social media. When propagation is usually modeled on discrete graph structures, we introduce here a continuous diffusion model, where nodes in a diffusion cascade are projected onto a latent space with the property that their proximity in this space reflects the temporal diffusion process. We focus on the task of predicting contaminated users for an initial initial information source and provide preliminary results on differents datasets. version:2
arxiv-1402-0170 | Collaborative Receptive Field Learning | http://arxiv.org/abs/1402.0170 | id:1402.0170 author:Shu Kong, Zhuolin Jiang, Qiang Yang category:cs.CV cs.LG cs.MM stat.ML  published:2014-02-02 summary:The challenge of object categorization in images is largely due to arbitrary translations and scales of the foreground objects. To attack this difficulty, we propose a new approach called collaborative receptive field learning to extract specific receptive fields (RF's) or regions from multiple images, and the selected RF's are supposed to focus on the foreground objects of a common category. To this end, we solve the problem by maximizing a submodular function over a similarity graph constructed by a pool of RF candidates. However, measuring pairwise distance of RF's for building the similarity graph is a nontrivial problem. Hence, we introduce a similarity metric called pyramid-error distance (PED) to measure their pairwise distances through summing up pyramid-like matching errors over a set of low-level features. Besides, in consistent with the proposed PED, we construct a simple nonparametric classifier for classification. Experimental results show that our method effectively discovers the foreground objects in images, and improves classification performance. version:1
arxiv-1401-2288 | Extension of Sparse Randomized Kaczmarz Algorithm for Multiple Measurement Vectors | http://arxiv.org/abs/1401.2288 | id:1401.2288 author:Hemant Kumar Aggarwal, Angshul Majumdar category:cs.NA cs.LG stat.ML  published:2014-01-10 summary:The Kaczmarz algorithm is popular for iteratively solving an overdetermined system of linear equations. The traditional Kaczmarz algorithm can approximate the solution in few sweeps through the equations but a randomized version of the Kaczmarz algorithm was shown to converge exponentially and independent of number of equations. Recently an algorithm for finding sparse solution to a linear system of equations has been proposed based on weighted randomized Kaczmarz algorithm. These algorithms solves single measurement vector problem; however there are applications were multiple-measurements are available. In this work, the objective is to solve a multiple measurement vector problem with common sparse support by modifying the randomized Kaczmarz algorithm. We have also modeled the problem of face recognition from video as the multiple measurement vector problem and solved using our proposed technique. We have compared the proposed algorithm with state-of-art spectral projected gradient algorithm for multiple measurement vectors on both real and synthetic datasets. The Monte Carlo simulations confirms that our proposed algorithm have better recovery and convergence rate than the MMV version of spectral projected gradient algorithm under fairness constraints. version:3
arxiv-1401-7715 | Video Compressive Sensing for Dynamic MRI | http://arxiv.org/abs/1401.7715 | id:1401.7715 author:Jianing V. Shi, Wotao Yin, Aswin C. Sankaranarayanan, Richard G. Baraniuk category:cs.CV math.OC  published:2014-01-30 summary:We present a video compressive sensing framework, termed kt-CSLDS, to accelerate the image acquisition process of dynamic magnetic resonance imaging (MRI). We are inspired by a state-of-the-art model for video compressive sensing that utilizes a linear dynamical system (LDS) to model the motion manifold. Given compressive measurements, the state sequence of an LDS can be first estimated using system identification techniques. We then reconstruct the observation matrix using a joint structured sparsity assumption. In particular, we minimize an objective function with a mixture of wavelet sparsity and joint sparsity within the observation matrix. We derive an efficient convex optimization algorithm through alternating direction method of multipliers (ADMM), and provide a theoretical guarantee for global convergence. We demonstrate the performance of our approach for video compressive sensing, in terms of reconstruction accuracy. We also investigate the impact of various sampling strategies. We apply this framework to accelerate the acquisition process of dynamic MRI and show it achieves the best reconstruction accuracy with the least computational time compared with existing algorithms in the literature. version:2
arxiv-1402-0099 | Dual-to-kernel learning with ideals | http://arxiv.org/abs/1402.0099 | id:1402.0099 author:Franz J. Király, Martin Kreuzer, Louis Theran category:stat.ML cs.LG math.AC math.AG math.ST stat.TH  published:2014-02-01 summary:In this paper, we propose a theory which unifies kernel learning and symbolic algebraic methods. We show that both worlds are inherently dual to each other, and we use this duality to combine the structure-awareness of algebraic methods with the efficiency and generality of kernels. The main idea lies in relating polynomial rings to feature space, and ideals to manifolds, then exploiting this generative-discriminative duality on kernel matrices. We illustrate this by proposing two algorithms, IPCA and AVICA, for simultaneous manifold and feature learning, and test their accuracy on synthetic and real world data. version:1
arxiv-1311-2663 | DinTucker: Scaling up Gaussian process models on multidimensional arrays with billions of elements | http://arxiv.org/abs/1311.2663 | id:1311.2663 author:Shandian Zhe, Yuan Qi, Youngja Park, Ian Molloy, Suresh Chari category:cs.LG cs.DC stat.ML  published:2013-11-12 summary:Infinite Tucker Decomposition (InfTucker) and random function prior models, as nonparametric Bayesian models on infinite exchangeable arrays, are more powerful models than widely-used multilinear factorization methods including Tucker and PARAFAC decomposition, (partly) due to their capability of modeling nonlinear relationships between array elements. Despite their great predictive performance and sound theoretical foundations, they cannot handle massive data due to a prohibitively high training time. To overcome this limitation, we present Distributed Infinite Tucker (DINTUCKER), a large-scale nonlinear tensor decomposition algorithm on MAPREDUCE. While maintaining the predictive accuracy of InfTucker, it is scalable on massive data. DINTUCKER is based on a new hierarchical Bayesian model that enables local training of InfTucker on subarrays and information integration from all local training results. We use distributed stochastic gradient descent, coupled with variational inference, to train this model. We apply DINTUCKER to multidimensional arrays with billions of elements from applications in the "Read the Web" project (Carlson et al., 2010) and in information security and compare it with the state-of-the-art large-scale tensor decomposition method, GigaTensor. On both datasets, DINTUCKER achieves significantly higher prediction accuracy with less computational time. version:5
arxiv-1306-2298 | Generative Model Selection Using a Scalable and Size-Independent Complex Network Classifier | http://arxiv.org/abs/1306.2298 | id:1306.2298 author:Sadegh Motallebi, Sadegh Aliakbary, Jafar Habibi category:cs.SI cs.LG physics.soc-ph stat.ML  published:2013-06-10 summary:Real networks exhibit nontrivial topological features such as heavy-tailed degree distribution, high clustering, and small-worldness. Researchers have developed several generative models for synthesizing artificial networks that are structurally similar to real networks. An important research problem is to identify the generative model that best fits to a target network. In this paper, we investigate this problem and our goal is to select the model that is able to generate graphs similar to a given network instance. By the means of generating synthetic networks with seven outstanding generative models, we have utilized machine learning methods to develop a decision tree for model selection. Our proposed method, which is named "Generative Model Selection for Complex Networks" (GMSCN), outperforms existing methods with respect to accuracy, scalability and size-independence. version:3
arxiv-1401-8269 | Experiments with Three Approaches to Recognizing Lexical Entailment | http://arxiv.org/abs/1401.8269 | id:1401.8269 author:Peter D. Turney, Saif M. Mohammad category:cs.CL cs.AI cs.LG H.3.1; I.2.6; I.2.7  published:2014-01-31 summary:Inference in natural language often involves recognizing lexical entailment (RLE); that is, identifying whether one word entails another. For example, "buy" entails "own". Two general strategies for RLE have been proposed: One strategy is to manually construct an asymmetric similarity measure for context vectors (directional similarity) and another is to treat RLE as a problem of learning to recognize semantic relations using supervised machine learning techniques (relation classification). In this paper, we experiment with two recent state-of-the-art representatives of the two general strategies. The first approach is an asymmetric similarity measure (an instance of the directional similarity strategy), designed to capture the degree to which the contexts of a word, a, form a subset of the contexts of another word, b. The second approach (an instance of the relation classification strategy) represents a word pair, a:b, with a feature vector that is the concatenation of the context vectors of a and b, and then applies supervised learning to a training set of labeled feature vectors. Additionally, we introduce a third approach that is a new instance of the relation classification strategy. The third approach represents a word pair, a:b, with a feature vector in which the features are the differences in the similarities of a and b to a set of reference words. All three approaches use vector space models (VSMs) of semantics, based on word-context matrices. We perform an extensive evaluation of the three approaches using three different datasets. The proposed new approach (similarity differences) performs significantly better than the other two approaches on some datasets and there is no dataset for which it is significantly worse. Our results suggest it is beneficial to make connections between the research in lexical entailment and the research in semantic relation classification. version:1
arxiv-1310-4822 | Principal motion components for gesture recognition using a single-example | http://arxiv.org/abs/1310.4822 | id:1310.4822 author:Hugo Jair Escalante, Isabelle Guyon, Vassilis Athitsos, Pat Jangyodsuk, Jun Wan category:cs.CV 68T45  published:2013-10-17 summary:This paper introduces principal motion components (PMC), a new method for one-shot gesture recognition. In the considered scenario a single training-video is available for each gesture to be recognized, which limits the application of traditional techniques (e.g., HMMs). In PMC, a 2D map of motion energy is obtained per each pair of consecutive frames in a video. Motion maps associated to a video are processed to obtain a PCA model, which is used for recognition under a reconstruction-error approach. The main benefits of the proposed approach are its simplicity, easiness of implementation, competitive performance and efficiency. We report experimental results in one-shot gesture recognition using the ChaLearn Gesture Dataset; a benchmark comprising more than 50,000 gestures, recorded as both RGB and depth video with a Kinect camera. Results obtained with PMC are competitive with alternative methods proposed for the same data set. version:2
arxiv-1401-8078 | Marginal and simultaneous predictive classification using stratified graphical models | http://arxiv.org/abs/1401.8078 | id:1401.8078 author:Henrik Nyman, Jie Xiong, Johan Pensar, Jukka Corander category:stat.ML  published:2014-01-31 summary:An inductive probabilistic classification rule must generally obey the principles of Bayesian predictive inference, such that all observed and unobserved stochastic quantities are jointly modeled and the parameter uncertainty is fully acknowledged through the posterior predictive distribution. Several such rules have been recently considered and their asymptotic behavior has been characterized under the assumption that the observed features or variables used for building a classifier are conditionally independent given a simultaneous labeling of both the training samples and those from an unknown origin. Here we extend the theoretical results to predictive classifiers acknowledging feature dependencies either through graphical models or sparser alternatives defined as stratified graphical models. We also show through experimentation with both synthetic and real data that the predictive classifiers based on stratified graphical models have consistently best accuracy compared with the predictive classifiers based on either conditionally independent features or on ordinary graphical models. version:1
arxiv-1401-8074 | Empirically Evaluating Multiagent Learning Algorithms | http://arxiv.org/abs/1401.8074 | id:1401.8074 author:Erik Zawadzki, Asher Lipson, Kevin Leyton-Brown category:cs.GT cs.LG  published:2014-01-31 summary:There exist many algorithms for learning how to play repeated bimatrix games. Most of these algorithms are justified in terms of some sort of theoretical guarantee. On the other hand, little is known about the empirical performance of these algorithms. Most such claims in the literature are based on small experiments, which has hampered understanding as well as the development of new multiagent learning (MAL) algorithms. We have developed a new suite of tools for running multiagent experiments: the MultiAgent Learning Testbed (MALT). These tools are designed to facilitate larger and more comprehensive experiments by removing the need to build one-off experimental code. MALT also provides baseline implementations of many MAL algorithms, hopefully eliminating or reducing differences between algorithm implementations and increasing the reproducibility of results. Using this test suite, we ran an experiment unprecedented in size. We analyzed the results according to a variety of performance metrics including reward, maxmin distance, regret, and several notions of equilibrium convergence. We confirmed several pieces of conventional wisdom, but also discovered some surprising results. For example, we found that single-agent $Q$-learning outperformed many more complicated and more modern MAL algorithms. version:1
arxiv-1401-8053 | Hallucinating optimal high-dimensional subspaces | http://arxiv.org/abs/1401.8053 | id:1401.8053 author:Ognjen Arandjelovic category:cs.CV  published:2014-01-31 summary:Linear subspace representations of appearance variation are pervasive in computer vision. This paper addresses the problem of robustly matching such subspaces (computing the similarity between them) when they are used to describe the scope of variations within sets of images of different (possibly greatly so) scales. A naive solution of projecting the low-scale subspace into the high-scale image space is described first and subsequently shown to be inadequate, especially at large scale discrepancies. A successful approach is proposed instead. It consists of (i) an interpolated projection of the low-scale subspace into the high-scale space, which is followed by (ii) a rotation of this initial estimate within the bounds of the imposed ``downsampling constraint''. The optimal rotation is found in the closed-form which best aligns the high-scale reconstruction of the low-scale subspace with the reference it is compared to. The method is evaluated on the problem of matching sets of (i) face appearances under varying illumination and (ii) object appearances under varying viewpoint, using two large data sets. In comparison to the naive matching, the proposed algorithm is shown to greatly increase the separation of between-class and within-class similarities, as well as produce far more meaningful modes of common appearance on which the match score is based. version:1
arxiv-1401-8017 | Sparse Bayesian Unsupervised Learning | http://arxiv.org/abs/1401.8017 | id:1401.8017 author:Stephane Gaiffas, Bertrand Michel category:stat.ML 62H30 G.3; H.3.3; I.5.3  published:2014-01-30 summary:This paper is about variable selection, clustering and estimation in an unsupervised high-dimensional setting. Our approach is based on fitting constrained Gaussian mixture models, where we learn the number of clusters $K$ and the set of relevant variables $S$ using a generalized Bayesian posterior with a sparsity inducing prior. We prove a sparsity oracle inequality which shows that this procedure selects the optimal parameters $K$ and $S$. This procedure is implemented using a Metropolis-Hastings algorithm, based on a clustering-oriented greedy proposal, which makes the convergence to the posterior very fast. version:1
arxiv-1401-8008 | Support vector comparison machines | http://arxiv.org/abs/1401.8008 | id:1401.8008 author:Toby Dylan Hocking, Supaporn Spanurattana, Masashi Sugiyama category:stat.ML cs.LG  published:2014-01-30 summary:In ranking problems, the goal is to learn a ranking function from labeled pairs of input points. In this paper, we consider the related comparison problem, where the label indicates which element of the pair is better, or if there is no significant difference. We cast the learning problem as a margin maximization, and show that it can be solved by converting it to a standard SVM. We use simulated nonlinear patterns and a real learning to rank sushi data set to show that our proposed SVMcompare algorithm outperforms SVMrank when there are equality pairs. version:1
arxiv-1401-8212 | Human Activity Recognition using Smartphone | http://arxiv.org/abs/1401.8212 | id:1401.8212 author:Amin Rasekh, Chien-An Chen, Yan Lu category:cs.CY cs.HC cs.LG  published:2014-01-30 summary:Human activity recognition has wide applications in medical research and human survey system. In this project, we design a robust activity recognition system based on a smartphone. The system uses a 3-dimentional smartphone accelerometer as the only sensor to collect time series signals, from which 31 features are generated in both time and frequency domain. Activities are classified using 4 different passive learning methods, i.e., quadratic classifier, k-nearest neighbor algorithm, support vector machine, and artificial neural networks. Dimensionality reduction is performed through both feature extraction and subset selection. Besides passive learning, we also apply active learning algorithms to reduce data labeling expense. Experiment results show that the classification rate of passive learning reaches 84.4% and it is robust to common positions and poses of cellphone. The results of active learning on real data demonstrate a reduction of labeling labor to achieve comparable performance with passive learning. version:1
arxiv-1401-7898 | Maximum Margin Multiclass Nearest Neighbors | http://arxiv.org/abs/1401.7898 | id:1401.7898 author:Aryeh Kontorovich, Roi Weiss category:cs.LG math.ST stat.TH  published:2014-01-30 summary:We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size $n$ and significantly improve the dependence on the number of classes $k$. Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of $k$. Although $k$-free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on $k$. As the best previous risk estimates in this setting were of order $\sqrt k$, our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on $n$ examples in $O(n^2\log n)$ time and evaluated on new points in $O(\log n)$ time. version:1
arxiv-1401-7743 | Effective Features of Remote Sensing Image Classification Using Interactive Adaptive Thresholding Method | http://arxiv.org/abs/1401.7743 | id:1401.7743 author:T. Balaji, Dr. M. Sumathi category:cs.CV  published:2014-01-30 summary:Remote sensing image classification can be performed in many different ways to extract meaningful features. One common approach is to perform edge detection. A second approach is to try and detect whole shapes, given the fact that these shapes usually tend to have distinctive properties such as object foreground or background. To get optimal results, these two approaches can be combined. This paper adopts a combinatorial optimization method to adaptively select threshold based features to improve remote sensing image. Feature selection is an important combinatorial optimization problem in the remote sensing image classification. The feature selection method has to achieve three characteristics: first the performance issues by facilitating data collection and reducing storage space and classification time, second to perform semantics analysis helping to understand the problem, and third to improve prediction accuracy by avoiding the curse of dimensionality. The goal of this thresholding an image is to classify pixels as either dark or light and evaluation of classification results. Interactive adaptive thresholding is a form of thresholding that takes into account spatial variations in illumination of remote sensing image. We present a technique for remote sensing based adaptive thresholding using the interactive satellite image of the input. However, our solution is more robust to illumination changes in the remote sensing image. Additionally, our method is simple and easy to implement but it is effective algorithm to classify the image pixels. This technique is suitable for preprocessing the remote sensing image classification, making it a valuable tool for interactive remote based applications such as augmented reality of the classification procedure. version:1
arxiv-1401-7727 | Security Evaluation of Support Vector Machines in Adversarial Environments | http://arxiv.org/abs/1401.7727 | id:1401.7727 author:Battista Biggio, Igino Corona, Blaine Nelson, Benjamin I. P. Rubinstein, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli category:cs.LG cs.CR  published:2014-01-30 summary:Support Vector Machines (SVMs) are among the most popular classification techniques adopted in security applications like malware detection, intrusion detection, and spam filtering. However, if SVMs are to be incorporated in real-world security systems, they must be able to cope with attack patterns that can either mislead the learning algorithm (poisoning), evade detection (evasion), or gain information about their internal parameters (privacy breaches). The main contributions of this chapter are twofold. First, we introduce a formal general framework for the empirical evaluation of the security of machine-learning systems. Second, according to our framework, we demonstrate the feasibility of evasion, poisoning and privacy attacks against SVMs in real-world security problems. For each attack technique, we evaluate its impact and discuss whether (and how) it can be countered through an adversary-aware design of SVMs. Our experiments are easily reproducible thanks to open-source code that we have made available, together with all the employed datasets, on a public repository. version:1
arxiv-1401-7713 | A Generalized Probabilistic Framework for Compact Codebook Creation | http://arxiv.org/abs/1401.7713 | id:1401.7713 author:Lingqiao Liu, Lei Wang, Chunhua Shen category:cs.CV  published:2014-01-30 summary:Compact and discriminative visual codebooks are preferred in many visual recognition tasks. In the literature, a number of works have taken the approach of hierarchically merging visual words of an initial large-sized codebook, but implemented this approach with different merging criteria. In this work, we propose a single probabilistic framework to unify these merging criteria, by identifying two key factors: the function used to model class-conditional distribution and the method used to estimate the distribution parameters. More importantly, by adopting new distribution functions and/or parameter estimation methods, our framework can readily produce a spectrum of novel merging criteria. Three of them are specifically focused in this work. In the first criterion, we adopt the multinomial distribution with Bayesian method; In the second criterion, we integrate Gaussian distribution with maximum likelihood parameter estimation. In the third criterion, which shows the best merging performance, we propose a max-margin-based parameter estimation method and apply it with multinomial distribution. Extensive experimental study is conducted to systematically analyse the performance of the above three criteria and compare them with existing ones. As demonstrated, the best criterion obtained in our framework achieves the overall best merging performance among the comparable merging criteria developed in the literature. version:1
arxiv-1401-7709 | Joint Inference of Multiple Label Types in Large Networks | http://arxiv.org/abs/1401.7709 | id:1401.7709 author:Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, Sofus A. Macskassy category:cs.LG cs.SI stat.ML  published:2014-01-30 summary:We tackle the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers, for users connected by a social network. Standard label propagation fails to consider the properties of the label types and the interactions between them. Our proposed method, called EdgeExplain, explicitly models these, while still enabling scalable inference under a distributed message-passing architecture. On a billion-node subset of the Facebook social network, EdgeExplain significantly outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3. version:1
arxiv-1311-2334 | Embed and Conquer: Scalable Embeddings for Kernel k-Means on MapReduce | http://arxiv.org/abs/1311.2334 | id:1311.2334 author:Ahmed Elgohary, Ahmed K. Farahat, Mohamed S. Kamel, Fakhri Karray category:cs.LG  published:2013-11-11 summary:The kernel $k$-means is an effective method for data clustering which extends the commonly-used $k$-means algorithm to work on a similarity matrix over complex data structures. The kernel $k$-means algorithm is however computationally very complex as it requires the complete data matrix to be calculated and stored. Further, the kernelized nature of the kernel $k$-means algorithm hinders the parallelization of its computations on modern infrastructures for distributed computing. In this paper, we are defining a family of kernel-based low-dimensional embeddings that allows for scaling kernel $k$-means on MapReduce via an efficient and unified parallelization strategy. Afterwards, we propose two methods for low-dimensional embedding that adhere to our definition of the embedding family. Exploiting the proposed parallelization strategy, we present two scalable MapReduce algorithms for kernel $k$-means. We demonstrate the effectiveness and efficiency of the proposed algorithms through an empirical evaluation on benchmark data sets. version:4
arxiv-1401-7625 | RES: Regularized Stochastic BFGS Algorithm | http://arxiv.org/abs/1401.7625 | id:1401.7625 author:Aryan Mokhtari, Alejandro Ribeiro category:cs.LG math.OC stat.ML  published:2014-01-29 summary:RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method is proposed to solve convex optimization problems with stochastic objectives. The use of stochastic gradient descent algorithms is widespread, but the number of iterations required to approximate optimal arguments can be prohibitive in high dimensional problems. Application of second order methods, on the other hand, is impracticable because computation of objective function Hessian inverses incurs excessive computational cost. BFGS modifies gradient descent by introducing a Hessian approximation matrix computed from finite gradient differences. RES utilizes stochastic gradients in lieu of deterministic gradients for both, the determination of descent directions and the approximation of the objective function's curvature. Since stochastic gradients can be computed at manageable computational cost RES is realizable and retains the convergence rate advantages of its deterministic counterparts. Convergence results show that lower and upper bounds on the Hessian egeinvalues of the sample functions are sufficient to guarantee convergence to optimal arguments. Numerical experiments showcase reductions in convergence time relative to stochastic gradient descent algorithms and non-regularized stochastic versions of BFGS. An application of RES to the implementation of support vector machines is developed. version:1
arxiv-1401-7620 | Bayesian nonparametric comorbidity analysis of psychiatric disorders | http://arxiv.org/abs/1401.7620 | id:1401.7620 author:Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fernando Perez-Cruz category:stat.ML cs.LG  published:2014-01-29 summary:The analysis of comorbidity is an open and complex research field in the branch of psychiatry, where clinical experience and several studies suggest that the relation among the psychiatric disorders may have etiological and treatment implications. In this paper, we are interested in applying latent feature modeling to find the latent structure behind the psychiatric disorders that can help to examine and explain the relationships among them. To this end, we use the large amount of information collected in the National Epidemiologic Survey on Alcohol and Related Conditions (NESARC) database and propose to model these data using a nonparametric latent model based on the Indian Buffet Process (IBP). Due to the discrete nature of the data, we first need to adapt the observation model for discrete random variables. We propose a generative model in which the observations are drawn from a multinomial-logit distribution given the IBP matrix. The implementation of an efficient Gibbs sampler is accomplished using the Laplace approximation, which allows integrating out the weighting factors of the multinomial-logit likelihood model. We also provide a variational inference algorithm for this model, which provides a complementary (and less expensive in terms of computational complexity) alternative to the Gibbs sampler allowing us to deal with a larger number of data. Finally, we use the model to analyze comorbidity among the psychiatric disorders diagnosed by experts from the NESARC database. version:1
arxiv-1401-7517 | Information quantity in a pixel of digital image | http://arxiv.org/abs/1401.7517 | id:1401.7517 author:M. Kharinov category:cs.CV cs.IT math.IT  published:2014-01-29 summary:The paper is devoted to the problem of integer-valued estimating of information quantity in a pixel of digital image. The definition of an integer estimation of information quantity based on constructing of the certain binary hierarchy of pixel clusters is proposed. The methods for constructing hierarchies of clusters and generating of hierarchical sequences of image approximations that minimally differ from the image by a standard deviation are developed. Experimental results on integer-valued estimation of information quantity are compared with the results obtained by utilizing of the classical formulas. version:1
arxiv-1401-7486 | Use HMM and KNN for classifying corneal data | http://arxiv.org/abs/1401.7486 | id:1401.7486 author:Payam Porkar Rezaeiye, mehrnoosh bazrafkan, ali akbar movassagh, Mojtaba Sedigh Fazli, Gholam hossein bazyari category:cs.CV  published:2014-01-29 summary:These days to gain classification system with high accuracy that can classify complicated pattern are so useful in medicine and industry. In this article a process for getting the best classifier for Lasik data is suggested. However at first it's been tried to find the best line and curve by this classifier in order to gain classifier fitting, and in the end by using the Markov method a classifier for topographies is gained. version:1
arxiv-1401-7406 | The parametrized probabilistic finite-state transducer probe game player fingerprint model | http://arxiv.org/abs/1401.7406 | id:1401.7406 author:Jeffrey Tsang category:cs.GT cs.NE  published:2014-01-29 summary:Fingerprinting operators generate functional signatures of game players and are useful for their automated analysis independent of representation or encoding. The theory for a fingerprinting operator which returns the length-weighted probability of a given move pair occurring from playing the investigated agent against a general parametrized probabilistic finite-state transducer (PFT) is developed, applicable to arbitrary iterated games. Results for the distinguishing power of the 1-state opponent model, uniform approximability of fingerprints of arbitrary players, analyticity and Lipschitz continuity of fingerprints for logically possible players, and equicontinuity of the fingerprints of bounded-state probabilistic transducers are derived. Algorithms for the efficient computation of special instances are given; the shortcomings of a previous model, strictly generalized here from a simple projection of the new model, are explained in terms of regularity condition violations, and the extra power and functional niceness of the new fingerprints demonstrated. The 2-state deterministic finite-state transducers (DFTs) are fingerprinted and pairwise distances computed; using this the structure of DFTs in strategy space is elucidated. version:1
arxiv-1401-7388 | Bounding Embeddings of VC Classes into Maximum Classes | http://arxiv.org/abs/1401.7388 | id:1401.7388 author:J. Hyam Rubinstein, Benjamin I. P. Rubinstein, Peter L. Bartlett category:cs.LG math.CO stat.ML  published:2014-01-29 summary:One of the earliest conjectures in computational learning theory-the Sample Compression conjecture-asserts that concept classes (equivalently set systems) admit compression schemes of size linear in their VC dimension. To-date this statement is known to be true for maximum classes---those that possess maximum cardinality for their VC dimension. The most promising approach to positively resolving the conjecture is by embedding general VC classes into maximum classes without super-linear increase to their VC dimensions, as such embeddings would extend the known compression schemes to all VC classes. We show that maximum classes can be characterised by a local-connectivity property of the graph obtained by viewing the class as a cubical complex. This geometric characterisation of maximum VC classes is applied to prove a negative embedding result which demonstrates VC-d classes that cannot be embedded in any maximum class of VC dimension lower than 2d. On the other hand, we show that every VC-d class C embeds in a VC-(d+D) maximum class where D is the deficiency of C, i.e., the difference between the cardinalities of a maximum VC-d class and of C. For VC-2 classes in binary n-cubes for 4 <= n <= 6, we give best possible results on embedding into maximum classes. For some special classes of Boolean functions, relationships with maximum classes are investigated. Finally we give a general recursive procedure for embedding VC-d classes into VC-(d+k) maximum classes for smallest k. version:1
arxiv-1401-1974 | Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts | http://arxiv.org/abs/1401.1974 | id:1401.1974 author:Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui category:cs.LG stat.ML  published:2014-01-09 summary:We present a Bayesian nonparametric framework for multilevel clustering which utilizes group-level context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polya-urn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains. version:4
arxiv-1401-8261 | Infrared face recognition: a comprehensive review of methodologies and databases | http://arxiv.org/abs/1401.8261 | id:1401.8261 author:Reza Shoja Ghiass, Ognjen Arandjelovic, Hakim Bendada, Xavier Maldague category:cs.CV  published:2014-01-29 summary:Automatic face recognition is an area with immense practical potential which includes a wide range of commercial and law enforcement applications. Hence it is unsurprising that it continues to be one of the most active research areas of computer vision. Even after over three decades of intense research, the state-of-the-art in face recognition continues to improve, benefitting from advances in a range of different research fields such as image processing, pattern recognition, computer graphics, and physiology. Systems based on visible spectrum images, the most researched face recognition modality, have reached a significant level of maturity with some practical success. However, they continue to face challenges in the presence of illumination, pose and expression changes, as well as facial disguises, all of which can significantly decrease recognition accuracy. Amongst various approaches which have been proposed in an attempt to overcome these limitations, the use of infrared (IR) imaging has emerged as a particularly promising research direction. This paper presents a comprehensive and timely review of the literature on this subject. Our key contributions are: (i) a summary of the inherent properties of infrared imaging which makes this modality promising in the context of face recognition, (ii) a systematic review of the most influential approaches, with a focus on emerging common trends as well as key differences between alternative methodologies, (iii) a description of the main databases of infrared facial images available to the researcher, and lastly (iv) a discussion of the most promising avenues for future research. version:1
arxiv-1312-4461 | Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks | http://arxiv.org/abs/1312.4461 | id:1312.4461 author:Andrew Davis, Itamar Arel category:cs.LG  published:2013-12-16 summary:Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced by Bengio, et. al., where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process. version:4
arxiv-1303-7461 | Universal Approximation Depth and Errors of Narrow Belief Networks with Discrete Units | http://arxiv.org/abs/1303.7461 | id:1303.7461 author:Guido F. Montúfar category:stat.ML cs.LG math.PR 82C32  60C05  68Q32  published:2013-03-29 summary:We generalize recent theoretical work on the minimal number of layers of narrow deep belief networks that can approximate any probability distribution on the states of their visible units arbitrarily well. We relax the setting of binary units (Sutskever and Hinton, 2008; Le Roux and Bengio, 2008, 2010; Mont\'ufar and Ay, 2011) to units with arbitrary finite state spaces, and the vanishing approximation error to an arbitrary approximation error tolerance. For example, we show that a $q$-ary deep belief network with $L\geq 2+\frac{q^{\lceil m-\delta \rceil}-1}{q-1}$ layers of width $n \leq m + \log_q(m) + 1$ for some $m\in \mathbb{N}$ can approximate any probability distribution on $\{0,1,\ldots,q-1\}^n$ without exceeding a Kullback-Leibler divergence of $\delta$. Our analysis covers discrete restricted Boltzmann machines and na\"ive Bayes models as special cases. version:2
arxiv-1401-7145 | Tempering by Subsampling | http://arxiv.org/abs/1401.7145 | id:1401.7145 author:Jan-Willem van de Meent, Brooks Paige, Frank Wood category:stat.ML  published:2014-01-28 summary:In this paper we demonstrate that tempering Markov chain Monte Carlo samplers for Bayesian models by recursively subsampling observations without replacement can improve the performance of baseline samplers in terms of effective sample size per computation. We present two tempering by subsampling algorithms, subsampled parallel tempering and subsampled tempered transitions. We provide an asymptotic analysis of the computational cost of tempering by subsampling, verify that tempering by subsampling costs less than traditional tempering, and demonstrate both algorithms on Bayesian approaches to learning the mean of a high dimensional multivariate Normal and estimating Gaussian process hyperparameters. version:1
arxiv-1401-7116 | Bayesian Properties of Normalized Maximum Likelihood and its Fast Computation | http://arxiv.org/abs/1401.7116 | id:1401.7116 author:Andrew Barron, Teemu Roos, Kazuho Watanabe category:cs.IT cs.LG math.IT stat.ML  published:2014-01-28 summary:The normalized maximized likelihood (NML) provides the minimax regret solution in universal data compression, gambling, and prediction, and it plays an essential role in the minimum description length (MDL) method of statistical modeling and estimation. Here we show that the normalized maximum likelihood has a Bayes-like representation as a mixture of the component models, even in finite samples, though the weights of linear combination may be both positive and negative. This representation addresses in part the relationship between MDL and Bayes modeling. This representation has the advantage of speeding the calculation of marginals and conditionals required for coding and prediction applications. version:1
arxiv-1401-6984 | Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN | http://arxiv.org/abs/1401.6984 | id:1401.6984 author:Yajie Miao category:cs.LG cs.CL  published:2014-01-27 summary:The Kaldi toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our open-source recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve. version:1
arxiv-1301-1318 | Efficient Eigen-updating for Spectral Graph Clustering | http://arxiv.org/abs/1301.1318 | id:1301.1318 author:Charanpal Dhanjal, Romaric Gaudel, Stéphan Clémençon category:stat.ML  published:2013-01-07 summary:Partitioning a graph into groups of vertices such that those within each group are more densely connected than vertices assigned to different groups, known as graph clustering, is often used to gain insight into the organisation of large scale networks and for visualisation purposes. Whereas a large number of dedicated techniques have been recently proposed for static graphs, the design of on-line graph clustering methods tailored for evolving networks is a challenging problem, and much less documented in the literature. Motivated by the broad variety of applications concerned, ranging from the study of biological networks to the analysis of networks of scientific references through the exploration of communications networks such as the World Wide Web, it is the main purpose of this paper to introduce a novel, computationally efficient, approach to graph clustering in the evolutionary context. Namely, the method promoted in this article can be viewed as an incremental eigenvalue solution for the spectral clustering method described by Ng. et al. (2001). The incremental eigenvalue solution is a general technique for finding the approximate eigenvectors of a symmetric matrix given a change. As well as outlining the approach in detail, we present a theoretical bound on the quality of the approximate eigenvectors using perturbation theory. We then derive a novel spectral clustering algorithm called Incremental Approximate Spectral Clustering (IASC). The IASC algorithm is simple to implement and its efficacy is demonstrated on both synthetic and real datasets modelling the evolution of a HIV epidemic, a citation network and the purchase history graph of an e-commerce website. version:4
arxiv-1309-0790 | SKYNET: an efficient and robust neural network training tool for machine learning in astronomy | http://arxiv.org/abs/1309.0790 | id:1309.0790 author:Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony N. Lasenby category:astro-ph.IM cs.LG cs.NE physics.data-an stat.ML  published:2013-09-03 summary:We present the first public release of our generic neural network training algorithm, called SkyNet. This efficient and robust machine learning tool is able to train large and deep feed-forward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SkyNet uses a `pre-training' method to obtain a set of network parameters that has empirically been shown to be close to a good solution, followed by further optimisation using a regularised variant of Newton's method, where the level of regularisation is determined and adjusted automatically; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques. SkyNet employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SkyNet are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SkyNet software, which is implemented in standard ANSI C and fully parallelised using MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/. version:2
arxiv-1401-6929 | Computing support for advanced medical data analysis and imaging | http://arxiv.org/abs/1401.6929 | id:1401.6929 author:W. Wiślicki, T. Bednarski, P. Białas, E. Czerwiński, Ł. Kapłon, A. Kochanowski, G. Korcyl, J. Kowal, P. Kowalski, T. Kozik, W. Krzemień, M. Molenda, P. Moskal, S. Niedźwiecki, M. Pałka, M. Pawlik, L. Raczyński, Z. Rudy, P. Salabura, N. G. Sharma, M. Silarski, A. Słomski, J. Smyrski, A. Strzelecki, A. Wieczorek, M. Zieliński, N. Zoń category:physics.comp-ph cs.CV cs.DC physics.ins-det physics.med-ph  published:2014-01-27 summary:We discuss computing issues for data analysis and image reconstruction of PET-TOF medical scanner or other medical scanning devices producing large volumes of data. Service architecture based on the grid and cloud concepts for distributed processing is proposed and critically discussed. version:1
arxiv-1209-5860 | Reversible MCMC on Markov equivalence classes of sparse directed acyclic graphs | http://arxiv.org/abs/1209.5860 | id:1209.5860 author:Yangbo He, Jinzhu Jia, Bin Yu category:stat.ML cs.DM stat.ME  published:2012-09-26 summary:Graphical models are popular statistical tools which are used to represent dependent or causal complex systems. Statistically equivalent causal or directed graphical models are said to belong to a Markov equivalent class. It is of great interest to describe and understand the space of such classes. However, with currently known algorithms, sampling over such classes is only feasible for graphs with fewer than approximately 20 vertices. In this paper, we design reversible irreducible Markov chains on the space of Markov equivalent classes by proposing a perfect set of operators that determine the transitions of the Markov chain. The stationary distribution of a proposed Markov chain has a closed form and can be computed easily. Specifically, we construct a concrete perfect set of operators on sparse Markov equivalence classes by introducing appropriate conditions on each possible operator. Algorithms and their accelerated versions are provided to efficiently generate Markov chains and to explore properties of Markov equivalence classes of sparse directed acyclic graphs (DAGs) with thousands of vertices. We find experimentally that in most Markov equivalence classes of sparse DAGs, (1) most edges are directed, (2) most undirected subgraphs are small and (3) the number of these undirected subgraphs grows approximately linearly with the number of vertices. The article contains supplement arXiv:1303.0632, http://dx.doi.org/10.1214/13-AOS1125SUPP version:3
arxiv-1401-6740 | Safe Sample Screening for Support Vector Machines | http://arxiv.org/abs/1401.6740 | id:1401.6740 author:Kohei Ogawa, Yoshiki Suzuki, Shinya Suzumura, Ichiro Takeuchi category:stat.ML  published:2014-01-27 summary:Sparse classifiers such as the support vector machines (SVM) are efficient in test-phases because the classifier is characterized only by a subset of the samples called support vectors (SVs), and the rest of the samples (non SVs) have no influence on the classification result. However, the advantage of the sparsity has not been fully exploited in training phases because it is generally difficult to know which sample turns out to be SV beforehand. In this paper, we introduce a new approach called safe sample screening that enables us to identify a subset of the non-SVs and screen them out prior to the training phase. Our approach is different from existing heuristic approaches in the sense that the screened samples are guaranteed to be non-SVs at the optimal solution. We investigate the advantage of the safe sample screening approach through intensive numerical experiments, and demonstrate that it can substantially decrease the computational cost of the state-of-the-art SVM solvers such as LIBSVM. In the current big data era, we believe that safe sample screening would be of great practical importance since the data size can be reduced without sacrificing the optimality of the final solution. version:1
arxiv-1309-1536 | Rank-frequency relation for Chinese characters | http://arxiv.org/abs/1309.1536 | id:1309.1536 author:W. B. Deng, A. E. Allahverdyan, B. Li, Q. A. Wang category:cs.CL physics.data-an  published:2013-09-06 summary:We show that the Zipf's law for Chinese characters perfectly holds for sufficiently short texts (few thousand different characters). The scenario of its validity is similar to the Zipf's law for words in short English texts. For long Chinese texts (or for mixtures of short Chinese texts), rank-frequency relations for Chinese characters display a two-layer, hierarchic structure that combines a Zipfian power-law regime for frequent characters (first layer) with an exponential-like regime for less frequent characters (second layer). For these two layers we provide different (though related) theoretical descriptions that include the range of low-frequency characters (hapax legomena). The comparative analysis of rank-frequency relations for Chinese characters versus English words illustrates the extent to which the characters play for Chinese writers the same role as the words for those writing within alphabetical systems. version:2
arxiv-1401-6638 | Painting Analysis Using Wavelets and Probabilistic Topic Models | http://arxiv.org/abs/1401.6638 | id:1401.6638 author:Tong Wu, Gungor Polatkan, David Steel, William Brown, Ingrid Daubechies, Robert Calderbank category:cs.CV cs.LG stat.ML  published:2014-01-26 summary:In this paper, computer-based techniques for stylistic analysis of paintings are applied to the five panels of the 14th century Peruzzi Altarpiece by Giotto di Bondone. Features are extracted by combining a dual-tree complex wavelet transform with a hidden Markov tree (HMT) model. Hierarchical clustering is used to identify stylistic keywords in image patches, and keyword frequencies are calculated for sub-images that each contains many patches. A generative hierarchical Bayesian model learns stylistic patterns of keywords; these patterns are then used to characterize the styles of the sub-images; this in turn, permits to discriminate between paintings. Results suggest that such unsupervised probabilistic topic models can be useful to distill characteristic elements of style. version:1
arxiv-1401-6597 | Ensembled Correlation Between Liver Analysis Outputs | http://arxiv.org/abs/1401.6597 | id:1401.6597 author:Sadi Evren Seker, Y. Unal, Z. Erdem, H. Erdinc Kocer category:stat.ML cs.CE cs.LG  published:2014-01-25 summary:Data mining techniques on the biological analysis are spreading for most of the areas including the health care and medical information. We have applied the data mining techniques, such as KNN, SVM, MLP or decision trees over a unique dataset, which is collected from 16,380 analysis results for a year. Furthermore we have also used meta-classifiers to question the increased correlation rate between the liver disorder and the liver analysis outputs. The results show that there is a correlation among ALT, AST, Billirubin Direct and Billirubin Total down to 15% of error rate. Also the correlation coefficient is up to 94%. This makes possible to predict the analysis results from each other or disease patterns can be applied over the linear correlation of the parameters. version:1
arxiv-1401-6574 | Category theory, logic and formal linguistics: some connections, old and new | http://arxiv.org/abs/1401.6574 | id:1401.6574 author:Jean Gillibert, Christian Retoré category:math.CT cs.CL cs.LO math.LO  published:2014-01-25 summary:We seize the opportunity of the publication of selected papers from the \emph{Logic, categories, semantics} workshop in the \emph{Journal of Applied Logic} to survey some current trends in logic, namely intuitionistic and linear type theories, that interweave categorical, geometrical and computational considerations. We thereafter present how these rich logical frameworks can model the way language conveys meaning. version:1
arxiv-1401-6573 | Deverbal semantics and the Montagovian generative lexicon | http://arxiv.org/abs/1401.6573 | id:1401.6573 author:Livy-Maria Real-Coelho, Christian Retoré category:cs.CL cs.LO  published:2014-01-25 summary:We propose a lexical account of action nominals, in particular of deverbal nominalisations, whose meaning is related to the event expressed by their base verb. The literature about nominalisations often assumes that the semantics of the base verb completely defines the structure of action nominals. We argue that the information in the base verb is not sufficient to completely determine the semantics of action nominals. We exhibit some data from different languages, especially from Romance language, which show that nominalisations focus on some aspects of the verb semantics. The selected aspects, however, seem to be idiosyncratic and do not automatically result from the internal structure of the verb nor from its interaction with the morphological suffix. We therefore propose a partially lexicalist approach view of deverbal nouns. It is made precise and computable by using the Montagovian Generative Lexicon, a type theoretical framework introduced by Bassac, Mery and Retor\'e in this journal in 2010. This extension of Montague semantics with a richer type system easily incorporates lexical phenomena like the semantics of action nominals in particular deverbals, including their polysemy and (in)felicitous copredications. version:1
arxiv-1401-6571 | Keyword and Keyphrase Extraction Using Centrality Measures on Collocation Networks | http://arxiv.org/abs/1401.6571 | id:1401.6571 author:Shibamouli Lahiri, Sagnik Ray Choudhury, Cornelia Caragea category:cs.CL cs.IR  published:2014-01-25 summary:Keyword and keyphrase extraction is an important problem in natural language processing, with applications ranging from summarization to semantic search to document clustering. Graph-based approaches to keyword and keyphrase extraction avoid the problem of acquiring a large in-domain training corpus by applying variants of PageRank algorithm on a network of words. Although graph-based approaches are knowledge-lean and easily adoptable in online systems, it remains largely open whether they can benefit from centrality measures other than PageRank. In this paper, we experiment with an array of centrality measures on word and noun phrase collocation networks, and analyze their performance on four benchmark datasets. Not only are there centrality measures that perform as well as or better than PageRank, but they are much simpler (e.g., degree, strength, and neighborhood size). Furthermore, centrality-based methods give results that are competitive with and, in some cases, better than two strong unsupervised baselines. version:1
arxiv-1401-6567 | A Machine Learning Approach for the Identification of Bengali Noun-Noun Compound Multiword Expressions | http://arxiv.org/abs/1401.6567 | id:1401.6567 author:Vivekananda Gayen, Kamal Sarkar category:cs.CL cs.LG  published:2014-01-25 summary:This paper presents a machine learning approach for identification of Bengali multiword expressions (MWE) which are bigram nominal compounds. Our proposed approach has two steps: (1) candidate extraction using chunk information and various heuristic rules and (2) training the machine learning algorithm called Random Forest to classify the candidates into two groups: bigram nominal compound MWE or not bigram nominal compound MWE. A variety of association measures, syntactic and linguistic clues and a set of WordNet-based similarity features have been used for our MWE identification task. The approach presented in this paper can be used to identify bigram nominal compound MWE in Bengali running text. version:1
arxiv-1303-1441 | A Hybrid Approach to Extract Keyphrases from Medical Documents | http://arxiv.org/abs/1303.1441 | id:1303.1441 author:Kamal Sarkar category:cs.IR cs.CL  published:2013-03-06 summary:Keyphrases are the phrases, consisting of one or more words, representing the important concepts in the articles. Keyphrases are useful for a variety of tasks such as text summarization, automatic indexing, clustering/classification, text mining etc. This paper presents a hybrid approach to keyphrase extraction from medical documents. The keyphrase extraction approach presented in this paper is an amalgamation of two methods: the first one assigns weights to candidate keyphrases based on an effective combination of features such as position, term frequency, inverse document frequency and the second one assign weights to candidate keyphrases using some knowledge about their similarities to the structure and characteristics of keyphrases available in the memory (stored list of keyphrases). An efficient candidate keyphrase identification method as the first component of the proposed keyphrase extraction system has also been introduced in this paper. The experimental results show that the proposed hybrid approach performs better than some state-of-the art keyphrase extraction approaches. version:2
arxiv-1304-6383 | The Stochastic Gradient Descent for the Primal L1-SVM Optimization Revisited | http://arxiv.org/abs/1304.6383 | id:1304.6383 author:Constantinos Panagiotakopoulos, Petroula Tsampouka category:cs.LG cs.AI  published:2013-04-23 summary:We reconsider the stochastic (sub)gradient approach to the unconstrained primal L1-SVM optimization. We observe that if the learning rate is inversely proportional to the number of steps, i.e., the number of times any training pattern is presented to the algorithm, the update rule may be transformed into the one of the classical perceptron with margin in which the margin threshold increases linearly with the number of steps. Moreover, if we cycle repeatedly through the possibly randomly permuted training set the dual variables defined naturally via the expansion of the weight vector as a linear combination of the patterns on which margin errors were made are shown to obey at the end of each complete cycle automatically the box constraints arising in dual optimization. This renders the dual Lagrangian a running lower bound on the primal objective tending to it at the optimum and makes available an upper bound on the relative accuracy achieved which provides a meaningful stopping criterion. In addition, we propose a mechanism of presenting the same pattern repeatedly to the algorithm which maintains the above properties. Finally, we give experimental evidence that algorithms constructed along these lines exhibit a considerably improved performance. version:2
arxiv-1401-6484 | Identification of Protein Coding Regions in Genomic DNA Using Unsupervised FMACA Based Pattern Classifier | http://arxiv.org/abs/1401.6484 | id:1401.6484 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu category:cs.CE cs.LG  published:2014-01-25 summary:Genes carry the instructions for making proteins that are found in a cell as a specific sequence of nucleotides that are found in DNA molecules. But, the regions of these genes that code for proteins may occupy only a small region of the sequence. Identifying the coding regions play a vital role in understanding these genes. In this paper we propose a unsupervised Fuzzy Multiple Attractor Cellular Automata (FMCA) based pattern classifier to identify the coding region of a DNA sequence. We propose a distinct K-Means algorithm for designing FMACA classifier which is simple, efficient and produces more accurate classifier than that has previously been obtained for a range of different sequence lengths. Experimental results confirm the scalability of the proposed Unsupervised FCA based classifier to handle large volume of datasets irrespective of the number of classes, tuples and attributes. Good classification accuracy has been established. version:1
arxiv-1312-6597 | Co-Multistage of Multiple Classifiers for Imbalanced Multiclass Learning | http://arxiv.org/abs/1312.6597 | id:1312.6597 author:Luis Marujo, Anatole Gershman, Jaime Carbonell, David Martins de Matos, João P. Neto category:cs.LG cs.IR  published:2013-12-23 summary:In this work, we propose two stochastic architectural models (CMC and CMC-M) with two layers of classifiers applicable to datasets with one and multiple skewed classes. This distinction becomes important when the datasets have a large number of classes. Therefore, we present a novel solution to imbalanced multiclass learning with several skewed majority classes, which improves minority classes identification. This fact is particularly important for text classification tasks, such as event detection. Our models combined with pre-processing sampling techniques improved the classification results on six well-known datasets. Finally, we have also introduced a new metric SG-Mean to overcome the multiplication by zero limitation of G-Mean. version:2
arxiv-1312-5578 | Multimodal Transitions for Generative Stochastic Networks | http://arxiv.org/abs/1312.5578 | id:1312.5578 author:Sherjil Ozair, Li Yao, Yoshua Bengio category:cs.LG stat.ML  published:2013-12-19 summary:Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs. version:4
arxiv-1401-6393 | Automatic Detection of Calibration Grids in Time-of-Flight Images | http://arxiv.org/abs/1401.6393 | id:1401.6393 author:Miles Hansard, Radu Horaud, Michel Amat, Georgios Evangelidis category:cs.CV  published:2014-01-24 summary:It is convenient to calibrate time-of-flight cameras by established methods, using images of a chequerboard pattern. The low resolution of the amplitude image, however, makes it difficult to detect the board reliably. Heuristic detection methods, based on connected image-components, perform very poorly on this data. An alternative, geometrically-principled method is introduced here, based on the Hough transform. The projection of a chequerboard is represented by two pencils of lines, which are identified as oriented clusters in the gradient-data of the image. A projective Hough transform is applied to each of the two clusters, in axis-aligned coordinates. The range of each transform is properly bounded, because the corresponding gradient vectors are approximately parallel. Each of the two transforms contains a series of collinear peaks; one for every line in the given pencil. This pattern is easily detected, by sweeping a dual line through the transform. The proposed Hough-based method is compared to the standard OpenCV detection routine, by application to several hundred time-of-flight images. It is shown that the new method detects significantly more calibration boards, over a greater variety of poses, without any overall loss of accuracy. This conclusion is based on an analysis of both geometric and photometric error. version:1
arxiv-1401-6376 | Steady-state performance of non-negative least-mean-square algorithm and its variants | http://arxiv.org/abs/1401.6376 | id:1401.6376 author:Jie Chen, José Carlos M. Bermudez, Cédric Richard category:cs.LG  published:2014-01-24 summary:Non-negative least-mean-square (NNLMS) algorithm and its variants have been proposed for online estimation under non-negativity constraints. The transient behavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMS algorithms have been studied in our previous work. In this technical report, we derive closed-form expressions for the steady-state excess mean-square error (EMSE) for the four algorithms. Simulations results illustrate the accuracy of the theoretical results. This is a complementary material to our previous work. version:1
arxiv-1401-3915 | Community Detection in Networks using Graph Distance | http://arxiv.org/abs/1401.3915 | id:1401.3915 author:Sharmodeep Bhattacharyya, Peter J. Bickel category:stat.ML cs.SI  published:2014-01-16 summary:The study of networks has received increased attention recently not only from the social sciences and statistics but also from physicists, computer scientists and mathematicians. One of the principal problem in networks is community detection. Many algorithms have been proposed for community finding but most of them do not have have theoretical guarantee for sparse networks and networks close to the phase transition boundary proposed by physicists. There are some exceptions but all have some incomplete theoretical basis. Here we propose an algorithm based on the graph distance of vertices in the network. We give theoretical guarantees that our method works in identifying communities for block models and can be extended for degree-corrected block models and block models with the number of communities growing with number of vertices. Despite favorable simulation results, we are not yet able to conclude that our method is satisfactory for worst possible case. We illustrate on a network of political blogs, Facebook networks and some other networks. version:2
arxiv-1401-6276 | The EM algorithm and the Laplace Approximation | http://arxiv.org/abs/1401.6276 | id:1401.6276 author:Niko Brümmer category:stat.ML  published:2014-01-24 summary:The Laplace approximation calls for the computation of second derivatives at the likelihood maximum. When the maximum is found by the EM-algorithm, there is a convenient way to compute these derivatives. The likelihood gradient can be obtained from the EM-auxiliary, while the Hessian can be obtained from this gradient with the Pearlmutter trick. version:1
arxiv-1401-6267 | Parallel Genetic Algorithm to Solve Traveling Salesman Problem on MapReduce Framework using Hadoop Cluster | http://arxiv.org/abs/1401.6267 | id:1401.6267 author:Harun Rasit Er, Nadia Erdogan category:cs.DC cs.NE  published:2014-01-24 summary:Traveling Salesman Problem (TSP) is one of the most common studied problems in combinatorial optimization. Given the list of cities and distances between them, the problem is to find the shortest tour possible which visits all the cities in list exactly once and ends in the city where it starts. Despite the Traveling Salesman Problem is NP-Hard, a lot of methods and solutions are proposed to the problem. One of them is Genetic Algorithm (GA). GA is a simple but an efficient heuristic method that can be used to solve Traveling Salesman Problem. In this paper, we will show a parallel genetic algorithm implementation on MapReduce framework in order to solve Traveling Salesman Problem. MapReduce is a framework used to support distributed computation on clusters of computers. We used free licensed Hadoop implementation as MapReduce framework. version:1
arxiv-1401-6240 | Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II) | http://arxiv.org/abs/1401.6240 | id:1401.6240 author:Shaobo Lin, Xia Liu, Jian Fang, Zongben Xu category:cs.LG 68T05 F.2.2  published:2014-01-24 summary:An extreme learning machine (ELM) can be regarded as a two stage feed-forward neural network (FNN) learning system which randomly assigns the connections with and within hidden neurons in the first stage and tunes the connections with output neurons in the second stage. Therefore, ELM training is essentially a linear learning problem, which significantly reduces the computational burden. Numerous applications show that such a computation burden reduction does not degrade the generalization capability. It has, however, been open that whether this is true in theory. The aim of our work is to study the theoretical feasibility of ELM by analyzing the pros and cons of ELM. In the previous part on this topic, we pointed out that via appropriate selection of the activation function, ELM does not degrade the generalization capability in the expectation sense. In this paper, we launch the study in a different direction and show that the randomness of ELM also leads to certain negative consequences. On one hand, we find that the randomness causes an additional uncertainty problem of ELM, both in approximation and learning. On the other hand, we theoretically justify that there also exists an activation function such that the corresponding ELM degrades the generalization capability. In particular, we prove that the generalization capability of ELM with Gaussian kernel is essentially worse than that of FNN with Gaussian kernel. To facilitate the use of ELM, we also provide a remedy to such a degradation. We find that the well-developed coefficient regularization technique can essentially improve the generalization capability. The obtained results reveal the essential characteristic of ELM and give theoretical guidance concerning how to use ELM. version:1
arxiv-1401-6224 | Word-length entropies and correlations of natural language written texts | http://arxiv.org/abs/1401.6224 | id:1401.6224 author:Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Konstantinos Karamanos, Fotis K. Diakonos, Harris Papageorgiou category:cs.CL physics.data-an  published:2014-01-24 summary:We study the frequency distributions and correlations of the word lengths of ten European languages. Our findings indicate that a) the word-length distribution of short words quantified by the mean value and the entropy distinguishes the Uralic (Finnish) corpus from the others, b) the tails at long words, manifested in the high-order moments of the distributions, differentiate the Germanic languages (except for English) from the Romanic languages and Greek and c) the correlations between nearby word lengths measured by the comparison of the real entropies with those of the shuffled texts are found to be smaller in the case of Germanic and Finnish languages. version:1
arxiv-1110-6317 | Risk-sensitive Markov control processes | http://arxiv.org/abs/1110.6317 | id:1110.6317 author:Yun Shen, Wilhelm Stannat, Klaus Obermayer category:math.OC cs.CE math.DS stat.ML  published:2011-10-28 summary:We introduce a general framework for measuring risk in the context of Markov control processes with risk maps on general Borel spaces that generalize known concepts of risk measures in mathematical finance, operations research and behavioral economics. Within the framework, applying weighted norm spaces to incorporate also unbounded costs, we study two types of infinite-horizon risk-sensitive criteria, discounted total risk and average risk, and solve the associated optimization problems by dynamic programming. For the discounted case, we propose a new discount scheme, which is different from the conventional form but consistent with the existing literature, while for the average risk criterion, we state Lyapunov-like stability conditions that generalize known conditions for Markov chains to ensure the existence of solutions to the optimality equation. version:5
arxiv-1401-6196 | Spatially regularized reconstruction of fibre orientation distributions in the presence of isotropic diffusion | http://arxiv.org/abs/1401.6196 | id:1401.6196 author:Q. Zhou, O. Michailovich, Y. Rathi category:cs.CV  published:2014-01-23 summary:The connectivity and structural integrity of the white matter of the brain is nowadays known to be implicated into a wide range of brain-related disorders. However, it was not before the advent of diffusion Magnetic Resonance Imaging (dMRI) that researches have been able to examine the properties of white matter in vivo. Presently, among a range of various methods of dMRI, high angular resolution diffusion imaging (HARDI) is known to excel in its ability to provide reliable information about the local orientations of neural fasciculi (aka fibre tracts). Moreover, as opposed to the more traditional diffusion tensor imaging (DTI), HARDI is capable of distinguishing the orientations of multiple fibres passing through a given spatial voxel. Unfortunately, the ability of HARDI to discriminate between neural fibres that cross each other at acute angles is always limited, which is the main reason behind the development of numerous post-processing tools, aiming at the improvement of the directional resolution of HARDI. Among such tools is spherical deconvolution (SD). Due to its ill-posed nature, however, SD standardly relies on a number of a priori assumptions which are to render its results unique and stable. In this paper, we propose a different approach to the problem of SD in HARDI, which accounts for the spatial continuity of neural fibres as well as the presence of isotropic diffusion. Subsequently, we demonstrate how the proposed solution can be used to successfully overcome the effect of partial voluming, while preserving the spatial coherency of cerebral diffusion at moderate-to-severe noise levels. In a series of both in silico and in vivo experiments, the performance of the proposed method is compared with that of several available alternatives, with the comparative results clearly supporting the viability and usefulness of our approach. version:1
arxiv-1311-2097 | Risk-sensitive Reinforcement Learning | http://arxiv.org/abs/1311.2097 | id:1311.2097 author:Yun Shen, Michael J. Tobia, Tobias Sommer, Klaus Obermayer category:cs.LG  published:2013-11-08 summary:We derive a family of risk-sensitive reinforcement learning methods for agents, who face sequential decision-making tasks in uncertain environments. By applying a utility function to the temporal difference (TD) error, nonlinear transformations are effectively applied not only to the received rewards but also to the true transition probabilities of the underlying Markov decision process. When appropriate utility functions are chosen, the agents' behaviors express key features of human behavior as predicted by prospect theory (Kahneman and Tversky, 1979), for example different risk-preferences for gains and losses as well as the shape of subjective probability curves. We derive a risk-sensitive Q-learning algorithm, which is necessary for modeling human behavior when transition probabilities are unknown, and prove its convergence. As a proof of principle for the applicability of the new framework we apply it to quantify human behavior in a sequential investment task. We find, that the risk-sensitive variant provides a significantly better fit to the behavioral data and that it leads to an interpretation of the subject's responses which is indeed consistent with prospect theory. The analysis of simultaneously measured fMRI signals show a significant correlation of the risk-sensitive TD error with BOLD signal change in the ventral striatum. In addition we find a significant correlation of the risk-sensitive Q-values with neural activity in the striatum, cingulate cortex and insula, which is not present if standard Q-values are used. version:3
arxiv-1401-6124 | Iterative Universal Hash Function Generator for Minhashing | http://arxiv.org/abs/1401.6124 | id:1401.6124 author:Fabricio Olivetti de Franca category:cs.LG cs.IR  published:2014-01-23 summary:Minhashing is a technique used to estimate the Jaccard Index between two sets by exploiting the probability of collision in a random permutation. In order to speed up the computation, a random permutation can be approximated by using an universal hash function such as the $h_{a,b}$ function proposed by Carter and Wegman. A better estimate of the Jaccard Index can be achieved by using many of these hash functions, created at random. In this paper a new iterative procedure to generate a set of $h_{a,b}$ functions is devised that eliminates the need for a list of random values and avoid the multiplication operation during the calculation. The properties of the generated hash functions remains that of an universal hash function family. This is possible due to the random nature of features occurrence on sparse datasets. Results show that the uniformity of hashing the features is maintaned while obtaining a speed up of up to $1.38$ compared to the traditional approach. version:1
arxiv-1401-6122 | Identifying Bengali Multiword Expressions using Semantic Clustering | http://arxiv.org/abs/1401.6122 | id:1401.6122 author:Tanmoy Chakraborty, Dipankar Das, Sivaji Bandyopadhyay category:cs.CL  published:2014-01-23 summary:One of the key issues in both natural language understanding and generation is the appropriate processing of Multiword Expressions (MWEs). MWEs pose a huge problem to the precise language processing due to their idiosyncratic nature and diversity in lexical, syntactical and semantic properties. The semantics of a MWE cannot be expressed after combining the semantics of its constituents. Therefore, the formalism of semantic clustering is often viewed as an instrument for extracting MWEs especially for resource constraint languages like Bengali. The present semantic clustering approach contributes to locate clusters of the synonymous noun tokens present in the document. These clusters in turn help measure the similarity between the constituent words of a potentially candidate phrase using a vector space model and judge the suitability of this phrase to be a MWE. In this experiment, we apply the semantic clustering approach for noun-noun bigram MWEs, though it can be extended to any types of MWEs. In parallel, the well known statistical models, namely Point-wise Mutual Information (PMI), Log Likelihood Ratio (LLR), Significance function are also employed to extract MWEs from the Bengali corpus. The comparative evaluation shows that the semantic clustering approach outperforms all other competing statistical models. As a by-product of this experiment, we have started developing a standard lexicon in Bengali that serves as a productive Bengali linguistic thesaurus. version:1
arxiv-1401-6050 | Integrative Semantic Dependency Parsing via Efficient Large-scale Feature Selection | http://arxiv.org/abs/1401.6050 | id:1401.6050 author:Hai Zhao, Xiaotian Zhang, Chunyu Kit category:cs.CL  published:2014-01-23 summary:Semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. Unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask, the one presented in this article integrates everything into one model, in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance. This integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier. We leverage adaptive pruning of argument candidates and large-scale feature selection engineering to allow the largest feature space ever in use so far in this field, it achieves a state-of-the-art performance on the evaluation data set for CoNLL-2008 shared task, on top of all but one top pipeline system, confirming its feasibility and effectiveness. version:1
arxiv-1401-6024 | Matrix factorization with Binary Components | http://arxiv.org/abs/1401.6024 | id:1401.6024 author:Martin Slawski, Matthias Hein, Pavlo Lutsik category:stat.ML cs.LG  published:2014-01-23 summary:Motivated by an application in computational biology, we consider low-rank matrix factorization with $\{0,1\}$-constraints on one of the factors and optionally convex constraints on the second one. In addition to the non-convexity shared with other matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size $2^{m \cdot r}$, where $m$ is the dimension of the data points and $r$ the rank of the factorization. Despite apparent intractability, we provide - in the line of recent work on non-negative matrix factorization by Arora et al. (2012) - an algorithm that provably recovers the underlying factorization in the exact case with $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain this result, we use theory around the Littlewood-Offord lemma from combinatorics. version:1
arxiv-1401-5980 | Reasoning about Meaning in Natural Language with Compact Closed Categories and Frobenius Algebras | http://arxiv.org/abs/1401.5980 | id:1401.5980 author:Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, Stephen Pulman, Bob Coecke category:cs.CL cs.AI math.CT  published:2014-01-23 summary:Compact closed categories have found applications in modeling quantum information protocols by Abramsky-Coecke. They also provide semantics for Lambek's pregroup algebras, applied to formalizing the grammatical structure of natural language, and are implicit in a distributional model of word meaning based on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh used the product category of pregroups with vector spaces and provided a distributional model of meaning for sentences. We recast this theory in terms of strongly monoidal functors and advance it via Frobenius algebras over vector spaces. The former are used to formalize topological quantum field theories by Atiyah and Baez-Dolan, and the latter are used to model classical data in quantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us to work in a single space in which meanings of words, phrases, and sentences of any structure live. Hence we can compare meanings of different language constructs and enhance the applicability of the theory. We report on experimental results on a number of language tasks and verify the theoretical predictions. version:1
arxiv-1401-5966 | On Image Block Loss Restoration Using the Sparsity Pattern as Side Information | http://arxiv.org/abs/1401.5966 | id:1401.5966 author:Hossein Hosseini, Ali Goli, Neda Barzegar Marvasti, Masoume Azghani, Farokh Marvasti category:cs.MM cs.CV  published:2014-01-23 summary:In this paper, we propose an image block loss restoration method based on the notion of sparse representation. The sparsity pattern is exploited as side information to efficiently restore block losses, by iteratively imposing the constraints of spatial and transform domains on the corrupted image. Two novel features, including a pre-interpolation and a criterion for stopping the iterations, are added for performance improvement. Besides, a scheme is presented for no-reference quality estimation of the restored image with respect to the original and sparse images. To the best of our knowledge, this is the first attempt to estimate the image quality in the restoration methods. Also, to deal with practical applications, we develop a technique to transmit the side information along with the image. In this technique, we first compress the side information and then embed its LDPC coded version in the least significant bits of the image pixels. This technique ensures the error-free transmission of the side information, while causing only a small perturbation on the transmitted image. Mathematical analysis and extensive simulations are performed to evaluate the method and investigate the efficiency of the proposed techniques. The results verify that the suggested method outperforms its counterparts for image block loss restoration. version:1
arxiv-1401-5900 | Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image Statistics | http://arxiv.org/abs/1401.5900 | id:1401.5900 author:Nan Wang, Jan Melchior, Laurenz Wiskott category:cs.NE cs.LG stat.ML  published:2014-01-23 summary:We present a theoretical analysis of Gaussian-binary restricted Boltzmann machines (GRBMs) from the perspective of density models. The key aspect of this analysis is to show that GRBMs can be formulated as a constrained mixture of Gaussians, which gives a much better insight into the model's capabilities and limitations. We show that GRBMs are capable of learning meaningful features both in a two-dimensional blind source separation task and in modeling natural images. Further, we show that reported difficulties in training GRBMs are due to the failure of the training algorithm rather than the model itself. Based on our analysis we are able to propose several training recipes, which allowed successful and fast training in our experiments. Finally, we discuss the relationship of GRBMs to several modifications that have been proposed to improve the model. version:1
arxiv-1401-5535 | Learning Mid-Level Features and Modeling Neuron Selectivity for Image Classification | http://arxiv.org/abs/1401.5535 | id:1401.5535 author:Shu Kong, Zhuolin Jiang, Qiang Yang category:cs.CV cs.LG cs.NE cs.RO  published:2014-01-22 summary:We now know that mid-level features can greatly enhance the performance of image learning, but how to automatically learn the image features efficiently and in an unsupervised manner is still an open question. In this paper, we present a very efficient mid-level feature learning approach (MidFea), which only involves simple operations such as $k$-means clustering, convolution, pooling, vector quantization and random projection. We explain why this simple method generates the desired features, and argue that there is no need to spend much time in learning low-level feature extractors. Furthermore, to boost the performance, we propose to model the neuron selectivity (NS) principle by building an additional layer over the mid-level features before feeding the features into the classifier. We show that the NS-layer learns category-specific neurons with both bottom-up inference and top-down analysis, and thus supports fast inference for a query image. We run extensive experiments on several public databases to demonstrate that our approach can achieve state-of-the-art performances for face recognition, gender classification, age estimation and object categorization. In particular, we demonstrate that our approach is more than an order of magnitude faster than some recently proposed sparse coding based methods. version:2
arxiv-1401-5891 | Hierarchical pixel clustering for image segmentation | http://arxiv.org/abs/1401.5891 | id:1401.5891 author:M. Kharinov category:cs.CV  published:2014-01-23 summary:In the paper a piecewise constant image approximations of sequential number of pixel clusters or segments are treated. A majorizing of optimal approximation sequence by hierarchical sequence of image approximations is studied. Transition from pixel clustering to image segmentation by reducing of segment numbers in clusters is provided. Algorithms are proved by elementary formulas. version:1
arxiv-1205-3234 | Asymptotic Accuracy of Bayes Estimation for Latent Variables with Redundancy | http://arxiv.org/abs/1205.3234 | id:1205.3234 author:Keisuke Yamazaki category:stat.ML  published:2012-05-15 summary:Hierarchical parametric models consisting of observable and latent variables are widely used for unsupervised learning tasks. For example, a mixture model is a representative hierarchical model for clustering. From the statistical point of view, the models can be regular or singular due to the distribution of data. In the regular case, the models have the identifiability; there is one-to-one relation between a probability density function for the model expression and the parameter. The Fisher information matrix is positive definite, and the estimation accuracy of both observable and latent variables has been studied. In the singular case, on the other hand, the models are not identifiable and the Fisher matrix is not positive definite. Conventional statistical analysis based on the inverse Fisher matrix is not applicable. Recently, an algebraic geometrical analysis has been developed and is used to elucidate the Bayes estimation of observable variables. The present paper applies this analysis to latent-variable estimation and determines its theoretical performance. Our results clarify behavior of the convergence of the posterior distribution. It is found that the posterior of the observable-variable estimation can be different from the one in the latent-variable estimation. Because of the difference, the Markov chain Monte Carlo method based on the parameter and the latent variable cannot construct the desired posterior distribution. version:5
arxiv-1401-6427 | Towards Unsupervised Learning of Temporal Relations between Events | http://arxiv.org/abs/1401.6427 | id:1401.6427 author:Seyed Abolghasem Mirroshandel, Gholamreza Ghassem-Sani category:cs.LG cs.CL  published:2014-01-23 summary:Automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as Question Answering, Information Extraction, and Summarization. Since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. This paper presents two different algorithms towards this goal. The first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. In the first stage, the algorithm learns a general classifier from an annotated corpus. Then, inspired by the hypothesis of "one type of temporal relation per discourse, it extracts useful information from a cluster of topically related documents. We show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. Our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. The second proposed method for temporal relation extraction is based on the expectation maximization (EM) algorithm. Within EM, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. We think that the experimental results of our EM based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging. version:1
arxiv-1405-5208 | A Tutorial on Dual Decomposition and Lagrangian Relaxation for Inference in Natural Language Processing | http://arxiv.org/abs/1405.5208 | id:1405.5208 author:Alexander M. Rush, Michael Collins category:cs.CL cs.AI  published:2014-01-23 summary:Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (NLP). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models. version:1
arxiv-1401-6422 | Automatic Aggregation by Joint Modeling of Aspects and Values | http://arxiv.org/abs/1401.6422 | id:1401.6422 author:Christina Sauper, Regina Barzilay category:cs.CL  published:2014-01-23 summary:We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese restaurant) and determines the corresponding sentiment of each aspect. This approach directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily extensible, and we describe several modifications and their effects on model structure and inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries. We evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. We demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis. version:1
arxiv-1401-6424 | Toward Supervised Anomaly Detection | http://arxiv.org/abs/1401.6424 | id:1401.6424 author:Nico Goernitz, Marius Micha Kloft, Konrad Rieck, Ulf Brefeld category:cs.LG  published:2014-01-23 summary:Anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. Our first contribution shows that classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. We argue that semi-supervised anomaly detection needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement. Although being intrinsically non-convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. Additionally, we propose an active learning strategy to automatically filter candidates for labeling. In an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies. version:1
arxiv-1401-6421 | Riffled Independence for Efficient Inference with Partial Rankings | http://arxiv.org/abs/1401.6421 | id:1401.6421 author:Jonathan Huang, Ashish Kapoor, Carlos Guestrin category:cs.LG  published:2014-01-23 summary:Distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. Modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. Some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. With respect to ranking, however, there is the additional challenge of what we refer to as human task complexity users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information. Simultaneously addressing all of these challenges i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. In this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. In particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. This correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based represen- tations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. Finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data. version:1
arxiv-1401-6876 | Improving Statistical Machine Translation for a Resource-Poor Language Using Related Resource-Rich Languages | http://arxiv.org/abs/1401.6876 | id:1401.6876 author:Preslav Ivanov Nakov, Hwee Tou Ng category:cs.CL  published:2014-01-23 summary:We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X_1 into a resource-rich language Y given a bi-text containing a limited number of parallel sentences for X_1-Y and a larger bi-text for X_2-Y for some resource-rich language X_2 that is closely related to X_1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X_1 and X_2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian- >English using Malay and for Spanish -> English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary "real training data by a factor of 2--5. version:1
arxiv-1401-5823 | Collaborative Regression | http://arxiv.org/abs/1401.5823 | id:1401.5823 author:Samuel M. Gross, Robert Tibshirani category:q-bio.QM stat.ML  published:2014-01-22 summary:We consider the scenario where one observes an outcome variable and sets of features from multiple assays, all measured on the same set of samples. One approach that has been proposed for dealing with this type of data is ``sparse multiple canonical correlation analysis'' (sparse mCCA). All of the current sparse mCCA techniques are biconvex and thus have no guarantees about reaching a global optimum. We propose a method for performing sparse supervised canonical correlation analysis (sparse sCCA), a specific case of sparse mCCA when one of the datasets is a vector. Our proposal for sparse sCCA is convex and thus does not face the same difficulties as the other methods. We derive efficient algorithms for this problem, and illustrate their use on simulated and real data. version:1
arxiv-1303-5145 | Node-Based Learning of Multiple Gaussian Graphical Models | http://arxiv.org/abs/1303.5145 | id:1303.5145 author:Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, Su-In Lee category:stat.ML cs.LG math.OC  published:2013-03-21 summary:We consider the problem of estimating high-dimensional Gaussian graphical models corresponding to a single set of variables under several distinct conditions. This problem is motivated by the task of recovering transcriptional regulatory networks on the basis of gene expression data {containing heterogeneous samples, such as different disease states, multiple species, or different developmental stages}. We assume that most aspects of the conditional dependence networks are shared, but that there are some structured differences between them. Rather than assuming that similarities and differences between networks are driven by individual edges, we take a node-based approach, which in many cases provides a more intuitive interpretation of the network differences. We consider estimation under two distinct assumptions: (1) differences between the K networks are due to individual nodes that are perturbed across conditions, or (2) similarities among the K networks are due to the presence of common hub nodes that are shared across all K networks. Using a row-column overlap norm penalty function, we formulate two convex optimization problems that correspond to these two assumptions. We solve these problems using an alternating direction method of multipliers algorithm, and we derive a set of necessary and sufficient conditions that allows us to decompose the problem into independent subproblems so that our algorithm can be scaled to high-dimensional settings. Our proposal is illustrated on synthetic data, a webpage data set, and a brain cancer gene expression data set. version:4
arxiv-1311-6396 | A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds | http://arxiv.org/abs/1311.6396 | id:1311.6396 author:N. Denizcan Vanli, Suleyman S. Kozat category:cs.LG  published:2013-11-25 summary:We study sequential prediction of real-valued, arbitrary and unknown sequences under the squared error loss as well as the best parametric predictor out of a large, continuous class of predictors. Inspired by recent results from computational learning theory, we refrain from any statistical assumptions and define the performance with respect to the class of general parametric predictors. In particular, we present generic lower and upper bounds on this relative performance by transforming the prediction task into a parameter learning problem. We first introduce the lower bounds on this relative performance in the mixture of experts framework, where we show that for any sequential algorithm, there always exists a sequence for which the performance of the sequential algorithm is lower bounded by zero. We then introduce a sequential learning algorithm to predict such arbitrary and unknown sequences, and calculate upper bounds on its total squared prediction error for every bounded sequence. We further show that in some scenarios we achieve matching lower and upper bounds demonstrating that our algorithms are optimal in a strong minimax sense such that their performances cannot be improved further. As an interesting result we also prove that for the worst case scenario, the performance of randomized algorithms can be achieved by sequential algorithms so that randomized algorithms does not improve the performance. version:2
arxiv-1401-6002 | Numerical weather prediction or stochastic modeling: an objective criterion of choice for the global radiation forecasting | http://arxiv.org/abs/1401.6002 | id:1401.6002 author:Cyril Voyant, Gilles Notton, Christophe Paoli, Marie Laure Nivet, Marc Muselli, Kahina Dahmani category:stat.AP cs.LG  published:2014-01-22 summary:Numerous methods exist and were developed for global radiation forecasting. The two most popular types are the numerical weather predictions (NWP) and the predictions using stochastic approaches. We propose to compute a parameter noted constructed in part from the mutual information which is a quantity that measures the mutual dependence of two variables. Both of these are calculated with the objective to establish the more relevant method between NWP and stochastic models concerning the current problem. version:1
arxiv-1301-1218 | Finding the True Frequent Itemsets | http://arxiv.org/abs/1301.1218 | id:1301.1218 author:Matteo Riondato, Fabio Vandin category:cs.LG cs.DB cs.DS stat.ML H.2.8  published:2013-01-07 summary:Frequent Itemsets (FIs) mining is a fundamental primitive in data mining. It requires to identify all itemsets appearing in at least a fraction $\theta$ of a transactional dataset $\mathcal{D}$. Often though, the ultimate goal of mining $\mathcal{D}$ is not an analysis of the dataset \emph{per se}, but the understanding of the underlying process that generated it. Specifically, in many applications $\mathcal{D}$ is a collection of samples obtained from an unknown probability distribution $\pi$ on transactions, and by extracting the FIs in $\mathcal{D}$ one attempts to infer itemsets that are frequently (i.e., with probability at least $\theta$) generated by $\pi$, which we call the True Frequent Itemsets (TFIs). Due to the inherently stochastic nature of the generative process, the set of FIs is only a rough approximation of the set of TFIs, as it often contains a huge number of \emph{false positives}, i.e., spurious itemsets that are not among the TFIs. In this work we design and analyze an algorithm to identify a threshold $\hat{\theta}$ such that the collection of itemsets with frequency at least $\hat{\theta}$ in $\mathcal{D}$ contains only TFIs with probability at least $1-\delta$, for some user-specified $\delta$. Our method uses results from statistical learning theory involving the (empirical) VC-dimension of the problem at hand. This allows us to identify almost all the TFIs without including any false positive. We also experimentally compare our method with the direct mining of $\mathcal{D}$ at frequency $\theta$ and with techniques based on widely-used standard bounds (i.e., the Chernoff bounds) of the binomial distribution, and show that our algorithm outperforms these methods and achieves even better results than what is guaranteed by the theoretical analysis. version:3
arxiv-1401-5644 | A new keyphrases extraction method based on suffix tree data structure for arabic documents clustering | http://arxiv.org/abs/1401.5644 | id:1401.5644 author:Issam Sahmoudi, Hanane Froud, Abdelmonaime Lachkar category:cs.CL cs.IR H.2.3  published:2014-01-22 summary:Document Clustering is a branch of a larger area of scientific study known as data mining .which is an unsupervised classification using to find a structure in a collection of unlabeled data. The useful information in the documents can be accompanied by a large amount of noise words when using Full Text Representation, and therefore will affect negatively the result of the clustering process. So it is with great need to eliminate the noise words and keeping just the useful information in order to enhance the quality of the clustering results. This problem occurs with different degree for any language such as English, European, Hindi, Chinese, and Arabic Language. To overcome this problem, in this paper, we propose a new and efficient Keyphrases extraction method based on the Suffix Tree data structure (KpST), the extracted Keyphrases are then used in the clustering process instead of Full Text Representation. The proposed method for Keyphrases extraction is language independent and therefore it may be applied to any language. In this investigation, we are interested to deal with the Arabic language which is one of the most complex languages. To evaluate our method, we conduct an experimental study on Arabic Documents using the most popular Clustering approach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with seven linkage techniques and a variety of distance functions and similarity measures to perform Arabic Document Clustering task. The obtained results show that our method for extracting Keyphrases increases the quality of the clustering results. We propose also to study the effect of using the stemming for the testing dataset to cluster it with the same documents clustering techniques and similarity/distance measures. version:1
arxiv-1401-5636 | Causal Discovery in a Binary Exclusive-or Skew Acyclic Model: BExSAM | http://arxiv.org/abs/1401.5636 | id:1401.5636 author:Takanori Inazumi, Takashi Washio, Shohei Shimizu, Joe Suzuki, Akihiro Yamamoto, Yoshinobu Kawahara category:stat.ML cs.LG  published:2014-01-22 summary:Discovering causal relations among observed variables in a given data set is a major objective in studies of statistics and artificial intelligence. Recently, some techniques to discover a unique causal model have been explored based on non-Gaussianity of the observed data distribution. However, most of these are limited to continuous data. In this paper, we present a novel causal model for binary data and propose an efficient new approach to deriving the unique causal model governing a given binary data set under skew distributions of external binary noises. Experimental evaluation shows excellent performance for both artificial and real world data sets. version:1
arxiv-1401-5632 | Enhancing Template Security of Face Biometrics by Using Edge Detection and Hashing | http://arxiv.org/abs/1401.5632 | id:1401.5632 author:Manoj Krishnaswamy, G. Hemantha Kumar category:cs.CV  published:2014-01-22 summary:In this paper we address the issues of using edge detection techniques on facial images to produce cancellable biometric templates and a novel method for template verification against tampering. With increasing use of biometrics, there is a real threat for the conventional systems using face databases, which store images of users in raw and unaltered form. If compromised not only it is irrevocable, but can be misused for cross-matching across different databases. So it is desirable to generate and store revocable templates for the same user in different applications to prevent cross-matching and to enhance security, while maintaining privacy and ethics. By comparing different edge detection methods it has been observed that the edge detection based on the Roberts Cross operator performs consistently well across multiple face datasets, in which the face images have been taken under a variety of conditions. We have proposed a novel scheme using hashing, for extra verification, in order to harden the security of the stored biometric templates. version:1
arxiv-1401-5625 | Identifiability of an Integer Modular Acyclic Additive Noise Model and its Causal Structure Discovery | http://arxiv.org/abs/1401.5625 | id:1401.5625 author:Joe Suzuki, Takanori Inazumi, Takashi Washio, Shohei Shimizu category:stat.ML  published:2014-01-22 summary:The notion of causality is used in many situations dealing with uncertainty. We consider the problem whether causality can be identified given data set generated by discrete random variables rather than continuous ones. In particular, for non-binary data, thus far it was only known that causality can be identified except rare cases. In this paper, we present necessary and sufficient condition for an integer modular acyclic additive noise (IMAN) of two variables. In addition, we relate bivariate and multivariate causal identifiability in a more explicit manner, and develop a practical algorithm to find the order of variables and their parent sets. We demonstrate its performance in applications to artificial data and real world body motion data with comparisons to conventional methods. version:1
arxiv-1401-5589 | The Gabor-Einstein Wavelet: A Model for the Receptive Fields of V1 to MT Neurons | http://arxiv.org/abs/1401.5589 | id:1401.5589 author:Stephen G. Odaibo category:q-bio.NC cs.CV physics.bio-ph  published:2014-01-22 summary:Our visual system is astonishingly efficient at detecting moving objects. This process is mediated by the neurons which connect the primary visual cortex (V1) to the middle temporal (MT) area. Interestingly, since Kuffler's pioneering experiments on retinal ganglion cells, mathematical models have been vital for advancing our understanding of the receptive fields of visual neurons. However, existing models were not designed to describe the most salient attributes of the highly specialized neurons in the V1 to MT motion processing stream; and they have not been able to do so. Here, we introduce the Gabor-Einstein wavelet, a new family of functions for representing the receptive fields of V1 to MT neurons. We show that the way space and time are mixed in the visual cortex is analogous to the way they are mixed in the special theory of relativity (STR). Hence we constrained the Gabor-Einstein model by requiring: (i) relativistic-invariance of the wave carrier, and (ii) the minimum possible number of parameters. From these two constraints, the sinc function emerged as a natural descriptor of the wave carrier. The particular distribution of lowpass to bandpass temporal frequency filtering properties of V1 to MT neurons (Foster et al 1985; DeAngelis et al 1993b; Hawken et al 1996) is clearly explained by the Gabor-Einstein basis. Furthermore, it does so in a manner innately representative of the motion-processing stream's neuronal hierarchy. Our analysis and computer simulations show that the distribution of temporal frequency filtering properties along the motion processing stream is a direct effect of the way the brain jointly encodes space and time. We uncovered this fundamental link by demonstrating that analogous mathematical structures underlie STR and joint cortical spacetime encoding. This link will provide new physiological insights into how the brain represents visual information. version:1
arxiv-1401-5559 | License Plate Recognition (LPR): A Review with Experiments for Malaysia Case Study | http://arxiv.org/abs/1401.5559 | id:1401.5559 author:Nuzulha Khilwani Ibrahim, Emaliana Kasmuri, Norazira A Jalil, Mohd Adili Norasikin, Sazilah Salam, Mohamad Riduwan Md Nawawi category:cs.CV  published:2014-01-22 summary:Most vehicle license plate recognition use neural network techniques to enhance its computing capability. The image of the vehicle license plate is captured and processed to produce a textual output for further processing. This paper reviews image processing and neural network techniques applied at different stages which are preprocessing, filtering, feature extraction, segmentation and recognition in such way to remove the noise of the image, to enhance the image quality and to expedite the computing process by converting the characters in the image into respective text. An exemplar experiment has been done in MATLAB to show the basic process of the image processing especially for license plate in Malaysia case study. An algorithm is adapted into the solution for parking management system. The solution then is implemented as proof of concept to the algorithm. version:1
arxiv-1303-7286 | On the symmetrical Kullback-Leibler Jeffreys centroids | http://arxiv.org/abs/1303.7286 | id:1303.7286 author:Frank Nielsen category:cs.IT cs.LG math.IT stat.ML  published:2013-03-29 summary:Due to the success of the bag-of-word modeling paradigm, clustering histograms has become an important ingredient of modern information processing. Clustering histograms can be performed using the celebrated $k$-means centroid-based algorithm. From the viewpoint of applications, it is usually required to deal with symmetric distances. In this letter, we consider the Jeffreys divergence that symmetrizes the Kullback-Leibler divergence, and investigate the computation of Jeffreys centroids. We first prove that the Jeffreys centroid can be expressed analytically using the Lambert $W$ function for positive histograms. We then show how to obtain a fast guaranteed approximation when dealing with frequency histograms. Finally, we conclude with some remarks on the $k$-means histogram clustering. version:3
arxiv-1309-6707 | Distributed Online Learning in Social Recommender Systems | http://arxiv.org/abs/1309.6707 | id:1309.6707 author:Cem Tekin, Simpson Zhang, Mihaela van der Schaar category:cs.SI cs.LG stat.ML  published:2013-09-26 summary:In this paper, we consider decentralized sequential decision making in distributed online recommender systems, where items are recommended to users based on their search query as well as their specific background including history of bought items, gender and age, all of which comprise the context information of the user. In contrast to centralized recommender systems, in which there is a single centralized seller who has access to the complete inventory of items as well as the complete record of sales and user information, in decentralized recommender systems each seller/learner only has access to the inventory of items and user information for its own products and not the products and user information of other sellers, but can get commission if it sells an item of another seller. Therefore the sellers must distributedly find out for an incoming user which items to recommend (from the set of own items or items of another seller), in order to maximize the revenue from own sales and commissions. We formulate this problem as a cooperative contextual bandit problem, analytically bound the performance of the sellers compared to the best recommendation strategy given the complete realization of user arrivals and the inventory of items, as well as the context-dependent purchase probabilities of each item, and verify our results via numerical examples on a distributed data set adapted based on Amazon data. We evaluate the dependence of the performance of a seller on the inventory of items the seller has, the number of connections it has with the other sellers, and the commissions which the seller gets by selling items of other sellers to its users. version:2
arxiv-1312-2646 | Guaranteed Model Order Estimation and Sample Complexity Bounds for LDA | http://arxiv.org/abs/1312.2646 | id:1312.2646 author:E. D. Gutiérrez category:stat.ML  published:2013-12-10 summary:The question of how to determine the number of independent latent factors (topics) in mixture models such as Latent Dirichlet Allocation (LDA) is of great practical importance. In most applications, the exact number of topics is unknown, and depends on the application and the size of the data set. Bayesian nonparametric methods can avoid the problem of topic number selection, but they can be impracticably slow for large sample sizes and are subject to local optima. We develop a guaranteed procedure for topic number recovery that does not necessitate learning the model's latent parameters beforehand. Our procedure relies on adapting results from random matrix theory. Performance of our topic number recovery procedure is superior to hLDA, a nonparametric method. We also discuss some implications of our results on the sample complexity and accuracy of popular spectral learning algorithms for LDA. Our results and procedure can be extended to spectral learning algorithms for other exchangeable mixture models as well as Hidden Markov Models. version:4
arxiv-1401-5789 | Reaserchnig the Development of the Electrical Power System Using Systemically Evolutionary Algorithm | http://arxiv.org/abs/1401.5789 | id:1401.5789 author:Jerzy Tchorzewski, Emil Chyzy category:cs.NE cs.SY  published:2014-01-21 summary:The paper contains the concept and the results of research concerning the evolutionary algorithm, identified based on the systems control theory, which was called the Systemically of Evolutionary Algorithm (SAE). Special attention was paid to two elements of evolutionary algorithms, which have not been fully solved yet, i.e. to the methods used to create the initial population and the method of creating the robustness (fitness) function. Other elements of the SEA algorithm, i.a. cross-over, mutation, selection, etc. were also defined from a systemic point of view. Computational experiments were conducted using a selected subsystem of the Polish Electrical Power System and three programming languages: Java, C++ and Matlab. Selected comparative results for the SAE algorithm in different implementations were also presented. version:1
arxiv-1401-5508 | Hilbert Space Methods for Reduced-Rank Gaussian Process Regression | http://arxiv.org/abs/1401.5508 | id:1401.5508 author:Arno Solin, Simo Särkkä category:stat.ML  published:2014-01-21 summary:This paper proposes a novel scheme for reduced-rank Gaussian process regression. The method is based on an approximate series expansion of the covariance function in terms of an eigenfunction expansion of the Laplace operator in a compact subset of $\mathbb{R}^d$. On this approximate eigenbasis the eigenvalues of the covariance function can be expressed as simple functions of the spectral density of the Gaussian process, which allows the GP inference to be solved under a computational cost scaling as $\mathcal{O}(nm^2)$ (initial) and $\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basis functions and $n$ data points. The approach also allows for rigorous error analysis with Hilbert space theory, and we show that the approximation becomes exact when the size of the compact subset and the number of eigenfunctions go to infinity. The expansion generalizes to Hilbert spaces with an inner product which is defined as an integral over a specified input density. The method is compared to previously proposed methods theoretically and through empirical tests with simulated and real data. version:1
arxiv-1401-5408 | On change point detection using the fused lasso method | http://arxiv.org/abs/1401.5408 | id:1401.5408 author:Cristian R. Rojas, Bo Wahlberg category:math.ST stat.ML stat.TH 62G08  62G20  published:2014-01-21 summary:In this paper we analyze the asymptotic properties of l1 penalized maximum likelihood estimation of signals with piece-wise constant mean values and/or variances. The focus is on segmentation of a non-stationary time series with respect to changes in these model parameters. This change point detection and estimation problem is also referred to as total variation denoising or l1 -mean filtering and has many important applications in most fields of science and engineering. We establish the (approximate) sparse consistency properties, including rate of convergence, of the so-called fused lasso signal approximator (FLSA). We show that this only holds if the sign of the corresponding consecutive changes are all different, and that this estimator is otherwise incapable of correctly detecting the underlying sparsity pattern. The key idea is to notice that the optimality conditions for this problem can be analyzed using techniques related to brownian bridge theory. version:1
arxiv-1401-5364 | HMACA: Towards Proposing a Cellular Automata Based Tool for Protein Coding, Promoter Region Identification and Protein Structure Prediction | http://arxiv.org/abs/1401.5364 | id:1401.5364 author:Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG  published:2014-01-21 summary:Human body consists of lot of cells, each cell consist of DeOxaRibo Nucleic Acid (DNA). Identifying the genes from the DNA sequences is a very difficult task. But identifying the coding regions is more complex task compared to the former. Identifying the protein which occupy little place in genes is a really challenging issue. For understating the genes coding region analysis plays an important role. Proteins are molecules with macro structure that are responsible for a wide range of vital biochemical functions, which includes acting as oxygen, cell signaling, antibody production, nutrient transport and building up muscle fibers. Promoter region identification and protein structure prediction has gained a remarkable attention in recent years. Even though there are some identification techniques addressing this problem, the approximate accuracy in identifying the promoter region is closely 68% to 72%. We have developed a Cellular Automata based tool build with hybrid multiple attractor cellular automata (HMACA) classifier for protein coding region, promoter region identification and protein structure prediction which predicts the protein and promoter regions with an accuracy of 76%. This tool also predicts the structure of protein with an accuracy of 80%. version:1
arxiv-1401-5330 | Study of Neural Network Algorithm for Straight-Line Drawings of Planar Graphs | http://arxiv.org/abs/1401.5330 | id:1401.5330 author:Mohamed A. El-Sayed, S. Abdel-Khalek, Hanan H. Amin category:cs.CG cs.NE  published:2014-01-21 summary:Graph drawing addresses the problem of finding a layout of a graph that satisfies given aesthetic and understandability objectives. The most important objective in graph drawing is minimization of the number of crossings in the drawing, as the aesthetics and readability of graph drawings depend on the number of edge crossings. VLSI layouts with fewer crossings are more easily realizable and consequently cheaper. A straight-line drawing of a planar graph G of n vertices is a drawing of G such that each edge is drawn as a straight-line segment without edge crossings. However, a problem with current graph layout methods which are capable of producing satisfactory results for a wide range of graphs is that they often put an extremely high demand on computational resources. This paper introduces a new layout method, which nicely draws internally convex of planar graph that consumes only little computational resources and does not need any heavy duty preprocessing. Here, we use two methods: The first is self organizing map known from unsupervised neural networks which is known as (SOM) and the second method is Inverse Self Organized Map (ISOM). version:1
arxiv-1401-5327 | Compositional Operators in Distributional Semantics | http://arxiv.org/abs/1401.5327 | id:1401.5327 author:Dimitri Kartsaklis category:cs.CL cs.AI math.CT  published:2014-01-21 summary:This survey presents in some detail the main advances that have been recently taking place in Computational Linguistics towards the unification of the two prominent semantic paradigms: the compositional formal semantics view and the distributional models of meaning based on vector spaces. After an introduction to these two approaches, I review the most important models that aim to provide compositionality in distributional semantics. Then I proceed and present in more detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based on the abstract mathematical setting of category theory, as a more complete example capable to demonstrate the diversity of techniques and scientific disciplines that this kind of research can draw from. This paper concludes with a discussion about important open issues that need to be addressed by the researchers in the future. version:1
arxiv-1401-5686 | Increasing Server Availability for Overall System Security: A Preventive Maintenance Approach Based on Failure Prediction | http://arxiv.org/abs/1401.5686 | id:1401.5686 author:Ayman M. Bahaa-Eldin, Hoda K. Mohamead, Sally S. Deraz category:cs.DC cs.NE  published:2014-01-21 summary:Server Availability (SA) is an important measure of overall systems security. Important security systems rely on the availability of their hosting servers to deliver critical security services. Many of these servers offer management interface through web mainly using an Apache server. This paper investigates the increase of Server Availability by the use of Artificial Neural Networks (ANN) to predict software aging phenomenon. Several resource usage data is collected and analyzed on a typical long-running software system (a web server). A Multi-Layer Perceptron feed forward Artificial Neural Network was trained on an Apache web server data-set to predict future server resource exhaustion through uni-variate time series forecasting. The results were benchmarked against those obtained from non-parametric statistical techniques, parametric time series models and empirical modeling techniques reported in the literature. version:1
arxiv-1401-5246 | Genetic Algorithms and its use with back-propagation network | http://arxiv.org/abs/1401.5246 | id:1401.5246 author:Ayman M. Bahaa-Eldin, A. M. A. Wahdan, H. M. K. Mahdi category:cs.NE  published:2014-01-21 summary:Genetic algorithms are considered as one of the most efficient search techniques. Although they do not offer an optimal solution, their ability to reach a suitable solution in considerably short time gives them their respectable role in many AI techniques. This work introduces genetic algorithms and describes their characteristics. Then a novel method using genetic algorithm in best training set generation and selection for a back-propagation network is proposed. This work also offers a new extension to the original genetic algorithms version:1
arxiv-1401-5245 | Edge detection of binary images using the method of masks | http://arxiv.org/abs/1401.5245 | id:1401.5245 author:Ayman M Bahaa-Eldeen, Abdel-Moneim A. Wahdan, Hani M. K. Mahdi category:cs.CV  published:2014-01-21 summary:In this work the method of masks, creating and using of inverted image masks, together with binary operation of image data are used in edge detection of binary images, monochrome images, which yields about 300 times faster than ordinary methods. The method is divided into three stages: Mask construction, Fundamental edge detection, and Edge Construction Comparison with an ordinary method and a fuzzy based method is carried out. version:1
arxiv-1401-5221 | Optimal Intelligent Control for Wind Turbulence Rejection in WECS Using ANNs and Genetic Fuzzy Approach | http://arxiv.org/abs/1401.5221 | id:1401.5221 author:Hadi kasiri, hamid reza momeni, Atiyeh Kasiri category:cs.SY cs.NE  published:2014-01-21 summary:One of the disadvantages in Connection of wind energy conversion systems (WECSs) to transmission networks is plentiful turbulence of wind speed. Therefore effects of this problem must be controlled. Nowadays, pitch-controlled WECSs are increasingly used for variable speed and pitch wind turbines. Megawatt class wind turbines generally turn at variable speed in wind farm. Thus turbine operation must be controlled in order to maximize the conversion efficiency below rated power and reduce loading on the drive-train. Due to random and non-linear nature of the wind turbulence and the ability of Multi-Layer Perceptron (MLP) and Radial Basis Function (RBF) Artificial Neural Networks (ANNs) in the modeling and control of this turbulence, in this study, widespread changes of wind have been perused using MLP and RBF artificial NNs. In addition in this study, a new genetic fuzzy system has been successfully applied to identify disturbance wind in turbine input. Thus output power has been regulated in optimal and nominal range by pitch angle regulation. Consequently, our proposed approaches have regulated output aerodynamic power and torque in the nominal rang. version:1
arxiv-1312-0451 | Consistency of weighted majority votes | http://arxiv.org/abs/1312.0451 | id:1312.0451 author:Daniel Berend, Aryeh Kontorovich category:math.PR cs.LG stat.ML 60C05  60F15  published:2013-12-02 summary:We revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory. version:5
arxiv-1401-5216 | Multi-GPU parallel memetic algorithm for capacitated vehicle routing problem | http://arxiv.org/abs/1401.5216 | id:1401.5216 author:Michał Karpiński, Maciej Pacut category:cs.DC cs.NE  published:2014-01-21 summary:The goal of this paper is to propose and test a new memetic algorithm for the capacitated vehicle routing problem in parallel computing environment. In this paper we consider simple variation of vehicle routing problem in which the only parameter is the capacity of the vehicle and each client only needs one package. We present simple reduction to prove the existence of polynomial-time algorithm for capacity 2. We analyze the efficiency of the algorithm using hierarchical Parallel Random Access Machine (PRAM) model and run experiments with code written in CUDA (for capacities larger than 2). version:1
arxiv-1401-5136 | A Unifying Framework for Typical Multi-Task Multiple Kernel Learning Problems | http://arxiv.org/abs/1401.5136 | id:1401.5136 author:Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos category:cs.LG  published:2014-01-21 summary:Over the past few years, Multi-Kernel Learning (MKL) has received significant attention among data-driven feature selection techniques in the context of kernel-based learning. MKL formulations have been devised and solved for a broad spectrum of machine learning problems, including Multi-Task Learning (MTL). Solving different MKL formulations usually involves designing algorithms that are tailored to the problem at hand, which is, typically, a non-trivial accomplishment. In this paper we present a general Multi-Task Multi-Kernel Learning (Multi-Task MKL) framework that subsumes well-known Multi-Task MKL formulations, as well as several important MKL approaches on single-task problems. We then derive a simple algorithm that can solve the unifying framework. To demonstrate the flexibility of the proposed framework, we formulate a new learning problem, namely Partially-Shared Common Space (PSCS) Multi-Task MKL, and demonstrate its merits through experimentation. version:1
arxiv-1401-5108 | An Identification System Using Eye Detection Based On Wavelets And Neural Networks | http://arxiv.org/abs/1401.5108 | id:1401.5108 author:Mohamed A. El-Sayed, Mohamed A. Khafagy category:cs.CV  published:2014-01-20 summary:The randomness and uniqueness of human eye patterns is a major breakthrough in the search for quicker, easier and highly reliable forms of automatic human identification. It is being used extensively in security solutions. This includes access control to physical facilities, security systems and information databases, Suspect tracking, surveillance and intrusion detection and by various Intelligence agencies through out the world. We use the advantage of human eye uniqueness to identify people and approve its validity as a biometric. . Eye detection involves first extracting the eye from a digital face image, and then encoding the unique patterns of the eye in such a way that they can be compared with pre-registered eye patterns. The eye detection system consists of an automatic segmentation system that is based on the wavelet transform, and then the Wavelet analysis is used as a pre-processor for a back propagation neural network with conjugate gradient learning. The inputs to the neural network are the wavelet maxima neighborhood coefficients of face images at a particular scale. The output of the neural network is the classification of the input into an eye or non-eye region. An accuracy of 90% is observed for identifying test images under different conditions included in training stage. version:1
arxiv-1401-5098 | Study of Efficient Technique Based On 2D Tsallis Entropy For Image Thresholding | http://arxiv.org/abs/1401.5098 | id:1401.5098 author:Mohamed A. El-Sayed, S. Abdel-Khalek, Eman Abdel-Aziz category:cs.CV 68U10  published:2014-01-20 summary:Thresholding is an important task in image processing. It is a main tool in pattern recognition, image segmentation, edge detection and scene analysis. In this paper, we present a new thresholding technique based on two-dimensional Tsallis entropy. The two-dimensional Tsallis entropy was obtained from the twodimensional histogram which was determined by using the gray value of the pixels and the local average gray value of the pixels, the work it was applied a generalized entropy formalism that represents a recent development in statistical mechanics. The effectiveness of the proposed method is demonstrated by using examples from the real-world and synthetic images. The performance evaluation of the proposed technique in terms of the quality of the thresholded images are presented. Experimental results demonstrate that the proposed method achieve better result than the Shannon method. version:1
arxiv-1401-4994 | A Review of Verbal and Non-Verbal Human-Robot Interactive Communication | http://arxiv.org/abs/1401.4994 | id:1401.4994 author:Nikolaos Mavridis category:cs.RO cs.CL  published:2014-01-20 summary:In this paper, an overview of human-robot interactive communication is presented, covering verbal as well as non-verbal aspects of human-robot interaction. Following a historical introduction, and motivation towards fluid human-robot communication, ten desiderata are proposed, which provide an organizational axis both of recent as well as of future research on human-robot communication. Then, the ten desiderata are examined in detail, culminating to a unifying discussion, and a forward-looking conclusion. version:1
arxiv-1401-4872 | Classification of IDS Alerts with Data Mining Techniques | http://arxiv.org/abs/1401.4872 | id:1401.4872 author:Hany Nashat Gabra, Ayman Mohammad Bahaa-Eldin, Huda Korashy category:cs.CR cs.DB cs.LG  published:2014-01-20 summary:A data mining technique to reduce the amount of false alerts within an IDS system is proposed. The new technique achieves an accuracy of 99% compared to 97% by the current systems. version:1
arxiv-1401-4869 | Does Syntactic Knowledge help English-Hindi SMT? | http://arxiv.org/abs/1401.4869 | id:1401.4869 author:Taraka Rama, Karthik Gali, Avinesh PVS category:cs.CL cs.AI  published:2014-01-20 summary:In this paper we explore various parameter settings of the state-of-art Statistical Machine Translation system to improve the quality of the translation for a `distant' language pair like English-Hindi. We proposed new techniques for efficient reordering. A slight improvement over the baseline is reported using these techniques. We also show that a simple pre-processing step can improve the quality of the translation significantly. version:1
arxiv-1401-4857 | A Genetic Algorithm to Optimize a Tweet for Retweetability | http://arxiv.org/abs/1401.4857 | id:1401.4857 author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE cs.CY cs.SI physics.soc-ph  published:2014-01-20 summary:Twitter is a popular microblogging platform. When users send out messages, other users have the ability to forward these messages to their own subgraph. Most research focuses on increasing retweetability from a node's perspective. Here, we center on improving message style to increase the chance of a message being forwarded. To this end, we simulate an artificial Twitter-like network with nodes deciding deterministically on retweeting a message or not. A genetic algorithm is used to optimize message composition, so that the reach of a message is increased. When analyzing the algorithm's runtime behavior across a set of different node types, we find that the algorithm consistently succeeds in significantly improving the retweetability of a message. version:1
arxiv-1401-4848 | An Evolutionary Approach towards Clustering Airborne Laser Scanning Data | http://arxiv.org/abs/1401.4848 | id:1401.4848 author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE  published:2014-01-20 summary:In land surveying, the generation of maps was greatly simplified with the introduction of orthophotos and at a later stage with airborne LiDAR laser scanning systems. While the original purpose of LiDAR systems was to determine the altitude of ground elevations, newer full wave systems provide additional information that can be used on classifying the type of ground cover and the generation of maps. The LiDAR resulting point clouds are huge, multidimensional data sets that need to be grouped in classes of ground cover. We propose a genetic algorithm that aids in classifying these data sets and thus make them usable for map generation. A key feature are tailor-made genetic operators and fitness functions for the subject. The algorithm is compared to a traditional k-means clustering. version:1
arxiv-1311-3651 | Smoothed Analysis of Tensor Decompositions | http://arxiv.org/abs/1311.3651 | id:1311.3651 author:Aditya Bhaskara, Moses Charikar, Ankur Moitra, Aravindan Vijayaraghavan category:cs.DS cs.LG stat.ML  published:2013-11-14 summary:Low rank tensor decompositions are a powerful tool for learning generative models, and uniqueness results give them a significant advantage over matrix decomposition methods. However, tensors pose significant algorithmic challenges and tensors analogs of much of the matrix algebra toolkit are unlikely to exist because of hardness results. Efficient decomposition in the overcomplete case (where rank exceeds dimension) is particularly challenging. We introduce a smoothed analysis model for studying these questions and develop an efficient algorithm for tensor decomposition in the highly overcomplete case (rank polynomial in the dimension). In this setting, we show that our algorithm is robust to inverse polynomial error -- a crucial property for applications in learning since we are only allowed a polynomial number of samples. While algorithms are known for exact tensor decomposition in some overcomplete settings, our main contribution is in analyzing their stability in the framework of smoothed analysis. Our main technical contribution is to show that tensor products of perturbed vectors are linearly independent in a robust sense (i.e. the associated matrix has singular values that are at least an inverse polynomial). This key result paves the way for applying tensor methods to learning problems in the smoothed setting. In particular, we use it to obtain results for learning multi-view models and mixtures of axis-aligned Gaussians where there are many more "components" than dimensions. The assumption here is that the model is not adversarially chosen, formalized by a perturbation of model parameters. We believe this an appealing way to analyze realistic instances of learning problems, since this framework allows us to overcome many of the usual limitations of using tensor methods. version:4
arxiv-1401-4788 | Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means | http://arxiv.org/abs/1401.4788 | id:1401.4788 author:Frank Nielsen category:cs.CV cs.IT math.IT  published:2014-01-20 summary:Bayesian classification labels observations based on given prior information, namely class-a priori and class-conditional probabilities. Bayes' risk is the minimum expected classification cost that is achieved by the Bayes' test, the optimal decision rule. When no cost incurs for correct classification and unit cost is charged for misclassification, Bayes' test reduces to the maximum a posteriori decision rule, and Bayes risk simplifies to Bayes' error, the probability of error. Since calculating this probability of error is often intractable, several techniques have been devised to bound it with closed-form formula, introducing thereby measures of similarity and divergence between distributions like the Bhattacharyya coefficient and its associated Bhattacharyya distance. The Bhattacharyya upper bound can further be tightened using the Chernoff information that relies on the notion of best error exponent. In this paper, we first express Bayes' risk using the total variation distance on scaled distributions. We then elucidate and extend the Bhattacharyya and the Chernoff upper bound mechanisms using generalized weighted means. We provide as a byproduct novel notions of statistical divergences and affinity coefficients. We illustrate our technique by deriving new upper bounds for the univariate Cauchy and the multivariate $t$-distributions, and show experimentally that those bounds are not too distant to the computationally intractable Bayes' error. version:1
arxiv-1401-4785 | Anomaly detection in reconstructed quantum states using a machine-learning technique | http://arxiv.org/abs/1401.4785 | id:1401.4785 author:Satoshi Hara, Takafumi Ono, Ryo Okamoto, Takashi Washio, Shigeki Takeuchi category:quant-ph stat.AP stat.ML 81V80  published:2014-01-20 summary:The accurate detection of small deviations in given density matrices is important for quantum information processing. Here we propose a new method based on the concept of data mining. We demonstrate that the proposed method can more accurately detect small erroneous deviations in reconstructed density matrices, which contain intrinsic fluctuations due to the limited number of samples, than a naive method of checking the trace distance from the average of the given density matrices. This method has the potential to be a key tool in broad areas of physics where the detection of small deviations of quantum states reconstructed using a limited number of samples are essential. version:1
arxiv-1401-4714 | Revolutionary Algorithms | http://arxiv.org/abs/1401.4714 | id:1401.4714 author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE  published:2014-01-19 summary:The optimization of dynamic problems is both widespread and difficult. When conducting dynamic optimization, a balance between reinitialization and computational expense has to be found. There are multiple approaches to this. In parallel genetic algorithms, multiple sub-populations concurrently try to optimize a potentially dynamic problem. But as the number of sub-population increases, their efficiency decreases. Cultural algorithms provide a framework that has the potential to make optimizations more efficient. But they adapt slowly to changing environments. We thus suggest a confluence of these approaches: revolutionary algorithms. These algorithms seek to extend the evolutionary and cultural aspects of the former to approaches with a notion of the political. By modeling how belief systems are changed by means of revolution, these algorithms provide a framework to model and optimize dynamic problems in an efficient fashion. version:1
arxiv-1401-4696 | Evolutionary Optimization for Decision Making under Uncertainty | http://arxiv.org/abs/1401.4696 | id:1401.4696 author:Ronald Hochreiter category:cs.NE  published:2014-01-19 summary:Optimizing decision problems under uncertainty can be done using a variety of solution methods. Soft computing and heuristic approaches tend to be powerful for solving such problems. In this overview article, we survey Evolutionary Optimization techniques to solve Stochastic Programming problems - both for the single-stage and multi-stage case. version:1
arxiv-1401-4674 | Evolving Accuracy: A Genetic Algorithm to Improve Election Night Forecasts | http://arxiv.org/abs/1401.4674 | id:1401.4674 author:Ronald Hochreiter, Christoph Waldhauser category:cs.NE  published:2014-01-19 summary:In this paper, we apply genetic algorithms to the field of electoral studies. Forecasting election results is one of the most exciting and demanding tasks in the area of market research, especially due to the fact that decisions have to be made within seconds on live television. We show that the proposed method outperforms currently applied approaches and thereby provide an argument to tighten the intersection between computer science and social science, especially political science, further. We scrutinize the performance of our algorithm's runtime behavior to evaluate its applicability in the field. Numerical results with real data from a local election in the Austrian province of Styria from 2010 substantiate the applicability of the proposed approach. version:1
arxiv-1401-4648 | Visual Tracking using Particle Swarm Optimization | http://arxiv.org/abs/1401.4648 | id:1401.4648 author:Rafid Siddiqui, Siamak Khatibi category:cs.CV  published:2014-01-19 summary:The problem of robust extraction of visual odometry from a sequence of images obtained by an eye in hand camera configuration is addressed. A novel approach toward solving planar template based tracking is proposed which performs a non-linear image alignment for successful retrieval of camera transformations. In order to obtain global optimum a bio-metaheuristic is used for optimization of similarity among the planar regions. The proposed method is validated on image sequences with real as well as synthetic transformations and found to be resilient to intensity variations. A comparative analysis of the various similarity measures as well as various state-of-art methods reveal that the algorithm succeeds in tracking the planar regions robustly and has good potential to be used in real applications. version:1
arxiv-1401-4634 | The Capacity of String-Replication Systems | http://arxiv.org/abs/1401.4634 | id:1401.4634 author:Farzad Farnoud, Moshe Schwartz, Jehoshua Bruck category:cs.IT cs.CL math.IT  published:2014-01-19 summary:It is known that the majority of the human genome consists of repeated sequences. Furthermore, it is believed that a significant part of the rest of the genome also originated from repeated sequences and has mutated to its current form. In this paper, we investigate the possibility of constructing an exponentially large number of sequences from a short initial sequence and simple replication rules, including those resembling genomic replication processes. In other words, our goal is to find out the capacity, or the expressive power, of these string-replication systems. Our results include exact capacities, and bounds on the capacities, of four fundamental string-replication systems. version:1
arxiv-1401-4612 | Modelling Observation Correlations for Active Exploration and Robust Object Detection | http://arxiv.org/abs/1401.4612 | id:1401.4612 author:Javier Velez, Garrett Hemann, Albert S. Huang, Ingmar Posner, Nicholas Roy category:cs.RO cs.CV  published:2014-01-18 summary:Today, mobile robots are expected to carry out increasingly complex tasks in multifarious, real-world environments. Often, the tasks require a certain semantic understanding of the workspace. Consider, for example, spoken instructions from a human collaborator referring to objects of interest; the robot must be able to accurately detect these objects to correctly understand the instructions. However, existing object detection, while competent, is not perfect. In particular, the performance of detection algorithms is commonly sensitive to the position of the sensor relative to the objects in the scene. This paper presents an online planning algorithm which learns an explicit model of the spatial dependence of object detection and generates plans which maximize the expected performance of the detection, and by extension the overall plan performance. Crucially, the learned sensor model incorporates spatial correlations between measurements, capturing the fact that successive measurements taken at the same or nearby locations are not independent. We show how this sensor model can be incorporated into an efficient forward search algorithm in the information space of detected objects, allowing the robot to generate motion plans efficiently. We investigate the performance of our approach by addressing the tasks of door and text detection in indoor environments and demonstrate significant improvement in detection performance during task execution over alternative methods in simulated and real robot experiments. version:1
arxiv-1401-5674 | Generalized Biwords for Bitext Compression and Translation Spotting | http://arxiv.org/abs/1401.5674 | id:1401.5674 author:Felipe Sánchez-Martínez, Rafael C. Carrasco, Miguel A. Martínez-Prieto, Joaquin Adiego category:cs.CL  published:2014-01-18 summary:Large bilingual parallel texts (also known as bitexts) are usually stored in a compressed form, and previous work has shown that they can be more efficiently compressed if the fact that the two texts are mutual translations is exploited. For example, a bitext can be seen as a sequence of biwords ---pairs of parallel words with a high probability of co-occurrence--- that can be used as an intermediate representation in the compression process. However, the simple biword approach described in the literature can only exploit one-to-one word alignments and cannot tackle the reordering of words. We therefore introduce a generalization of biwords which can describe multi-word expressions and reorderings. We also describe some methods for the binary compression of generalized biword sequences, and compare their performance when different schemes are applied to the extraction of the biword sequence. In addition, we show that this generalization of biwords allows for the implementation of an efficient algorithm to look on the compressed bitext for words or text segments in one of the texts and retrieve their counterpart translations in the other text ---an application usually referred to as translation spotting--- with only some minor modifications in the compression algorithm. version:1
arxiv-1401-5390 | Learning to Win by Reading Manuals in a Monte-Carlo Framework | http://arxiv.org/abs/1401.5390 | id:1401.5390 author:S. R. K. Branavan, David Silver, Regina Barzilay category:cs.CL cs.AI cs.LG  published:2014-01-18 summary:Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization. version:1
arxiv-1401-4603 | Semantic Similarity Measures Applied to an Ontology for Human-Like Interaction | http://arxiv.org/abs/1401.4603 | id:1401.4603 author:Esperanza Albacete, Javier Calle, Elena Castro, Dolores Cuadra category:cs.AI cs.CL  published:2014-01-18 summary:The focus of this paper is the calculation of similarity between two concepts from an ontology for a Human-Like Interaction system. In order to facilitate this calculation, a similarity function is proposed based on five dimensions (sort, compositional, essential, restrictive and descriptive) constituting the structure of ontological knowledge. The paper includes a proposal for computing a similarity function for each dimension of knowledge. Later on, the similarity values obtained are weighted and aggregated to obtain a global similarity measure. In order to calculate those weights associated to each dimension, four training methods have been proposed. The training methods differ in the element to fit: the user, concepts or pairs of concepts, and a hybrid approach. For evaluating the proposal, the knowledge base was fed from WordNet and extended by using a knowledge editing toolkit (Cognos). The evaluation of the proposal is carried out through the comparison of system responses with those given by human test subjects, both providing a measure of the soundness of the procedure and revealing ways in which the proposal may be improved. version:1
arxiv-1402-2561 | The CQC Algorithm: Cycling in Graphs to Semantically Enrich and Enhance a Bilingual Dictionary | http://arxiv.org/abs/1402.2561 | id:1402.2561 author:Tiziano Flati, Roberto Navigli category:cs.CL  published:2014-01-18 summary:Bilingual machine-readable dictionaries are knowledge resources useful in many automatic tasks. However, compared to monolingual computational lexicons like WordNet, bilingual dictionaries typically provide a lower amount of structured information, such as lexical and semantic relations, and often do not cover the entire range of possible translations for a word of interest. In this paper we present Cycles and Quasi-Cycles (CQC), a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machine-readable dictionary. The dictionary is represented as a graph, and cyclic patterns are sought in the graph to assign an appropriate sense tag to each translation in a lexical entry. Further, we use the algorithms output to improve the quality of the dictionary itself, by suggesting accurate solutions to structural problems such as misalignments, partial alignments and missing entries. Finally, we successfully apply CQC to the task of synonym extraction. version:1
arxiv-1401-4590 | Combining Evaluation Metrics via the Unanimous Improvement Ratio and its Application to Clustering Tasks | http://arxiv.org/abs/1401.4590 | id:1401.4590 author:Enrique Amigó, Julio Gonzalo, Javier Artiles, Felisa Verdejo category:cs.AI cs.LG  published:2014-01-18 summary:Many Artificial Intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. A problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. This paper introduces the Unanimous Improvement Ratio (UIR), a measure that complements standard metric combination criteria (such as van Rijsbergen's F-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. UIR is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted. Besides discussing the theoretical foundations of UIR, this paper presents empirical results that confirm the validity and usefulness of the metric for the Text Clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. Remarkably, our experiments show that UIR can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed. version:1
arxiv-1401-4589 | miRNA and Gene Expression based Cancer Classification using Self- Learning and Co-Training Approaches | http://arxiv.org/abs/1401.4589 | id:1401.4589 author:Rania Ibrahim, Noha A. Yousri, Mohamed A. Ismail, Nagwa M. El-Makky category:cs.CE cs.LG  published:2014-01-18 summary:miRNA and gene expression profiles have been proved useful for classifying cancer samples. Efficient classifiers have been recently sought and developed. A number of attempts to classify cancer samples using miRNA/gene expression profiles are known in literature. However, the use of semi-supervised learning models have been used recently in bioinformatics, to exploit the huge corpuses of publicly available sets. Using both labeled and unlabeled sets to train sample classifiers, have not been previously considered when gene and miRNA expression sets are used. Moreover, there is a motivation to integrate both miRNA and gene expression for a semi-supervised cancer classification as that provides more information on the characteristics of cancer samples. In this paper, two semi-supervised machine learning approaches, namely self-learning and co-training, are adapted to enhance the quality of cancer sample classification. These approaches exploit the huge public corpuses to enrich the training data. In self-learning, miRNA and gene based classifiers are enhanced independently. While in co-training, both miRNA and gene expression profiles are used simultaneously to provide different views of cancer samples. To our knowledge, it is the first attempt to apply these learning approaches to cancer classification. The approaches were evaluated using breast cancer, hepatocellular carcinoma (HCC) and lung cancer expression sets. Results show up to 20% improvement in F1-measure over Random Forests and SVM classifiers. Co-Training also outperforms Low Density Separation (LDS) approach by around 25% improvement in F1-measure in breast cancer. version:1
arxiv-1401-4408 | Embedding Graphs under Centrality Constraints for Network Visualization | http://arxiv.org/abs/1401.4408 | id:1401.4408 author:Brian Baingana, Georgios B. Giannakis category:stat.ML  published:2014-01-17 summary:Visual rendering of graphs is a key task in the mapping of complex network data. Although most graph drawing algorithms emphasize aesthetic appeal, certain applications such as travel-time maps place more importance on visualization of structural network properties. The present paper advocates two graph embedding approaches with centrality considerations to comply with node hierarchy. The problem is formulated first as one of constrained multi-dimensional scaling (MDS), and it is solved via block coordinate descent iterations with successive approximations and guaranteed convergence to a KKT point. In addition, a regularization term enforcing graph smoothness is incorporated with the goal of reducing edge crossings. A second approach leverages the locally-linear embedding (LLE) algorithm which assumes that the graph encodes data sampled from a low-dimensional manifold. Closed-form solutions to the resulting centrality-constrained optimization problems are determined yielding meaningful embeddings. Experimental results demonstrate the efficacy of both approaches, especially for visualizing large networks on the order of thousands of nodes. version:1
arxiv-1303-0691 | Learning AMP Chain Graphs and some Marginal Models Thereof under Faithfulness: Extended Version | http://arxiv.org/abs/1303.0691 | id:1303.0691 author:Jose M. Peña category:stat.ML cs.AI cs.LG  published:2013-03-04 summary:This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP) interpretation. In particular, we present a constraint based algorithm for learning an AMP chain graph a given probability distribution is faithful to. Moreover, we show that the extension of Meek's conjecture to AMP chain graphs does not hold, which compromises the development of efficient and correct score+search learning algorithms under assumptions weaker than faithfulness. We also introduce a new family of graphical models that consists of undirected and bidirected edges. We name this new family maximal covariance-concentration graphs (MCCGs) because it includes both covariance and concentration graphs as subfamilies. However, every MCCG can be seen as the result of marginalizing out some nodes in an AMP CG. We describe global, local and pairwise Markov properties for MCCGs and prove their equivalence. We characterize when two MCCGs are Markov equivalent, and show that every Markov equivalence class of MCCGs has a distinguished member. We present a constraint based algorithm for learning a MCCG a given probability distribution is faithful to. Finally, we present a graphical criterion for reading dependencies from a MCCG of a probability distribution that satisfies the graphoid properties, weak transitivity and composition. We prove that the criterion is sound and complete in certain sense. version:3
arxiv-1209-2918 | A new class of metrics for spike trains | http://arxiv.org/abs/1209.2918 | id:1209.2918 author:Cătălin V. Rusu, Răzvan V. Florian category:cs.IT cs.NE math.IT q-bio.NC  published:2012-09-13 summary:The distance between a pair of spike trains, quantifying the differences between them, can be measured using various metrics. Here we introduce a new class of spike train metrics, inspired by the Pompeiu-Hausdorff distance, and compare them with existing metrics. Some of our new metrics (the modulus-metric and the max-metric) have characteristics that are qualitatively different than those of classical metrics like the van Rossum distance or the Victor & Purpura distance. The modulus-metric and the max-metric are particularly suitable for measuring distances between spike trains where information is encoded in bursts, but the number and the timing of spikes inside a burst does not carry information. The modulus-metric does not depend on any parameters and can be computed using a fast algorithm, in a time that depends linearly on the number of spikes in the two spike trains. We also introduce localized versions of the new metrics, which could have the biologically-relevant interpretation of measuring the differences between spike trains as they are perceived at a particular moment in time by a neuron receiving these spike trains. version:3
arxiv-1302-3913 | Multiclass Data Segmentation using Diffuse Interface Methods on Graphs | http://arxiv.org/abs/1302.3913 | id:1302.3913 author:Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi, Arjuna Flenner, Allon Percus category:stat.ML 62-XX  published:2013-02-15 summary:We present two graph-based algorithms for multiclass segmentation of high-dimensional data. The algorithms use a diffuse interface model based on the Ginzburg-Landau functional, related to total variation compressed sensing and image processing. A multiclass extension is introduced using the Gibbs simplex, with the functional's double-well potential modified to handle the multiclass case. The first algorithm minimizes the functional using a convex splitting numerical scheme. The second algorithm is a uses a graph adaptation of the classical numerical Merriman-Bence-Osher (MBO) scheme, which alternates between diffusion and thresholding. We demonstrate the performance of both algorithms experimentally on synthetic data, grayscale and color images, and several benchmark data sets such as MNIST, COIL and WebKB. We also make use of fast numerical solvers for finding the eigenvectors and eigenvalues of the graph Laplacian, and take advantage of the sparsity of the matrix. Experiments indicate that the results are competitive with or better than the current state-of-the-art multiclass segmentation algorithms. version:2
arxiv-1401-4221 | Distortion-driven Turbulence Effect Removal using Variational Model | http://arxiv.org/abs/1401.4221 | id:1401.4221 author:Yuan Xie, Wensheng Zhang, Dacheng Tao, Wenrui Hu, Yanyun Qu, Hanzi Wang category:cs.CV  published:2014-01-17 summary:It remains a challenge to simultaneously remove geometric distortion and space-time-varying blur in frames captured through a turbulent atmospheric medium. To solve, or at least reduce these effects, we propose a new scheme to recover a latent image from observed frames by integrating a new variational model and distortion-driven spatial-temporal kernel regression. The proposed scheme first constructs a high-quality reference image from the observed frames using low-rank decomposition. Then, to generate an improved registered sequence, the reference image is iteratively optimized using a variational model containing a new spatial-temporal regularization. The proposed fast algorithm efficiently solves this model without the use of partial differential equations (PDEs). Next, to reduce blur variation, distortion-driven spatial-temporal kernel regression is carried out to fuse the registered sequence into one image by introducing the concept of the near-stationary patch. Applying a blind deconvolution algorithm to the fused image produces the final output. Extensive experimental testing shows, both qualitatively and quantitatively, that the proposed method can effectively alleviate distortion and blur and recover details of the original scene compared to state-of-the-art methods. version:1
arxiv-1401-4205 | Entropy analysis of word-length series of natural language texts: Effects of text language and genre | http://arxiv.org/abs/1401.4205 | id:1401.4205 author:Maria Kalimeri, Vassilios Constantoudis, Constantinos Papadimitriou, Kostantinos Karamanos, Fotis K. Diakonos, Haris Papageorgiou category:cs.CL physics.data-an  published:2014-01-17 summary:We estimate the $n$-gram entropies of natural language texts in word-length representation and find that these are sensitive to text language and genre. We attribute this sensitivity to changes in the probability distribution of the lengths of single words and emphasize the crucial role of the uniformity of probabilities of having words with length between five and ten. Furthermore, comparison with the entropies of shuffled data reveals the impact of word length correlations on the estimated $n$-gram entropies. version:1
arxiv-1401-4143 | Convex Optimization for Binary Classifier Aggregation in Multiclass Problems | http://arxiv.org/abs/1401.4143 | id:1401.4143 author:Sunho Park, TaeHyun Hwang, Seungjin Choi category:cs.LG  published:2014-01-16 summary:Multiclass problems are often decomposed into multiple binary problems that are solved by individual binary classifiers whose results are integrated into a final answer. Various methods, including all-pairs (APs), one-versus-all (OVA), and error correcting output code (ECOC), have been studied, to decompose multiclass problems into binary problems. However, little study has been made to optimally aggregate binary problems to determine a final answer to the multiclass problem. In this paper we present a convex optimization method for an optimal aggregation of binary classifiers to estimate class membership probabilities in multiclass problems. We model the class membership probability as a softmax function which takes a conic combination of discrepancies induced by individual binary classifiers, as an input. With this model, we formulate the regularized maximum likelihood estimation as a convex optimization problem, which is solved by the primal-dual interior point method. Connections of our method to large margin classifiers are presented, showing that the large margin formulation can be considered as a limiting case of our convex formulation. Numerical experiments on synthetic and real-world data sets demonstrate that our method outperforms existing aggregation methods as well as direct methods, in terms of the classification accuracy and the quality of class membership probability estimates. version:1
arxiv-1401-3291 | Detection of Anomalous Crowd Behavior Using Spatio-Temporal Multiresolution Model and Kronecker Sum Decompositions | http://arxiv.org/abs/1401.3291 | id:1401.3291 author:Kristjan Greenewald, Alfred Hero category:stat.ML  published:2014-01-14 summary:In this work we consider the problem of detecting anomalous spatio-temporal behavior in videos. Our approach is to learn the normative multiframe pixel joint distribution and detect deviations from it using a likelihood based approach. Due to the extreme lack of available training samples relative to the dimension of the distribution, we use a mean and covariance approach and consider methods of learning the spatio-temporal covariance in the low-sample regime. Our approach is to estimate the covariance using parameter reduction and sparse models. The first method considered is the representation of the covariance as a sum of Kronecker products as in (Greenewald et al 2013), which is found to be an accurate approximation in this setting. We propose learning algorithms relevant to our problem. We then consider the sparse multiresolution model of (Choi et al 2010) and apply the Kronecker product methods to it for further parameter reduction, as well as introducing modifications for enhanced efficiency and greater applicability to spatio-temporal covariance matrices. We apply our methods to the detection of crowd behavior anomalies in the University of Minnesota crowd anomaly dataset, and achieve competitive results. version:2
arxiv-1401-4128 | Towards the selection of patients requiring ICD implantation by automatic classification from Holter monitoring indices | http://arxiv.org/abs/1401.4128 | id:1401.4128 author:Charles-Henri Cappelaere, R. Dubois, P. Roussel, G. Dreyfus category:cs.LG stat.AP  published:2014-01-16 summary:The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term ECG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the physiological decomposition of the arrhythmogenic factors into three disjoint groups: the myocardial substrate, the triggers and the autonomic nervous system (ANS). In this ad hoc network, the features were assigned to each group; one neural network classifier per group was designed and its complexity was optimized. The outputs of the classifiers were fed to a single neuron that provided the required probability estimate. The latter was thresholded for final discrimination A dataset composed of 186 pre-implantation 30-mn Holter recordings of patients equipped with an implantable cardioverter defibrillator (ICD) in primary prevention was used in order to design and test this classifier. 44 out of 186 patients underwent at least one treated ventricular arrhythmia during the six-month follow-up period. Performances of the designed classifier were evaluated using a cross-test strategy that consists in splitting the database into several combinations of a training set and a test set. The average arrhythmia prediction performances of the ad-hoc classifier are NPV = 77% $\pm$ 13% and PPV = 31% $\pm$ 19% (Negative Predictive Value $\pm$ std, Positive Predictive Value $\pm$ std). According to our study, improving prophylactic ICD-implantation candidate selection by automatic classification from ECG features may be possible, but the availability of a sizable dataset appears to be essential to decrease the number of False Negatives. version:1
arxiv-1401-4107 | Revisiting loss-specific training of filter-based MRFs for image restoration | http://arxiv.org/abs/1401.4107 | id:1401.4107 author:Yunjin Chen, Thomas Pock, René Ranftl, Horst Bischof category:cs.CV  published:2014-01-16 summary:It is now well known that Markov random fields (MRFs) are particularly effective for modeling image priors in low-level vision. Recent years have seen the emergence of two main approaches for learning the parameters in MRFs: (1) probabilistic learning using sampling-based algorithms and (2) loss-specific training based on MAP estimate. After investigating existing training approaches, it turns out that the performance of the loss-specific training has been significantly underestimated in existing work. In this paper, we revisit this approach and use techniques from bi-level optimization to solve it. We show that we can get a substantial gain in the final performance by solving the lower-level problem in the bi-level framework with high accuracy using our newly proposed algorithm. As a result, our trained model is on par with highly specialized image denoising algorithms and clearly outperforms probabilistically trained MRF models. Our findings suggest that for the loss-specific training scheme, solving the lower-level problem with higher accuracy is beneficial. Our trained model comes along with the additional advantage, that inference is extremely efficient. Our GPU-based implementation takes less than 1s to produce state-of-the-art performance. version:1
arxiv-1401-4105 | Learning $\ell_1$-based analysis and synthesis sparsity priors using bi-level optimization | http://arxiv.org/abs/1401.4105 | id:1401.4105 author:Yunjin Chen, Thomas Pock, Horst Bischof category:cs.CV  published:2014-01-16 summary:We consider the analysis operator and synthesis dictionary learning problems based on the the $\ell_1$ regularized sparse representation model. We reveal the internal relations between the $\ell_1$-based analysis model and synthesis model. We then introduce an approach to learn both analysis operator and synthesis dictionary simultaneously by using a unified framework of bi-level optimization. Our aim is to learn a meaningful operator (dictionary) such that the minimum energy solution of the analysis (synthesis)-prior based model is as close as possible to the ground-truth. We solve the bi-level optimization problem using the implicit differentiation technique. Moreover, we demonstrate the effectiveness of our leaning approach by applying the learned analysis operator (dictionary) to the image denoising task and comparing its performance with state-of-the-art methods. Under this unified framework, we can compare the performance of the two types of priors. version:1
arxiv-1309-5655 | Reweighted message passing revisited | http://arxiv.org/abs/1309.5655 | id:1309.5655 author:Vladimir Kolmogorov category:cs.AI cs.CV cs.LG  published:2013-09-22 summary:We propose a new family of message passing techniques for MAP estimation in graphical models which we call {\em Sequential Reweighted Message Passing} (SRMP). Special cases include well-known techniques such as {\em Min-Sum Diffusion} (MSD) and a faster {\em Sequential Tree-Reweighted Message Passing} (TRW-S). Importantly, our derivation is simpler than the original derivation of TRW-S, and does not involve a decomposition into trees. This allows easy generalizations. We present such a generalization for the case of higher-order graphical models, and test it on several real-world problems with promising results. version:2
arxiv-1212-4093 | Co-clustering separately exchangeable network data | http://arxiv.org/abs/1212.4093 | id:1212.4093 author:David Choi, Patrick J. Wolfe category:math.ST cs.SI math.CO stat.ML stat.TH  published:2012-12-17 summary:This article establishes the performance of stochastic blockmodels in addressing the co-clustering problem of partitioning a binary array into subsets, assuming only that the data are generated by a nonparametric process satisfying the condition of separate exchangeability. We provide oracle inequalities with rate of convergence $\mathcal{O}_P(n^{-1/4})$ corresponding to profile likelihood maximization and mean-square error minimization, and show that the blockmodel can be interpreted in this setting as an optimal piecewise-constant approximation to the generative nonparametric model. We also show for large sample sizes that the detection of co-clusters in such data indicates with high probability the existence of co-clusters of equal size and asymptotically equivalent connectivity in the underlying generative process. version:5
arxiv-1401-3973 | An Empirical Evaluation of Similarity Measures for Time Series Classification | http://arxiv.org/abs/1401.3973 | id:1401.3973 author:Joan Serrà, Josep Lluis Arcos category:cs.LG cs.CV stat.ML  published:2014-01-16 summary:Time series are ubiquitous, and a measure to assess their similarity is a core part of many computational systems. In particular, the similarity measure is the most essential ingredient of time series clustering and classification systems. Because of this importance, countless approaches to estimate time series similarity have been proposed. However, there is a lack of comparative studies using empirical, rigorous, quantitative, and large-scale assessment strategies. In this article, we provide an extensive evaluation of similarity measures for time series classification following the aforementioned principles. We consider 7 different measures coming from alternative measure `families', and 45 publicly-available time series data sets coming from a wide variety of scientific domains. We focus on out-of-sample classification accuracy, but in-sample accuracies and parameter choices are also discussed. Our work is based on rigorous evaluation methodologies and includes the use of powerful statistical significance tests to derive meaningful conclusions. The obtained results show the equivalence, in terms of accuracy, of a number of measures, but with one single candidate outperforming the rest. Such findings, together with the followed methodology, invite researchers on the field to adopt a more consistent evaluation criteria and a more informed decision regarding the baseline measures to which new developments should be compared. version:1
arxiv-1401-3940 | Nonparametric Latent Tree Graphical Models: Inference, Estimation, and Structure Learning | http://arxiv.org/abs/1401.3940 | id:1401.3940 author:Le Song, Han Liu, Ankur Parikh, Eric Xing category:stat.ML  published:2014-01-16 summary:Tree structured graphical models are powerful at expressing long range or hierarchical dependency among many variables, and have been widely applied in different areas of computer science and statistics. However, existing methods for parameter estimation, inference, and structure learning mainly rely on the Gaussian or discrete assumptions, which are restrictive under many applications. In this paper, we propose new nonparametric methods based on reproducing kernel Hilbert space embeddings of distributions that can recover the latent tree structures, estimate the parameters, and perform inference for high dimensional continuous and non-Gaussian variables. The usefulness of the proposed methods are illustrated by thorough numerical results. version:1
arxiv-1308-5609 | Frequency Recognition in SSVEP-based BCI using Multiset Canonical Correlation Analysis | http://arxiv.org/abs/1308.5609 | id:1308.5609 author:Yu Zhang, Guoxu Zhou, Jing Jin, Xingyu Wang, Andrzej Cichocki category:stat.ML  published:2013-08-26 summary:Canonical correlation analysis (CCA) has been one of the most popular methods for frequency recognition in steady-state visual evoked potential (SSVEP)-based brain-computer interfaces (BCIs). Despite its efficiency, a potential problem is that using pre-constructed sine-cosine waves as the required reference signals in the CCA method often does not result in the optimal recognition accuracy due to their lack of features from the real EEG data. To address this problem, this study proposes a novel method based on multiset canonical correlation analysis (MsetCCA) to optimize the reference signals used in the CCA method for SSVEP frequency recognition. The MsetCCA method learns multiple linear transforms that implement joint spatial filtering to maximize the overall correlation among canonical variates, and hence extracts SSVEP common features from multiple sets of EEG data recorded at the same stimulus frequency. The optimized reference signals are formed by combination of the common features and completely based on training data. Experimental study with EEG data from ten healthy subjects demonstrates that the MsetCCA method improves the recognition accuracy of SSVEP frequency in comparison with the CCA method and other two competing methods (multiway CCA (MwayCCA) and phase constrained CCA (PCCA)), especially for a small number of channels and a short time window length. The superiority indicates that the proposed MsetCCA method is a new promising candidate for frequency recognition in SSVEP-based BCIs. version:2
arxiv-1401-3908 | Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity | http://arxiv.org/abs/1401.3908 | id:1401.3908 author:Ricardo Ribeiro, David Martins de Matos category:cs.IR cs.CL  published:2014-01-16 summary:In automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. This model produces extractive summaries that are generic, and language- and domain-independent. Thorough automatic evaluation shows that the method achieves state-of-the-art performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches. version:1
arxiv-1401-3907 | Policy Invariance under Reward Transformations for General-Sum Stochastic Games | http://arxiv.org/abs/1401.3907 | id:1401.3907 author:Xiaosong Lu, Howard M. Schwartz, Sidney N. Givigi Jr category:cs.GT cs.LG  published:2014-01-16 summary:We extend the potential-based shaping method from Markov decision processes to multi-player general-sum stochastic games. We prove that the Nash equilibria in a stochastic game remains unchanged after potential-based shaping is applied to the environment. The property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game. version:1
arxiv-1401-6131 | Controlling Complexity in Part-of-Speech Induction | http://arxiv.org/abs/1401.6131 | id:1401.6131 author:João V. Graça, Kuzman Ganchev, Luisa Coheur, Fernando Pereira, Ben Taskar category:cs.CL cs.LG  published:2014-01-16 summary:We consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. The standard maximum-likelihood hidden Markov model for this task performs poorly, because of its weak inductive bias and large model capacity. We address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. Our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. We develop an efficient learning algorithm that is not much more computationally intensive than standard training. We also provide an open-source implementation of the algorithm. Our experiments on five diverse languages (Bulgarian, Danish, English, Portuguese, Spanish) achieve significant improvements compared with previous methods for the same task. version:1
arxiv-1401-3894 | Efficient Multi-Start Strategies for Local Search Algorithms | http://arxiv.org/abs/1401.3894 | id:1401.3894 author:András György, Levente Kocsis category:cs.LG cs.AI stat.ML  published:2014-01-16 summary:Local search algorithms applied to optimization problems often suffer from getting trapped in a local optimum. The common solution for this deficiency is to restart the algorithm when no progress is observed. Alternatively, one can start multiple instances of a local search algorithm, and allocate computational resources (in particular, processing time) to the instances depending on their behavior. Hence, a multi-start strategy has to decide (dynamically) when to allocate additional resources to a particular instance and when to start new instances. In this paper we propose multi-start strategies motivated by works on multi-armed bandit problems and Lipschitz optimization with an unknown constant. The strategies continuously estimate the potential performance of each algorithm instance by supposing a convergence rate of the local search algorithm up to an unknown constant, and in every phase allocate resources to those instances that could converge to the optimum for a particular range of the constant. Asymptotic bounds are given on the performance of the strategies. In particular, we prove that at most a quadratic increase in the number of times the target function is evaluated is needed to achieve the performance of a local search algorithm started from the attraction region of the optimum. Experiments are provided using SPSA (Simultaneous Perturbation Stochastic Approximation) and k-means as local search algorithms, and the results indicate that the proposed strategies work well in practice, and, in all cases studied, need only logarithmically more evaluations of the target function as opposed to the theoretically suggested quadratic increase. version:1
arxiv-1401-3880 | Regression Conformal Prediction with Nearest Neighbours | http://arxiv.org/abs/1401.3880 | id:1401.3880 author:Harris Papadopoulos, Vladimir Vovk, Alex Gammerman category:cs.LG  published:2014-01-16 summary:In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours Regression (k-NNR) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. Unlike traditional regression methods which produce point predictions, Conformal Predictors output predictive regions that satisfy a given confidence level. The regions produced by any Conformal Predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each CP. In effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. We define six novel nonconformity measures based on the k-Nearest Neighbours Regression algorithm and develop the corresponding CPs following both the original (transductive) and the inductive CP approaches. A comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures. version:1
arxiv-1401-3877 | Properties of Bethe Free Energies and Message Passing in Gaussian Models | http://arxiv.org/abs/1401.3877 | id:1401.3877 author:Botond Cseke, Tom Heskes category:cs.LG cs.AI stat.ML  published:2014-01-16 summary:We address the problem of computing approximate marginals in Gaussian probabilistic models by using mean field and fractional Bethe approximations. We define the Gaussian fractional Bethe free energy in terms of the moment parameters of the approximate marginals, derive a lower and an upper bound on the fractional Bethe free energy and establish a necessary condition for the lower bound to be bounded from below. It turns out that the condition is identical to the pairwise normalizability condition, which is known to be a sufficient condition for the convergence of the message passing algorithm. We show that stable fixed points of the Gaussian message passing algorithm are local minima of the Gaussian Bethe free energy. By a counterexample, we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm. version:1
arxiv-1401-3871 | Non-Deterministic Policies in Markovian Decision Processes | http://arxiv.org/abs/1401.3871 | id:1401.3871 author:Mahdi Milani Fard, Joelle Pineau category:cs.AI cs.LG  published:2014-01-16 summary:Markovian processes have long been used to model stochastic environments. Reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. In recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in Markovian environments. Although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the users input. Without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems. This paper introduces the new concept of non-deterministic policies to allow more flexibility in the users decision-making process, while constraining decisions to remain near optimal solutions. We provide two algorithms to compute non-deterministic policies in discrete domains. We study the output and running time of these method on a set of synthetic and real-world problems. In an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task. version:1
arxiv-1401-3870 | Learning to Make Predictions In Partially Observable Environments Without a Generative Model | http://arxiv.org/abs/1401.3870 | id:1401.3870 author:Erik Talvitie, Satinder Singh category:cs.LG cs.AI stat.ML  published:2014-01-16 summary:When faced with the problem of learning a model of a high-dimensional environment, a common approach is to limit the model to make only a restricted set of predictions, thereby simplifying the learning problem. These partial models may be directly useful for making decisions or may be combined together to form a more complete, structured model. However, in partially observable (non-Markov) environments, standard model-learning methods learn generative models, i.e. models that provide a probability distribution over all possible futures (such as POMDPs). It is not straightforward to restrict such models to make only certain predictions, and doing so does not always simplify the learning problem. In this paper we present prediction profile models: non-generative partial models for partially observable systems that make only a given set of predictions, and are therefore far simpler than generative models in some cases. We formalize the problem of learning a prediction profile model as a transformation of the original model-learning problem, and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models. version:1
arxiv-1405-5202 | Narrowing the Modeling Gap: A Cluster-Ranking Approach to Coreference Resolution | http://arxiv.org/abs/1405.5202 | id:1405.5202 author:Altaf Rahman, Vincent Ng category:cs.CL  published:2014-01-16 summary:Traditional learning-based coreference resolvers operate by training the mention-pair model for determining whether two mentions are coreferent or not. Though conceptually simple and easy to understand, the mention-pair model is linguistically rather unappealing and lags far behind the heuristic-based coreference models proposed in the pre-statistical NLP era in terms of sophistication. Two independent lines of recent research have attempted to improve the mention-pair model, one by acquiring the mention-ranking model to rank preceding mentions for a given anaphor, and the other by training the entity-mention model to determine whether a preceding cluster is coreferent with a given mention. We propose a cluster-ranking approach to coreference resolution, which combines the strengths of the mention-ranking model and the entity-mention model, and is therefore theoretically more appealing than both of these models. In addition, we seek to improve cluster rankers via two extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity by jointly modeling anaphoricity determination and coreference resolution. Experimental results on the ACE data sets demonstrate the superior performance of cluster rankers to competing approaches as well as the effectiveness of our two extensions. version:1
arxiv-1401-3865 | Evaluating Temporal Graphs Built from Texts via Transitive Reduction | http://arxiv.org/abs/1401.3865 | id:1401.3865 author:Xavier Tannier, Philippe Muller category:cs.CL cs.IR  published:2014-01-16 summary:Temporal information has been the focus of recent attention in information extraction, leading to some standardization effort, in particular for the task of relating events in a text. This task raises the problem of comparing two annotations of a given text, because relations between events in a story are intrinsically interdependent and cannot be evaluated separately. A proper evaluation measure is also crucial in the context of a machine learning approach to the problem. Finding a common comparison referent at the text level is not obvious, and we argue here in favor of a shift from event-based measures to measures on a unique textual object, a minimal underlying temporal graph, or more formally the transitive reduction of the graph of relations between event boundaries. We support it by an investigation of its properties on synthetic data and on a well-know temporal corpus. version:1
arxiv-1406-3270 | Kalman Temporal Differences | http://arxiv.org/abs/1406.3270 | id:1406.3270 author:Matthieu Geist, Olivier Pietquin category:cs.LG  published:2014-01-16 summary:Because reinforcement learning suffers from a lack of scalability, online value (and Q-) function approximation has received increasing interest this last decade. This contribution introduces a novel approximation scheme, namely the Kalman Temporal Differences (KTD) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. A first KTD-based algorithm is provided for deterministic Markov Decision Processes (MDP) which produces biased estimates in the case of stochastic transitions. Than the eXtended KTD framework (XKTD), solving stochastic MDP, is described. Convergence is analyzed for special cases for both deterministic and stochastic transitions. Related algorithms are experimented on classical benchmarks. They compare favorably to the state of the art while exhibiting the announced features. version:1
arxiv-1401-5389 | Which Clustering Do You Want? Inducing Your Ideal Clustering with Minimal Feedback | http://arxiv.org/abs/1401.5389 | id:1401.5389 author:Sajib Dasgupta, Vincent Ng category:cs.IR cs.CL cs.LG  published:2014-01-16 summary:While traditional research on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the authors mood, gender, age, or sentiment. Without knowing the users intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. To address the problem of clustering documents along the user-desired dimension, previous work has focused on learning a similarity metric from data manually annotated with the users intention or having a human construct a feature space in an interactive manner during the clustering process. With the goal of reducing reliance on human knowledge for fine-tuning the similarity function or selecting the relevant features required by these approaches, we propose a novel active clustering algorithm, which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words. We demonstrate the viability of our algorithm on a variety of commonly-used sentiment datasets. version:1
arxiv-1401-4436 | Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction | http://arxiv.org/abs/1401.4436 | id:1401.4436 author:Muhammad Arshad Ul Abedin, Vincent Ng, Latifur Khan category:cs.CL cs.LG  published:2014-01-16 summary:The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloffs Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly. version:1
arxiv-1405-7713 | Using Local Alignments for Relation Recognition | http://arxiv.org/abs/1405.7713 | id:1405.7713 author:Sophia Katrenko, Pieter Adriaans, Maarten van Someren category:cs.CL cs.IR cs.LG  published:2014-01-16 summary:This paper discusses the problem of marrying structural similarity with semantic relatedness for Information Extraction from text. Aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. We give a definition of a local alignment (LA) kernel based on the Smith-Waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. We show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. Our experiments suggest that the LA kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. Additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the LA kernel is comparable to the current state-of-the-art results. version:1
arxiv-1401-3836 | An Active Learning Approach for Jointly Estimating Worker Performance and Annotation Reliability with Crowdsourced Data | http://arxiv.org/abs/1401.3836 | id:1401.3836 author:Liyue Zhao, Yu Zhang, Gita Sukthankar category:cs.LG cs.HC  published:2014-01-16 summary:Crowdsourcing platforms offer a practical solution to the problem of affordably annotating large datasets for training supervised classifiers. Unfortunately, poor worker performance frequently threatens to compromise annotation reliability, and requesting multiple labels for every instance can lead to large cost increases without guaranteeing good results. Minimizing the required training samples using an active learning selection procedure reduces the labeling requirement but can jeopardize classifier training by focusing on erroneous annotations. This paper presents an active learning approach in which worker performance, task difficulty, and annotation reliability are jointly estimated and used to compute the risk function guiding the sample selection procedure. We demonstrate that the proposed approach, which employs active learning with Bayesian networks, significantly improves training accuracy and correctly ranks the expertise of unknown labelers in the presence of annotation noise. version:1
arxiv-1401-3832 | Constructing Reference Sets from Unstructured, Ungrammatical Text | http://arxiv.org/abs/1401.3832 | id:1401.3832 author:Matthew Michelson, Craig A. Knoblock category:cs.CL cs.IR  published:2014-01-16 summary:Vast amounts of text on the Web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. We call such text "posts." Despite their inconsistent structure and lack of grammar, posts are full of useful information. This paper presents work on semi-automatically building tables of relational information, called "reference sets," by analyzing such posts directly. Reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. Our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. We also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. To evaluate the utility of the machine-constructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. Our results show the reference sets constructed by our method outperform manually constructed reference sets. We also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. These results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data. version:1
arxiv-1401-6875 | Context-based Word Acquisition for Situated Dialogue in a Virtual World | http://arxiv.org/abs/1401.6875 | id:1401.6875 author:Shaolin Qu, Joyce Y. Chai category:cs.CL  published:2014-01-16 summary:To tackle the vocabulary problem in conversational systems, previous work has applied unsupervised learning approaches on co-occurring speech and eye gaze during interaction to automatically acquire new words. Although these approaches have shown promise, several issues related to human language behavior and human-machine conversation have not been addressed. First, psycholinguistic studies have shown certain temporal regularities between human eye movement and language production. While these regularities can potentially guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. Second, conversational systems generally have an existing knowledge base about the domain and vocabulary. While the existing knowledge can potentially help bootstrap and constrain the acquired new words, it has not been incorporated in the previous models. Third, eye gaze could serve different functions in human-machine conversation. Some gaze streams may not be closely coupled with speech stream, and thus are potentially detrimental to word acquisition. Automated recognition of closely-coupled speech-gaze streams based on conversation context is important. To address these issues, we developed new approaches that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. We evaluated these approaches in the context of situated dialogue in a virtual world. Our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance. version:1
arxiv-1401-3829 | RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel | http://arxiv.org/abs/1401.3829 | id:1401.3829 author:Amy Greenwald, Seong Jae Lee, Victor Naroditskiy category:cs.GT cs.LG  published:2014-01-16 summary:In this paper, we describe our autonomous bidding agent, RoxyBot, who emerged victorious in the travel division of the 2006 Trading Agent Competition in a photo finish. At a high level, the design of many successful trading agents can be summarized as follows: (i) price prediction: build a model of market prices; and (ii) optimization: solve for an approximately optimal set of bids, given this model. To predict, RoxyBot builds a stochastic model of market prices by simulating simultaneous ascending auctions. To optimize, RoxyBot relies on the sample average approximation method, a stochastic optimization technique. version:1
arxiv-1405-7711 | Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language | http://arxiv.org/abs/1405.7711 | id:1405.7711 author:David L. Chen, Joohyun Kim, Raymond J. Mooney category:cs.CL  published:2014-01-16 summary:We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain. version:1
arxiv-1312-4824 | Generation, Implementation and Appraisal of an N-gram based Stemming Algorithm | http://arxiv.org/abs/1312.4824 | id:1312.4824 author:B. P. Pande, Pawan Tamta, H. S. Dhami category:cs.IR cs.CL  published:2013-12-17 summary:A language independent stemmer has always been looked for. Single N-gram tokenization technique works well, however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N gram stemming one step ahead and compare our method with an established algorithm in the field, Porter's Stemmer. Results indicate that our N gram stemmer is not inferior to Porter's linguistic stemmer. version:2
arxiv-1312-2298 | On the Estimation of Pointwise Dimension | http://arxiv.org/abs/1312.2298 | id:1312.2298 author:Shohei Hidaka, Neeraj Kashyap category:physics.data-an math.DS nlin.CD physics.comp-ph stat.ML  published:2013-12-09 summary:Our goal in this paper is to develop an effective estimator of fractal dimension. We survey existing ideas in dimension estimation, with a focus on the currently popular method of Grassberger and Procaccia for the estimation of correlation dimension. There are two major difficulties in estimation based on this method. The first is the insensitivity of correlation dimension itself to differences in dimensionality over data, which we term "dimension blindness". The second comes from the reliance of the method on the inference of limiting behavior from finite data. We propose pointwise dimension as an object for estimation in response to the dimension blindness of correlation dimension. Pointwise dimension is a local quantity, and the distribution of pointwise dimensions over the data contains the information to which correlation dimension is blind. We use a "limit-free" description of pointwise dimension to develop a new estimator. We conclude by discussing potential applications of our estimator as well as some challenges it raises. version:3
arxiv-1401-3818 | Structured Priors for Sparse-Representation-Based Hyperspectral Image Classification | http://arxiv.org/abs/1401.3818 | id:1401.3818 author:Xiaoxia Sun, Qing Qu, Nasser M. Nasrabadi, Trac D. Tran category:cs.CV cs.LG stat.ML  published:2014-01-16 summary:Pixel-wise classification, where each pixel is assigned to a predefined class, is one of the most important procedures in hyperspectral image (HSI) analysis. By representing a test pixel as a linear combination of a small subset of labeled pixels, a sparse representation classifier (SRC) gives rather plausible results compared with that of traditional classifiers such as the support vector machine (SVM). Recently, by incorporating additional structured sparsity priors, the second generation SRCs have appeared in the literature and are reported to further improve the performance of HSI. These priors are based on exploiting the spatial dependencies between the neighboring pixels, the inherent structure of the dictionary, or both. In this paper, we review and compare several structured priors for sparse-representation-based HSI classification. We also propose a new structured prior called the low rank group prior, which can be considered as a modification of the low rank prior. Furthermore, we will investigate how different structured priors improve the result for the HSI classification. version:1
arxiv-1401-3813 | Seeded Graph Matching Via Joint Optimization of Fidelity and Commensurability | http://arxiv.org/abs/1401.3813 | id:1401.3813 author:Vince Lyzinski, Sancar Adali, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe category:stat.ML stat.AP stat.ME  published:2014-01-16 summary:We present a novel approximate graph matching algorithm that incorporates seeded data into the graph matching paradigm. Our Joint Optimization of Fidelity and Commensurability (JOFC) algorithm embeds two graphs into a common Euclidean space where the matching inference task can be performed. Through real and simulated data examples, we demonstrate the versatility of our algorithm in matching graphs with various characteristics--weightedness, directedness, loopiness, many-to-one and many-to-many matchings, and soft seedings. version:1
arxiv-1401-3737 | Coordinate Descent with Online Adaptation of Coordinate Frequencies | http://arxiv.org/abs/1401.3737 | id:1401.3737 author:Tobias Glasmachers, Ürün Dogan category:stat.ML cs.LG  published:2014-01-15 summary:Coordinate descent (CD) algorithms have become the method of choice for solving a number of optimization problems in machine learning. They are particularly popular for training linear models, including linear support vector machine classification, LASSO regression, and logistic regression. We consider general CD with non-uniform selection of coordinates. Instead of fixing selection frequencies beforehand we propose an online adaptation mechanism for this important parameter, called the adaptive coordinate frequencies (ACF) method. This mechanism removes the need to estimate optimal coordinate frequencies beforehand, and it automatically reacts to changing requirements during an optimization run. We demonstrate the usefulness of our ACF-CD approach for a variety of optimization problems arising in machine learning contexts. Our algorithm offers significant speed-ups over state-of-the-art training methods. version:1
arxiv-1401-3069 | Use Case Point Approach Based Software Effort Estimation using Various Support Vector Regression Kernel Methods | http://arxiv.org/abs/1401.3069 | id:1401.3069 author:Shashank Mouli Satapathy, Santanu Kumar Rath category:cs.SE cs.LG  published:2014-01-14 summary:The job of software effort estimation is a critical one in the early stages of the software development life cycle when the details of requirements are usually not clearly identified. Various optimization techniques help in improving the accuracy of effort estimation. The Support Vector Regression (SVR) is one of several different soft-computing techniques that help in getting optimal estimated values. The idea of SVR is based upon the computation of a linear regression function in a high dimensional feature space where the input data are mapped via a nonlinear function. Further, the SVR kernel methods can be applied in transforming the input data and then based on these transformations, an optimal boundary between the possible outputs can be obtained. The main objective of the research work carried out in this paper is to estimate the software effort using use case point approach. The use case point approach relies on the use case diagram to estimate the size and effort of software projects. Then, an attempt has been made to optimize the results obtained from use case point analysis using various SVR kernel methods to achieve better prediction accuracy. version:2
arxiv-1401-3669 | Hrebs and Cohesion Chains as similar tools for semantic text properties research | http://arxiv.org/abs/1401.3669 | id:1401.3669 author:D. Tatar, M. Lupea, E. Kapetanios category:cs.CL  published:2014-01-15 summary:In this study it is proven that the Hrebs used in Denotation analysis of texts and Cohesion Chains (de?ned as a fusion between Lexical Chains and Coreference Chains) represent similar linguistic tools. This result gives us the possibility to extend to Cohesion Chains (CCs) some important indicators as, for example the Kernel of CCs, the topicality of a CC, text concentration, CC-di?useness and mean di?useness of the text. Let us mention that nowhere in the Lexical Chains or Coreference Chains literature these kinds of indicators are introduced and used since now. Similarly, some applications of CCs in the study of a text (as for example segmentation or summarization of a text) could be realized starting from hrebs. As an illustration of the similarity between Hrebs and CCs a detailed analyze of the poem "Lacul" by Mihai Eminescu is given. version:1
arxiv-1401-3592 | Intelligent Systems for Information Security | http://arxiv.org/abs/1401.3592 | id:1401.3592 author:Ayman M. Bahaa-Eldin category:cs.NE cs.CR  published:2014-01-15 summary:This thesis aims to use intelligent systems to extend and improve performance and security of cryptographic techniques. Genetic algorithms framework for cryptanalysis problem is addressed. A novel extension to the differential cryptanalysis using genetic algorithm is proposed and a fitness measure based on the differential characteristics of the cipher being attacked is also proposed. The complexity of the proposed attack is shown to be less than quarter of normal differential cryptanalysis of the same cipher by applying the proposed attack to both the basic Substitution Permutation Network and the Feistel Network. The basic models of modern block ciphers are attacked instead of actual cipher to prove that the attack is applicable to other ciphers vulnerable to differential cryptanalysis. A new attack for block cipher based on the ability of neural networks to perform an approximation of mapping is proposed. A complete problem formulation is explained and implementation of the attack on some hypothetical Feistel cipher not vulnerable to differential or linear attacks is presented. A new block cipher based on the neural networks is proposed. A complete cipher structure is given and a key scheduling is also shown. The main properties of neural network being able to perform mapping between large dimension domains in a very fast and a very small memory compared to S-Boxes is used as a base for the cipher. version:1
arxiv-1401-3510 | Improving Performance Of English-Hindi Cross Language Information Retrieval Using Transliteration Of Query Terms | http://arxiv.org/abs/1401.3510 | id:1401.3510 author:Saurabh Varshney, Jyoti Bajpai category:cs.IR cs.CL  published:2014-01-15 summary:The main issue in Cross Language Information Retrieval (CLIR) is the poor performance of retrieval in terms of average precision when compared to monolingual retrieval performance. The main reasons behind poor performance of CLIR are mismatching of query terms, lexical ambiguity and un-translated query terms. The existing problems of CLIR are needed to be addressed in order to increase the performance of the CLIR system. In this paper, we are putting our effort to solve the given problem by proposed an algorithm for improving the performance of English-Hindi CLIR system. We used all possible combination of Hindi translated query using transliteration of English query terms and choosing the best query among them for retrieval of documents. The experiment is performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets. The experimental result show that the proposed approach gives better performance of English-Hindi CLIR system and also helps in overcoming existing problems and outperforms the existing English-Hindi CLIR system in terms of average precision. version:1
arxiv-1401-5699 | Text Relatedness Based on a Word Thesaurus | http://arxiv.org/abs/1401.5699 | id:1401.5699 author:George Tsatsaronis, Iraklis Varlamis, Michalis Vazirgiannis category:cs.CL  published:2014-01-15 summary:The computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. Without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. Such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. In this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. The approach exploits only a word thesaurus in order to devise implicit semantic links between words. Based on this approach, we introduce Omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (SR) and extends it to measure the relatedness between texts. We gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely sentence-to-sentence similarity and paraphrase recognition. Experimental evaluation shows that the proposed method outperforms every lexicon-based method of semantic relatedness in the selected tasks and the used data sets, and competes well against corpus-based and hybrid approaches. version:1
arxiv-1401-5694 | Cross-lingual Annotation Projection for Semantic Roles | http://arxiv.org/abs/1401.5694 | id:1401.5694 author:Sebastian Pado, Mirella Lapata category:cs.CL  published:2014-01-15 summary:This article considers the task of automatically inducing role-semantic annotations in the FrameNet paradigm for new languages. We propose a general framework that is based on annotation projection, phrased as a graph optimization problem. It is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic resources. Within this framework, we present projection models that exploit lexical and syntactic information. We provide an experimental evaluation on an English-German parallel corpus which demonstrates the feasibility of inducing high-precision German semantic role annotation both for manually and automatically annotated English data. version:1
arxiv-1401-5695 | Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches | http://arxiv.org/abs/1401.5695 | id:1401.5695 author:Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, Regina Barzilay category:cs.CL  published:2014-01-15 summary:We demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. The central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. We consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. Both approaches are formulated as hierarchical Bayesian models, using Markov Chain Monte Carlo sampling techniques for inference. Our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. We also found that performance improves steadily as the number of available languages increases. version:1
arxiv-1401-3488 | Content Modeling Using Latent Permutations | http://arxiv.org/abs/1401.3488 | id:1401.3488 author:Harr Chen, S. R. K. Branavan, Regina Barzilay, David R. Karger category:cs.IR cs.CL cs.LG  published:2014-01-15 summary:We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods. version:1
arxiv-1401-3482 | Enhancing QA Systems with Complex Temporal Question Processing Capabilities | http://arxiv.org/abs/1401.3482 | id:1401.3482 author:Estela Saquete, Jose Luis Vicedo, Patricio Martínez-Barco, Rafael Muñoz, Hector Llorens category:cs.CL cs.AI cs.IR  published:2014-01-15 summary:This paper presents a multilayered architecture that enhances the capabilities of current QA systems and allows different types of complex questions or queries to be processed. The answers to these questions need to be gathered from factual information scattered throughout different documents. Specifically, we designed a specialized layer to process the different types of temporal questions. Complex temporal questions are first decomposed into simple questions, according to the temporal relations expressed in the original question. In the same way, the answers to the resulting simple questions are recomposed, fulfilling the temporal restrictions of the original complex question. A novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources, with the final aim of obtaining a portable platform that is easily extensible to other languages. In this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level. The temporal layer was first performed for English, then evaluated and compared with: a) a general purpose QA system (F-measure 65.47% for QA plus English temporal layer vs. 38.01% for the general QA system), and b) a well-known QA system. Much better results were obtained for temporal questions with the multilayered system. This system was therefore extended to Spanish and very good results were again obtained in the evaluation (F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general QA system). version:1
arxiv-1401-3479 | Complex Question Answering: Unsupervised Learning Approaches and Experiments | http://arxiv.org/abs/1401.3479 | id:1401.3479 author:Yllias Chali, Shafiq Rayhan Joty, Sadid A. Hasan category:cs.CL cs.IR cs.LG  published:2014-01-15 summary:Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. In this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: K-means and Expectation Maximization (EM), for computing relative importance of the sentences. We compare the results of these approaches. Our experiments show that the empirical approach outperforms the other two techniques and EM performs better than K-means. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. In order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. We use a local search technique to learn the weights of the features. To the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). For each of our methods of generating summaries (i.e. empirical, K-means and EM) we show the effects of syntactic and shallow-semantic features over the bag-of-words (BOW) features. version:1
arxiv-1401-3478 | Efficient Markov Network Structure Discovery Using Independence Tests | http://arxiv.org/abs/1401.3478 | id:1401.3478 author:Facundo Bromberg, Dimitris Margaritis, Vasant Honavar category:cs.LG cs.AI stat.ML  published:2014-01-15 summary:We present two algorithms for learning the structure of a Markov network from data: GSMN* and GSIMN. Both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests. Until very recently, algorithms for structure learning were based on maximum likelihood estimation, which has been proved to be NP-hard for Markov networks due to the difficulty of estimating the parameters of the network, needed for the computation of the data likelihood. The independence-based approach does not require the computation of the likelihood, and thus both GSMN* and GSIMN can compute the structure efficiently (as shown in our experiments). GSMN* is an adaptation of the Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of Bayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls well-known properties of the conditional independence relation to infer novel independences from known ones, thus avoiding the performance of statistical tests to estimate them. To accomplish this efficiently GSIMN uses the Triangle theorem, also introduced in this work, which is a simplified version of the set of Markov axioms. Experimental comparisons on artificial and real-world data sets show GSIMN can yield significant savings with respect to GSMN*, while generating a Markov network with comparable or in some cases improved quality. We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH, that produces all possible conditional independences resulting from repeatedly applying Pearls theorems on the known conditional independence tests. The results of this comparison show that GSIMN, by the sole use of the Triangle theorem, is nearly optimal in terms of the set of independences tests that it infers. version:1
arxiv-1401-5696 | Unsupervised Methods for Determining Object and Relation Synonyms on the Web | http://arxiv.org/abs/1401.5696 | id:1401.5696 author:Alexander Pieter Yates, Oren Etzioni category:cs.CL  published:2014-01-15 summary:The task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. This paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. The paper presents a scalable, fully-implemented system that runs in O(KN log N) time in the number of extractions, N, and the maximum number of synonyms per word, K. The system, called Resolver, introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. On a set of two million assertions extracted from the Web, Resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. Several variations of resolvers probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve F1 by 5%. An extension to the basic Resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the TREC corpus. version:1
arxiv-1401-5700 | Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora | http://arxiv.org/abs/1401.5700 | id:1401.5700 author:Felipe Sánchez-Martínez, Mikel L. Forcada category:cs.CL  published:2014-01-15 summary:This paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (MT) system from small parallel corpora. The structural transfer rules are based on alignment templates, like those used in statistical MT. Alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the MT system and control their application as transfer rules. The experiments conducted using three different language pairs in the free/open-source MT platform Apertium show that translation quality is improved as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is close to that obtained using hand-coded transfer rules. The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied. version:1
arxiv-1401-3464 | Learning Bayesian Network Equivalence Classes with Ant Colony Optimization | http://arxiv.org/abs/1401.3464 | id:1401.3464 author:Rónán Daly, Qiang Shen category:cs.NE cs.AI cs.LG  published:2014-01-15 summary:Bayesian networks are a useful tool in the representation of uncertain knowledge. This paper proposes a new algorithm called ACO-E, to learn the structure of a Bayesian network. It does this by conducting a search through the space of equivalence classes of Bayesian networks using Ant Colony Optimization (ACO). To this end, two novel extensions of traditional ACO techniques are proposed and implemented. Firstly, multiple types of moves are allowed. Secondly, moves can be given in terms of indices that are not based on construction graph nodes. The results of testing show that ACO-E performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes. version:1
arxiv-1401-5697 | Wikipedia-based Semantic Interpretation for Natural Language Processing | http://arxiv.org/abs/1401.5697 | id:1401.5697 author:Evgeniy Gabrilovich, Shaul Markovitch category:cs.CL  published:2014-01-15 summary:Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users. version:1
arxiv-1401-5693 | Sentence Compression as Tree Transduction | http://arxiv.org/abs/1401.5693 | id:1401.5693 author:Trevor Anthony Cohn, Mirella Lapata category:cs.CL  published:2014-01-15 summary:This paper presents a tree-to-tree transduction method for sentence compression. Our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. We describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. Experimental results on sentence compression bring significant improvements over a state-of-the-art model. version:1
arxiv-1401-3457 | Learning Document-Level Semantic Properties from Free-Text Annotations | http://arxiv.org/abs/1401.3457 | id:1401.3457 author:S. R. K. Branavan, Harr Chen, Jacob Eisenstein, Regina Barzilay category:cs.CL cs.IR  published:2014-01-15 summary:This paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. Such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. One especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as a real bargain or good value. These annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. To learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. The paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. Our approach is implemented as a hierarchical Bayesian model with joint inference. We find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. Multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases. version:1
arxiv-1401-3454 | A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics | http://arxiv.org/abs/1401.3454 | id:1401.3454 author:Sherief Abdallah, Victor Lesser category:cs.LG cs.MA  published:2014-01-15 summary:Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms. version:1
arxiv-1401-5698 | Identification of Pleonastic It Using the Web | http://arxiv.org/abs/1401.5698 | id:1401.5698 author:Yifan Li, Petr Musilek, Marek Reformat, Loren Wyard-Scott category:cs.CL  published:2014-01-15 summary:In a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. This phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. In this paper, a novel approach is proposed to identify such uses of it: the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. The system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. The identification results are comparable to those obtained by human efforts. version:1
arxiv-1401-3447 | Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based Approach | http://arxiv.org/abs/1401.3447 | id:1401.3447 author:Saher Esmeir, Shaul Markovitch category:cs.LG  published:2014-01-15 summary:Machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to resource management during learning and classification. In this work we introduce ACT (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. ACT is an anytime algorithm that allows learning time to be increased in return for lower classification costs. It builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. Using sampling techniques, ACT approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to escape local minima, into which greedy methods may be trapped. Experiments with a variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive tree learners. The results show that for the majority of domains ACT produces significantly less costly trees. ACT also exhibits good anytime behavior with diminishing returns. version:1
arxiv-1401-3446 | Amino Acid Interaction Network Prediction using Multi-objective Optimization | http://arxiv.org/abs/1401.3446 | id:1401.3446 author:Md. Shiplu Hawlader, Saifuddin Md. Tareeq category:cs.CE cs.NE  published:2014-01-15 summary:Protein can be represented by amino acid interaction network. This network is a graph whose vertices are the proteins amino acids and whose edges are the interactions between them. This interaction network is the first step of proteins three-dimensional structure prediction. In this paper we present a multi-objective evolutionary algorithm for interaction prediction and ant colony probabilistic optimization algorithm is used to confirm the interaction. version:1
arxiv-1401-3441 | Transductive Rademacher Complexity and its Applications | http://arxiv.org/abs/1401.3441 | id:1401.3441 author:Ran El-Yaniv, Dmitry Pechyony category:cs.LG cs.AI stat.ML  published:2014-01-15 summary:We develop a technique for deriving data-dependent error bounds for transductive learning algorithms based on transductive Rademacher complexity. Our technique is based on a novel general error bound for transduction in terms of transductive Rademacher complexity, together with a novel bounding technique for Rademacher averages for particular algorithms, in terms of their "unlabeled-labeled" representation. This technique is relevant to many advanced graph-based transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms. Finally, we present a new PAC-Bayesian bound for mixtures of transductive algorithms based on our Rademacher bounds. version:1
arxiv-1401-3434 | Adaptive Stochastic Resource Control: A Machine Learning Approach | http://arxiv.org/abs/1401.3434 | id:1401.3434 author:Balázs Csanád Csáji, László Monostori category:cs.LG  published:2014-01-15 summary:The paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. This approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. First, reactive solutions are considered and defined as control policies of suitably reformulated Markov decision processes (MDPs). We argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. Next, approximate dynamic programming (ADP) methods, such as fitted Q-learning, are suggested for computing an efficient control policy. In order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (SVR), particularly, nu-SVRs. Several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. Finally, experimental results on both benchmark and industry-related data are presented. version:1
arxiv-1401-3432 | A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for Range Finders in Dynamic Environments | http://arxiv.org/abs/1401.3432 | id:1401.3432 author:Tinne De Laet, Joris De Schutter, Herman Bruyninckx category:cs.AI cs.LG  published:2014-01-15 summary:This paper proposes and experimentally validates a Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach results in a transparent and intuitive model. With respect to the state of the art beam model this paper: (i) proposes a different functional form for the probability of range measurements caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in te state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. The proposed beam model is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood and a variational Bayesian estimator (both based on expectation-maximization) are proposed to learn the model parameters. Furthermore, the RBBM is extended to a full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. The full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. In contrast to Gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. This sample-based approximation enables handling dynamic environments and capturing multi-modality, which occurs even in simple static environments. version:1
arxiv-1401-3429 | Latent Tree Models and Approximate Inference in Bayesian Networks | http://arxiv.org/abs/1401.3429 | id:1401.3429 author:Yi Wang, Nevin L. Zhang, Tao Chen category:cs.LG  published:2014-01-15 summary:We propose a novel method for approximate inference in Bayesian networks (BNs). The idea is to sample data from a BN, learn a latent tree model (LTM) from the data offline, and when online, make inference with the LTM instead of the original BN. Because LTMs are tree-structured, inference takes linear time. In the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. Empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost. version:1
arxiv-1401-3427 | Analogical Dissimilarity: Definition, Algorithms and Two Experiments in Machine Learning | http://arxiv.org/abs/1401.3427 | id:1401.3427 author:Laurent Miclet, Sabri Bayoudh, Arnaud Delhay category:cs.LG cs.AI  published:2014-01-15 summary:This paper defines the notion of analogical dissimilarity between four objects, with a special focus on objects structured as sequences. Firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. Secondly, when one of these objects is unknown, it gives algorithms to compute it. Thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. In particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. It gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. Two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer. version:1
arxiv-1401-3413 | Infinite Mixed Membership Matrix Factorization | http://arxiv.org/abs/1401.3413 | id:1401.3413 author:Avneesh Saluja, Mahdi Pakdaman, Dongzhen Piao, Ankur P. Parikh category:cs.LG cs.IR  published:2014-01-15 summary:Rating and recommendation systems have become a popular application area for applying a suite of machine learning techniques. Current approaches rely primarily on probabilistic interpretations and extensions of matrix factorization, which factorizes a user-item ratings matrix into latent user and item vectors. Most of these methods fail to model significant variations in item ratings from otherwise similar users, a phenomenon known as the "Napoleon Dynamite" effect. Recent efforts have addressed this problem by adding a contextual bias term to the rating, which captures the mood under which a user rates an item or the context in which an item is rated by a user. In this work, we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data, and derive Gibbs sampling inference procedures for our model. We evaluate our approach on the MovieLens 1M dataset, and show significant improvements over the optimal parametric baseline, more than twice the improvements previously encountered for this task. We also extract and evaluate a DBLP dataset, wherein we predict the number of papers co-authored by two authors, and present improvements over the parametric baseline on this alternative domain as well. version:1
arxiv-1401-2668 | MRFalign: Protein Homology Detection through Alignment of Markov Random Fields | http://arxiv.org/abs/1401.2668 | id:1401.2668 author:Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.CE cs.LG  published:2014-01-12 summary:Sequence-based protein homology detection has been extensively studied and so far the most sensitive method is based upon comparison of protein sequence profiles, which are derived from multiple sequence alignment (MSA) of sequence homologs in a protein family. A sequence profile is usually represented as a position-specific scoring matrix (PSSM) or an HMM (Hidden Markov Model) and accordingly PSSM-PSSM or HMM-HMM comparison is used for homolog detection. This paper presents a new homology detection method MRFalign, consisting of three key components: 1) a Markov Random Fields (MRF) representation of a protein family; 2) a scoring function measuring similarity of two MRFs; and 3) an efficient ADMM (Alternating Direction Method of Multipliers) algorithm aligning two MRFs. Compared to HMM that can only model very short-range residue correlation, MRFs can model long-range residue interaction pattern and thus, encode information for the global 3D structure of a protein family. Consequently, MRF-MRF comparison for remote homology detection shall be much more sensitive than HMM-HMM or PSSM-PSSM comparison. Experiments confirm that MRFalign outperforms several popular HMM or PSSM-based methods in terms of both alignment accuracy and remote homology detection and that MRFalign works particularly well for mainly beta proteins. For example, tested on the benchmark SCOP40 (8353 proteins) for homology detection, PSSM-PSSM and HMM-HMM succeed on 48% and 52% of proteins, respectively, at superfamily level, and on 15% and 27% of proteins, respectively, at fold level. In contrast, MRFalign succeeds on 57.3% and 42.5% of proteins at superfamily and fold level, respectively. This study implies that long-range residue interaction patterns are very helpful for sequence-based homology detection. The software is available for download at http://raptorx.uchicago.edu/download/. version:2
arxiv-1401-3390 | Binary Classifier Calibration: Non-parametric approach | http://arxiv.org/abs/1401.3390 | id:1401.3390 author:Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht category:stat.ML cs.LG  published:2014-01-14 summary:Accurate calibration of probabilistic predictive models learned is critical for many practical prediction and decision-making tasks. There are two main categories of methods for building calibrated classifiers. One approach is to develop methods for learning probabilistic models that are well-calibrated, ab initio. The other approach is to use some post-processing methods for transforming the output of a classifier to be well calibrated, as for example histogram binning, Platt scaling, and isotonic regression. One advantage of the post-processing approach is that it can be applied to any existing probabilistic classification model that was constructed using any machine-learning method. In this paper, we first introduce two measures for evaluating how well a classifier is calibrated. We prove three theorems showing that using a simple histogram binning post-processing method, it is possible to make a classifier be well calibrated while retaining its discrimination capability. Also, by casting the histogram binning method as a density-based non-parametric binary classifier, we can extend it using two simple non-parametric density estimation methods. We demonstrate the performance of the proposed calibration methods on synthetic and real datasets. Experimental results show that the proposed methods either outperform or are comparable to existing calibration methods. version:1
arxiv-1401-3385 | A programme to determine the exact interior of any connected digital picture | http://arxiv.org/abs/1401.3385 | id:1401.3385 author:Antonio Elias Fabris, Valério Ramos Batista category:cs.CG cs.CV cs.GR  published:2014-01-14 summary:Region filling is one of the most important and fundamental operations in computer graphics and image processing. Many filling algorithms and their implementations are based on the Euclidean geometry, which are then translated into computational models moving carelessly from the continuous to the finite discrete space of the computer. The consequences of this approach is that most implementations fail when tested for challenging degenerate and nearly degenerate regions. We present a correct integer-only procedure that works for all connected digital pictures. It finds all possible interior points, which are then displayed and stored in a locating matrix. Namely, we present a filling and locating procedure that can be used in computer graphics and image processing applications. version:1
arxiv-1401-3372 | Learning Language from a Large (Unannotated) Corpus | http://arxiv.org/abs/1401.3372 | id:1401.3372 author:Linas Vepstas, Ben Goertzel category:cs.CL cs.LG  published:2014-01-14 summary:A novel approach to the fully automated, unsupervised extraction of dependency grammars and associated syntax-to-semantic-relationship mappings from large text corpora is described. The suggested approach builds on the authors' prior work with the Link Grammar, RelEx and OpenCog systems, as well as on a number of prior papers and approaches from the statistical language learning literature. If successful, this approach would enable the mining of all the information needed to power a natural language comprehension and generation system, directly from a large, unannotated corpus. version:1
arxiv-1401-3358 | Survey On The Estimation Of Mutual Information Methods as a Measure of Dependency Versus Correlation Analysis | http://arxiv.org/abs/1401.3358 | id:1401.3358 author:D. Gencaga, N. K. Malakar, D. J. Lary category:stat.ML 94A17  published:2014-01-14 summary:In this survey, we present and compare different approaches to estimate Mutual Information (MI) from data to analyse general dependencies between variables of interest in a system. We demonstrate the performance difference of MI versus correlation analysis, which is only optimal in case of linear dependencies. First, we use a piece-wise constant Bayesian methodology using a general Dirichlet prior. In this estimation method, we use a two-stage approach where we approximate the probability distribution first and then calculate the marginal and joint entropies. Here, we demonstrate the performance of this Bayesian approach versus the others for computing the dependency between different variables. We also compare these with linear correlation analysis. Finally, we apply MI and correlation analysis to the identification of the bias in the determination of the aerosol optical depth (AOD) by the satellite based Moderate Resolution Imaging Spectroradiometer (MODIS) and the ground based AErosol RObotic NETwork (AERONET). Here, we observe that the AOD measurements by these two instruments might be different for the same location. The reason of this bias is explored by quantifying the dependencies between the bias and 15 other variables including cloud cover, surface reflectivity and others. version:1
arxiv-1401-3258 | A Boosting Approach to Learning Graph Representations | http://arxiv.org/abs/1401.3258 | id:1401.3258 author:Rajmonda Caceres, Kevin Carter, Jeremy Kun category:cs.LG cs.SI stat.ML  published:2014-01-14 summary:Learning the right graph representation from noisy, multisource data has garnered significant interest in recent years. A central tenet of this problem is relational learning. Here the objective is to incorporate the partial information each data source gives us in a way that captures the true underlying relationships. To address this challenge, we present a general, boosting-inspired framework for combining weak evidence of entity associations into a robust similarity metric. We explore the extent to which different quality measurements yield graph representations that are suitable for community detection. We then present empirical results on both synthetic and real datasets demonstrating the utility of this framework. Our framework leads to suitable global graph representations from quality measurements local to each edge. Finally, we discuss future extensions and theoretical considerations of learning useful graph representations from weak feedback in general application settings. version:1
arxiv-1401-3198 | Online Markov decision processes with Kullback-Leibler control cost | http://arxiv.org/abs/1401.3198 | id:1401.3198 author:Peng Guan, Maxim Raginsky, Rebecca Willett category:math.OC cs.LG cs.SY  published:2014-01-14 summary:This paper considers an online (real-time) control problem that involves an agent performing a discrete-time random walk over a finite state space. The agent's action at each time step is to specify the probability distribution for the next state given the current state. Following the set-up of Todorov, the state-action cost at each time step is a sum of a state cost and a control cost given by the Kullback-Leibler (KL) divergence between the agent's next-state distribution and that determined by some fixed passive dynamics. The online aspect of the problem is due to the fact that the state cost functions are generated by a dynamic environment, and the agent learns the current state cost only after selecting an action. An explicit construction of a computationally efficient strategy with small regret (i.e., expected difference between its actual total cost and the smallest cost attainable using noncausal knowledge of the state costs) under mild regularity conditions is presented, along with a demonstration of the performance of the proposed strategy on a simulated target tracking problem. A number of new results on Markov decision processes with KL control cost are also obtained. version:1
arxiv-1401-3148 | Dynamic Topology Adaptation and Distributed Estimation for Smart Grids | http://arxiv.org/abs/1401.3148 | id:1401.3148 author:S. Xu, R. C. de Lamare, H. V. Poor category:cs.IT cs.LG math.IT  published:2014-01-14 summary:This paper presents new dynamic topology adaptation strategies for distributed estimation in smart grids systems. We propose a dynamic exhaustive search--based topology adaptation algorithm and a dynamic sparsity--inspired topology adaptation algorithm, which can exploit the topology of smart grids with poor--quality links and obtain performance gains. We incorporate an optimized combining rule, named Hastings rule into our proposed dynamic topology adaptation algorithms. Compared with the existing works in the literature on distributed estimation, the proposed algorithms have a better convergence rate and significantly improve the system performance. The performance of the proposed algorithms is compared with that of existing algorithms in the IEEE 14--bus system. version:1
arxiv-1310-1562 | Dependence Measure for non-additive model | http://arxiv.org/abs/1310.1562 | id:1310.1562 author:Hangjin Jiang, Yiming Ding category:stat.ML  published:2013-10-06 summary:We proposed a new statistical dependency measure called Copula Dependency Coefficient(CDC) for two sets of variables based on copula. It is robust to outliers, easy to implement, powerful and appropriate to high-dimensional variables. These properties are important in many applications. Experimental results show that CDC can detect the dependence between variables in both additive and non-additive models. version:3
arxiv-1401-2955 | Binary Classifier Calibration: Bayesian Non-Parametric Approach | http://arxiv.org/abs/1401.2955 | id:1401.2955 author:Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht category:stat.ML cs.LG  published:2014-01-13 summary:A set of probabilistic predictions is well calibrated if the events that are predicted to occur with probability p do in fact occur about p fraction of the time. Well calibrated predictions are particularly important when machine learning models are used in decision analysis. This paper presents two new non-parametric methods for calibrating outputs of binary classification models: a method based on the Bayes optimal selection and a method based on the Bayesian model averaging. The advantage of these methods is that they are independent of the algorithm used to learn a predictive model, and they can be applied in a post-processing step, after the model is learned. This makes them applicable to a wide variety of machine learning models and methods. These calibration methods, as well as other methods, are tested on a variety of datasets in terms of both discrimination and calibration performance. The results show the methods either outperform or are comparable in performance to the state-of-the-art calibration methods. version:1
arxiv-1401-2943 | ONTS: "Optima" News Translation System | http://arxiv.org/abs/1401.2943 | id:1401.2943 author:Marco Turchi, Martin Atkinson, Alastair Wilcox, Brett Crawley, Stefano Bucci, Ralf Steinberger, Erik Van der Goot category:cs.CL I.2.7; H.3.3; H.3.6  published:2014-01-13 summary:We propose a real-time machine translation system that allows users to select a news category and to translate the related live news articles from Arabic, Czech, Danish, Farsi, French, German, Italian, Polish, Portuguese, Spanish and Turkish into English. The Moses-based system was optimised for the news domain and differs from other available systems in four ways: (1) News items are automatically categorised on the source side, before translation; (2) Named entity translation is optimised by recognising and extracting them on the source side and by re-inserting their translation in the target language, making use of a separate entity repository; (3) News titles are translated with a separate translation system which is optimised for the specific style of news titles; (4) The system was optimised for speed in order to cope with the large volume of daily news articles. version:1
arxiv-1401-2937 | A survey of methods to ease the development of highly multilingual text mining applications | http://arxiv.org/abs/1401.2937 | id:1401.2937 author:Ralf Steinberger category:cs.CL  published:2014-01-13 summary:Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines - most of all: extreme simplicity - can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingual text mining applications. We will argue that - to achieve this - the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools. version:1
arxiv-1401-1753 | A Solution of Degree Constrained Spanning Tree Using Hybrid GA | http://arxiv.org/abs/1401.1753 | id:1401.1753 author:Sounak Sadhukhan, Samar Sen Sarma category:cs.NE cs.DS  published:2014-01-08 summary:In real life, it is always an urge to reach our goal in minimum effort i.e., it should have a minimum constrained path. The path may be shortest route in practical life, either physical or electronic medium. The scenario is to represents the ambiance as a graph and to find a spanning tree with custom design criteria. Here, we have chosen a minimum degree spanning tree, which can be generated in real time with minimum turnaround time. The problem is NP-complete in nature [1, 2]. The solution approach, in general, is approximate. We have used a heuristic approach, namely hybrid genetic algorithm (GA), with motivated criteria of encoded data structures of graph. We compare the experimental result with the existing approximate algorithm and the result is so encouraging that we are interested to use it in our future applications. version:2
arxiv-1310-4378 | Efficient Monte Carlo and greedy heuristic for the inference of stochastic block models | http://arxiv.org/abs/1310.4378 | id:1310.4378 author:Tiago P. Peixoto category:physics.data-an cond-mat.stat-mech cs.SI physics.comp-ph stat.ML  published:2013-10-16 summary:We present an efficient algorithm for the inference of stochastic block models in large networks. The algorithm can be used as an optimized Markov chain Monte Carlo (MCMC) method, with a fast mixing time and a much reduced susceptibility to getting trapped in metastable states, or as a greedy agglomerative heuristic, with an almost linear $O(N\ln^2N)$ complexity, where $N$ is the number of nodes in the network, independent on the number of blocks being inferred. We show that the heuristic is capable of delivering results which are indistinguishable from the more exact and numerically expensive MCMC method in many artificial and empirical networks, despite being much faster. The method is entirely unbiased towards any specific mixing pattern, and in particular it does not favor assortative community structures. version:3
arxiv-1401-2871 | Tensor Representation and Manifold Learning Methods for Remote Sensing Images | http://arxiv.org/abs/1401.2871 | id:1401.2871 author:Lefei Zhang category:cs.CV 68  published:2014-01-13 summary:One of the main purposes of earth observation is to extract interested information and knowledge from remote sensing (RS) images with high efficiency and accuracy. However, with the development of RS technologies, RS system provide images with higher spatial and temporal resolution and more spectral channels than before, and it is inefficient and almost impossible to manually interpret these images. Thus, it is of great interests to explore automatic and intelligent algorithms to quickly process such massive RS data with high accuracy. This thesis targets to develop some efficient information extraction algorithms for RS images, by relying on the advanced technologies in machine learning. More precisely, we adopt the manifold learning algorithms as the mainline and unify the regularization theory, tensor-based method, sparse learning and transfer learning into the same framework. The main contributions of this thesis are as follows. version:1
arxiv-1012-4116 | lp-Recovery of the Most Significant Subspace among Multiple Subspaces with Outliers | http://arxiv.org/abs/1012.4116 | id:1012.4116 author:Gilad Lerman, Teng Zhang category:stat.ML cs.CV math.FA  published:2010-12-18 summary:We assume data sampled from a mixture of d-dimensional linear subspaces with spherically symmetric distributions within each subspace and an additional outlier component with spherically symmetric distribution within the ambient space (for simplicity we may assume that all distributions are uniform on their corresponding unit spheres). We also assume mixture weights for the different components. We say that one of the underlying subspaces of the model is most significant if its mixture weight is higher than the sum of the mixture weights of all other subspaces. We study the recovery of the most significant subspace by minimizing the lp-averaged distances of data points from d-dimensional subspaces, where p>0. Unlike other lp minimization problems, this minimization is non-convex for all p>0 and thus requires different methods for its analysis. We show that if 0<p<=1, then for any fraction of outliers the most significant subspace can be recovered by lp minimization with overwhelming probability (which depends on the generating distribution and its parameters). We show that when adding small noise around the underlying subspaces the most significant subspace can be nearly recovered by lp minimization for any 0<p<=1 with an error proportional to the noise level. On the other hand, if p>1 and there is more than one underlying subspace, then with overwhelming probability the most significant subspace cannot be recovered or nearly recovered. This last result does not require spherically symmetric outliers. version:4
arxiv-1401-2838 | GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation | http://arxiv.org/abs/1401.2838 | id:1401.2838 author:Edward Meeds, Max Welling category:cs.LG q-bio.QM stat.ML  published:2014-01-13 summary:Scientists often express their understanding of the world through a computationally demanding simulation program. Analyzing the posterior distribution of the parameters given observations (the inverse problem) can be extremely challenging. The Approximate Bayesian Computation (ABC) framework is the standard statistical tool to handle these likelihood free problems, but they require a very large number of simulations. In this work we develop two new ABC sampling algorithms that significantly reduce the number of simulations necessary for posterior inference. Both algorithms use confidence estimates for the accept probability in the Metropolis Hastings step to adaptively choose the number of necessary simulations. Our GPS-ABC algorithm stores the information obtained from every simulation in a Gaussian process which acts as a surrogate function for the simulated statistics. Experiments on a challenging realistic biological problem illustrate the potential of these algorithms. version:1
arxiv-1401-2804 | Insights into analysis operator learning: From patch-based sparse models to higher-order MRFs | http://arxiv.org/abs/1401.2804 | id:1401.2804 author:Yunjin Chen, René Ranftl, Thomas Pock category:cs.CV  published:2014-01-13 summary:This paper addresses a new learning algorithm for the recently introduced co-sparse analysis model. First, we give new insights into the co-sparse analysis model by establishing connections to filter-based MRF models, such as the Field of Experts (FoE) model of Roth and Black. For training, we introduce a technique called bi-level optimization to learn the analysis operators. Compared to existing analysis operator learning approaches, our training procedure has the advantage that it is unconstrained with respect to the analysis operator. We investigate the effect of different aspects of the co-sparse analysis model and show that the sparsity promoting function (also called penalty function) is the most important factor in the model. In order to demonstrate the effectiveness of our training approach, we apply our trained models to various classical image restoration problems. Numerical experiments show that our trained models clearly outperform existing analysis operator learning approaches and are on par with state-of-the-art image denoising algorithms. Our approach develops a framework that is intuitive to understand and easy to implement. version:1
arxiv-1308-6804 | A Low-Dimensional Representation for Robust Partial Isometric Correspondences Computation | http://arxiv.org/abs/1308.6804 | id:1308.6804 author:Alan Brunton, Michael Wand, Stefanie Wuhrer, Hans-Peter Seidel, Tino Weinkauf category:cs.CV cs.GR  published:2013-08-30 summary:Intrinsic isometric shape matching has become the standard approach for pose invariant correspondence estimation among deformable shapes. Most existing approaches assume global consistency, i.e., the metric structure of the whole manifold must not change significantly. While global isometric matching is well understood, only a few heuristic solutions are known for partial matching. Partial matching is particularly important for robustness to topological noise (incomplete data and contacts), which is a common problem in real-world 3D scanner data. In this paper, we introduce a new approach to partial, intrinsic isometric matching. Our method is based on the observation that isometries are fully determined by purely local information: a map of a single point and its tangent space fixes an isometry for both global and the partial maps. From this idea, we develop a new representation for partial isometric maps based on equivalence classes of correspondences between pairs of points and their tangent spaces. From this, we derive a local propagation algorithm that find such mappings efficiently. In contrast to previous heuristics based on RANSAC or expectation maximization, our method is based on a simple and sound theoretical model and fully deterministic. We apply our approach to register partial point clouds and compare it to the state-of-the-art methods, where we obtain significant improvements over global methods for real-world data and stronger guarantees than previous heuristic partial matching algorithms. version:2
arxiv-1401-2771 | A variational Bayes framework for sparse adaptive estimation | http://arxiv.org/abs/1401.2771 | id:1401.2771 author:Konstantinos E. Themelis, Athanasios A. Rontogiannis, Konstantinos D. Koutroumbas category:stat.ML  published:2014-01-13 summary:Recently, a number of mostly $\ell_1$-norm regularized least squares type deterministic algorithms have been proposed to address the problem of \emph{sparse} adaptive signal estimation and system identification. From a Bayesian perspective, this task is equivalent to maximum a posteriori probability estimation under a sparsity promoting heavy-tailed prior for the parameters of interest. Following a different approach, this paper develops a unifying framework of sparse \emph{variational Bayes} algorithms that employ heavy-tailed priors in conjugate hierarchical form to facilitate posterior inference. The resulting fully automated variational schemes are first presented in a batch iterative form. Then it is shown that by properly exploiting the structure of the batch estimation task, new sparse adaptive variational Bayes algorithms can be derived, which have the ability to impose and track sparsity during real-time processing in a time-varying environment. The most important feature of the proposed algorithms is that they completely eliminate the need for computationally costly parameter fine-tuning, a necessary ingredient of sparse adaptive deterministic algorithms. Extensive simulation results are provided to demonstrate the effectiveness of the new sparse variational Bayes algorithms against state-of-the-art deterministic techniques for adaptive channel estimation. The results show that the proposed algorithms are numerically robust and exhibit in general superior estimation performance compared to their deterministic counterparts. version:1
arxiv-1401-2688 | PSMACA: An Automated Protein Structure Prediction Using MACA (Multiple Attractor Cellular Automata) | http://arxiv.org/abs/1401.2688 | id:1401.2688 author:Pokkuluri Kiran Sree, Inamupudi Ramesh Babu, SSSN Usha Devi N category:cs.CE cs.LG  published:2014-01-13 summary:Protein Structure Predication from sequences of amino acid has gained a remarkable attention in recent years. Even though there are some prediction techniques addressing this problem, the approximate accuracy in predicting the protein structure is closely 75%. An automated procedure was evolved with MACA (Multiple Attractor Cellular Automata) for predicting the structure of the protein. Most of the existing approaches are sequential which will classify the input into four major classes and these are designed for similar sequences. PSMACA is designed to identify ten classes from the sequences that share twilight zone similarity and identity with the training sequences. This method also predicts three states (helix, strand, and coil) for the structure. Our comprehensive design considers 10 feature selection methods and 4 classifiers to develop MACA (Multiple Attractor Cellular Automata) based classifiers that are build for each of the ten classes. We have tested the proposed classifier with twilight-zone and 1-high-similarity benchmark datasets with over three dozens of modern competing predictors shows that PSMACA provides the best overall accuracy that ranges between 77% and 88.7% depending on the dataset. version:1
arxiv-1401-2686 | A parameterless scale-space approach to find meaningful modes in histograms - Application to image and spectrum segmentation | http://arxiv.org/abs/1401.2686 | id:1401.2686 author:Jérôme Gilles, Kathryn Heal category:cs.CV  published:2014-01-13 summary:In this paper, we present an algorithm to automatically detect meaningful modes in a histogram. The proposed method is based on the behavior of local minima in a scale-space representation. We show that the detection of such meaningful modes is equivalent in a two classes clustering problem on the length of minima scale-space curves. The algorithm is easy to implement, fast, and does not require any parameters. We present several results on histogram and spectrum segmentation, grayscale image segmentation and color image reduction. version:1
arxiv-1401-2663 | Dictionary-Based Concept Mining: An Application for Turkish | http://arxiv.org/abs/1401.2663 | id:1401.2663 author:Cem Rıfkı Aydın, Ali Erkan, Tunga Güngör, Hidayet Takçı category:cs.CL I.2.7  published:2014-01-12 summary:In this study, a dictionary-based method is used to extract expressive concepts from documents. So far, there have been many studies concerning concept mining in English, but this area of study for Turkish, an agglutinative language, is still immature. We used dictionary instead of WordNet, a lexical database grouping words into synsets that is widely used for concept extraction. The dictionaries are rarely used in the domain of concept mining, but taking into account that dictionary entries have synonyms, hypernyms, hyponyms and other relationships in their meaning texts, the success rate has been high for determining concepts. This concept extraction method is implemented on documents, that are collected from different corpora. version:1
arxiv-1401-2651 | An Overview of Schema Theory | http://arxiv.org/abs/1401.2651 | id:1401.2651 author:David White category:cs.NE  published:2014-01-12 summary:The purpose of this paper is to give an introduction to the field of Schema Theory written by a mathematician and for mathematicians. In particular, we endeavor to to highlight areas of the field which might be of interest to a mathematician, to point out some related open problems, and to suggest some large-scale projects. Schema theory seeks to give a theoretical justification for the efficacy of the field of genetic algorithms, so readers who have studied genetic algorithms stand to gain the most from this paper. However, nothing beyond basic probability theory is assumed of the reader, and for this reason we write in a fairly informal style. Because the mathematics behind the theorems in schema theory is relatively elementary, we focus more on the motivation and philosophy. Many of these results have been proven elsewhere, so this paper is designed to serve a primarily expository role. We attempt to cast known results in a new light, which makes the suggested future directions natural. This involves devoting a substantial amount of time to the history of the field. We hope that this exposition will entice some mathematicians to do research in this area, that it will serve as a road map for researchers new to the field, and that it will help explain how schema theory developed. Furthermore, we hope that the results collected in this document will serve as a useful reference. Finally, as far as the author knows, the questions raised in the final section are new. version:1
arxiv-1210-3831 | Graphical Modelling in Genetics and Systems Biology | http://arxiv.org/abs/1210.3831 | id:1210.3831 author:Marco Scutari category:stat.ME q-bio.MN stat.ML  published:2012-10-14 summary:Graphical modelling has a long history in statistics as a tool for the analysis of multivariate data, starting from Wright's path analysis and Gibbs' applications to statistical physics at the beginning of the last century. In its modern form, it was pioneered by Lauritzen and Wermuth and Pearl in the 1980s, and has since found applications in fields as diverse as bioinformatics, customer satisfaction surveys and weather forecasts. Genetics and systems biology are unique among these fields in the dimension of the data sets they study, which often contain several hundreds of variables and only a few tens or hundreds of observations. This raises problems in both computational complexity and the statistical significance of the resulting networks, collectively known as the "curse of dimensionality". Furthermore, the data themselves are difficult to model correctly due to the limited understanding of the underlying mechanisms. In the following, we will illustrate how such challenges affect practical graphical modelling and some possible solutions. version:2
arxiv-1401-2641 | Towards a Generic Framework for the Development of Unicode Based Digital Sindhi Dictionaries | http://arxiv.org/abs/1401.2641 | id:1401.2641 author:Imdad Ali Ismaili, Zeeshan Bhatti, Azhar Ali Shah category:cs.CL  published:2014-01-12 summary:Dictionaries are essence of any language providing vital linguistic recourse for the language learners, researchers and scholars. This paper focuses on the methodology and techniques used in developing software architecture for a UBSESD (Unicode Based Sindhi to English and English to Sindhi Dictionary). The proposed system provides an accurate solution for construction and representation of Unicode based Sindhi characters in a dictionary implementing Hash Structure algorithm and a custom java Object as its internal data structure saved in a file. The System provides facilities for Insertion, Deletion and Editing of new records of Sindhi. Through this framework any type of Sindhi to English and English to Sindhi Dictionary (belonging to different domains of knowledge, e.g. engineering, medicine, computer, biology etc.) could be developed easily with accurate representation of Unicode Characters in font independent manner. version:1
arxiv-1401-2618 | Sentiment Analysis Using Collaborated Opinion Mining | http://arxiv.org/abs/1401.2618 | id:1401.2618 author:Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi category:cs.IR cs.CL  published:2014-01-12 summary:Opinion mining and Sentiment analysis have emerged as a field of study since the widespread of World Wide Web and internet. Opinion refers to extraction of those lines or phrase in the raw and huge data which express an opinion. Sentiment analysis on the other hand identifies the polarity of the opinion being extracted. In this paper we propose the sentiment analysis in collaboration with opinion extraction, summarization, and tracking the records of the students. The paper modifies the existing algorithm in order to obtain the collaborated opinion about the students. The resultant opinion is represented as very high, high, moderate, low and very low. The paper is based on a case study where teachers give their remarks about the students and by applying the proposed sentiment analysis algorithm the opinion is extracted and represented. version:1
arxiv-1401-2569 | Multi Terminal Probabilistic Compressed Sensing | http://arxiv.org/abs/1401.2569 | id:1401.2569 author:Saeid Haghighatshoar category:cs.IT math.IT stat.ML  published:2014-01-11 summary:In this paper, the `Approximate Message Passing' (AMP) algorithm, initially developed for compressed sensing of signals under i.i.d. Gaussian measurement matrices, has been extended to a multi-terminal setting (MAMP algorithm). It has been shown that similar to its single terminal counterpart, the behavior of MAMP algorithm is fully characterized by a `State Evolution' (SE) equation for large block-lengths. This equation has been used to obtain the rate-distortion curve of a multi-terminal memoryless source. It is observed that by spatially coupling the measurement matrices, the rate-distortion curve of MAMP algorithm undergoes a phase transition, where the measurement rate region corresponding to a low distortion (approximately zero distortion) regime is fully characterized by the joint and conditional Renyi information dimension (RID) of the multi-terminal source. This measurement rate region is very similar to the rate region of the Slepian-Wolf distributed source coding problem where the RID plays a role similar to the discrete entropy. Simulations have been done to investigate the empirical behavior of MAMP algorithm. It is observed that simulation results match very well with predictions of SE equation for reasonably large block-lengths. version:1
arxiv-1310-1757 | A Deep and Tractable Density Estimator | http://arxiv.org/abs/1310.1757 | id:1310.1757 author:Benigno Uria, Iain Murray, Hugo Larochelle category:stat.ML cs.LG  published:2013-10-07 summary:The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance. version:2
arxiv-1401-2517 | The semantic similarity ensemble | http://arxiv.org/abs/1401.2517 | id:1401.2517 author:Andrea Ballatore, Michela Bertolotto, David C. Wilson category:cs.CL  published:2014-01-11 summary:Computational measures of semantic similarity between geographic terms provide valuable support across geographic information retrieval, data mining, and information integration. To date, a wide variety of approaches to geo-semantic similarity have been devised. A judgment of similarity is not intrinsically right or wrong, but obtains a certain degree of cognitive plausibility, depending on how closely it mimics human behavior. Thus selecting the most appropriate measure for a specific task is a significant challenge. To address this issue, we make an analogy between computational similarity measures and soliciting domain expert opinions, which incorporate a subjective set of beliefs, perceptions, hypotheses, and epistemic biases. Following this analogy, we define the semantic similarity ensemble (SSE) as a composition of different similarity measures, acting as a panel of experts having to reach a decision on the semantic similarity of a set of geographic terms. The approach is evaluated in comparison to human judgments, and results indicate that an SSE performs better than the average of its parts. Although the best member tends to outperform the ensemble, all ensembles outperform the average performance of each ensemble's member. Hence, in contexts where the best measure is unknown, the ensemble provides a more cognitively plausible approach. version:1
arxiv-1401-2504 | Multi-Step-Ahead Time Series Prediction using Multiple-Output Support Vector Regression | http://arxiv.org/abs/1401.2504 | id:1401.2504 author:Yukun Bao, Tao Xiong, Zhongyi Hu category:cs.LG stat.ML  published:2014-01-11 summary:Accurate time series prediction over long future horizons is challenging and of great interest to both practitioners and academics. As a well-known intelligent algorithm, the standard formulation of Support Vector Regression (SVR) could be taken for multi-step-ahead time series prediction, only relying either on iterated strategy or direct strategy. This study proposes a novel multiple-step-ahead time series prediction approach which employs multiple-output support vector regression (M-SVR) with multiple-input multiple-output (MIMO) prediction strategy. In addition, the rank of three leading prediction strategies with SVR is comparatively examined, providing practical implications on the selection of the prediction strategy for multi-step-ahead forecasting while taking SVR as modeling technique. The proposed approach is validated with the simulated and real datasets. The quantitative and comprehensive assessments are performed on the basis of the prediction accuracy and computational cost. The results indicate that: 1) the M-SVR using MIMO strategy achieves the best accurate forecasts with accredited computational load, 2) the standard SVR using direct strategy achieves the second best accurate forecasts, but with the most expensive computational cost, and 3) the standard SVR using iterated strategy is the worst in terms of prediction accuracy, but with the least computational cost. version:1
arxiv-1401-2497 | Multiscale Shrinkage and Lévy Processes | http://arxiv.org/abs/1401.2497 | id:1401.2497 author:Xin Yuan, Vinayak Rao, Shaobo Han, Lawrence Carin category:stat.ML  published:2014-01-11 summary:A new shrinkage-based construction is developed for a compressible vector $\boldsymbol{x}\in\mathbb{R}^n$, for cases in which the components of $\xv$ are naturally associated with a tree structure. Important examples are when $\xv$ corresponds to the coefficients of a wavelet or block-DCT representation of data. The method we consider in detail, and for which numerical results are presented, is based on increments of a gamma process. However, we demonstrate that the general framework is appropriate for many other types of shrinkage priors, all within the L\'{e}vy process family, with the gamma process a special case. Bayesian inference is carried out by approximating the posterior with samples from an MCMC algorithm, as well as by constructing a heuristic variational approximation to the posterior. We also consider expectation-maximization (EM) for a MAP (point) solution. State-of-the-art results are manifested for compressive sensing and denoising applications, the latter with spiky (non-Gaussian) noise. version:1
arxiv-1401-2490 | An Online Expectation-Maximisation Algorithm for Nonnegative Matrix Factorisation Models | http://arxiv.org/abs/1401.2490 | id:1401.2490 author:Sinan Yildirim, A. Taylan Cemgil, Sumeetpal S. Singh category:cs.LG stat.CO stat.ML  published:2014-01-11 summary:In this paper we formulate the nonnegative matrix factorisation (NMF) problem as a maximum likelihood estimation problem for hidden Markov models and propose online expectation-maximisation (EM) algorithms to estimate the NMF and the other unknown static parameters. We also propose a sequential Monte Carlo approximation of our online EM algorithm. We show the performance of the proposed method with two numerical examples. version:1
arxiv-1203-0905 | Autocalibration with the Minimum Number of Cameras with Known Pixel Shape | http://arxiv.org/abs/1203.0905 | id:1203.0905 author:José I. Ronda, Antonio Valdés, Guillermo Gallego category:cs.CV  published:2012-03-05 summary:In 3D reconstruction, the recovery of the calibration parameters of the cameras is paramount since it provides metric information about the observed scene, e.g., measures of angles and ratios of distances. Autocalibration enables the estimation of the camera parameters without using a calibration device, but by enforcing simple constraints on the camera parameters. In the absence of information about the internal camera parameters such as the focal length and the principal point, the knowledge of the camera pixel shape is usually the only available constraint. Given a projective reconstruction of a rigid scene, we address the problem of the autocalibration of a minimal set of cameras with known pixel shape and otherwise arbitrarily varying intrinsic and extrinsic parameters. We propose an algorithm that only requires 5 cameras (the theoretical minimum), thus halving the number of cameras required by previous algorithms based on the same constraint. To this purpose, we introduce as our basic geometric tool the six-line conic variety (SLCV), consisting in the set of planes intersecting six given lines of 3D space in points of a conic. We show that the set of solutions of the Euclidean upgrading problem for three cameras with known pixel shape can be parameterized in a computationally efficient way. This parameterization is then used to solve autocalibration from five or more cameras, reducing the three-dimensional search space to a two-dimensional one. We provide experiments with real images showing the good performance of the technique. version:2
arxiv-1312-6171 | Learning Paired-associate Images with An Unsupervised Deep Learning Architecture | http://arxiv.org/abs/1312.6171 | id:1312.6171 author:Ti Wang, Daniel L. Silver category:cs.NE cs.CV cs.LG  published:2013-12-20 summary:This paper presents an unsupervised multi-modal learning system that learns associative representation from two input modalities, or channels, such that input on one channel will correctly generate the associated response at the other and vice versa. In this way, the system develops a kind of supervised classification model meant to simulate aspects of human associative memory. The system uses a deep learning architecture (DLA) composed of two input/output channels formed from stacked Restricted Boltzmann Machines (RBM) and an associative memory network that combines the two channels. The DLA is trained on pairs of MNIST handwritten digit images to develop hierarchical features and associative representations that are able to reconstruct one image given its paired-associate. Experiments show that the multi-modal learning system generates models that are as accurate as back-propagation networks but with the advantage of a bi-directional network and unsupervised learning from either paired or non-paired training examples. version:2
arxiv-1401-2468 | N2Sky - Neural Networks as Services in the Clouds | http://arxiv.org/abs/1401.2468 | id:1401.2468 author:Erich Schikuta, Erwin Mann category:cs.NE H.3.5; I.2  published:2014-01-10 summary:We present the N2Sky system, which provides a framework for the exchange of neural network specific knowledge, as neural network paradigms and objects, by a virtual organization environment. It follows the sky computing paradigm delivering ample resources by the usage of federated Clouds. N2Sky is a novel Cloud-based neural network simulation environment, which follows a pure service oriented approach. The system implements a transparent environment aiming to enable both novice and experienced users to do neural network research easily and comfortably. N2Sky is built using the RAVO reference architecture of virtual organizations which allows itself naturally integrating into the Cloud service stack (SaaS, PaaS, and IaaS) of service oriented architectures. version:1
arxiv-1401-2451 | Online Matrix Completion Through Nuclear Norm Regularisation | http://arxiv.org/abs/1401.2451 | id:1401.2451 author:Charanpal Dhanjal, Romaric Gaudel, Stéphan Clémençon category:stat.ML  published:2014-01-10 summary:It is the main goal of this paper to propose a novel method to perform matrix completion on-line. Motivated by a wide variety of applications, ranging from the design of recommender systems to sensor network localization through seismic data reconstruction, we consider the matrix completion problem when entries of the matrix of interest are observed gradually. Precisely, we place ourselves in the situation where the predictive rule should be refined incrementally, rather than recomputed from scratch each time the sample of observed entries increases. The extension of existing matrix completion methods to the sequential prediction context is indeed a major issue in the Big Data era, and yet little addressed in the literature. The algorithm promoted in this article builds upon the Soft Impute approach introduced in Mazumder et al. (2010). The major novelty essentially arises from the use of a randomised technique for both computing and updating the Singular Value Decomposition (SVD) involved in the algorithm. Though of disarming simplicity, the method proposed turns out to be very efficient, while requiring reduced computations. Several numerical experiments based on real datasets illustrating its performance are displayed, together with preliminary results giving it a theoretical basis. version:1
arxiv-1401-2416 | Satellite image classification and segmentation using non-additive entropy | http://arxiv.org/abs/1401.2416 | id:1401.2416 author:Lucas Assirati, Alexandre Souto Martinez, Odemir Martinez Bruno category:cs.CV  published:2014-01-10 summary:Here we compare the Boltzmann-Gibbs-Shannon (standard) with the Tsallis entropy on the pattern recognition and segmentation of coloured images obtained by satellites, via "Google Earth". By segmentation we mean split an image to locate regions of interest. Here, we discriminate and define an image partition classes according to a training basis. This training basis consists of three pattern classes: aquatic, urban and vegetation regions. Our numerical experiments demonstrate that the Tsallis entropy, used as a feature vector composed of distinct entropic indexes $q$ outperforms the standard entropy. There are several applications of our proposed methodology, once satellite images can be used to monitor migration form rural to urban regions, agricultural activities, oil spreading on the ocean etc. version:1
arxiv-1401-2411 | Clustering, Coding, and the Concept of Similarity | http://arxiv.org/abs/1401.2411 | id:1401.2411 author:L. Thorne McCarty category:cs.LG  published:2014-01-10 summary:This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\bf x})$, which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, $U({\bf x})$, and its gradient, $\nabla U({\bf x})$. We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data. version:1
arxiv-1401-2949 | Exploiting generalisation symmetries in accuracy-based learning classifier systems: An initial study | http://arxiv.org/abs/1401.2949 | id:1401.2949 author:Larry Bull category:cs.NE cs.LG  published:2014-01-10 summary:Modern learning classifier systems typically exploit a niched genetic algorithm to facilitate rule discovery. When used for reinforcement learning, such rules represent generalisations over the state-action-reward space. Whilst encouraging maximal generality, the niching can potentially hinder the formation of generalisations in the state space which are symmetrical, or very similar, over different actions. This paper introduces the use of rules which contain multiple actions, maintaining accuracy and reward metrics for each action. It is shown that problem symmetries can be exploited, improving performance, whilst not degrading performance when symmetries are reduced. version:1
arxiv-1401-2304 | Lasso and equivalent quadratic penalized models | http://arxiv.org/abs/1401.2304 | id:1401.2304 author:Stefan Hummelsheim category:stat.ML cs.LG 62J07 (primary)  published:2014-01-10 summary:The least absolute shrinkage and selection operator (lasso) and ridge regression produce usually different estimates although input, loss function and parameterization of the penalty are identical. In this paper we look for ridge and lasso models with identical solution set. It turns out, that the lasso model with shrink vector $\lambda$ and a quadratic penalized model with shrink matrix as outer product of $\lambda$ with itself are equivalent, in the sense that they have equal solutions. To achieve this, we have to restrict the estimates to be positive. This doesn't limit the area of application since we can easily decompose every estimate in a positive and negative part. The resulting problem can be solved with a non negative least square algorithm. Beside this quadratic penalized model, an augmented regression model with positive bounded estimates is developed which is also equivalent to the lasso model, but is probably faster to solve. version:1
arxiv-1401-2258 | Assessing Wikipedia-Based Cross-Language Retrieval Models | http://arxiv.org/abs/1401.2258 | id:1401.2258 author:Benjamin Roth category:cs.IR cs.CL  published:2014-01-10 summary:This work compares concept models for cross-language retrieval: First, we adapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents. Experiments with different weighting schemes show that a weighting method favoring documents of similar length in both language sides gives best results. Considering that both monolingual and multilingual Latent Dirichlet Allocation (LDA) behave alike when applied for such documents, we use a training corpus built on Wikipedia where all documents are length-normalized and obtain improvements over previously reported scores for LDA. Another focus of our work is on model combination. For this end we include Explicit Semantic Analysis (ESA) in the experiments. We observe that ESA is not competitive with LDA in a query based retrieval task on CLEF 2000 data. The combination of machine translation with concept models increased performance by 21.1% map in comparison to machine translation alone. Machine translation relies on parallel corpora, which may not be available for many language pairs. We further explore how much cross-lingual information can be carried over by a specific information source in Wikipedia, namely linked text. The best results are obtained using a language modeling approach, entirely without information from parallel corpora. The need for smoothing raises interesting questions on soundness and efficiency. Link models capture only a certain kind of information and suggest weighting schemes to emphasize particular words. For a combined model, another interesting question is therefore how to integrate different weighting schemes. Using a very simple combination scheme, we obtain results that compare favorably to previously reported results on the CLEF 2000 dataset. version:1
arxiv-1401-2224 | A Comparative Study of Reservoir Computing for Temporal Signal Processing | http://arxiv.org/abs/1401.2224 | id:1401.2224 author:Alireza Goudarzi, Peter Banda, Matthew R. Lakin, Christof Teuscher, Darko Stefanovic category:cs.NE cs.LG  published:2014-01-10 summary:Reservoir computing (RC) is a novel approach to time series prediction using recurrent neural networks. In RC, an input signal perturbs the intrinsic dynamics of a medium called a reservoir. A readout layer is then trained to reconstruct a target output from the reservoir's state. The multitude of RC architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task-solving performance and computational power of RC. In addition, in contrast to traditional computation models, the reservoir is a dynamical system in which computation and memory are inseparable, and therefore hard to analyze. Here, we compare echo state networks (ESN), a popular RC architecture, with tapped-delay lines (DL) and nonlinear autoregressive exogenous (NARX) networks, which we use to model systems with limited computation and limited memory respectively. We compare the performance of the three systems while computing three common benchmark time series: H{\'e}non Map, NARMA10, and NARMA20. We find that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs. The DL and the NARX network have higher memorization capability, but fall short of the generalization power of the ESN. version:1
arxiv-1401-2139 | Distinguishing noise from chaos: objective versus subjective criteria using Horizontal Visibility Graph | http://arxiv.org/abs/1401.2139 | id:1401.2139 author:Martín Gómez Ravetti, Laura C. Carpi, Bruna Amin Gonçalves, Alejandro C. Frery, Osvaldo A. Rosso category:stat.ML cs.IT math.IT nlin.CD  published:2014-01-09 summary:A recently proposed methodology called the Horizontal Visibility Graph (HVG) [Luque {\it et al.}, Phys. Rev. E., 80, 046103 (2009)] that constitutes a geometrical simplification of the well known Visibility Graph algorithm [Lacasa {\it et al.\/}, Proc. Natl. Sci. U.S.A. 105, 4972 (2008)], has been used to study the distinction between deterministic and stochastic components in time series [L. Lacasa and R. Toral, Phys. Rev. E., 82, 036120 (2010)]. Specifically, the authors propose that the node degree distribution of these processes follows an exponential functional of the form $P(\kappa)\sim \exp(-\lambda~\kappa)$, in which $\kappa$ is the node degree and $\lambda$ is a positive parameter able to distinguish between deterministic (chaotic) and stochastic (uncorrelated and correlated) dynamics. In this work, we investigate the characteristics of the node degree distributions constructed by using HVG, for time series corresponding to $28$ chaotic maps and $3$ different stochastic processes. We thoroughly study the methodology proposed by Lacasa and Toral finding several cases for which their hypothesis is not valid. We propose a methodology that uses the HVG together with Information Theory quantifiers. An extensive and careful analysis of the node degree distributions obtained by applying HVG allow us to conclude that the Fisher-Shannon information plane is a remarkable tool able to graphically represent the different nature, deterministic or stochastic, of the systems under study. version:1
arxiv-1401-2851 | Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery | http://arxiv.org/abs/1401.2851 | id:1401.2851 author:Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, Kazi Zakia Sultana category:cs.IR cs.CL  published:2014-01-09 summary:The correlation and interactions among different biological entities comprise the biological system. Although already revealed interactions contribute to the understanding of different existing systems, researchers face many questions everyday regarding inter-relationships among entities. Their queries have potential role in exploring new relations which may open up a new area of investigation. In this paper, we introduce a text mining based method for answering the biological queries in terms of statistical computation such that researchers can come up with new knowledge discovery. It facilitates user to submit their query in natural linguistic form which can be treated as hypothesis. Our proposed approach analyzes the hypothesis and measures the p-value of the hypothesis with respect to the existing literature. Based on the measured value, the system either accepts or rejects the hypothesis from statistical point of view. Moreover, even it does not find any direct relationship among the entities of the hypothesis, it presents a network to give an integral overview of all the entities through which the entities might be related. This is also congenial for the researchers to widen their view and thus think of new hypothesis for further investigation. It assists researcher to get a quantitative evaluation of their assumptions such that they can reach a logical conclusion and thus aids in relevant re-searches of biological knowledge discovery. The system also provides the researchers a graphical interactive interface to submit their hypothesis for assessment in a more convenient way. version:1
arxiv-1401-2058 | Gesture recognition based mouse events | http://arxiv.org/abs/1401.2058 | id:1401.2058 author:Rachit Puri category:cs.CV  published:2014-01-09 summary:This paper presents the maneuver of mouse pointer and performs various mouse operations such as left click, right click, double click, drag etc using gestures recognition technique. Recognizing gestures is a complex task which involves many aspects such as motion modeling, motion analysis, pattern recognition and machine learning. Keeping all the essential factors in mind a system has been created which recognizes the movement of fingers and various patterns formed by them. Color caps have been used for fingers to distinguish it from the background color such as skin color. Thus recognizing the gestures various mouse events have been performed. The application has been created on MATLAB environment with operating system as windows 7. version:1
arxiv-1401-2051 | Enhancement performance of road recognition system of autonomous robots in shadow scenario | http://arxiv.org/abs/1401.2051 | id:1401.2051 author:Olusanya Y. Agunbiade, Tranos Zuva, Awosejo O. Johnson, Keneilwe Zuva category:cs.RO cs.CV  published:2014-01-09 summary:Road region recognition is a main feature that is gaining increasing attention from intellectuals because it helps autonomous vehicle to achieve a successful navigation without accident. However, different techniques based on camera sensor have been used by various researchers and outstanding results have been achieved. Despite their success, environmental noise like shadow leads to inaccurate recognition of road region which eventually leads to accident for autonomous vehicle. In this research, we conducted an investigation on shadow and its effects, optimized the road region recognition system of autonomous vehicle by introducing an algorithm capable of detecting and eliminating the effects of shadow. The experimental performance of our system was tested and compared using the following schemes: Total Positive Rate (TPR), False Negative Rate (FNR), Total Negative Rate (TNR), Error Rate (ERR) and False Positive Rate (FPR). The performance result of the system improved on road recognition in shadow scenario and this advancement has added tremendously to successful navigation approaches for autonomous vehicle. version:1
arxiv-1401-1990 | Brazilian License Plate Detection Using Histogram of Oriented Gradients and Sliding Windows | http://arxiv.org/abs/1401.1990 | id:1401.1990 author:R. F. Prates, G. Cámara-Chávez, William R. Schwartz, D. Menotti category:cs.CV  published:2014-01-09 summary:Due to the increasingly need for automatic traffic monitoring, vehicle license plate detection is of high interest to perform automatic toll collection, traffic law enforcement, parking lot access control, among others. In this paper, a sliding window approach based on Histogram of Oriented Gradients (HOG) features is used for Brazilian license plate detection. This approach consists in scanning the whole image in a multiscale fashion such that the license plate is located precisely. The main contribution of this work consists in a deep study of the best setup for HOG descriptors on the detection of Brazilian license plates, in which HOG have never been applied before. We also demonstrate the reliability of this method ensured by a recall higher than 98% (with a precision higher than 78%) in a publicly available data set. version:1
arxiv-1306-0186 | RNADE: The real-valued neural autoregressive density-estimator | http://arxiv.org/abs/1306.0186 | id:1306.0186 author:Benigno Uria, Iain Murray, Hugo Larochelle category:stat.ML cs.LG  published:2013-06-02 summary:We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case. version:2
arxiv-1401-1946 | Hand-guided 3D surface acquisition by combining simple light sectioning with real-time algorithms | http://arxiv.org/abs/1401.1946 | id:1401.1946 author:Oliver Arold, Svenja Ettl, Florian Willomitzer, Gerd Häusler category:physics.optics cs.CV  published:2014-01-09 summary:Precise 3D measurements of rigid surfaces are desired in many fields of application like quality control or surgery. Often, views from all around the object have to be acquired for a full 3D description of the object surface. We present a sensor principle called "Flying Triangulation" which avoids an elaborate "stop-and-go" procedure. It combines a low-cost classical light-section sensor with an algorithmic pipeline. A hand-guided sensor captures a continuous movie of 3D views while being moved around the object. The views are automatically aligned and the acquired 3D model is displayed in real time. In contrast to most existing sensors no bandwidth is wasted for spatial or temporal encoding of the projected lines. Nor is an expensive color camera necessary for 3D acquisition. The achievable measurement uncertainty and lateral resolution of the generated 3D data is merely limited by physics. An alternating projection of vertical and horizontal lines guarantees the existence of corresponding points in successive 3D views. This enables a precise registration without surface interpolation. For registration, a variant of the iterative closest point algorithm - adapted to the specific nature of our 3D views - is introduced. Furthermore, data reduction and smoothing without losing lateral resolution as well as the acquisition and mapping of a color texture is presented. The precision and applicability of the sensor is demonstrated by simulation and measurement results. version:1
arxiv-1405-7349 | Radial basis function process neural network training based on generalized frechet distance and GA-SA hybrid strategy | http://arxiv.org/abs/1405.7349 | id:1405.7349 author:Bing Wang, Yao-hua Meng, Xiao-hong Yu category:cs.NE  published:2014-01-09 summary:For learning problem of Radial Basis Function Process Neural Network (RBF-PNN), an optimization training method based on GA combined with SA is proposed in this paper. Through building generalized Fr\'echet distance to measure similarity between time-varying function samples, the learning problem of radial basis centre functions and connection weights is converted into the training on corresponding discrete sequence coefficients. Network training objective function is constructed according to the least square error criterion, and global optimization solving of network parameters is implemented in feasible solution space by use of global optimization feature of GA and probabilistic jumping property of SA . The experiment results illustrate that the training algorithm improves the network training efficiency and stability. version:1
arxiv-1401-1926 | A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters Optimization | http://arxiv.org/abs/1401.1926 | id:1401.1926 author:Yukun Bao, Zhongyi Hu, Tao Xiong category:cs.LG cs.AI cs.NE stat.ML  published:2014-01-09 summary:Addressing the issue of SVMs parameters optimization, this study proposes an efficient memetic algorithm based on Particle Swarm Optimization algorithm (PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is responsible for exploration of the search space and the detection of the potential regions with optimum solutions, while pattern search (PS) is used to produce an effective exploitation on the potential regions obtained by PSO. Moreover, a novel probabilistic selection strategy is proposed to select the appropriate individuals among the current population to undergo local refinement, keeping a well balance between exploration and exploitation. Experimental results confirm that the local refinement with PS and our proposed selection strategy are effective, and finally demonstrate effectiveness and robustness of the proposed PSO-PS based MA for SVMs parameters optimization. version:1
arxiv-1401-1916 | Multiple-output support vector regression with a firefly algorithm for interval-valued stock price index forecasting | http://arxiv.org/abs/1401.1916 | id:1401.1916 author:Tao Xiong, Yukun Bao, Zhongyi Hu category:cs.CE cs.LG q-fin.ST  published:2014-01-09 summary:Highly accurate interval forecasting of a stock price index is fundamental to successfully making a profit when making investment decisions, by providing a range of values rather than a point estimate. In this study, we investigate the possibility of forecasting an interval-valued stock price index series over short and long horizons using multi-output support vector regression (MSVR). Furthermore, this study proposes a firefly algorithm (FA)-based approach, built on the established MSVR, for determining the parameters of MSVR (abbreviated as FA-MSVR). Three globally traded broad market indices are used to compare the performance of the proposed FA-MSVR method with selected counterparts. The quantitative and comprehensive assessments are performed on the basis of statistical criteria, economic criteria, and computational cost. In terms of statistical criteria, we compare the out-of-sample forecasting using goodness-of-forecast measures and testing approaches. In terms of economic criteria, we assess the relative forecast performance with a simple trading strategy. The results obtained in this study indicate that the proposed FA-MSVR method is a promising alternative for forecasting interval-valued financial time series. version:1
arxiv-1401-1905 | A Parameterized Complexity Analysis of Bi-level Optimisation with Evolutionary Algorithms | http://arxiv.org/abs/1401.1905 | id:1401.1905 author:Dogan Corus, Per Kristian Lehre, Frank Neumann, Mojgan Pourhassan category:cs.NE  published:2014-01-09 summary:Bi-level optimisation problems have gained increasing interest in the field of combinatorial optimisation in recent years. With this paper, we start the runtime analysis of evolutionary algorithms for bi-level optimisation problems. We examine two NP-hard problems, the generalised minimum spanning tree problem (GMST), and the generalised travelling salesman problem (GTSP) in the context of parameterised complexity. For the generalised minimum spanning tree problem, we analyse the two approaches presented by Hu and Raidl (2012) with respect to the number of clusters that distinguish each other by the chosen representation of possible solutions. Our results show that a (1+1) EA working with the spanning nodes representation is not a fixed-parameter evolutionary algorithm for the problem, whereas the global structure representation enables to solve the problem in fixed-parameter time. We present hard instances for each approach and show that the two approaches are highly complementary by proving that they solve each other's hard instances very efficiently. For the generalised travelling salesman problem, we analyse the problem with respect to the number of clusters in the problem instance. Our results show that a (1+1) EA working with the global structure representation is a fixed-parameter evolutionary algorithm for the problem. version:1
arxiv-1401-1895 | Efficient unimodality test in clustering by signature testing | http://arxiv.org/abs/1401.1895 | id:1401.1895 author:Mahdi Shahbaba, Soosan Beheshti category:cs.LG stat.ML  published:2014-01-09 summary:This paper provides a new unimodality test with application in hierarchical clustering methods. The proposed method denoted by signature test (Sigtest), transforms the data based on its statistics. The transformed data has much smaller variation compared to the original data and can be evaluated in a simple proposed unimodality test. Compared with the existing unimodality tests, Sigtest is more accurate in detecting the overlapped clusters and has a much less computational complexity. Simulation results demonstrate the efficiency of this statistic test for both real and synthetic data sets. version:1
arxiv-1401-1882 | Image reconstruction from few views by L0-norm optimization | http://arxiv.org/abs/1401.1882 | id:1401.1882 author:Yuli Sun, Jinxu Tao category:cs.IT cs.CV math.IT  published:2014-01-09 summary:The L1-norm of the gradient-magnitude images (GMI), which is the well-known total variation (TV) model, is widely used as regularization in the few views CT reconstruction. As the L1-norm TV regularization is tending to uniformly penalize the image gradient and the low-contrast structures are sometimes over smoothed, we proposed a new algorithm based on the L0-norm of the GMI to deal with the few views problem. To rise to the challenges introduced by the L0-norm DGT, the algorithm uses a pseudo-inverse transform of DGT and adapts an iterative hard thresholding (IHT) algorithm, whose convergence and effective efficiency have been theoretically proven. The simulation indicates that the algorithm proposed in this paper can obviously improve the reconstruction quality. version:1
arxiv-1401-1842 | Robust Large Scale Non-negative Matrix Factorization using Proximal Point Algorithm | http://arxiv.org/abs/1401.1842 | id:1401.1842 author:Jason Gejie Liu, Shuchin Aeron category:stat.ML cs.IT cs.LG cs.NA math.IT  published:2014-01-08 summary:A robust algorithm for non-negative matrix factorization (NMF) is presented in this paper with the purpose of dealing with large-scale data, where the separability assumption is satisfied. In particular, we modify the Linear Programming (LP) algorithm of [9] by introducing a reduced set of constraints for exact NMF. In contrast to the previous approaches, the proposed algorithm does not require the knowledge of factorization rank (extreme rays [3] or topics [7]). Furthermore, motivated by a similar problem arising in the context of metabolic network analysis [13], we consider an entirely different regime where the number of extreme rays or topics can be much larger than the dimension of the data vectors. The performance of the algorithm for different synthetic data sets are provided. version:1
arxiv-1401-1803 | Learning Multilingual Word Representations using a Bag-of-Words Autoencoder | http://arxiv.org/abs/1401.1803 | id:1401.1803 author:Stanislas Lauly, Alex Boulanger, Hugo Larochelle category:cs.CL cs.LG stat.ML  published:2014-01-08 summary:Recent work on learning multilingual word representations usually relies on the use of word-level alignements (e.g. infered with the help of GIZA++) between translated sentences, in order to align the word embeddings in different languages. In this workshop paper, we investigate an autoencoder model for learning multilingual word representations that does without such word-level alignements. The autoencoder is trained to reconstruct the bag-of-word representation of given sentence from an encoded representation extracted from its translation. We evaluate our approach on a multilingual document classification task, where labeled data is available only for one language (e.g. English) while classification must be performed in a different language (e.g. French). In our experiments, we observe that our method compares favorably with a previously proposed method that exploits word-level alignments to learn word representations. version:1
