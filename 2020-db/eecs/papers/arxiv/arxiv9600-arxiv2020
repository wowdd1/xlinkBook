arxiv-1504-04085 | FPA-CS: Focal Plane Array-based Compressive Imaging in Short-wave Infrared | http://arxiv.org/abs/1504.04085 | id:1504.04085 author:Huaijin Chen, M. Salman Asif, Aswin C. Sankaranarayanan, Ashok Veeraraghavan category:cs.CV  published:2015-04-16 summary:Cameras for imaging in short and mid-wave infrared spectra are significantly more expensive than their counterparts in visible imaging. As a result, high-resolution imaging in those spectrum remains beyond the reach of most consumers. Over the last decade, compressive sensing (CS) has emerged as a potential means to realize inexpensive short-wave infrared cameras. One approach for doing this is the single-pixel camera (SPC) where a single detector acquires coded measurements of a high-resolution image. A computational reconstruction algorithm is then used to recover the image from these coded measurements. Unfortunately, the measurement rate of a SPC is insufficient to enable imaging at high spatial and temporal resolutions. We present a focal plane array-based compressive sensing (FPA-CS) architecture that achieves high spatial and temporal resolutions. The idea is to use an array of SPCs that sense in parallel to increase the measurement rate, and consequently, the achievable spatio-temporal resolution of the camera. We develop a proof-of-concept prototype in the short-wave infrared using a sensor with 64$\times$ 64 pixels; the prototype provides a 4096$\times$ increase in the measurement rate compared to the SPC and achieves a megapixel resolution at video rate using CS techniques. version:1
arxiv-1412-4385 | Unsupervised Domain Adaptation with Feature Embeddings | http://arxiv.org/abs/1412.4385 | id:1412.4385 author:Yi Yang, Jacob Eisenstein category:cs.CL cs.LG  published:2014-12-14 summary:Representation learning is the dominant technique for unsupervised domain adaptation, but existing approaches often require the specification of "pivot features" that generalize across domains, which are selected by task-specific heuristics. We show that a novel but simple feature embedding approach provides better performance, by exploiting the feature template structure common in NLP problems. version:3
arxiv-1504-02518 | Unsupervised Feature Learning from Temporal Data | http://arxiv.org/abs/1504.02518 | id:1504.02518 author:Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun category:cs.CV cs.LG  published:2015-04-09 summary:Current state-of-the-art classification and detection algorithms rely on supervised training. In this work we study unsupervised feature learning in the context of temporally coherent video data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity. We establish a connection between slow feature learning to metric learning and show that the trained encoder can be used to define a more temporally and semantically coherent metric. version:2
arxiv-1504-04054 | A Generative Model for Deep Convolutional Learning | http://arxiv.org/abs/1504.04054 | id:1504.04054 author:Yunchen Pu, Xin Yuan, Lawrence Carin category:stat.ML cs.LG cs.NE  published:2015-04-15 summary:A generative model is developed for deep (multi-layered) convolutional dictionary learning. A novel probabilistic pooling operation is integrated into the deep model, yielding efficient bottom-up (pretraining) and top-down (refinement) probabilistic learning. Experimental results demonstrate powerful capabilities of the model to learn multi-layer features from images, and excellent classification results are obtained on the MNIST and Caltech 101 datasets. version:1
arxiv-1412-7063 | Diverse Embedding Neural Network Language Models | http://arxiv.org/abs/1412.7063 | id:1412.7063 author:Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran category:cs.CL cs.LG cs.NE  published:2014-12-22 summary:We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higher-dimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM. version:5
arxiv-1504-04003 | Anatomy-specific classification of medical images using deep convolutional nets | http://arxiv.org/abs/1504.04003 | id:1504.04003 author:Holger R. Roth, Christopher T. Lee, Hoo-Chang Shin, Ari Seff, Lauren Kim, Jianhua Yao, Le Lu, Ronald M. Summers category:cs.CV  published:2015-04-15 summary:Automated classification of human anatomy is an important prerequisite for many computer-aided diagnosis systems. The spatial complexity and variability of anatomy throughout the human body makes classification difficult. "Deep learning" methods such as convolutional networks (ConvNets) outperform other state-of-the-art methods in image classification tasks. In this work, we present a method for organ- or body-part-specific anatomical classification of medical images acquired using computed tomography (CT) with ConvNets. We train a ConvNet, using 4,298 separate axial 2D key-images to learn 5 anatomical classes. Key-images were mined from a hospital PACS archive, using a set of 1,675 patients. We show that a data augmentation approach can help to enrich the data set and improve classification performance. Using ConvNets and data augmentation, we achieve anatomy-specific classification error of 5.9 % and area-under-the-curve (AUC) values of an average of 0.998 in testing. We demonstrate that deep learning can be used to train very reliable and accurate classifiers that could initialize further computer-aided diagnosis. version:1
arxiv-1412-6596 | Training Deep Neural Networks on Noisy Labels with Bootstrapping | http://arxiv.org/abs/1412.6596 | id:1412.6596 author:Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, Andrew Rabinovich category:cs.CV cs.LG cs.NE  published:2014-12-20 summary:Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction objective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on several datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the- art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection. version:3
arxiv-1412-6856 | Object Detectors Emerge in Deep Scene CNNs | http://arxiv.org/abs/1412.6856 | id:1412.6856 author:Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba category:cs.CV cs.NE  published:2014-12-22 summary:With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects. version:2
arxiv-1504-03967 | Deep convolutional networks for pancreas segmentation in CT imaging | http://arxiv.org/abs/1504.03967 | id:1504.03967 author:Holger R. Roth, Amal Farag, Le Lu, Evrim B. Turkbey, Ronald M. Summers category:cs.CV  published:2015-04-15 summary:Automatic organ segmentation is an important prerequisite for many computer-aided diagnosis systems. The high anatomical variability of organs in the abdomen, such as the pancreas, prevents many segmentation methods from achieving high accuracies when compared to other segmentation of organs like the liver, heart or kidneys. Recently, the availability of large annotated training sets and the accessibility of affordable parallel computing resources via GPUs have made it feasible for "deep learning" methods such as convolutional networks (ConvNets) to succeed in image classification tasks. These methods have the advantage that used classification features are trained directly from the imaging data. We present a fully-automated bottom-up method for pancreas segmentation in computed tomography (CT) images of the abdomen. The method is based on hierarchical coarse-to-fine classification of local image regions (superpixels). Superpixels are extracted from the abdominal region using Simple Linear Iterative Clustering (SLIC). An initial probability response map is generated, using patch-level confidences and a two-level cascade of random forest classifiers, from which superpixel regions with probabilities larger 0.5 are retained. These retained superpixels serve as a highly sensitive initial input of the pancreas and its surroundings to a ConvNet that samples a bounding box around each superpixel at different scales (and random non-rigid deformations at training time) in order to assign a more distinct probability of each superpixel region being pancreas or not. We evaluate our method on CT images of 82 patients (60 for training, 2 for validation, and 20 for testing). Using ConvNets we achieve average Dice scores of 68%+-10% (range, 43-80%) in testing. This shows promise for accurate pancreas segmentation, using a deep learning approach and compares favorably to state-of-the-art methods. version:1
arxiv-1406-5670 | 3D ShapeNets: A Deep Representation for Volumetric Shapes | http://arxiv.org/abs/1406.5670 | id:1406.5670 author:Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao category:cs.CV  published:2014-06-22 summary:3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet -- a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks. version:3
arxiv-1502-05491 | Optimizing Text Quantifiers for Multivariate Loss Functions | http://arxiv.org/abs/1502.05491 | id:1502.05491 author:Andrea Esuli, Fabrizio Sebastiani category:cs.LG cs.IR  published:2015-02-19 summary:We address the problem of \emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods. version:2
arxiv-1504-05137 | Application of Enhanced-2D-CWT in Topographic Images for Mapping Landslide Risk Areas | http://arxiv.org/abs/1504.05137 | id:1504.05137 author:V. V. Vermehren Valenzuela, R. D. Lins, H. M. de Oliveira category:cs.CV physics.geo-ph  published:2015-04-15 summary:There has been lately a number of catastrophic events of landslides and mudslides in the mountainous region of Rio de Janeiro, Brazil. Those were caused by intense rain in localities where there was unplanned occupation of slopes of hills and mountains. Thus, it became imperative creating an inventory of landslide risk areas in densely populated cities. This work presents a way of demarcating risk areas by using the bidimensional Continuous Wavelet Transform (2D-CWT) applied to high resolution topographic images of the mountainous region of Rio de Janeiro. version:1
arxiv-1412-6568 | Improving zero-shot learning by mitigating the hubness problem | http://arxiv.org/abs/1412.6568 | id:1412.6568 author:Georgiana Dinu, Angeliki Lazaridou, Marco Baroni category:cs.CL cs.LG  published:2014-12-20 summary:The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains. version:3
arxiv-1504-03892 | Linear Maximum Margin Classifier for Learning from Uncertain Data | http://arxiv.org/abs/1504.03892 | id:1504.03892 author:Christos Tzelepis, Vasileios Mezaris, Ioannis Patras category:cs.LG  published:2015-04-15 summary:In this paper, we propose a maximum margin classifier that deals with uncertainty in data input. Specifically, we reformulate the SVM framework such that each input training entity is not solely a feature vector representation, but a multi-dimensional Gaussian distribution with given probability density, i.e., with a given mean and covariance matrix. The latter expresses the uncertainty. We arrive at a convex optimization problem, which is solved in the primal form using a gradient descent approach. The resulting classifier, which we name SVM with Gaussian Sample Uncertainty (SVM-GSU), is tested on synthetic data, as well as on the problem of event detection in video using the large-scale TRECVID MED 2014 dataset, and the problem of image classification using the MNIST dataset of handwritten digits. Experimental results verify the effectiveness of the proposed classifier. version:1
arxiv-1504-03874 | Bridging belief function theory to modern machine learning | http://arxiv.org/abs/1504.03874 | id:1504.03874 author:Thomas Burger category:cs.AI cs.LG  published:2015-04-15 summary:Machine learning is a quickly evolving field which now looks really different from what it was 15 years ago, when classification and clustering were major issues. This document proposes several trends to explore the new questions of modern machine learning, with the strong afterthought that the belief function framework has a major role to play. version:1
arxiv-1504-03811 | Tracking Live Fish from Low-Contrast and Low-Frame-Rate Stereo Videos | http://arxiv.org/abs/1504.03811 | id:1504.03811 author:Meng-Che Chuang, Jenq-Neng Hwang, Kresimir Williams, Richard Towler category:cs.CV  published:2015-04-15 summary:Non-extractive fish abundance estimation with the aid of visual analysis has drawn increasing attention. Unstable illumination, ubiquitous noise and low frame rate video capturing in the underwater environment, however, make conventional tracking methods unreliable. In this paper, we present a multiple fish tracking system for low-contrast and low-frame-rate stereo videos with the use of a trawl-based underwater camera system. An automatic fish segmentation algorithm overcomes the low-contrast issues by adopting a histogram backprojection approach on double local-thresholded images to ensure an accurate segmentation on the fish shape boundaries. Built upon a reliable feature-based object matching method, a multiple-target tracking algorithm via a modified Viterbi data association is proposed to overcome the poor motion continuity and frequent entrance/exit of fish targets under low-frame-rate scenarios. In addition, a computationally efficient block-matching approach performs successful stereo matching, which enables an automatic fish-body tail compensation to greatly reduce segmentation error and allows for an accurate fish length measurement. Experimental results show that an effective and reliable tracking performance for multiple live fish with underwater stereo cameras is achieved. version:1
arxiv-1504-03810 | Text Localization in Video Using Multiscale Weber's Local Descriptor | http://arxiv.org/abs/1504.03810 | id:1504.03810 author:B. H. Shekar, Smitha M. L. category:cs.CV  published:2015-04-15 summary:In this paper, we propose a novel approach for detecting the text present in videos and scene images based on the Multiscale Weber's Local Descriptor (MWLD). Given an input video, the shots are identified and the key frames are extracted based on their spatio-temporal relationship. From each key frame, we detect the local region information using WLD with different radius and neighborhood relationship of pixel values and hence obtained intensity enhanced key frames at multiple scales. These multiscale WLD key frames are merged together and then the horizontal gradients are computed using morphological operations. The obtained results are then binarized and the false positives are eliminated based on geometrical properties. Finally, we employ connected component analysis and morphological dilation operation to determine the text regions that aids in text localization. The experimental results obtained on publicly available standard Hua, Horizontal-1 and Horizontal-2 video dataset illustrate that the proposed method can accurately detect and localize texts of various sizes, fonts and colors in videos. version:1
arxiv-1412-7144 | Fully Convolutional Multi-Class Multiple Instance Learning | http://arxiv.org/abs/1412.7144 | id:1412.7144 author:Deepak Pathak, Evan Shelhamer, Jonathan Long, Trevor Darrell category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:Multiple instance learning (MIL) can reduce the need for costly annotation in tasks such as semantic segmentation by weakening the required degree of supervision. We propose a novel MIL formulation of multi-class semantic segmentation learning by a fully convolutional network. In this setting, we seek to learn a semantic segmentation model from just weak image-level labels. The model is trained end-to-end to jointly optimize the representation while disambiguating the pixel-image label assignment. Fully convolutional training accepts inputs of any size, does not need object proposal pre-processing, and offers a pixelwise loss map for selecting latent instances. Our multi-class MIL loss exploits the further supervision given by images with multiple labels. We evaluate this approach through preliminary experiments on the PASCAL VOC segmentation challenge. version:4
arxiv-1206-0068 | Posterior contraction of the population polytope in finite admixture models | http://arxiv.org/abs/1206.0068 | id:1206.0068 author:XuanLong Nguyen category:math.ST cs.LG stat.TH  published:2012-06-01 summary:We study the posterior contraction behavior of the latent population structure that arises in admixture models as the amount of data increases. We adopt the geometric view of admixture models - alternatively known as topic models - as a data generating mechanism for points randomly sampled from the interior of a (convex) population polytope, whose extreme points correspond to the population structure variables of interest. Rates of posterior contraction are established with respect to Hausdorff metric and a minimum matching Euclidean metric defined on polytopes. Tools developed include posterior asymptotics of hierarchical models and arguments from convex geometry. version:3
arxiv-1408-4045 | Relax, no need to round: integrality of clustering formulations | http://arxiv.org/abs/1408.4045 | id:1408.4045 author:Pranjal Awasthi, Afonso S. Bandeira, Moses Charikar, Ravishankar Krishnaswamy, Soledad Villar, Rachel Ward category:stat.ML cs.DS cs.LG math.ST stat.TH  published:2014-08-18 summary:We study exact recovery conditions for convex relaxations of point cloud clustering problems, focusing on two of the most common optimization problems for unsupervised clustering: $k$-means and $k$-median clustering. Motivations for focusing on convex relaxations are: (a) they come with a certificate of optimality, and (b) they are generic tools which are relatively parameter-free, not tailored to specific assumptions over the input. More precisely, we consider the distributional setting where there are $k$ clusters in $\mathbb{R}^m$ and data from each cluster consists of $n$ points sampled from a symmetric distribution within a ball of unit radius. We ask: what is the minimal separation distance between cluster centers needed for convex relaxations to exactly recover these $k$ clusters as the optimal integral solution? For the $k$-median linear programming relaxation we show a tight bound: exact recovery is obtained given arbitrarily small pairwise separation $\epsilon > 0$ between the balls. In other words, the pairwise center separation is $\Delta > 2+\epsilon$. Under the same distributional model, the $k$-means LP relaxation fails to recover such clusters at separation as large as $\Delta = 4$. Yet, if we enforce PSD constraints on the $k$-means LP, we get exact cluster recovery at center separation $\Delta > 2\sqrt2(1+\sqrt{1/m})$. In contrast, common heuristics such as Lloyd's algorithm (a.k.a. the $k$-means algorithm) can fail to recover clusters in this setting; even with arbitrarily large cluster separation, k-means++ with overseeding by any constant factor fails with high probability at exact cluster recovery. To complement the theoretical analysis, we provide an experimental study of the recovery guarantees for these various methods, and discuss several open problems which these experiments suggest. version:5
arxiv-1503-00036 | Norm-Based Capacity Control in Neural Networks | http://arxiv.org/abs/1503.00036 | id:1503.00036 author:Behnam Neyshabur, Ryota Tomioka, Nathan Srebro category:cs.LG cs.AI cs.NE stat.ML  published:2015-02-27 summary:We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks. version:2
arxiv-1504-03707 | Background Subtraction via Generalized Fused Lasso Foreground Modeling | http://arxiv.org/abs/1504.03707 | id:1504.03707 author:Bo Xin, Yuan Tian, Yizhou Wang, Wen Gao category:cs.CV  published:2015-04-14 summary:Background Subtraction (BS) is one of the key steps in video analysis. Many background models have been proposed and achieved promising performance on public data sets. However, due to challenges such as illumination change, dynamic background etc. the resulted foreground segmentation often consists of holes as well as background noise. In this regard, we consider generalized fused lasso regularization to quest for intact structured foregrounds. Together with certain assumptions about the background, such as the low-rank assumption or the sparse-composition assumption (depending on whether pure background frames are provided), we formulate BS as a matrix decomposition problem using regularization terms for both the foreground and background matrices. Moreover, under the proposed formulation, the two generally distinctive background assumptions can be solved in a unified manner. The optimization was carried out via applying the augmented Lagrange multiplier (ALM) method in such a way that a fast parametric-flow algorithm is used for updating the foreground matrix. Experimental results on several popular BS data sets demonstrate the advantage of the proposed model compared to state-of-the-arts. version:1
arxiv-1504-03701 | Probabilistic Clustering of Time-Evolving Distance Data | http://arxiv.org/abs/1504.03701 | id:1504.03701 author:Julia E. Vogt, Marius Kloft, Stefan Stark, Sudhir S. Raman, Sandhya Prabhakaran, Volker Roth, Gunnar Rätsch category:cs.LG stat.ML  published:2015-04-14 summary:We present a novel probabilistic clustering model for objects that are represented via pairwise distances and observed at different time points. The proposed method utilizes the information given by adjacent time points to find the underlying cluster structure and obtain a smooth cluster evolution. This approach allows the number of objects and clusters to differ at every time point, and no identification on the identities of the objects is needed. Further, the model does not require the number of clusters being specified in advance -- they are instead determined automatically using a Dirichlet process prior. We validate our model on synthetic data showing that the proposed method is more accurate than state-of-the-art clustering methods. Finally, we use our dynamic clustering model to analyze and illustrate the evolution of brain cancer patients over time. version:1
arxiv-1504-03659 | Temporal ordering of clinical events | http://arxiv.org/abs/1504.03659 | id:1504.03659 author:Azad Dehghan category:cs.CL cs.AI  published:2015-04-14 summary:This report describes a minimalistic set of methods engineered to anchor clinical events onto a temporal space. Specifically, we describe methods to extract clinical events (e.g., Problems, Treatments and Tests), temporal expressions (i.e., time, date, duration, and frequency), and temporal links (e.g., Before, After, Overlap) between events and temporal entities. These methods are developed and validated using high quality datasets. version:1
arxiv-1411-4952 | From Captions to Visual Concepts and Back | http://arxiv.org/abs/1411.4952 | id:1411.4952 author:Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, Geoffrey Zweig category:cs.CV cs.CL  published:2014-11-18 summary:This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time. version:3
arxiv-1504-03641 | Learning to Compare Image Patches via Convolutional Neural Networks | http://arxiv.org/abs/1504.03641 | id:1504.03641 author:Sergey Zagoruyko, Nikos Komodakis category:cs.CV cs.LG cs.NE  published:2015-04-14 summary:In this paper we show how to learn directly from image data (i.e., without resorting to manually-designed features) a general similarity function for comparing image patches, which is a task of fundamental importance for many computer vision problems. To encode such a function, we opt for a CNN-based model that is trained to account for a wide variety of changes in image appearance. To that end, we explore and study multiple neural network architectures, which are specifically adapted to this task. We show that such an approach can significantly outperform the state-of-the-art on several problems and benchmark datasets. version:1
arxiv-1405-2476 | A Canonical Semi-Deterministic Transducer | http://arxiv.org/abs/1405.2476 | id:1405.2476 author:Achilles Beros, Colin de la Higuera category:cs.LG  published:2014-05-10 summary:We prove the existence of a canonical form for semi-deterministic transducers with incomparable sets of output strings. Based on this, we develop an algorithm which learns semi-deterministic transducers given access to translation queries. We also prove that there is no learning algorithm for semi-deterministic transducers that uses only domain knowledge. version:3
arxiv-1504-03608 | A data-based classification of Slavic languages: Indices of qualitative variation applied to grapheme frequencies | http://arxiv.org/abs/1504.03608 | id:1504.03608 author:Michaela Koscová, Ján Macutek, Emmerich Kelih category:stat.AP cs.CL  published:2015-04-14 summary:The Ord's graph is a simple graphical method for displaying frequency distributions of data or theoretical distributions in the two-dimensional plane. Its coordinates are proportions of the first three moments, either empirical or theoretical ones. A modification of the Ord's graph based on proportions of indices of qualitative variation is presented. Such a modification makes the graph applicable also to data of categorical character. In addition, the indices are normalized with values between 0 and 1, which enables comparing data files divided into different numbers of categories. Both the original and the new graph are used to display grapheme frequencies in eleven Slavic languages. As the original Ord's graph requires an assignment of numbers to the categories, graphemes were ordered decreasingly according to their frequencies. Data were taken from parallel corpora, i.e., we work with grapheme frequencies from a Russian novel and its translations to ten other Slavic languages. Then, cluster analysis is applied to the graph coordinates. While the original graph yields results which are not linguistically interpretable, the modification reveals meaningful relations among the languages. version:1
arxiv-1410-8546 | A Solution for Multi-Alignment by Transformation Synchronisation | http://arxiv.org/abs/1410.8546 | id:1410.8546 author:Florian Bernard, Johan Thunberg, Peter Gemmar, Frank Hertel, Andreas Husch, Jorge Goncalves category:cs.CV stat.ML  published:2014-10-30 summary:The alignment of a set of objects by means of transformations plays an important role in computer vision. Whilst the case for only two objects can be solved globally, when multiple objects are considered usually iterative methods are used. In practice the iterative methods perform well if the relative transformations between any pair of objects are free of noise. However, if only noisy relative transformations are available (e.g. due to missing data or wrong correspondences) the iterative methods may fail. Based on the observation that the underlying noise-free transformations can be retrieved from the null space of a matrix that can directly be obtained from pairwise alignments, this paper presents a novel method for the synchronisation of pairwise transformations such that they are transitively consistent. Simulations demonstrate that for noisy transformations, a large proportion of missing data and even for wrong correspondence assignments the method delivers encouraging results. version:2
arxiv-1411-4894 | Low-level Vision by Consensus in a Spatial Hierarchy of Regions | http://arxiv.org/abs/1411.4894 | id:1411.4894 author:Ayan Chakrabarti, Ying Xiong, Steven J. Gortler, Todd Zickler category:cs.CV  published:2014-11-18 summary:We introduce a multi-scale framework for low-level vision, where the goal is estimating physical scene values from image data---such as depth from stereo image pairs. The framework uses a dense, overlapping set of image regions at multiple scales and a "local model," such as a slanted-plane model for stereo disparity, that is expected to be valid piecewise across the visual field. Estimation is cast as optimization over a dichotomous mixture of variables, simultaneously determining which regions are inliers with respect to the local model (binary variables) and the correct co-ordinates in the local model space for each inlying region (continuous variables). When the regions are organized into a multi-scale hierarchy, optimization can occur in an efficient and parallel architecture, where distributed computational units iteratively perform calculations and share information through sparse connections between parents and children. The framework performs well on a standard benchmark for binocular stereo, and it produces a distributional scene representation that is appropriate for combining with higher-level reasoning and other low-level cues. version:2
arxiv-1504-03573 | Building Proteins in a Day: Efficient 3D Molecular Reconstruction | http://arxiv.org/abs/1504.03573 | id:1504.03573 author:Marcus A. Brubaker, Ali Punjani, David J. Fleet category:cs.CV q-bio.QM  published:2015-04-14 summary:Discovering the 3D atomic structure of molecules such as proteins and viruses is a fundamental research problem in biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising vision-based technique for structure estimation which attempts to reconstruct 3D structures from 2D images. This paper addresses the challenging problem of 3D reconstruction from 2D Cryo-EM images. A new framework for estimation is introduced which relies on modern stochastic optimization techniques to scale to large datasets. We also introduce a novel technique which reduces the cost of evaluating the objective function during optimization by over five orders or magnitude. The net result is an approach capable of estimating 3D molecular structure from large scale datasets in about a day on a single workstation. version:1
arxiv-1504-03522 | Efficient Scene Text Localization and Recognition with Local Character Refinement | http://arxiv.org/abs/1504.03522 | id:1504.03522 author:Lukáš Neumann, Jiří Matas category:cs.CV  published:2015-04-14 summary:An unconstrained end-to-end text localization and recognition method is presented. The method detects initial text hypothesis in a single pass by an efficient region-based method and subsequently refines the text hypothesis using a more robust local text model, which deviates from the common assumption of region-based methods that all characters are detected as connected components. Additionally, a novel feature based on character stroke area estimation is introduced. The feature is efficiently computed from a region distance map, it is invariant to scaling and rotations and allows to efficiently detect text regions regardless of what portion of text they capture. The method runs in real time and achieves state-of-the-art text localization and recognition results on the ICDAR 2013 Robust Reading dataset. version:1
arxiv-1504-01800 | A Multicomponent Approach to Nonrigid Registration of Diffusion Tensor Images | http://arxiv.org/abs/1504.01800 | id:1504.01800 author:Mohammed Khader, A. Ben Hamza category:cs.CV  published:2015-04-08 summary:We propose a nonrigid registration approach for diffusion tensor images using a multicomponent information-theoretic measure. Explicit orientation optimization is enabled by incorporating tensor reorientation, which is necessary for wrapping diffusion tensor images. Experimental results on diffusion tensor images indicate the feasibility of the proposed approach and a much better performance compared to the affine registration method based on mutual information in terms of registration accuracy in the presence of geometric distortion. version:2
arxiv-1503-06960 | Sample compression schemes for VC classes | http://arxiv.org/abs/1503.06960 | id:1503.06960 author:Shay Moran, Amir Yehudayoff category:cs.LG  published:2015-03-24 summary:Sample compression schemes were defined by Littlestone and Warmuth (1986) as an abstraction of the structure underlying many learning algorithms. Roughly speaking, a sample compression scheme of size $k$ means that given an arbitrary list of labeled examples, one can retain only $k$ of them in a way that allows to recover the labels of all other examples in the list. They showed that compression implies PAC learnability for binary-labeled classes, and asked whether the other direction holds. We answer their question and show that every concept class $C$ with VC dimension $d$ has a sample compression scheme of size exponential in $d$. The proof uses an approximate minimax phenomenon for binary matrices of low VC dimension, which may be of interest in the context of game theory. version:2
arxiv-1504-03504 | Sketch-based 3D Shape Retrieval using Convolutional Neural Networks | http://arxiv.org/abs/1504.03504 | id:1504.03504 author:Fang Wang, Le Kang, Yi Li category:cs.CV  published:2015-04-14 summary:Retrieving 3D models from 2D human sketches has received considerable attention in the areas of graphics, image retrieval, and computer vision. Almost always in state of the art approaches a large amount of "best views" are computed for 3D models, with the hope that the query sketch matches one of these 2D projections of 3D models using predefined features. We argue that this two stage approach (view selection -- matching) is pragmatic but also problematic because the "best views" are subjective and ambiguous, which makes the matching inputs obscure. This imprecise nature of matching further makes it challenging to choose features manually. Instead of relying on the elusive concept of "best views" and the hand-crafted features, we propose to define our views using a minimalism approach and learn features for both sketches and views. Specifically, we drastically reduce the number of views to only two predefined directions for the whole dataset. Then, we learn two Siamese Convolutional Neural Networks (CNNs), one for the views and one for the sketches. The loss function is defined on the within-domain as well as the cross-domain similarities. Our experiments on three benchmark datasets demonstrate that our method is significantly better than state of the art approaches, and outperforms them in all conventional metrics. version:1
arxiv-1412-3709 | An active search strategy for efficient object class detection | http://arxiv.org/abs/1412.3709 | id:1412.3709 author:Abel Gonzalez-Garcia, Alexander Vezhnevets, Vittorio Ferrari category:cs.CV  published:2014-12-11 summary:Object class detectors typically apply a window classifier to all the windows in a large set, either in a sliding window manner or using object proposals. In this paper, we develop an active search strategy that sequentially chooses the next window to evaluate based on all the information gathered before. This results in a substantial reduction in the number of classifier evaluations and in a more elegant approach in general. Our search strategy is guided by two forces. First, we exploit context as the statistical relation between the appearance of a window and its location relative to the object, as observed in the training set. This enables to jump across distant regions in the image (e.g. observing a sky region suggests that cars might be far below) and is done efficiently in a Random Forest framework. Second, we exploit the score of the classifier to attract the search to promising areas surrounding a highly scored window, and to keep away from areas near low scored ones. Our search strategy can be applied on top of any classifier as it treats it as a black-box. In experiments with R-CNN on the challenging SUN2012 dataset, our method matches the detection accuracy of evaluating all windows independently, while evaluating 9x fewer windows. version:2
arxiv-1407-1531 | The jump set under geometric regularisation. Part 1: Basic technique and first-order denoising | http://arxiv.org/abs/1407.1531 | id:1407.1531 author:Tuomo Valkonen category:math.FA cs.CV 26B30  49Q20  65J20  published:2014-07-06 summary:Let $u \in \mbox{BV}(\Omega)$ solve the total variation denoising problem with $L^2$-squared fidelity and data $f$. Caselles et al. [Multiscale Model. Simul. 6 (2008), 879--894] have shown the containment $\mathcal{H}^{m-1}(J_u \setminus J_f)=0$ of the jump set $J_u$ of $u$ in that of $f$. Their proof unfortunately depends heavily on the co-area formula, as do many results in this area, and as such is not directly extensible to higher-order, curvature-based, and other advanced geometric regularisers, such as total generalised variation (TGV) and Euler's elastica. These have received increased attention in recent times due to their better practical regularisation properties compared to conventional total variation or wavelets. We prove analogous jump set containment properties for a general class of regularisers. We do this with novel Lipschitz transformation techniques, and do not require the co-area formula. In the present Part 1 we demonstrate the general technique on first-order regularisers, while in Part 2 we will extend it to higher-order regularisers. In particular, we concentrate in this part on TV and, as a novelty, Huber-regularised TV. We also demonstrate that the technique would apply to non-convex TV models as well as the Perona-Malik anisotropic diffusion, if these approaches were well-posed to begin with. version:2
arxiv-1504-03154 | Real-world Object Recognition with Off-the-shelf Deep Conv Nets: How Many Objects can iCub Learn? | http://arxiv.org/abs/1504.03154 | id:1504.03154 author:Giulia Pasquale, Carlo Ciliberto, Francesca Odone, Lorenzo Rosasco, Lorenzo Natale category:cs.RO cs.CV cs.LG  published:2015-04-13 summary:The ability to visually recognize objects is a fundamental skill for robotics systems. Indeed, a large variety of tasks involving manipulation, navigation or interaction with other agents, deeply depends on the accurate understanding of the visual scene. Yet, at the time being, robots are lacking good visual perceptual systems, which often become the main bottleneck preventing the use of autonomous agents for real-world applications. Lately in computer vision, systems that learn suitable visual representations and based on multi-layer deep convolutional networks are showing remarkable performance in tasks such as large-scale visual recognition and image retrieval. To this regard, it is natural to ask whether such remarkable performance would generalize also to the robotic setting. In this paper we investigate such possibility, while taking further steps in developing a computational vision system to be embedded on a robotic platform, the iCub humanoid robot. In particular, we release a new dataset ({\sc iCubWorld28}) that we use as a benchmark to address the question: {\it how many objects can iCub recognize?} Our study is developed in a learning framework which reflects the typical visual experience of a humanoid robot like the iCub. Experiments shed interesting insights on the strength and weaknesses of current computer vision approaches applied in real robotic settings. version:2
arxiv-1504-03425 | Automated Analysis and Prediction of Job Interview Performance | http://arxiv.org/abs/1504.03425 | id:1504.03425 author:Iftekhar Naim, M. Iftekhar Tanveer, Daniel Gildea, Mohammed, Hoque category:cs.HC cs.AI cs.CL  published:2015-04-14 summary:We present a computational framework for automatically quantifying verbal and nonverbal behaviors in the context of job interviews. The proposed framework is trained by analyzing the videos of 138 interview sessions with 69 internship-seeking undergraduates at the Massachusetts Institute of Technology (MIT). Our automated analysis includes facial expressions (e.g., smiles, head gestures, facial tracking points), language (e.g., word counts, topic modeling), and prosodic information (e.g., pitch, intonation, and pauses) of the interviewees. The ground truth labels are derived by taking a weighted average over the ratings of 9 independent judges. Our framework can automatically predict the ratings for interview traits such as excitement, friendliness, and engagement with correlation coefficients of 0.75 or higher, and can quantify the relative importance of prosody, language, and facial expressions. By analyzing the relative feature weights learned by the regression models, our framework recommends to speak more fluently, use less filler words, speak as "we" (vs. "I"), use more unique words, and smile more. We also find that the students who were rated highly while answering the first interview question were also rated highly overall (i.e., first impression matters). Finally, our MIT Interview dataset will be made available to other researchers to further validate and expand our findings. version:1
arxiv-1412-0623 | Material Recognition in the Wild with the Materials in Context Database | http://arxiv.org/abs/1412.0623 | id:1412.0623 author:Sean Bell, Paul Upchurch, Noah Snavely, Kavita Bala category:cs.CV  published:2014-12-01 summary:Recognizing materials in real-world images is a challenging task. Real-world materials have rich surface texture, geometry, lighting conditions, and clutter, which combine to make the problem particularly difficult. In this paper, we introduce a new, large-scale, open dataset of materials in the wild, the Materials in Context Database (MINC), and combine this dataset with deep learning to achieve material recognition and segmentation of images in the wild. MINC is an order of magnitude larger than previous material databases, while being more diverse and well-sampled across its 23 categories. Using MINC, we train convolutional neural networks (CNNs) for two tasks: classifying materials from patches, and simultaneous material recognition and segmentation in full images. For patch-based classification on MINC we found that the best performing CNN architectures can achieve 85.2% mean class accuracy. We convert these trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected conditional random field (CRF) to predict the material at every pixel in an image, achieving 73.1% mean class accuracy. Our experiments demonstrate that having a large, well-sampled dataset such as MINC is crucial for real-world material recognition and segmentation. version:2
arxiv-1412-2306 | Deep Visual-Semantic Alignments for Generating Image Descriptions | http://arxiv.org/abs/1412.2306 | id:1412.2306 author:Andrej Karpathy, Li Fei-Fei category:cs.CV  published:2014-12-07 summary:We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations. version:2
arxiv-1504-03415 | HHCART: An Oblique Decision Tree | http://arxiv.org/abs/1504.03415 | id:1504.03415 author:D. C. Wickramarachchi, B. L. Robertson, M. Reale, C. J. Price, J. Brown category:stat.ML cs.LG  published:2015-04-14 summary:Decision trees are a popular technique in statistical data classification. They recursively partition the feature space into disjoint sub-regions until each sub-region becomes homogeneous with respect to a particular class. The basic Classification and Regression Tree (CART) algorithm partitions the feature space using axis parallel splits. When the true decision boundaries are not aligned with the feature axes, this approach can produce a complicated boundary structure. Oblique decision trees use oblique decision boundaries to potentially simplify the boundary structure. The major limitation of this approach is that the tree induction algorithm is computationally expensive. In this article we present a new decision tree algorithm, called HHCART. The method utilizes a series of Householder matrices to reflect the training data at each node during the tree construction. Each reflection is based on the directions of the eigenvectors from each classes' covariance matrix. Considering axis parallel splits in the reflected training data provides an efficient way of finding oblique splits in the unreflected training data. Experimental results show that the accuracy and size of the HHCART trees are comparable with some benchmark methods in the literature. The appealing feature of HHCART is that it can handle both qualitative and quantitative features in the same oblique split. version:1
arxiv-1504-03413 | Consensus based Detection in the Presence of Data Falsification Attacks | http://arxiv.org/abs/1504.03413 | id:1504.03413 author:Bhavya Kailkhura, Swastik Brahma, Pramod K. Varshney category:cs.SY cs.DC stat.AP stat.ML  published:2015-04-14 summary:This paper considers the problem of detection in distributed networks in the presence of data falsification (Byzantine) attacks. Detection approaches considered in the paper are based on fully distributed consensus algorithms, where all of the nodes exchange information only with their neighbors in the absence of a fusion center. In such networks, we characterize the negative effect of Byzantines on the steady-state and transient detection performance of the conventional consensus based detection algorithms. To address this issue, we study the problem from the network designer's perspective. More specifically, we first propose a distributed weighted average consensus algorithm that is robust to Byzantine attacks. We show that, under reasonable assumptions, the global test statistic for detection can be computed locally at each node using our proposed consensus algorithm. We exploit the statistical distribution of the nodes' data to devise techniques for mitigating the influence of data falsifying Byzantines on the distributed detection system. Since some parameters of the statistical distribution of the nodes' data might not be known a priori, we propose learning based techniques to enable an adaptive design of the local fusion or update rules. version:1
arxiv-1504-03410 | Simultaneous Feature Learning and Hash Coding with Deep Neural Networks | http://arxiv.org/abs/1504.03410 | id:1504.03410 author:Hanjiang Lai, Yan Pan, Ye Liu, Shuicheng Yan category:cs.CV  published:2015-04-14 summary:Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods. version:1
arxiv-1504-03409 | Clustering Assisted Fundamental Matrix Estimation | http://arxiv.org/abs/1504.03409 | id:1504.03409 author:Hao Wu, Yi Wan category:cs.CV  published:2015-04-14 summary:In computer vision, the estimation of the fundamental matrix is a basic problem that has been extensively studied. The accuracy of the estimation imposes a significant influence on subsequent tasks such as the camera trajectory determination and 3D reconstruction. In this paper we propose a new method for fundamental matrix estimation that makes use of clustering a group of 4D vectors. The key insight is the observation that among the 4D vectors constructed from matching pairs of points obtained from the SIFT algorithm, well-defined cluster points tend to be reliable inliers suitable for fundamental matrix estimation. Based on this, we utilizes a recently proposed efficient clustering method through density peaks seeking and propose a new clustering assisted method. Experimental results show that the proposed algorithm is faster and more accurate than currently commonly used methods. version:1
arxiv-1412-6599 | Hot Swapping for Online Adaptation of Optimization Hyperparameters | http://arxiv.org/abs/1412.6599 | id:1412.6599 author:Kevin Bache, Dennis DeCoste, Padhraic Smyth category:cs.LG 62L20 G.1.6; I.2.6  published:2014-12-20 summary:We describe a general framework for online adaptation of optimization hyperparameters by `hot swapping' their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search. version:3
arxiv-1412-6563 | Self-informed neural network structure learning | http://arxiv.org/abs/1412.6563 | id:1412.6563 author:David Warde-Farley, Andrew Rabinovich, Dragomir Anguelov category:stat.ML cs.CV cs.LG cs.NE  published:2014-12-20 summary:We study the problem of large scale, multi-label visual recognition with a large number of possible classes. We propose a method for augmenting a trained neural network classifier with auxiliary capacity in a manner designed to significantly improve upon an already well-performing model, while minimally impacting its computational footprint. Using the predictions of the network itself as a descriptor for assessing visual similarity, we define a partitioning of the label space into groups of visually similar entities. We then augment the network with auxilliary hidden layer pathways with connectivity only to these groups of label units. We report a significant improvement in mean average precision on a large-scale object recognition task with the augmented model, while increasing the number of multiply-adds by less than 3%. version:2
arxiv-1503-08909 | Beyond Short Snippets: Deep Networks for Video Classification | http://arxiv.org/abs/1503.08909 | id:1503.08909 author:Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat Monga, George Toderici category:cs.CV  published:2015-03-31 summary:Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1% vs. 60.9%) and the UCF-101 datasets with (88.6% vs. 88.0%) and without additional optical flow information (82.6% vs. 72.8%). version:2
arxiv-1504-03285 | Multiple Measurements and Joint Dimensionality Reduction for Large Scale Image Search with Short Vectors - Extended Version | http://arxiv.org/abs/1504.03285 | id:1504.03285 author:Filip Radenovic, Herve Jegou, Ondrej Chum category:cs.CV  published:2015-04-13 summary:This paper addresses the construction of a short-vector (128D) image representation for large-scale image and particular object retrieval. In particular, the method of joint dimensionality reduction of multiple vocabularies is considered. We study a variety of vocabulary generation techniques: different k-means initializations, different descriptor transformations, different measurement regions for descriptor extraction. Our extensive evaluation shows that different combinations of vocabularies, each partitioning the descriptor space in a different yet complementary manner, results in a significant performance improvement, which exceeds the state-of-the-art. version:1
arxiv-1508-02060 | Egyptian Dialect Stopword List Generation from Social Network Data | http://arxiv.org/abs/1508.02060 | id:1508.02060 author:Walaa Medhat, Ahmed H. Yousef, Hoda Korashy category:cs.CL  published:2015-04-13 summary:This paper proposes a methodology for generating a stopword list from online social network (OSN) corpora in Egyptian Dialect(ED). The aim of the paper is to investigate the effect of removingED stopwords on the Sentiment Analysis (SA) task. The stopwords lists generated before were on Modern Standard Arabic (MSA) which is not the common language used in OSN. We have generated a stopword list of Egyptian dialect to be used with the OSN corpora. We compare the efficiency of text classification when using the generated list along with previously generated lists of MSA and combining the Egyptian dialect list with the MSA list. The text classification was performed using Na\"ive Bayes and Decision Tree classifiers and two feature selection approaches, unigram and bigram. The experiments show that removing ED stopwords give better performance than using lists of MSA stopwords only. version:1
arxiv-1504-03212 | Optimal Parameter Choices Through Self-Adjustment: Applying the 1/5-th Rule in Discrete Settings | http://arxiv.org/abs/1504.03212 | id:1504.03212 author:Benjamin Doerr, Carola Doerr category:cs.NE  published:2015-04-13 summary:While evolutionary algorithms are known to be very successful for a broad range of applications, the algorithm designer is often left with many algorithmic choices, for example, the size of the population, the mutation rates, and the crossover rates of the algorithm. These parameters are known to have a crucial influence on the optimization time, and thus need to be chosen carefully, a task that often requires substantial efforts. Moreover, the optimal parameters can change during the optimization process. It is therefore of great interest to design mechanisms that dynamically choose best-possible parameters. An example for such an update mechanism is the one-fifth success rule for step-size adaption in evolutionary strategies. While in continuous domains this principle is well understood also from a mathematical point of view, no comparable theory is available for problems in discrete domains. In this work we show that the one-fifth success rule can be effective also in discrete settings. We regard the $(1+(\lambda,\lambda))$~GA proposed in [Doerr/Doerr/Ebel: From black-box complexity to designing new genetic algorithms, TCS 2015]. We prove that if its population size is chosen according to the one-fifth success rule then the expected optimization time on \textsc{OneMax} is linear. This is better than what \emph{any} static population size $\lambda$ can achieve and is asymptotically optimal also among all adaptive parameter choices. version:1
arxiv-1503-07490 | Sensitivity Analysis for additive STDP rule | http://arxiv.org/abs/1503.07490 | id:1503.07490 author:Subhajit Sengupta, Karthik S. Gurumoorthy, Arunava Banerjee category:q-bio.NC cs.NE 91E40  68T05  92C42  published:2015-02-28 summary:Spike Timing Dependent Plasticity (STDP) is a Hebbian like synaptic learning rule. The basis of STDP has strong experimental evidences and it depends on precise input and output spike timings. In this paper we show that under biologically plausible spiking regime, slight variability in the spike timing leads to drastically different evolution of synaptic weights when its dynamics are governed by the additive STDP rule. version:2
arxiv-1502-05908 | Learning Descriptors for Object Recognition and 3D Pose Estimation | http://arxiv.org/abs/1502.05908 | id:1502.05908 author:Paul Wohlhart, Vincent Lepetit category:cs.CV  published:2015-02-20 summary:Detecting poorly textured objects and estimating their 3D pose reliably is still a very challenging problem. We introduce a simple but powerful approach to computing descriptors for object views that efficiently capture both the object identity and 3D pose. By contrast with previous manifold-based approaches, we can rely on the Euclidean distance to evaluate the similarity between descriptors, and therefore use scalable Nearest Neighbor search methods to efficiently handle a large number of objects under a large range of poses. To achieve this, we train a Convolutional Neural Network to compute these descriptors by enforcing simple similarity and dissimilarity constraints between the descriptors. We show that our constraints nicely untangle the images from different objects and different views into clusters that are not only well-separated but also structured as the corresponding sets of poses: The Euclidean distance between descriptors is large when the descriptors are from different objects, and directly related to the distance between the poses when the descriptors are from the same object. These important properties allow us to outperform state-of-the-art object views representations on challenging RGB and RGB-D data. version:2
arxiv-1504-03183 | Adaptive Randomized Dimension Reduction on Massive Data | http://arxiv.org/abs/1504.03183 | id:1504.03183 author:Gregory Darnell, Stoyan Georgiev, Sayan Mukherjee, Barbara E Engelhardt category:stat.ML q-bio.QM  published:2015-04-13 summary:The scalability of statistical estimators is of increasing importance in modern applications. One approach to implementing scalable algorithms is to compress data into a low dimensional latent space using dimension reduction methods. In this paper we develop an approach for dimension reduction that exploits the assumption of low rank structure in high dimensional data to gain both computational and statistical advantages. We adapt recent randomized low-rank approximation algorithms to provide an efficient solution to principal component analysis (PCA), and we use this efficient solver to improve parameter estimation in large-scale linear mixed models (LMM) for association mapping in statistical and quantitative genomics. A key observation in this paper is that randomization serves a dual role, improving both computational and statistical performance by implicitly regularizing the covariance matrix estimate of the random effect in a LMM. These statistical and computational advantages are highlighted in our experiments on simulated data and large-scale genomic studies. version:1
arxiv-1504-03156 | Streaming, Memory Limited Matrix Completion with Noise | http://arxiv.org/abs/1504.03156 | id:1504.03156 author:Se-Young Yun, Marc Lelarge, Alexandre Proutiere category:math.SP stat.ML  published:2015-04-13 summary:In this paper, we consider the streaming memory-limited matrix completion problem when the observed entries are noisy versions of a small random fraction of the original entries. We are interested in scenarios where the matrix size is very large so the matrix is very hard to store and manipulate. Here, columns of the observed matrix are presented sequentially and the goal is to complete the missing entries after one pass on the data with limited memory space and limited computational complexity. We propose a streaming algorithm which produces an estimate of the original matrix with a vanishing mean square error, uses memory space scaling linearly with the ambient dimension of the matrix, i.e. the memory required to store the output alone, and spends computations as much as the number of non-zero entries of the input matrix. version:1
arxiv-1410-6333 | A Regularization Approach to Blind Deblurring and Denoising of QR Barcodes | http://arxiv.org/abs/1410.6333 | id:1410.6333 author:Yves van Gennip, Prashant Athavale, Jérôme Gilles, Rustum Choksi category:cs.CV math.NA 68U10  65K10  published:2014-10-23 summary:QR bar codes are prototypical images for which part of the image is a priori known (required patterns). Open source bar code readers, such as ZBar, are readily available. We exploit both these facts to provide and assess purely regularization-based methods for blind deblurring of QR bar codes in the presence of noise. version:2
arxiv-1504-03106 | Learning Multiple Visual Tasks while Discovering their Structure | http://arxiv.org/abs/1504.03106 | id:1504.03106 author:Carlo Ciliberto, Lorenzo Rosasco, Silvia Villa category:cs.LG cs.CV  published:2015-04-13 summary:Multi-task learning is a natural approach for computer vision applications that require the simultaneous solution of several distinct but related problems, e.g. object detection, classification, tracking of multiple agents, or denoising, to name a few. The key idea is that exploring task relatedness (structure) can lead to improved performances. In this paper, we propose and study a novel sparse, non-parametric approach exploiting the theory of Reproducing Kernel Hilbert Spaces for vector-valued functions. We develop a suitable regularization framework which can be formulated as a convex optimization problem, and is provably solvable using an alternating minimization approach. Empirical tests show that the proposed method compares favorably to state of the art techniques and further allows to recover interpretable structures, a problem of interest in its own right. version:1
arxiv-1412-6806 | Striving for Simplicity: The All Convolutional Net | http://arxiv.org/abs/1412.6806 | id:1412.6806 author:Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller category:cs.LG cs.CV cs.NE  published:2014-12-21 summary:Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches. version:3
arxiv-1409-3518 | Topic Modeling of Hierarchical Corpora | http://arxiv.org/abs/1409.3518 | id:1409.3518 author:Do-kyum Kim, Geoffrey M. Voelker, Lawrence K. Saul category:stat.ML cs.IR cs.LG  published:2014-09-11 summary:We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy. We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation. The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For these models we show that there exists a simple variational approximation for probabilistic inference. The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model's hierarchy. We compare our approach to existing implementations of nonparametric HDPs. On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods. Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security---one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy. version:2
arxiv-1503-06813 | Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation | http://arxiv.org/abs/1503.06813 | id:1503.06813 author:Haopeng Zhang, Tarek El-Gaaly, Ahmed Elgammal, Zhiguo Jiang category:cs.CV  published:2015-03-23 summary:Due to large variations in shape, appearance, and viewing conditions, object recognition is a key precursory challenge in the fields of object manipulation and robotic/AI visual reasoning in general. Recognizing object categories, particular instances of objects and viewpoints/poses of objects are three critical subproblems robots must solve in order to accurately grasp/manipulate objects and reason about their environments. Multi-view images of the same object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g. visual/depth descriptor spaces). These object manifolds share the same topology despite being geometrically different. Each object manifold can be represented as a deformed version of a unified manifold. The object manifolds can thus be parameterized by its homeomorphic mapping/reconstruction from the unified manifold. In this work, we develop a novel framework to jointly solve the three challenging recognition sub-problems, by explicitly modeling the deformations of object manifolds and factorizing it in a view-invariant space for recognition. We perform extensive experiments on several challenging datasets and achieve state-of-the-art results. version:2
arxiv-1504-02382 | Robust, scalable and fast bootstrap method for analyzing large scale data | http://arxiv.org/abs/1504.02382 | id:1504.02382 author:Shahab Basiri, Esa Ollila, Visa Koivunen category:stat.ME cs.IR cs.IT math.IT stat.CO stat.ML  published:2015-04-09 summary:In this paper we address the problem of performing statistical inference for large scale data sets i.e., Big Data. The volume and dimensionality of the data may be so high that it cannot be processed or stored in a single computing node. We propose a scalable, statistically robust and computationally efficient bootstrap method, compatible with distributed processing and storage systems. Bootstrap resamples are constructed with smaller number of distinct data points on multiple disjoint subsets of data, similarly to the bag of little bootstrap method (BLB) [1]. Then significant savings in computation is achieved by avoiding the re-computation of the estimator for each bootstrap sample. Instead, a computationally efficient fixed-point estimation equation is analytically solved via a smart approximation following the Fast and Robust Bootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use of highly robust statistical methods in analyzing large scale data sets. The favorable statistical properties of the method are established analytically. Numerical examples demonstrate scalability, low complexity and robust statistical performance of the method in analyzing large data sets. version:2
arxiv-0912-3648 | Geometric Representations of Random Hypergraphs | http://arxiv.org/abs/0912.3648 | id:0912.3648 author:Simón Lunagómez, Sayan Mukherjee, Robert L. Wolpert, Edoardo M. Airoldi category:math.ST math.PR stat.ML stat.TH 60K35  published:2009-12-18 summary:A parametrization of hypergraphs based on the geometry of points in $\mathbf{R}^d$ is developed. Informative prior distributions on hypergraphs are induced through this parametrization by priors on point configurations via spatial processes. This prior specification is used to infer conditional independence models or Markov structure of multivariate distributions. Specifically, we can recover both the junction tree factorization as well as the hyper Markov law. This approach offers greater control on the distribution of graph features than Erd\"os-R\'enyi random graphs, supports inference of factorizations that cannot be retrieved by a graph alone, and leads to new Metropolis\slash Hastings Markov chain Monte Carlo algorithms with both local and global moves in graph space. We illustrate the utility of this parametrization and prior specification using simulations. version:3
arxiv-1504-02975 | Classification with Extreme Learning Machine and Ensemble Algorithms Over Randomly Partitioned Data | http://arxiv.org/abs/1504.02975 | id:1504.02975 author:Ferhat Özgür Çatak category:cs.LG  published:2015-04-12 summary:In this age of Big Data, machine learning based data mining methods are extensively used to inspect large scale data sets. Deriving applicable predictive modeling from these type of data sets is a challenging obstacle because of their high complexity. Opportunity with high data availability levels, automated classification of data sets has become a critical and complicated function. In this paper, the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) are explored to build reliable predictive bag of classification models. Thus, (i) dataset ensembles are build; (ii) ELM algorithm is used to build weak classification models; and (iii) build a strong classification model from a set of weak classification models. This training model is applied to the publicly available knowledge discovery and data mining datasets. version:1
arxiv-1504-02972 | Computing trading strategies based on financial sentiment data using evolutionary optimization | http://arxiv.org/abs/1504.02972 | id:1504.02972 author:Ronald Hochreiter category:q-fin.PM cs.NE  published:2015-04-12 summary:In this paper we apply evolutionary optimization techniques to compute optimal rule-based trading strategies based on financial sentiment data. The sentiment data was extracted from the social media service StockTwits to accommodate the level of bullishness or bearishness of the online trading community towards certain stocks. Numerical results for all stocks from the Dow Jones Industrial Average (DJIA) index are presented and a comparison to classical risk-return portfolio selection is provided. version:1
arxiv-1503-00593 | Learning a Convolutional Neural Network for Non-uniform Motion Blur Removal | http://arxiv.org/abs/1503.00593 | id:1503.00593 author:Jian Sun, Wenfei Cao, Zongben Xu, Jean Ponce category:cs.CV I.4  published:2015-03-02 summary:In this paper, we address the problem of estimating and removing non-uniform motion blur from a single blurry image. We propose a deep learning approach to predicting the probabilistic distribution of motion blur at the patch level using a convolutional neural network (CNN). We further extend the candidate set of motion kernels predicted by the CNN using carefully designed image rotations. A Markov random field model is then used to infer a dense non-uniform motion blur field enforcing motion smoothness. Finally, motion blur is removed by a non-uniform deblurring model using patch-level image prior. Experimental evaluations show that our approach can effectively estimate and remove complex non-uniform motion blur that is not handled well by previous approaches. version:3
arxiv-1504-02945 | Deep Transform: Cocktail Party Source Separation via Complex Convolution in a Deep Neural Network | http://arxiv.org/abs/1504.02945 | id:1504.02945 author:Andrew J. R. Simpson category:cs.SD cs.LG cs.NE 68Txx  published:2015-04-12 summary:Convolutional deep neural networks (DNN) are state of the art in many engineering problems but have not yet addressed the issue of how to deal with complex spectrograms. Here, we use circular statistics to provide a convenient probabilistic estimate of spectrogram phase in a complex convolutional DNN. In a typical cocktail party source separation scenario, we trained a convolutional DNN to re-synthesize the complex spectrograms of two source speech signals given a complex spectrogram of the monaural mixture - a discriminative deep transform (DT). We then used this complex convolutional DT to obtain probabilistic estimates of the magnitude and phase components of the source spectrograms. Our separation results are on a par with equivalent binary-mask based non-complex separation approaches. version:1
arxiv-1501-05892 | Capacity-achieving Sparse Superposition Codes via Approximate Message Passing Decoding | http://arxiv.org/abs/1501.05892 | id:1501.05892 author:Cynthia Rush, Adam Greig, Ramji Venkataramanan category:cs.IT math.IT stat.ML  published:2015-01-23 summary:Sparse superposition codes were recently introduced by Barron and Joseph for reliable communication over the AWGN channel at rates approaching the channel capacity. The codebook is defined in terms of a Gaussian design matrix, and codewords are sparse linear combinations of columns of the matrix. In this paper, we propose an approximate message passing decoder for sparse superposition codes, whose decoding complexity scales linearly with the size of the design matrix. The performance of the decoder is rigorously analyzed and it is shown to asymptotically achieve the AWGN capacity with an appropriate power allocation. Simulation results are provided to demonstrate the performance of the decoder at finite blocklengths. We introduce a power allocation scheme to improve the empirical performance, and demonstrate how the decoding complexity can be significantly reduced by using Hadamard design matrices. version:2
arxiv-1504-02931 | Generalized Correntropy for Robust Adaptive Filtering | http://arxiv.org/abs/1504.02931 | id:1504.02931 author:Badong Chen, Lei Xing, Haiquan Zhao, Nanning Zheng, José C. Príncipe category:stat.ML cs.IT math.IT  published:2015-04-12 summary:As a robust nonlinear similarity measure in kernel space, correntropy has received increasing attention in domains of machine learning and signal processing. In particular, the maximum correntropy criterion (MCC) has recently been successfully applied in robust regression and filtering. The default kernel function in correntropy is the Gaussian kernel, which is, of course, not always the best choice. In this work, we propose a generalized correntropy that adopts the generalized Gaussian density (GGD) function as the kernel (not necessarily a Mercer kernel), and present some important properties. We further propose the generalized maximum correntropy criterion (GMCC), and apply it to adaptive filtering. An adaptive algorithm, called the GMCC algorithm, is derived, and the mean square convergence performance is studied. We show that the proposed algorithm is very stable and can achieve zero probability of divergence (POD). Simulation results confirm the theoretical expectations and demonstrate the desirable performance of the new algorithm. version:1
arxiv-1502-07645 | Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo | http://arxiv.org/abs/1502.07645 | id:1502.07645 author:Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola category:stat.ML cs.LG  published:2015-02-26 summary:We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to "differential privacy:, a cryptographic approach to protect individual-level privacy while permiting database-level utility. Specifically, we show that that under standard assumptions, getting one single sample from a posterior distribution is differentially private "for free". We will see that estimator is statistically consistent, near optimal and computationally tractable whenever the Bayesian model of interest is consistent, optimal and tractable. Similarly but separately, we show that a recent line of works that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an "anytime" algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets. version:2
arxiv-1412-1587 | The entropic barrier: a simple and optimal universal self-concordant barrier | http://arxiv.org/abs/1412.1587 | id:1412.1587 author:Sébastien Bubeck, Ronen Eldan category:math.OC cs.IT cs.LG math.IT  published:2014-12-04 summary:We prove that the Cram\'er transform of the uniform measure on a convex body in $\mathbb{R}^n$ is a $(1+o(1)) n$-self-concordant barrier, improving a seminal result of Nesterov and Nemirovski. This gives the first explicit construction of a universal barrier for convex bodies with optimal self-concordance parameter. The proof is based on basic geometry of log-concave distributions, and elementary duality in exponential families. version:3
arxiv-1412-6598 | Automatic Discovery and Optimization of Parts for Image Classification | http://arxiv.org/abs/1412.6598 | id:1412.6598 author:Sobhan Naderi Parizi, Andrea Vedaldi, Andrew Zisserman, Pedro Felzenszwalb category:cs.CV cs.LG  published:2014-12-20 summary:Part-based representations have been shown to be very useful for image classification. Learning part-based models is often viewed as a two-stage problem. First, a collection of informative parts is discovered, using heuristics that promote part distinctiveness and diversity, and then classifiers are trained on the vector of part responses. In this paper we unify the two stages and learn the image classifiers and a set of shared parts jointly. We generate an initial pool of parts by randomly sampling part candidates and selecting a good subset using L1/L2 regularization. All steps are driven "directly" by the same objective namely the classification loss on a training set. This lets us do away with engineered heuristics. We also introduce the notion of "negative parts", intended as parts that are negatively correlated with one or more classes. Negative parts are complementary to the parts discovered by other methods, which look only for positive correlations. version:2
arxiv-1504-02902 | Gradual Training Method for Denoising Auto Encoders | http://arxiv.org/abs/1504.02902 | id:1504.02902 author:Alexander Kalmanovich, Gal Chechik category:cs.LG cs.NE  published:2015-04-11 summary:Stacked denoising auto encoders (DAEs) are well known to learn useful deep representations, which can be used to improve supervised training by initializing a deep network. We investigate a training scheme of a deep DAE, where DAE layers are gradually added and keep adapting as additional layers are added. We show that in the regime of mid-sized datasets, this gradual training provides a small but consistent improvement over stacked training in both reconstruction quality and classification error over stacked training on MNIST and CIFAR datasets. version:1
arxiv-1212-1108 | On the Convergence Properties of Optimal AdaBoost | http://arxiv.org/abs/1212.1108 | id:1212.1108 author:Joshua Belanich, Luis E. Ortiz category:cs.LG cs.AI stat.ML I.2.6  published:2012-12-05 summary:AdaBoost is one of the most popular machine-learning algorithms. It is simple to implement and often found very effective by practitioners, while still being mathematically elegant and theoretically sound. AdaBoost's behavior in practice, and in particular the test-error behavior, has puzzled many eminent researchers for over a decade: It seems to defy our general intuition in machine learning regarding the fundamental trade-off between model complexity and generalization performance. In this paper, we establish the convergence of "Optimal AdaBoost," a term coined by Rudin, Daubechies, and Schapire in 2004. We prove the convergence, with the number of rounds, of the classifier itself, its generalization error, and its resulting margins for fixed data sets, under certain reasonable conditions. More generally, we prove that the time/per-round average of almost any function of the example weights converges. Our approach is to frame AdaBoost as a dynamical system, to provide sufficient conditions for the existence of an invariant measure, and to employ tools from ergodic theory. Unlike previous work, we do not assume AdaBoost cycles; actually, we present empirical evidence against it on real-world datasets. Our main theoretical results hold under a weaker condition. We show sufficient empirical evidence that Optimal AdaBoost always met the condition on every real-world dataset we tried. Our results formally ground future convergence-rate analyses, and may even provide opportunities for slight algorithmic modifications to optimize the generalization ability of AdaBoost classifiers, thus reducing a practitioner's burden of deciding how long to run the algorithm. version:2
arxiv-1502-05137 | Prediction of Search Targets From Fixations in Open-World Settings | http://arxiv.org/abs/1502.05137 | id:1502.05137 author:Hosnieh Sattar, Sabine Müller, Mario Fritz, Andreas Bulling category:cs.CV  published:2015-02-18 summary:Previous work on predicting the target of visual search from human fixations only considered closed-world settings in which training labels are available and predictions are performed for a known set of potential targets. In this work we go beyond the state of the art by studying search target prediction in an open-world setting in which we no longer assume that we have fixation data to train for the search targets. We present a dataset containing fixation data of 18 users searching for natural images from three image categories within synthesised image collages of about 80 images. In a closed-world baseline experiment we show that we can predict the correct target image out of a candidate set of five images. We then present a new problem formulation for search target prediction in the open-world setting that is based on learning compatibilities between fixations and potential targets. version:3
arxiv-1501-07180 | End-to-End Photo-Sketch Generation via Fully Convolutional Representation Learning | http://arxiv.org/abs/1501.07180 | id:1501.07180 author:Liliang Zhang, Liang Lin, Xian Wu, Shengyong Ding, Lei Zhang category:cs.CV I.2.10  published:2015-01-28 summary:Sketch-based face recognition is an interesting task in vision and multimedia research, yet it is quite challenging due to the great difference between face photos and sketches. In this paper, we propose a novel approach for photo-sketch generation, aiming to automatically transform face photos into detail-preserving personal sketches. Unlike the traditional models synthesizing sketches based on a dictionary of exemplars, we develop a fully convolutional network to learn the end-to-end photo-sketch mapping. Our approach takes whole face photos as inputs and directly generates the corresponding sketch images with efficient inference and learning, in which the architecture are stacked by only convolutional kernels of very small sizes. To well capture the person identity during the photo-sketch transformation, we define our optimization objective in the form of joint generative-discriminative minimization. In particular, a discriminative regularization term is incorporated into the photo-sketch generation, enhancing the discriminability of the generated person sketches against other individuals. Extensive experiments on several standard benchmarks suggest that our approach outperforms other state-of-the-art methods in both photo-sketch generation and face sketch verification. version:2
arxiv-1503-04338 | Towards radio astronomical imaging using an arbitrary basis | http://arxiv.org/abs/1503.04338 | id:1503.04338 author:Matthias Petschow category:astro-ph.IM cs.CV  published:2015-03-14 summary:The new generation of radio telescopes, such as the Square Kilometer Array (SKA), requires dramatic advances in computer hardware and software, in order to process the large amounts of produced data efficiently. In this document, we explore a new approach to wide-field imaging. By generalizing the image reconstruction, which is performed by an inverse Fourier transform, to arbitrary transformations, we gain enormous new possibilities. In particular, we outline an approach that might allow to obtain a sky image of size P times Q in (optimal) O(PQ) time. This could be a step in the direction of real-time, wide-field sky imaging for future telescopes. version:2
arxiv-1504-02870 | Quick sensitivity analysis for incremental data modification and its application to leave-one-out CV in linear classification problems | http://arxiv.org/abs/1504.02870 | id:1504.02870 author:Shota Okumura, Yoshiki Suzuki, Ichiro Takeuchi category:stat.ML cs.LG  published:2015-04-11 summary:We introduce a novel sensitivity analysis framework for large scale classification problems that can be used when a small number of instances are incrementally added or removed. For quickly updating the classifier in such a situation, incremental learning algorithms have been intensively studied in the literature. Although they are much more efficient than solving the optimization problem from scratch, their computational complexity yet depends on the entire training set size. It means that, if the original training set is large, completely solving an incremental learning problem might be still rather expensive. To circumvent this computational issue, we propose a novel framework that allows us to make an inference about the updated classifier without actually re-optimizing it. Specifically, the proposed framework can quickly provide a lower and an upper bounds of a quantity on the unknown updated classifier. The main advantage of the proposed framework is that the computational cost of computing these bounds depends only on the number of updated instances. This property is quite advantageous in a typical sensitivity analysis task where only a small number of instances are updated. In this paper we demonstrate that the proposed framework is applicable to various practical sensitivity analysis tasks, and the bounds provided by the framework are often sufficiently tight for making desired inferences. version:1
arxiv-1504-02863 | Appearance-Based Gaze Estimation in the Wild | http://arxiv.org/abs/1504.02863 | id:1504.02863 author:Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling category:cs.CV  published:2015-04-11 summary:Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have been not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is significantly more variable than existing ones with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks that significantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation. We present an extensive evaluation of several state-of-the-art image-based gaze estimation algorithms on three current datasets, including our own. This evaluation provides clear insights and allows us to identify key research challenges of gaze estimation in the wild. version:1
arxiv-1504-02856 | High Density Noise Removal by Cascading Algorithms | http://arxiv.org/abs/1504.02856 | id:1504.02856 author:Arabinda Dash, Sujaya Kumar Sathua category:cs.CV  published:2015-04-11 summary:An advanced non-linear cascading filter algorithm for the removal of high density salt and pepper noise from the digital images is proposed. The proposed method consists of two stages. The first stage Decision base Median Filter (DMF) acts as the preliminary noise removal algorithm. The second stage is either Modified Decision Base Partial Trimmed Global Mean Filter (MDBPTGMF) or Modified Decision Based Unsymmetric Trimmed Median Filter (MDBUTMF) which is used to remove the remaining noise and enhance the image quality. The DMF algorithm performs well at low noise density but it fails to remove the noise at medium and high level. The MDBPTGMF and MDUTMF have excellent performance at low, medium and high noise density but these reduce the image quality and blur the image at high noise level. So the basic idea behind this paper is to combine the advantages of the filters used in both the stages to remove the Salt and Pepper noise and enhance the image quality at all the noise density level. The proposed method is tested against different gray scale images and it gives better Mean Absolute Error (MAE), Peak Signal to Noise Ratio (PSNR) and Image Enhancement Factor (IEF) than the Adaptive Median Filter (AMF), Decision Base Unsymmetric Trimmed Median Filter (DBUTMF), Modified Decision Base Unsymmetric Trimmed Median Filter (MDBUTMF) and Decision Base Partial Trimmed Global Mean Filter (DBPTGMF). version:1
arxiv-1504-02840 | siftservice.com - Turning a Computer Vision algorithm into a World Wide Web Service | http://arxiv.org/abs/1504.02840 | id:1504.02840 author:Ahmad Pahlavan Tafti, Hamid Hassannia, Zeyun Yu category:cs.CV  published:2015-04-11 summary:Image features detection and description is a longstanding topic in computer vision and pattern recognition areas. The Scale Invariant Feature Transform (SIFT) is probably the most popular and widely demanded feature descriptor which facilitates a variety of computer vision applications such as image registration, object tracking, image forgery detection, and 3D surface reconstruction. This work introduces a Software as a Service (SaaS) based implementation of the SIFT algorithm which is freely available at http://siftservice.com for any academic, educational and research purposes. The service provides application-to-application interaction and aims Rapid Application Development (RAD) and also fast prototyping for computer vision students and researchers all around the world. An Internet connection is all they need! version:1
arxiv-1412-8419 | Simple Image Description Generator via a Linear Phrase-Based Approach | http://arxiv.org/abs/1412.8419 | id:1412.8419 author:Remi Lebret, Pedro O. Pinheiro, Ronan Collobert category:cs.CL cs.CV cs.NE  published:2014-12-29 summary:Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results on the recently release Microsoft COCO dataset. version:3
arxiv-1412-7054 | Attention for Fine-Grained Categorization | http://arxiv.org/abs/1412.7054 | id:1412.7054 author:Pierre Sermanet, Andrea Frome, Esteban Real category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:This paper presents experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, specifically fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments, whereas we present results for fine-grained categorization better than the state-of-the-art GoogLeNet classification model. We show that our model learns to direct high resolution attention to the most discriminative regions without any spatial supervision such as bounding boxes, and it is able to discriminate fine-grained dog breeds moderately well even when given only an initial low-resolution context image and narrow, inexpensive glimpses at faces and fur patterns. This and similar attention models have the major advantage of being trained end-to-end, as opposed to other current detection and recognition pipelines with hand-engineered components where information is lost. While our model is state-of-the-art, further work is needed to fully leverage the sequential input. version:3
arxiv-1504-02813 | Modelling Multi-level Power Usage with Latent States and Smooth Functions | http://arxiv.org/abs/1504.02813 | id:1504.02813 author:Camila P. E. de Souza, Nancy E. Heckman category:stat.ME stat.AP stat.ML  published:2015-04-10 summary:We develop and apply a new approach for analyzing a building's business day power usage. We treat each business day as a replicate and model power usage as arising from two smooth functions, one function giving power usage when the cooling system is off, the other function giving power usage when the cooling system is on. The condition "chiller on"/"chiller off" at any particular time cannot be observed directly, thus forming a latent process. In general, our method can be applied to multi-curve data where each curve is driven by a latent state process. The state at any particular point determines a smooth function. Thus each curve follows what we call a switching nonparametric regression model. We develop an EM algorithm to estimate the parameters of the latent process and the function corresponding to each state. We also obtain standard errors for the parameter estimates of the state process. Simulations studies show the frequentist properties of our estimates. version:1
arxiv-1412-7028 | Joint RNN-Based Greedy Parsing and Word Composition | http://arxiv.org/abs/1412.7028 | id:1412.7028 author:Joël Legrand, Ronan Collobert category:cs.LG cs.CL cs.NE  published:2014-12-22 summary:This paper introduces a greedy parser based on neural networks, which leverages a new compositional sub-tree representation. The greedy parser and the compositional procedure are jointly trained, and tightly depends on each-other. The composition procedure outputs a vector representation which summarizes syntactically (parsing tags) and semantically (words) sub-trees. Composition and tagging is achieved over continuous (word or tag) representations, and recurrent neural networks. We reach F1 performance on par with well-known existing parsers, while having the advantage of speed, thanks to the greedy nature of the parser. We provide a fully functional implementation of the method described in this paper. version:4
arxiv-1504-02800 | High-Dimensional Classification for Brain Decoding | http://arxiv.org/abs/1504.02800 | id:1504.02800 author:Nicole Croteau, Farouk S. Nathoo, Jiguo Cao, Ryan Budney category:stat.ML  published:2015-04-10 summary:Brain decoding involves the determination of a subject's cognitive state or an associated stimulus from functional neuroimaging data measuring brain activity. In this setting the cognitive state is typically characterized by an element of a finite set, and the neuroimaging data comprise voluminous amounts of spatiotemporal data measuring some aspect of the neural signal. The associated statistical problem is one of classification from high-dimensional data. We explore the use of functional principal component analysis, mutual information networks, and persistent homology for examining the data through exploratory analysis and for constructing features characterizing the neural signal for brain decoding. We review each approach from this perspective, and we incorporate the features into a classifier based on symmetric multinomial logistic regression with elastic net regularization. The approaches are illustrated in an application where the task is to infer, from brain activity measured with magnetoencephalography (MEG), the type of video stimulus shown to a subject. version:1
arxiv-1412-6597 | An Analysis of Unsupervised Pre-training in Light of Recent Advances | http://arxiv.org/abs/1412.6597 | id:1412.6597 author:Tom Le Paine, Pooya Khorrami, Wei Han, Thomas S. Huang category:cs.CV cs.LG cs.NE  published:2014-12-20 summary:Convolutional neural networks perform well on object recognition because of a number of recent advances: rectified linear units (ReLUs), data augmentation, dropout, and large labelled datasets. Unsupervised data has been proposed as another way to improve performance. Unfortunately, unsupervised pre-training is not used by state-of-the-art methods leading to the following question: Is unsupervised pre-training still useful given recent advances? If so, when? We answer this in three parts: we 1) develop an unsupervised method that incorporates ReLUs and recent unsupervised regularization techniques, 2) analyze the benefits of unsupervised pre-training compared to data augmentation and dropout on CIFAR-10 while varying the ratio of unsupervised to supervised samples, 3) verify our findings on STL-10. We discover unsupervised pre-training, as expected, helps when the ratio of unsupervised to supervised samples is high, and surprisingly, hurts when the ratio is low. We also use unsupervised pre-training with additional color augmentation to achieve near state-of-the-art performance on STL-10. version:4
arxiv-1406-3269 | Scheduled denoising autoencoders | http://arxiv.org/abs/1406.3269 | id:1406.3269 author:Krzysztof J. Geras, Charles Sutton category:cs.LG stat.ML  published:2014-06-12 summary:We present a representation learning method that learns features at multiple different levels of scale. Working within the unsupervised framework of denoising autoencoders, we observe that when the input is heavily corrupted during training, the network tends to learn coarse-grained features, whereas when the input is only slightly corrupted, the network tends to learn fine-grained features. This motivates the scheduled denoising autoencoder, which starts with a high level of noise that lowers as training progresses. We find that the resulting representation yields a significant boost on a later supervised task compared to the original input, or to a standard denoising autoencoder trained at a single noise level. After supervised fine-tuning our best model achieves the lowest ever reported error on the CIFAR-10 data set among permutation-invariant methods. version:3
arxiv-1504-02412 | Phase Transitions in Spectral Community Detection of Large Noisy Networks | http://arxiv.org/abs/1504.02412 | id:1504.02412 author:Pin-Yu Chen, Alfred O. Hero III category:cs.SI physics.data-an physics.soc-ph stat.ML  published:2015-04-09 summary:In this paper, we study the sensitivity of the spectral clustering based community detection algorithm subject to a Erdos-Renyi type random noise model. We prove phase transitions in community detectability as a function of the external edge connection probability and the noisy edge presence probability under a general network model where two arbitrarily connected communities are interconnected by random external edges. Specifically, the community detection performance transitions from almost perfect detectability to low detectability as the inter-community edge connection probability exceeds some critical value. We derive upper and lower bounds on the critical value and show that the bounds are identical when the two communities have the same size. The phase transition results are validated using network simulations. Using the derived expressions for the phase transition threshold we propose a method for estimating this threshold from observed data. version:2
arxiv-1412-7580 | Fast Convolutional Nets With fbfft: A GPU Performance Evaluation | http://arxiv.org/abs/1412.7580 | id:1412.7580 author:Nicolas Vasilache, Jeff Johnson, Michael Mathieu, Soumith Chintala, Serkan Piantino, Yann LeCun category:cs.LG cs.DC cs.NE  published:2014-12-24 summary:We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided. version:3
arxiv-1412-7479 | Deep Networks With Large Output Spaces | http://arxiv.org/abs/1412.7479 | id:1412.7479 author:Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, Jay Yagnik category:cs.NE cs.LG  published:2014-12-23 summary:Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods. version:4
arxiv-1504-02764 | A Coarse-to-Fine Model for 3D Pose Estimation and Sub-category Recognition | http://arxiv.org/abs/1504.02764 | id:1504.02764 author:Roozbeh Mottaghi, Yu Xiang, Silvio Savarese category:cs.CV  published:2015-04-10 summary:Despite the fact that object detection, 3D pose estimation, and sub-category recognition are highly correlated tasks, they are usually addressed independently from each other because of the huge space of parameters. To jointly model all of these tasks, we propose a coarse-to-fine hierarchical representation, where each level of the hierarchy represents objects at a different level of granularity. The hierarchical representation prevents performance loss, which is often caused by the increase in the number of parameters (as we consider more tasks to model), and the joint modelling enables resolving ambiguities that exist in independent modelling of these tasks. We augment PASCAL3D+ dataset with annotations for these tasks and show that our hierarchical model is effective in joint modelling of object detection, 3D pose estimation, and sub-category recognition. version:1
arxiv-1412-6564 | Move Evaluation in Go Using Deep Convolutional Neural Networks | http://arxiv.org/abs/1412.6564 | id:1412.6564 author:Chris J. Maddison, Aja Huang, Ilya Sutskever, David Silver category:cs.LG cs.NE  published:2014-12-20 summary:The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GnuGo in 97% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates a million positions per move. version:2
arxiv-1412-7004 | Tailoring Word Embeddings for Bilexical Predictions: An Experimental Comparison | http://arxiv.org/abs/1412.7004 | id:1412.7004 author:Pranava Swaroop Madhyastha, Xavier Carreras, Ariadna Quattoni category:cs.CL cs.LG  published:2014-12-22 summary:We investigate the problem of inducing word embeddings that are tailored for a particular bilexical relation. Our learning algorithm takes an existing lexical vector space and compresses it such that the resulting word embeddings are good predictors for a target bilexical relation. In experiments we show that task-specific embeddings can benefit both the quality and efficiency in lexical prediction tasks. version:2
arxiv-1406-2080 | Training Convolutional Networks with Noisy Labels | http://arxiv.org/abs/1406.2080 | id:1406.2080 author:Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, Rob Fergus category:cs.CV cs.LG cs.NE  published:2014-06-09 summary:The availability of large labeled datasets has allowed Convolutional Network models to achieve impressive recognition results. However, in many settings manual annotation of the data is impractical; instead our data has noisy labels, i.e. there is some freely available label for each image which may or may not be accurate. In this paper, we explore the performance of discriminatively-trained Convnets when trained on such noisy data. We introduce an extra noise layer into the network which adapts the network outputs to match the noisy label distribution. The parameters of this noise layer can be estimated as part of the training process and involve simple modifications to current training infrastructures for deep networks. We demonstrate the approaches on several datasets, including large scale experiments on the ImageNet classification benchmark. version:4
arxiv-1409-1556 | Very Deep Convolutional Networks for Large-Scale Image Recognition | http://arxiv.org/abs/1409.1556 | id:1409.1556 author:Karen Simonyan, Andrew Zisserman category:cs.CV  published:2014-09-04 summary:In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision. version:6
arxiv-1504-02719 | Diffusion Component Analysis: Unraveling Functional Topology in Biological Networks | http://arxiv.org/abs/1504.02719 | id:1504.02719 author:Hyunghoon Cho, Bonnie Berger, Jian Peng category:q-bio.MN cs.LG cs.SI stat.ML  published:2015-04-10 summary:Complex biological systems have been successfully modeled by biochemical and genetic interaction networks, typically gathered from high-throughput (HTP) data. These networks can be used to infer functional relationships between genes or proteins. Using the intuition that the topological role of a gene in a network relates to its biological function, local or diffusion based "guilt-by-association" and graph-theoretic methods have had success in inferring gene functions. Here we seek to improve function prediction by integrating diffusion-based methods with a novel dimensionality reduction technique to overcome the incomplete and noisy nature of network data. In this paper, we introduce diffusion component analysis (DCA), a framework that plugs in a diffusion model and learns a low-dimensional vector representation of each node to encode the topological properties of a network. As a proof of concept, we demonstrate DCA's substantial improvement over state-of-the-art diffusion-based approaches in predicting protein function from molecular interaction networks. Moreover, our DCA framework can integrate multiple networks from heterogeneous sources, consisting of genomic information, biochemical experiments and other resources, to even further improve function prediction. Yet another layer of performance gain is achieved by integrating the DCA framework with support vector machines that take our node vector representations as features. Overall, our DCA framework provides a novel representation of nodes in a network that can be used as a plug-in architecture to other machine learning algorithms to decipher topological properties of and obtain novel insights into interactomes. version:1
arxiv-1412-5903 | Deep Structured Output Learning for Unconstrained Text Recognition | http://arxiv.org/abs/1412.5903 | id:1412.5903 author:Max Jaderberg, Karen Simonyan, Andrea Vedaldi, Andrew Zisserman category:cs.CV  published:2014-12-18 summary:We develop a representation suitable for the unconstrained recognition of words in natural images: the general case of no fixed lexicon and unknown length. To this end we propose a convolutional neural network (CNN) based architecture which incorporates a Conditional Random Field (CRF) graphical model, taking the whole word image as a single input. The unaries of the CRF are provided by a CNN that predicts characters at each position of the output, while higher order terms are provided by another CNN that detects the presence of N-grams. We show that this entire model (CRF, character predictor, N-gram predictor) can be jointly optimised by back-propagating the structured output loss, essentially requiring the system to perform multi-task learning, and training uses purely synthetically generated data. The resulting model is a more accurate system on standard real-world text recognition benchmarks than character prediction alone, setting a benchmark for systems that have not been trained on a particular lexicon. In addition, our model achieves state-of-the-art accuracy in lexicon-constrained scenarios, without being specifically modelled for constrained recognition. To test the generalisation of our model, we also perform experiments with random alpha-numeric strings to evaluate the method when no visual language model is applicable. version:5
arxiv-1504-02712 | Gradient of Probability Density Functions based Contrasts for Blind Source Separation (BSS) | http://arxiv.org/abs/1504.02712 | id:1504.02712 author:Dharmani Bhaveshkumar C category:cs.LG cs.IT math.IT stat.ML 94A17  published:2015-04-10 summary:The article derives some novel independence measures and contrast functions for Blind Source Separation (BSS) application. For the $k^{th}$ order differentiable multivariate functions with equal hyper-volumes (region bounded by hyper-surfaces) and with a constraint of bounded support for $k>1$, it proves that equality of any $k^{th}$ order derivatives implies equality of the functions. The difference between product of marginal Probability Density Functions (PDFs) and joint PDF of a random vector is defined as Function Difference (FD) of a random vector. Assuming the PDFs are $k^{th}$ order differentiable, the results on generalized functions are applied to the independence condition. This brings new sets of independence measures and BSS contrasts based on the $L^p$-Norm, $ p \geq 1$ of - FD, gradient of FD (GFD) and Hessian of FD (HFD). Instead of a conventional two stage indirect estimation method for joint PDF based BSS contrast estimation, a single stage direct estimation of the contrasts is desired. The article targets both the efficient estimation of the proposed contrasts and extension of the potential theory for an information field. The potential theory has a concept of reference potential and it is used to derive closed form expression for the relative analysis of potential field. Analogous to it, there are introduced concepts of Reference Information Potential (RIP) and Cross Reference Information Potential (CRIP) based on the potential due to kernel functions placed at selected sample points as basis in kernel methods. The quantities are used to derive closed form expressions for information field analysis using least squares. The expressions are used to estimate $L^2$-Norm of FD and $L^2$-Norm of GFD based contrasts. version:1
arxiv-1412-6277 | N-gram-Based Low-Dimensional Representation for Document Classification | http://arxiv.org/abs/1412.6277 | id:1412.6277 author:Rémi Lebret, Ronan Collobert category:cs.CL  published:2014-12-19 summary:The bag-of-words (BOW) model is the common approach for classifying documents, where words are used as feature for training a classifier. This generally involves a huge number of features. Some techniques, such as Latent Semantic Analysis (LSA) or Latent Dirichlet Allocation (LDA), have been designed to summarize documents in a lower dimension with the least semantic information loss. Some semantic information is nevertheless always lost, since only words are considered. Instead, we aim at using information coming from n-grams to overcome this limitation, while remaining in a low-dimension space. Many approaches, such as the Skip-gram model, provide good word vector representations very quickly. We propose to average these representations to obtain representations of n-grams. All n-grams are thus embedded in a same semantic space. A K-means clustering can then group them into semantic concepts. The number of features is therefore dramatically reduced and documents can be represented as bag of semantic concepts. We show that this model outperforms LSA and LDA on a sentiment classification task, and yields similar results than a traditional BOW-model with far less features. version:2
arxiv-1409-4271 | The Ordered Weighted $\ell_1$ Norm: Atomic Formulation, Projections, and Algorithms | http://arxiv.org/abs/1409.4271 | id:1409.4271 author:Xiangrong Zeng, Mário A. T. Figueiredo category:cs.DS cs.CV cs.IT cs.LG math.IT  published:2014-09-15 summary:The ordered weighted $\ell_1$ norm (OWL) was recently proposed, with two different motivations: its good statistical properties as a sparsity promoting regularizer; the fact that it generalizes the so-called {\it octagonal shrinkage and clustering algorithm for regression} (OSCAR), which has the ability to cluster/group regression variables that are highly correlated. This paper contains several contributions to the study and application of OWL regularization: the derivation of the atomic formulation of the OWL norm; the derivation of the dual of the OWL norm, based on its atomic formulation; a new and simpler derivation of the proximity operator of the OWL norm; an efficient scheme to compute the Euclidean projection onto an OWL ball; the instantiation of the conditional gradient (CG, also known as Frank-Wolfe) algorithm for linear regression problems under OWL regularization; the instantiation of accelerated projected gradient algorithms for the same class of problems. Finally, a set of experiments give evidence that accelerated projected gradient algorithms are considerably faster than CG, for the class of problems considered. version:5
arxiv-1412-3717 | Unsupervised Neural Architecture for Saliency Detection: Extended Version | http://arxiv.org/abs/1412.3717 | id:1412.3717 author:Natalia Efremova, Sergey Tarasenko category:cs.CV cs.NE  published:2014-11-18 summary:We propose a novel neural network architecture for visual saliency detections, which utilizes neurophysiologically plausible mechanisms for extraction of salient regions. The model has been significantly inspired by recent findings from neurophysiology and aimed to simulate the bottom-up processes of human selective attention. Two types of features were analyzed: color and direction of maximum variance. The mechanism we employ for processing those features is PCA, implemented by means of normalized Hebbian learning and the waves of spikes. To evaluate performance of our model we have conducted psychological experiment. Comparison of simulation results with those of experiment indicates good performance of our model. version:2
arxiv-1410-8516 | NICE: Non-linear Independent Components Estimation | http://arxiv.org/abs/1410.8516 | id:1410.8516 author:Laurent Dinh, David Krueger, Yoshua Bengio category:cs.LG  published:2014-10-30 summary:We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting. version:6
arxiv-1411-3784 | Deep Narrow Boltzmann Machines are Universal Approximators | http://arxiv.org/abs/1411.3784 | id:1411.3784 author:Guido Montufar category:stat.ML cs.LG math.PR  published:2014-11-14 summary:We show that deep narrow Boltzmann machines are universal approximators of probability distributions on the activities of their visible units, provided they have sufficiently many hidden layers, each containing the same number of units as the visible layer. We show that, within certain parameter domains, deep Boltzmann machines can be studied as feedforward networks. We provide upper and lower bounds on the sufficient depth and width of universal approximators. These results settle various intuitions regarding undirected networks and, in particular, they show that deep narrow Boltzmann machines are at least as compact universal approximators as narrow sigmoid belief networks and restricted Boltzmann machines, with respect to the currently available bounds for those models. version:3
arxiv-1504-02622 | Maximum Entropy Linear Manifold for Learning Discriminative Low-dimensional Representation | http://arxiv.org/abs/1504.02622 | id:1504.02622 author:Wojciech Marian Czarnecki, Rafał Józefowicz, Jacek Tabor category:cs.LG  published:2015-04-10 summary:Representation learning is currently a very hot topic in modern machine learning, mostly due to the great success of the deep learning methods. In particular low-dimensional representation which discriminates classes can not only enhance the classification procedure, but also make it faster, while contrary to the high-dimensional embeddings can be efficiently used for visual based exploratory data analysis. In this paper we propose Maximum Entropy Linear Manifold (MELM), a multidimensional generalization of Multithreshold Entropy Linear Classifier model which is able to find a low-dimensional linear data projection maximizing discriminativeness of projected classes. As a result we obtain a linear embedding which can be used for classification, class aware dimensionality reduction and data visualization. MELM provides highly discriminative 2D projections of the data which can be used as a method for constructing robust classifiers. We provide both empirical evaluation as well as some interesting theoretical properties of our objective function such us scale and affine transformation invariance, connections with PCA and bounding of the expected balanced accuracy error. version:1
arxiv-1504-02590 | Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic Algorithm, Using Symmetric Travelling Salesman Problem | http://arxiv.org/abs/1504.02590 | id:1504.02590 author:Hassan Ismkhan, Kamran Zamanifar category:cs.NE  published:2015-04-10 summary:The Travelling Salesman Problem (TSP) is one of the most famous optimization problems. The Genetic Algorithm (GA) is one of metaheuristics that have been applied to TSP. The Crossover and mutation operators are two important elements of GA. There are many TSP solver crossover operators. In this paper, we state implementation of some recent TSP solver crossovers at first and then we use each of them in GA to solve some Symmetric TSP (STSP) instances and finally compare their effects on speed and accuracy of presented GA. version:1
arxiv-1503-08663 | Visual Saliency Based on Multiscale Deep Features | http://arxiv.org/abs/1503.08663 | id:1503.08663 author:Guanbin Li, Yizhou Yu category:cs.CV  published:2015-03-30 summary:Visual saliency is a fundamental problem in both cognitive and computational sciences, including computer vision. In this CVPR 2015 paper, we discover that a high-quality visual saliency model can be trained with multiscale features extracted using a popular deep learning architecture, convolutional neural networks (CNNs), which have had many successes in visual recognition tasks. For learning such saliency models, we introduce a neural network architecture, which has fully connected layers on top of CNNs responsible for extracting features at three different scales. We then propose a refinement method to enhance the spatial coherence of our saliency results. Finally, aggregating multiple saliency maps computed for different levels of image segmentation can further boost the performance, yielding saliency maps better than those generated from a single segmentation. To promote further research and evaluation of visual saliency models, we also construct a new large database of 4447 challenging images and their pixelwise saliency annotation. Experimental results demonstrate that our proposed method is capable of achieving state-of-the-art performance on all public benchmarks, improving the F-Measure by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset (HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively on these two datasets. version:3
arxiv-1412-7155 | Learning Compact Convolutional Neural Networks with Nested Dropout | http://arxiv.org/abs/1412.7155 | id:1412.7155 author:Chelsea Finn, Lisa Anne Hendricks, Trevor Darrell category:cs.CV cs.LG cs.NE  published:2014-12-22 summary:Recently, nested dropout was proposed as a method for ordering representation units in autoencoders by their information content, without diminishing reconstruction cost. However, it has only been applied to training fully-connected autoencoders in an unsupervised setting. We explore the impact of nested dropout on the convolutional layers in a CNN trained by backpropagation, investigating whether nested dropout can provide a simple and systematic way to determine the optimal representation size with respect to the desired accuracy and desired task and data complexity. version:4
arxiv-1503-08596 | Fast Optimal Transport Averaging of Neuroimaging Data | http://arxiv.org/abs/1503.08596 | id:1503.08596 author:Alexandre Gramfort, Gabriel Peyré, Marco Cuturi category:cs.CV  published:2015-03-30 summary:Knowing how the Human brain is anatomically and functionally organized at the level of a group of healthy individuals or patients is the primary goal of neuroimaging research. Yet computing an average of brain imaging data defined over a voxel grid or a triangulation remains a challenge. Data are large, the geometry of the brain is complex and the between subjects variability leads to spatially or temporally non-overlapping effects of interest. To address the problem of variability, data are commonly smoothed before group linear averaging. In this work we build on ideas originally introduced by Kantorovich to propose a new algorithm that can average efficiently non-normalized data defined over arbitrary discrete domains using transportation metrics. We show how Kantorovich means can be linked to Wasserstein barycenters in order to take advantage of an entropic smoothing approach. It leads to a smooth convex optimization problem and an algorithm with strong convergence guarantees. We illustrate the versatility of this tool and its empirical behavior on functional neuroimaging data, functional MRI and magnetoencephalography (MEG) source estimates, defined on voxel grids and triangulations of the folded cortical surface. version:2
arxiv-1504-02526 | Learning Arbitrary Statistical Mixtures of Discrete Distributions | http://arxiv.org/abs/1504.02526 | id:1504.02526 author:Jian Li, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy category:cs.LG cs.DS  published:2015-04-10 summary:We study the problem of learning from unlabeled samples very general statistical mixture models on large finite sets. Specifically, the model to be learned, $\vartheta$, is a probability distribution over probability distributions $p$, where each such $p$ is a probability distribution over $[n] = \{1,2,\dots,n\}$. When we sample from $\vartheta$, we do not observe $p$ directly, but only indirectly and in very noisy fashion, by sampling from $[n]$ repeatedly, independently $K$ times from the distribution $p$. The problem is to infer $\vartheta$ to high accuracy in transportation (earthmover) distance. We give the first efficient algorithms for learning this mixture model without making any restricting assumptions on the structure of the distribution $\vartheta$. We bound the quality of the solution as a function of the size of the samples $K$ and the number of samples used. Our model and results have applications to a variety of unsupervised learning scenarios, including learning topic models and collaborative filtering. version:1
arxiv-1504-02164 | Linearly Supporting Feature Extraction For Automated Estimation Of Stellar Atmospheric Parameters | http://arxiv.org/abs/1504.02164 | id:1504.02164 author:Xiangru Li, Yu Lu, Georges Comte, Ali Luo, Yongheng Zhao, Yongjun Wang category:astro-ph.SR astro-ph.IM cs.CV  published:2015-04-09 summary:We describe a scheme to extract linearly supporting (LSU) features from stellar spectra to automatically estimate the atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H]. "Linearly supporting" means that the atmospheric parameters can be accurately estimated from the extracted features through a linear model. The successive steps of the process are as follow: first, decompose the spectrum using a wavelet packet (WP) and represent it by the derived decomposition coefficients; second, detect representative spectral features from the decomposition coefficients using the proposed method Least Absolute Shrinkage and Selection Operator (LARS)$_{bs}$; third, estimate the atmospheric parameters $T_{eff}$, log$~g$, and [Fe/H] from the detected features using a linear regression method. One prominent characteristic of this scheme is its ability to evaluate quantitatively the contribution of each detected feature to the atmospheric parameter estimate and also to trace back the physical significance of that feature. This work also shows that the usefulness of a component depends on both wavelength and frequency. The proposed scheme has been evaluated on both real spectra from the Sloan Digital Sky Survey (SDSS)/SEGUE and synthetic spectra calculated from Kurucz's NEWODF models. On real spectra, we extracted 23 features to estimate $T_{eff}$, 62 features for log$~g$, and 68 features for [Fe/H]. Test consistencies between our estimates and those provided by the Spectroscopic Sarameter Pipeline of SDSS show that the mean absolute errors (MAEs) are 0.0062 dex for log$~T_{eff}$ (83 K for $T_{eff}$), 0.2345 dex for log$~g$, and 0.1564 dex for [Fe/H]. For the synthetic spectra, the MAE test accuracies are 0.0022 dex for log$~T_{eff}$ (32 K for $T_{eff}$), 0.0337 dex for log$~g$, and 0.0268 dex for [Fe/H]. version:2
arxiv-1502-02063 | Visual Recognition by Counting Instances: A Multi-Instance Cardinality Potential Kernel | http://arxiv.org/abs/1502.02063 | id:1502.02063 author:Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, Greg Mori category:cs.CV  published:2015-02-06 summary:Many visual recognition problems can be approached by counting instances. To determine whether an event is present in a long internet video, one could count how many frames seem to contain the activity. Classifying the activity of a group of people can be done by counting the actions of individual people. Encoding these cardinality relationships can reduce sensitivity to clutter, in the form of irrelevant frames or individuals not involved in a group activity. Learned parameters can encode how many instances tend to occur in a class of interest. To this end, this paper develops a powerful and flexible framework to infer any cardinality relation between latent labels in a multi-instance model. Hard or soft cardinality relations can be encoded to tackle diverse levels of ambiguity. Experiments on tasks such as human activity recognition, video event detection, and video summarization demonstrate the effectiveness of using cardinality relations for improving recognition results. version:2
arxiv-1412-5068 | Towards Deep Neural Network Architectures Robust to Adversarial Examples | http://arxiv.org/abs/1412.5068 | id:1412.5068 author:Shixiang Gu, Luca Rigazio category:cs.LG cs.CV cs.NE  published:2014-12-11 summary:Recent work has shown deep neural networks (DNNs) to be highly susceptible to well-designed, small perturbations at the input layer, or so-called adversarial examples. Taking images as an example, such distortions are often imperceptible, but can result in 100% mis-classification for a state of the art DNN. We study the structure of adversarial examples and explore network topology, pre-processing and training strategies to improve the robustness of DNNs. We perform various experiments to assess the removability of adversarial examples by corrupting with additional noise and pre-processing with denoising autoencoders (DAEs). We find that DAEs can remove substantial amounts of the adversarial noise. How- ever, when stacking the DAE with the original DNN, the resulting network can again be attacked by new adversarial examples with even smaller distortion. As a solution, we propose Deep Contractive Network, a model with a new end-to-end training procedure that includes a smoothness penalty inspired by the contractive autoencoder (CAE). This increases the network robustness to adversarial examples, without a significant performance penalty. version:4
arxiv-1410-3916 | Memory Networks | http://arxiv.org/abs/1410.3916 | id:1410.3916 author:Jason Weston, Sumit Chopra, Antoine Bordes category:cs.AI cs.CL stat.ML  published:2014-10-15 summary:We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs. version:11
arxiv-1504-02490 | Leveraging Twitter for Low-Resource Conversational Speech Language Modeling | http://arxiv.org/abs/1504.02490 | id:1504.02490 author:Aaron Jaech, Mari Ostendorf category:cs.CL  published:2015-04-09 summary:In applications involving conversational speech, data sparsity is a limiting factor in building a better language model. We propose a simple, language-independent method to quickly harvest large amounts of data from Twitter to supplement a smaller training set that is more closely matched to the domain. The techniques lead to a significant reduction in perplexity on four low-resource languages even though the presence on Twitter of these languages is relatively small. We also find that the Twitter text is more useful for learning word classes than the in-domain text and that use of these word classes leads to further reductions in perplexity. Additionally, we introduce a method of using social and textual information to prioritize the download queue during the Twitter crawling. This maximizes the amount of useful data that can be collected, impacting both perplexity and vocabulary coverage. version:1
arxiv-1503-06699 | Video-Based Action Recognition Using Rate-Invariant Analysis of Covariance Trajectories | http://arxiv.org/abs/1503.06699 | id:1503.06699 author:Zhengwu Zhang, Jingyong Su, Eric Klassen, Huiling Le, Anuj Srivastava category:cs.CV  published:2015-03-23 summary:Statistical classification of actions in videos is mostly performed by extracting relevant features, particularly covariance features, from image frames and studying time series associated with temporal evolutions of these features. A natural mathematical representation of activity videos is in form of parameterized trajectories on the covariance manifold, i.e. the set of symmetric, positive-definite matrices (SPDMs). The variable execution-rates of actions implies variable parameterizations of the resulting trajectories, and complicates their classification. Since action classes are invariant to execution rates, one requires rate-invariant metrics for comparing trajectories. A recent paper represented trajectories using their transported square-root vector fields (TSRVFs), defined by parallel translating scaled-velocity vectors of trajectories to a reference tangent space on the manifold. To avoid arbitrariness of selecting the reference and to reduce distortion introduced during this mapping, we develop a purely intrinsic approach where SPDM trajectories are represented by redefining their TSRVFs at the starting points of the trajectories, and analyzed as elements of a vector bundle on the manifold. Using a natural Riemannain metric on vector bundles of SPDMs, we compute geodesic paths and geodesic distances between trajectories in the quotient space of this vector bundle, with respect to the re-parameterization group. This makes the resulting comparison of trajectories invariant to their re-parameterization. We demonstrate this framework on two applications involving video classification: visual speech recognition or lip-reading and hand-gesture recognition. In both cases we achieve results either comparable to or better than the current literature. version:2
arxiv-1504-02485 | What Do Deep CNNs Learn About Objects? | http://arxiv.org/abs/1504.02485 | id:1504.02485 author:Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko category:cs.CV  published:2015-04-09 summary:Deep convolutional neural networks learn extremely powerful image representations, yet most of that power is hidden in the millions of deep-layer parameters. What exactly do these parameters represent? Recent work has started to analyse CNN representations, finding that, e.g., they are invariant to some 2D transformations Fischer et al. (2014), but are confused by particular types of image noise Nguyen et al. (2014). In this work, we delve deeper and ask: how invariant are CNNs to object-class variations caused by 3D shape, pose, and photorealism? version:1
arxiv-1504-02437 | Predicting Complete 3D Models of Indoor Scenes | http://arxiv.org/abs/1504.02437 | id:1504.02437 author:Ruiqi Guo, Chuhang Zou, Derek Hoiem category:cs.CV  published:2015-04-09 summary:One major goal of vision is to infer physical models of objects, surfaces, and their layout from sensors. In this paper, we aim to interpret indoor scenes from one RGBD image. Our representation encodes the layout of walls, which must conform to a Manhattan structure but is otherwise flexible, and the layout and extent of objects, modeled with CAD-like 3D shapes. We represent both the visible and occluded portions of the scene, producing a $complete$ 3D parse. Such a scene interpretation is useful for robotics and visual reasoning, but difficult to produce due to the well-known challenge of segmentation, the high degree of occlusion, and the diversity of objects in indoor scene. We take a data-driven approach, generating sets of potential object regions, matching to regions in training images, and transferring and aligning associated 3D models while encouraging fit to observations and overall consistency. We demonstrate encouraging results on the NYU v2 dataset and highlight a variety of interesting directions for future work. version:1
arxiv-1502-01423 | Collaborative Feature Learning from Social Media | http://arxiv.org/abs/1502.01423 | id:1502.01423 author:Chen Fang, Hailin Jin, Jianchao Yang, Zhe Lin category:cs.CV  published:2015-02-05 summary:Image feature representation plays an essential role in image recognition and related tasks. The current state-of-the-art feature learning paradigm is supervised learning from labeled data. However, this paradigm requires large-scale category labels, which limits its applicability to domains where labels are hard to obtain. In this paper, we propose a new data-driven feature learning paradigm which does not rely on category labels. Instead, we learn from user behavior data collected on social media. Concretely, we use the image relationship discovered in the latent space from the user behavior data to guide the image feature learning. We collect a large-scale image and user behavior dataset from Behance.net. The dataset consists of 1.9 million images and over 300 million view records from 1.9 million users. We validate our feature learning paradigm on this dataset and find that the learned feature significantly outperforms the state-of-the-art image features in learning better image similarities. We also show that the learned feature performs competitively on various recognition benchmarks. version:3
arxiv-1504-02406 | Deciding when to stop: Efficient stopping of active learning guided drug-target prediction | http://arxiv.org/abs/1504.02406 | id:1504.02406 author:Maja Temerinac-Ott, Armaghan W. Naik, Robert F. Murphy category:q-bio.QM cs.LG stat.ML  published:2015-04-09 summary:Active learning has shown to reduce the number of experiments needed to obtain high-confidence drug-target predictions. However, in order to actually save experiments using active learning, it is crucial to have a method to evaluate the quality of the current prediction and decide when to stop the experimentation process. Only by applying reliable stoping criteria to active learning, time and costs in the experimental process can be actually saved. We compute active learning traces on simulated drug-target matrices in order to learn a regression model for the accuracy of the active learner. By analyzing the performance of the regression model on simulated data, we design stopping criteria for previously unseen experimental matrices. We demonstrate on four previously characterized drug effect data sets that applying the stopping criteria can result in upto 40% savings of the total experiments for highly accurate predictions. version:1
arxiv-1504-02398 | Real-time Monocular Object SLAM | http://arxiv.org/abs/1504.02398 | id:1504.02398 author:Dorian Gálvez-López, Marta Salas, Juan D. Tardós, J. M. M. Montiel category:cs.RO cs.CV  published:2015-04-09 summary:We present a real-time object-based SLAM system that leverages the largest object database to date. Our approach comprises two main components: 1) a monocular SLAM algorithm that exploits object rigidity constraints to improve the map and find its real scale, and 2) a novel object recognition algorithm based on bags of binary words, which provides live detections with a database of 500 3D objects. The two components work together and benefit each other: the SLAM algorithm accumulates information from the observations of the objects, anchors object features to especial map landmarks and sets constrains on the optimization. At the same time, objects partially or fully located within the map are used as a prior to guide the recognition algorithm, achieving higher recall. We evaluate our proposal on five real environments showing improvements on the accuracy of the map and efficiency with respect to other state-of-the-art techniques. version:1
arxiv-1504-02366 | A Collection of Challenging Optimization Problems in Science, Engineering and Economics | http://arxiv.org/abs/1504.02366 | id:1504.02366 author:Dhagash Mehta, Crina Grosan category:cs.NA cs.MS cs.NE math.AG math.NA math.OC physics.comp-ph  published:2015-04-09 summary:Function optimization and finding simultaneous solutions of a system of nonlinear equations (SNE) are two closely related and important optimization problems. However, unlike in the case of function optimization in which one is required to find the global minimum and sometimes local minima, a database of challenging SNEs where one is required to find stationary points (extrama and saddle points) is not readily available. In this article, we initiate building such a database of important SNE (which also includes related function optimization problems), arising from Science, Engineering and Economics. After providing a short review of the most commonly used mathematical and computational approaches to find solutions of such systems, we provide a preliminary list of challenging problems by writing the Mathematical formulation down, briefly explaning the origin and importance of the problem and giving a short account on the currently known results, for each of the problems. We anticipate that this database will not only help benchmarking novel numerical methods for solving SNEs and function optimization problems but also will help advancing the corresponding research areas. version:1
arxiv-1504-02356 | Exploring EEG for Object Detection and Retrieval | http://arxiv.org/abs/1504.02356 | id:1504.02356 author:Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Giró-i-Nieto, Graham Healy, Kevin McGuinness, Noel O'Connor, Alan F. Smeaton category:cs.HC cs.CV cs.IR H.1.2; H.3.3  published:2015-04-09 summary:This paper explores the potential for using Brain Computer Interfaces (BCI) as a relevance feedback mechanism in content-based image retrieval. We investigate if it is possible to capture useful EEG signals to detect if relevant objects are present in a dataset of realistic and complex images. We perform several experiments using a rapid serial visual presentation (RSVP) of images at different rates (5Hz and 10Hz) on 8 users with different degrees of familiarization with BCI and the dataset. We then use the feedback from the BCI and mouse-based interfaces to retrieve localized objects in a subset of TRECVid images. We show that it is indeed possible to detect such objects in complex images and, also, that users with previous knowledge on the dataset or experience with the RSVP outperform others. When the users have limited time to annotate the images (100 seconds in our experiments) both interfaces are comparable in performance. Comparing our best users in a retrieval task, we found that EEG-based relevance feedback outperforms mouse-based feedback. The realistic and complex image dataset differentiates our work from previous studies on EEG for image retrieval. version:1
arxiv-1504-02351 | When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition | http://arxiv.org/abs/1504.02351 | id:1504.02351 author:Guosheng Hu, Yongxin Yang, Dong Yi, Josef Kittler, William Christmas, Stan Z. Li, Timothy Hospedales category:cs.CV cs.LG cs.NE  published:2015-04-09 summary:Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available. version:1
arxiv-1412-6296 | Generative Modeling of Convolutional Neural Networks | http://arxiv.org/abs/1412.6296 | id:1412.6296 author:Jifeng Dai, Yang Lu, Ying-Nian Wu category:cs.CV cs.LG cs.NE  published:2014-12-19 summary:The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs. The main contributions include: (1) We construct a generative model for the CNN in the form of exponential tilting of a reference distribution. (2) We propose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used discriminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sampling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the challenging ImageNet benchmark show that the proposed generative gradient pre-training consistently helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and varied samples of synthetic images from a large-scale deep CNN. version:2
arxiv-1504-02340 | Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor | http://arxiv.org/abs/1504.02340 | id:1504.02340 author:Wongun Choi category:cs.CV  published:2015-04-09 summary:In this paper, we focus on the two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI and MOT datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-arts. version:1
arxiv-1406-2989 | Techniques for Learning Binary Stochastic Feedforward Neural Networks | http://arxiv.org/abs/1406.2989 | id:1406.2989 author:Tapani Raiko, Mathias Berglund, Guillaume Alain, Laurent Dinh category:stat.ML cs.LG cs.NE  published:2014-06-11 summary:Stochastic binary hidden units in a multi-layer perceptron (MLP) network give at least three potential benefits when compared to deterministic MLP networks. (1) They allow to learn one-to-many type of mappings. (2) They can be used in structured prediction problems, where modeling the internal structure of the output is important. (3) Stochasticity has been shown to be an excellent regularizer, which makes generalization performance potentially better in general. However, training stochastic networks is considerably more difficult. We study training using M samples of hidden activations per input. We show that the case M=1 leads to a fundamentally different behavior where the network tries to avoid stochasticity. We propose two new estimators for the training gradient and propose benchmark tests for comparing training algorithms. Our experiments confirm that training stochastic networks is difficult and show that the proposed two estimators perform favorably among all the five known estimators. version:3
arxiv-1504-02247 | Projective simulation with generalization | http://arxiv.org/abs/1504.02247 | id:1504.02247 author:Alexey A. Melnikov, Adi Makmal, Vedran Dunjko, Hans J. Briegel category:cs.AI cs.LG stat.ML  published:2015-04-09 summary:The ability to generalize is an important feature of any intelligent agent. Not only because it may allow the agent to cope with large amounts of data, but also because in some environments, an agent with no generalization ability is simply doomed to fail. In this work we outline several criteria for generalization, and present a dynamic and autonomous machinery that enables projective simulation agents to meaningfully generalize. Projective simulation, a novel, physical, approach to artificial intelligence, was recently shown to perform well, in comparison with standard models, on both simple reinforcement learning problems, as well as on more complicated canonical tasks, such as the "grid world" and the "mountain car problem". Both the basic projective simulation model and the presented generalization machinery are based on very simple principles. This simplicity allows us to provide a full analytical analysis of the agent's performance and to illustrate the benefit the agent gains by generalizing. Specifically, we show how such an ability allows the agent to learn in rather extreme environments, in which learning is otherwise impossible. version:1
arxiv-1504-02235 | Extraction of Protein Sequence Motif Information using PSO K-Means | http://arxiv.org/abs/1504.02235 | id:1504.02235 author:R. Gowri, R. Rathipriya category:cs.CV  published:2015-04-09 summary:The main objective of the paper is to find the motif information.The functionalities of the proteins are ideally found from their motif information which is extracted using various techniques like clustering with k-means, hybrid k-means, self-organising maps, etc., in the literature. In this work protein sequence information is extracted using optimised k-means algorithm. The particle swarm optimisation technique is one of the frequently used optimisation method. In the current work the PSO k-means is used for motif information extraction. This paper also deals with the comparison between the motif information obtained from clusters and biclustersusing PSO k-means algorithm. The motif information acquired is based on the structure homogeneity of the protein sequence. version:1
arxiv-1502-03671 | Phrase-based Image Captioning | http://arxiv.org/abs/1502.03671 | id:1502.03671 author:Rémi Lebret, Pedro O. Pinheiro, Ronan Collobert category:cs.CL  published:2015-02-12 summary:Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO. version:2
arxiv-1411-1045 | Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet | http://arxiv.org/abs/1411.1045 | id:1411.1045 author:Matthias Kümmerer, Lucas Theis, Matthias Bethge category:cs.CV q-bio.NC stat.AP  published:2014-11-04 summary:Recent results suggest that state-of-the-art saliency models perform far from optimal in predicting fixations. This lack in performance has been attributed to an inability to model the influence of high-level image features such as objects. Recent seminal advances in applying deep neural networks to tasks like object recognition suggests that they are able to capture this kind of structure. However, the enormous amount of training data necessary to train these networks makes them difficult to apply directly to saliency prediction. We present a novel way of reusing existing neural networks that have been pretrained on the task of object recognition in models of fixation prediction. Using the well-known network of Krizhevsky et al. (2012), we come up with a new saliency model that significantly outperforms all state-of-the-art models on the MIT Saliency Benchmark. We show that the structure of this network allows new insights in the psychophysics of fixation selection and potentially their neural implementation. To train our network, we build on recent work on the modeling of saliency as point processes. version:4
arxiv-1410-4871 | Bayesian estimation of the multifractality parameter for image texture using a Whittle approximation | http://arxiv.org/abs/1410.4871 | id:1410.4871 author:Sébastien Combrexelle, Herwig Wendt, Nicolas Dobigeon, Jean-Yves Tourneret, Steve McLaughlin, Patrice Abry category:physics.data-an cs.CV stat.ME  published:2014-10-17 summary:Texture characterization is a central element in many image processing applications. Multifractal analysis is a useful signal and image processing tool, yet, the accurate estimation of multifractal parameters for image texture remains a challenge. This is due in the main to the fact that current estimation procedures consist of performing linear regressions across frequency scales of the two-dimensional (2D) dyadic wavelet transform, for which only a few such scales are computable for images. The strongly non-Gaussian nature of multifractal processes, combined with their complicated dependence structure, makes it difficult to develop suitable models for parameter estimation. Here, we propose a Bayesian procedure that addresses the difficulties in the estimation of the multifractality parameter. The originality of the procedure is threefold: The construction of a generic semi-parametric statistical model for the logarithm of wavelet leaders; the formulation of Bayesian estimators that are associated with this model and the set of parameter values admitted by multifractal theory; the exploitation of a suitable Whittle approximation within the Bayesian model which enables the otherwise infeasible evaluation of the posterior distribution associated with the model. Performance is assessed numerically for several 2D multifractal processes, for several image sizes and a large range of process parameters. The procedure yields significant benefits over current benchmark estimators in terms of estimation performance and ability to discriminate between the two most commonly used classes of multifractal process models. The gains in performance are particularly pronounced for small image sizes, notably enabling for the first time the analysis of image patches as small as 64x64 pixels. version:2
arxiv-1312-3790 | Sample Complexity of Dictionary Learning and other Matrix Factorizations | http://arxiv.org/abs/1312.3790 | id:1312.3790 author:Rémi Gribonval, Rodolphe Jenatton, Francis Bach, Martin Kleinsteuber, Matthias Seibert category:stat.ML cs.IT math.IT  published:2013-12-13 summary:Many modern tools in machine learning and signal processing, such as sparse dictionary learning, principal component analysis (PCA), non-negative matrix factorization (NMF), $K$-means clustering, etc., rely on the factorization of a matrix obtained by concatenating high-dimensional vectors from a training collection. While the idealized task would be to optimize the expected quality of the factors over the underlying distribution of training vectors, it is achieved in practice by minimizing an empirical average over the considered collection. The focus of this paper is to provide sample complexity estimates to uniformly control how much the empirical average deviates from the expected cost function. Standard arguments imply that the performance of the empirical predictor also exhibit such guarantees. The level of genericity of the approach encompasses several possible constraints on the factors (tensor product structure, shift-invariance, sparsity \ldots), thus providing a unified perspective on the sample complexity of several widely used matrix factorization schemes. The derived generalization bounds behave proportional to $\sqrt{\log(n)/n}$ w.r.t.\ the number of samples $n$ for the considered matrix factorization techniques. version:3
arxiv-1504-02191 | `local' vs. `global' parameters -- breaking the gaussian complexity barrier | http://arxiv.org/abs/1504.02191 | id:1504.02191 author:Shahar Mendelson category:stat.ML math.ST stat.TH  published:2015-04-09 summary:We show that if $F$ is a convex class of functions that is $L$-subgaussian, the error rate of learning problems generated by independent noise is equivalent to a fixed point determined by `local' covering estimates of the class, rather than by the gaussian averages. To that end, we establish new sharp upper and lower estimates on the error rate for such problems. version:1
arxiv-1411-6382 | Mid-level Deep Pattern Mining | http://arxiv.org/abs/1411.6382 | id:1411.6382 author:Yao Li, Lingqiao Liu, Chunhua Shen, Anton van den Hengel category:cs.CV  published:2014-11-24 summary:Mid-level visual element discovery aims to find clusters of image patches that are both representative and discriminative. In this work, we study this problem from the prospective of pattern mining while relying on the recently popularized Convolutional Neural Networks (CNNs). Specifically, we find that for an image patch, activations extracted from the first fully-connected layer of CNNs have two appealing properties which enable its seamless integration with pattern mining. Patterns are then discovered from a large number of CNN activations of image patches through the well-known association rule mining. When we retrieve and visualize image patches with the same pattern, surprisingly, they are not only visually similar but also semantically consistent. We apply our approach to scene and object classification tasks, and demonstrate that our approach outperforms all previous works on mid-level visual element discovery by a sizeable margin with far fewer elements being used. Our approach also outperforms or matches recent works using CNN for these tasks. Source code of the complete system is available online. version:3
arxiv-1412-7009 | Generative Class-conditional Autoencoders | http://arxiv.org/abs/1412.7009 | id:1412.7009 author:Jan Rudy, Graham Taylor category:cs.NE cs.LG  published:2014-12-22 summary:Recent work by Bengio et al. (2013) proposes a sampling procedure for denoising autoencoders which involves learning the transition operator of a Markov chain. The transition operator is typically unimodal, which limits its capacity to model complex data. In order to perform efficient sampling from conditional distributions, we extend this work, both theoretically and algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is able to generate convincing class-conditional samples when trained on both the MNIST and TFD datasets. version:3
arxiv-1410-1165 | Understanding Locally Competitive Networks | http://arxiv.org/abs/1410.1165 | id:1410.1165 author:Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez, Jürgen Schmidhuber category:cs.NE cs.LG 68T30  68T10 I.2.6  published:2014-10-05 summary:Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of computational units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks. version:3
arxiv-1504-02150 | Exploring Lexical, Syntactic, and Semantic Features for Chinese Textual Entailment in NTCIR RITE Evaluation Tasks | http://arxiv.org/abs/1504.02150 | id:1504.02150 author:Wei-Jie Huang, Chao-Lin Liu category:cs.CL cs.AI cs.DL I.2.7  published:2015-04-08 summary:We computed linguistic information at the lexical, syntactic, and semantic levels for Recognizing Inference in Text (RITE) tasks for both traditional and simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing, named-entity recognition, and near synonym recognition were employed, and features like counts of common words, statement lengths, negation words, and antonyms were considered to judge the entailment relationships of two statements, while we explored both heuristics-based functions and machine-learning approaches. The reported systems showed robustness by simultaneously achieving second positions in the binary-classification subtasks for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted more experiments with the test data of NTCIR-9 RITE, with good results. We also extended our work to search for better configurations of our classifiers and investigated contributions of individual features. This extended work showed interesting results and should encourage further discussion. version:1
arxiv-1504-02148 | Mining and discovering biographical information in Difangzhi with a language-model-based approach | http://arxiv.org/abs/1504.02148 | id:1504.02148 author:Peter K. Bol, Chao-Lin Liu, Hongsu Wang category:cs.CL cs.CY cs.DL I.2.7  published:2015-04-08 summary:We present results of expanding the contents of the China Biographical Database by text mining historical local gazetteers, difangzhi. The goal of the database is to see how people are connected together, through kinship, social connections, and the places and offices in which they served. The gazetteers are the single most important collection of names and offices covering the Song through Qing periods. Although we begin with local officials we shall eventually include lists of local examination candidates, people from the locality who served in government, and notable local figures with biographies. The more data we collect the more connections emerge. The value of doing systematic text mining work is that we can identify relevant connections that are either directly informative or can become useful without deep historical research. Academia Sinica is developing a name database for officials in the central governments of the Ming and Qing dynasties. version:1
arxiv-1504-02147 | Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction | http://arxiv.org/abs/1504.02147 | id:1504.02147 author:Tom Goldstein, Gavin Taylor, Kawika Barabin, Kent Sayre category:cs.DC cs.LG  published:2015-04-08 summary:Recent approaches to distributed model fitting rely heavily on consensus ADMM, where each node solves small sub-problems using only local data. We propose iterative methods that solve {\em global} sub-problems over an entire distributed dataset. This is possible using transpose reduction strategies that allow a single node to solve least-squares over massive datasets without putting all the data in one place. This results in simple iterative methods that avoid the expensive inner loops required for consensus methods. To demonstrate the efficiency of this approach, we fit linear classifiers and sparse linear models to datasets over 5 Tb in size using a distributed implementation with over 7000 cores in far less time than previous approaches. version:1
arxiv-1412-3708 | Compact Part-Based Image Representations | http://arxiv.org/abs/1412.3708 | id:1412.3708 author:Marc Goessling, Yali Amit category:cs.CV cs.LG stat.ML  published:2014-12-11 summary:Learning compact, interpretable image representations is a very natural task which has not been solved satisfactorily even for simple classes of binary images. In this paper, we review various ways of composing parts (or experts) for binary data and argue that competitive forms of interaction are best suited to learn low-dimensional representations. We propose a new composition rule which discourages parts from focusing on similar structures and which penalizes opposing votes strongly so that abstaining from voting becomes more attractive. We also introduce a novel sequential initialization procedure based on a process of oversimplification and correction. Experiments show that with our approach very intuitive models can be learned. version:3
arxiv-1504-02125 | Residential Demand Response Applications Using Batch Reinforcement Learning | http://arxiv.org/abs/1504.02125 | id:1504.02125 author:Frederik Ruelens, Bert Claessens, Stijn Vandael, Bart De Schutter, Robert Babuska, Ronnie Belmans category:cs.SY cs.LG  published:2015-04-08 summary:Driven by recent advances in batch Reinforcement Learning (RL), this paper contributes to the application of batch RL to demand response. In contrast to conventional model-based approaches, batch RL techniques do not require a system identification step, which makes them more suitable for a large-scale implementation. This paper extends fitted Q-iteration, a standard batch RL technique, to the situation where a forecast of the exogenous data is provided. In general, batch RL techniques do not rely on expert knowledge on the system dynamics or the solution. However, if some expert knowledge is provided, it can be incorporated by using our novel policy adjustment method. Finally, we tackle the challenge of finding an open-loop schedule required to participate in the day-ahead market. We propose a model-free Monte-Carlo estimator method that uses a metric to construct artificial trajectories and we illustrate this method by finding the day-ahead schedule of a heat-pump thermostat. Our experiments show that batch RL techniques provide a valuable alternative to model-based controllers and that they can be used to construct both closed-loop and open-loop policies. version:1
arxiv-1412-4930 | Rehabilitation of Count-based Models for Word Vector Representations | http://arxiv.org/abs/1412.4930 | id:1412.4930 author:Rémi Lebret, Ronan Collobert category:cs.CL  published:2014-12-16 summary:Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarties as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words. version:2
arxiv-1504-02059 | Supporting Language Learners with the Meanings Of Closed Class Items | http://arxiv.org/abs/1504.02059 | id:1504.02059 author:Hayat Alrefaie, Allan Ramsay category:cs.AI cs.CL  published:2015-04-08 summary:The process of language learning involves the mastery of countless tasks: making the constituent sounds of the language being learned, learning the grammatical patterns, and acquiring the requisite vocabulary for reception and production. While a plethora of computational tools exist to facilitate the first and second of these tasks, a number of challenges arise with respect to enabling the third. This paper describes a tool that has been designed to support language learners with the challenge of understanding the use of closed-class lexical items. The process of learning the Arabic for office is (mktb) is relatively simple and should be possible by means of simple repetition of the word. However, it is much more difficult to learn and correctly use the Arabic equivalent of the word on. The current paper describes a mechanism for the delivery of diagnostic information regarding specific lexical examples, with the aim of clearly demonstrating why a particular translation of a given closed-class item may be appropriate in certain situations but not others, thereby helping learners to understand and use the term correctly. version:1
arxiv-1504-02010 | A Chaotic Dynamical System that Paints | http://arxiv.org/abs/1504.02010 | id:1504.02010 author:Tuhin Sahai, George Mathew, Amit Surana category:nlin.CD cs.LG  published:2015-04-08 summary:Can a dynamical system paint masterpieces such as Da Vinci's Mona Lisa or Monet's Water Lilies? Moreover, can this dynamical system be chaotic in the sense that although the trajectories are sensitive to initial conditions, the same painting is created every time? Setting aside the creative aspect of painting a picture, in this work, we develop a novel algorithm to reproduce paintings and photographs. Combining ideas from ergodic theory and control theory, we construct a chaotic dynamical system with predetermined statistical properties. If one makes the spatial distribution of colors in the picture the target distribution, akin to a human, the algorithm first captures large scale features and then goes on to refine small scale features. Beyond reproducing paintings, this approach is expected to have a wide variety of applications such as uncertainty quantification, sampling for efficient inference in scalable machine learning for big data, and developing effective strategies for search and rescue. In particular, our preliminary studies demonstrate that this algorithm provides significant acceleration and higher accuracy than competing methods for Markov Chain Monte Carlo (MCMC). version:1
arxiv-1402-3337 | Zero-bias autoencoders and the benefits of co-adapting features | http://arxiv.org/abs/1402.3337 | id:1402.3337 author:Kishore Konda, Roland Memisevic, David Krueger category:stat.ML cs.CV cs.LG cs.NE  published:2014-02-13 summary:Regularized training of an autoencoder typically results in hidden unit biases that take on large negative values. We show that negative biases are a natural result of using a hidden layer whose responsibility is to both represent the input data and act as a selection mechanism that ensures sparsity of the representation. We then show that negative biases impede the learning of data distributions whose intrinsic dimensionality is high. We also propose a new activation function that decouples the two roles of the hidden layer and that allows us to learn representations on data with very high intrinsic dimensionality, where standard autoencoders typically fail. Since the decoupled activation function acts like an implicit regularizer, the model can be trained by minimizing the reconstruction error of training data, without requiring any additional regularization. version:5
arxiv-1504-01989 | Pixel-wise Deep Learning for Contour Detection | http://arxiv.org/abs/1504.01989 | id:1504.01989 author:Jyh-Jing Hwang, Tyng-Luh Liu category:cs.CV cs.LG cs.NE  published:2015-04-08 summary:We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and verify their performance on BSDS500. version:1
arxiv-1504-01982 | Decoupled Adapt-then-Combine diffusion networks with adaptive combiners | http://arxiv.org/abs/1504.01982 | id:1504.01982 author:Jesus Fernandez-Bes, Jerónimo Arenas-García, Magno T. M. Silva, Luis A. Azpicueta-Ruiz category:cs.SY cs.LG  published:2015-04-08 summary:In this paper we analyze a novel diffusion strategy for adaptive networks called Decoupled Adapt-then-Combine, which keeps a fully local estimate of the solution for the adaptation step. Our strategy, which is specially convenient for heterogeneous networks, is compared with the standard Adapt-then-Combine scheme and theoretically analyzed using energy conservation arguments. Such comparison shows the need of implementing adaptive combiners for both schemes to obtain a good performance in case of heterogeneous networks. Therefore, we propose two adaptive rules to learn the combination coefficients that are useful for our diffusion strategy. Several experiments simulating both stationary estimation and tracking problems show that our method outperforms state-of-the-art techniques, becoming a competitive approach in different scenarios. version:1
arxiv-1312-2988 | Protein Contact Prediction by Integrating Joint Evolutionary Coupling Analysis and Supervised Learning | http://arxiv.org/abs/1312.2988 | id:1312.2988 author:Jianzhu Ma, Sheng Wang, Zhiyong Wang, Jinbo Xu category:q-bio.QM cs.LG math.OC q-bio.BM stat.ML  published:2013-12-10 summary:Protein contacts contain important information for protein structure and functional study, but contact prediction from sequence remains very challenging. Both evolutionary coupling (EC) analysis and supervised machine learning methods are developed to predict contacts, making use of different types of information, respectively. This paper presents a group graphical lasso (GGL) method for contact prediction that integrates joint multi-family EC analysis and supervised learning. Different from existing single-family EC analysis that uses residue co-evolution information in only the target protein family, our joint EC analysis uses residue co-evolution in both the target family and its related families, which may have divergent sequences but similar folds. To implement joint EC analysis, we model a set of related protein families using Gaussian graphical models (GGM) and then co-estimate their precision matrices by maximum-likelihood, subject to the constraint that the precision matrices shall share similar residue co-evolution patterns. To further improve the accuracy of the estimated precision matrices, we employ a supervised learning method to predict contact probability from a variety of evolutionary and non-evolutionary information and then incorporate the predicted probability as prior into our GGL framework. Experiments show that our method can predict contacts much more accurately than existing methods, and that our method performs better on both conserved and family-specific contacts. version:5
arxiv-1504-01954 | Image Subset Selection Using Gabor Filters and Neural Networks | http://arxiv.org/abs/1504.01954 | id:1504.01954 author:Heider K. Ali, Anthony Whitehead category:cs.CV  published:2015-04-08 summary:An automatic method for the selection of subsets of images, both modern and historic, out of a set of landmark large images collected from the Internet is presented in this paper. This selection depends on the extraction of dominant features using Gabor filtering. Features are selected carefully from a preliminary image set and fed into a neural network as a training data. The method collects a large set of raw landmark images containing modern and historic landmark images and non-landmark images. The method then processes these images to classify them as landmark and non-landmark images. The classification performance highly depends on the number of candidate features of the landmark. version:1
arxiv-1504-01942 | MOTChallenge 2015: Towards a Benchmark for Multi-Target Tracking | http://arxiv.org/abs/1504.01942 | id:1504.01942 author:Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, Konrad Schindler category:cs.CV  published:2015-04-08 summary:In the recent past, the computer vision community has developed centralized benchmarks for the performance evaluation of a variety of tasks, including generic object and pedestrian detection, 3D reconstruction, optical flow, single-object short-term tracking, and stereo estimation. Despite potential pitfalls of such benchmarks, they have proved to be extremely helpful to advance the state of the art in the respective area. Interestingly, there has been rather limited work on the standardization of quantitative benchmarks for multiple target tracking. One of the few exceptions is the well-known PETS dataset, targeted primarily at surveillance applications. Despite being widely used, it is often applied inconsistently, for example involving using different subsets of the available data, different ways of training the models, or differing evaluation scripts. This paper describes our work toward a novel multiple object tracking benchmark aimed to address such issues. We discuss the challenges of creating such a framework, collecting existing and new data, gathering state-of-the-art methods to be tested on the datasets, and finally creating a unified evaluation system. With MOTChallenge we aim to pave the way toward a unified evaluation framework for a more meaningful quantification of multi-target tracking. version:1
arxiv-1504-01934 | Data Mining for Prediction of Human Performance Capability in the Software-Industry | http://arxiv.org/abs/1504.01934 | id:1504.01934 author:Gaurav Singh Thakur, Anubhav Gupta, Sangita Gupta category:cs.LG  published:2015-04-08 summary:The recruitment of new personnel is one of the most essential business processes which affect the quality of human capital within any company. It is highly essential for the companies to ensure the recruitment of right talent to maintain a competitive edge over the others in the market. However IT companies often face a problem while recruiting new people for their ongoing projects due to lack of a proper framework that defines a criteria for the selection process. In this paper we aim to develop a framework that would allow any project manager to take the right decision for selecting new talent by correlating performance parameters with the other domain-specific attributes of the candidates. Also, another important motivation behind this project is to check the validity of the selection procedure often followed by various big companies in both public and private sectors which focus only on academic scores, GPA/grades of students from colleges and other academic backgrounds. We test if such a decision will produce optimal results in the industry or is there a need for change that offers a more holistic approach to recruitment of new talent in the software companies. The scope of this work extends beyond the IT domain and a similar procedure can be adopted to develop a recruitment framework in other fields as well. Data-mining techniques provide useful information from the historical projects depending on which the hiring-manager can make decisions for recruiting high-quality workforce. This study aims to bridge this hiatus by developing a data-mining framework based on an ensemble-learning technique to refocus on the criteria for personnel selection. The results from this research clearly demonstrated that there is a need to refocus on the selection-criteria for quality objectives. version:1
arxiv-1504-01920 | Evaluating Two-Stream CNN for Video Classification | http://arxiv.org/abs/1504.01920 | id:1504.01920 author:Hao Ye, Zuxuan Wu, Rui-Wei Zhao, Xi Wang, Yu-Gang Jiang, Xiangyang Xue category:cs.CV  published:2015-04-08 summary:Videos contain very rich semantic information. Traditional hand-crafted features are known to be inadequate in analyzing complex video semantics. Inspired by the huge success of the deep learning methods in analyzing image, audio and text data, significant efforts are recently being devoted to the design of deep nets for video analytics. Among the many practical needs, classifying videos (or video clips) based on their major semantic categories (e.g., "skiing") is useful in many applications. In this paper, we conduct an in-depth study to investigate important implementation options that may affect the performance of deep nets on video classification. Our evaluations are conducted on top of a recent two-stream convolutional neural network (CNN) pipeline, which uses both static frames and motion optical flows, and has demonstrated competitive performance against the state-of-the-art methods. In order to gain insights and to arrive at a practical guideline, many important options are studied, including network architectures, model fusion, learning parameters and the final prediction methods. Based on the evaluations, very competitive results are attained on two popular video classification benchmarks. We hope that the discussions and conclusions from this work can help researchers in related fields to quickly set up a good basis for further investigations along this very promising direction. version:1
arxiv-1504-01883 | Robust real time face recognition and tracking on gpu using fusion of rgb and depth image | http://arxiv.org/abs/1504.01883 | id:1504.01883 author:Narmada Naik, G. N Rathna category:cs.CV  published:2015-04-08 summary:This paper presents a real-time face recognition system using kinect sensor. The algorithm is implemented on GPU using opencl and significant speed improvements are observed. We use kinect depth image to increase the robustness and reduce computational cost of conventional LBP based face recognition. The main objective of this paper was to perform robust, high speed fusion based face recognition and tracking. The algorithm is mainly composed of three steps. First step is to detect all faces in the video using viola jones algorithm. The second step is online database generation using a tracking window on the face. A modified LBP feature vector is calculated using fusion information from depth and greyscale image on GPU. This feature vector is used to train a svm classifier. Third step involves recognition of multiple faces based on our modified feature vector. version:1
arxiv-1502-04110 | Modeling Brain Circuitry over a Wide Range of Scales | http://arxiv.org/abs/1502.04110 | id:1502.04110 author:Pascal Fua, Graham Knott category:cs.CV  published:2015-02-13 summary:If we are ever to unravel the mysteries of brain function at its most fundamental level, we will need a precise understanding of how its component neurons connect to each other. Electron Microscopes (EM) can now provide the nanometer resolution that is needed to image synapses, and therefore connections, while Light Microscopes (LM) see at the micrometer resolution required to model the 3D structure of the dendritic network. Since both the topology and the connection strength are integral parts of the brain's wiring diagram, being able to combine these two modalities is critically important. In fact, these microscopes now routinely produce high-resolution imagery in such large quantities that the bottleneck becomes automated processing and interpretation, which is needed for such data to be exploited to its full potential. In this paper, we briefly review the Computer Vision techniques we have developed at EPFL to address this need. They include delineating dendritic arbors from LM imagery, segmenting organelles from EM, and combining the two into a consistent representation. version:2
arxiv-1504-01840 | Autonomous CRM Control via CLV Approximation with Deep Reinforcement Learning in Discrete and Continuous Action Space | http://arxiv.org/abs/1504.01840 | id:1504.01840 author:Yegor Tkachenko category:cs.LG  published:2015-04-08 summary:The paper outlines a framework for autonomous control of a CRM (customer relationship management) system. First, it explores how a modified version of the widely accepted Recency-Frequency-Monetary Value system of metrics can be used to define the state space of clients or donors. Second, it describes a procedure to determine the optimal direct marketing action in discrete and continuous action space for the given individual, based on his position in the state space. The procedure involves the use of model-free Q-learning to train a deep neural network that relates a client's position in the state space to rewards associated with possible marketing actions. The estimated value function over the client state space can be interpreted as customer lifetime value, and thus allows for a quick plug-in estimation of CLV for a given client. Experimental results are presented, based on KDD Cup 1998 mailing dataset of donation solicitations. version:1
arxiv-1504-01823 | Structured Matrix Completion with Applications to Genomic Data Integration | http://arxiv.org/abs/1504.01823 | id:1504.01823 author:Tianxi Cai, T. Tony Cai, Anru Zhang category:stat.ME math.ST stat.ML stat.TH  published:2015-04-08 summary:Matrix completion has attracted significant recent attention in many fields including statistics, applied mathematics and electrical engineering. Current literature on matrix completion focuses primarily on independent sampling models under which the individual observed entries are sampled independently. Motivated by applications in genomic data integration, we propose a new framework of structured matrix completion (SMC) to treat structured missingness by design. Specifically, our proposed method aims at efficient matrix recovery when a subset of the rows and columns of an approximately low-rank matrix are observed. We provide theoretical justification for the proposed SMC method and derive lower bound for the estimation errors, which together establish the optimal rate of recovery over certain classes of approximately low-rank matrices. Simulation studies show that the method performs well in finite sample under a variety of configurations. The method is applied to integrate several ovarian cancer genomic studies with different extent of genomic measurements, which enables us to construct more accurate prediction rules for ovarian cancer survival. version:1
arxiv-1504-01807 | Low Rank Representation on Grassmann Manifolds: An Extrinsic Perspective | http://arxiv.org/abs/1504.01807 | id:1504.01807 author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV  published:2015-04-08 summary:Many computer vision algorithms employ subspace models to represent data. The Low-rank representation (LRR) has been successfully applied in subspace clustering for which data are clustered according to their subspace structures. The possibility of extending LRR on Grassmann manifold is explored in this paper. Rather than directly embedding Grassmann manifold into a symmetric matrix space, an extrinsic view is taken by building the self-representation of LRR over the tangent space of each Grassmannian point. A new algorithm for solving the proposed Grassmannian LRR model is designed and implemented. Several clustering experiments are conducted on handwritten digits dataset, dynamic texture video clips and YouTube celebrity face video data. The experimental results show our method outperforms a number of existing methods. version:1
arxiv-1504-01806 | Kernelized Low Rank Representation on Grassmann Manifolds | http://arxiv.org/abs/1504.01806 | id:1504.01806 author:Boyue Wang, Yongli Hu, Junbin Gao, Yanfeng Sun, Baocai Yin category:cs.CV  published:2015-04-08 summary:Low rank representation (LRR) has recently attracted great interest due to its pleasing efficacy in exploring low-dimensional subspace structures embedded in data. One of its successful applications is subspace clustering which means data are clustered according to the subspaces they belong to. In this paper, at a higher level, we intend to cluster subspaces into classes of subspaces. This is naturally described as a clustering problem on Grassmann manifold. The novelty of this paper is to generalize LRR on Euclidean space onto an LRR model on Grassmann manifold in a uniform kernelized framework. The new methods have many applications in computer vision tasks. Several clustering experiments are conducted on handwritten digit images, dynamic textures, human face clips and traffic scene sequences. The experimental results show that the proposed methods outperform a number of state-of-the-art subspace clustering methods. version:1
arxiv-1504-00941 | A Simple Way to Initialize Recurrent Networks of Rectified Linear Units | http://arxiv.org/abs/1504.00941 | id:1504.00941 author:Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton category:cs.NE cs.LG  published:2015-04-03 summary:Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem. version:2
arxiv-1412-7659 | Transformation Properties of Learned Visual Representations | http://arxiv.org/abs/1412.7659 | id:1412.7659 author:Taco S. Cohen, Max Welling category:cs.LG cs.CV cs.NE  published:2014-12-24 summary:When a three-dimensional object moves relative to an observer, a change occurs on the observer's image plane and in the visual representation computed by a learned model. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show, using the theory of group representations, that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a striking relationship between irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decorrelated. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the non-commutative 3D rotation group SO(3). version:3
arxiv-1412-2113 | Consistent Collective Matrix Completion under Joint Low Rank Structure | http://arxiv.org/abs/1412.2113 | id:1412.2113 author:Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang category:stat.ML cs.LG  published:2014-12-05 summary:We address the collective matrix completion problem of jointly recovering a collection of matrices with shared structure from partial (and potentially noisy) observations. To ensure well--posedness of the problem, we impose a joint low rank structure, wherein each component matrix is low rank and the latent space of the low rank factors corresponding to each entity is shared across the entire collection. We first develop a rigorous algebra for representing and manipulating collective--matrix structure, and identify sufficient conditions for consistent estimation of collective matrices. We then propose a tractable convex estimator for solving the collective matrix completion problem, and provide the first non--trivial theoretical guarantees for consistency of collective matrix completion. We show that under reasonable assumptions stated in Section 3.1, with high probability, the proposed estimator exactly recovers the true matrices whenever sample complexity requirements dictated by Theorem 1 are met. The sample complexity requirement derived in the paper are optimum up to logarithmic factors, and significantly improve upon the requirements obtained by trivial extensions of standard matrix completion. Finally, we propose a scalable approximate algorithm to solve the proposed convex program, and corroborate our results through simulated experiments. version:3
arxiv-1504-01753 | Design and Implementation of a 3D Undersea Camera System | http://arxiv.org/abs/1504.01753 | id:1504.01753 author:Xida Chen, Steve Sutphen, Paul Macoun, Yee-Hong Yang category:cs.CV  published:2015-04-07 summary:In this paper, we present the design and development of an undersea camera system. The goal of our system is to provide a 3D model of the undersea habitat in a long-term continuous manner. The most important feature of our system is the use of multiple cameras and multiple projectors, which is able to provide accurate 3D models with an accuracy of a millimeter. By introducing projectors in our system, we can use many different structured light methods for different tasks. There are two main advantages comparing our system with using ROVs or AUVs. First, our system can provide continuous monitoring of the undersea habitat. Second, our system has a low hardware cost. Comparing to existing deployed camera systems, the advantage of our system is that it can provide accurate 3D models and provides opportunities for future development of innovative algorithms for undersea research. version:1
arxiv-1412-6617 | Understanding Minimum Probability Flow for RBMs Under Various Kinds of Dynamics | http://arxiv.org/abs/1412.6617 | id:1412.6617 author:Daniel Jiwoong Im, Ethan Buchman, Graham W. Taylor category:cs.LG  published:2014-12-20 summary:Energy-based models are popular in machine learning due to the elegance of their formulation and their relationship to statistical physics. Among these, the Restricted Boltzmann Machine (RBM), and its staple training algorithm contrastive divergence (CD), have been the prototype for some recent advancements in the unsupervised training of deep neural networks. However, CD has limited theoretical motivation, and can in some cases produce undesirable behavior. Here, we investigate the performance of Minimum Probability Flow (MPF) learning for training RBMs. Unlike CD, with its focus on approximating an intractable partition function via Gibbs sampling, MPF proposes a tractable, consistent, objective function defined in terms of a Taylor expansion of the KL divergence with respect to sampling dynamics. Here we propose a more general form for the sampling dynamics in MPF, and explore the consequences of different choices for these dynamics for training RBMs. Experimental results show MPF outperforming CD for various RBM configurations. version:6
arxiv-1504-01697 | Tensor machines for learning target-specific polynomial features | http://arxiv.org/abs/1504.01697 | id:1504.01697 author:Jiyan Yang, Alex Gittens category:cs.LG stat.ML  published:2015-04-07 summary:Recent years have demonstrated that using random feature maps can significantly decrease the training and testing times of kernel-based algorithms without significantly lowering their accuracy. Regrettably, because random features are target-agnostic, typically thousands of such features are necessary to achieve acceptable accuracies. In this work, we consider the problem of learning a small number of explicit polynomial features. Our approach, named Tensor Machines, finds a parsimonious set of features by optimizing over the hypothesis class introduced by Kar and Karnick for random feature maps in a target-specific manner. Exploiting a natural connection between polynomials and tensors, we provide bounds on the generalization error of Tensor Machines. Empirically, Tensor Machines behave favorably on several real-world datasets compared to other state-of-the-art techniques for learning polynomial features, and deliver significantly more parsimonious models. version:1
arxiv-1504-01684 | Large Margin Nearest Neighbor Embedding for Knowledge Representation | http://arxiv.org/abs/1504.01684 | id:1504.01684 author:Miao Fan, Qiang Zhou, Thomas Fang Zheng, Ralph Grishman category:cs.AI cs.CL  published:2015-04-07 summary:Traditional way of storing facts in triplets ({\it head\_entity, relation, tail\_entity}), abbreviated as ({\it h, r, t}), makes the knowledge intuitively displayed and easily acquired by mankind, but hardly computed or even reasoned by AI machines. Inspired by the success in applying {\it Distributed Representations} to AI-related fields, recent studies expect to represent each entity and relation with a unique low-dimensional embedding, which is different from the symbolic and atomic framework of displaying knowledge in triplets. In this way, the knowledge computing and reasoning can be essentially facilitated by means of a simple {\it vector calculation}, i.e. ${\bf h} + {\bf r} \approx {\bf t}$. We thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities ${\bf t^{+}}$ to get together and close to {\bf h} + {\bf r} ({\it Nearest Neighbor}), and simultaneously pushing the negatives ${\bf t^{-}}$ away from the positives ${\bf t^{+}}$ via keeping a {\it Large Margin}. We also design a corresponding learning algorithm to efficiently find the optimal solution based on {\it Stochastic Gradient Descent} in iterative fashion. Quantitative experiments illustrate that our approach can achieve the state-of-the-art performance, compared with several latest methods on some benchmark datasets for two classical applications, i.e. {\it Link prediction} and {\it Triplet classification}. Moreover, we analyze the parameter complexities among all the evaluated models, and analytical results indicate that our model needs fewer computational resources on outperforming the other methods. version:1
arxiv-1504-01577 | From Averaging to Acceleration, There is Only a Step-size | http://arxiv.org/abs/1504.01577 | id:1504.01577 author:Nicolas Flammarion, Francis Bach category:stat.ML math.OC  published:2015-04-07 summary:We show that accelerated gradient descent, averaged gradient descent and the heavy-ball method for non-strongly-convex problems may be reformulated as constant parameter second-order difference equation algorithms, where stability of the system is equivalent to convergence at rate O(1/n 2), where n is the number of iterations. We provide a detailed analysis of the eigenvalues of the corresponding linear dynamical system , showing various oscillatory and non-oscillatory behaviors, together with a sharp stability result with explicit constants. We also consider the situation where noisy gradients are available, where we extend our general convergence result, which suggests an alternative algorithm (i.e., with different step sizes) that exhibits the good aspects of both averaging and acceleration. version:1
arxiv-1504-01561 | Modeling Spatial-Temporal Clues in a Hybrid Deep Learning Framework for Video Classification | http://arxiv.org/abs/1504.01561 | id:1504.01561 author:Zuxuan Wu, Xi Wang, Yu-Gang Jiang, Hao Ye, Xiangyang Xue category:cs.CV cs.MM  published:2015-04-07 summary:Classifying videos according to content semantics is an important problem with a wide range of applications. In this paper, we propose a hybrid deep learning framework for video classification, which is able to model static spatial information, short-term motion, as well as long-term temporal clues in the videos. Specifically, the spatial and the short-term motion features are extracted separately by two Convolutional Neural Networks (CNN). These two types of CNN-based features are then combined in a regularized feature fusion network for classification, which is able to learn and utilize feature relationships for improved performance. In addition, Long Short Term Memory (LSTM) networks are applied on top of the two features to further model longer-term temporal clues. The main contribution of this work is the hybrid learning framework that can model several important aspects of the video data. We also show that (1) combining the spatial and the short-term motion features in the regularized fusion network is better than direct classification and fusion using the CNN with a softmax layer, and (2) the sequence-based LSTM is highly complementary to the traditional classification strategy without considering the temporal frame orders. Extensive experiments are conducted on two popular and challenging benchmarks, the UCF-101 Human Actions and the Columbia Consumer Videos (CCV). On both benchmarks, our framework achieves to-date the best reported performance: $91.3\%$ on the UCF-101 and $83.5\%$ on the CCV. version:1
arxiv-1504-01502 | Separable time-causal and time-recursive spatio-temporal receptive fields | http://arxiv.org/abs/1504.01502 | id:1504.01502 author:Tony Lindeberg category:cs.CV q-bio.NC  published:2015-04-07 summary:We present an improved model and theory for time-causal and time-recursive spatio-temporal receptive fields, obtained by a combination of Gaussian receptive fields over the spatial domain and first-order integrators or equivalently truncated exponential filters coupled in cascade over the temporal domain. Compared to previous spatio-temporal scale-space formulations in terms of non-enhancement of local extrema or scale invariance, these receptive fields are based on different scale-space axiomatics over time by ensuring non-creation of new local extrema or zero-crossings with increasing temporal scale. Specifically, extensions are presented about parameterizing the intermediate temporal scale levels, analysing the resulting temporal dynamics and transferring the theory to a discrete implementation in terms of recursive filters over time. version:1
arxiv-0910-2381 | Fractional differentiation based image processing | http://arxiv.org/abs/0910.2381 | id:0910.2381 author:Amelia Carolina Sparavigna category:cs.CV  published:2009-10-13 summary:There are many resources useful for processing images, most of them freely available and quite friendly to use. In spite of this abundance of tools, a study of the processing methods is still worthy of efforts. Here, we want to discuss the possibilities arising from the use of fractional differential calculus. This calculus evolved in the research field of pure mathematics until 1920, when applied science started to use it. Only recently, fractional calculus was involved in image processing methods. As we shall see, the fractional calculation is able to enhance the quality of images, with interesting possibilities in edge detection and image restoration. We suggest also the fractional differentiation as a tool to reveal faint objects in astronomical images. version:4
arxiv-1504-01496 | Voice based self help System: User Experience Vs Accuracy | http://arxiv.org/abs/1504.01496 | id:1504.01496 author:Sunil Kumar Kopparapu category:cs.CL  published:2015-04-07 summary:In general, self help systems are being increasingly deployed by service based industries because they are capable of delivering better customer service and increasingly the switch is to voice based self help systems because they provide a natural interface for a human to interact with a machine. A speech based self help system ideally needs a speech recognition engine to convert spoken speech to text and in addition a language processing engine to take care of any misrecognitions by the speech recognition engine. Any off-the-shelf speech recognition engine is generally a combination of acoustic processing and speech grammar. While this is the norm, we believe that ideally a speech recognition application should have in addition to a speech recognition engine a separate language processing engine to give the system better performance. In this paper, we discuss ways in which the speech recognition engine and the language processing engine can be combined to give a better user experience. version:1
arxiv-1504-01492 | Efficient SDP Inference for Fully-connected CRFs Based on Low-rank Decomposition | http://arxiv.org/abs/1504.01492 | id:1504.01492 author:Peng Wang, Chunhua Shen, Anton van den Hengel category:cs.CV cs.LG stat.ML  published:2015-04-07 summary:Conditional Random Fields (CRF) have been widely used in a variety of computer vision tasks. Conventional CRFs typically define edges on neighboring image pixels, resulting in a sparse graph such that efficient inference can be performed. However, these CRFs fail to model long-range contextual relationships. Fully-connected CRFs have thus been proposed. While there are efficient approximate inference methods for such CRFs, usually they are sensitive to initialization and make strong assumptions. In this work, we develop an efficient, yet general algorithm for inference on fully-connected CRFs. The algorithm is based on a scalable SDP algorithm and the low- rank approximation of the similarity/kernel matrix. The core of the proposed algorithm is a tailored quasi-Newton method that takes advantage of the low-rank matrix approximation when solving the specialized SDP dual problem. Experiments demonstrate that our method can be applied on fully-connected CRFs that cannot be solved previously, such as pixel-level image co-segmentation. version:1
arxiv-1504-01488 | On-line Handwritten Devanagari Character Recognition using Fuzzy Directional Features | http://arxiv.org/abs/1504.01488 | id:1504.01488 author:Sunil Kumar Kopparapu, Lajish VL category:cs.CV  published:2015-04-07 summary:This paper describes a new feature set for use in the recognition of on-line handwritten Devanagari script based on Fuzzy Directional Features. Experiments are conducted for the automatic recognition of isolated handwritten character primitives (sub-character units). Initially we describe the proposed feature set, called the Fuzzy Directional Features (FDF) and then show how these features can be effectively utilized for writer independent character recognition. Experimental results show that FDF set perform well for writer independent data set at stroke level recognition. The main contribution of this paper is the introduction of a novel feature set and establish experimentally its ability in recognition of handwritten Devanagari script. version:1
arxiv-1504-01483 | Transferring Knowledge from a RNN to a DNN | http://arxiv.org/abs/1504.01483 | id:1504.01483 author:William Chan, Nan Rosemary Ke, Ian Lane category:cs.LG cs.CL cs.NE stat.ML  published:2015-04-07 summary:Deep Neural Network (DNN) acoustic models have yielded many state-of-the-art results in Automatic Speech Recognition (ASR) tasks. More recently, Recurrent Neural Network (RNN) models have been shown to outperform DNNs counterparts. However, state-of-the-art DNN and RNN models tend to be impractical to deploy on embedded systems with limited computational capacity. Traditionally, the approach for embedded platforms is to either train a small DNN directly, or to train a small DNN that learns the output distribution of a large DNN. In this paper, we utilize a state-of-the-art RNN to transfer knowledge to small DNN. We use the RNN model to generate soft alignments and minimize the Kullback-Leibler divergence against the small DNN. The small DNN trained on the soft RNN alignments achieved a 3.93 WER on the Wall Street Journal (WSJ) eval92 task compared to a baseline 4.54 WER or more than 13% relative improvement. version:1
arxiv-1504-01482 | Deep Recurrent Neural Networks for Acoustic Modelling | http://arxiv.org/abs/1504.01482 | id:1504.01482 author:William Chan, Ian Lane category:cs.LG cs.CL cs.NE stat.ML  published:2015-04-07 summary:We present a novel deep Recurrent Neural Network (RNN) model for acoustic modelling in Automatic Speech Recognition (ASR). We term our contribution as a TC-DNN-BLSTM-DNN model, the model combines a Deep Neural Network (DNN) with Time Convolution (TC), followed by a Bidirectional Long Short-Term Memory (BLSTM), and a final DNN. The first DNN acts as a feature processor to our model, the BLSTM then generates a context from the sequence acoustic signal, and the final DNN takes the context and models the posterior probabilities of the acoustic states. We achieve a 3.47 WER on the Wall Street Journal (WSJ) eval92 task or more than 8% relative improvement over the baseline DNN models. version:1
arxiv-1504-01476 | Mobile Phone Based Vehicle License Plate Recognition for Road Policing | http://arxiv.org/abs/1504.01476 | id:1504.01476 author:Lajish V. L., Sunil Kumar Kopparapu category:cs.CV  published:2015-04-07 summary:Identity of a vehicle is done through the vehicle license plate by traffic police in general. Au- tomatic vehicle license plate recognition has several applications in intelligent traffic management systems. The security situation across the globe and particularly in India demands a need to equip the traffic police with a system that enables them to get instant details of a vehicle. The system should be easy to use, should be mobile, and work 24 x 7. In this paper, we describe a mobile phone based, client-server architected, license plate recognition system. While we use the state of the art image processing and pattern recognition algorithms tuned for Indian conditions to automatically recognize non-uniform license plates, the main contribution is in creating an end to end usable solution. The client application runs on a mobile device and a server application, with access to vehicle information database, is hosted centrally. The solution enables capture of license plate image captured by the phone camera and passes to the server; on the server the license plate number is recognized; the data associated with the number plate is then sent back to the mobile device, instantaneously. We describe the end to end system architecture in detail. A working prototype of the proposed system has been implemented in the lab environment. version:1
arxiv-1404-7552 | The geometry of kernelized spectral clustering | http://arxiv.org/abs/1404.7552 | id:1404.7552 author:Geoffrey Schiebinger, Martin J. Wainwright, Bin Yu category:math.ST stat.ML stat.TH  published:2014-04-29 summary:Clustering of data sets is a standard problem in many areas of science and engineering. The method of spectral clustering is based on embedding the data set using a kernel function, and using the top eigenvectors of the normalized Laplacian to recover the connected components. We study the performance of spectral clustering in recovering the latent labels of i.i.d. samples from a finite mixture of nonparametric distributions. The difficulty of this label recovery problem depends on the overlap between mixture components and how easily a mixture component is divided into two nonoverlapping components. When the overlap is small compared to the indivisibility of the mixture components, the principal eigenspace of the population-level normalized Laplacian operator is approximately spanned by the square-root kernelized component densities. In the finite sample setting, and under the same assumption, embedded samples from different components are approximately orthogonal with high probability when the sample size is large. As a corollary we control the fraction of samples mislabeled by spectral clustering under finite mixtures with nonparametric components. version:3
arxiv-1505-06219 | A comparative study between proposed Hyper Kurtosis based Modified Duo-Histogram Equalization (HKMDHE) and Contrast Limited Adaptive Histogram Equalization (CLAHE) for Contrast Enhancement Purpose of Low Contrast Human Brain CT scan images | http://arxiv.org/abs/1505.06219 | id:1505.06219 author:Sabyasachi Mukhopadhyay, Soham Mandal, Sawon Pratiher, Satyasaran Changdar, Ritwik Burman, Nirmalya Ghosh, Prasanta K. Panigrahi category:cs.CV  published:2015-04-07 summary:In this paper, a comparative study between proposed hyper kurtosis based modified duo-histogram equalization (HKMDHE) algorithm and contrast limited adaptive histogram enhancement (CLAHE) has been presented for the implementation of contrast enhancement and brightness preservation of low contrast human brain CT scan images. In HKMDHE algorithm, contrast enhancement is done on the hyper-kurtosis based application. The results are very promising of proposed HKMDHE technique with improved PSNR values and lesser AMMBE values than CLAHE technique. version:1
arxiv-1504-01446 | Totally Corrective Boosting with Cardinality Penalization | http://arxiv.org/abs/1504.01446 | id:1504.01446 author:Vasil S. Denchev, Nan Ding, Shin Matsushima, S. V. N. Vishwanathan, Hartmut Neven category:cs.LG quant-ph  published:2015-04-07 summary:We propose a totally corrective boosting algorithm with explicit cardinality regularization. The resulting combinatorial optimization problems are not known to be efficiently solvable with existing classical methods, but emerging quantum optimization technology gives hope for achieving sparser models in practice. In order to demonstrate the utility of our algorithm, we use a distributed classical heuristic optimizer as a stand-in for quantum hardware. Even though this evaluation methodology incurs large time and resource costs on classical computing machinery, it allows us to gauge the potential gains in generalization performance and sparsity of the resulting boosted ensembles. Our experimental results on public data sets commonly used for benchmarking of boosting algorithms decidedly demonstrate the existence of such advantages. If actual quantum optimization were to be used with this algorithm in the future, we would expect equivalent or superior results at much smaller time and energy costs during training. Moreover, studying cardinality-penalized boosting also sheds light on why unregularized boosting algorithms with early stopping often yield better results than their counterparts with explicit convex regularization: Early stopping performs suboptimal cardinality regularization. The results that we present here indicate it is beneficial to explicitly solve the combinatorial problem still left open at early termination. version:1
arxiv-1504-01427 | A Metric to Classify Style of Spoken Speech | http://arxiv.org/abs/1504.01427 | id:1504.01427 author:Sunil Kopparapu, Saurabh Bhatnagar, K. Sahana, Sathyanarayana, Akhilesh Srivastava, P. V. S. Rao category:cs.CL  published:2015-04-06 summary:The ability to classify spoken speech based on the style of speaking is an important problem. With the advent of BPO's in recent times, specifically those that cater to a population other than the local population, it has become necessary for BPO's to identify people with certain style of speaking (American, British etc). Today BPO's employ accent analysts to identify people having the required style of speaking. This process while involving human bias, it is becoming increasingly infeasible because of the high attrition rate in the BPO industry. In this paper, we propose a new metric, which robustly and accurately helps classify spoken speech based on the style of speaking. The role of the proposed metric is substantiated by using it to classify real speech data collected from over seventy different people working in a BPO. We compare the performance of the metric against human experts who independently carried out the classification process. Experimental results show that the performance of the system using the novel metric performs better than two different human expert. version:1
arxiv-1412-6615 | Explorations on high dimensional landscapes | http://arxiv.org/abs/1412.6615 | id:1412.6615 author:Levent Sagun, V. Ugur Guney, Gerard Ben Arous, Yann LeCun category:stat.ML cs.LG  published:2014-12-20 summary:Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps. version:4
arxiv-1504-01420 | Knowledge driven Offline to Online Script Conversion | http://arxiv.org/abs/1504.01420 | id:1504.01420 author:Sunil Kopparapu, Devanuj, Akhilesh Srivastava, P. V. S. Rao category:cs.CV  published:2015-04-06 summary:The problem of offline to online script conversion is a challenging and an ill-posed problem. The interest in offline to online conversion exists because there are a plethora of robust algorithms in online script literature which can not be used on offline scripts. In this paper, we propose a method, based on heuristics, to extract online script information from offline bitmap image. We show the performance of the proposed method on a real sample signature offline image, whose online information is known. version:1
arxiv-1412-2302 | Theano-based Large-Scale Visual Recognition with Multiple GPUs | http://arxiv.org/abs/1412.2302 | id:1412.2302 author:Weiguang Ding, Ruoyan Wang, Fei Mao, Graham Taylor category:cs.LG  published:2014-12-07 summary:In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date. version:4
arxiv-1504-01383 | QUOTUS: The Structure of Political Media Coverage as Revealed by Quoting Patterns | http://arxiv.org/abs/1504.01383 | id:1504.01383 author:Vlad Niculae, Caroline Suen, Justine Zhang, Cristian Danescu-Niculescu-Mizil, Jure Leskovec category:cs.CL cs.SI physics.soc-ph  published:2015-04-06 summary:Given the extremely large pool of events and stories available, media outlets need to focus on a subset of issues and aspects to convey to their audience. Outlets are often accused of exhibiting a systematic bias in this selection process, with different outlets portraying different versions of reality. However, in the absence of objective measures and empirical evidence, the direction and extent of systematicity remains widely disputed. In this paper we propose a framework based on quoting patterns for quantifying and characterizing the degree to which media outlets exhibit systematic bias. We apply this framework to a massive dataset of news articles spanning the six years of Obama's presidency and all of his speeches, and reveal that a systematic pattern does indeed emerge from the outlet's quoting behavior. Moreover, we show that this pattern can be successfully exploited in an unsupervised prediction setting, to determine which new quotes an outlet will select to broadcast. By encoding bias patterns in a low-rank space we provide an analysis of the structure of political media coverage. This reveals a latent media bias space that aligns surprisingly well with political ideology and outlet type. A linguistic analysis exposes striking differences across these latent dimensions, showing how the different types of media outlets portray different realities even when reporting on the same events. For example, outlets mapped to the mainstream conservative side of the latent space focus on quotes that portray a presidential persona disproportionately characterized by negativity. version:1
arxiv-1504-01365 | PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent | http://arxiv.org/abs/1504.01365 | id:1504.01365 author:Cho-Jui Hsieh, Hsiang-Fu Yu, Inderjit S. Dhillon category:cs.LG  published:2015-04-06 summary:Stochastic Dual Coordinate Descent (SDCD) has become one of the most efficient ways to solve the family of $\ell_2$-regularized empirical risk minimization problems, including linear SVM, logistic regression, and many others. The vanilla implementation of DCD is quite slow; however, by maintaining primal variables while updating dual variables, the time complexity of SDCD can be significantly reduced. Such a strategy forms the core algorithm in the widely-used LIBLINEAR package. In this paper, we parallelize the SDCD algorithms in LIBLINEAR. In recent research, several synchronized parallel SDCD algorithms have been proposed, however, they fail to achieve good speedup in the shared memory multi-core setting. In this paper, we propose a family of asynchronous stochastic dual coordinate descent algorithms (ASDCD). Each thread repeatedly selects a random dual variable and conducts coordinate updates using the primal variables that are stored in the shared memory. We analyze the convergence properties when different locking/atomic mechanisms are applied. For implementation with atomic operations, we show linear convergence under mild conditions. For implementation without any atomic operations or locking, we present the first {\it backward error analysis} for ASDCD under the multi-core environment, showing that the converged solution is the exact solution for a primal problem with perturbed regularizer. Experimental results show that our methods are much faster than previous parallel coordinate descent solvers. version:1
arxiv-1504-01344 | Early Stopping is Nonparametric Variational Inference | http://arxiv.org/abs/1504.01344 | id:1504.01344 author:Dougal Maclaurin, David Duvenaud, Ryan P. Adams category:stat.ML cs.LG  published:2015-04-06 summary:We show that unconverged stochastic gradient descent can be interpreted as a procedure that samples from a nonparametric variational approximate posterior distribution. This distribution is implicitly defined as the transformation of an initial distribution by a sequence of optimization updates. By tracking the change in entropy over this sequence of transformations during optimization, we form a scalable, unbiased estimate of the variational lower bound on the log marginal likelihood. We can use this bound to optimize hyperparameters instead of using cross-validation. This Bayesian interpretation of SGD suggests improved, overfitting-resistant optimization procedures, and gives a theoretical foundation for popular tricks such as early stopping and ensembling. We investigate the properties of this marginal likelihood estimator on neural network models. version:1
arxiv-1502-02127 | Hyperparameter Search in Machine Learning | http://arxiv.org/abs/1502.02127 | id:1502.02127 author:Marc Claesen, Bart De Moor category:cs.LG stat.ML  published:2015-02-07 summary:We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential. version:2
arxiv-1504-01220 | Matching-CNN Meets KNN: Quasi-Parametric Human Parsing | http://arxiv.org/abs/1504.01220 | id:1504.01220 author:Si Liu, Xiaodan Liang, Luoqi Liu, Xiaohui Shen, Jianchao Yang, Changsheng Xu, Liang Lin, Xiaochun Cao, Shuicheng Yan category:cs.CV  published:2015-04-06 summary:Both parametric and non-parametric approaches have demonstrated encouraging performances in the human parsing task, namely segmenting a human image into several semantic regions (e.g., hat, bag, left arm, face). In this work, we aim to develop a new solution with the advantages of both methodologies, namely supervision from annotated data and the flexibility to use newly annotated (possibly uncommon) images, and present a quasi-parametric human parsing model. Under the classic K Nearest Neighbor (KNN)-based nonparametric framework, the parametric Matching Convolutional Neural Network (M-CNN) is proposed to predict the matching confidence and displacements of the best matched region in the testing image for a particular semantic region in one KNN image. Given a testing image, we first retrieve its KNN images from the annotated/manually-parsed human image corpus. Then each semantic region in each KNN image is matched with confidence to the testing image using M-CNN, and the matched regions from all KNN images are further fused, followed by a superpixel smoothing procedure to obtain the ultimate human parsing result. The M-CNN differs from the classic CNN in that the tailored cross image matching filters are introduced to characterize the matching between the testing image and the semantic region of a KNN image. The cross image matching filters are defined at different convolutional layers, each aiming to capture a particular range of displacements. Comprehensive evaluations over a large dataset with 7,700 annotated human images well demonstrate the significant performance gain from the quasi-parametric model over the state-of-the-arts, for the human parsing task. version:1
arxiv-1504-01182 | Bengali to Assamese Statistical Machine Translation using Moses (Corpus Based) | http://arxiv.org/abs/1504.01182 | id:1504.01182 author:Nayan Jyoti Kalita, Baharul Islam category:cs.CL  published:2015-04-06 summary:Machine dialect interpretation assumes a real part in encouraging man-machine correspondence and in addition men-men correspondence in Natural Language Processing (NLP). Machine Translation (MT) alludes to utilizing machine to change one dialect to an alternate. Statistical Machine Translation is a type of MT consisting of Language Model (LM), Translation Model (TM) and decoder. In this paper, Bengali to Assamese Statistical Machine Translation Model has been created by utilizing Moses. Other translation tools like IRSTLM for Language Model and GIZA-PP-V1.0.7 for Translation model are utilized within this framework which is accessible in Linux situations. The purpose of the LM is to encourage fluent output and the purpose of TM is to encourage similarity between input and output, the decoder increases the probability of translated text in target language. A parallel corpus of 17100 sentences in Bengali and Assamese has been utilized for preparing within this framework. Measurable MT procedures have not so far been generally investigated for Indian dialects. It might be intriguing to discover to what degree these models can help the immense continuous MT deliberations in the nation. version:1
arxiv-1504-01169 | Efficient Dictionary Learning via Very Sparse Random Projections | http://arxiv.org/abs/1504.01169 | id:1504.01169 author:Farhad Pourkamali-Anaraki, Stephen Becker, Shannon M. Hughes category:stat.ML cs.LG  published:2015-04-05 summary:Performing signal processing tasks on compressive measurements of data has received great attention in recent years. In this paper, we extend previous work on compressive dictionary learning by showing that more general random projections may be used, including sparse ones. More precisely, we examine compressive K-means clustering as a special case of compressive dictionary learning and give theoretical guarantees for its performance for a very general class of random projections. We then propose a memory and computation efficient dictionary learning algorithm, specifically designed for analyzing large volumes of high-dimensional data, which learns the dictionary from very sparse random projections. Experimental results demonstrate that our approach allows for reduction of computational complexity and memory/data access, with controllable loss in accuracy. version:1
arxiv-1504-01167 | Heuristic algorithms for obtaining Polynomial Threshold Functions with low densities | http://arxiv.org/abs/1504.01167 | id:1504.01167 author:Can Eren Sezener, Erhan Oztop category:cs.CC cs.NE  published:2015-04-05 summary:In this paper we present several heuristic algorithms, including a Genetic Algorithm (GA), for obtaining polynomial threshold function (PTF) representations of Boolean functions (BFs) with small number of monomials. We compare these among each other and against the algorithm of Oztop via computational experiments. The results indicate that our heuristic algorithms find more parsimonious representations compared to the those of non-heuristic and GA-based algorithms. version:1
arxiv-1504-01142 | Ultra-large alignments using Phylogeny-aware Profiles | http://arxiv.org/abs/1504.01142 | id:1504.01142 author:Nam-phuong Nguyen, Siavash Mirarab, Keerthana Kumar, Tandy Warnow category:q-bio.GN cs.CE cs.LG  published:2015-04-05 summary:Many biological questions, including the estimation of deep evolutionary histories and the detection of remote homology between protein sequences, rely upon multiple sequence alignments (MSAs) and phylogenetic trees of large datasets. However, accurate large-scale multiple sequence alignment is very difficult, especially when the dataset contains fragmentary sequences. We present UPP, an MSA method that uses a new machine learning technique - the Ensemble of Hidden Markov Models - that we propose here. UPP produces highly accurate alignments for both nucleotide and amino acid sequences, even on ultra-large datasets or datasets containing fragmentary sequences. UPP is available at https://github.com/smirarab/sepp. version:1
arxiv-1504-01072 | EM-Based Channel Estimation from Crowd-Sourced RSSI Samples Corrupted by Noise and Interference | http://arxiv.org/abs/1504.01072 | id:1504.01072 author:Silvija Kokalj-Filipovic, Larry Greenstein category:cs.LG  published:2015-04-05 summary:We propose a method for estimating channel parameters from RSSI measurements and the lost packet count, which can work in the presence of losses due to both interference and signal attenuation below the noise floor. This is especially important in the wireless networks, such as vehicular, where propagation model changes with the density of nodes. The method is based on Stochastic Expectation Maximization, where the received data is modeled as a mixture of distributions (no/low interference and strong interference), incomplete (censored) due to packet losses. The PDFs in the mixture are Gamma, according to the commonly accepted model for wireless signal and interference power. This approach leverages the loss count as additional information, hence outperforming maximum likelihood estimation, which does not use this information (ML-), for a small number of received RSSI samples. Hence, it allows inexpensive on-line channel estimation from ad-hoc collected data. The method also outperforms ML- on uncensored data mixtures, as ML- assumes that samples are from a single-mode PDF. version:1
arxiv-1504-01070 | Sync-Rank: Robust Ranking, Constrained Ranking and Rank Aggregation via Eigenvector and Semidefinite Programming Synchronization | http://arxiv.org/abs/1504.01070 | id:1504.01070 author:Mihai Cucuringu category:cs.LG cs.SI math.OC stat.ML  published:2015-04-05 summary:We consider the classic problem of establishing a statistical ranking of a set of n items given a set of inconsistent and incomplete pairwise comparisons between such items. Instantiations of this problem occur in numerous applications in data analysis (e.g., ranking teams in sports data), computer vision, and machine learning. We formulate the above problem of ranking with incomplete noisy information as an instance of the group synchronization problem over the group SO(2) of planar rotations, whose usefulness has been demonstrated in numerous applications in recent years. Its least squares solution can be approximated by either a spectral or a semidefinite programming (SDP) relaxation, followed by a rounding procedure. We perform extensive numerical simulations on both synthetic and real-world data sets, showing that our proposed method compares favorably to other algorithms from the recent literature. Existing theoretical guarantees on the group synchronization problem imply lower bounds on the largest amount of noise permissible in the ranking data while still achieving exact recovery. We propose a similar synchronization-based algorithm for the rank-aggregation problem, which integrates in a globally consistent ranking pairwise comparisons given by different rating systems on the same set of items. We also discuss the problem of semi-supervised ranking when there is available information on the ground truth rank of a subset of players, and propose an algorithm based on SDP which recovers the ranks of the remaining players. Finally, synchronization-based ranking, combined with a spectral technique for the densest subgraph problem, allows one to extract locally-consistent partial rankings, in other words, to identify the rank of a small subset of players whose pairwise comparisons are less noisy than the rest of the data, which other methods are not able to identify. version:1
arxiv-1504-01052 | Fast algorithms for morphological operations using run-length encoded binary images | http://arxiv.org/abs/1504.01052 | id:1504.01052 author:Gregor Ehrensperger, Alexander Ostermann, Felix Schwitzer category:cs.CV cs.GR cs.IT math.IT I.4.3; I.5; I.4.10  published:2015-04-04 summary:This paper presents innovative algorithms to efficiently compute erosions and dilations of run-length encoded (RLE) binary images with arbitrary shaped structuring elements. An RLE image is given by a set of runs, where a run is a horizontal concatenation of foreground pixels. The proposed algorithms extract the skeleton of the structuring element and build distance tables of the input image, which are storing the distance to the next background pixel on the left and right hand sides. This information is then used to speed up the calculations of the erosion and dilation operator by enabling the use of techniques which allow to skip the analysis of certain pixels whenever a hit or miss occurs. Additionally the input image gets trimmed during the preprocessing steps on the base of two primitive criteria. Experimental results show the advantages over other algorithms. The source code of our algorithms is available in C++. version:1
arxiv-1504-01050 | An Online Approach to Dynamic Channel Access and Transmission Scheduling | http://arxiv.org/abs/1504.01050 | id:1504.01050 author:Yang Liu, Mingyan Liu category:cs.LG cs.SY F.1.2; C.2.1; G.3  published:2015-04-04 summary:Making judicious channel access and transmission scheduling decisions is essential for improving performance as well as energy and spectral efficiency in multichannel wireless systems. This problem has been a subject of extensive study in the past decade, and the resulting dynamic and opportunistic channel access schemes can bring potentially significant improvement over traditional schemes. However, a common and severe limitation of these dynamic schemes is that they almost always require some form of a priori knowledge of the channel statistics. A natural remedy is a learning framework, which has also been extensively studied in the same context, but a typical learning algorithm in this literature seeks only the best static policy, with performance measured by weak regret, rather than learning a good dynamic channel access policy. There is thus a clear disconnect between what an optimal channel access policy can achieve with known channel statistics that actively exploits temporal, spatial and spectral diversity, and what a typical existing learning algorithm aims for, which is the static use of a single channel devoid of diversity gain. In this paper we bridge this gap by designing learning algorithms that track known optimal or sub-optimal dynamic channel access and transmission scheduling policies, thereby yielding performance measured by a form of strong regret, the accumulated difference between the reward returned by an optimal solution when a priori information is available and that by our online algorithm. We do so in the context of two specific algorithms that appeared in [1] and [2], respectively, the former for a multiuser single-channel setting and the latter for a single-user multichannel setting. In both cases we show that our algorithms achieve sub-linear regret uniform in time and outperforms the standard weak-regret learning algorithms. version:1
arxiv-1504-00325 | Microsoft COCO Captions: Data Collection and Evaluation Server | http://arxiv.org/abs/1504.00325 | id:1504.00325 author:Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, C. Lawrence Zitnick category:cs.CV cs.CL  published:2015-04-01 summary:In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided. version:2
arxiv-1504-00923 | A Unified Deep Neural Network for Speaker and Language Recognition | http://arxiv.org/abs/1504.00923 | id:1504.00923 author:Fred Richardson, Douglas Reynolds, Najim Dehak category:cs.CL cs.CV cs.LG cs.NE stat.ML  published:2015-04-03 summary:Learned feature representations and sub-phoneme posteriors from Deep Neural Networks (DNNs) have been used separately to produce significant performance gains for speaker and language recognition tasks. In this work we show how these gains are possible using a single DNN for both speaker and language recognition. The unified DNN approach is shown to yield substantial performance improvements on the the 2013 Domain Adaptation Challenge speaker recognition task (55% reduction in EER for the out-of-domain condition) and on the NIST 2011 Language Recognition Evaluation (48% reduction in EER for the 30s test condition). version:1
arxiv-1501-02320 | On model misspecification and KL separation for Gaussian graphical models | http://arxiv.org/abs/1501.02320 | id:1501.02320 author:Varun Jog, Po-Ling Loh category:cs.IT math.IT math.ST stat.ML stat.TH 62B10  published:2015-01-10 summary:We establish bounds on the KL divergence between two multivariate Gaussian distributions in terms of the Hamming distance between the edge sets of the corresponding graphical models. We show that the KL divergence is bounded below by a constant when the graphs differ by at least one edge; this is essentially the tightest possible bound, since classes of graphs exist for which the edge discrepancy increases but the KL divergence remains bounded above by a constant. As a natural corollary to our KL lower bound, we also establish a sample size requirement for correct model selection via maximum likelihood estimation. Our results rigorize the notion that it is essential to estimate the edge structure of a Gaussian graphical model accurately in order to approximate the true distribution to close precision. version:2
arxiv-1412-6448 | Embedding Word Similarity with Neural Machine Translation | http://arxiv.org/abs/1412.6448 | id:1412.6448 author:Felix Hill, Kyunghyun Cho, Sebastien Jean, Coline Devin, Yoshua Bengio category:cs.CL  published:2014-12-19 summary:Neural language models learn word representations, or embeddings, that capture rich linguistic and conceptual information. Here we investigate the embeddings learned by neural machine translation models, a recently-developed class of neural language model. We show that embeddings from translation models outperform those learned by monolingual models at tasks that require knowledge of both conceptual similarity and lexical-syntactic role. We further show that these effects hold when translating from both English to French and English to German, and argue that the desirable properties of translation embeddings should emerge largely independently of the source and target languages. Finally, we apply a new method for training neural translation models with very large vocabularies, and show that this vocabulary expansion algorithm results in minimal degradation of embedding quality. Our embedding spaces can be queried in an online demo and downloaded from our web page. Overall, our analyses indicate that translation-based embeddings should be used in applications that require concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness. version:4
arxiv-1408-3378 | Beta diffusion trees and hierarchical feature allocations | http://arxiv.org/abs/1408.3378 | id:1408.3378 author:Creighton Heaukulani, David A. Knowles, Zoubin Ghahramani category:stat.ML  published:2014-08-14 summary:We define the beta diffusion tree, a random tree structure with a set of leaves that defines a collection of overlapping subsets of objects, known as a feature allocation. A generative process for the tree structure is defined in terms of particles (representing the objects) diffusing in some continuous space, analogously to the Dirichlet diffusion tree (Neal, 2003), which defines a tree structure over partitions (i.e., non-overlapping subsets) of the objects. Unlike in the Dirichlet diffusion tree, multiple copies of a particle may exist and diffuse along multiple branches in the beta diffusion tree, and an object may therefore belong to multiple subsets of particles. We demonstrate how to build a hierarchically-clustered factor analysis model with the beta diffusion tree and how to perform inference over the random tree structures with a Markov chain Monte Carlo algorithm. We conclude with several numerical experiments on missing data problems with data sets of gene expression microarrays, international development statistics, and intranational socioeconomic measurements. version:2
arxiv-1504-00854 | Evaluation Evaluation a Monte Carlo study | http://arxiv.org/abs/1504.00854 | id:1504.00854 author:David M. W. Powers category:cs.AI cs.CL stat.ML  published:2015-04-03 summary:Over the last decade there has been increasing concern about the biases embodied in traditional evaluation methods for Natural Language Processing/Learning, particularly methods borrowed from Information Retrieval. Without knowledge of the Bias and Prevalence of the contingency being tested, or equivalently the expectation due to chance, the simple conditional probabilities Recall, Precision and Accuracy are not meaningful as evaluation measures, either individually or in combinations such as F-factor. The existence of bias in NLP measures leads to the 'improvement' of systems by increasing their bias, such as the practice of improving tagging and parsing scores by using most common value (e.g. water is always a Noun) rather than the attempting to discover the correct one. The measures Cohen Kappa and Powers Informedness are discussed as unbiased alternative to Recall and related to the psychologically significant measure DeltaP. In this paper we will analyze both biased and unbiased measures theoretically, characterizing the precise relationship between all these measures as well as evaluating the evaluation measures themselves empirically using a Monte Carlo simulation. version:1
arxiv-1409-1484 | The Evolution of First Person Vision Methods: A Survey | http://arxiv.org/abs/1409.1484 | id:1409.1484 author:Alejandro Betancourt, Pietro Morerio, Carlo S. Regazzoni, Matthias Rauterberg category:cs.CV  published:2014-09-04 summary:The emergence of new wearable technologies such as action cameras and smart-glasses has increased the interest of computer vision scientists in the First Person perspective. Nowadays, this field is attracting attention and investments of companies aiming to develop commercial devices with First Person Vision recording capabilities. Due to this interest, an increasing demand of methods to process these videos, possibly in real-time, is expected. Current approaches present a particular combinations of different image features and quantitative methods to accomplish specific objectives like object detection, activity recognition, user machine interaction and so on. This paper summarizes the evolution of the state of the art in First Person Vision video analysis between 1997 and 2014, highlighting, among others, most commonly used features, methods, challenges and opportunities within the field. version:3
arxiv-1301-5220 | Properties of the Least Squares Temporal Difference learning algorithm | http://arxiv.org/abs/1301.5220 | id:1301.5220 author:Kamil Ciosek category:stat.ML cs.LG  published:2013-01-22 summary:This paper presents four different ways of looking at the well-known Least Squares Temporal Differences (LSTD) algorithm for computing the value function of a Markov Reward Process, each of them leading to different insights: the operator-theory approach via the Galerkin method, the statistical approach via instrumental variables, the linear dynamical system view as well as the limit of the TD iteration. We also give a geometric view of the algorithm as an oblique projection. Furthermore, there is an extensive comparison of the optimization problem solved by LSTD as compared to Bellman Residual Minimization (BRM). We then review several schemes for the regularization of the LSTD solution. We then proceed to treat the modification of LSTD for the case of episodic Markov Reward Processes. version:2
arxiv-1504-00781 | The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth Selection in Univariate and Multivariate Kernel Density Estimations | http://arxiv.org/abs/1504.00781 | id:1504.00781 author:Dharmani Bhaveshkumar C category:cs.LG stat.CO stat.ME stat.ML 11Kxx I.5.0  published:2015-04-03 summary:The article derives a novel Gram-Charlier A (GCA) Series based Extended Rule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation (KDE). There are existing various bandwidth selection rules achieving minimization of the Asymptotic Mean Integrated Square Error (AMISE) between the estimated probability density function (PDF) and the actual PDF. The rules differ in a way to estimate the integration of the squared second order derivative of an unknown PDF $(f(\cdot))$, identified as the roughness $R(f''(\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\cdot))$ with an assumption that the density being estimated is Gaussian. Intuitively, better estimation of $R(f''(\cdot))$ and consequently better bandwidth selection rules can be derived, if the unknown PDF is approximated through an infinite series expansion based on a more generalized density assumption. As a demonstration and verification to this concept, the ExROT derived in the article uses an extended assumption that the density being estimated is near Gaussian. This helps use of the GCA expansion as an approximation to the unknown near Gaussian PDF. The ExROT for univariate KDE is extended to that for multivariate KDE. The required multivariate AMISE criteria is re-derived using elementary calculus of several variables, instead of Tensor calculus. The derivation uses the Kronecker product and the vector differential operator to achieve the AMISE expression in vector notations. There is also derived ExROT for kernel based density derivative estimator. version:1
arxiv-1504-00757 | Learning Mixed Membership Mallows Models from Pairwise Comparisons | http://arxiv.org/abs/1504.00757 | id:1504.00757 author:Weicong Ding, Prakash Ishwar, Venkatesh Saligrama category:cs.LG stat.ML  published:2015-04-03 summary:We propose a novel parameterized family of Mixed Membership Mallows Models (M4) to account for variability in pairwise comparisons generated by a heterogeneous population of noisy and inconsistent users. M4 models individual preferences as a user-specific probabilistic mixture of shared latent Mallows components. Our key algorithmic insight for estimation is to establish a statistical connection between M4 and topic models by viewing pairwise comparisons as words, and users as documents. This key insight leads us to explore Mallows components with a separable structure and leverage recent advances in separable topic discovery. While separability appears to be overly restrictive, we nevertheless show that it is an inevitable outcome of a relatively small number of latent Mallows components in a world of large number of items. We then develop an algorithm based on robust extreme-point identification of convex polygons to learn the reference rankings, and is provably consistent with polynomial sample complexity guarantees. We demonstrate that our new model is empirically competitive with the current state-of-the-art approaches in predicting real-world preferences. version:1
arxiv-1504-00736 | Unsupervised Feature Selection with Adaptive Structure Learning | http://arxiv.org/abs/1504.00736 | id:1504.00736 author:Liang Du, Yi-Dong Shen category:cs.LG  published:2015-04-03 summary:The problem of feature selection has raised considerable interests in the past decade. Traditional unsupervised methods select the features which can faithfully preserve the intrinsic structures of data, where the intrinsic structures are estimated using all the input features of data. However, the estimated intrinsic structures are unreliable/inaccurate when the redundant and noisy features are not removed. Therefore, we face a dilemma here: one need the true structures of data to identify the informative features, and one need the informative features to accurately estimate the true structures of data. To address this, we propose a unified learning framework which performs structure learning and feature selection simultaneously. The structures are adaptively learned from the results of feature selection, and the informative features are reselected to preserve the refined structures of data. By leveraging the interactions between these two essential tasks, we are able to capture accurate structures and select more informative features. Experimental results on many benchmark data sets demonstrate that the proposed method outperforms many state of the art unsupervised feature selection methods. version:1
arxiv-1410-6801 | Dimensionality Reduction for k-Means Clustering and Low Rank Approximation | http://arxiv.org/abs/1410.6801 | id:1410.6801 author:Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, Madalina Persu category:cs.DS cs.LG  published:2014-10-24 summary:We show how to approximate a data matrix $\mathbf{A}$ with a much smaller sketch $\mathbf{\tilde A}$ that can be used to solve a general class of constrained k-rank approximation problems to within $(1+\epsilon)$ error. Importantly, this class of problems includes $k$-means clustering and unconstrained low rank approximation (i.e. principal component analysis). By reducing data points to just $O(k)$ dimensions, our methods generically accelerate any exact, approximate, or heuristic algorithm for these ubiquitous problems. For $k$-means dimensionality reduction, we provide $(1+\epsilon)$ relative error results for many common sketching techniques, including random row projection, column selection, and approximate SVD. For approximate principal component analysis, we give a simple alternative to known algorithms that has applications in the streaming setting. Additionally, we extend recent work on column-based matrix reconstruction, giving column subsets that not only `cover' a good subspace for $\bv{A}$, but can be used directly to compute this subspace. Finally, for $k$-means clustering, we show how to achieve a $(9+\epsilon)$ approximation by Johnson-Lindenstrauss projecting data points to just $O(\log k/\epsilon^2)$ dimensions. This gives the first result that leverages the specific structure of $k$-means to achieve dimension independent of input size and sublinear in $k$. version:3
arxiv-1412-6567 | Classifier with Hierarchical Topographical Maps as Internal Representation | http://arxiv.org/abs/1412.6567 | id:1412.6567 author:Thomas Trappenberg, Paul Hollensen, Pitoyo Hartono category:cs.NE  published:2014-12-20 summary:In this study we want to connect our previously proposed context-relevant topographical maps with the deep learning community. Our architecture is a classifier with hidden layers that are hierarchical two-dimensional topographical maps. These maps differ from the conventional self-organizing maps in that their organizations are influenced by the context of the data labels in a top-down manner. In this way bottom-up and top-down learning are combined in a biologically relevant representational learning setting. Compared to our previous work, we are here specifically elaborating the model in a more challenging setting compared to our previous experiments and to advance more hidden representation layers to bring our discussions into the context of deep representational learning. version:4
arxiv-1412-1897 | Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images | http://arxiv.org/abs/1412.1897 | id:1412.1897 author:Anh Nguyen, Jason Yosinski, Jeff Clune category:cs.CV cs.AI cs.NE  published:2014-12-05 summary:Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision. version:4
arxiv-1504-00641 | A Probabilistic Theory of Deep Learning | http://arxiv.org/abs/1504.00641 | id:1504.00641 author:Ankit B. Patel, Tan Nguyen, Richard G. Baraniuk category:stat.ML cs.CV cs.LG cs.NE  published:2015-04-02 summary:A grand challenge in machine learning is the development of computational algorithms that match or outperform humans in perceptual inference tasks that are complicated by nuisance variation. For instance, visual object recognition involves the unknown object position, orientation, and scale in object recognition while speech recognition involves the unknown voice pronunciation, pitch, and speed. Recently, a new breed of deep learning algorithms have emerged for high-nuisance inference tasks that routinely yield pattern recognition systems with near- or super-human capabilities. But a fundamental question remains: Why do they work? Intuitions abound, but a coherent framework for understanding, analyzing, and synthesizing deep learning architectures has remained elusive. We answer this question by developing a new probabilistic framework for deep learning based on the Deep Rendering Model: a generative probabilistic model that explicitly captures latent nuisance variation. By relaxing the generative model to a discriminative one, we can recover two of the current leading deep learning systems, deep convolutional neural networks and random decision forests, providing insights into their successes and shortcomings, as well as a principled route to their improvement. version:1
arxiv-1502-03492 | Gradient-based Hyperparameter Optimization through Reversible Learning | http://arxiv.org/abs/1502.03492 | id:1502.03492 author:Dougal Maclaurin, David Duvenaud, Ryan P. Adams category:stat.ML cs.LG  published:2015-02-11 summary:Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum. version:3
arxiv-1504-00593 | The Approximation of the Dissimilarity Projection | http://arxiv.org/abs/1504.00593 | id:1504.00593 author:Emanuele Olivetti, Thien Bao Nguyen, Paolo Avesani category:stat.ML cs.CV  published:2015-04-02 summary:Diffusion magnetic resonance imaging (dMRI) data allow to reconstruct the 3D pathways of axons within the white matter of the brain as a tractography. The analysis of tractographies has drawn attention from the machine learning and pattern recognition communities providing novel challenges such as finding an appropriate representation space for the data. Many of the current learning algorithms require the input to be from a vectorial space. This requirement contrasts with the intrinsic nature of the tractography because its basic elements, called streamlines or tracks, have different lengths and different number of points and for this reason they cannot be directly represented in a common vectorial space. In this work we propose the adoption of the dissimilarity representation which is an Euclidean embedding technique defined by selecting a set of streamlines called prototypes and then mapping any new streamline to the vector of distances from prototypes. We investigate the degree of approximation of this projection under different prototype selection policies and prototype set sizes in order to characterise its use on tractography data. Additionally we propose the use of a scalable approximation of the most effective prototype selection policy that provides fast and accurate dissimilarity approximations of complete tractographies. version:1
arxiv-1504-00580 | Quantum image classification using principal component analysis | http://arxiv.org/abs/1504.00580 | id:1504.00580 author:Mateusz Ostaszewski, Przemysław Sadowski, Piotr Gawron category:quant-ph cs.CV cs.LG  published:2015-04-02 summary:We present a novel quantum algorithm for classification of images. The algorithm is constructed using principal component analysis and von Neuman quantum measurements. In order to apply the algorithm we present a new quantum representation of grayscale images. version:1
arxiv-1401-6354 | Local Identification of Overcomplete Dictionaries | http://arxiv.org/abs/1401.6354 | id:1401.6354 author:Karin Schnass category:cs.IT math.IT stat.ML  published:2014-01-24 summary:This paper presents the first theoretical results showing that stable identification of overcomplete $\mu$-coherent dictionaries $\Phi \in \mathbb{R}^{d\times K}$ is locally possible from training signals with sparsity levels $S$ up to the order $O(\mu^{-2})$ and signal to noise ratios up to $O(\sqrt{d})$. In particular the dictionary is recoverable as the local maximum of a new maximisation criterion that generalises the K-means criterion. For this maximisation criterion results for asymptotic exact recovery for sparsity levels up to $O(\mu^{-1})$ and stable recovery for sparsity levels up to $O(\mu^{-2})$ as well as signal to noise ratios up to $O(\sqrt{d})$ are provided. These asymptotic results translate to finite sample size recovery results with high probability as long as the sample size $N$ scales as $O(K^3dS \tilde \varepsilon^{-2})$, where the recovery precision $\tilde \varepsilon$ can go down to the asymptotically achievable precision. Further, to actually find the local maxima of the new criterion, a very simple Iterative Thresholding and K (signed) Means algorithm (ITKM), which has complexity $O(dKN)$ in each iteration, is presented and its local efficiency is demonstrated in several experiments. version:2
arxiv-1210-5992 | Strong oracle optimality of folded concave penalized estimation | http://arxiv.org/abs/1210.5992 | id:1210.5992 author:Jianqing Fan, Lingzhou Xue, Hui Zou category:math.ST stat.CO stat.ML stat.TH  published:2012-10-22 summary:Folded concave penalization methods have been shown to enjoy the strong oracle property for high-dimensional sparse estimation. However, a folded concave penalization problem usually has multiple local solutions and the oracle property is established only for one of the unknown local solutions. A challenging fundamental issue still remains that it is not clear whether the local optimum computed by a given optimization algorithm possesses those nice theoretical properties. To close this important theoretical gap in over a decade, we provide a unified theory to show explicitly how to obtain the oracle solution via the local linear approximation algorithm. For a folded concave penalized estimation problem, we show that as long as the problem is localizable and the oracle estimator is well behaved, we can obtain the oracle estimator by using the one-step local linear approximation. In addition, once the oracle estimator is obtained, the local linear approximation algorithm converges, namely it produces the same estimator in the next iteration. The general theory is demonstrated by using four classical sparse estimation problems, that is, sparse linear regression, sparse logistic regression, sparse precision matrix estimation and sparse quantile regression. version:4
arxiv-1412-1283 | Convolutional Feature Masking for Joint Object and Stuff Segmentation | http://arxiv.org/abs/1412.1283 | id:1412.1283 author:Jifeng Dai, Kaiming He, Jian Sun category:cs.CV  published:2014-12-03 summary:The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs). The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming. In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and "stuff" (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed. version:4
arxiv-1502-02322 | Rademacher Observations, Private Data, and Boosting | http://arxiv.org/abs/1502.02322 | id:1502.02322 author:Richard Nock, Giorgio Patrini, Arik Friedman category:cs.LG 68Q32 E.4; I.2.6  published:2015-02-09 summary:The minimization of the logistic loss is a popular approach to batch supervised learning. Our paper starts from the surprising observation that, when fitting linear (or kernelized) classifiers, the minimization of the logistic loss is \textit{equivalent} to the minimization of an exponential \textit{rado}-loss computed (i) over transformed data that we call Rademacher observations (rados), and (ii) over the \textit{same} classifier as the one of the logistic loss. Thus, a classifier learnt from rados can be \textit{directly} used to classify \textit{observations}. We provide a learning algorithm over rados with boosting-compliant convergence rates on the \textit{logistic loss} (computed over examples). Experiments on domains with up to millions of examples, backed up by theoretical arguments, display that learning over a small set of random rados can challenge the state of the art that learns over the \textit{complete} set of examples. We show that rados comply with various privacy requirements that make them good candidates for machine learning in a privacy framework. We give several algebraic, geometric and computational hardness results on reconstructing examples from rados. We also show how it is possible to craft, and efficiently learn from, rados in a differential privacy framework. Tests reveal that learning from differentially private rados can compete with learning from random rados, and hence with batch learning from examples, achieving non-trivial privacy vs accuracy tradeoffs. version:2
arxiv-1501-00192 | Learning Parameters for Weighted Matrix Completion via Empirical Estimation | http://arxiv.org/abs/1501.00192 | id:1501.00192 author:Jason Jo category:stat.ML  published:2014-12-31 summary:Recently theoretical guarantees have been obtained for matrix completion in the non-uniform sampling regime. In particular, if the sampling distribution aligns with the underlying matrix's leverage scores, then with high probability nuclear norm minimization will exactly recover the low rank matrix. In this article, we analyze the scenario in which the non-uniform sampling distribution may or may not not align with the underlying matrix's leverage scores. Here we explore learning the parameters for weighted nuclear norm minimization in terms of the empirical sampling distribution. We provide a sufficiency condition for these learned weights which provide an exact recovery guarantee for weighted nuclear norm minimization. It has been established that a specific choice of weights in terms of the true sampling distribution not only allows for weighted nuclear norm minimization to exactly recover the low rank matrix, but also allows for a quantifiable relaxation in the exact recovery conditions. In this article we extend this quantifiable relaxation in exact recovery conditions for a specific choice of weights defined analogously in terms of the empirical distribution as opposed to the true sampling distribution. To accomplish this we employ a concentration of measure bound and a large deviation bound. We also present numerical evidence for the healthy robustness of the weighted nuclear norm minimization algorithm to the choice of empirically learned weights. These numerical experiments show that for a variety of easily computable empirical weights, weighted nuclear norm minimization outperforms unweighted nuclear norm minimization in the non-uniform sampling regime. version:4
arxiv-1504-00430 | Direct l_(2,p)-Norm Learning for Feature Selection | http://arxiv.org/abs/1504.00430 | id:1504.00430 author:Hanyang Peng, Yong Fan category:cs.LG cs.CV  published:2015-04-02 summary:In this paper, we propose a novel sparse learning based feature selection method that directly optimizes a large margin linear classification model sparsity with l_(2,p)-norm (0 < p < 1)subject to data-fitting constraints, rather than using the sparsity as a regularization term. To solve the direct sparsity optimization problem that is non-smooth and non-convex when 0<p<1, we provide an efficient iterative algorithm with proved convergence by converting it to a convex and smooth optimization problem at every iteration step. The proposed algorithm has been evaluated based on publicly available datasets, and extensive comparison experiments have demonstrated that our algorithm could achieve feature selection performance competitive to state-of-the-art algorithms. version:1
arxiv-1412-5758 | Decomposition-Based Domain Adaptation for Real-World Font Recognition | http://arxiv.org/abs/1412.5758 | id:1412.5758 author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV  published:2014-12-18 summary:We present a domain adaption framework to address a domain mismatch between synthetic training and real-world testing data. We demonstrate our method on a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This real-to-synthetic domain gap caused poor generalization to new real data in previous font recognition methods (Chen et al. (2014)). In this paper, we introduce a Convolutional Neural Network decomposition approach, leveraging a large training corpus of synthetic data to obtain effective features for classification. This is done using an adaptation technique based on a Stacked Convolutional Auto-Encoder that exploits a large collection of unlabeled real-world text images combined with synthetic data preprocessed in a specific way. The proposed DeepFont method achieves an accuracy of higher than 80% (top-5) on a new large labeled real-world dataset we collected. version:4
arxiv-1504-00386 | Signatures of Infinity: Nonergodicity and Resource Scaling in Prediction, Complexity, and Learning | http://arxiv.org/abs/1504.00386 | id:1504.00386 author:James P. Crutchfield, Sarah Marzen category:cond-mat.stat-mech cs.IT cs.LG math.IT stat.ML  published:2015-04-01 summary:We introduce a simple analysis of the structural complexity of infinite-memory processes built from random samples of stationary, ergodic finite-memory component processes. Such processes are familiar from the well known multi-arm Bandit problem. We contrast our analysis with computation-theoretic and statistical inference approaches to understanding their complexity. The result is an alternative view of the relationship between predictability, complexity, and learning that highlights the distinct ways in which informational and correlational divergences arise in complex ergodic and nonergodic processes. We draw out consequences for the resource divergences that delineate the structural hierarchy of ergodic processes and for processes that are themselves hierarchical. version:1
arxiv-1504-00377 | Bayesian Clustering of Shapes of Curves | http://arxiv.org/abs/1504.00377 | id:1504.00377 author:Zhengwu Zhang, Debdeep Pati, Anuj Srivastava category:stat.ML cs.LG  published:2015-04-01 summary:Unsupervised clustering of curves according to their shapes is an important problem with broad scientific applications. The existing model-based clustering techniques either rely on simple probability models (e.g., Gaussian) that are not generally valid for shape analysis or assume the number of clusters. We develop an efficient Bayesian method to cluster curve data using an elastic shape metric that is based on joint registration and comparison of shapes of curves. The elastic-inner product matrix obtained from the data is modeled using a Wishart distribution whose parameters are assigned carefully chosen prior distributions to allow for automatic inference on the number of clusters. Posterior is sampled through an efficient Markov chain Monte Carlo procedure based on the Chinese restaurant process to infer (1) the posterior distribution on the number of clusters, and (2) clustering configuration of shapes. This method is demonstrated on a variety of synthetic data and real data examples on protein structure analysis, cell shape analysis in microscopy images, and clustering of shaped from MPEG7 database. version:1
arxiv-1410-1606 | Hierarchical Sparse and Collaborative Low-Rank Representation for Emotion Recognition | http://arxiv.org/abs/1410.1606 | id:1410.1606 author:Xiang Xiang, Minh Dao, Gregory D. Hager, Trac D. Tran category:cs.CV  published:2014-10-07 summary:In this paper, we design a Collaborative-Hierarchical Sparse and Low-Rank (C-HiSLR) model that is natural for recognizing human emotion in visual data. Previous attempts require explicit expression components, which are often unavailable and difficult to recover. Instead, our model exploits the lowrank property over expressive facial frames and rescue inexact sparse representations by incorporating group sparsity. For the CK+ dataset, C-HiSLR on raw expressive faces performs as competitive as the Sparse Representation based Classification (SRC) applied on manually prepared emotions. C-HiSLR performs even better than SRC in terms of true positive rate. version:2
arxiv-1503-02445 | Representation Learning with Deep Extreme Learning Machines for Efficient Image Set Classification | http://arxiv.org/abs/1503.02445 | id:1503.02445 author:Muhammad Uzair, Faisal Shafait, Bernard Ghanem, Ajmal Mian category:cs.CV  published:2015-03-09 summary:Efficient and accurate joint representation of a collection of images, that belong to the same class, is a major research challenge for practical image set classification. Existing methods either make prior assumptions about the data structure, or perform heavy computations to learn structure from the data itself. In this paper, we propose an efficient image set representation that does not make any prior assumptions about the structure of the underlying data. We learn the non-linear structure of image sets with Deep Extreme Learning Machines (DELM) that are very efficient and generalize well even on a limited number of training samples. Extensive experiments on a broad range of public datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm consistently outperforms state-of-the-art image set classification methods both in terms of speed and accuracy. version:3
arxiv-1504-00154 | A New Repair Operator for Multi-objective Evolutionary Algorithm in Constrained Optimization Problems | http://arxiv.org/abs/1504.00154 | id:1504.00154 author:Zhun Fan, Wenji Li, Xinye Cai, Huibiao Lin, Shuxiang Xie, Erik Goodman category:cs.NE 68Q01 G.1.6  published:2015-04-01 summary:In this paper, we design a set of multi-objective constrained optimization problems (MCOPs) and propose a new repair operator to address them. The proposed repair operator is used to fix the solutions that violate the box constraints. More specifically, it employs a reversed correction strategy that can effectively avoid the population falling into local optimum. In addition, we integrate the proposed repair operator into two classical multi-objective evolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator is compared with other two kinds of commonly used repair operators on benchmark problems CTPs and MCOPs. The experiment results demonstrate that our proposed approach is very effective in terms of convergence and diversity. version:1
arxiv-1503-08985 | Iterative Regularization for Learning with Convex Loss Functions | http://arxiv.org/abs/1503.08985 | id:1503.08985 author:Junhong Lin, Lorenzo Rosasco, Ding-Xuan Zhou category:stat.ML math.OC  published:2015-03-31 summary:We consider the problem of supervised learning with convex loss functions and propose a new form of iterative regularization based on the subgradient method. Unlike other regularization approaches, in iterative regularization no constraint or penalization is considered, and generalization is achieved by (early) stopping an empirical iteration. We consider a nonparametric setting, in the framework of reproducing kernel Hilbert spaces, and prove finite sample bounds on the excess risk under general regularity conditions. Our study provides a new class of efficient regularized learning algorithms and gives insights on the interplay between statistics and optimization in machine learning. version:2
arxiv-1410-0582 | Multidimensional Digital Smoothing Filters for Target Detection | http://arxiv.org/abs/1410.0582 | id:1410.0582 author:Hugh L. Kennedy category:cs.CV  published:2014-10-02 summary:Recursive, causal and non-causal, multidimensional digital filters, with infinite impulse responses and maximally flat magnitude and delay responses in the low-frequency region, are designed to negate correlated clutter and interference in the background and to accumulate power due to dim targets in the foreground of a surveillance sensor. Expressions relating mean impulse-response duration, frequency selectivity and group delay, to low-order linear-difference-equation coefficients are derived using discrete Laguerre polynomials and discounted least-squares regression, then verified through simulation. version:5
arxiv-1504-00110 | The Libra Toolkit for Probabilistic Models | http://arxiv.org/abs/1504.00110 | id:1504.00110 author:Daniel Lowd, Amirmohammad Rooshenas category:cs.LG cs.AI  published:2015-04-01 summary:The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry. version:1
arxiv-1302-5374 | A Weight-coded Evolutionary Algorithm for the Multidimensional Knapsack Problem | http://arxiv.org/abs/1302.5374 | id:1302.5374 author:Quan Yuan, Zhixin Yang category:cs.NE math.OC 90B50  published:2013-02-21 summary:A revised weight-coded evolutionary algorithm (RWCEA) is proposed for solving multidimensional knapsack problems. This RWCEA uses a new decoding method and incorporates a heuristic method in initialization. Computational results show that the RWCEA performs better than a weight-coded evolutionary algorithm proposed by Raidl (1999) and to some existing benchmarks, it can yield better results than the ones reported in the OR-library. version:4
arxiv-1504-00083 | A Theory of Feature Learning | http://arxiv.org/abs/1504.00083 | id:1504.00083 author:Brendan van Rooyen, Robert C. Williamson category:stat.ML cs.LG  published:2015-04-01 summary:Feature Learning aims to extract relevant information contained in data sets in an automated fashion. It is driving force behind the current deep learning trend, a set of methods that have had widespread empirical success. What is lacking is a theoretical understanding of different feature learning schemes. This work provides a theoretical framework for feature learning and then characterizes when features can be learnt in an unsupervised fashion. We also provide means to judge the quality of features via rate-distortion theory and its generalizations. version:1
arxiv-1504-00064 | Crowdsourcing Feature Discovery via Adaptively Chosen Comparisons | http://arxiv.org/abs/1504.00064 | id:1504.00064 author:James Y. Zou, Kamalika Chaudhuri, Adam Tauman Kalai category:stat.ML cs.LG  published:2015-03-31 summary:We introduce an unsupervised approach to efficiently discover the underlying features in a data set via crowdsourcing. Our queries ask crowd members to articulate a feature common to two out of three displayed examples. In addition we also ask the crowd to provide binary labels to the remaining examples based on the discovered features. The triples are chosen adaptively based on the labels of the previously discovered features on the data set. In two natural models of features, hierarchical and independent, we show that a simple adaptive algorithm, using "two-out-of-three" similarity queries, recovers all features with less labor than any nonadaptive algorithm. Experimental results validate the theoretical findings. version:1
arxiv-1504-00052 | Improved Error Bounds Based on Worst Likely Assignments | http://arxiv.org/abs/1504.00052 | id:1504.00052 author:Eric Bax category:stat.ML cs.IT cs.LG math.IT math.PR  published:2015-03-31 summary:Error bounds based on worst likely assignments use permutation tests to validate classifiers. Worst likely assignments can produce effective bounds even for data sets with 100 or fewer training examples. This paper introduces a statistic for use in the permutation tests of worst likely assignments that improves error bounds, especially for accurate classifiers, which are typically the classifiers of interest. version:1
arxiv-1504-00045 | Weakly Supervised Learning of Objects, Attributes and their Associations | http://arxiv.org/abs/1504.00045 | id:1504.00045 author:Zhiyuan Shi, Yongxin Yang, Timothy M. Hospedales, Tao Xiang category:cs.CV  published:2015-03-31 summary:When humans describe images they tend to use combinations of nouns and adjectives, corresponding to objects and their associated attributes respectively. To generate such a description automatically, one needs to model objects, attributes and their associations. Conventional methods require strong annotation of object and attribute locations, making them less scalable. In this paper, we model object-attribute associations from weakly labelled images, such as those widely available on media sharing sites (e.g. Flickr), where only image-level labels (either object or attributes) are given, without their locations and associations. This is achieved by introducing a novel weakly supervised non-parametric Bayesian model. Once learned, given a new image, our model can describe the image, including objects, attributes and their associations, as well as their locations and segmentation. Extensive experiments on benchmark datasets demonstrate that our weakly supervised model performs at par with strongly supervised models on tasks such as image description and retrieval based on object-attribute associations. version:1
arxiv-1504-00028 | Real-World Font Recognition Using Deep Network and Domain Adaptation | http://arxiv.org/abs/1504.00028 | id:1504.00028 author:Zhangyang Wang, Jianchao Yang, Hailin Jin, Eli Shechtman, Aseem Agarwala, Jonathan Brandt, Thomas S. Huang category:cs.CV cs.LG  published:2015-03-31 summary:We address a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This real-to-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)). In this paper, we refer to Convolutional Neural Networks, and use an adaptation technique based on a Stacked Convolutional Auto-Encoder that exploits unlabeled real-world images combined with synthetic data. The proposed method achieves an accuracy of higher than 80% (top-5) on a real-world dataset. version:1
arxiv-1412-6177 | Example Selection For Dictionary Learning | http://arxiv.org/abs/1412.6177 | id:1412.6177 author:Tomoki Tsuchida, Garrison W. Cottrell category:cs.LG cs.AI stat.ML  published:2014-12-18 summary:In unsupervised learning, an unbiased uniform sampling strategy is typically used, in order that the learned features faithfully encode the statistical structure of the training data. In this work, we explore whether active example selection strategies - algorithms that select which examples to use, based on the current estimate of the features - can accelerate learning. Specifically, we investigate effects of heuristic and saliency-inspired selection algorithms on the dictionary learning task with sparse activations. We show that some selection algorithms do improve the speed of learning, and we speculate on why they might work. version:3
arxiv-1503-09144 | Towards Using Machine Translation Techniques to Induce Multilingual Lexica of Discourse Markers | http://arxiv.org/abs/1503.09144 | id:1503.09144 author:António Lopes, David Martins de Matos, Vera Cabarrão, Ricardo Ribeiro, Helena Moniz, Isabel Trancoso, Ana Isabel Mata category:cs.CL I.2.7  published:2015-03-31 summary:Discourse markers are universal linguistic events subject to language variation. Although an extensive literature has already reported language specific traits of these events, little has been said on their cross-language behavior and on building an inventory of multilingual lexica of discourse markers. This work describes new methods and approaches for the description, classification, and annotation of discourse markers in the specific domain of the Europarl corpus. The study of discourse markers in the context of translation is crucial due to the idiomatic nature of these structures. Multilingual lexica together with the functional analysis of such structures are useful tools for the hard task of translating discourse markers into possible equivalents from one language to another. Using Daniel Marcu's validated discourse markers for English, extracted from the Brown Corpus, our purpose is to build multilingual lexica of discourse markers for other languages, based on machine translation techniques. The major assumption in this study is that the usage of a discourse marker is independent of the language, i.e., the rhetorical function of a discourse marker in a sentence in one language is equivalent to the rhetorical function of the same discourse marker in another language. version:1
arxiv-1503-09129 | Encoding Spike Patterns in Multilayer Spiking Neural Networks | http://arxiv.org/abs/1503.09129 | id:1503.09129 author:Brian Gardner, Ioana Sporea, André Grüning category:cs.NE  published:2015-03-31 summary:Information encoding in the nervous system is supported through the precise spike-timings of neurons; however, an understanding of the underlying processes by which such representations are formed in the first place remains unclear. Here we examine how networks of spiking neurons can learn to encode for input patterns using a fully temporal coding scheme. To this end, we introduce a learning rule for spiking networks containing hidden neurons which optimizes the likelihood of generating desired output spiking patterns. We show the proposed learning rule allows for a large number of accurate input-output spike pattern mappings to be learnt, which outperforms other existing learning rules for spiking neural networks: both in the number of mappings that can be learnt as well as the complexity of spike train encodings that can be utilised. The learning rule is successful even in the presence of input noise, is demonstrated to solve the linearly non-separable XOR computation and generalizes well on an example dataset. We further present a biologically plausible implementation of backpropagated learning in multilayer spiking networks, and discuss the neural mechanisms that might underlie its function. Our approach contributes both to a systematic understanding of how pattern encodings might take place in the nervous system, and a learning rule that displays strong technical capability. version:1
arxiv-1412-7210 | Denoising autoencoder with modulated lateral connections learns invariant representations of natural images | http://arxiv.org/abs/1412.7210 | id:1412.7210 author:Antti Rasmus, Tapani Raiko, Harri Valpola category:cs.NE cs.CV cs.LG stat.ML  published:2014-12-22 summary:Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings. version:4
arxiv-1403-7877 | ROML: A Robust Feature Correspondence Approach for Matching Objects in A Set of Images | http://arxiv.org/abs/1403.7877 | id:1403.7877 author:Kui Jia, Tsung-Han Chan, Zinan Zeng, Shenghua Gao, Gang Wang, Tianzhu Zhang, Yi Ma category:cs.CV  published:2014-03-31 summary:Feature-based object matching is a fundamental problem for many applications in computer vision, such as object recognition, 3D reconstruction, tracking, and motion segmentation. In this work, we consider simultaneously matching object instances in a set of images, where both inlier and outlier features are extracted. The task is to identify the inlier features and establish their consistent correspondences across the image set. This is a challenging combinatorial problem, and the problem complexity grows exponentially with the image number. To this end, we propose a novel framework, termed ROML, to address this problem. ROML optimizes simultaneously a partial permutation matrix (PPM) for each image, and feature correspondences are established by the obtained PPMs. Two of our key contributions are summarized as follows. (1) We formulate the problem as rank and sparsity minimization for PPM optimization, and treat simultaneous optimization of multiple PPMs as a regularized consensus problem in the context of distributed optimization. (2) We use the ADMM method to solve the thus formulated ROML problem, in which a subproblem associated with a single PPM optimization appears to be a difficult integer quadratic program (IQP). We prove that under wildly applicable conditions, this IQP is equivalent to a linear sum assignment problem (LSAP), which can be efficiently solved to an exact solution. Extensive experiments on rigid/non-rigid object matching, matching instances of a common object category, and common object localization show the efficacy of our proposed method. version:2
arxiv-1412-6452 | Algorithmic Robustness for Learning via $(ε, γ, τ)$-Good Similarity Functions | http://arxiv.org/abs/1412.6452 | id:1412.6452 author:Maria-Irina Nicolae, Marc Sebban, Amaury Habrard, Éric Gaussier, Massih-Reza Amini category:cs.LG  published:2014-12-19 summary:The notion of metric plays a key role in machine learning problems such as classification, clustering or ranking. However, it is worth noting that there is a severe lack of theoretical guarantees that can be expected on the generalization capacity of the classifier associated to a given metric. The theoretical framework of $(\epsilon, \gamma, \tau)$-good similarity functions (Balcan et al., 2008) has been one of the first attempts to draw a link between the properties of a similarity function and those of a linear classifier making use of it. In this paper, we extend and complete this theory by providing a new generalization bound for the associated classifier based on the algorithmic robustness framework. version:3
arxiv-1406-7498 | Thompson Sampling for Learning Parameterized Markov Decision Processes | http://arxiv.org/abs/1406.7498 | id:1406.7498 author:Aditya Gopalan, Shie Mannor category:stat.ML cs.LG  published:2014-06-29 summary:We consider reinforcement learning in parameterized Markov Decision Processes (MDPs), where the parameterization may induce correlation across transition probabilities or rewards. Consequently, observing a particular state transition might yield useful information about other, unobserved, parts of the MDP. We present a version of Thompson sampling for parameterized reinforcement learning problems, and derive a frequentist regret bound for priors over general parameter spaces. The result shows that the number of instants where suboptimal actions are chosen scales logarithmically with time, with high probability. It holds for prior distributions that put significant probability near the true model, without any additional, specific closed-form structure such as conjugate or product-form priors. The constant factor in the logarithmic scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler geometry of the parameter space. version:3
arxiv-1503-08873 | Fast Label Embeddings for Extremely Large Output Spaces | http://arxiv.org/abs/1503.08873 | id:1503.08873 author:Paul Mineiro, Nikos Karampatziakis category:cs.LG  published:2015-03-30 summary:Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results. version:1
arxiv-1503-08855 | Decentralized learning for wireless communications and networking | http://arxiv.org/abs/1503.08855 | id:1503.08855 author:Georgios B. Giannakis, Qing Ling, Gonzalo Mateos, Ioannis D. Schizas, Hao Zhu category:math.OC cs.IT cs.LG cs.MA cs.SY math.IT stat.ML  published:2015-03-30 summary:This chapter deals with decentralized learning algorithms for in-network processing of graph-valued data. A generic learning problem is formulated and recast into a separable form, which is iteratively minimized using the alternating-direction method of multipliers (ADMM) so as to gain the desired degree of parallelization. Without exchanging elements from the distributed training sets and keeping inter-node communications at affordable levels, the local (per-node) learners consent to the desired quantity inferred globally, meaning the one obtained if the entire training data set were centrally available. Impact of the decentralized learning framework to contemporary wireless communications and networking tasks is illustrated through case studies including target tracking using wireless sensor networks, unveiling Internet traffic anomalies, power system state estimation, as well as spectrum cartography for wireless cognitive radio networks. version:1
arxiv-1503-08853 | Reconciling saliency and object center-bias hypotheses in explaining free-viewing fixations | http://arxiv.org/abs/1503.08853 | id:1503.08853 author:Ali Borji, James Tanner category:cs.CV  published:2015-03-30 summary:Predicting where people look in natural scenes has attracted a lot of interest in computer vision and computational neuroscience over the past two decades. Two seemingly contrasting categories of cues have been proposed to influence where people look: \textit{low-level image saliency} and \textit{high-level semantic information}. Our first contribution is to take a detailed look at these cues to confirm the hypothesis proposed by Henderson~\cite{henderson1993eye} and Nuthmann \& Henderson~\cite{nuthmann2010object} that observers tend to look at the center of objects. We analyzed fixation data for scene free-viewing over 17 observers on 60 fully annotated images with various types of objects. Images contained different types of scenes, such as natural scenes, line drawings, and 3D rendered scenes. Our second contribution is to propose a simple combined model of low-level saliency and object center-bias that outperforms each individual component significantly over our data, as well as on the OSIE dataset by Xu et al.~\cite{xu2014predicting}. The results reconcile saliency with object center-bias hypotheses and highlight that both types of cues are important in guiding fixations. Our work opens new directions to understand strategies that humans use in observing scenes and objects, and demonstrates the construction of combined models of low-level saliency and high-level object-based information. version:1
arxiv-1503-08843 | Globally Tuned Cascade Pose Regression via Back Propagation with Application in 2D Face Pose Estimation and Heart Segmentation in 3D CT Images | http://arxiv.org/abs/1503.08843 | id:1503.08843 author:Peng Sun, James K. Min, Guanglei Xiong category:cs.CV  published:2015-03-30 summary:Recently, a successful pose estimation algorithm, called Cascade Pose Regression (CPR), was proposed in the literature. Trained over Pose Index Feature, CPR is a regressor ensemble that is similar to Boosting. In this paper we show how CPR can be represented as a Neural Network. Specifically, we adopt a Graph Transformer Network (GTN) representation and accordingly train CPR with Back Propagation (BP) that permits globally tuning. In contrast, previous CPR literature only took a layer wise training without any post fine tuning. We empirically show that global training with BP outperforms layer-wise (pre-)training. Our CPR-GTN adopts a Multi Layer Percetron as the regressor, which utilized sparse connection to learn local image feature representation. We tested the proposed CPR-GTN on 2D face pose estimation problem as in previous CPR literature. Besides, we also investigated the possibility of extending CPR-GTN to 3D pose estimation by doing experiments using 3D Computed Tomography dataset for heart segmentation. version:1
arxiv-1503-02725 | Deep Hierarchical Parsing for Semantic Segmentation | http://arxiv.org/abs/1503.02725 | id:1503.02725 author:Abhishek Sharma, Oncel Tuzel, David W. Jacobs category:cs.CV  published:2015-03-09 summary:This paper proposes a learning-based approach to scene parsing inspired by the deep Recursive Context Propagation Network (RCPN). RCPN is a deep feed-forward neural network that utilizes the contextual information from the entire image, through bottom-up followed by top-down context propagation via random binary parse trees. This improves the feature representation of every super-pixel in the image for better classification into semantic categories. We analyze RCPN and propose two novel contributions to further improve the model. We first analyze the learning of RCPN parameters and discover the presence of bypass error paths in the computation graph of RCPN that can hinder contextual propagation. We propose to tackle this problem by including the classification loss of the internal nodes of the random parse trees in the original RCPN loss function. Secondly, we use an MRF on the parse tree nodes to model the hierarchical dependency present in the output. Both modifications provide performance boosts over the original RCPN and the new system achieves state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler urban datasets. version:2
arxiv-1206-0823 | Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results | http://arxiv.org/abs/1206.0823 | id:1206.0823 author:Yudong Chen, Constantine Caramanis category:math.ST cs.IT math.IT stat.ML stat.TH  published:2012-06-05 summary:Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. This paper develops efficient OMP-like algorithms to deal with precisely this setting. Our algorithms are as efficient as OMP, and improve on the best-known results for missing and noisy data in regression, both in the high-dimensional setting where we seek to recover a sparse vector from only a few measurements, and in the classical low-dimensional setting where we recover an unstructured regressor. In the high-dimensional setting, our support-recovery algorithm requires no knowledge of even the statistics of the noise. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise. version:2
arxiv-1309-6964 | Online Algorithms for Factorization-Based Structure from Motion | http://arxiv.org/abs/1309.6964 | id:1309.6964 author:Ryan Kennedy, Laura Balzano, Stephen J. Wright, Camillo J. Taylor category:cs.CV  published:2013-09-26 summary:We present a family of online algorithms for real-time factorization-based structure from motion, leveraging a relationship between incremental singular value decomposition and recently proposed methods for online matrix completion. Our methods are orders of magnitude faster than previous state of the art, can handle missing data and a variable number of feature points, and are robust to noise and sparse outliers. We demonstrate our methods on both real and synthetic sequences and show that they perform well in both online and batch settings. We also provide an implementation which is able to produce 3D models in real time using a laptop with a webcam. version:3
arxiv-1503-08727 | A Parzen-based distance between probability measures as an alternative of summary statistics in Approximate Bayesian Computation | http://arxiv.org/abs/1503.08727 | id:1503.08727 author:Carlos D. Zuluaga, Edgar A. Valencia, Mauricio A. Álvarez category:stat.ML  published:2015-03-30 summary:Approximate Bayesian Computation (ABC) are likelihood-free Monte Carlo methods. ABC methods use a comparison between simulated data, using different parameters drew from a prior distribution, and observed data. This comparison process is based on computing a distance between the summary statistics from the simulated data and the observed data. For complex models, it is usually difficult to define a methodology for choosing or constructing the summary statistics. Recently, a nonparametric ABC has been proposed, that uses a dissimilarity measure between discrete distributions based on empirical kernel embeddings as an alternative for summary statistics. The nonparametric ABC outperforms other methods including ABC, kernel ABC or synthetic likelihood ABC. However, it assumes that the probability distributions are discrete, and it is not robust when dealing with few observations. In this paper, we propose to apply kernel embeddings using an smoother density estimator or Parzen estimator for comparing the empirical data distributions, and computing the ABC posterior. Synthetic data and real data were used to test the Bayesian inference of our method. We compare our method with respect to state-of-the-art methods, and demonstrate that our method is a robust estimator of the posterior distribution in terms of the number of observations. version:1
arxiv-1412-6286 | Regression with Linear Factored Functions | http://arxiv.org/abs/1412.6286 | id:1412.6286 author:Wendelin Böhmer, Klaus Obermayer category:cs.LG stat.ML  published:2014-12-19 summary:Many applications that use empirically estimated functions face a curse of dimensionality, because the integrals over most function classes must be approximated by sampling. This paper introduces a novel regression-algorithm that learns linear factored functions (LFF). This class of functions has structural properties that allow to analytically solve certain integrals and to calculate point-wise products. Applications like belief propagation and reinforcement learning can exploit these properties to break the curse and speed up computation. We derive a regularized greedy optimization scheme, that learns factored basis functions during training. The novel regression algorithm performs competitively to Gaussian processes on benchmark tasks, and the learned LFF functions are with 4-9 factored basis functions on average very compact. version:3
arxiv-1503-08639 | Sparse plus low-rank autoregressive identification in neuroimaging time series | http://arxiv.org/abs/1503.08639 | id:1503.08639 author:Raphaël Liégeois, Bamdev Mishra, Mattia Zorzi, Rodolphe Sepulchre category:cs.LG cs.SY  published:2015-03-30 summary:This paper considers the problem of identifying multivariate autoregressive (AR) sparse plus low-rank graphical models. Based on the corresponding problem formulation recently presented, we use the alternating direction method of multipliers (ADMM) to efficiently solve it and scale it to sizes encountered in neuroimaging applications. We apply this decomposition on synthetic and real neuroimaging datasets with a specific focus on the information encoded in the low-rank structure of our model. In particular, we illustrate that this information captures the spatio-temporal structure of the original data, generalizing classical component analysis approaches. version:1
arxiv-1312-6849 | Speech Recognition Front End Without Information Loss | http://arxiv.org/abs/1312.6849 | id:1312.6849 author:Matthew Ager, Zoran Cvetkovic, Peter Sollich category:cs.CL cs.CV cs.LG  published:2013-12-24 summary:Speech representation and modelling in high-dimensional spaces of acoustic waveforms, or a linear transformation thereof, is investigated with the aim of improving the robustness of automatic speech recognition to additive noise. The motivation behind this approach is twofold: (i) the information in acoustic waveforms that is usually removed in the process of extracting low-dimensional features might aid robust recognition by virtue of structured redundancy analogous to channel coding, (ii) linear feature domains allow for exact noise adaptation, as opposed to representations that involve non-linear processing which makes noise adaptation challenging. Thus, we develop a generative framework for phoneme modelling in high-dimensional linear feature domains, and use it in phoneme classification and recognition tasks. Results show that classification and recognition in this framework perform better than analogous PLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional and MFCC features at the likelihood level performs uniformly better than either of the individual representations across all noise levels. version:2
arxiv-1503-08581 | LSHTC: A Benchmark for Large-Scale Text Classification | http://arxiv.org/abs/1503.08581 | id:1503.08581 author:Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, Patrick Galinari category:cs.IR cs.CL cs.LG  published:2015-03-30 summary:LSHTC is a series of challenges which aims to assess the performance of classification systems in large-scale classification in a a large number of classes (up to hundreds of thousands). This paper describes the dataset that have been released along the LSHTC series. The paper details the construction of the datsets and the design of the tracks as well as the evaluation measures that we implemented and a quick overview of the results. All of these datasets are available online and runs may still be submitted on the online server of the challenges. version:1
arxiv-1503-08167 | Normalization of Non-Standard Words in Croatian Texts | http://arxiv.org/abs/1503.08167 | id:1503.08167 author:Slobodan Beliga, Miran Pobar, Sanda Martinčić-Ipšić category:cs.CL  published:2015-03-27 summary:This paper presents text normalization which is an integral part of any text-to-speech synthesis system. Text normalization is a set of methods with a task to write non-standard words, like numbers, dates, times, abbreviations, acronyms and the most common symbols, in their full expanded form are presented. The whole taxonomy for classification of non-standard words in Croatian language together with rule-based normalization methods combined with a lookup dictionary are proposed. Achieved token rate for normalization of Croatian texts is 95%, where 80% of expanded words are in correct morphological form. version:2
arxiv-1307-3301 | Optimal Bounds on Approximation of Submodular and XOS Functions by Juntas | http://arxiv.org/abs/1307.3301 | id:1307.3301 author:Vitaly Feldman, Jan Vondrak category:cs.DS cs.CC cs.LG  published:2013-07-12 summary:We investigate the approximability of several classes of real-valued functions by functions of a small number of variables ({\em juntas}). Our main results are tight bounds on the number of variables required to approximate a function $f:\{0,1\}^n \rightarrow [0,1]$ within $\ell_2$-error $\epsilon$ over the uniform distribution: 1. If $f$ is submodular, then it is $\epsilon$-close to a function of $O(\frac{1}{\epsilon^2} \log \frac{1}{\epsilon})$ variables. This is an exponential improvement over previously known results. We note that $\Omega(\frac{1}{\epsilon^2})$ variables are necessary even for linear functions. 2. If $f$ is fractionally subadditive (XOS) it is $\epsilon$-close to a function of $2^{O(1/\epsilon^2)}$ variables. This result holds for all functions with low total $\ell_1$-influence and is a real-valued analogue of Friedgut's theorem for boolean functions. We show that $2^{\Omega(1/\epsilon)}$ variables are necessary even for XOS functions. As applications of these results, we provide learning algorithms over the uniform distribution. For XOS functions, we give a PAC learning algorithm that runs in time $2^{poly(1/\epsilon)} poly(n)$. For submodular functions we give an algorithm in the more demanding PMAC learning model (Balcan and Harvey, 2011) which requires a multiplicative $1+\gamma$ factor approximation with probability at least $1-\epsilon$ over the target distribution. Our uniform distribution algorithm runs in time $2^{poly(1/(\gamma\epsilon))} poly(n)$. This is the first algorithm in the PMAC model that over the uniform distribution can achieve a constant approximation factor arbitrarily close to 1 for all submodular functions. As follows from the lower bounds in (Feldman et al., 2013) both of these algorithms are close to optimal. We also give applications for proper learning, testing and agnostic learning with value queries of these classes. version:3
arxiv-1503-08542 | Nonparametric Relational Topic Models through Dependent Gamma Processes | http://arxiv.org/abs/1503.08542 | id:1503.08542 author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML cs.CL cs.IR cs.LG  published:2015-03-30 summary:Traditional Relational Topic Models provide a way to discover the hidden topics from a document network. Many theoretical and practical tasks, such as dimensional reduction, document clustering, link prediction, benefit from this revealed knowledge. However, existing relational topic models are based on an assumption that the number of hidden topics is known in advance, and this is impractical in many real-world applications. Therefore, in order to relax this assumption, we propose a nonparametric relational topic model in this paper. Instead of using fixed-dimensional probability distributions in its generative model, we use stochastic processes. Specifically, a gamma process is assigned to each document, which represents the topic interest of this document. Although this method provides an elegant solution, it brings additional challenges when mathematically modeling the inherent network structure of typical document network, i.e., two spatially closer documents tend to have more similar topics. Furthermore, we require that the topics are shared by all the documents. In order to resolve these challenges, we use a subsampling strategy to assign each document a different gamma process from the global gamma process, and the subsampling probabilities of documents are assigned with a Markov Random Field constraint that inherits the document network structure. Through the designed posterior inference algorithm, we can discover the hidden topics and its number simultaneously. Experimental results on both synthetic and real-world network datasets demonstrate the capabilities of learning the hidden topics and, more importantly, the number of topics. version:1
arxiv-1503-08535 | Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process | http://arxiv.org/abs/1503.08535 | id:1503.08535 author:Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo category:stat.ML cs.IR cs.LG  published:2015-03-30 summary:Incorporating the side information of text corpus, i.e., authors, time stamps, and emotional tags, into the traditional text mining models has gained significant interests in the area of information retrieval, statistical natural language processing, and machine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors's interests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) model to resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determine the number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in order to capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma process that accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm with each conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets show the capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topics simultaneously. version:1
arxiv-1503-01800 | EmoNets: Multimodal deep learning approaches for emotion recognition in video | http://arxiv.org/abs/1503.01800 | id:1503.01800 author:Samira Ebrahimi Kahou, Xavier Bouthillier, Pascal Lamblin, Caglar Gulcehre, Vincent Michalski, Kishore Konda, Sébastien Jean, Pierre Froumenty, Yann Dauphin, Nicolas Boulanger-Lewandowski, Raul Chandias Ferrari, Mehdi Mirza, David Warde-Farley, Aaron Courville, Pascal Vincent, Roland Memisevic, Christopher Pal, Yoshua Bengio category:cs.LG cs.CV  published:2015-03-05 summary:The task of the emotion recognition in the wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based "bag-of-mouths" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 EmotiW challenge and achieved a test set accuracy of 47.67% on the 2014 dataset. version:2
arxiv-1503-08479 | Active Authentication on Mobile Devices via Stylometry, Application Usage, Web Browsing, and GPS Location | http://arxiv.org/abs/1503.08479 | id:1503.08479 author:Lex Fridman, Steven Weber, Rachel Greenstadt, Moshe Kam category:cs.CR stat.ML  published:2015-03-29 summary:Active authentication is the problem of continuously verifying the identity of a person based on behavioral aspects of their interaction with a computing device. In this study, we collect and analyze behavioral biometrics data from 200subjects, each using their personal Android mobile device for a period of at least 30 days. This dataset is novel in the context of active authentication due to its size, duration, number of modalities, and absence of restrictions on tracked activity. The geographical colocation of the subjects in the study is representative of a large closed-world environment such as an organization where the unauthorized user of a device is likely to be an insider threat: coming from within the organization. We consider four biometric modalities: (1) text entered via soft keyboard, (2) applications used, (3) websites visited, and (4) physical location of the device as determined from GPS (when outdoors) or WiFi (when indoors). We implement and test a classifier for each modality and organize the classifiers as a parallel binary decision fusion architecture. We are able to characterize the performance of the system with respect to intruder detection time and to quantify the contribution of each modality to the overall performance. version:1
arxiv-1412-8380 | A simple coding for cross-domain matching with dimension reduction via spectral graph embedding | http://arxiv.org/abs/1412.8380 | id:1412.8380 author:Hidetoshi Shimodaira category:stat.ML cs.CV cs.LG  published:2014-12-29 summary:Data vectors are obtained from multiple domains. They are feature vectors of images or vector representations of words. Domains may have different numbers of data vectors with different dimensions. These data vectors from multiple domains are projected to a common space by linear transformations in order to search closely related vectors across domains. We would like to find projection matrices to minimize distances between closely related data vectors. This formulation of cross-domain matching is regarded as an extension of the spectral graph embedding to multi-domain setting, and it includes several multivariate analysis methods of statistics such as multiset canonical correlation analysis, correspondence analysis, and principal component analysis. Similar approaches are very popular recently in pattern recognition and vision. In this paper, instead of proposing a novel method, we will introduce an embarrassingly simple idea of coding the data vectors for explaining all the above mentioned approaches. A data vector is concatenated with zero vectors from all other domains to make an augmented vector. The cross-domain matching is solved by applying the single-domain version of spectral graph embedding to these augmented vectors of all the domains. An interesting connection to the classical associative memory model of neural networks is also discussed by noticing a coding for association. A cross-validation method for choosing the dimension of the common space and a regularization parameter will be discussed in an illustrative numerical example. version:2
arxiv-1503-08818 | Founding Digital Currency on Imprecise Commodity | http://arxiv.org/abs/1503.08818 | id:1503.08818 author:Zimu Yuan, Zhiwei Xu category:cs.CY cs.LG  published:2015-03-29 summary:Current digital currency schemes provide instantaneous exchange on precise commodity, in which "precise" means a buyer can possibly verify the function of the commodity without error. However, imprecise commodities, e.g. statistical data, with error existing are abundant in digital world. Existing digital currency schemes do not offer a mechanism to help the buyer for payment decision on precision of commodity, which may lead the buyer to a dilemma between having to buy and being unconfident. In this paper, we design a currency schemes IDCS for imprecise digital commodity. IDCS completes a trade in three stages of handshake between a buyer and providers. We present an IDCS prototype implementation that assigns weights on the trustworthy of the providers, and calculates a confidence level for the buyer to decide the quality of a imprecise commodity. In experiment, we characterize the performance of IDCS prototype under varying impact factors. version:1
arxiv-1402-1939 | Maximum Entropy, Word-Frequency, Chinese Characters, and Multiple Meanings | http://arxiv.org/abs/1402.1939 | id:1402.1939 author:Xiao-Yong Yan, Petter Minnhagen category:physics.soc-ph cs.CL  published:2014-02-09 summary:The word-frequency distribution of a text written by an author is well accounted for by a maximum entropy distribution, the RGF (random group formation)-prediction. The RGF-distribution is completely determined by the a priori values of the total number of words in the text (M), the number of distinct words (N) and the number of repetitions of the most common word (k_max). It is here shown that this maximum entropy prediction also describes a text written in Chinese characters. In particular it is shown that although the same Chinese text written in words and Chinese characters have quite differently shaped distributions, they are nevertheless both well predicted by their respective three a priori characteristic values. It is pointed out that this is analogous to the change in the shape of the distribution when translating a given text to another language. Another consequence of the RGF-prediction is that taking a part of a long text will change the input parameters (M, N, k_max) and consequently also the shape of the frequency distribution. This is explicitly confirmed for texts written in Chinese characters. Since the RGF-prediction has no system-specific information beyond the three a priori values (M, N, k_max), any specific language characteristic has to be sought in systematic deviations from the RGF-prediction and the measured frequencies. One such systematic deviation is identified and, through a statistical information theoretical argument and an extended RGF-model, it is proposed that this deviation is caused by multiple meanings of Chinese characters. The effect is stronger for Chinese characters than for Chinese words. The relation between Zipf's law, the Simon-model for texts and the present results are discussed. version:2
arxiv-1411-6400 | Mutual Information-Based Unsupervised Feature Transformation for Heterogeneous Feature Subset Selection | http://arxiv.org/abs/1411.6400 | id:1411.6400 author:Min Wei, Tommy W. S. Chow, Rosa H. M. Chan category:stat.ML cs.LG  published:2014-11-24 summary:Conventional mutual information (MI) based feature selection (FS) methods are unable to handle heterogeneous feature subset selection properly because of data format differences or estimation methods of MI between feature subset and class label. A way to solve this problem is feature transformation (FT). In this study, a novel unsupervised feature transformation (UFT) which can transform non-numerical features into numerical features is developed and tested. The UFT process is MI-based and independent of class label. MI-based FS algorithms, such as Parzen window feature selector (PWFS), minimum redundancy maximum relevance feature selection (mRMR), and normalized MI feature selection (NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unlike traditional FT methods, the proposed UFT is unbiased while PWFS is utilized to its full advantage. Simulations and analyses of large-scale datasets showed that feature subset selected by the integrated method, UFT-PWFS, outperformed other FT-FS integrated methods in classification accuracy. version:2
arxiv-1503-08381 | Towards Shockingly Easy Structured Classification: A Search-based Probabilistic Online Learning Framework | http://arxiv.org/abs/1503.08381 | id:1503.08381 author:Xu Sun category:cs.LG cs.AI  published:2015-03-29 summary:There are two major approaches for structured classification. One is the probabilistic gradient-based methods such as conditional random fields (CRF), which has high accuracy but with drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other one is the search-based learning methods such as perceptrons and margin infused relaxed algorithm (MIRA), which have fast training but also with drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and "shockingly easy" solution, a search-based probabilistic online learning method, to address most of those issues. This method searches the output candidates, derives probabilities, and conduct efficient online learning. We show that this method is with fast training, support search-based optimization, very easy to implement, with top accuracy, with probabilities, and with theoretical guarantees of convergence. Experiments on well-known tasks show that our method has better accuracy than CRF and almost as fast training speed as perceptron and MIRA. Results also show that SAPO can easily beat the state-of-the-art systems on those highly-competitive tasks, achieving record-breaking accuracies. version:1
arxiv-1503-08370 | Global Bandits | http://arxiv.org/abs/1503.08370 | id:1503.08370 author:Onur Atan, Cem Tekin, Mihaela van der Schaar category:cs.LG  published:2015-03-29 summary:Standard multi-armed bandits model decision problems in which the consequences of each action choice are unknown and independent of each other. But in a wide variety of decision problems - from drug dosage to dynamic pricing - the consequences (rewards) of different actions are correlated, so that selecting one action provides information about the consequences (rewards) of other actions as well. We propose and analyze a class of models of such decision problems; we call this class of models global bandits. When rewards across actions (arms) are sufficiently correlated we construct a greedy policy that achieves bounded regret, with a bound that depends on the true parameters of the problem. In the special case in which rewards of all arms are deterministic functions of a single unknown parameter, we construct a (more sophisticated) greedy policy that achieves bounded regret, with a bound that depends on the single true parameter of the problem. For this special case we also obtain a bound on regret that is independent of the true parameter; this bound is sub-linear, with an exponent that depends on the informativeness of the arms (which measures the strength of correlation between arm rewards). version:1
arxiv-1503-08363 | Active Model Aggregation via Stochastic Mirror Descent | http://arxiv.org/abs/1503.08363 | id:1503.08363 author:Ravi Ganti category:stat.ML cs.AI cs.LG  published:2015-03-28 summary:We consider the problem of learning convex aggregation of models, that is as good as the best convex aggregation, for the binary classification problem. Working in the stream based active learning setting, where the active learner has to make a decision on-the-fly, if it wants to query for the label of the point currently seen in the stream, we propose a stochastic-mirror descent algorithm, called SMD-AMA, with entropy regularization. We establish an excess risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of the order of $O\left(\sqrt{\frac{\log(M)}{{T^{1-\mu}}}}\right)$, where $\mu\in [0,1)$ is an algorithm dependent parameter, that trades-off the number of labels queried, and excess risk. version:1
arxiv-1503-08348 | Sparse Linear Regression With Missing Data | http://arxiv.org/abs/1503.08348 | id:1503.08348 author:Ravi Ganti, Rebecca M. Willett category:stat.ML cs.LG stat.ME  published:2015-03-28 summary:This paper proposes a fast and accurate method for sparse regression in the presence of missing data. The underlying statistical model encapsulates the low-dimensional structure of the incomplete data matrix and the sparsity of the regression coefficients, and the proposed algorithm jointly learns the low-dimensional structure of the data and a linear regressor with sparse coefficients. The proposed stochastic optimization method, Sparse Linear Regression with Missing Data (SLRM), performs an alternating minimization procedure and scales well with the problem size. Large deviation inequalities shed light on the impact of the various problem-dependent parameters on the expected squared loss of the learned regressor. Extensive simulations on both synthetic and real datasets show that SLRM performs better than competing algorithms in a variety of contexts. version:1
arxiv-1405-4251 | Selection Bias Correction and Effect Size Estimation under Dependence | http://arxiv.org/abs/1405.4251 | id:1405.4251 author:Kean Ming Tan, Noah Simon, Daniela Witten category:stat.ME stat.AP stat.ML  published:2014-05-16 summary:We consider large-scale studies in which it is of interest to test a very large number of hypotheses, and then to estimate the effect sizes corresponding to the rejected hypotheses. For instance, this setting arises in the analysis of gene expression or DNA sequencing data. However, naive estimates of the effect sizes suffer from selection bias, i.e., some of the largest naive estimates are large due to chance alone. Many authors have proposed methods to reduce the effects of selection bias under the assumption that the naive estimates of the effect sizes are independent. Unfortunately, when the effect size estimates are dependent, these existing techniques can have very poor performance, and in practice there will often be dependence. We propose an estimator that adjusts for selection bias under a recently-proposed frequentist framework, without the independence assumption. We study some properties of the proposed estimator, and illustrate that it outperforms past proposals in a simulation study and on two gene expression data sets. version:2
arxiv-1503-08322 | Some Further Evidence about Magnification and Shape in Neural Gas | http://arxiv.org/abs/1503.08322 | id:1503.08322 author:Giacomo Parigi, Andrea Pedrini, Marco Piastra category:cs.NE  published:2015-03-28 summary:Neural gas (NG) is a robust vector quantization algorithm with a well-known mathematical model. According to this, the neural gas samples the underlying data distribution following a power law with a magnification exponent that depends on data dimensionality only. The effects of shape in the input data distribution, however, are not entirely covered by the NG model above, due to the technical difficulties involved. The experimental work described here shows that shape is indeed relevant in determining the overall NG behavior; in particular, some experiments reveal richer and complex behaviors induced by shape that cannot be explained by the power law alone. Although a more comprehensive analytical model remains to be defined, the evidence collected in these experiments suggests that the NG algorithm has an interesting potential for detecting complex shapes in noisy datasets. version:1
arxiv-1410-4985 | Evolvability signatures of generative encodings: beyond standard performance benchmarks | http://arxiv.org/abs/1410.4985 | id:1410.4985 author:Danesh Tarapore, Jean-Baptiste Mouret category:cs.NE  published:2014-10-18 summary:Evolutionary robotics is a promising approach to autonomously synthesize machines with abilities that resemble those of animals, but the field suffers from a lack of strong foundations. In particular, evolutionary systems are currently assessed solely by the fitness score their evolved artifacts can achieve for a specific task, whereas such fitness-based comparisons provide limited insights about how the same system would evaluate on different tasks, and its adaptive capabilities to respond to changes in fitness (e.g., from damages to the machine, or in new situations). To counter these limitations, we introduce the concept of "evolvability signatures", which picture the post-mutation statistical distribution of both behavior diversity (how different are the robot behaviors after a mutation?) and fitness values (how different is the fitness after a mutation?). We tested the relevance of this concept by evolving controllers for hexapod robot locomotion using five different genotype-to-phenotype mappings (direct encoding, generative encoding of open-loop and closed-loop central pattern generators, generative encoding of neural networks, and single-unit pattern generators (SUPG)). We observed a predictive relationship between the evolvability signature of each encoding and the number of generations required by hexapods to adapt from incurred damages. Our study also reveals that, across the five investigated encodings, the SUPG scheme achieved the best evolvability signature, and was always foremost in recovering an effective gait following robot damages. Overall, our evolvability signatures neatly complement existing task-performance benchmarks, and pave the way for stronger foundations for research in evolutionary robotics. version:2
arxiv-1503-08294 | A Multi-signal Variant for the GPU-based Parallelization of Growing Self-Organizing Networks | http://arxiv.org/abs/1503.08294 | id:1503.08294 author:Giacomo Parigi, Angelo Stramieri, Danilo Pau, Marco Piastra category:cs.DC cs.NE  published:2015-03-28 summary:Among the many possible approaches for the parallelization of self-organizing networks, and in particular of growing self-organizing networks, perhaps the most common one is producing an optimized, parallel implementation of the standard sequential algorithms reported in the literature. In this paper we explore an alternative approach, based on a new algorithm variant specifically designed to match the features of the large-scale, fine-grained parallelism of GPUs, in which multiple input signals are processed at once. Comparative tests have been performed, using both parallel and sequential implementations of the new algorithm variant, in particular for a growing self-organizing network that reconstructs surfaces from point clouds. The experimental results show that this approach allows harnessing in a more effective way the intrinsic parallelism that the self-organizing networks algorithms seem intuitively to suggest, obtaining better performances even with networks of smaller size. version:1
arxiv-1503-08272 | Robust Bayesian compressive sensing with data loss recovery for structural health monitoring signals | http://arxiv.org/abs/1503.08272 | id:1503.08272 author:Yong Huang, James L. Beck, Stephen Wu, Hui Li category:stat.AP stat.CO stat.ML  published:2015-03-28 summary:The application of compressive sensing (CS) to structural health monitoring is an emerging research topic. The basic idea in CS is to use a specially-designed wireless sensor to sample signals that are sparse in some basis (e.g. wavelet basis) directly in a compressed form, and then to reconstruct (decompress) these signals accurately using some inversion algorithm after transmission to a central processing unit. However, most signals in structural health monitoring are only approximately sparse, i.e. only a relatively small number of the signal coefficients in some basis are significant, but the other coefficients are usually not exactly zero. In this case, perfect reconstruction from compressed measurements is not expected. A new Bayesian CS algorithm is proposed in which robust treatment of the uncertain parameters is explored, including integration over the prediction-error precision parameter to remove it as a "nuisance" parameter. The performance of the new CS algorithm is investigated using compressed data from accelerometers installed on a space-frame structure and on a cable-stayed bridge. Compared with other state-of-the-art CS methods including our previously-published Bayesian method which uses MAP (maximum a posteriori) estimation of the prediction-error precision parameter, the new algorithm shows superior performance in reconstruction robustness and posterior uncertainty quantification. Furthermore, our method can be utilized for recovery of lost data during wireless transmission, regardless of the level of sparseness in the signal. version:1
arxiv-1503-08263 | CRF Learning with CNN Features for Image Segmentation | http://arxiv.org/abs/1503.08263 | id:1503.08263 author:Fayao Liu, Guosheng Lin, Chunhua Shen category:cs.CV  published:2015-03-28 summary:Conditional Random Rields (CRF) have been widely applied in image segmentations. While most studies rely on hand-crafted features, we here propose to exploit a pre-trained large convolutional neural network (CNN) to generate deep features for CRF learning. The deep CNN is trained on the ImageNet dataset and transferred to image segmentations here for constructing potentials of superpixels. Then the CRF parameters are learnt using a structured support vector machine (SSVM). To fully exploit context information in inference, we construct spatially related co-occurrence pairwise potentials and incorporate them into the energy function. This prefers labelling of object pairs that frequently co-occur in a certain spatial layout and at the same time avoids implausible labellings during the inference. Extensive experiments on binary and multi-class segmentation benchmarks demonstrate the promise of the proposed method. We thus provide new baselines for the segmentation performance on the Weizmann horse, Graz-02, MSRC-21, Stanford Background and PASCAL VOC 2011 datasets. version:1
arxiv-1503-08223 | A System View of the Recognition and Interpretation of Observed Human Shape, Pose and Action | http://arxiv.org/abs/1503.08223 | id:1503.08223 author:David W. Arathorn category:cs.CV  published:2015-03-27 summary:There is physiological evidence that our ability to interpret human pose and action from 2D visual imagery (binocular or monocular) engages the circuitry of the motor cortices as well as the visual areas of the brain. This implies that the capability of the motor cortices to solve inverse kinematics is flexible enough to apply to both motion planning as well as serving as a generative model for the visual processing of human figures, despite the differing functional requirements of the two tasks. This paper provides a computational model of the cooperation between visual and motor areas: in other words, a system view of an important class of brain computations. The model unifies the solution of the separate inverse problems involved in the task, visual transformation discovery, inverse kinematics, and adaptation to morphology variations, using several instances of the Map-seeking Circuit algorithm. While the paper is weighted toward the exposition of a neurobiological hypothesis, from mathematical formalization of the problem to neuronal circuitry, the algorithmic expression of the solution is also a functional machine vision system for human figure recognition, and 3D pose and body morphology reconstruction from monocular, perspective-less input imagery. With an inverse kinematic generative model capable of imposing a variety of endogenous and exogenous constraints the machine vision implementation acquires characteristics currently unique among such systems. version:1
arxiv-1503-08169 | RankMap: A Platform-Aware Framework for Distributed Learning from Dense Datasets | http://arxiv.org/abs/1503.08169 | id:1503.08169 author:Azalia Mirhoseini, Eva. L. Dyer, Ebrahim. M. Songhori, Richard Baraniuk, Farinaz Koushanfar category:cs.DC cs.LG  published:2015-03-27 summary:This paper introduces RankMap, a platform-aware end-to-end framework for efficient execution of a broad class of iterative learning algorithms for massive and dense datasets. In contrast to the existing dense (iterative) data analysis methods that are oblivious to the platform, for the first time, we introduce novel scalable data transformation and mapping algorithms that enable optimizing for the underlying computing platforms' cost/constraints. The cost is defined by the number of arithmetic and (within-platform) message passing operations incurred by the variable updates in each iteration, while the constraints are set by the available memory resources. RankMap's transformation scalably factorizes data into an ensemble of lower dimensional subspaces, while its mapping schedules the flow of iterative computation on the transformed data onto the pertinent computing machine. We show a trade-off between the desired level of accuracy for the learning algorithm and the achieved efficiency. RankMap provides two APIs, one matrix-based and one graph-based, which facilitate automated adoption of the framework for performing several contemporary iterative learning applications optimized to the platform. To demonstrate the utility of RankMap, we solve sparse recovery and power iteration problems on various real-world datasets with up to 1.8 billion non-zeros. Our evaluations are performed on Amazon EC2 and IBM iDataPlex platforms using up to 244 cores. The results demonstrate up to 2 orders of magnitude improvements in memory usage, execution speed, and bandwidth compared with the best reported prior work. version:1
arxiv-1503-08155 | Learning Embedding Representations for Knowledge Inference on Imperfect and Incomplete Repositories | http://arxiv.org/abs/1503.08155 | id:1503.08155 author:Miao Fan, Qiang Zhou, Thomas Fang Zheng category:cs.AI cs.CL  published:2015-03-27 summary:This paper considers the problem of knowledge inference on large-scale imperfect repositories with incomplete coverage by means of embedding entities and relations at the first attempt. We propose IIKE (Imperfect and Incomplete Knowledge Embedding), a probabilistic model which measures the probability of each belief, i.e. $\langle h,r,t\rangle$, in large-scale knowledge bases such as NELL and Freebase, and our objective is to learn a better low-dimensional vector representation for each entity ($h$ and $t$) and relation ($r$) in the process of minimizing the loss of fitting the corresponding confidence given by machine learning (NELL) or crowdsouring (Freebase), so that we can use $ {\bf h} + {\bf r} - {\bf t} $ to assess the plausibility of a belief when conducting inference. We use subsets of those inexact knowledge bases to train our model and test the performances of link prediction and triplet classification on ground truth beliefs, respectively. The results of extensive experiments show that IIKE achieves significant improvement compared with the baseline and state-of-the-art approaches. version:1
arxiv-1409-8500 | Hyper-Spectral Image Analysis with Partially-Latent Regression and Spatial Markov Dependencies | http://arxiv.org/abs/1409.8500 | id:1409.8500 author:Antoine Deleforge, Florence Forbes, Sileye Ba, Radu Horaud category:stat.AP cs.CV  published:2014-09-30 summary:Hyper-spectral data can be analyzed to recover physical properties at large planetary scales. This involves resolving inverse problems which can be addressed within machine learning, with the advantage that, once a relationship between physical parameters and spectra has been established in a data-driven fashion, the learned relationship can be used to estimate physical parameters for new hyper-spectral observations. Within this framework, we propose a spatially-constrained and partially-latent regression method which maps high-dimensional inputs (hyper-spectral images) onto low-dimensional responses (physical parameters such as the local chemical composition of the soil). The proposed regression model comprises two key features. Firstly, it combines a Gaussian mixture of locally-linear mappings (GLLiM) with a partially-latent response model. While the former makes high-dimensional regression tractable, the latter enables to deal with physical parameters that cannot be observed or, more generally, with data contaminated by experimental artifacts that cannot be explained with noise models. Secondly, spatial constraints are introduced in the model through a Markov random field (MRF) prior which provides a spatial structure to the Gaussian-mixture hidden variables. Experiments conducted on a database composed of remotely sensed observations collected from the Mars planet by the Mars Express orbiter demonstrate the effectiveness of the proposed model. version:2
arxiv-1502-05313 | Variational Optimization of Annealing Schedules | http://arxiv.org/abs/1502.05313 | id:1502.05313 author:Taichi Kiwaki category:stat.ML  published:2015-02-18 summary:Annealed importance sampling (AIS) is a common algorithm to estimate partition functions of useful stochastic models. One important problem for obtaining accurate AIS estimates is the selection of an annealing schedule. Conventionally, an annealing schedule is often determined heuristically or is simply set as a linearly increasing sequence. In this paper, we propose an algorithm for the optimal schedule by deriving a functional that dominates the AIS estimation error and by numerically minimizing this functional. We experimentally demonstrate that the proposed algorithm mostly outperforms conventional scheduling schemes with large quantization numbers. version:2
arxiv-1503-07793 | Gibbs Sampling with Low-Power Spiking Digital Neurons | http://arxiv.org/abs/1503.07793 | id:1503.07793 author:Srinjoy Das, Bruno Umbria Pedroni, Paul Merolla, John Arthur, Andrew S. Cassidy, Bryan L. Jackson, Dharmendra Modha, Gert Cauwenberghs, Ken Kreutz-Delgado category:cs.NE  published:2015-03-26 summary:Restricted Boltzmann Machines and Deep Belief Networks have been successfully used in a wide variety of applications including image classification and speech recognition. Inference and learning in these algorithms uses a Markov Chain Monte Carlo procedure called Gibbs sampling. A sigmoidal function forms the kernel of this sampler which can be realized from the firing statistics of noisy integrate-and-fire neurons on a neuromorphic VLSI substrate. This paper demonstrates such an implementation on an array of digital spiking neurons with stochastic leak and threshold properties for inference tasks and presents some key performance metrics for such a hardware-based sampler in both the generative and discriminative contexts. version:2
arxiv-1401-1137 | Sparse graphs using exchangeable random measures | http://arxiv.org/abs/1401.1137 | id:1401.1137 author:François Caron, Emily B. Fox category:stat.ME cs.SI math.ST stat.ML stat.TH  published:2014-01-06 summary:Statistical network modeling has focused on representing the graph as a discrete structure, namely the adjacency matrix, and considering the exchangeability of this array. In such cases, the Aldous-Hoover representation theorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph is necessarily either dense or empty. In this paper, we instead consider representing the graph as a measure on $\mathbb{R}_+^2$. For the associated definition of exchangeability in this continuous space, we rely on the Kallenberg representation theorem (Kallenberg, 2005). We show that for certain choices of such exchangeable random measures underlying our graph construction, our network process is sparse with power-law degree distribution. In particular, we build on the framework of completely random measures (CRMs) and use the theory associated with such processes to derive important network properties, such as an urn representation for our analysis and network simulation. Our theoretical results are explored empirically and compared to common network models. We then present a Hamiltonian Monte Carlo algorithm for efficient exploration of the posterior distribution and demonstrate that we are able to recover graphs ranging from dense to sparse--and perform associated tests--based on our flexible CRM-based formulation. We explore network properties in a range of real datasets, including Facebook social circles, a political blogosphere, protein networks, citation networks, and world wide web networks, including networks with hundreds of thousands of nodes and millions of edges. version:3
arxiv-1412-6550 | FitNets: Hints for Thin Deep Nets | http://arxiv.org/abs/1412.6550 | id:1412.6550 author:Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio category:cs.LG cs.NE  published:2014-12-19 summary:While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network. version:4
arxiv-1503-07998 | Real-time multi-view deconvolution | http://arxiv.org/abs/1503.07998 | id:1503.07998 author:Benjamin Schmid, Jan Huisken category:q-bio.QM cs.CV  published:2015-03-27 summary:In light-sheet microscopy, overall image content and resolution are improved by acquiring and fusing multiple views of the sample from different directions. State-of-the-art multi-view (MV) deconvolution employs the point spread functions (PSF) of the different views to simultaneously fuse and deconvolve the images in 3D, but processing takes a multiple of the acquisition time and constitutes the bottleneck in the imaging pipeline. Here we show that MV deconvolution in 3D can finally be achieved in real-time by reslicing the acquired data and processing cross-sectional planes individually on the massively parallel architecture of a graphics processing unit (GPU). version:1
arxiv-1503-07990 | Estimation of a common covariance matrix for multiple classes with applications in meta- and discriminant analysis | http://arxiv.org/abs/1503.07990 | id:1503.07990 author:Anders Ellern Bilgrau, Poul Svante Eriksen, Karen Dybkær, Martin Bøgsted category:stat.ML q-bio.GN stat.ME  published:2015-03-27 summary:We propose a hierarchical random effects model for a common covariance matrix in cases where multiple classes are present. It is applicable where the classes are believed to share a common covariance matrix of interest obscured by class-dependent noise. As such, it provides a basis for integrative or meta-analysis of covariance matrices where the classes are formed by datasets. Our approach is inspired by traditional meta-analysis using random effects models but the model is also shown to be applicable as an intermediate between linear and quadratic discriminant analysis. We derive basic properties and estimators of the model and compare their properties. Simple inference and interpretation of the introduced parameter measuring the inter-class homogeneity is suggested. version:1
arxiv-1503-07989 | Discriminative Bayesian Dictionary Learning for Classification | http://arxiv.org/abs/1503.07989 | id:1503.07989 author:Naveed Akhtar, Faisal Shafait, Ajmal Mian category:cs.CV cs.LG 68T10  published:2015-03-27 summary:We propose a Bayesian approach to learn discriminative dictionaries for sparse representation of data. The proposed approach infers probability distributions over the atoms of a discriminative dictionary using a Beta Process. It also computes sets of Bernoulli distributions that associate class labels to the learned dictionary atoms. This association signifies the selection probabilities of the dictionary atoms in the expansion of class-specific data. Furthermore, the non-parametric character of the proposed approach allows it to infer the correct size of the dictionary. We exploit the aforementioned Bernoulli distributions in separately learning a linear classifier. The classifier uses the same hierarchical Bayesian model as the dictionary, which we present along the analytical inference solution for Gibbs sampling. For classification, a test instance is first sparsely encoded over the learned dictionary and the codes are fed to the classifier. We performed experiments for face and action recognition; and object and scene-category classification using five public datasets and compared the results with state-of-the-art discriminative sparse representation approaches. Experiments show that the proposed Bayesian approach consistently outperforms the existing approaches. version:1
arxiv-1503-07970 | Bayesian Cross Validation and WAIC for Predictive Prior Design in Regular Asymptotic Theory | http://arxiv.org/abs/1503.07970 | id:1503.07970 author:Sumio Watanabe category:cs.LG stat.ML  published:2015-03-27 summary:Prior design is one of the most important problems in both statistics and machine learning. The cross validation (CV) and the widely applicable information criterion (WAIC) are predictive measures of the Bayesian estimation, however, it has been difficult to apply them to find the optimal prior because their mathematical properties in prior evaluation have been unknown and the region of the hyperparameters is too wide to be examined. In this paper, we derive a new formula by which the theoretical relation among CV, WAIC, and the generalization loss is clarified and the optimal hyperparameter can be directly found. By the formula, three facts are clarified about predictive prior design. Firstly, CV and WAIC have the same second order asymptotic expansion, hence they are asymptotically equivalent to each other as the optimizer of the hyperparameter. Secondly, the hyperparameter which minimizes CV or WAIC makes the average generalization loss to be minimized asymptotically but does not the random generalization loss. And lastly, by using the mathematical relation between priors, the variances of the optimized hyperparameters by CV and WAIC are made smaller with small computational costs. Also we show that the optimized hyperparameter by DIC or the marginal likelihood does not minimize the average or random generalization loss in general. version:1
arxiv-1503-07940 | Competitive Distribution Estimation | http://arxiv.org/abs/1503.07940 | id:1503.07940 author:Alon Orlitsky, Ananda Theertha Suresh category:cs.IT cs.DS cs.LG math.IT math.ST stat.TH  published:2015-03-27 summary:Estimating an unknown distribution from its samples is a fundamental problem in statistics. The common, min-max, formulation of this goal considers the performance of the best estimator over all distributions in a class. It shows that with $n$ samples, distributions over $k$ symbols can be learned to a KL divergence that decreases to zero with the sample size $n$, but grows unboundedly with the alphabet size $k$. Min-max performance can be viewed as regret relative to an oracle that knows the underlying distribution. We consider two natural and modest limits on the oracle's power. One where it knows the underlying distribution only up to symbol permutations, and the other where it knows the exact distribution but is restricted to use natural estimators that assign the same probability to symbols that appeared equally many times in the sample. We show that in both cases the competitive regret reduces to $\min(k/n,\tilde{\mathcal{O}}(1/\sqrt n))$, a quantity upper bounded uniformly for every alphabet size. This shows that distributions can be estimated nearly as well as when they are essentially known in advance, and nearly as well as when they are completely known in advance but need to be estimated via a natural estimator. We also provide an estimator that runs in linear time and incurs competitive regret of $\tilde{\mathcal{O}}(\min(k/n,1/\sqrt n))$, and show that for natural estimators this competitive regret is inevitable. We also demonstrate the effectiveness of competitive estimators using simulations. version:1
arxiv-1503-07906 | Generalized K-fan Multimodal Deep Model with Shared Representations | http://arxiv.org/abs/1503.07906 | id:1503.07906 author:Gang Chen, Sargur N. Srihari category:cs.LG stat.ML 68T10 I.2.6  published:2015-03-26 summary:Multimodal learning with deep Boltzmann machines (DBMs) is an generative approach to fuse multimodal inputs, and can learn the shared representation via Contrastive Divergence (CD) for classification and information retrieval tasks. However, it is a 2-fan DBM model, and cannot effectively handle multiple prediction tasks. Moreover, this model cannot recover the hidden representations well by sampling from the conditional distribution when more than one modalities are missing. In this paper, we propose a K-fan deep structure model, which can handle the multi-input and muti-output learning problems effectively. In particular, the deep structure has K-branch for different inputs where each branch can be composed of a multi-layer deep model, and a shared representation is learned in an discriminative manner to tackle multimodal tasks. Given the deep structure, we propose two objective functions to handle two multi-input and multi-output tasks: joint visual restoration and labeling, and the multi-view multi-calss object recognition tasks. To estimate the model parameters, we initialize the deep model parameters with CD to maximize the joint distribution, and then we use backpropagation to update the model according to specific objective function. The experimental results demonstrate that the model can effectively leverages multi-source information and predict multiple tasks well over competitive baselines. version:1
arxiv-1503-07884 | Transductive Multi-class and Multi-label Zero-shot Learning | http://arxiv.org/abs/1503.07884 | id:1503.07884 author:Yanwei Fu, Yongxin Yang, Timothy M. Hospedales, Tao Xiang, Shaogang Gong category:cs.LG cs.CV  published:2015-03-26 summary:Recently, zero-shot learning (ZSL) has received increasing interest. The key idea underpinning existing ZSL approaches is to exploit knowledge transfer via an intermediate-level semantic representation which is assumed to be shared between the auxiliary and target datasets, and is used to bridge between these domains for knowledge transfer. The semantic representation used in existing approaches varies from visual attributes to semantic word vectors and semantic relatedness. However, the overall pipeline is similar: a projection mapping low-level features to the semantic representation is learned from the auxiliary dataset by either classification or regression models and applied directly to map each instance into the same semantic representation space where a zero-shot classifier is used to recognise the unseen target class instances with a single known 'prototype' of each target class. In this paper we discuss two related lines of work improving the conventional approach: exploiting transductive learning ZSL, and generalising ZSL to the multi-label case. version:1
arxiv-1503-07795 | Multi-Labeled Classification of Demographic Attributes of Patients: a case study of diabetics patients | http://arxiv.org/abs/1503.07795 | id:1503.07795 author:Naveen Kumar Parachur Cotha, Marina Sokolova category:cs.LG  published:2015-03-26 summary:Automated learning of patients demographics can be seen as multi-label problem where a patient model is based on different race and gender groups. The resulting model can be further integrated into Privacy-Preserving Data Mining, where it can be used to assess risk of identification of different patient groups. Our project considers relations between diabetes and demographics of patients as a multi-labelled problem. Most research in this area has been done as binary classification, where the target class is finding if a person has diabetes or not. But very few, and maybe no work has been done in multi-labeled analysis of the demographics of patients who are likely to be diagnosed with diabetes. To identify such groups, we applied ensembles of several multi-label learning algorithms. version:1
arxiv-1503-07790 | Transductive Multi-label Zero-shot Learning | http://arxiv.org/abs/1503.07790 | id:1503.07790 author:Yanwei Fu, Yongxin Yang, Tim Hospedales, Tao Xiang, Shaogang Gong category:cs.LG cs.CV  published:2015-03-26 summary:Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems. These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However, they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data. In this paper, for the first time, we investigate and formalise a general framework for multi-label zero-shot learning, addressing the unique challenge therein: how to exploit multi-label correlation at test time with no training data for those classes? In particular, we propose (1) a multi-output deep regression model to project an image into a semantic word space, which explicitly exploits the correlations in the intermediate semantic layer of word vectors; (2) a novel zero-shot learning algorithm for multi-label data that exploits the unique compositionality property of semantic word vector representations; and (3) a transductive learning strategy to enable the regression model learned from seen classes to generalise well to unseen classes. Our zero-shot learning experiments on a number of standard multi-label datasets demonstrate that our method outperforms a variety of baselines. version:1
arxiv-1503-07783 | Towards Learning free Naive Bayes Nearest Neighbor-based Domain Adaptation | http://arxiv.org/abs/1503.07783 | id:1503.07783 author:Faraz Saeedan, Barbara Caputo category:cs.CV  published:2015-03-26 summary:As of today, object categorization algorithms are not able to achieve the level of robustness and generality necessary to work reliably in the real world. Even the most powerful convolutional neural network we can train fails to perform satisfactorily when trained and tested on data from different databases. This issue, known as domain adaptation and/or dataset bias in the literature, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. Recent work showed that by casting the problem into the image-to-class recognition framework, the domain adaptation problem is significantly alleviated \cite{danbnn}. Here we follow this approach, and show how a very simple, learning free Naive Bayes Nearest Neighbor (NBNN)-based domain adaptation algorithm can significantly alleviate the distribution mismatch among source and target data, especially when the number of classes and the number of sources grow. Experiments on standard benchmarks used in the literature show that our approach (a) is competitive with the current state of the art on small scale problems, and (b) achieves the current state of the art as the number of classes and sources grows, with minimal computational requirements. version:1
arxiv-1412-7489 | A Unified Perspective on Multi-Domain and Multi-Task Learning | http://arxiv.org/abs/1412.7489 | id:1412.7489 author:Yongxin Yang, Timothy M. Hospedales category:stat.ML cs.LG cs.NE  published:2014-12-23 summary:In this paper, we provide a new neural-network based perspective on multi-task learning (MTL) and multi-domain learning (MDL). By introducing the concept of a semantic descriptor, this framework unifies MDL and MTL as well as encompassing various classic and recent MTL/MDL algorithms by interpreting them as different ways of constructing semantic descriptors. Our interpretation provides an alternative pipeline for zero-shot learning (ZSL), where a model for a novel class can be constructed without training data. Moreover, it leads to a new and practically relevant problem setting of zero-shot domain adaptation (ZSDA), which is the analogous to ZSL but for novel domains: A model for an unseen domain can be generated by its semantic descriptor. Experiments across this range of problems demonstrate that our framework outperforms a variety of alternatives. version:3
arxiv-1407-2490 | On Gridless Sparse Methods for Line Spectral Estimation From Complete and Incomplete Data | http://arxiv.org/abs/1407.2490 | id:1407.2490 author:Zai Yang, Lihua Xie category:cs.IT math.IT stat.ML  published:2014-07-09 summary:This paper is concerned about sparse, continuous frequency estimation in line spectral estimation, and focused on developing gridless sparse methods which overcome grid mismatches and correspond to limiting scenarios of existing grid-based approaches, e.g., $\ell_1$ optimization and SPICE, with an infinitely dense grid. We generalize AST (atomic-norm soft thresholding) to the case of nonconsecutively sampled data (incomplete data) inspired by recent atomic norm based techniques. We present a gridless version of SPICE (gridless SPICE, or GLS), which is applicable to both complete and incomplete data without the knowledge of noise level. We further prove the equivalence between GLS and atomic norm-based techniques under different assumptions of noise. Moreover, we extend GLS to a systematic framework consisting of model order selection and robust frequency estimation, and present feasible algorithms for AST and GLS. Numerical simulations are provided to validate our theoretical analysis and demonstrate performance of our methods compared to existing ones. version:2
arxiv-1412-1058 | Effective Use of Word Order for Text Categorization with Convolutional Neural Networks | http://arxiv.org/abs/1412.1058 | id:1412.1058 author:Rie Johnson, Tong Zhang category:cs.CL cs.LG stat.ML  published:2014-12-01 summary:Convolutional neural network (CNN) is a neural network that can make use of the internal structure of data such as the 2D structure of image data. This paper studies CNN on text categorization to exploit the 1D structure (namely, word order) of text data for accurate prediction. Instead of using low-dimensional word vectors as input as is often done, we directly apply CNN to high-dimensional text data, which leads to directly learning embedding of small text regions for use in classification. In addition to a straightforward adaptation of CNN from image to text, a simple but new variation which employs bag-of-word conversion in the convolution layer is proposed. An extension to combine multiple convolution layers is also explored for higher accuracy. The experiments demonstrate the effectiveness of our approach in comparison with state-of-the-art methods. version:2
arxiv-1503-07706 | Pain Intensity Estimation by a Self--Taught Selection of Histograms of Topographical Features | http://arxiv.org/abs/1503.07706 | id:1503.07706 author:Corneliu Florea, Laura Florea, Raluca Boia, Alessandra Bandrabur, Constantin Vertan category:cs.CV  published:2015-03-26 summary:Pain assessment through observational pain scales is necessary for special categories of patients such as neonates, patients with dementia, critically ill patients, etc. The recently introduced Prkachin-Solomon score allows pain assessment directly from facial images opening the path for multiple assistive applications. In this paper, we introduce the Histograms of Topographical (HoT) features, which are a generalization of the topographical primal sketch, for the description of the face parts contributing to the mentioned score. We propose a semi-supervised, clustering oriented self--taught learning procedure developed on the emotion oriented Cohn-Kanade database. We use this procedure to improve the discrimination between different pain intensity levels and the generalization with respect to the monitored persons, while testing on the UNBC McMaster Shoulder Pain database. version:1
arxiv-1503-07697 | Robust Eye Centers Localization with Zero--Crossing Encoded Image Projections | http://arxiv.org/abs/1503.07697 | id:1503.07697 author:Laura Florea, Corneliu Florea, Constantin Vertan category:cs.CV  published:2015-03-26 summary:This paper proposes a new framework for the eye centers localization by the joint use of encoding of normalized image projections and a Multi Layer Perceptron (MLP) classifier. The encoding is novel and it consists in identifying the zero-crossings and extracting the relevant parameters from the resulting modes. The compressed normalized projections produce feature descriptors that are inputs to a properly-trained MLP, for discriminating among various categories of image regions. The proposed framework forms a fast and reliable system for the eye centers localization, especially in the context of face expression analysis in unconstrained environments. We successfully test the proposed method on a wide variety of databases including BioID, Cohn-Kanade, Extended Yale B and Labelled Faces in the Wild (LFW) databases. version:1
arxiv-1404-5793 | Bayesian Reconstruction of Missing Observations | http://arxiv.org/abs/1404.5793 | id:1404.5793 author:Shun Kataoka, Muneki Yasuda, Kazuyuki Tanaka category:stat.ML  published:2014-04-23 summary:We focus on an interpolation method referred to Bayesian reconstruction in this paper. Whereas in standard interpolation methods missing data are interpolated deterministically, in Bayesian reconstruction, missing data are interpolated probabilistically using a Bayesian treatment. In this paper, we address the framework of Bayesian reconstruction and its application to the traffic data reconstruction problem in the field of traffic engineering. In the latter part of this paper, we describe the evaluation of the statistical performance of our Bayesian traffic reconstruction model using a statistical mechanical approach and clarify its statistical behavior. version:2
arxiv-1503-07613 | Unsupervised authorship attribution | http://arxiv.org/abs/1503.07613 | id:1503.07613 author:David Fifield, Torbjørn Follan, Emil Lunde category:cs.CL  published:2015-03-26 summary:We describe a technique for attributing parts of a written text to a set of unknown authors. Nothing is assumed to be known a priori about the writing styles of potential authors. We use multiple independent clusterings of an input text to identify parts that are similar and dissimilar to one another. We describe algorithms necessary to combine the multiple clusterings into a meaningful output. We show results of the application of the technique on texts having multiple writing styles. version:1
arxiv-1503-07609 | An Evolutionary Algorithm for Error-Driven Learning via Reinforcement | http://arxiv.org/abs/1503.07609 | id:1503.07609 author:Yanping Liu, Erik D. Reichle category:cs.AI cs.NE  published:2015-03-26 summary:Although different learning systems are coordinated to afford complex behavior, little is known about how this occurs. This article describes a theoretical framework that specifies how complex behaviors that might be thought to require error-driven learning might instead be acquired through simple reinforcement. This framework includes specific assumptions about the mechanisms that contribute to the evolution of (artificial) neural networks to generate topologies that allow the networks to learn large-scale complex problems using only information about the quality of their performance. The practical and theoretical implications of the framework are discussed, as are possible biological analogs of the approach. version:1
arxiv-1503-05768 | On learning optimized reaction diffusion processes for effective image restoration | http://arxiv.org/abs/1503.05768 | id:1503.05768 author:Yunjin Chen, Wei Yu, Thomas Pock category:cs.CV  published:2015-03-19 summary:For several decades, image restoration remains an active research topic in low-level computer vision and hence new approaches are constantly emerging. However, many recently proposed algorithms achieve state-of-the-art performance only at the expense of very high computation time, which clearly limits their practical relevance. In this work, we propose a simple but effective approach with both high computational efficiency and high restoration quality. We extend conventional nonlinear reaction diffusion models by several parametrized linear filters as well as several parametrized influence functions. We propose to train the parameters of the filters and the influence functions through a loss based approach. Experiments show that our trained nonlinear reaction diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for image restoration. Due to their structural simplicity, our trained models are highly efficient and are also well-suited for parallel computation on GPUs. version:2
