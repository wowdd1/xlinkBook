arxiv-1610-01980 | Polynomial-time Tensor Decompositions with Sum-of-Squares | http://arxiv.org/abs/1610.01980 | id:1610.01980 author:Tengyu Ma, Jonathan Shi, David Steurer category:cs.DS cs.LG  published:2016-10-06 summary:We give new algorithms based on the sum-of-squares method for tensor decomposition. Our results improve the best known running times from quasi-polynomial to polynomial for several problems, including decomposing random overcomplete 3-tensors and learning overcomplete dictionaries with constant relative sparsity. We also give the first robust analysis for decomposing overcomplete 4-tensors in the smoothed analysis model. A key ingredient of our analysis is to establish small spectral gaps in moment matrices derived from solutions to sum-of-squares relaxations. To enable this analysis we augment sum-of-squares relaxations with spectral analogs of maximum entropy constraints. version:1
arxiv-1610-01959 | Efficient L1-Norm Principal-Component Analysis via Bit Flipping | http://arxiv.org/abs/1610.01959 | id:1610.01959 author:Panos P. Markopoulos, Sandipan Kundu, Shubham Chamadia, Dimitris A. Pados category:cs.DS cs.LG stat.ML  published:2016-10-06 summary:It was shown recently that the $K$ L1-norm principal components (L1-PCs) of a real-valued data matrix $\mathbf X \in \mathbb R^{D \times N}$ ($N$ data samples of $D$ dimensions) can be exactly calculated with cost $\mathcal{O}(2^{NK})$ or, when advantageous, $\mathcal{O}(N^{dK - K + 1})$ where $d=\mathrm{rank}(\mathbf X)$, $K<d$ [1],[2]. In applications where $\mathbf X$ is large (e.g., "big" data of large $N$ and/or "heavy" data of large $d$), these costs are prohibitive. In this work, we present a novel suboptimal algorithm for the calculation of the $K < d$ L1-PCs of $\mathbf X$ of cost $\mathcal O(ND \mathrm{min} \{ N,D\} + N^2(K^4 + dK^2) + dNK^3)$, which is comparable to that of standard (L2-norm) PC analysis. Our theoretical and experimental studies show that the proposed algorithm calculates the exact optimal L1-PCs with high frequency and achieves higher value in the L1-PC optimization metric than any known alternative algorithm of comparable computational cost. The superiority of the calculated L1-PCs over standard L2-PCs (singular vectors) in characterizing potentially faulty data/measurements is demonstrated with experiments on data dimensionality reduction and disease diagnosis from genomic data. version:1
arxiv-1610-01944 | PetroSurf3D - A high-resolution 3D Dataset of Rock Art for Surface Segmentation | http://arxiv.org/abs/1610.01944 | id:1610.01944 author:Georg Poier, Markus Seidl, Matthias Zeppelzauer, Christian Reinbacher, Martin Schaich, Giovanna Bellando, Alberto Marretta, Horst Bischof category:cs.CV  published:2016-10-06 summary:Ancient rock engravings (so called petroglyphs) represent one of the earliest surviving artifacts describing life of our ancestors. Recently, modern 3D scanning techniques found their application in the domain of rock art documentation by providing high-resolution reconstructions of rock surfaces. Reconstruction results demonstrate the strengths of novel 3D techniques and have the potential to replace the traditional (manual) documentation techniques of archaeologists. An important analysis task in rock art documentation is the segmentation of petroglyphs. To foster automation of this tedious step, we present a high-resolution 3D surface dataset of natural rock surfaces which exhibit different petroglyphs together with accurate expert ground-truth annotations. To our knowledge, this dataset is the first public 3D surface dataset which allows for surface segmentation at sub-millimeter scale. We conduct experiments with state-of-the-art methods to generate a baseline for the dataset and verify that the size and variability of the data is sufficient to successfully adopt even recent data-hungry Convolutional Neural Networks (CNNs). Furthermore, we experimentally demonstrate that the provided geometric information is key to successful automatic segmentation and strongly outperforms color-based segmentation. The introduced dataset represents a novel benchmark for 3D surface segmentation methods in general and is intended to foster comparability among different approaches in future. version:1
arxiv-1610-01935 | Sequence-based Sleep Stage Classification using Conditional Neural Fields | http://arxiv.org/abs/1610.01935 | id:1610.01935 author:Intan Nurma Yulita, Mohamad Ivan Fanany, Aniati Murni Arymurthy category:cs.NE cs.LG 68Txx I.2.4; I.2.6  published:2016-10-06 summary:Sleep signals from a polysomnographic database are sequences in nature. Commonly employed analysis and classification methods, however, ignored this fact and treated the sleep signals as non-sequence data. Treating the sleep signals as sequences, this paper compared two powerful unsupervised feature extractors and three sequence-based classifiers regarding accuracy and computational (training and testing) time after 10-folds cross-validation. The compared feature extractors are Deep Belief Networks (DBN) and Fuzzy C-Means (FCM) clustering. Whereas the compared sequence-based classifiers are Hidden Markov Models (HMM), Conditional Random Fields (CRF) and its variants, i.e., Hidden-state CRF (HCRF) and Latent-Dynamic CRF (LDCRF); and Conditional Neural Fields (CNF) and its variant (LDCNF). In this study, we use two datasets. The first dataset is an open (public) polysomnographic dataset downloadable from the Internet, while the second dataset is our polysomnographic dataset (also available for download). For the first dataset, the combination of FCM and CNF gives the highest accuracy (96.75\%) with relatively short training time (0.33 hours). For the second dataset, the combination of DBN and CRF gives the accuracy of 99.96\% but with 1.02 hours training time, whereas the combination of DBN and CNF gives slightly less accuracy (99.69\%) but also less computation time (0.89 hours). version:1
arxiv-1610-01925 | Metaheuristic Algorithms for Convolution Neural Network | http://arxiv.org/abs/1610.01925 | id:1610.01925 author:L. M. Rasdi Rere, Mohamad Ivan Fanany, Aniati Murni Arymurthy category:cs.CV cs.AI cs.NE 68Txx I.2.10  published:2016-10-06 summary:A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, that is, simulated annealing, differential evolution, and harmony search, to optimize CNN. The performances of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 7.14 percent). version:1
arxiv-1610-01922 | Adaptive Online Sequential ELM for Concept Drift Tackling | http://arxiv.org/abs/1610.01922 | id:1610.01922 author:Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin category:cs.AI cs.LG cs.NE 68Txx I.2.4  published:2016-10-06 summary:A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition. version:1
arxiv-1610-01910 | Toward Automatic Understanding of the Function of Affective Language in Support Groups | http://arxiv.org/abs/1610.01910 | id:1610.01910 author:Amit Navindgi, Caroline Brun, CÃ©cile Boulard Masson, Scott Nowson category:cs.CL  published:2016-10-06 summary:Understanding expressions of emotions in support forums has considerable value and NLP methods are key to automating this. Many approaches understandably use subjective categories which are more fine-grained than a straightforward polarity-based spectrum. However, the definition of such categories is non-trivial and, in fact, we argue for a need to incorporate communicative elements even beyond subjectivity. To support our position, we report experiments on a sentiment-labelled corpus of posts taken from a medical support forum. We argue that not only is a more fine-grained approach to text analysis important, but simultaneously recognising the social function behind affective expressions enable a more accurate and valuable level of understanding. version:1
arxiv-1610-01906 | A Vision-based Indoor Positioning System on Shopping Mall Context | http://arxiv.org/abs/1610.01906 | id:1610.01906 author:Ziwei Xu, Haitian Zheng, Minjian Pang category:cs.CV  published:2016-10-06 summary:With the help of a map and GPS, outdoor navigation from one spot to another can be done quickly and well. Unfortunately, inside a shopping mall, where GPS signal is hardly available, navigation becomes troublesome. In this paper, we propose an indoor navigation system to address the problem. Unlike most existing indoor navigation systems, which relies heavily on infrastructures and pre-labelled maps, our system uses only photos taken by cellphone cameras as input. We utilize multiple image processing techniques to parse photos of a mall's shopping instruction and a construct topological map of the mall. During navigation, we make use of deep neural networks to extract information from environment and find out the real-time position of the user. We propose a new feature fusion method to help automatically identifying shops in a photo. version:1
arxiv-1610-01891 | A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text | http://arxiv.org/abs/1610.01891 | id:1610.01891 author:Sadikin Mujiono, Mohamad Ivan Fanany, Chan Basaruddin category:cs.CL cs.AI cs.LG cs.NE 68Txx I.2.4  published:2016-10-06 summary:One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645. version:1
arxiv-1610-01874 | Neural-based Noise Filtering from Word Embeddings | http://arxiv.org/abs/1610.01874 | id:1610.01874 author:Kim Anh Nguyen, Sabine Schulte im Walde, Ngoc Thang Vu category:cs.CL  published:2016-10-06 summary:Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings. version:1
arxiv-1610-01430 | LAYERS: Yet another Neural Network toolkit | http://arxiv.org/abs/1610.01430 | id:1610.01430 author:Roberto Paredes, JosÃ©-Miguel BenedÃ­ category:cs.NE  published:2016-10-05 summary:Layers is an open source neural network toolkit aim at providing an easy way to implement modern neural networks. The main user target are students and to this end layers provides an easy scriptting language that can be early adopted. The user has to focus only on design details as network totpology and parameter tunning. version:2
arxiv-1610-01860 | Distortion Varieties | http://arxiv.org/abs/1610.01860 | id:1610.01860 author:Joe Kileel, Zuzana Kukelova, Tomas Pajdla, Bernd Sturmfels category:math.AG cs.CV  published:2016-10-06 summary:The distortion varieties of a given projective variety are parametrized by duplicating coordinates and multiplying them with monomials. We study their degrees and defining equations. Exact formulas are obtained for the case of one-parameter distortions. These are based on Chow polytopes and Gr\"obner bases. Multi-parameter distortions are studied using tropical geometry. The motivation for distortion varieties comes from multi-view geometry in computer vision. Our theory furnishes a new framework for formulating and solving minimal problems for camera models with image distortion. version:1
arxiv-1610-01858 | A Robust Framework for Classifying Evolving Document Streams in an Expert-Machine-Crowd Setting | http://arxiv.org/abs/1610.01858 | id:1610.01858 author:Muhammad Imran, Sanjay Chawla, Carlos Castillo category:cs.CL cs.IR  published:2016-10-06 summary:An emerging challenge in the online classification of social media data streams is to keep the categories used for classification up-to-date. In this paper, we propose an innovative framework based on an Expert-Machine-Crowd (EMC) triad to help categorize items by continuously identifying novel concepts in heterogeneous data streams often riddled with outliers. We unify constrained clustering and outlier detection by formulating a novel optimization problem: COD-Means. We design an algorithm to solve the COD-Means problem and show that COD-Means will not only help detect novel categories but also seamlessly discover human annotation errors and improve the overall quality of the categorization process. Experiments on diverse real data sets demonstrate that our approach is both effective and efficient. version:1
arxiv-1610-01854 | Do They All Look the Same? Deciphering Chinese, Japanese and Koreans by Fine-Grained Deep Learning | http://arxiv.org/abs/1610.01854 | id:1610.01854 author:Yu Wang, Haofu Liao, Yang Feng, Xiangyang Xu, Jiebo Luo category:cs.CV  published:2016-10-06 summary:We study to what extend Chinese, Japanese and Korean faces can be classified and which facial attributes offer the most important cues. First, we propose a novel way of obtaining large numbers of facial images with nationality labels. Then we train state-of-the-art neural networks with these labeled images. We are able to achieve an accuracy of 75.03% in the classification task, with chances being 33.33% and human accuracy 38.89% . Further, we train multiple facial attribute classifiers to identify the most distinctive features for each group. We find that Chinese, Japanese and Koreans do exhibit substantial differences in certain attributes, such as bangs, smiling, and bushy eyebrows. Along the way, we uncover several gender-related cross-country patterns as well. Our work, which complements existing APIs such as Microsoft Cognitive Services and Face++, could find potential applications in tourism, e-commerce, social media marketing, criminal justice and even counter-terrorism. version:1
arxiv-1610-01801 | Searching Scenes by Abstracting Things | http://arxiv.org/abs/1610.01801 | id:1610.01801 author:Svetlana Kordumova, Jan C. van Gemert, Cees G. M. Snoek, Arnold W. M. Smeulders category:cs.CV  published:2016-10-06 summary:In this paper we propose to represent a scene as an abstraction of 'things'. We start from 'things' as generated by modern object proposals, and we investigate their immediately observable properties: position, size, aspect ratio and color, and those only. Where the recent successes and excitement of the field lie in object identification, we represent the scene composition independent of object identities. We make three contributions in this work. First, we study simple observable properties of 'things', and call it things syntax. Second, we propose translating the things syntax in linguistic abstract statements and study their descriptive effect to retrieve scenes. Thirdly, we propose querying of scenes with abstract block illustrations and study their effectiveness to discriminate among different types of scenes. The benefit of abstract statements and block illustrations is that we generate them directly from the images, without any learning beforehand as in the standard attribute learning. Surprisingly, we show that even though we use the simplest of features from 'things' layout and no learning at all, we can still retrieve scenes reasonably well. version:1
arxiv-1610-01795 | Multiple Regularizations Deep Learning for Paddy Growth Stages Classification from LANDSAT-8 | http://arxiv.org/abs/1610.01795 | id:1610.01795 author:Ines Heidieni Ikasari, Vina Ayumi, Mohamad Ivan Fanany, Sidik Mulyono category:cs.CV cs.NE  published:2016-10-06 summary:This study uses remote sensing technology that can provide information about the condition of the earth's surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset. version:1
arxiv-1610-01766 | Constrained Maximum Correntropy Adaptive Filtering | http://arxiv.org/abs/1610.01766 | id:1610.01766 author:Siyuan Peng, Badong Chen, Lei Sun, Zhiping Lin, Wee Ser category:stat.ML  published:2016-10-06 summary:Constrained adaptive filtering algorithms inculding constrained least mean square (CLMS), constrained affine projection (CAP) and constrained recursive least squares (CRLS) have been extensively studied in many applications. Most existing constrained adaptive filtering algorithms are developed under mean square error (MSE) criterion, which is an ideal optimality criterion under Gaussian noises. This assumption however fails to model the behavior of non-Gaussian noises found in practice. Motivated by the robustness and simplicity of maximum correntropy criterion (MCC) in non-Gaussian impulsive noises, this paper proposes a new adaptive filtering algorithm called constrained maximum correntropy criterion (CMCC). Specifically, CMCC incorporates a linear constraint into a MCC filter to solve a constrained optimization problem explicitly. The proposed adaptive filtering algorithm is easy to implement and has low computational complexity, and in terms of convergence accuracy (say lower mean square deviation) and stability, can significantly outperform those MSE based constrained adaptive algorithms in presence of heavy-tailed impulsive noises. Additionally, the mean square convergence behaviors are studied under energy conservation relation, and a sufficient condition to ensure the mean square convergence and the steady-state mean square deviation (MSD) of the proposed algorithm are obtained. Simulation results confirm the theoretical predictions under both Gaussian and non- Gaussian noises, and demonstrate the excellent performance of the novel algorithm by comparing it with other conventional methods. version:1
arxiv-1610-01757 | Ischemic Stroke Identification Based on EEG and EOG using 1D Convolutional Neural Network and Batch Normalization | http://arxiv.org/abs/1610.01757 | id:1610.01757 author:Endang Purnama Giri, Mohamad Ivan Fanany, Aniati Murni Arymurthy category:cs.LG cs.NE  published:2016-10-06 summary:In 2015, stroke was the number one cause of death in Indonesia. The majority type of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan. For developing countries like Indonesia, the availability of CT-Scan is very limited and still relatively expensive. Because of the availability, another device that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke occurs because of obstruction that can make the cerebral blood flow (CBF) on a person with stroke has become lower than CBF on a normal person (control) so that the EEG signal have a deceleration. On this study, we perform the ability of 1D Convolutional Neural Network (1DCNN) to construct classification model that can distinguish the EEG and EOG stroke data from EEG and EOG control data. To accelerate training process our model we use Batch Normalization. Involving 62 person data object and from leave one out the scenario with five times repetition of measurement we obtain the average of accuracy 0.86 (F-Score 0.861) only at 200 epoch. This result is better than all over shallow and popular classifiers as the comparator (the best result of accuracy 0.69 and F-Score 0.72 ). The feature used in our study were only 24 handcrafted feature with simple feature extraction process. version:1
arxiv-1610-01741 | Combining Generative and Discriminative Neural Networks for Sleep Stages Classification | http://arxiv.org/abs/1610.01741 | id:1610.01741 author:Endang Purnama Giri, Mohamad Ivan Fanany, Aniati Murni Arymurthy category:cs.LG cs.NE 68Txx I.2.4; I.2.6  published:2016-10-06 summary:Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy. The main idea is to combine the generative capability of Deep Belief Network (DBN) with a discriminative ability and sequence pattern recognizing capability of Long Short-term Memory (LSTM). We use DBN that is treated as an automatic higher level features generator. The input to DBN is 28 "handcrafted" features as used in previous sleep stages studies. We compared our method with other techniques which combined DBN with Hidden Markov Model (HMM).In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. In this study, we used two datasets: an open data set that is treated as a benchmark; the other dataset is our sleep stages dataset (available for download) to verify the results further. Our experiments showed that the combination of DBN with LSTM gives better overall accuracy 98.75\% (Fscore=0.9875) for benchmark dataset and 98.94\% (Fscore=0.9894) for MKG dataset. This result is better than the state of the art of sleep stages classification that was 91.31\%. version:1
arxiv-1610-02273 | Near-Data Processing for Machine Learning | http://arxiv.org/abs/1610.02273 | id:1610.02273 author:Hyeokjun Choe, Seil Lee, Seongsik Park, Seijoon Kim, Eui-Young Chung, Sungroh Yoon category:cs.AR cs.DC cs.LG  published:2016-10-06 summary:In computer architecture, near-data processing (NDP) refers to augmenting the memory or the storage with processing power so that it can process the data stored therein, passing only the processed data upwards in the memory hierarchy. By offloading the computational burden of CPU and saving the need for transferring raw data, NDP has a great potential in terms of accelerating computation and reducing power consumption. Despite its potential, NDP had only limited success until recently, mainly due to the performance mismatch in logic and memory process technologies. Recently, there have been two major changes in the game, making NDP more appealing than ever. The first is the success of deep learning, which often requires frequent transfers of big data for training. The second is the advent of NAND flash-based solid-state drives (SSDs) containing multicore CPUs that can be used for data processing. In this paper, we evaluate the potential of NDP for machine learning using a new SSD platform that allows us to simulate in-storage processing (ISP) of machine learning workloads. Although our platform named ISPML can execute various algorithms, this paper focuses on the stochastic gradient decent (SGD) algorithm, which is the de facto standard method for training deep neural networks. We implement and compare three variants of SGD (synchronous, downpour, and elastic averaging) using the ISP-ML platform, in which we exploit the multiple NAND channels for implementing parallel SGD. In addition, we compare the performance of ISP optimization and that of conventional in-host processing optimization. To the best of our knowledge, this is one of the first attempts to apply NDP to the optimization for machine learning. version:1
arxiv-1610-01112 | Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning with Stochastic Initial States | http://arxiv.org/abs/1610.01112 | id:1610.01112 author:William Montgomery, Anurag Ajay, Chelsea Finn, Pieter Abbeel, Sergey Levine category:cs.LG cs.RO  published:2016-10-04 summary:Autonomous learning of robotic skills can allow general-purpose robots to learn wide behavioral repertoires without requiring extensive manual engineering. However, robotic skill learning methods typically make one of several trade-offs to enable practical real-world learning, such as requiring manually designed policy or value function representations, initialization from human-provided demonstrations, instrumentation of the training environment, or extremely long training times. In this paper, we propose a new reinforcement learning algorithm for learning manipulation skills that can train general-purpose neural network policies with minimal human engineering, while still allowing for fast, efficient learning in stochastic environments. Our approach builds on the guided policy search (GPS) algorithm, which transforms the reinforcement learning problem into supervised learning from a computational teacher (without human demonstrations). In contrast to prior GPS methods, which require a consistent set of initial states to which the system must be reset after each episode, our approach can handle randomized initial states, allowing it to be used in environments where deterministic resets are impossible. We compare our method to existing policy search techniques in simulation, showing that it can train high-dimensional neural network policies with the same sample efficiency as prior GPS methods, and present real-world results on a PR2 robotic manipulator. version:2
arxiv-1610-01732 | PCA-aided Fully Convolutional Networks for Semantic Segmentation of Multi-channel fMRI | http://arxiv.org/abs/1610.01732 | id:1610.01732 author:Lei Tai, Qiong Ye, Ming Liu category:cs.CV cs.RO  published:2016-10-06 summary:Semantic segmentation of functional magnetic resonance imaging (fMRI) makes great sense for pathology diagnosis and decision system of medical robots. The multi-channel fMRI data provide more information of the pathological features. But the increased amount of data causes complexity in feature detection. This paper proposes a principal component analysis (PCA)-aided fully convolutional network to particularly deal with multi-channel fMRI. We transfer the learned weights of contemporary classification networks to the segmentation task by fine-tuning. The experiments results are compared with various methods e.g. k-NN. A new labelling strategy is proposed to solve the semantic segmentation problem with unclear boundaries. Even with a small-sized training dataset, the test results demonstrate that our model outperforms other pathological feature detection methods. Besides, its forward inference only takes 90 milliseconds for a single set of fMRI data. To our knowledge, this is the first time to realize pixel-wise labeling of multi-channel magnetic resonance image using FCN. version:1
arxiv-1610-02281 | Effective Classification of MicroRNA Precursors Using Combinatorial Feature Mining and AdaBoost Algorithms | http://arxiv.org/abs/1610.02281 | id:1610.02281 author:Ling Zhong, Jason T. L. Wang category:q-bio.GN cs.CE cs.LG  published:2016-10-06 summary:MicroRNAs (miRNAs) are non-coding RNAs with approximately 22 nucleotides (nt) that are derived from precursor molecules. These precursor molecules or pre-miRNAs often fold into stem-loop hairpin structures. However, a large number of sequences with pre-miRNA-like hairpins can be found in genomes. It is a challenge to distinguish the real pre-miRNAs from other hairpin sequences with similar stem-loops (referred to as pseudo pre-miRNAs). Several computational methods have been developed to tackle this challenge. In this paper we propose a new method, called MirID, for identifying and classifying microRNA precursors. We collect 74 features from the sequences and secondary structures of pre-miRNAs; some of these features are taken from our previous studies on non-coding RNA prediction while others were suggested in the literature. We develop a combinatorial feature mining algorithm to identify suitable feature sets. These feature sets are then used to train support vector machines to obtain classification models, based on which classifier ensemble is constructed. Finally we use an AdaBoost algorithm to further enhance the accuracy of the classifier ensemble. Experimental results on a variety of species demonstrate the good performance of the proposed method, and its superiority over existing tools. version:1
arxiv-1610-01720 | Automatic Detection of Small Groups of Persons, Influential Members, Relations and Hierarchy in Written Conversations Using Fuzzy Logic | http://arxiv.org/abs/1610.01720 | id:1610.01720 author:French Pope III, Rouzbeh A. Shirvani, Mugizi Robert Rwebangira, Mohamed Chouikha, Ayo Taylor, Andres Alarcon Ramirez, Amirsina Torfi category:cs.CL cs.SI  published:2016-10-06 summary:Nowadays a lot of data is collected in online forums. One of the key tasks is to determine the social structure of these online groups, for example the identification of subgroups within a larger group. We will approach the grouping of individual as a classification problem. The classifier will be based on fuzzy logic. The input to the classifier will be linguistic features and degree of relationships (among individuals). The output of the classifiers are the groupings of individuals. We also incorporate a method that ranks the members of the detected subgroup to identify the hierarchies in each subgroup. Data from the HBO television show The Wire is used to analyze the efficacy and usefulness of fuzzy logic based methods as alternative methods to classical statistical methods usually used for these problems. The proposed methodology could detect automatically the most influential members of each organization The Wire with 90% accuracy. version:1
arxiv-1610-01713 | Generating Simulations of Motion Events from Verbal Descriptions | http://arxiv.org/abs/1610.01713 | id:1610.01713 author:James Pustejovsky, Nikhil Krishnaswamy category:cs.CL  published:2016-10-06 summary:In this paper, we describe a computational model for motion events in natural language that maps from linguistic expressions, through a dynamic event interpretation, into three-dimensional temporal simulations in a model. Starting with the model from (Pustejovsky and Moszkowicz, 2011), we analyze motion events using temporally-traced Labelled Transition Systems. We model the distinction between path- and manner-motion in an operational semantics, and further distinguish different types of manner-of-motion verbs in terms of the mereo-topological relations that hold throughout the process of movement. From these representations, we generate minimal models, which are realized as three-dimensional simulations in software developed with the game engine, Unity. The generated simulations act as a conceptual "debugger" for the semantics of different motion verbs: that is, by testing for consistency and informativeness in the model, simulations expose the presuppositions associated with linguistic expressions and their compositions. Because the model generation component is still incomplete, this paper focuses on an implementation which maps directly from linguistic interpretations into the Unity code snippets that create the simulations. version:1
arxiv-1610-01712 | A Methodology for Customizing Clinical Tests for Esophageal Cancer based on Patient Preferences | http://arxiv.org/abs/1610.01712 | id:1610.01712 author:Asis Roy, Sourangshu Bhattacharya, Kalyan Guin category:cs.LG stat.ML  published:2016-10-06 summary:Tests for Esophageal cancer can be expensive, uncomfortable and can have side effects. For many patients, we can predict non-existence of disease with 100% certainty, just using demographics, lifestyle, and medical history information. Our objective is to devise a general methodology for customizing tests using user preferences so that expensive or uncomfortable tests can be avoided. We propose to use classifiers trained from electronic health records (EHR) for selection of tests. The key idea is to design classifiers with 100% false normal rates, possibly at the cost higher false abnormals. We compare Naive Bayes classification (NB), Random Forests (RF), Support Vector Machines (SVM) and Logistic Regression (LR), and find kernel Logistic regression to be most suitable for the task. We propose an algorithm for finding the best probability threshold for kernel LR, based on test set accuracy. Using the proposed algorithm, we describe schemes for selecting tests, which appear as features in the automatic classification algorithm, using preferences on costs and discomfort of the users. We test our methodology with EHRs collected for more than 3000 patients, as a part of project carried out by a reputed hospital in Mumbai, India. Kernel SVM and kernel LR with a polynomial kernel of degree 3, yields an accuracy of 99.8% and sensitivity 100%, without the MP features, i.e. using only clinical tests. We demonstrate our test selection algorithm using two case studies, one using cost of clinical tests, and other using "discomfort" values for clinical tests. We compute the test sets corresponding to the lowest false abnormals for each criterion described above, using exhaustive enumeration of 15 clinical tests. The sets turn out to different, substantiating our claim that one can customize test sets based on user preferences. version:1
arxiv-1610-01708 | A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection | http://arxiv.org/abs/1610.01708 | id:1610.01708 author:Nian Liu, Junwei Han category:cs.CV  published:2016-10-06 summary:Traditional saliency models usually adopt hand-crafted image features and human-designed mechanisms to calculate local or global contrast. In this paper, we propose a novel computational saliency model, i.e., deep spatial contextual long-term recurrent convolutional network (DSCLRCN) to predict where people looks in natural scenes. DSCLRCN first automatically learns saliency related local features on each image location in parallel. Then, in contrast with most other deep network based saliency models which infer saliency in local contexts, DSCLRCN can mimic the cortical lateral inhibition mechanisms in human visual system to incorporate global contexts to assess the saliency of each image location by leveraging the deep spatial long short-term memory (DSLSTM) model. Moreover, we also integrate scene context modulation in DSLSTM for saliency inference, leading to a novel deep spatial contextual LSTM (DSCLSTM) model. The whole network can be trained end-to-end and works efficiently when testing. Experimental results on two benchmark datasets show that DSCLRCN can achieve state-of-the-art performance on saliency detection. Furthermore, the proposed DSCLSTM model can significantly boost the saliency detection performance by incorporating both global spatial interconnections and scene context modulation, which may uncover novel inspirations for studies on them in computational saliency models. version:1
arxiv-1610-01706 | Exploiting Depth from Single Monocular Images for Object Detection and Semantic Segmentation | http://arxiv.org/abs/1610.01706 | id:1610.01706 author:Yuanzhouhan Cao, Chunhua Shen, Heng Tao Shen category:cs.CV  published:2016-10-06 summary:Augmenting RGB data with measured depth has been shown to improve the performance of a range of tasks in computer vision including object detection and semantic segmentation. Although depth sensors such as the Microsoft Kinect have facilitated easy acquisition of such depth information, the vast majority of images used in vision tasks do not contain depth information. In this paper, we show that augmenting RGB images with estimated depth can also improve the accuracy of both object detection and semantic segmentation. Specifically, we first exploit the recent success of depth estimation from monocular images and learn a deep depth estimation model. Then we learn deep depth features from the estimated depth and combine with RGB features for object detection and semantic segmentation. Additionally, we propose an RGB-D semantic segmentation method which applies a multi-task training scheme: semantic label prediction and depth value regression. We test our methods on several datasets and demonstrate that incorporating information from estimated depth improves the performance of object detection and semantic segmentation remarkably. version:1
arxiv-1610-00388 | Learning to Translate in Real-time with Neural Machine Translation | http://arxiv.org/abs/1610.00388 | id:1610.00388 author:Jiatao Gu, Graham Neubig, Kyunghyun Cho, Victor O. K. Li category:cs.CL cs.LG  published:2016-10-03 summary:Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. version:2
arxiv-1610-01698 | Human Decision-Making under Limited Time | http://arxiv.org/abs/1610.01698 | id:1610.01698 author:Pedro A. Ortega, Alan A. Stocker category:stat.ML cs.AI  published:2016-10-06 summary:Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints - i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits. Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations. version:1
arxiv-1610-01424 | Non-Parametric Cluster Significance Testing with Reference to a Unimodal Null Distribution | http://arxiv.org/abs/1610.01424 | id:1610.01424 author:Erika S. Helgeson, Eric Bair category:stat.ME stat.ML  published:2016-10-05 summary:Cluster analysis is an unsupervised learning strategy that can be employed to identify subgroups of observations in data sets of unknown structure. This strategy is particularly useful for analyzing high-dimensional data such as microarray gene expression data. Many clustering methods are available, but it is challenging to determine if the identified clusters represent distinct subgroups. We propose a novel strategy to investigate the significance of identified clusters by comparing the within- cluster sum of squares from the original data to that produced by clustering an appropriate unimodal null distribution. The null distribution we present for this problem uses kernel density estimation and thus does not require that the data follow any particular distribution. We find that our method can accurately test for the presence of clustering even when the number of features is high. version:2
arxiv-1610-01690 | Low-tubal-rank Tensor Completion using Alternating Minimization | http://arxiv.org/abs/1610.01690 | id:1610.01690 author:Xiao-Yang Liu, Shuchin Aeron, Vaneet Aggarwal, Xiaodong Wang category:cs.LG cs.IT math.IT  published:2016-10-05 summary:The low-tubal-rank tensor model has been recently proposed for real-world multidimensional data. In this paper, we study the low-tubal-rank tensor completion problem, i.e., to recover a third-order tensor by observing a subset of its elements selected uniformly at random. We propose a fast iterative algorithm, called {\em Tubal-Alt-Min}, that is inspired by a similar approach for low-rank matrix completion. The unknown low-tubal-rank tensor is represented as the product of two much smaller tensors with the low-tubal-rank property being automatically incorporated, and Tubal-Alt-Min alternates between estimating those two tensors using tensor least squares minimization. First, we note that tensor least squares minimization is different from its matrix counterpart and nontrivial as the circular convolution operator of the low-tubal-rank tensor model is intertwined with the sub-sampling operator. Second, the theoretical performance guarantee is challenging since Tubal-Alt-Min is iterative and nonconvex in nature. We prove that 1) Tubal-Alt-Min guarantees exponential convergence to the global optima, and 2) for an $n \times n \times k$ tensor with tubal-rank $r \ll n$, the required sampling complexity is $O(nr^2k \log^3 n)$ and the computational complexity is $O(n^2rk^2 \log^2 n)$. Third, on both synthetic data and real-world video data, evaluation results show that compared with tensor-nuclear norm minimization (TNN-ADMM), Tubal-Alt-Min improves the recovery error dramatically (by orders of magnitude). It is estimated that Tubal-Alt-Min converges at an exponential rate $10^{-0.4423 \text{Iter}}$ where $\text{Iter}$ denotes the number of iterations, which is much faster than TNN-ADMM's $10^{-0.0332 \text{Iter}}$, and the running time can be accelerated by more than $5$ times for a $200 \times 200 \times 20$ tensor. version:1
arxiv-1610-01687 | Sampled Fictitious Play is Hannan Consistent | http://arxiv.org/abs/1610.01687 | id:1610.01687 author:Zifan Li, Ambuj Tewari category:cs.GT cs.LG stat.ML  published:2016-10-05 summary:Fictitious play is a simple and widely studied adaptive heuristic for playing repeated games. It is well known that fictitious play fails to be Hannan consistent. Several variants of fictitious play including regret matching, generalized regret matching and smooth fictitious play, are known to be Hannan consistent. In this note, we consider sampled fictitious play: at each round, the player samples past times and plays the best response to previous moves of other players at the sampled time points. We show that sampled fictitious play, using Bernoulli sampling, is Hannan consistent. Unlike several existing Hannan consistency proofs that rely on concentration of measure results, ours instead uses anti-concentration results from Littlewood-Offord theory. version:1
arxiv-1610-01685 | Supervision via Competition: Robot Adversaries for Learning Tasks | http://arxiv.org/abs/1610.01685 | id:1610.01685 author:Lerrel Pinto, James Davidson, Abhinav Gupta category:cs.RO cs.CV cs.LG  published:2016-10-05 summary:There has been a recent paradigm shift in robotics to data-driven learning for planning and control. Due to large number of experiences required for training, most of these approaches use a self-supervised paradigm: using sensors to measure success/failure. However, in most cases, these sensors provide weak supervision at best. In this work, we propose an adversarial learning framework that pits an adversary against the robot learning the task. In an effort to defeat the adversary, the original robot learns to perform the task with more robustness leading to overall improved performance. We show that this adversarial framework forces the the robot to learn a better grasping model in order to overcome the adversary. By grasping 82% of presented novel objects compared to 68% without an adversary, we demonstrate the utility of creating adversaries. We also demonstrate via experiments that having robots in adversarial setting might be a better learning strategy as compared to having collaborative multiple robots. version:1
arxiv-1610-01683 | Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks | http://arxiv.org/abs/1610.01683 | id:1610.01683 author:Orestis Tsinalis, Paul M. Matthews, Yike Guo, Stefanos Zafeiriou category:stat.ML cs.LG  published:2016-10-05 summary:We used convolutional neural networks (CNNs) for automatic sleep stage scoring based on single-channel electroencephalography (EEG) to learn task-specific filters for classification without using prior domain knowledge. We used an openly available dataset from 20 healthy young adults for evaluation and applied 20-fold cross-validation. We used class-balanced random sampling within the stochastic gradient descent (SGD) optimization of the CNN to avoid skewed performance in favor of the most represented sleep stages. We achieved high mean F1-score (81%, range 79-83%), mean accuracy across individual sleep stages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all subjects. By analyzing and visualizing the filters that our CNN learns, we found that rules learned by the filters correspond to sleep scoring criteria in the American Academy of Sleep Medicine (AASM) manual that human experts follow. Our method's performance is balanced across classes and our results are comparable to state-of-the-art methods with hand-engineered features. We show that, without using prior domain knowledge, a CNN can automatically learn to distinguish among different normal sleep stages. version:1
arxiv-1610-01675 | Generalized Inverse Classification | http://arxiv.org/abs/1610.01675 | id:1610.01675 author:Michael T. Lash, Qihang Lin, W. Nick Street, Jennifer G. Robinson, Jeffrey Ohlmann category:cs.LG stat.ML  published:2016-10-05 summary:Inverse classification is the process of perturbing an instance in a meaningful way such that it is more likely to conform to a specific class. Historical methods that address such a problem are often framed to leverage only a single classifier, or specific set of classifiers. These works are often accompanied by naive assumptions. In this work we propose generalized inverse classification (GIC), which avoids restricting the classification model that can be used. We incorporate this formulation into a refined framework in which GIC takes place. Under this framework, GIC operates on features that are immediately actionable. Each change incurs an individual cost, either linear or non-linear. Such changes are subjected to occur within a specified level of cumulative change (budget). Furthermore, our framework incorporates the estimation of features that change as a consequence of direct actions taken (indirectly changeable features). To solve such a problem, we propose three real-valued heuristic-based methods and two sensitivity analysis-based comparison methods, each of which is evaluated on two freely available real-world datasets. Our results demonstrate the validity and benefits of our formulation, framework, and methods. version:1
arxiv-1610-01644 | Understanding intermediate layers using linear classifier probes | http://arxiv.org/abs/1610.01644 | id:1610.01644 author:Guillaume Alain, Yoshua Bengio category:stat.ML cs.LG  published:2016-10-05 summary:Neural network models have a reputation for being black boxes. We propose a new method to understand better the roles and dynamics of the intermediate layers. This has direct consequences on the design of such models and it enables the expert to be able to justify certain heuristics (such as the auxiliary heads in the Inception model). Our method uses linear classifiers, referred to as "probes", where a probe can only use the hidden units of a given intermediate layer as discriminating features. Moreover, these probes cannot affect the training phase of a model, and they are generally added after training. They allow the user to visualize the state of the model at multiple steps of training. We demonstrate how this can be used to develop a better intuition about a known model and to diagnose potential problems. version:1
arxiv-1610-01642 | Learning Protein Dynamics with Metastable Switching Systems | http://arxiv.org/abs/1610.01642 | id:1610.01642 author:Bharath Ramsundar, Vijay S. Pande category:stat.ML cs.LG  published:2016-10-05 summary:We introduce a machine learning approach for extracting fine-grained representations of protein evolution from molecular dynamics datasets. Metastable switching linear dynamical systems extend standard switching models with a physically-inspired stability constraint. This constraint enables the learning of nuanced representations of protein dynamics that closely match physical reality. We derive an EM algorithm for learning, where the E-step extends the forward-backward algorithm for HMMs and the M-step requires the solution of large biconvex optimization problems. We construct an approximate semidefinite program solver based on the Frank-Wolfe algorithm and use it to solve the M-step. We apply our EM algorithm to learn accurate dynamics from large simulation datasets for the opioid peptide met-enkephalin and the proto-oncogene Src-kinase. Our learned models demonstrate significant improvements in temporal coherence over HMMs and standard switching models for met-enkephalin, and sample transition paths (possibly useful in rational drug design) for Src-kinase. version:1
arxiv-1610-01633 | Binary classification of multi-channel EEG records based on the $Îµ$-complexity of continuous vector functions | http://arxiv.org/abs/1610.01633 | id:1610.01633 author:Boris Darkhovsky, Alexandra Piryatinska, Alexander Kaplan category:stat.AP stat.ML  published:2016-10-05 summary:A methodology for binary classification of EEG records which correspond to different mental states is proposed. This model-free methodology is based on our theory of the $\epsilon$-complexity of continuous functions which is extended here (see Appendix) to the case of vector functions. This extension permits us to handle multichannel EEG recordings. The essence of the methodology is to use the $\epsilon$-complexity coefficients as features to classify (using well known classifiers) different types of vector functions representing EEG-records corresponding to different types of mental states. We apply our methodology to the problem of classification of multichannel EEG-records related to a group of healthy adolescents and a group of adolescents with schizophrenia. We found that our methodology permits accurate classification of the data in the four-dimensional feather space of the $\epsilon$-complexity coefficients. version:1
arxiv-1610-01588 | Neural Structural Correspondence Learning for Domain Adaptation | http://arxiv.org/abs/1610.01588 | id:1610.01588 author:Yftah Ziser, Roi Reichart category:cs.CL  published:2016-10-05 summary:Domain adaptation, adapting models from domains rich in labeled training data to domains poor in such data, is a fundamental NLP challenge. We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks. Particularly, our model is a three-layer neural network that learns to encode the nonpivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. On the task of cross-domain product sentiment classification (Blitzer et al., 2007), consisting of 12 domain pairs, our model outperforms both the SCL and the marginalized stacked denoising autoencoder (MSDA, (Chen et al., 2012)) methods by 3.77% and 2.17% respectively, on average across domain pairs. version:1
arxiv-1610-01578 | A new algorithm for identity verification based on the analysis of a handwritten dynamic signature | http://arxiv.org/abs/1610.01578 | id:1610.01578 author:Krzysztof Cpalka, Marcin Zalasinski, Leszek Rutkowski category:cs.CV cs.AI cs.HC  published:2016-10-05 summary:Identity verification based on authenticity assessment of a handwritten signature is an important issue in biometrics. There are many effective methods for signature verification taking into account dynamics of a signing process. Methods based on partitioning take a very important place among them. In this paper we propose a new approach to signature partitioning. Its most important feature is the possibility of selecting and processing of hybrid partitions in order to increase a precision of the test signature analysis. Partitions are formed by a combination of vertical and horizontal sections of the signature. Vertical sections correspond to the initial, middle, and final time moments of the signing process. In turn, horizontal sections correspond to the signature areas associated with high and low pen velocity and high and low pen pressure on the surface of a graphics tablet. Our previous research on vertical and horizontal sections of the dynamic signature (created independently) led us to develop the algorithm presented in this paper. Selection of sections, among others, allows us to define the stability of the signing process in the partitions, promoting signature areas of greater stability (and vice versa). In the test of the proposed method two databases were used: public MCYT-100 and paid BioSecure. version:1
arxiv-1610-01563 | DeepGaze II: Reading fixations from deep features trained on object recognition | http://arxiv.org/abs/1610.01563 | id:1610.01563 author:Matthias KÃ¼mmerer, Thomas S. A. Wallis, Matthias Bethge category:cs.CV q-bio.NC stat.AP  published:2016-10-05 summary:Here we present DeepGaze II, a model that predicts where people look in images. The model uses the features from the VGG-19 deep neural network trained to identify objects in images. Contrary to other saliency models that use deep features, here we use the VGG features for saliency prediction with no additional fine-tuning (rather, a few readout layers are trained on top of the VGG features to predict saliency). The model is therefore a strong test of transfer learning. After conservative cross-validation, DeepGaze II explains about 87% of the explainable information gain in the patterns of fixations and achieves top performance in area under the curve metrics on the MIT300 hold-out benchmark. These results corroborate the finding from DeepGaze I (which explained 56% of the explainable information gain), that deep features trained on object recognition provide a versatile feature space for performing related visual tasks. We explore the factors that contribute to this success and present several informative image examples. A web service is available to compute model predictions at http://deepgaze.bethgelab.org. version:1
arxiv-1610-01561 | Summarizing Situational and Topical Information During Crises | http://arxiv.org/abs/1610.01561 | id:1610.01561 author:Koustav Rudra, Siddhartha Banerjee, Niloy Ganguly, Pawan Goyal, Muhammad Imran, Prasenjit Mitra category:cs.SI cs.CL  published:2016-10-05 summary:The use of microblogging platforms such as Twitter during crises has become widespread. More importantly, information disseminated by affected people contains useful information like reports of missing and found people, requests for urgent needs etc. For rapid crisis response, humanitarian organizations look for situational awareness information to understand and assess the severity of the crisis. In this paper, we present a novel framework (i) to generate abstractive summaries useful for situational awareness, and (ii) to capture sub-topics and present a short informative summary for each of these topics. A summary is generated using a two stage framework that first extracts a set of important tweets from the whole set of information through an Integer-linear programming (ILP) based optimization technique and then follows a word graph and concept event based abstractive summarization technique to produce the final summary. High accuracies obtained for all the tasks show the effectiveness of the proposed framework. version:1
arxiv-1610-01549 | A Novel Representation of Neural Networks | http://arxiv.org/abs/1610.01549 | id:1610.01549 author:Anthony Caterini, Dong Eui Chang category:stat.ML cs.AI cs.NE I.5.1; I.2.6  published:2016-10-05 summary:Deep Neural Networks (DNNs) have become very popular for prediction in many areas. Their strength is in representation with a high number of parameters that are commonly learned via gradient descent or similar optimization methods. However, the representation is non-standardized, and the gradient calculation methods are often performed using component-based approaches that break parameters down into scalar units, instead of considering the parameters as whole entities. In this work, these problems are addressed. Standard notation is used to represent DNNs in a compact framework. Gradients of DNN loss functions are calculated directly over the inner product space on which the parameters are defined. This framework is general and is applied to two common network types: the Multilayer Perceptron and the Deep Autoencoder. version:1
arxiv-1610-01520 | Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database | http://arxiv.org/abs/1610.01520 | id:1610.01520 author:Edgar Altszyler, Mariano Sigman, Diego FernÃ¡ndez Slezak category:cs.CL cs.IR  published:2016-10-05 summary:Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology version:1
arxiv-1610-01508 | VoxML: A Visualization Modeling Language | http://arxiv.org/abs/1610.01508 | id:1610.01508 author:James Pustejovsky, Nikhil Krishnaswamy category:cs.CL  published:2016-10-05 summary:We present the specification for a modeling language, VoxML, which encodes semantic knowledge of real-world objects represented as three-dimensional models, and of events and attributes related to and enacted over these objects. VoxML is intended to overcome the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a variety of systems and platforms, leading to multimodal simulations of real-world scenarios using conceptual objects that represent their semantic values. version:1
arxiv-1610-01492 | Recovering Multiple Nonnegative Time Series From a Few Temporal Aggregates | http://arxiv.org/abs/1610.01492 | id:1610.01492 author:Jiali Mei, Yohann De Castro, Yannig Goude, Georges HÃ©brail category:stat.ML  published:2016-10-05 summary:Motivated by electricity consumption metering, we extend existing nonnegative matrix factorization (NMF) algorithms to use linear measurements as observations, instead of matrix entries. The objective is to estimate multiple time series at a fine temporal scale from temporal aggregates measured on each individual series. Furthermore, our algorithm is extended to take into account individual autocorrelation to provide better estimation, using a recent convex relaxation of quadratically constrained quadratic program. Extensive experiments on synthetic and real-world electricity consumption datasets illustrate the effectiveness of our matrix recovery algorithms. version:1
arxiv-1610-01486 | A tentative model for dimensionless phoneme distance from binary distinctive features | http://arxiv.org/abs/1610.01486 | id:1610.01486 author:Tiago Tresoldi category:cs.CL  published:2016-10-05 summary:This work proposes a tentative model for the calculation of dimensionless distances between phonemes; sounds are described with binary distinctive features and distances show linear consistency in terms of such features. The model can be used as a scoring function for local and global pairwise alignment of phoneme sequences, and the distances can be used as prior probabilities for Bayesian analyses on the phylogenetic relationship between languages, particularly for cognate identification in cases where no empirical prior probability is available. version:1
arxiv-1610-01476 | $\ell_1$ Regularized Gradient Temporal-Difference Learning | http://arxiv.org/abs/1610.01476 | id:1610.01476 author:Dominik Meyer, Hao Shen, Klaus Diepold category:cs.AI cs.LG  published:2016-10-05 summary:In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular $\ell_1$ regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with $\ell_1$ regularization. We propose a family of $\ell_1$ regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments. version:1
arxiv-1610-01465 | Visual Question Answering: Datasets, Algorithms, and Future Challenges | http://arxiv.org/abs/1610.01465 | id:1610.01465 author:Kushal Kafle, Christopher Kanan category:cs.CV cs.AI cs.CL  published:2016-10-05 summary:Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research. version:1
arxiv-1610-01852 | Compressive Imaging with Iterative Forward Models | http://arxiv.org/abs/1610.01852 | id:1610.01852 author:Hsiou-Yuan Liu, Ulugbek S. Kamilov, Dehong Liu, Hassan Mansour, Petros T. Boufounos category:cs.CV physics.optics  published:2016-10-05 summary:We propose a new compressive imaging method for reconstructing 2D or 3D objects from their scattered wave-field measurements. Our method relies on a novel, nonlinear measurement model that can account for the multiple scattering phenomenon, which makes the method preferable in applications where linear measurement models are inaccurate. We construct the measurement model by expanding the scattered wave-field with an accelerated-gradient method, which is guaranteed to converge and is suitable for large-scale problems. We provide explicit formulas for computing the gradient of our measurement model with respect to the unknown image, which enables image formation with a sparsity- driven numerical optimization algorithm. We validate the method both analytically and with numerical simulations. version:1
arxiv-1610-01444 | Markov Chain Modeling and Simulation of Breathing Patterns | http://arxiv.org/abs/1610.01444 | id:1610.01444 author:Davide Alinovi, Gianluigi Ferrari, Francesco Pisani, Riccardo Raheli category:stat.AP cs.CV  published:2016-10-05 summary:The lack of large video databases obtained from real patients with respiratory disorders makes the design and optimization of video-based monitoring systems quite critical. The purpose of this study is the development of suitable models and simulators of breathing behaviors and disorders, such as respiratory pauses and apneas, in order to allow efficient design and test of video-based monitoring systems. More precisely, a novel Continuous-Time Markov Chain (CTMC) statistical model of breathing patterns is presented. The Respiratory Rate (RR) pattern, estimated by measured vital signs of hospital-monitored patients, is approximated as a CTMC, whose states and parameters are selected through an appropriate statistical analysis. Then, two simulators, software- and hardware-based, are proposed. After validation of the CTMC model, the proposed simulators are tested with previously developed video-based algorithms for the estimation of the RR and the detection of apnea events. Examples of application to assess the performance of systems for video-based RR estimation and apnea detection are presented. The results, in terms of Kullback-Leibler divergence, show that realistic breathing patterns, including specific respiratory disorders, can be accurately described by the proposed model; moreover, the simulators are able to reproduce practical breathing patterns for video analysis. The presented CTMC statistical model can be strategic to describe realistic breathing patterns and devise simulators useful to develop and test novel and effective video processing-based monitoring systems. version:1
arxiv-1610-01439 | Nonlinear Systems Identification Using Deep Dynamic Neural Networks | http://arxiv.org/abs/1610.01439 | id:1610.01439 author:Olalekan Ogunmolu, Xuejun Gu, Steve Jiang, Nicholas Gans category:cs.NE  published:2016-10-05 summary:Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data version:1
arxiv-1610-00768 | cleverhans v0.1: an adversarial machine learning library | http://arxiv.org/abs/1610.00768 | id:1610.00768 author:Ian Goodfellow, Nicolas Papernot, Patrick McDaniel category:cs.LG cs.CR stat.ML  published:2016-10-03 summary:cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system. version:2
arxiv-1610-01417 | Decentralized Topic Modelling with Latent Dirichlet Allocation | http://arxiv.org/abs/1610.01417 | id:1610.01417 author:Igor Colin, Christophe Dupuy category:stat.ML cs.LG  published:2016-10-05 summary:Privacy preserving networks can be modelled as decentralized networks (e.g., sensors, connected objects, smartphones), where communication between nodes of the network is not controlled by an all-knowing, central node. For this type of networks, the main issue is to gather/learn global information on the network (e.g., by optimizing a global cost function) while keeping the (sensitive) information at each node. In this work, we focus on text information that agents do not want to share (e.g., text messages, emails, confidential reports). We use recent advances on decentralized optimization and topic models to infer topics from a graph with limited communication. We propose a method to adapt latent Dirichlet allocation (LDA) model to decentralized optimization and show on synthetic data that we still recover similar parameters and similar performance at each node than with stochastic methods accessing to the whole information in the graph. version:1
arxiv-1610-01407 | Towards semi-episodic learning for robot damage recovery | http://arxiv.org/abs/1610.01407 | id:1610.01407 author:Konstantinos Chatzilygeroudis, Antoine Cully, Jean-Baptiste Mouret category:cs.RO cs.AI cs.NE  published:2016-10-05 summary:The recently introduced Intelligent Trial and Error algorithm (IT\&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT\&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot's lifetime autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results. version:1
arxiv-1610-01400 | Convex Histogram-Based Joint Image Segmentation with Regularized Optimal Transport Cost | http://arxiv.org/abs/1610.01400 | id:1610.01400 author:Nicolas Papadakis, Julien Rabin category:cs.CV math.OC  published:2016-10-05 summary:We investigate in this work a versatile convex framework for multiple image segmentation, relying on the regularized optimal mass transport theory. In this setting, several transport cost functions are considered and used to match statistical distributions of features. In practice, global multidimensional histograms are estimated from the segmented image regions, and are compared to referring models that are either fixed histograms given a priori, or directly inferred in the non-supervised case. The different convex problems studied are solved efficiently using primal-dual algorithms. The proposed approach is generic and enables multi-phase segmentation as well as co-segmentation of multiple images. version:1
arxiv-1610-01394 | Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions | http://arxiv.org/abs/1610.01394 | id:1610.01394 author:Shaofei Wang, Charless C. Fowlkes category:cs.CV  published:2016-10-05 summary:We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about cooccurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. In this learning framework, we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective, one based on an LP relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions. We find the greedy algorithms achieve almost equivalent accuracy to the LP relaxation while being up to 10x faster than a commercial LP solver. We evaluate trained models on three challenging benchmarks. Surprisingly, we find that with proper parameter learning, our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning. version:1
arxiv-1610-01390 | Reliability of PET/CT shape and heterogeneity features in functional and morphological components of Non-Small Cell Lung Cancer tumors: a repeatability analysis in a prospective multi-center cohort | http://arxiv.org/abs/1610.01390 | id:1610.01390 author:Marie-Charlotte Desseroit, Florent Tixier, Wolfgang Weber, Barry A Siegel, Catherine Cheze Le Rest, Dimitris Visvikis, Mathieu Hatt category:cs.CV physics.med-ph  published:2016-10-05 summary:Purpose: The main purpose of this study was to assess the reliability of shape and heterogeneity features in both Positron Emission Tomography (PET) and low-dose Computed Tomography (CT) components of PET/CT. A secondary objective was to investigate the impact of image quantization.Material and methods: A Health Insurance Portability and Accountability Act -compliant secondary analysis of deidentified prospectively acquired PET/CT test-retest datasets of 74 patients from multi-center Merck and ACRIN trials was performed. Metabolically active volumes were automatically delineated on PET with Fuzzy Locally Adaptive Bayesian algorithm. 3DSlicerTM was used to semi-automatically delineate the anatomical volumes on low-dose CT components. Two quantization methods were considered: a quantization into a set number of bins (quantizationB) and an alternative quantization with bins of fixed width (quantizationW). Four shape descriptors, ten first-order metrics and 26 textural features were computed. Bland-Altman analysis was used to quantify repeatability. Features were subsequently categorized as very reliable, reliable, moderately reliable and poorly reliable with respect to the corresponding volume variability. Results: Repeatability was highly variable amongst features. Numerous metrics were identified as poorly or moderately reliable. Others were (very) reliable in both modalities, and in all categories (shape, 1st-, 2nd- and 3rd-order metrics). Image quantization played a major role in the features repeatability. Features were more reliable in PET with quantizationB, whereas quantizationW showed better results in CT.Conclusion: The test-retest repeatability of shape and heterogeneity features in PET and low-dose CT varied greatly amongst metrics. The level of repeatability also depended strongly on the quantization step, with different optimal choices for each modality. The repeatability of PET and low-dose CT features should be carefully taken into account when selecting metrics to build multiparametric models. version:1
arxiv-1610-01382 | Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest | http://arxiv.org/abs/1610.01382 | id:1610.01382 author:Abdul Malik Badshah, Jamil Ahmad, Mi Young Lee, Sung Wook Baik category:cs.SD cs.CL  published:2016-10-05 summary:Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches. version:1
arxiv-1610-02276 | Universal Clustering via Crowdsourcing | http://arxiv.org/abs/1610.02276 | id:1610.02276 author:Ravi Kiran Raman, Lav Varshney category:cs.HC stat.ML  published:2016-10-05 summary:Consider unsupervised clustering of objects drawn from a discrete set, through the use of human intelligence available in crowdsourcing platforms. This paper defines and studies the problem of universal clustering using responses of crowd workers, without knowledge of worker reliability or task difficulty. We model stochastic worker response distributions by incorporating traits of memory for similar objects and traits of distance among differing objects. We are particularly interested in two limiting worker types---temporary workers who retain no memory of responses and long-term workers with memory. We first define clustering algorithms for these limiting cases and then integrate them into an algorithm for the unified worker model. We prove asymptotic consistency of the algorithms and establish sufficient conditions on the sample complexity of the algorithm. Converse arguments establish necessary conditions on sample complexity, proving that the defined algorithms are asymptotically order-optimal in cost. version:1
arxiv-1610-01376 | Recognizing and Presenting the Storytelling Video Structure with Deep Multimodal Networks | http://arxiv.org/abs/1610.01376 | id:1610.01376 author:Lorenzo Baraldi, Costantino Grana, Rita Cucchiara category:cs.CV  published:2016-10-05 summary:This paper presents a novel approach for temporal and semantic segmentation of edited videos into meaningful segments, from the point of view of the storytelling structure. The objective is to decompose a long video into more manageable sequences, which can in turn be used to retrieve the most significant parts of it given a textual query and to provide an effective summarization. Previous video decomposition methods mainly employed perceptual cues, tackling the problem either as a story change detection, or as a similarity grouping task, and the lack of semantics limited their ability to identify story boundaries. Our proposal connects together perceptual, audio and semantic cues in a specialized deep network architecture designed with a combination of CNNs which generate an appropriate embedding, and clusters shots into connected sequences of semantic scenes, i.e. stories. A retrieval presentation strategy is also proposed, by selecting the semantically and aesthetically "most valuable" thumbnails to present, considering the query in order to improve the storytelling presentation. Finally, the subjective nature of the task is considered, by conducting experiments with different annotators and by proposing an algorithm to maximize the agreement between automatic results and human annotators. version:1
arxiv-1610-01374 | Soft-margin learning for multiple feature-kernel combinations with Domain Adaptation, for recognition in surveillance face datasets | http://arxiv.org/abs/1610.01374 | id:1610.01374 author:Samik Banerjee, Sukhendu Das category:cs.CV cs.AI cs.HC cs.LG  published:2016-10-05 summary:Face recognition (FR) is the most preferred mode for biometric-based surveillance, due to its passive nature of detecting subjects, amongst all different types of biometric traits. FR under surveillance scenario does not give satisfactory performance due to low contrast, noise and poor illumination conditions on probes, as compared to the training samples. A state-of-the-art technology, Deep Learning, even fails to perform well in these scenarios. We propose a novel soft-margin based learning method for multiple feature-kernel combinations, followed by feature transformed using Domain Adaptation, which outperforms many recent state-of-the-art techniques, when tested using three real-world surveillance face datasets. version:1
arxiv-1610-01367 | Monaural Multi-Talker Speech Recognition using Factorial Speech Processing Models | http://arxiv.org/abs/1610.01367 | id:1610.01367 author:Mahdi Khademian, Mohammad Mehdi Homayounpour category:cs.CL cs.SD  published:2016-10-05 summary:A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past super-human results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first super-human system and 2.7% absolute task performance improvement compared to its recent competitor. version:1
arxiv-1610-01030 | Applications of Online Deep Learning for Crisis Response Using Social Media Information | http://arxiv.org/abs/1610.01030 | id:1610.01030 author:Dat Tien Nguyen, Shafiq Joty, Muhammad Imran, Hassan Sajjad, Prasenjit Mitra category:cs.CL cs.CY cs.LG  published:2016-10-04 summary:During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset. version:2
arxiv-1610-01326 | Mobility Map Computations for Autonomous Navigation using an RGBD Sensor | http://arxiv.org/abs/1610.01326 | id:1610.01326 author:NicolÃ² Genesio, Tariq Abuhashim, Fabio Solari, Manuela Chessa, Lorenzo Natale category:cs.RO cs.CV  published:2016-10-05 summary:In recent years, the numbers of life-size humanoids as well as their mobile capabilities have steadily grown. Stable walking motion and control for humanoid robots are active fields of research. In this scenario an open question is how to model and analyse the scene so that a motion planning algorithm can generate an appropriate walking pattern. This paper presents the current work towards scene modelling and understanding, using an RGBD sensor. The main objective is to provide the humanoid robot iCub with capabilities to navigate safely and interact with various parts of the environment. In this sense we address the problem of traversability analysis of the scene, focusing on classification of point clouds as a function of mobility, and hence walking safety. version:1
arxiv-1610-01291 | Word2Vec vs DBnary: Augmenting METEOR using Vector Representations or Lexical Resources? | http://arxiv.org/abs/1610.01291 | id:1610.01291 author:Christophe Servan, Alexandre Berard, Zied Elloumi, HervÃ© Blanchon, Laurent Besacier category:cs.CL  published:2016-10-05 summary:This paper presents an approach combining lexico-semantic resources and distributed representations of words applied to the evaluation in machine translation (MT). This study is made through the enrichment of a well-known MT evaluation metric: METEOR. This metric enables an approximate match (synonymy or morphological similarity) between an automatic and a reference translation. Our experiments are made in the framework of the Metrics task of WMT 2014. We show that distributed representations are a good alternative to lexico-semantic resources for MT evaluation and they can even bring interesting additional information. The augmented versions of METEOR, using vector representations, are made available on our Github page. version:1
arxiv-1610-01283 | EPOpt: Learning Robust Neural Network Policies Using Model Ensembles | http://arxiv.org/abs/1610.01283 | id:1610.01283 author:Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, Balaraman Ravindran category:cs.LG cs.AI cs.RO  published:2016-10-05 summary:Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks -- especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including to unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation. version:1
arxiv-1601-05834 | Active Sensing of Social Networks | http://arxiv.org/abs/1601.05834 | id:1601.05834 author:Hoi-To Wai, Anna Scaglione, Amir Leshem category:cs.SI stat.ML  published:2016-01-21 summary:This paper develops an active sensing method to estimate the relative weight (or trust) agents place on their neighbors' information in a social network. The model used for the regression is based on the steady state equation in the linear DeGroot model under the influence of stubborn agents, i.e., agents whose opinions are not influenced by their neighbors. This method can be viewed as a \emph{social RADAR}, where the stubborn agents excite the system and the latter can be estimated through the reverberation observed from the analysis of the agents' opinions. The social network sensing problem can be interpreted as a blind compressed sensing problem with a sparse measurement matrix. We prove that the network structure will be revealed when a sufficient number of stubborn agents independently influence a number of ordinary (non-stubborn) agents. We investigate the scenario with a deterministic or randomized DeGroot model and propose a consistent estimator of the steady states for the latter scenario. Simulation results on synthetic and real world networks support our findings. version:2
arxiv-1610-01271 | Solving Heterogeneous Estimating Equations with Gradient Forests | http://arxiv.org/abs/1610.01271 | id:1610.01271 author:Susan Athey, Julie Tibshirani, Stefan Wager category:stat.ME stat.ML  published:2016-10-05 summary:Forest-based methods are being used in an increasing variety of statistical tasks, including causal inference, survival analysis, and quantile regression. Extending forest-based methods to these new statistical settings requires specifying tree-growing algorithms that are targeted to the task at hand, and the ad-hoc design of such algorithms can require considerable effort. In this paper, we develop a unified framework for the design of fast tree-growing procedures for tasks that can be characterized by heterogeneous estimating equations. The resulting gradient forest consists of trees grown by recursively applying a pre-processing step where we label each observation with gradient-based pseudo-outcomes, followed by a regression step that runs a standard CART regression split on these pseudo-outcomes. We apply our framework to two important statistical problems, non-parametric quantile regression and heterogeneous treatment effect estimation via instrumental variables, and we show that the resulting procedures considerably outperform baseline forests whose splitting rules do not take into account the statistical question at hand. Finally, we prove the consistency of gradient forests, and establish a central limit theorem. Our method will be available as an R-package, gradientForest, which draws from the ranger package for random forests. version:1
arxiv-1610-01256 | On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products | http://arxiv.org/abs/1610.01256 | id:1610.01256 author:Kush R. Varshney, Homa Alemzadeh category:cs.CY stat.ML  published:2016-10-05 summary:Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and a myriad of other engineered socio-technical systems, we must consider the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in the machine learning context; in this paper, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences and data products, finding that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data. version:1
arxiv-1610-01247 | ECAT: Event Capture Annotation Tool | http://arxiv.org/abs/1610.01247 | id:1610.01247 author:Tuan Do, Nikhil Krishnaswamy, James Pustejovsky category:cs.CL cs.CV  published:2016-10-05 summary:This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft's Kinect(R) hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool's workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT's output to VoxML will also be addressed. version:1
arxiv-1610-01238 | Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for Urban Autonomy | http://arxiv.org/abs/1610.01238 | id:1610.01238 author:Dan Barnes, Will Maddern, Ingmar Posner category:cs.RO cs.AI cs.CV cs.LG  published:2016-10-05 summary:We present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the large-scale KITTI and Oxford RobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving. version:1
arxiv-1604-02218 | A Low Complexity Algorithm with $O(\sqrt{T})$ Regret and Finite Constraint Violations for Online Convex Optimization with Long Term Constraints | http://arxiv.org/abs/1604.02218 | id:1604.02218 author:Hao Yu, Michael J. Neely category:math.OC cs.LG stat.ML  published:2016-04-08 summary:This paper considers online convex optimization over a complicated constraint set, which typically consists of multiple functional constraints and a set constraint. The conventional projection based online projection algorithm (Zinkevich, 2003) can be difficult to implement due to the potentially high computation complexity of the projection operation. In this paper, we relax the functional constraints by allowing them to be violated at each round but still requiring them to be satisfied in the long term. This type of relaxed online convex optimization (with long term constraints) was first considered in Mahdavi et al. (2012). That prior work proposes an algorithm to achieve $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations for general problems and another algorithm to achieve an $O(T^{2/3})$ bound for both regret and constraint violations when the constraint set can be described by a finite number of linear constraints. A recent extension in Jenatton et al. (2016) can achieve $O(T^{\max\{\beta,1-\beta\}})$ regret and $O(T^{1-\beta/2})$ constraint violations where $\beta\in (0,1)$. The current paper proposes a new simple algorithm that yields improved performance in comparison to prior works. The new algorithm achieves an $O(\sqrt{T})$ regret bound with finite constraint violations. version:2
arxiv-1610-01132 | A Non-generative Framework and Convex Relaxations for Unsupervised Learning | http://arxiv.org/abs/1610.01132 | id:1610.01132 author:Elad Hazan, Tengyu Ma category:cs.LG cs.DS stat.ML  published:2016-10-04 summary:We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization. version:2
arxiv-1610-01234 | Ensemble Validation: Selectivity has a Price, but Variety is Free | http://arxiv.org/abs/1610.01234 | id:1610.01234 author:Eric Bax, Farshad Kooti category:stat.ML cs.LG  published:2016-10-04 summary:If classifiers are selected from a hypothesis class to form an ensemble, bounds on average error rate over the selected classifiers include a component for selectivity, which grows as the fraction of hypothesis classifiers selected for the ensemble shrinks, and a component for variety, which grows with the size of the hypothesis class or in-sample data set. We show that the component for selectivity asymptotically dominates the component for variety, meaning that variety is essentially free. version:1
arxiv-1610-01223 | Feature Learning from Spectrograms for Assessment of Personality Traits | http://arxiv.org/abs/1610.01223 | id:1610.01223 author:Marc-AndrÃ© Carbonneau, Eric Granger, Yazid Attabi, Ghyslain Gagnon category:cs.CV  published:2016-10-04 summary:Several methods have recently been proposed to analyze speech and automatically infer the personality of the speaker. These methods often rely on prosodic and other hand crafted speech processing features extracted with off-the-shelf toolboxes. To achieve high accuracy, numerous features are typically extracted using complex and highly parameterized algorithms. In this paper, a new method based on feature learning and spectrogram analysis is proposed to simplify the feature extraction process while maintaining a high level of accuracy. The proposed method learns a dictionary of discriminant features from patches extracted in the spectrogram representations of training speech segments. Each speech segment is then encoded using the dictionary, and the resulting feature set is used to perform classification of personality traits. Experiments indicate that the proposed method achieves state-of-the-art results with a significant reduction in complexity when compared to the most recent reference methods. The number of features, and difficulties linked to the feature extraction process are greatly reduced as only one type of descriptors is used, for which the 6 parameters can be tuned automatically. In contrast, the simplest reference method uses 4 types of descriptors to which 6 functionals are applied, resulting in over 20 parameters to be tuned. version:1
arxiv-1610-01119 | Knowledge Guided Disambiguation for Large-Scale Scene Classification with Multi-Resolution CNNs | http://arxiv.org/abs/1610.01119 | id:1610.01119 author:Limin Wang, Sheng Guo, Weilin Huang, Yuanjun Xiong, Yu Qiao category:cs.CV  published:2016-10-04 summary:Thanks to the available large-scale scene datasets such as Places and Places2, Convolutional Neural Networks (CNNs) have made remarkable progress on the problem of scene recognition. However, scene categories are often defined according its functions and there exist large intra-class variations in a single scene category. Meanwhile, as the number of scene classes is increasing, some classes tend to overlap with others and label ambiguity is becoming a problem. This paper focuses on large-scale scene recognition and makes two major contributions to tackle these issues. First, we propose a multi-resolution CNN architecture to capture visual content and structure at different scales. Our proposed multi-resolution CNNs are composed of coarse resolution CNNs and fine resolution CNNs, whose performance is complementary to each other. Second, we design two knowledge guided disambiguation techniques to deal with the problem of label ambiguity. In the first scenario, we exploit the knowledge from confusion matrix at validation data to merge similar classes into a super category, while in the second scenario, we utilize the knowledge of extra networks to produce a soft label for each image. Both the information of super category and soft labels are exploited to train CNNs on the Places2 datasets. We conduct experiments on three large-scale image classification datasets (ImangeNet, Places, Places2) to demonstrate the effectiveness of our proposed approach. In addition, our method takes part in two major scene recognition challenges, and we achieve the 2$^{nd}$ place at the Places2 challenge 2015 and 1$^{st}$ place at the LSUN challenge 2016. Finally, we transfer the learned representations to the datasets of MIT Indoor67 and SUN397, which yields the state-of-the-art performance (86.7% and 72.0%) on both datasets. version:1
arxiv-1610-01108 | Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions | http://arxiv.org/abs/1610.01108 | id:1610.01108 author:Marcin Junczys-Dowmunt, Tomasz Dwojak, Hieu Hoang category:cs.CL  published:2016-10-04 summary:In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios. version:1
arxiv-1610-01101 | A SMART Stochastic Algorithm for Nonconvex Optimization with Applications to Robust Machine Learning | http://arxiv.org/abs/1610.01101 | id:1610.01101 author:Aleksandr Aravkin, Damek Davis category:stat.ML cs.LG math.OC 65K10  65K05  published:2016-10-04 summary:Machine learning theory typically assumes that training data is unbiased and not adversarially generated. When real training data deviates from these assumptions, trained models make erroneous predictions, sometimes with disastrous effects. Robust losses, such as the huber norm, were designed to mitigate the effects of such contaminated data, but they are limited to the regression context. In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods. version:1
arxiv-1610-01096 | FLOCK: Combating Astroturfing on Livestreaming Platforms | http://arxiv.org/abs/1610.01096 | id:1610.01096 author:Neil Shah category:cs.SI cs.LG  published:2016-10-04 summary:Livestreaming platforms have become increasingly popular in recent years as a means of sharing and advertising creative content. Popular content streamers who attract large viewership to their live broadcasts can earn a living by means of ad revenue, donations and channel subscriptions. Unfortunately, this incentivized popularity has simultaneously resulted in incentive for fraudsters to provide services to astroturf, or artificially inflate viewership metrics by providing fake "live" views to customers. Our work provides a number of major contributions: (a) formulation: we are the first to introduce and characterize the viewbot fraud problem in livestreaming platforms, (b) methodology: we propose FLOCK, a principled and unsupervised method which efficiently and effectively identifies botted broadcasts and their constituent botted views, and (c) practicality: our approach achieves over 98% precision in identifying botted broadcasts and over 90% precision/recall against sizable synthetically generated viewbot attacks on a real-world livestreaming workload of over 16 million views and 92 thousand broadcasts. FLOCK successfully operates on larger datasets in practice and is regularly used at a large, undisclosed livestreaming corporation. version:1
arxiv-1610-01076 | Tutorial on Answering Questions about Images with Deep Learning | http://arxiv.org/abs/1610.01076 | id:1610.01076 author:Mateusz Malinowski, Mario Fritz category:cs.CV cs.AI cs.CL cs.LG cs.NE  published:2016-10-04 summary:Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task. version:1
arxiv-1610-01068 | Fast Image Classification by Boosting Fuzzy Classifiers | http://arxiv.org/abs/1610.01068 | id:1610.01068 author:Marcin Korytkowski, Leszek Rutkowski, RafaÅ Scherer category:cs.CV  published:2016-10-04 summary:This paper presents a novel approach to visual objects classification based on generating simple fuzzy classifiers using local image features to distinguish between one known class and other classes. Boosting meta learning is used to find the most representative local features. The proposed approach is tested on a state-of-the-art image dataset and compared with the bag-of-features image representation model combined with the Support Vector Machine classification. The novel method gives better classification accuracy and the time of learning and testing process is more than 30% shorter. version:1
arxiv-1610-01066 | Sparsity-based Color Image Super Resolution via Exploiting Cross Channel Constraints | http://arxiv.org/abs/1610.01066 | id:1610.01066 author:Hojjat S. Mousavi, Vishal Monga category:cs.CV  published:2016-10-04 summary:Sparsity constrained single image super-resolution (SR) has been of much recent interest. A typical approach involves sparsely representing patches in a low-resolution (LR) input image via a dictionary of example LR patches, and then using the coefficients of this representation to generate the high-resolution (HR) output via an analogous HR dictionary. However, most existing sparse representation methods for super resolution focus on the luminance channel information and do not capture interactions between color channels. In this work, we extend sparsity based super-resolution to multiple color channels by taking color information into account. Edge similarities amongst RGB color bands are exploited as cross channel correlation constraints. These additional constraints lead to a new optimization problem which is not easily solvable; however, a tractable solution is proposed to solve it efficiently. Moreover, to fully exploit the complementary information among color channels, a dictionary learning method is also proposed specifically to learn color dictionaries that encourage edge similarities. Merits of the proposed method over state of the art are demonstrated both visually and quantitatively using image quality metrics. version:1
arxiv-1610-01052 | A novel and effective scoring scheme for structure classification and pairwise similarity measurement | http://arxiv.org/abs/1610.01052 | id:1610.01052 author:Rezaul Karim, Md. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman category:cs.CV  published:2016-10-04 summary:Protein tertiary structure defines its functions, classification and binding sites. Similar structural characteristics between two proteins often lead to the similar characteristics thereof. Determining structural similarity accurately in real time is a crucial research issue. In this paper, we present a novel and effective scoring scheme that is dependent on novel features extracted from protein alpha carbon distance matrices. Our scoring scheme is inspired from pattern recognition and computer vision. Our method is significantly better than the current state of the art methods in terms of family match of pairs of protein structures and other statistical measurements. The effectiveness of our method is tested on standard benchmark structures. A web service is available at http://research.buet.ac.bd:8080/Comograd/score.html where you can get the similarity measurement score between two protein structures based on our method. version:1
arxiv-1610-01000 | Real-time wind power forecast | http://arxiv.org/abs/1610.01000 | id:1610.01000 author:AurÃ©lie Fischer, Lucie Montuelle, Mathilde Mougeot, Dominique Picard category:stat.AP stat.ML  published:2016-10-04 summary:We focus on short-term wind power forecast using machine learning techniques. We show on real data provided by the wind energy company Maia Eolis, that parametric models, even following closely the physical equation relating wind production to wind speed are out-performed by intelligent learning algorithms. In particular, the CART-Bagging algorithm gives very stable and promising results. Besides, we show on this application that the default methodology to select a subset of predictors provided in the standard random forest package can be refined, especially when there exists among the predictors one variable which has a major impact. version:1
arxiv-1610-00970 | Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite-Sum Structure | http://arxiv.org/abs/1610.00970 | id:1610.00970 author:Alberto Bietti, Julien Mairal category:stat.ML cs.LG math.OC  published:2016-10-04 summary:Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. However, in the context of empirical risk minimization, it is often helpful to augment the training set by considering random perturbations of input examples. In this case, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for this setting when the objective is strongly convex. After an initial linearly convergent phase, the algorithm achieves a $O(1/t)$ convergence rate in expectation like SGD, but with a constant factor that is typically much smaller, depending on the variance of gradient estimates due to perturbations on a single example. version:1
arxiv-1610-00960 | QuickeNing: A Generic Quasi-Newton Algorithm for Faster Gradient-Based Optimization * | http://arxiv.org/abs/1610.00960 | id:1610.00960 author:Hongzhou Lin, Julien Mairal, Zaid Harchaoui category:stat.ML math.OC  published:2016-10-04 summary:We propose an approach to accelerate gradient-based optimization algorithms by giving them the ability to exploit curvature information using quasi-Newton update rules. The proposed scheme, called QuickeNing, is generic and can be applied to a large class of first-order methods such as incremental and block-coordinate algorithms; it is also compatible with composite objectives, meaning that it has the ability to provide exactly sparse solutions when the objective involves a sparsity-inducing regularization. QuickeNing relies on limited-memory BFGS rules, making it appropriate for solving high-dimensional optimization problems; with no line-search, it is also simple to use and to implement. Besides, it enjoys a worst-case linear convergence rate for strongly convex problems. We present experimental results where QuickeNing gives significant improvements over competing methods for solving large-scale high-dimensional machine learning problems. version:1
arxiv-1610-00956 | Embracing data abundance: BookTest Dataset for Reading Comprehension | http://arxiv.org/abs/1610.00956 | id:1610.00956 author:Ondrej Bajgar, Rudolf Kadlec, Jan Kleindienst category:cs.CL cs.AI cs.LG cs.NE  published:2016-10-04 summary:There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement. version:1
arxiv-1609-08810 | Effective Combination of Language and Vision Through Model Composition and the R-CCA Method | http://arxiv.org/abs/1609.08810 | id:1609.08810 author:Hagar Loeub, Roi Reichart category:cs.CL  published:2016-09-28 summary:We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields. version:2
arxiv-1610-00907 | Model Selection for Gaussian Process Regression by Approximation Set Coding | http://arxiv.org/abs/1610.00907 | id:1610.00907 author:Benjamin Fischer, Nico Gorbach, Stefan Bauer, Yatao Bian, Joachim M. Buhmann category:stat.ML  published:2016-10-04 summary:Gaussian processes are powerful, yet analytically tractable models for supervised learning. A Gaussian process is characterized by a mean function and a covariance function (kernel), which are determined by a model selection criterion. The functions to be compared do not just differ in their parametrization but in their fundamental structure. It is often not clear which function structure to choose, for instance to decide between a squared exponential and a rational quadratic kernel. Based on the principle of approximation set coding, we develop a framework for model selection to rank kernels for Gaussian process regression. In our experiments approximation set coding shows promise to become a model selection criterion competitive with maximum evidence (also called marginal likelihood) and leave-one-out cross-validation. version:1
arxiv-1109-1844 | Weighted Clustering | http://arxiv.org/abs/1109.1844 | id:1109.1844 author:Margareta Ackerman, Shai Ben-David, Simina BrÃ¢nzei, David Loker category:cs.LG  published:2011-09-08 summary:One of the most prominent challenges in clustering is "the user's dilemma," which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this problem relies on the identification of succinct, user-friendly properties that formally capture when certain clustering methods are preferred over others. Until now these properties focused on advantages of classical Linkage-Based algorithms, failing to identify when other clustering paradigms, such as popular center-based methods, are preferable. We present surprisingly simple new properties that delineate the differences between common clustering paradigms, which clearly and formally demonstrates advantages of center-based approaches for some applications. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. version:2
arxiv-1610-00893 | Compressed Sensing and Adaptive Graph Total Variation for Tomographic Reconstructions | http://arxiv.org/abs/1610.00893 | id:1610.00893 author:Faisal Mahmood, Nauman Shahid, Ulf Skoglund, Pierre Vandergheynst category:cs.CV  published:2016-10-04 summary:Compressed Sensing (CS) and Total Variation (TV)- based iterative image reconstruction algorithms have received increased attention recently. This is due to the ability of such methods to reconstruct from limited and noisy data. Local TV methods fail to preserve texture details and fine structures, which are tedious for the method to distinguish from noise. In many cases local methods also create additional artifacts due to over smoothing. Non-Local Total Variation (NLTV) has been increasingly used for medical imaging applications. However, it is not updated in every iteration of the algorithm, has a high computational complexity and depends on the scale of pairwise parameters. In this work we propose using Adaptive Graph- based TV in combination with CS (ACSGT). Similar to NLTV our proposed method goes beyond spatial similarity between different regions of an image being reconstructed by establishing a connection between similar regions in the image regardless of spatial distance. However, it is computationally much more efficient and scalable when compared to NLTV due to the use of approximate nearest neighbor search algorithm. Moreover, our method is adaptive, i.e, it involves updating the graph prior every iteration making the connection between similar regions stronger. Since TV is a special case of graph TV the proposed method can be seen as a generalization of CS and TV methods. We test our proposed algorithm by reconstructing a variety of different phantoms from limited and corrupted data and observe that we achieve a better result with ACSGT in every case. version:1
arxiv-1610-00889 | Cardea: Context-Aware Visual Privacy Protection from Pervasive Cameras | http://arxiv.org/abs/1610.00889 | id:1610.00889 author:Jiayu Shu, Rui Zheng, Pan Hui category:cs.CR cs.CV  published:2016-10-04 summary:The growing popularity of mobile and wearable devices with built-in cameras, the bright prospect of camera related applications such as augmented reality and life-logging system, the increased ease of taking and sharing photos, and advances in computer vision techniques have greatly facilitated people's lives in many aspects, but have also inevitably raised people's concerns about visual privacy at the same time. Motivated by recent user studies that people's privacy concerns are dependent on the context, in this paper, we propose Cardea, a context-aware and interactive visual privacy protection framework that enforces privacy protection according to people's privacy preferences. The framework provides people with fine-grained visual privacy protection using: i) personal privacy profiles, with which people can define their context-dependent privacy preferences; and ii) visual indicators: face features, for devices to automatically locate individuals who request privacy protection; and iii) hand gestures, for people to flexibly interact with cameras to temporarily change their privacy preferences. We design and implement the framework consisting of the client app on Android devices and the cloud server. Our evaluation results confirm this framework is practical and effective with 86% overall accuracy, showing promising future for context-aware visual privacy protection from pervasive cameras. version:1
arxiv-1610-00883 | Are Word Embedding-based Features Useful for Sarcasm Detection? | http://arxiv.org/abs/1610.00883 | id:1610.00883 author:Aditya Joshi, Vaibhav Tripathi, Kevin Patel, Pushpak Bhattacharyya, Mark Carman category:cs.CL  published:2016-10-04 summary:This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4\% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection. version:1
arxiv-1610-00879 | A Computational Approach to Automatic Prediction of Drunk Texting | http://arxiv.org/abs/1610.00879 | id:1610.00879 author:Aditya Joshi, Abhijit Mishra, Balamurali AR, Pushpak Bhattacharyya, Mark Carman category:cs.CL  published:2016-10-04 summary:Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting. version:1
arxiv-1610-00852 | Ensemble Maximum Entropy Classification and Linear Regression for Author Age Prediction | http://arxiv.org/abs/1610.00852 | id:1610.00852 author:Joey Hong, Chris Mattmann, Paul Ramirez category:cs.LG cs.CL  published:2016-10-04 summary:The evolution of the internet has created an abundance of unstructured data on the web, a significant part of which is textual. The task of author profiling seeks to find the demographics of people solely from their linguistic and content-based features in text. The ability to describe traits of authors clearly has applications in fields such as security and forensics, as well as marketing. Instead of seeing age as just a classification problem, we also frame age as a regression one, but use an ensemble chain method that incorporates the power of both classification and regression to learn the authors exact age. version:1
arxiv-1610-00844 | Revisiting Role Discovery in Networks: From Node to Edge Roles | http://arxiv.org/abs/1610.00844 | id:1610.00844 author:Nesreen K. Ahmed, Ryan A. Rossi, Theodore L. Willke, Rong Zhou category:stat.ML cs.LG cs.SI  published:2016-10-04 summary:Previous work in network analysis has focused on modeling the mixed-memberships of node roles in the graph, but not the roles of edges. We introduce the edge role discovery problem and present a generalizable framework for learning and extracting edge roles from arbitrary graphs automatically. Furthermore, while existing node-centric role models have mainly focused on simple degree and egonet features, this work also explores graphlet features for role discovery. In addition, we also develop an approach for automatically learning and extracting important and useful edge features from an arbitrary graph. The experimental results demonstrate the utility of edge roles for network analysis tasks on a variety of graphs from various problem domains. version:1
arxiv-1610-00843 | The Search Problem in Mixture Models | http://arxiv.org/abs/1610.00843 | id:1610.00843 author:Avik Ray, Joe Neeman, Sujay Sanghavi, Sanjay Shakkottai category:stat.ML cs.LG  published:2016-10-04 summary:We consider the task of learning the parameters of a {\em single} component of a mixture model, for the case when we are given {\em side information} about that component; we call this the "search problem" in mixture models. We would like to solve this with computational and sample complexity lower than solving the overall original problem, where one learns parameters of all components. Our main contributions are the development of a simple but general model for the notion of side information, and a corresponding simple matrix-based algorithm for solving the search problem in this general setting. We then specialize this model and algorithm to four common scenarios: Gaussian mixture models, LDA topic models, subspace clustering, and mixed linear regression. For each one of these we show that if (and only if) the side information is informative, we obtain better sample complexity than existing standard mixture model algorithms (e.g. tensor methods). We also illustrate several natural ways one can obtain such side information, for specific problem instances. Our experiments on real datasets (NY Times, Yelp, BSDS500) further demonstrate the practicality of our algorithms showing significant improvement in runtime and accuracy. version:1
arxiv-1610-00842 | Chinese Event Extraction Using DeepNeural Network with Word Embedding | http://arxiv.org/abs/1610.00842 | id:1610.00842 author:Yandi Xia, Yang Liu category:cs.CL  published:2016-10-04 summary:A lot of prior work on event extraction has exploited a variety of features to represent events. Such methods have several drawbacks: 1) the features are often specific for a particular domain and do not generalize well; 2) the features are derived from various linguistic analyses and are error-prone; and 3) some features may be expensive and require domain expert. In this paper, we develop a Chinese event extraction system that uses word embedding vectors to represent language, and deep neural networks to learn the abstract feature representation in order to greatly reduce the effort of feature engineering. In addition, in this framework, we leverage large amount of unlabeled data, which can address the problem of limited labeled corpus for this task. Our experiments show that our proposed method performs better compared to the system using rich language features, and using unlabeled data benefits the word embeddings. This study suggests the potential of DNN and word embedding for the event extraction task. version:1
arxiv-1610-00838 | Image Aesthetic Assessment: An Experimental Survey | http://arxiv.org/abs/1610.00838 | id:1610.00838 author:Yubin Deng, Chen Change Loy, Xiaoou Tang category:cs.CV  published:2016-10-04 summary:This survey aims at reviewing recent techniques used in the assessment of image aesthetic quality. The assessment of image aesthetic quality is the process of computationally distinguishing high-quality photos from low-quality ones based on photographic rules or artistic perceptions. A variety of approaches have been proposed in the literature trying to solve this challenging problem. In this survey, we present a systematic listing of the reviewed approaches based on feature types (hand-crafted features and deep features) and evaluation criteria (dataset characteristics and evaluation metrics). Main contributions and novelties of the reviewed approaches are highlighted and discussed. In addition, following the emergence of deep learning techniques, we systematically evaluate recent deep learning settings that are useful for developing a robust deep model for aesthetic scoring. Experiments are conducted using simple yet solid baselines that are competitive with the current state-of-the-arts. Moreover, we discuss the relation between image aesthetic assessment and automatic image cropping. We hope that this survey could serve as a comprehensive reference source for future research on the study of image aesthetic assessment. version:1
arxiv-1610-00824 | Real Time Fine-Grained Categorization with Accuracy and Interpretability | http://arxiv.org/abs/1610.00824 | id:1610.00824 author:Shaoli Huang, Dacheng Tao category:cs.CV  published:2016-10-04 summary:A well-designed fine-grained categorization system usually has three contradictory requirements: accuracy (the ability to identify objects among subordinate categories); interpretability (the ability to provide human-understandable explanation of recognition system behavior); and efficiency (the speed of the system). To handle the trade-off between accuracy and interpretability, we propose a novel "Deeper Part-Stacked CNN" architecture armed with interpretability by modeling subtle differences between object parts. The proposed architecture consists of a part localization network, a two-stream classification network that simultaneously encodes object-level and part-level cues, and a feature vectors fusion component. Specifically, the part localization network is implemented by exploring a new paradigm for key point localization that first samples a small number of representable pixels and then determine their labels via a convolutional layer followed by a softmax layer. We also use a cropping layer to extract part features and propose a scale mean-max layer for feature fusion learning. Experimentally, our proposed method outperform state-of-the-art approaches both in part localization task and classification task on Caltech-UCSD Birds-200-2011. Moreover, by adopting a set of sharing strategies between the computation of multiple object parts, our single model is fairly efficient running at 32 frames/sec. version:1
arxiv-1610-00790 | Adaptive Neuron Apoptosis for Accelerating Deep Learning on Large Scale Systems | http://arxiv.org/abs/1610.00790 | id:1610.00790 author:Charles Siegel, Jeff Daily, Abhinav Vishnu category:cs.NE  published:2016-10-03 summary:We present novel techniques to accelerate the convergence of Deep Learning algorithms by conducting low overhead removal of redundant neurons -- apoptosis of neurons -- which do not contribute to model learning, during the training phase itself. We provide in-depth theoretical underpinnings of our heuristics (bounding accuracy loss and handling apoptosis of several neuron types), and present the methods to conduct adaptive neuron apoptosis. Specifically, we are able to improve the training time for several datasets by 2-3x, while reducing the number of parameters by up to 30x (4-5x on average) on datasets such as ImageNet classification. For the Higgs Boson dataset, our implementation improves the accuracy (measured by Area Under Curve (AUC)) for classification from 0.88/1 to 0.94/1, while reducing the number of parameters by 3x in comparison to existing literature. The proposed methods achieve a 2.44x speedup in comparison to the default (no apoptosis) algorithm. version:1
arxiv-1610-01145 | Error bounds for approximations with deep ReLU networks | http://arxiv.org/abs/1610.01145 | id:1610.01145 author:Dmitry Yarotsky category:cs.LG cs.NE  published:2016-10-03 summary:We study how approximation errors of neural networks with ReLU activation functions depend on the depth of the network. We establish rigorous error bounds showing that deep ReLU networks are significantly more expressive than shallow ones as long as approximations of smooth functions are concerned. At the same time, we show that on a set of functions constrained only by their degree of smoothness, a ReLU network architecture cannot in general achieve approximation accuracy with better than a power law dependence on the network size, regardless of its depth. version:1
arxiv-1610-00765 | Grounding the Lexical Sets of Causative-Inchoative Verbs with Word Embedding | http://arxiv.org/abs/1610.00765 | id:1610.00765 author:Edoardo Maria Ponti, Elisabetta Jezek, Bernardo Magnini category:cs.CL  published:2016-10-03 summary:Lexical sets contain the words filling the argument positions of a verb in one of its senses. They can be grounded empirically through their automatic extraction from corpora. The purpose of this paper is demonstrating that their vector representation based on word embedding provides insights onto many linguistic phenomena, and in particular about verbs undergoing the causative-inchoative alternation. A first experiment aims at investigating the internal structure of the sets, which are known to be radial and continuous categories cognitively. A second experiment shows that the distance between the subject set and object set is correlated with a semantic factor, namely the spontaneity of the verb. version:1
arxiv-1610-00759 | Prediction of Manipulation Actions | http://arxiv.org/abs/1610.00759 | id:1610.00759 author:Cornelia FermÃ¼ller, Fang Wang, Yezhou Yang, Konstantinos Zampogiannis, Yi Zhang, Francisco Barranco, Michael Pfeiffer category:cs.CV  published:2016-10-03 summary:Looking at a person's hands one often can tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor's intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded from subjects performing different manipulation actions on the same object, such as "squeezing", "flipping", "washing", "wiping" and "scratching" with a sponge. In psychophysical experiments, we evaluated human observers' skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets showed that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithm to predict what and how a dexterous action is performed. version:1
arxiv-1610-00748 | Real-Time RGB-D based Template Matching Pedestrian Detection | http://arxiv.org/abs/1610.00748 | id:1610.00748 author:Omid Hosseini jafari, Michael Ying Yang category:cs.CV cs.RO  published:2016-10-03 summary:Pedestrian detection is one of the most popular topics in computer vision and robotics. Considering challenging issues in multiple pedestrian detection, we present a real-time depth-based template matching people detector. In this paper, we propose different approaches for training the depth-based template. We train multiple templates for handling issues due to various upper-body orientations of the pedestrians and different levels of detail in depth-map of the pedestrians with various distances from the camera. And, we take into account the degree of reliability for different regions of sliding window by proposing the weighted template approach. Furthermore, we combine the depth-detector with an appearance based detector as a verifier to take advantage of the appearance cues for dealing with the limitations of depth data. We evaluate our method on the challenging ETH dataset sequence. We show that our method outperforms the state-of-the-art approaches. version:1
arxiv-1610-00731 | Can Ground Truth Label Propagation from Video help Semantic Segmentation? | http://arxiv.org/abs/1610.00731 | id:1610.00731 author:Siva Karthik Mustikovela, Michael Ying Yang, Carsten Rother category:cs.CV  published:2016-10-03 summary:For state-of-the-art semantic segmentation task, training convolutional neural networks (CNNs) requires dense pixelwise ground truth (GT) labeling, which is expensive and involves extensive human effort. In this work, we study the possibility of using auxiliary ground truth, so-called \textit{pseudo ground truth} (PGT) to improve the performance. The PGT is obtained by propagating the labels of a GT frame to its subsequent frames in the video using a simple CRF-based, cue integration framework. Our main contribution is to demonstrate the use of noisy PGT along with GT to improve the performance of a CNN. We perform a systematic analysis to find the right kind of PGT that needs to be added along with the GT for training a CNN. In this regard, we explore three aspects of PGT which influence the learning of a CNN: i) the PGT labeling has to be of good quality; ii) the PGT images have to be different compared to the GT images; iii) the PGT has to be trusted differently than GT. We conclude that PGT which is diverse from GT images and has good quality of labeling can indeed help improve the performance of a CNN. Also, when PGT is multiple folds larger than GT, weighing down the trust on PGT helps in improving the accuracy. Finally, We show that using PGT along with GT, the performance of Fully Convolutional Network (FCN) on Camvid data is increased by $2.7\%$ on IoU accuracy. We believe such an approach can be used to train CNNs for semantic video segmentation where sequentially labeled image frames are needed. To this end, we provide recommendations for using PGT strategically for semantic segmentation and hence bypass the need for extensive human efforts in labeling. version:1
arxiv-1610-00696 | Deep Visual Foresight for Planning Robot Motion | http://arxiv.org/abs/1610.00696 | id:1610.00696 author:Chelsea Finn, Sergey Levine category:cs.LG cs.AI cs.CV cs.RO  published:2016-10-03 summary:A key challenge in scaling up robot learning to many skills and environments is removing the need for human supervision, so that robots can collect their own data and improve their own performance without being limited by the cost of requesting human feedback. Model-based reinforcement learning holds the promise of enabling an agent to learn to predict the effects of its actions, which could provide flexible predictive models for a wide range of tasks and environments, without detailed human supervision. We develop a method for combining deep action-conditioned video prediction models with model-predictive control that uses entirely unlabeled training data. Our approach does not require a calibrated camera, an instrumented training set-up, nor precise sensing and actuation. Our results show that our method enables a real robot to perform nonprehensile manipulation -- pushing objects -- and can handle novel objects not seen during training. version:1
arxiv-1610-00681 | Network structures and fast distributed MMSE estimation | http://arxiv.org/abs/1610.00681 | id:1610.00681 author:Muhammed O. Sayin, Suleyman S. Kozat category:cs.SY cs.LG  published:2016-10-03 summary:We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation. version:1
arxiv-1610-00673 | Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search | http://arxiv.org/abs/1610.00673 | id:1610.00673 author:Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, Sergey Levine category:cs.LG cs.AI cs.RO I.2.6; I.2.9  published:2016-10-03 summary:In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative. version:1
arxiv-1610-00667 | Data Integration with High Dimensionality | http://arxiv.org/abs/1610.00667 | id:1610.00667 author:Xin Gao, Raymond J. Carroll category:stat.ML math.ST stat.TH  published:2016-10-03 summary:We consider a problem of data integration. Consider determining which genes affect a disease. The genes, which we call predictor objects, can be measured in different experiments on the same individual. We address the question of finding which genes are predictors of disease by any of the experiments. Our formulation is more general. In a given data set, there are a fixed number of responses for each individual, which may include a mix of discrete, binary and continuous variables. There is also a class of predictor objects, which may differ within a subject depending on how the predictor object is measured, i.e., depend on the experiment. The goal is to select which predictor objects affect any of the responses, where the number of such informative predictor objects or features tends to infinity as sample size increases. There are marginal likelihoods for each way the predictor object is measured, i.e., for each experiment. We specify a pseudolikelihood combining the marginal likelihoods, and propose a pseudolikelihood information criterion. Under regularity conditions, we establish selection consistency for the pseudolikelihood information criterion with unbounded true model size, which includes a Bayesian information criterion with appropriate penalty term as a special case. Simulations indicate that data integration improves upon, sometimes dramatically, using only one of the data sources. version:1
arxiv-1610-00660 | Kernel Selection using Multiple Kernel Learning and Domain Adaptation in Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance Scenario | http://arxiv.org/abs/1610.00660 | id:1610.00660 author:Samik Banerjee, Sukhendu Das category:cs.CV cs.LG  published:2016-10-03 summary:Face Recognition (FR) has been the interest to several researchers over the past few decades due to its passive nature of biometric authentication. Despite high accuracy achieved by face recognition algorithms under controlled conditions, achieving the same performance for face images obtained in surveillance scenarios, is a major hurdle. Some attempts have been made to super-resolve the low-resolution face images and improve the contrast, without considerable degree of success. The proposed technique in this paper tries to cope with the very low resolution and low contrast face images obtained from surveillance cameras, for FR under surveillance conditions. For Support Vector Machine classification, the selection of appropriate kernel has been a widely discussed issue in the research community. In this paper, we propose a novel kernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to obtain the best feature-kernel pairing. Our proposed technique employs a effective kernel selection by Multiple Kernel Learning (MKL) method, to choose the optimal kernel to be used along with unsupervised domain adaptation method in the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem. Rigorous experimentation has been performed on three real-world surveillance face datasets : FR\_SURV, SCface and ChokePoint. Results have been shown using Rank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method outperforms all other recent state-of-the-art techniques by a considerable margin. version:1
arxiv-1610-01206 | Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods | http://arxiv.org/abs/1610.01206 | id:1610.01206 author:Yingming Li, Ming Yang, Zhongfei Zhang category:cs.LG cs.CV cs.IR  published:2016-10-03 summary:Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its several extensions. And then we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and current developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications. version:1
arxiv-1610-00634 | Orthographic Syllable as basic unit for SMT between Related Languages | http://arxiv.org/abs/1610.00634 | id:1610.00634 author:Anoop Kunchukuttan, Pushpak Bhattacharyya category:cs.CL  published:2016-10-03 summary:We explore the use of the orthographic syllable, a variable-length consonant-vowel sequence, as a basic unit of translation between related languages which use abugida or alphabetic scripts. We show that orthographic syllable level translation significantly outperforms models trained over other basic units (word, morpheme and character) when training over small parallel corpora. version:1
arxiv-1610-00633 | Deep Reinforcement Learning for Robotic Manipulation | http://arxiv.org/abs/1610.00633 | id:1610.00633 author:Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine category:cs.RO cs.AI cs.LG  published:2016-10-03 summary:Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations. version:1
arxiv-1610-00602 | Multimodal Semantic Simulations of Linguistically Underspecified Motion Events | http://arxiv.org/abs/1610.00602 | id:1610.00602 author:Nikhil Krishnaswamy, James Pustejovsky category:cs.CL  published:2016-10-03 summary:In this paper, we describe a system for generating three-dimensional visual simulations of natural language motion expressions. We use a rich formal model of events and their participants to generate simulations that satisfy the minimal constraints entailed by the associated utterance, relying on semantic knowledge of physical objects and motion events. This paper outlines technical considerations and discusses implementing the aforementioned semantic models into such a system. version:1
arxiv-1610-00572 | An Arabic-Hebrew parallel corpus of TED talks | http://arxiv.org/abs/1610.00572 | id:1610.00572 author:Mauro Cettolo category:cs.CL cs.IR  published:2016-10-03 summary:We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3, the Web inventory that repurposes the original content of the TED website in a way which is more convenient for MT researchers. The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign. In addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark. version:1
arxiv-1610-00564 | End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural Networks | http://arxiv.org/abs/1610.00564 | id:1610.00564 author:Timothy J. O'Shea, Seth Hitefield, Johnathan Corgan category:cs.LG cs.NI  published:2016-10-03 summary:We investigate sequence machine learning techniques on raw radio signal time-series data. By applying deep recurrent neural networks we learn to discriminate between several application layer traffic types on top of a constant envelope modulation without using an expert demodulation algorithm. We show that complex protocol sequences can be learned and used for both classification and generation tasks using this approach. version:1
arxiv-1610-00529 | Path Integral Guided Policy Search | http://arxiv.org/abs/1610.00529 | id:1610.00529 author:Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, Sergey Levine category:cs.RO cs.LG  published:2016-10-03 summary:We present a policy search method for learning complex feedback control policies that map from high-dimensional sensory inputs to motor torques, for manipulation tasks with discontinuous contact dynamics. We build on a prior technique called guided policy search (GPS), which iteratively optimizes a set of local policies for specific instances of a task, and uses these to train a complex, high-dimensional global policy that generalizes across task instances. We extend GPS in the following ways: (1) we propose the use of a model-free local optimizer based on path integral stochastic optimal control (PI2), which enables us to learn local policies for tasks with highly discontinuous contact dynamics; and (2) we enable GPS to train on a new set of task instances in every iteration by using on-policy sampling: this increases the diversity of the instances that the policy is trained on, and is crucial for achieving good generalization. We show that these contributions enable us to learn deep neural network policies that can directly perform torque control from visual input. We validate the method on a challenging door opening task and a pick-and-place task, and we demonstrate that our approach substantially outperforms the prior LQR-based local policy optimizer on these tasks. Furthermore, we show that on-policy sampling significantly increases the generalization ability of these policies. version:1
arxiv-1610-00527 | Video Pixel Networks | http://arxiv.org/abs/1610.00527 | id:1610.00527 author:Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, Koray Kavukcuoglu category:cs.CV cs.LG  published:2016-10-03 summary:We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects. version:1
arxiv-1610-00520 | Semi-supervised Learning with Sparse Autoencoders in Phone Classification | http://arxiv.org/abs/1610.00520 | id:1610.00520 author:Akash Kumar Dhaka, Giampiero Salvi category:stat.ML cs.CL cs.LG  published:2016-10-03 summary:We propose the application of a semi-supervised learning method to improve the performance of acoustic modelling for automatic speech recognition based on deep neural net- works. As opposed to unsupervised initialisation followed by supervised fine tuning, our method takes advantage of both unlabelled and labelled data simultaneously through mini- batch stochastic gradient descent. We tested the method with varying proportions of labelled vs unlabelled observations in frame-based phoneme classification on the TIMIT database. Our experiments show that the method outperforms standard supervised training for an equal amount of labelled data and provides competitive error rates compared to state-of-the-art graph-based semi-supervised learning techniques. version:1
arxiv-1610-00494 | The Blessing of Dimensionality: Separation Theorems in the Thermodynamic Limit | http://arxiv.org/abs/1610.00494 | id:1610.00494 author:Alexander N. Gorban, Ivan Yu. Tyukin, Ilya Romanenko category:stat.ML cs.LG  published:2016-10-03 summary:We consider and analyze properties of large sets of randomly selected (i.i.d.) points in high dimensional spaces. In particular, we consider the problem of whether a single data point that is randomly chosen from a finite set of points can be separated from the rest of the data set by a linear hyperplane. We formulate and prove stochastic separation theorems, including: 1) with probability close to one a random point may be separated from a finite random set by a linear functional; 2) with probability close to one for every point in a finite random set there is a linear functional separating this point from the rest of the data. The total number of points in the random sets are allowed to be exponentially large with respect to dimension. Various laws governing distributions of points are considered, and explicit formulae for the probability of separation are provided. These theorems reveal an interesting implication for machine learning and data mining applications that deal with large data sets (big data) and high-dimensional data (many attributes): simple linear decision rules and learning machines are surprisingly efficient tools for separating and filtering out arbitrarily assigned points in large dimensions. version:1
arxiv-1610-00479 | Nonsymbolic Text Representation | http://arxiv.org/abs/1610.00479 | id:1610.00479 author:Hinrich Schuetze category:cs.CL  published:2016-10-03 summary:We introduce the first generic text representation model that is completely nonsymbolic, i.e., it does not require the availability of a segmentation or tokenization method that attempts to identify words or other symbolic units in text. This applies to training the parameters of the model on a training corpus as well as to applying it when computing the representation of a new text. We show that our model performs better than prior work on an information extraction and a text denoising task. version:1
arxiv-1610-00470 | A new kernel-based approach to system identification with quantized output data | http://arxiv.org/abs/1610.00470 | id:1610.00470 author:Giulio Bottegal, HÃ¥kan Hjalmarsson, Gianluigi Pillonetto category:cs.SY stat.ML  published:2016-10-03 summary:In this paper we introduce a novel method for linear system identification with quantized output data. We model the impulse response as a zero-mean Gaussian process whose covariance (kernel) is given by the recently proposed stable spline kernel, which encodes information on regularity and exponential stability. This serves as a starting point to cast our system identification problem into a Bayesian framework. We employ Markov Chain Monte Carlo methods to provide an estimate of the system. In particular, we design two methods based on the so-called Gibbs sampler that allow also to estimate the kernel hyperparameters by marginal likelihood maximization via the Expectation-Maximization method. Numerical simulations show the effectiveness of the proposed scheme, as compared to the state-of-the-art kernel-based methods when these are employed in system identification with quantized data. version:1
arxiv-1610-00465 | Can Evolutionary Sampling Improve Bagged Ensembles? | http://arxiv.org/abs/1610.00465 | id:1610.00465 author:Harsh Nisar, Bhanu Pratap Singh Rawat category:cs.LG cs.AI  published:2016-10-03 summary:Perturb and Combine (P&C) group of methods generate multiple versions of the predictor by perturbing the training set or construction and then combining them into a single predictor (Breiman, 1996b). The motive is to improve the accuracy in unstable classification and regression methods. One of the most well known method in this group is Bagging. Arcing or Adaptive Resampling and Combining methods like AdaBoost are smarter variants of P&C methods. In this extended abstract, we lay the groundwork for a new family of methods under the P&C umbrella, known as Evolutionary Sampling (ES). We employ Evolutionary algorithms to suggest smarter sampling in both the feature space (sub-spaces) as well as training samples. We discuss multiple fitness functions to assess ensembles and empirically compare our performance against randomized sampling of training data and feature sub-spaces. version:1
arxiv-1609-08905 | Statistical comparison of classifiers through Bayesian hierarchical modelling | http://arxiv.org/abs/1609.08905 | id:1609.08905 author:Giorgio Corani, Alessio Benavoli, Janez DemÅ¡ar, Francesca Mangili, Marco Zaffalon category:cs.LG stat.ME stat.ML  published:2016-09-28 summary:We propose a new approach for the statistical comparison of algorithms which have been cross-validated on multiple data sets. It is a Bayesian hierarchical method; it draws inferences on single and on multiple datasets taking into account the mean and the variability of the cross-validation results. It is able to detect equivalent classifiers and to claim significances which have a practical impact. On each data sets it estimates more accurately than the existing methods the difference of accuracy between the two classifiers thanks to shrinkage. Such advantages are demonstrated by simulations on synthetic and real data. version:2
arxiv-1610-00427 | Rain structure transfer using an exemplar rain image for synthetic rain image generation | http://arxiv.org/abs/1610.00427 | id:1610.00427 author:Chang-Hwan Son, Xiao-Ping Zhang category:cs.CV  published:2016-10-03 summary:This letter proposes a simple method of transferring rain structures of a given exemplar rain image into a target image. Given the exemplar rain image and its corresponding masked rain image, rain patches including rain structures are extracted randomly, and then residual rain patches are obtained by subtracting those rain patches from their mean patches. Next, residual rain patches are selected randomly, and then added to the given target image along a raster scanning direction. To decrease boundary artifacts around the added patches on the target image, minimum error boundary cuts are found using dynamic programming, and then blending is conducted between overlapping patches. Our experiment shows that the proposed method can generate realistic rain images that have similar rain structures in the exemplar images. Moreover, it is expected that the proposed method can be used for rain removal. More specifically, natural images and synthetic rain images generated via the proposed method can be used to learn classifiers, for example, deep neural networks, in a supervised manner. version:1
arxiv-1610-00410 | On the Empirical Effect of Gaussian Noise in Under-sampled MRI Reconstruction | http://arxiv.org/abs/1610.00410 | id:1610.00410 author:Patrick Virtue, Michael Lustig category:cs.CV cs.IT math.IT physics.med-ph  published:2016-10-03 summary:In Fourier-based medical imaging, sampling below the Nyquist rate results in an underdetermined system, in which linear reconstructions will exhibit artifacts. Another consequence of under-sampling is lower signal to noise ratio (SNR) due to fewer acquired measurements. Even if an oracle provided the information to perfectly disambiguate the underdetermined system, the reconstructed image could still have lower image quality than a corresponding fully sampled acquisition because of the reduced measurement time. The effects of lower SNR and the underdetermined system are coupled during reconstruction, making it difficult to isolate the impact of lower SNR on image quality. To this end, we present an image quality prediction process that reconstructs fully sampled, fully determined data with noise added to simulate the loss of SNR induced by a given under-sampling pattern. The resulting prediction image empirically shows the effect of noise in under-sampled image reconstruction without any effect from an underdetermined system. We discuss how our image quality prediction process can simulate the distribution of noise for a given under-sampling pattern, including variable density sampling that produces colored noise in the measurement data. An interesting consequence of our prediction model is that we can show that recovery from underdetermined non-uniform sampling is equivalent to a weighted least squares optimization that accounts for heterogeneous noise levels across measurements. Through a series of experiments with synthetic and in vivo datasets, we demonstrate the efficacy of the image quality prediction process and show that it provides a better estimation of reconstruction image quality than the corresponding fully-sampled reference image. version:1
arxiv-1610-00405 | Seeing into Darkness: Scotopic Visual Recognition | http://arxiv.org/abs/1610.00405 | id:1610.00405 author:Bo Chen, Pietro Perona category:cs.CV  published:2016-10-03 summary:Images are formed by counting how many photons traveling from a given set of directions hit an image sensor during a given time interval. When photons are few and far in between, the concept of `image' breaks down and it is best to consider directly the flow of photons. Computer vision in this regime, which we call `scotopic', is radically different from the classical image-based paradigm in that visual computations (classification, control, search) have to take place while the stream of photons is captured and decisions may be taken as soon as enough information is available. The scotopic regime is important for biomedical imaging, security, astronomy and many other fields. Here we develop a framework that allows a machine to classify objects with as few photons as possible, while maintaining the error rate below an acceptable threshold. A dynamic and asymptotically optimal speed-accuracy tradeoff is a key feature of this framework. We propose and study an algorithm to optimize the tradeoff of a convolutional network directly from lowlight images and evaluate on simulated images from standard datasets. Surprisingly, scotopic systems can achieve comparable classification performance as traditional vision systems while using less than 0.1% of the photons in a conventional image. In addition, we demonstrate that our algorithms work even when the illuminance of the environment is unknown and varying. Last, we outline a spiking neural network coupled with photon-counting sensors as a power-efficient hardware realization of scotopic algorithms. version:1
arxiv-1610-00386 | Rain Removal via Shrinkage-Based Sparse Coding and Learned Rain Dictionary | http://arxiv.org/abs/1610.00386 | id:1610.00386 author:Chang-Hwan Son, Xiao-Ping Zhang category:cs.CV  published:2016-10-03 summary:This paper introduces a new rain removal model based on the shrinkage of the sparse codes for a single image. Recently, dictionary learning and sparse coding have been widely used for image restoration problems. These methods can also be applied to the rain removal by learning two types of rain and non-rain dictionaries and forcing the sparse codes of the rain dictionary to be zero vectors. However, this approach can generate unwanted edge artifacts and detail loss in the non-rain regions. Based on this observation, a new approach for shrinking the sparse codes is presented in this paper. To effectively shrink the sparse codes in the rain and non-rain regions, an error map between the input rain image and the reconstructed rain image is generated by using the learned rain dictionary. Based on this error map, both the sparse codes of rain and non-rain dictionaries are used jointly to represent the image structures of objects and avoid the edge artifacts in the non-rain regions. In the rain regions, the correlation matrix between the rain and non-rain dictionaries is calculated. Then, the sparse codes corresponding to the highly correlated signal-atoms in the rain and non-rain dictionaries are shrunk jointly to improve the removal of the rain structures. The experimental results show that the proposed shrinkage-based sparse coding can preserve image structures and avoid the edge artifacts in the non-rain regions, and it can remove the rain structures in the rain regions. Also, visual quality evaluation confirms that the proposed method outperforms the conventional texture and rain removal methods. version:1
arxiv-1607-07429 | Much Ado About Time: Exhaustive Annotation of Temporal Data | http://arxiv.org/abs/1607.07429 | id:1607.07429 author:Gunnar A. Sigurdsson, Olga Russakovsky, Ali Farhadi, Ivan Laptev, Abhinav Gupta category:cs.HC cs.CV  published:2016-07-25 summary:Large-scale annotated datasets allow AI systems to learn from and build upon the knowledge of the crowd. Many crowdsourcing techniques have been developed for collecting image annotations. These techniques often implicitly rely on the fact that a new input image takes a negligible amount of time to perceive. In contrast, we investigate and determine the most cost-effective way of obtaining high-quality multi-label annotations for temporal data such as videos. Watching even a short 30-second video clip requires a significant time investment from a crowd worker; thus, requesting multiple annotations following a single viewing is an important cost-saving strategy. But how many questions should we ask per video? We conclude that the optimal strategy is to ask as many questions as possible in a HIT (up to 52 binary questions after watching a 30-second video clip in our experiments). We demonstrate that while workers may not correctly answer all questions, the cost-benefit analysis nevertheless favors consensus from multiple such cheap-yet-imperfect iterations over more complex alternatives. When compared with a one-question-per-video baseline, our method is able to achieve a 10% improvement in recall 76.7% ours versus 66.7% baseline) at comparable precision (83.8% ours versus 83.0% baseline) in about half the annotation time (3.8 minutes ours compared to 7.1 minutes baseline). We demonstrate the effectiveness of our method by collecting multi-label annotations of 157 human activities on 1,815 videos. version:2
arxiv-1610-00382 | Near-Infrared Coloring via a Contrast-Preserving Mapping Model | http://arxiv.org/abs/1610.00382 | id:1610.00382 author:Chang-Hwan Son, Xiao-Ping Zhang category:cs.CV  published:2016-10-03 summary:Near-infrared gray images captured together with corresponding visible color images have recently proven useful for image restoration and classification. This paper introduces a new coloring method to add colors to near-infrared gray images based on a contrast-preserving mapping model. A naive coloring method directly adds the colors from the visible color image to the near-infrared gray image; however, this method results in an unrealistic image because of the discrepancies in brightness and image structure between the captured near-infrared gray image and the visible color image. To solve the discrepancy problem, first we present a new contrast-preserving mapping model to create a new near-infrared gray image with a similar appearance in the luminance plane to the visible color image, while preserving the contrast and details of the captured near-infrared gray image. Then based on the proposed contrast-preserving mapping model, we develop a method to derive realistic colors that can be added to the newly created near-infrared gray image. Experimental results show that the proposed method can not only preserve the local contrasts and details of the captured near-infrared gray image, but transfers the realistic colors from the visible color image to the newly created near-infrared gray image. Experimental results also show that the proposed approach can be applied to near-infrared denoising. version:1
arxiv-1610-00369 | Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep Recurrent models | http://arxiv.org/abs/1610.00369 | id:1610.00369 author:A. Hassan, N. Mohammed, A. K. A. Azad category:cs.CL cs.IR cs.LG cs.NE  published:2016-10-02 summary:Sentiment Analysis (SA) is an action research area in the digital age. With rapid and constant growth of online social media sites and services, and the increasing amount of textual data such as - statuses, comments, reviews etc. available in them, application of automatic SA is on the rise. However, most of the research works on SA in natural language processing (NLP) are based on English language. Despite being the sixth most widely spoken language in the world, Bangla still does not have a large and standard dataset. Because of this, recent research works in Bangla have failed to produce results that can be both comparable to works done by others and reusable as stepping stones for future researchers to progress in this field. Therefore, we first tried to provide a textual dataset - that includes not just Bangla, but Romanized Bangla texts as well, is substantial, post-processed and multiple validated, ready to be used in SA experiments. We tested this dataset in Deep Recurrent model, specifically, Long Short Term Memory (LSTM), using two types of loss functions - binary crossentropy and categorical crossentropy, and also did some experimental pre-training by using data from one validation to pre-train the other and vice versa. Lastly, we documented the results along with some analysis on them, which were promising. version:1
arxiv-1610-00366 | Funneled Bayesian Optimization for Design, Tuning and Control of Autonomous Systems | http://arxiv.org/abs/1610.00366 | id:1610.00366 author:Ruben Martinez-Cantin category:cs.AI cs.LG stat.ML  published:2016-10-02 summary:Bayesian optimization has become a fundamental global optimization algorithm in many problems where sample efficiency is of paramount importance. Recently, there has been proposed a large number of new applications in fields such as robotics, machine learning, experimental design, simulation, etc. In this paper, we focus on several problems that appear in robotics and autonomous systems: algorithm tuning, automatic control and intelligent design. All those problems can be mapped to global optimization problems. However, they become hard optimization problems. Bayesian optimization internally uses a probabilistic surrogate model (e.g.: Gaussian process) to learn from the process and reduce the number of samples required. In order to generalize to unknown functions in a black-box fashion, the common assumption is that the underlying function can be modeled with a stationary process. Nonstationary Gaussian process regression cannot generalize easily and it typically requires prior knowledge of the function. Some works have designed techniques to generalize Bayesian optimization to nonstationary functions in an indirect way, but using techniques originally designed for regression, where the objective is to improve the quality of the surrogate model everywhere. Instead optimization should focus on improving the surrogate model near the optimum. In this paper, we present a novel kernel function specially designed for Bayesian optimization, that allows nonstationary behavior of the surrogate model in an adaptive local region. In our experiments, we found that this new kernel results in an improved local search (exploitation), without penalizing the global search (exploration). We provide results in well-known benchmarks and real applications. The new method outperforms the state of the art in Bayesian optimization both in stationary and nonstationary problems. version:1
arxiv-1610-00362 | An Optimal Treatment Assignment Strategy to Evaluate Demand Response Effect | http://arxiv.org/abs/1610.00362 | id:1610.00362 author:Pan Li, Baosen Zhang category:cs.SY math.OC stat.ML  published:2016-10-02 summary:Demand response is designed to motivate electricity customers to modify their loads at critical time periods. The accurate estimation of impact of demand response signals to customers' consumption is central to any successful program. In practice, learning these response is nontrivial because operators can only send a limited number of signals. In addition, customer behavior also depends on a large number of exogenous covariates. These two features lead to a high dimensional inference problem with limited number of observations. In this paper, we formulate this problem by using a multivariate linear model and adopt an experimental design approach to estimate the impact of demand response signals. We show that randomized assignment, which is widely used to estimate the average treatment effect, is not efficient in reducing the variance of the estimator when a large number of covariates is present. In contrast, we present a tractable algorithm that strategically assigns demand response signals to customers. This algorithm achieves the optimal reduction in estimation variance, independent of the number of covariates. The results are validated from simulations on synthetic data. version:1
arxiv-1610-00345 | Density Estimation with Distribution Element Trees | http://arxiv.org/abs/1610.00345 | id:1610.00345 author:Daniel W. Meyer category:stat.ME cs.MS stat.CO stat.ML 62G07 G.3  published:2016-10-02 summary:The estimation of probability densities based on available data is a central task in many statistical applications. Especially in the case of large ensembles with many samples or high-dimensional sample spaces, computationally efficient methods are needed. We propose a new method that is based on a decomposition of the distribution to be estimated in terms of so-called distribution elements (DEs). These elements enable an adaptive and hierarchical discretization of the sample space with small or large elements in regions with high and variable or low densities, respectively. The refinement strategy that we propose is based on statistical goodness-of-fit and independence tests that evaluate the local approximation of the distribution in terms of DEs. The capabilities of our new method are inspected based on several low and high-dimensional examples. version:1
arxiv-1610-00324 | Accelerating Deep Convolutional Networks using low-precision and sparsity | http://arxiv.org/abs/1610.00324 | id:1610.00324 author:Ganesh Venkatesh, Eriko Nurvitadhi, Debbie Marr category:cs.LG cs.NE  published:2016-10-02 summary:We explore techniques to significantly improve the compute efficiency and performance of Deep Convolution Networks without impacting their accuracy. To improve the compute efficiency, we focus on achieving high accuracy with extremely low-precision (2-bit) weight networks, and to accelerate the execution time, we aggressively skip operations on zero-values. We achieve the highest reported accuracy of 76.6% Top-1/93% Top-5 on the Imagenet object classification challenge with low-precision network\footnote{github release of the source code coming soon} while reducing the compute requirement by ~3x compared to a full-precision network that achieves similar accuracy. Furthermore, to fully exploit the benefits of our low-precision networks, we build a deep learning accelerator core, dLAC, that can achieve up to 1 TFLOP/mm^2 equivalent for single-precision floating-point operations (~2 TFLOP/mm^2 for half-precision). version:1
arxiv-1610-00321 | Low-dose CT denoising with convolutional neural network | http://arxiv.org/abs/1610.00321 | id:1610.00321 author:Hu Chen, Yi Zhang, Weihua Zhang, Peixi Liao, Ke Li, Jiliu Zhou, Ge Wang category:physics.med-ph cs.CV  published:2016-10-02 summary:To reduce the potential radiation risk, low-dose CT has attracted much attention. However, simply lowering the radiation dose will lead to significant deterioration of the image quality. In this paper, we propose a noise reduction method for low-dose CT via deep neural network without accessing original projection data. A deep convolutional neural network is trained to transform low-dose CT images towards normal-dose CT images, patch by patch. Visual and quantitative evaluation demonstrates a competing performance of the proposed method. version:1
arxiv-1610-00320 | Stacked Autoencoders for Medical Image Search | http://arxiv.org/abs/1610.00320 | id:1610.00320 author:S. Sharma, I. Umar, L. Ospina, D. Wong, H. R. Tizhoosh category:cs.CV  published:2016-10-02 summary:Medical images can be a valuable resource for reliable information to support medical diagnosis. However, the large volume of medical images makes it challenging to retrieve relevant information given a particular scenario. To solve this challenge, content-based image retrieval (CBIR) attempts to characterize images (or image regions) with invariant content information in order to facilitate image search. This work presents a feature extraction technique for medical images using stacked autoencoders, which encode images to binary vectors. The technique is applied to the IRMA dataset, a collection of 14,410 x-ray images in order to demonstrate the ability of autoencoders to retrieve similar x-rays given test queries. Using IRMA dataset as a benchmark, it was found that stacked autoencoders gave excellent results with a retrieval error of 376 for 1,733 test images with a compression of 74.61%. version:1
arxiv-1610-00318 | MinMax Radon Barcodes for Medical Image Retrieval | http://arxiv.org/abs/1610.00318 | id:1610.00318 author:H. R. Tizhoosh, Shujin Zhu, Hanson Lo, Varun Chaudhari, Tahmid Mehdi category:cs.CV  published:2016-10-02 summary:Content-based medical image retrieval can support diagnostic decisions by clinical experts. Examining similar images may provide clues to the expert to remove uncertainties in his/her final diagnosis. Beyond conventional feature descriptors, binary features in different ways have been recently proposed to encode the image content. A recent proposal is "Radon barcodes" that employ binarized Radon projections to tag/annotate medical images with content-based binary vectors, called barcodes. In this paper, MinMax Radon barcodes are introduced which are superior to "local thresholding" scheme suggested in the literature. Using IRMA dataset with 14,410 x-ray images from 193 different classes, the advantage of using MinMax Radon barcodes over \emph{thresholded} Radon barcodes are demonstrated. The retrieval error for direct search drops by more than 15\%. As well, SURF, as a well-established non-binary approach, and BRISK, as a recent binary method are examined to compare their results with MinMax Radon barcodes when retrieving images from IRMA dataset. The results demonstrate that MinMax Radon barcodes are faster and more accurate when applied on IRMA images. version:1
arxiv-1610-00311 | Syntactic Structures and Code Parameters | http://arxiv.org/abs/1610.00311 | id:1610.00311 author:Kevin Shu, Matilde Marcolli category:cs.CL  published:2016-10-02 summary:We assign binary and ternary error-correcting codes to the data of syntactic structures of world languages and we study the distribution of code points in the space of code parameters. We show that, while most codes populate the lower region approximating a superposition of Thomae functions, there is a substantial presence of codes above the Gilbert-Varshamov bound and even above the asymptotic bound and the Plotkin bound. We investigate the dynamics induced on the space of code parameters by spin glass models of language change, and show that, in the presence of entailment relations between syntactic parameters the dynamics can sometimes improve the code. For large sets of languages and syntactic data, one can gain information on the spin glass dynamics from the induced dynamics in the space of code parameters. version:1
arxiv-1610-00307 | Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection | http://arxiv.org/abs/1610.00307 | id:1610.00307 author:Mahdyar Ravanbakhsh, Moin Nabi, Hossein Mousavi, Enver Sangineto, Nicu Sebe category:cs.CV  published:2016-10-02 summary:Most of the crowd abnormal event detection methods rely on complex hand-crafted features to represent the crowd motion and appearance. Convolutional Neural Networks (CNN) have shown to be a powerful tool with excellent representational capacities, which can leverage the need for hand-crafted features. In this paper, we show that keeping track of the changes in the CNN feature across time can facilitate capturing the local abnormality. We specifically propose a novel measure-based method which allows measuring the local abnormality in a video by combining semantic information (inherited from existing CNN models) with low-level Optical-Flow. One of the advantage of this method is that it can be used without the fine-tuning costs. The proposed method is validated on challenging abnormality detection datasets and the results show the superiority of our method compared to the state-of-the-art methods. version:1
arxiv-1610-00291 | Deep Feature Consistent Variational Autoencoder | http://arxiv.org/abs/1610.00291 | id:1610.00291 author:Xianxu Hou, Linlin Shen, Ke Sun, Guoping Qiu category:cs.CV  published:2016-10-02 summary:We present a novel method for constructing Variational Autoencoder (VAE). Instead of using pixel-by-pixel loss, we enforce deep feature consistency between the input and the output of a VAE, which ensures the VAE's output to preserve the spatial correlation characteristics of the input, thus leading the output to have a more natural visual appearance and better perceptual quality. Based on recent deep learning works such as style transfer, we employ a pre-trained deep convolutional neural network (CNN) and use its hidden features to define a feature perceptual loss for VAE training. Evaluated on the CelebA face dataset, we show that our model produces better results than other methods in the literature. We also show that our method can produce latent vectors that can capture the semantic information of face expressions and can be used to achieve state-of-the-art performance in facial attribute prediction. version:1
arxiv-1610-00279 | Deep Learning Algorithms for Signal Recognition in Long Perimeter Monitoring Distributed Fiber Optic Sensors | http://arxiv.org/abs/1610.00279 | id:1610.00279 author:A. V. Makarenko category:cs.CV stat.ML  published:2016-10-02 summary:In this paper, we show an approach to build deep learning algorithms for recognizing signals in distributed fiber optic monitoring and security systems for long perimeters. Synthesizing such detection algorithms poses a non-trivial research and development challenge, because these systems face stringent error (type I and II) requirements and operate in difficult signal-jamming environments, with intensive signal-like jamming and a variety of changing possible signal portraits of possible recognized events. To address these issues, we have developed a twolevel event detection architecture, where the primary classifier is based on an ensemble of deep convolutional networks, can recognize 7 classes of signals and receives time-space data frames as input. Using real-life data, we have shown that the applied methods result in efficient and robust multiclass detection algorithms that have a high degree of adaptability. version:1
arxiv-1610-00277 | Very Deep Convolutional Neural Networks for Robust Speech Recognition | http://arxiv.org/abs/1610.00277 | id:1610.00277 author:Yanmin Qian, Philip C Woodland category:cs.CL  published:2016-10-02 summary:This paper describes the extension and optimization of our previous work on very deep convolutional neural networks (CNNs) for effective recognition of noisy speech in the Aurora 4 task. The appropriate number of convolutional layers, the sizes of the filters, pooling operations and input feature maps are all modified: the filter and pooling sizes are reduced and dimensions of input feature maps are extended to allow adding more convolutional layers. Furthermore appropriate input padding and input feature map selection strategies are developed. In addition, an adaptation framework using joint training of very deep CNN with auxiliary features i-vector and fMLLR features is developed. These modifications give substantial word error rate reductions over the standard CNN used as baseline. Finally the very deep CNN is combined with an LSTM-RNN acoustic model and it is shown that state-level weighted log likelihood score combination in a joint acoustic model decoding scheme is very effective. On the Aurora 4 task, the very deep CNN achieves a WER of 8.81%, further 7.99% with auxiliary feature joint training, and 7.09% with LSTM-RNN joint decoding. version:1
arxiv-1610-00270 | Sparsity-driven weighted ensemble classifier | http://arxiv.org/abs/1610.00270 | id:1610.00270 author:Atilla ÃzgÃ¼r, Hamit Erdem, Fatih Nar category:stat.ML cs.LG  published:2016-10-02 summary:In this letter, a novel weighted ensemble classifier is proposed that improves classification accuracy and minimizes the number of classifiers. Ensemble weight finding problem is modeled as a cost function with following terms: (a) a data fidelity term aiming to decrease misclassification rate, (b) a sparsity term aiming to decrease the number of classifiers, and (c) a non-negativity constraint on the weights of the classifiers. The proposed cost function is a non-convex and hard to solve; thus, convex relaxation techniques and novel approximations are employed to obtain a numerically efficient solution. The proposed method achieves better or similar performance compared to state-of-the art classifier ensemble methods, while using lower number of classifiers. version:1
arxiv-1610-00246 | HNP3: A Hierarchical Nonparametric Point Process for Modeling Content Diffusion over Social Media | http://arxiv.org/abs/1610.00246 | id:1610.00246 author:Seyed Abbas Hosseini, Ali Khodadadi, Soheil Arabzade, Hamid R. Rabiee category:stat.ML cs.LG cs.SI  published:2016-10-02 summary:This paper introduces a novel framework for modeling temporal events with complex longitudinal dependency that are generated by dependent sources. This framework takes advantage of multidimensional point processes for modeling time of events. The intensity function of the proposed process is a mixture of intensities, and its complexity grows with the complexity of temporal patterns of data. Moreover, it utilizes a hierarchical dependent nonparametric approach to model marks of events. These capabilities allow the proposed model to adapt its temporal and topical complexity according to the complexity of data, which makes it a suitable candidate for real world scenarios. An online inference algorithm is also proposed that makes the framework applicable to a vast range of applications. The framework is applied to a real world application, modeling the diffusion of contents over networks. Extensive experiments reveal the effectiveness of the proposed framework in comparison with state-of-the-art methods. version:1
arxiv-1610-00243 | Deep unsupervised learning through spatial contrasting | http://arxiv.org/abs/1610.00243 | id:1610.00243 author:Elad Hoffer, Itay Hubara, Nir Ailon category:cs.LG cs.AI stat.ML  published:2016-10-02 summary:Convolutional networks have marked their place over the last few years as the best performing model for various visual tasks. They are, however, most suited for supervised learning from large amounts of labeled data. Previous attempts have been made to use unlabeled data to improve model performance by applying unsupervised techniques. These attempts require different architectures and training methods. In this work we present a novel approach for unsupervised training of Convolutional networks that is based on contrasting between spatial regions within images. This criterion can be employed within conventional neural networks and trained using standard techniques such as SGD and back-propagation, thus complementing supervised methods. version:1
arxiv-1610-00219 | Text Network Exploration via Heterogeneous Web of Topics | http://arxiv.org/abs/1610.00219 | id:1610.00219 author:Junxian He, Ying Huang, Changfeng Liu, Jiaming Shen, Yuting Jia, Xinbing Wang category:cs.SI cs.CL cs.IR  published:2016-10-02 summary:A text network refers to a data type that each vertex is associated with a text document and the relationship between documents is represented by edges. The proliferation of text networks such as hyperlinked webpages and academic citation networks has led to an increasing demand for quickly developing a general sense of a new text network, namely text network exploration. In this paper, we address the problem of text network exploration through constructing a heterogeneous web of topics, which allows people to investigate a text network associating word level with document level. To achieve this, a probabilistic generative model for text and links is proposed, where three different relationships in the heterogeneous topic web are quantified. We also develop a prototype demo system named TopicAtlas to exhibit such heterogeneous topic web, and demonstrate how this system can facilitate the task of text network exploration. Extensive qualitative analyses are included to verify the effectiveness of this heterogeneous topic web. Besides, we validate our model on real-life text networks, showing that it preserves good performance on objective evaluation metrics. version:1
arxiv-1610-00211 | Sentence Segmentation in Narrative Transcripts from Neuropsycological Tests using Recurrent Convolutional Neural Networks | http://arxiv.org/abs/1610.00211 | id:1610.00211 author:Marcos VinÃ­cius Treviso, Christopher Shulby, Sandra Maria AluÃ­sio category:cs.CL 68T50  published:2016-10-02 summary:Automated discourse analysis tools based on Natural Language Processing (NLP) aiming at the diagnosis of language-impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in the transcripts prevents the direct application of NLP methods which rely on these marks in order to function properly, such as taggers and parsers. We present the first steps taken towards automatic neuropsychological evaluation based on narrative discourse analysis, presenting a new automatic sentence segmentation method for impaired speech. Our model uses recurrent convolutional neural networks with prosodic, Part of Speech (PoS) features, and word embeddings. It was evaluated intrinsically on impaired, spontaneous speech as well as normal, prepared speech. The results suggest that our model is robust for impaired speech and can be used in automated discourse analysis tools to differentiate narratives produced by Mild Cognitive Impairment and healthy elderly patients. version:1
arxiv-1609-09475 | Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge | http://arxiv.org/abs/1609.09475 | id:1609.09475 author:Andy Zeng, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker Jr., Alberto Rodriguez, Jianxiong Xiao category:cs.CV cs.LG cs.RO  published:2016-09-29 summary:Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://www.andyzeng.com/apc2016 version:2
arxiv-1610-00207 | Tuning Parameter Calibration in High-dimensional Logistic Regression With Theoretical Guarantees | http://arxiv.org/abs/1610.00207 | id:1610.00207 author:Wei Li, Johannes Lederer category:stat.ME math.ST stat.ML stat.TH  published:2016-10-01 summary:Feature selection is a standard approach to understanding and modeling high-dimensional classification data, but the corresponding statistical methods hinge on tuning parameters that are difficult to calibrate. In particular, existing calibration schemes in the logistic regression framework lack any finite sample guarantees. In this paper, we introduce a novel calibration scheme for penalized logistic regression. It is based on simple tests along the tuning parameter path and satisfies optimal finite sample bounds. It is also amenable to easy and efficient implementations, and it rivals or outmatches existing methods in simulations and real data applications. version:1
arxiv-1610-00199 | Convergence of a Grassmannian Gradient Descent Algorithm for Subspace Estimation From Undersampled Data | http://arxiv.org/abs/1610.00199 | id:1610.00199 author:Dejiao Zhang, Laura Balzano category:cs.NA math.NA stat.ML  published:2016-10-01 summary:Subspace learning and matrix factorization problems have a great many applications in science and engineering, and efficient algorithms are critical as dataset sizes continue to grow. Many relevant problem formulations are non-convex, and in a variety of contexts it has been observed that solving the non-convex problem directly is not only efficient but reliably accurate. We discuss convergence theory for a particular method: first order incremental gradient descent constrained to the Grassmannian. The output of the algorithm is an orthonormal basis for a $d$-dimensional subspace spanned by an input streaming data matrix. We study two sampling cases: where each data vector of the streaming matrix is fully sampled, or where it is undersampled by a sampling matrix $A_t\in \R^{m\times n}$ with $m\ll n$. We propose an adaptive stepsize scheme that depends only on the sampled data and algorithm outputs. We prove that with fully sampled data, the stepsize scheme maximizes the improvement of our convergence metric at each iteration, and this method converges from any random initialization to the true subspace, despite the non-convex formulation and orthogonality constraints. For the case of undersampled data, we establish monotonic improvement on the defined convergence metric for each iteration with high probability. version:1
arxiv-1610-00197 | Coherent structure coloring: identification of coherent structures from sparse data using graph theory | http://arxiv.org/abs/1610.00197 | id:1610.00197 author:Kristy L. Schlueter-Kuck, John O. Dabiri category:physics.flu-dyn math.DS stat.ML  published:2016-10-01 summary:We present a frame-invariant method for detecting coherent structures from Lagrangian flow trajectories that can be sparse in number, as is the case in many fluid mechanics applications of practical interest. The method, based on principles used in graph coloring and spectral graph drawing algorithms, examines a measure of the kinematic dissimilarity of all pairs of fluid trajectories, either measured experimentally, e.g. using particle tracking velocimetry; or numerically, by advecting fluid particles in the Eulerian velocity field. Coherence is assigned to groups of particles whose kinematics remain similar throughout the time interval for which trajectory data is available, regardless of their physical proximity to one another. Through the use of several analytical and experimental validation cases, this algorithm is shown to robustly detect coherent structures using significantly less flow data than is required by existing spectral graph theory methods. version:1
arxiv-1610-00192 | A large scale study of SVM based methods for abstract screening in systematic reviews | http://arxiv.org/abs/1610.00192 | id:1610.00192 author:Tanay Kumar Saha, Mourad Ouzzani, Ahmed K. Elmagarmid category:cs.IR cs.LG  published:2016-10-01 summary:A major task in systematic reviews is abstract screening, i.e., excluding, often hundreds or thousand of, irrelevant citations returned from a database search based on titles and abstracts. Thus, a systematic review platform that can automate the abstract screening process is of huge importance. Several methods have been proposed for this task. However, it is very hard to clearly understand the applicability of these methods in a systematic review platform because of the following challenges: (1) the use of non-overlapping metrics for the evaluation of the proposed methods, (2) usage of features that are very hard to collect, (3) using a small set of reviews for the evaluation, and (4) no solid statistical testing or equivalence grouping of the methods. In this paper, we use feature representation that can be extracted per citation. We evaluate SVM-based methods (commonly used) on a large set of reviews ($61$) and metrics ($11$) to provide equivalence grouping of methods based on a solid statistical test. Our analysis also includes a strong variability of the metrics using $500$x$2$ cross validation. While some methods shine for different metrics and for different datasets, there is no single method that dominates the pack. Furthermore, we observe that in some cases relevant (included) citations can be found after screening only 15-20% of them via a certainty based sampling. A few included citations present outlying characteristics and can only be found after a very large number of screening steps. Finally, we present an ensemble algorithm for producing a $5$-star rating of citations based on their relevance. Such algorithm combines the best methods from our evaluation and through its $5$-star rating outputs a more easy-to-consume prediction. version:1
arxiv-1610-00189 | A Birth and Death Process for Bayesian Network Structure Inference | http://arxiv.org/abs/1610.00189 | id:1610.00189 author:D. Jennings, J. N. Corcoran category:stat.ML  published:2016-10-01 summary:Bayesian networks (BNs) are graphical models that are useful for representing high-dimensional probability distributions. There has been a great deal of interest in recent years in the NP-hard problem of learning the structure of a BN from observed data. Typically, one assigns a score to various structures and the search becomes an optimization problem that can be approached with either deterministic or stochastic methods. In this paper, we walk through the space of graphs by modeling the appearance and disappearance of edges as a birth and death process and compare our novel approach to the popular Metropolis-Hastings search strategy. We give empirical evidence that the birth and death process has superior mixing properties. version:1
arxiv-1610-00175 | Near-Infrared Image Dehazing Via Color Regularization | http://arxiv.org/abs/1610.00175 | id:1610.00175 author:Chang-Hwan Son, Xiao-Ping Zhang category:cs.CV  published:2016-10-01 summary:Near-infrared imaging can capture haze-free near-infrared gray images and visible color images, according to physical scattering models, e.g., Rayleigh or Mie models. However, there exist serious discrepancies in brightness and image structures between the near-infrared gray images and the visible color images. The direct use of the near-infrared gray images brings about another color distortion problem in the dehazed images. Therefore, the color distortion should also be considered for near-infrared dehazing. To reflect this point, this paper presents an approach of adding a new color regularization to conventional dehazing framework. The proposed color regularization can model the color prior for unknown haze-free images from two captured images. Thus, natural-looking colors and fine details can be induced on the dehazed images. The experimental results show that the proposed color regularization model can help remove the color distortion and the haze at the same time. Also, the effectiveness of the proposed color regularization is verified by comparing with other conventional regularizations. It is also shown that the proposed color regularization can remove the edge artifacts which arise from the use of the conventional dark prior model. version:1
arxiv-1610-00168 | Learning Optimized Risk Scores on Large-Scale Datasets | http://arxiv.org/abs/1610.00168 | id:1610.00168 author:Berk Ustun, Cynthia Rudin category:stat.ML math.OC stat.ME  published:2016-10-01 summary:Risk scores are simple classification models that let users quickly assess risk by adding, subtracting and multiplying a few small numbers. These models are used for high-stakes applications in healthcare and criminology, but are difficult to learn from data because they need to be risk-calibrated, use small integer coefficients, and obey operational constraints. In this paper, we present a new approach to learn optimized risk scores from data by solving a discrete optimization problem. We formulate the risk score problem as a mixed integer nonlinear program, and present a new cutting plane algorithm to efficiently recover the optimal solution while avoiding the stalling behavior that occurs when we use existing cutting plane algorithms on non-convex problems. We pair our cutting plane algorithm with specialized procedures to generate feasible solutions, narrow the optimality gap, and reduce data-related computation. The resulting approach can learn optimized risk scores in a way that scales linearly in the number of samples, provides a proof of optimality, and accommodates complex operational constraints. We illustrate the benefits of our approach through extensive numerical experiments. version:1
arxiv-1610-00163 | X-CNN: Cross-modal Convolutional Neural Networks for Sparse Datasets | http://arxiv.org/abs/1610.00163 | id:1610.00163 author:Petar VeliÄkoviÄ, Duo Wang, Nicholas D. Lane, Pietro LiÃ² category:stat.ML cs.AI cs.CV  published:2016-10-01 summary:In this paper we propose cross-modal convolutional neural networks (X-CNNs), a novel biologically inspired type of CNN architectures, treating gradient descent-specialised CNNs as individual units of processing in a larger-scale network topology, while allowing for unconstrained information flow and/or weight sharing between analogous hidden layers of the network---thus generalising the already well-established concept of neural network ensembles (where information typically may flow only between the output layers of the individual networks). The constituent networks are individually designed to learn the output function on their own subset of the input data, after which cross-connections between them are introduced after each pooling operation to periodically allow for information exchange between them. This injection of knowledge into a model (by prior partition of the input data through domain knowledge or unsupervised methods) is expected to yield greatest returns in sparse data environments, which are typically less suitable for training CNNs. For evaluation purposes, we have compared a standard four-layer CNN as well as a sophisticated FitNet4 architecture against their cross-modal variants on the CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data being removed, and find that at lower levels of data availability, the X-CNNs significantly outperform their baselines (typically providing a 2--6% benefit, depending on the dataset size and whether data augmentation is used), while still maintaining an edge on all of the full dataset tests. version:1
arxiv-1609-06323 | Automated Visual Fin Identification of Individual Great White Sharks | http://arxiv.org/abs/1609.06323 | id:1609.06323 author:Benjamin Hughes, Tilo Burghardt category:cs.CV  published:2016-09-20 summary:This paper discusses the automated visual identification of individual great white sharks from dorsal fin imagery. We propose a computer vision photo ID system and report recognition results over a database of thousands of unconstrained fin images. To the best of our knowledge this line of work establishes the first fully automated contour-based visual ID system in the field of animal biometrics. The approach put forward appreciates shark fins as textureless, flexible and partially occluded objects with an individually characteristic shape. In order to recover animal identities from an image we first introduce an open contour stroke model, which extends multi-scale region segmentation to achieve robust fin detection. Secondly, we show that combinatorial, scale-space selective fingerprinting can successfully encode fin individuality. We then measure the species-specific distribution of visual individuality along the fin contour via an embedding into a global `fin space'. Exploiting this domain, we finally propose a non-linear model for individual animal recognition and combine all approaches into a fine-grained multi-instance framework. We provide a system evaluation, compare results to prior work, and report performance and properties in detail. version:2
arxiv-1609-05518 | Towards Deep Symbolic Reinforcement Learning | http://arxiv.org/abs/1609.05518 | id:1609.05518 author:Marta Garnelo, Kai Arulkumaran, Murray Shanahan category:cs.AI cs.LG  published:2016-09-18 summary:Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game. version:2
arxiv-1610-00134 | How Transferable are CNN-based Features for Age and Gender Classification? | http://arxiv.org/abs/1610.00134 | id:1610.00134 author:GÃ¶khan Ãzbulak, Yusuf Aytar, HazÄ±m Kemal Ekenel category:cs.CV  published:2016-10-01 summary:Age and gender are complementary soft biometric traits for face recognition. Successful estimation of age and gender from facial images taken under real-world conditions can contribute improving the identification results in the wild. In this study, in order to achieve robust age and gender classification in the wild, we have benefited from Deep Convolutional Neural Networks based representation. We have explored transferability of existing deep convolutional neural network (CNN) models for age and gender classification. The generic AlexNet-like architecture and domain specific VGG-Face CNN model are employed and fine-tuned with the Adience dataset prepared for age and gender classification in uncontrolled environments. In addition, task specific GilNet CNN model has also been utilized and used as a baseline method in order to compare with transferred models. Experimental results show that both transferred deep CNN models outperform the GilNet CNN model, which is the state-of-the-art age and gender classification approach on the Adience dataset, by an absolute increase of 7% and 4.5% in accuracy, respectively. This outcome indicates that transferring a deep CNN model can provide better classification performance than a task specific CNN model, which has a limited number of layers and trained from scratch using a limited amount of data as in the case of GilNet. Domain specific VGG-Face CNN model has been found to be more useful and provided better performance for both age and gender classification tasks, when compared with generic AlexNet-like model, which shows that transfering from a closer domain is more useful. version:1
arxiv-1610-01178 | A Tour of TensorFlow | http://arxiv.org/abs/1610.01178 | id:1610.01178 author:Peter Goldsborough category:cs.LG  published:2016-10-01 summary:Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released $\textit{TensorFlow}$, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry. version:1
arxiv-1610-00087 | Very Deep Convolutional Neural Networks for Raw Waveforms | http://arxiv.org/abs/1610.00087 | id:1610.00087 author:Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, Samarjit Das category:cs.SD cs.LG cs.NE  published:2016-10-01 summary:Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (~2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features. version:1
arxiv-1610-00085 | Latent Tree Analysis | http://arxiv.org/abs/1610.00085 | id:1610.00085 author:Nevin L. Zhang, Leonard K. M. Poon category:cs.LG  published:2016-10-01 summary:Latent tree analysis seeks to model the correlations among a set of random variables using a tree of latent variables. It was proposed as an improvement to latent class analysis --- a method widely used in social sciences and medicine to identify homogeneous subgroups in a population. It provides new and fruitful perspectives on a number of machine learning areas, including cluster analysis, topic detection, and deep probabilistic modeling. This paper gives an overview of the research on latent tree analysis and various ways it is used in practice. version:1
arxiv-1610-00081 | Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction | http://arxiv.org/abs/1610.00081 | id:1610.00081 author:Junbo Zhang, Yu Zheng, Dekang Qi category:cs.AI cs.LG  published:2016-10-01 summary:Forecasting the flow of crowds is of great importance to traffic management and public safety, yet a very challenging task affected by many complex factors, such as inter-region traffic, events and weather. In this paper, we propose a deep-learning-based approach, called ST-ResNet, to collectively forecast the in-flow and out-flow of crowds in each and every region through a city. We design an end-to-end structure of ST-ResNet based on unique properties of spatio-temporal data. More specifically, we employ the framework of the residual neural networks to model the temporal closeness, period, and trend properties of the crowd traffic, respectively. For each property, we design a branch of residual convolutional units, each of which models the spatial properties of the crowd traffic. ST-ResNet learns to dynamically aggregate the output of the three residual neural networks based on data, assigning different weights to different branches and regions. The aggregation is further combined with external factors, such as weather and day of the week, to predict the final traffic of crowds in each and every region. We evaluate ST-ResNet based on two types of crowd flows in Beijing and NYC, finding that its performance exceeds six well-know methods. version:1
arxiv-1610-00072 | Vocabulary Selection Strategies for Neural Machine Translation | http://arxiv.org/abs/1610.00072 | id:1610.00072 author:Gurvan L'Hostis, David Grangier, Michael Auli category:cs.CL  published:2016-10-01 summary:Classical translation models constrain the space of possible outputs by selecting a subset of translation rules based on the input sentence. Recent work on improving the efficiency of neural translation models adopted a similar strategy by restricting the output vocabulary to a subset of likely candidates given the source. In this paper we experiment with context and embedding-based selection methods and extend previous work by examining speed and accuracy trade-offs in more detail. We show that decoding time on CPUs can be reduced by up to 90% and training time by 25% on the WMT15 English-German and WMT16 English-Romanian tasks at the same or only negligible change in accuracy. This brings the time to decode with a state of the art neural translation system to just over 140 msec per sentence on a single CPU core for English-German. version:1
arxiv-1610-00070 | Radial Velocity Retrieval for Multichannel SAR Moving Targets with Time-Space Doppler De-ambiguity | http://arxiv.org/abs/1610.00070 | id:1610.00070 author:Zu-Zhen Huang, Jia Xu, Zhi-Rui Wang, Li Xiao, Xiang-Gen Xia, Teng Long category:cs.IT cs.CV math.IT  published:2016-10-01 summary:In this paper, for multichannel synthetic aperture radar (SAR) systems we first formulate the effects of Doppler ambiguities on the radial velocity (RV) estimation of a ground moving target in range-compressed domain, range-Doppler domain and image domain, respectively, where cascaded time-space Doppler ambiguity (CTSDA) may occur, that is, time domain Doppler ambiguity (TDDA) in each channel occurs at first and then spatial domain Doppler ambiguity (SDDA) among multi-channels occurs subsequently. Accordingly, the multichannel SAR systems with different parameters are divided into three cases with different Doppler ambiguity properties, i.e., only TDDA occurs in Case I, and CTSDA occurs in Cases II and III, while the CTSDA in Case II can be simply seen as the SDDA. Then, a multi-frequency SAR is proposed to obtain the RV estimation by solving the ambiguity problem based on Chinese remainder theorem (CRT). For Cases I and II, the ambiguity problem can be solved by the existing closed-form robust CRT. For Case III, we show that the problem is different from the conventional CRT problem and we call it a double remaindering problem. We then propose a sufficient condition under which the double remaindering problem, i.e., the CTSDA, can be solved by the closed-form robust CRT. When the sufficient condition is not satisfied, a searching based method is proposed. Finally, some numerical experiments are provided to demonstrate the effectiveness of the proposed methods. version:1
arxiv-1610-00064 | Faster Kernels for Graphs with Continuous Attributes via Hashing | http://arxiv.org/abs/1610.00064 | id:1610.00064 author:Christopher Morris, Nils M. Kriege, Kristian Kersting, Petra Mutzel category:cs.LG stat.ML  published:2016-10-01 summary:While state-of-the-art kernels for graphs with discrete labels scale well to graphs with thousands of nodes, the few existing kernels for graphs with continuous attributes, unfortunately, do not scale well. To overcome this limitation, we present hash graph kernels, a general framework to derive kernels for graphs with continuous attributes from discrete ones. The idea is to iteratively turn continuous attributes into discrete labels using randomized hash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman subtree kernel and for the shortest-path kernel. The resulting novel graph kernels are shown to be, both, able to handle graphs with continuous attributes and scalable to large graphs and data sets. This is supported by our theoretical analysis and demonstrated by an extensive experimental evaluation. version:1
arxiv-1610-00054 | Outlier Detection from Network Data with Subnetwork Interpretation | http://arxiv.org/abs/1610.00054 | id:1610.00054 author:Xuan-Hong Dang, Arlei Silva, Ambuj Singh, Ananthram Swami, Prithwish Basu category:cs.AI cs.LG  published:2016-09-30 summary:Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why the network is exceptional, expressed in the form of subnetwork, is also equally important. In this paper, we develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace/subgraph discovery and we show that it converges to a global optimum. Evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting, but also discovers highly relevant and interpretable local subnetworks, further enhancing our understanding of anomalous networks. version:1
arxiv-1610-00053 | Superconducting optoelectronic circuits for neuromorphic computing | http://arxiv.org/abs/1610.00053 | id:1610.00053 author:Jeffrey M. Shainline, Sonia M. Buckley, Richard P. Mirin, Sae Woo Nam category:cs.NE cond-mat.supr-con physics.optics  published:2016-09-30 summary:We propose a hybrid semiconductor-superconductor hardware platform for the implementation of neural networks and large-scale neuromorphic computing. The platform combines semiconducting few-photon light-emitting diodes with superconducting-nanowire single-photon detectors to behave as spiking neurons. These processing units are connected via a network of optical waveguides, and variable weights of connection can be implemented using several approaches. The use of light as a signaling mechanism overcomes the requirement for time-multiplexing that has limited the event rates of purely electronic platforms. The proposed processing units can operate at $20$ MHz with fully asynchronous activity, light-speed-limited latency, and power densities on the order of 1 mW/cm$^2$ for neurons with 700 connections operating at full speed at 2 K. The processing units achieve an energy efficiency of $\approx 20$ aJ per synapse event. By leveraging multilayer photonics with low-temperature-deposited waveguides and superconductors with feature sizes $>$ 100 nm, this approach could scale to massive interconnectivity near that of the human brain, and could surpass the brain in speed and energy efficiency. version:1
arxiv-1610-00040 | A Primer on Coordinate Descent Algorithms | http://arxiv.org/abs/1610.00040 | id:1610.00040 author:Hao-Jun Michael Shi, Shenyinying Tu, Yangyang Xu, Wotao Yin category:math.OC stat.ML  published:2016-09-30 summary:This monograph presents a class of algorithms called coordinate descent algorithms for mathematicians, statisticians, and engineers outside the field of optimization. This particular class of algorithms has recently gained popularity due to their effectiveness in solving large-scale optimization problems in machine learning, compressed sensing, image processing, and computational statistics. Coordinate descent algorithms solve optimization problems by successively minimizing along each coordinate or coordinate hyperplane, which is ideal for parallelized and distributed computing. Avoiding detailed technicalities and proofs, this monograph gives relevant theory and examples for practitioners to effectively apply coordinate descent to modern problems in data science and engineering. To keep the primer up-to-date, we intend to publish this monograph only after no additional topics need to be added and we foresee no further major advances in the area. version:1
arxiv-1610-00031 | Discriminating Similar Languages: Evaluations and Explorations | http://arxiv.org/abs/1610.00031 | id:1610.00031 author:Cyril Goutte, Serge LÃ©ger, Shervin Malmasi, Marcos Zampieri category:cs.CL  published:2016-09-30 summary:We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation. version:1
arxiv-1610-00030 | Modeling Language Change in Historical Corpora: The Case of Portuguese | http://arxiv.org/abs/1610.00030 | id:1610.00030 author:Marcos Zampieri, Shervin Malmasi, Mark Dras category:cs.CL  published:2016-09-30 summary:This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change. version:1
arxiv-1609-09869 | Structured Inference Networks for Nonlinear State Space Models | http://arxiv.org/abs/1609.09869 | id:1609.09869 author:Rahul G. Krishnan, Uri Shalit, David Sontag category:stat.ML cs.AI cs.LG  published:2016-09-30 summary:Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood. version:1
arxiv-1609-09850 | Latent fingerprint minutia extraction using fully convolutional network | http://arxiv.org/abs/1609.09850 | id:1609.09850 author:Yao Tang, Fei Gao, Jufu Feng category:cs.CV  published:2016-09-30 summary:Minutiae have played an important role in fingerprint identification. Extracting effective minutiae is difficult for latent fingerprints which are usually of poor quality. Instead of conventional hand-craft features, a fully convolutional network(FCN) is learned end-to-end to extract minutiae from raw fingerprints in pixel level. FCN is used to map raw fingerprints to a correspondingly-sized minutia-score map with a fixed stride. And thus a large number of minutiae will be extracted through a given thresh. Then small regions centering at these minutia points are put into a convolutional neural network(CNN) to reclassify these minutiae and calculate their orientations. The CNN shares the convolutional layers with the fully convolutional network to speed up. For the VGG model~\cite{simonyan2014very}, 0.45 second is used on average to detect one fingerprint on a GPU. On the NIST SD27 database we achieve 53\% recall rate and 53\% precise rate that beats many other algorithms. Our trained model is also visualized to see that we have successfully extracted features preserving ridge information of a latent fingerprint. version:1
arxiv-1609-09823 | On the Worst-case Communication Overhead for Distributed Data Shuffling | http://arxiv.org/abs/1609.09823 | id:1609.09823 author:Mohamed Attia, Ravi Tandon category:cs.IT cs.DC cs.LG math.IT  published:2016-09-30 summary:Distributed learning platforms for processing large scale data-sets are becoming increasingly prevalent. In typical distributed implementations, a centralized master node breaks the data-set into smaller batches for parallel processing across distributed workers to achieve speed-up and efficiency. Several computational tasks are of sequential nature, and involve multiple passes over the data. At each iteration over the data, it is common practice to randomly re-shuffle the data at the master node, assigning different batches for each worker to process. This random re-shuffling operation comes at the cost of extra communication overhead, since at each shuffle, new data points need to be delivered to the distributed workers. In this paper, we focus on characterizing the information theoretically optimal communication overhead for the distributed data shuffling problem. We propose a novel coded data delivery scheme for the case of no excess storage, where every worker can only store the assigned data batches under processing. Our scheme exploits a new type of coding opportunity and is applicable to any arbitrary shuffle, and for any number of workers. We also present an information theoretic lower bound on the minimum communication overhead for data shuffling, and show that the proposed scheme matches this lower bound for the worst-case communication overhead. version:1
arxiv-1609-09799 | Optimal spectral transportation with application to music transcription | http://arxiv.org/abs/1609.09799 | id:1609.09799 author:RÃ©mi Flamary, CÃ©dric FÃ©votte, Nicolas Courty, Valentin Emiya category:stat.ML cs.LG cs.SD  published:2016-09-30 summary:Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timber can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data. version:1
arxiv-1609-08436 | Non-flat Road Detection Based on A Local Descriptor | http://arxiv.org/abs/1609.08436 | id:1609.08436 author:Kangru Wang, Lei Qu, Lili Chen, Yuzhang Gu, Xiaolin Zhang category:cs.CV  published:2016-09-27 summary:The detection of road surface and free space remains challenging for non-flat plane, especially with the varying latitudinal and longitudinal slope or in the case of multi-ground plane. In this paper, we propose a framework of the road surface detection with stereo vision. The main contribution of this paper is a newly proposed descriptor which is implemented in disparity image to obtain a disparity feature image. The road regions can be distinguished from their surroundings effectively in the disparity feature image. Because the descriptor is implemented in the local area of the image, it can address well the problem of non-flat plane. And we also present a complete framework to detect the road surface regions base on the disparity feature image with a convolutional neural network architecture. version:2
arxiv-1610-00580 | Flint Water Crisis: Data-Driven Risk Assessment Via Residential Water Testing | http://arxiv.org/abs/1610.00580 | id:1610.00580 author:Jacob Abernethy, Cyrus Anderson, Chengyu Dai, Arya Farahi, Linh Nguyen, Adam Rauh, Eric Schwartz, Wenbo Shen, Guangsha Shi, Jonathan Stroud, Xinyu Tan, Jared Webb, Sheng Yang category:cs.LG stat.AP  published:2016-09-30 summary:Recovery from the Flint Water Crisis has been hindered by uncertainty in both the water testing process and the causes of contamination. In this work, we develop an ensemble of predictive models to assess the risk of lead contamination in individual homes and neighborhoods. To train these models, we utilize a wide range of data sources, including voluntary residential water tests, historical records, and city infrastructure data. Additionally, we use our models to identify the most prominent factors that contribute to a high risk of lead contamination. In this analysis, we find that lead service lines are not the only factor that is predictive of the risk of lead contamination of water. These results could be used to guide the long-term recovery efforts in Flint, minimize the immediate damages, and improve resource-allocation decisions for similar water infrastructure crises. version:1
arxiv-1609-06870 | Distributed Training of Deep Neural Networks: Theoretical and Practical Limits of Parallel Scalability | http://arxiv.org/abs/1609.06870 | id:1609.06870 author:Janis Keuper, Franz-Josef Preundt category:cs.CV  published:2016-09-22 summary:This paper presents a theoretical analysis and practical evaluation of the main bottlenecks towards a scalable distributed solution for the training of Deep Neuronal Networks (DNNs). The presented results show, that the current state of the art approach, using data-parallelized Stochastic Gradient Descent (SGD), is quickly turning into a vastly communication bound problem. In addition, we present simple but fixed theoretic constraints, preventing effective scaling of DNN training beyond only a few dozen nodes. This leads to poor scalability of DNN training in most practical scenarios. version:2
arxiv-1609-09744 | Phase Unmixing : Multichannel Source Separation with Magnitude Constraints | http://arxiv.org/abs/1609.09744 | id:1609.09744 author:Antoine Deleforge, Yann Traonmilin category:cs.SD stat.ML  published:2016-09-30 summary:We consider the problem of estimating the phases of K mixed complex signals from a multichannel observation, when the mixing matrix and signal magnitudes are known. This problem can be cast as a non-convex quadratically constrained quadratic program which is known to be NP-hard in general. We propose three approaches to tackle it: a heuristic method, an alternate minimization method, and a convex relaxation into a semi-definite program. These approaches are showed to outperform the oracle multichannel Wiener filter in under-determined informed source separation tasks, using simulated and speech signals. The convex relaxation approach yields best results, including the potential for exact source separation in under-determined settings. version:1
arxiv-1609-09713 | A deep representation for depth images from synthetic data | http://arxiv.org/abs/1609.09713 | id:1609.09713 author:Fabio Maria Carlucci, Paolo Russo, Barbara Caputo category:cs.CV  published:2016-09-30 summary:Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pre-trained on 2D datasets. Experiments on two publicly available databases show the power of our approach. version:1
arxiv-1609-09698 | Training a Feedback Loop for Hand Pose Estimation | http://arxiv.org/abs/1609.09698 | id:1609.09698 author:Markus Oberweger, Paul Wohlhart, Vincent Lepetit category:cs.CV  published:2016-09-30 summary:We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU. version:1
arxiv-1609-09143 | Recurrent Convolutional Networks for Pulmonary Nodule Detection in CT Imaging | http://arxiv.org/abs/1609.09143 | id:1609.09143 author:Petros-Pavlos Ypsilantis, Giovanni Montana category:stat.ML cs.CV  published:2016-09-28 summary:Computed tomography (CT) generates a stack of cross-sectional images covering a region of the body. The visual assessment of these images for the identification of potential abnormalities is a challenging and time consuming task due to the large amount of information that needs to be processed. In this article we propose a deep artificial neural network architecture, ReCTnet, for the fully-automated detection of pulmonary nodules in CT scans. The architecture learns to distinguish nodules and normal structures at the pixel level and generates three-dimensional probability maps highlighting areas that are likely to harbour the objects of interest. Convolutional and recurrent layers are combined to learn expressive image representations exploiting the spatial dependencies across axial slices. We demonstrate that leveraging intra-slice dependencies substantially increases the sensitivity to detect pulmonary nodules without inflating the false positive rate. On the publicly available LIDC/IDRI dataset consisting of 1,018 annotated CT scans, ReCTnet reaches a detection sensitivity of 90.5% with an average of 4.5 false positives per scan. Comparisons with a competing multi-channel convolutional neural network for multi-slice segmentation and other published methodologies using the same dataset provide evidence that ReCTnet offers significant performance gains. version:2
arxiv-1609-09681 | Predicting the consequence of action in digital control state spaces | http://arxiv.org/abs/1609.09681 | id:1609.09681 author:Emmanuel DaucÃ© category:cs.LG cs.SY  published:2016-09-30 summary:The objective of this dissertation is to shed light on some fundamental impediments in learning control laws in continuous state spaces. In particular, if one wants to build artificial devices capable to learn motor tasks the same way they learn to classify signals and images, one needs to establish control rules that do not necessitate comparisons between quantities of the surrounding space. We propose, in that context, to take inspiration from the "end effector control" principle, as suggested by neuroscience studies, as opposed to the "displacement control" principle used in the classical control theory. version:1
arxiv-1609-09671 | Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks | http://arxiv.org/abs/1609.09671 | id:1609.09671 author:Roberto DiCecco, Griffin Lacey, Jasmina Vasiljevic, Paul Chow, Graham Taylor, Shawki Areibi category:cs.CV cs.DC  published:2016-09-30 summary:Convolutional Neural Networks (CNNs) have gained significant traction in the field of machine learning, particularly due to their high accuracy in visual recognition. Recent works have pushed the performance of GPU implementations of CNNs to significantly improve their classification and training times. With these improvements, many frameworks have become available for implementing CNNs on both CPUs and GPUs, with no support for FPGA implementations. In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. This allows for classification using CNN models and specialized FPGA implementations with the flexibility of reprogramming the device when necessary, seamless memory transactions between host and device, simple-to-use test benches, and the ability to create pipelined layer implementations. To validate the framework, we use the Xilinx SDAccel environment to implement an FPGA-based Winograd convolution engine and show that the FPGA layer can be used alongside other layers running on a host processor to run several popular CNNs (AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework achieves 50 GFLOPS across 3x3 convolutions in the benchmarks. This is achieved within a practical framework, which will aid in future development of FPGA-based CNNs. version:1
arxiv-1609-06374 | A Consumer BCI for Automated Music Evaluation Within a Popular On-Demand Music Streaming Service - Taking Listener's Brainwaves to Extremes | http://arxiv.org/abs/1609.06374 | id:1609.06374 author:Fotis Kalaganis, Dimitrios A. Adamos, Nikos Laskaris category:cs.AI cs.CY cs.HC cs.MM cs.NE  published:2016-09-20 summary:We investigated the possibility of using a machine-learning scheme in conjunction with commercial wearable EEG-devices for translating listener's subjective experience of music into scores that can be used for the automated annotation of music in popular on-demand streaming services. Based on the established -neuroscientifically sound- concepts of brainwave frequency bands, activation asymmetry index and cross-frequency-coupling (CFC), we introduce a Brain Computer Interface (BCI) system that automatically assigns a rating score to the listened song. Our research operated in two distinct stages: i) a generic feature engineering stage, in which features from signal-analytics were ranked and selected based on their ability to associate music induced perturbations in brainwaves with listener's appraisal of music. ii) a personalization stage, during which the efficiency of ex- treme learning machines (ELMs) is exploited so as to translate the derived pat- terns into a listener's score. Encouraging experimental results, from a pragmatic use of the system, are presented. version:2
arxiv-1610-00552 | FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks | http://arxiv.org/abs/1610.00552 | id:1610.00552 author:Minjae Lee, Kyuyeon Hwang, Jinhwan Park, Sungwook Choi, Sungho Shin, Wonyong Sung category:cs.CL cs.LG cs.SD  published:2016-09-30 summary:In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-to-character RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N-best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time. version:1
arxiv-1609-09660 | On Identification of Sparse Multivariable ARX Model: A Sparse Bayesian Learning Approach | http://arxiv.org/abs/1609.09660 | id:1609.09660 author:J. Jin, Y. Yuan, W. Pan, D. L. T. Pham, C. J. Tomlin, A. Webb, J. Goncalves category:cs.SY cs.LG stat.ML  published:2016-09-30 summary:This paper begins with considering the identification of sparse linear time-invariant networks described by multivariable ARX models. Such models possess relatively simple structure thus used as a benchmark to promote further research. With identifiability of the network guaranteed, this paper presents an identification method that infers both the Boolean structure of the network and the internal dynamics between nodes. Identification is performed directly from data without any prior knowledge of the system, including its order. The proposed method solves the identification problem using Maximum a posteriori estimation (MAP) but with inseparable penalties for complexity, both in terms of element (order of nonzero connections) and group sparsity (network topology). Such an approach is widely applied in Compressive Sensing (CS) and known as Sparse Bayesian Learning (SBL). We then propose a novel scheme that combines sparse Bayesian and group sparse Bayesian to efficiently solve the problem. The resulted algorithm has a similar form of the standard Sparse Group Lasso (SGL) while with known noise variance, it simplifies to exact re-weighted SGL. The method and the developed toolbox can be applied to infer networks from a wide range of fields, including systems biology applications such as signaling and genetic regulatory networks. version:1
arxiv-1609-09642 | A CNN Cascade for Landmark Guided Semantic Part Segmentation | http://arxiv.org/abs/1609.09642 | id:1609.09642 author:Aaron Jackson, Michel Valstar, Georgios Tzimiropoulos category:cs.CV  published:2016-09-30 summary:This paper proposes a CNN cascade for semantic part segmentation guided by pose-specific information encoded in terms of a set of landmarks (or keypoints). There is large amount of prior work on each of these tasks separately, yet, to the best of our knowledge, this is the first time in literature that the interplay between pose estimation and semantic part segmentation is investigated. To address this limitation of prior work, in this paper, we propose a CNN cascade of tasks that firstly performs landmark localisation and then uses this information as input for guiding semantic part segmentation. We applied our architecture to the problem of facial part segmentation and report large performance improvement over the standard unguided network on the most challenging face datasets. Testing code and models will be published online at http://cs.nott.ac.uk/~psxasj/. version:1
arxiv-1609-09270 | Pano2CAD: Room Layout From A Single Panorama Image | http://arxiv.org/abs/1609.09270 | id:1609.09270 author:Jiu Xu, Bjorn Stenger, Tommi Kerola, Tony Tung category:cs.CV  published:2016-09-29 summary:This paper presents a method of estimating the geometry of a room and the 3D pose of objects from a single 360-degree panorama image. Assuming Manhattan World geometry, we formulate the task as a Bayesian inference problem in which we estimate positions and orientations of walls and objects. The method combines surface normal estimation, 2D object detection and 3D object pose estimation. Quantitative results are presented on a dataset of synthetically generated 3D rooms containing objects, as well as on a subset of hand-labeled images from the public SUN360 dataset. version:2
arxiv-1609-09619 | Big Data analytics. Three use cases with R, Python and Spark | http://arxiv.org/abs/1609.09619 | id:1609.09619 author:Philippe Besse, Brendan Guillouet, Jean-Michel Loubes category:stat.AP cs.LG  published:2016-09-30 summary:Management and analysis of big data are systematically associated with a data distributed architecture in the Hadoop and now Spark frameworks. This article offers an introduction for statisticians to these technologies by comparing the performance obtained by the direct use of three reference environments: R, Python Scikit-learn, Spark MLlib on three public use cases: character recognition, recommending films, categorizing products. As main result, it appears that, if Spark is very efficient for data munging and recommendation by collaborative filtering (non-negative factorization), current implementations of conventional learning methods (logistic regression, random forests) in MLlib or SparkML do not ou poorly compete habitual use of these methods (R, Python Scikit-learn) in an integrated or undistributed architecture version:1
arxiv-1609-09597 | Social Computing for Mobile Big Data in Wireless Networks | http://arxiv.org/abs/1609.09597 | id:1609.09597 author:Xing Zhang, Zhenglei Yi, Zhi Yan, Geyong Min, Wenbo Wang, Sabita Maharjan, Yan Zhang category:cs.SI cs.LG stat.ML  published:2016-09-30 summary:Mobile big data contains vast statistical features in various dimensions, including spatial, temporal, and the underlying social domain. Understanding and exploiting the features of mobile data from a social network perspective will be extremely beneficial to wireless networks, from planning, operation, and maintenance to optimization and marketing. In this paper, we categorize and analyze the big data collected from real wireless cellular networks. Then, we study the social characteristics of mobile big data and highlight several research directions for mobile big data in the social computing areas. version:1
arxiv-1609-08417 | Learning convolutional neural network to maximize Pos@Top performance measure | http://arxiv.org/abs/1609.08417 | id:1609.08417 author:Ru-Ze Liang, Gaoyuan Liang, Weizhi Li, Yi Gu, Qinfeng Li, Jim Jing-Yan Wang category:cs.CV  published:2016-09-27 summary:In the machine learning problems, the performance measure is used to evaluate the machine learning models. Recently, the number positive data points ranked at the top positions (Pos@Top) has been a popular performance measure in the machine learning community. In this paper, we propose to learn a convolutional neural network (CNN) model to maximize the Pos@Top performance measure. The CNN model is used to represent the multi-instance data point, and a classifier function is used to predict the label from the its CNN representation. We propose to minimize the loss function of Pos@Top over a training set to learn the filters of CNN and the classifier parameter. The classifier parameter vector is solved by the Lagrange multiplier method, and the filters are updated by the gradient descent method alternately in an iterative algorithm. Experiments over benchmark data sets show that the proposed method outperforms the state-of-the-art Pos@Top maximization methods. version:2
arxiv-1609-09582 | Digitizing Municipal Street Inspections Using Computer Vision | http://arxiv.org/abs/1609.09582 | id:1609.09582 author:Varun Adibhatla, Shi Fan, Krystof Litomisky, Patrick Atwater category:cs.CY cs.CV  published:2016-09-30 summary:"People want an authority to tell them how to value things. But they chose this authority not based on facts or results. They chose it because it seems authoritative and familiar." - The Big Short The pavement condition index is one such a familiar measure used by many US cities to measure street quality and justify billions of dollars spent every year on street repair. These billion-dollar decisions are based on evaluation criteria that are subjective and not representative. In this paper, we build upon our initial submission to D4GX 2015 that approaches this problem of information asymmetry in municipal decision-making. We describe a process to identify street-defects using computer vision techniques on data collected using the Street Quality Identification Device (SQUID). A User Interface to host a large quantity of image data towards digitizing the street inspection process and enabling actionable intelligence for a core public service is also described. This approach of combining device, data and decision-making around street repair enables cities make targeted decisions about street repair and could lead to an anticipatory response which can result in significant cost savings. Lastly, we share lessons learnt from the deployment of SQUID in the city of Syracuse, NY. version:1
arxiv-1609-09580 | Referential Uncertainty and Word Learning in High-dimensional, Continuous Meaning Spaces | http://arxiv.org/abs/1609.09580 | id:1609.09580 author:Michael Spranger, Katrien Beuls category:cs.CL  published:2016-09-30 summary:This paper discusses lexicon word learning in high-dimensional meaning spaces from the viewpoint of referential uncertainty. We investigate various state-of-the-art Machine Learning algorithms and discuss the impact of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact. version:1
arxiv-1610-00579 | Quantifying Urban Traffic Anomalies | http://arxiv.org/abs/1610.00579 | id:1610.00579 author:Zhengyi Zhou, Philipp Meerkamp, Chris Volinsky category:cs.LG  published:2016-09-30 summary:Detecting and quantifying anomalies in urban traffic is critical for real-time alerting or re-routing in the short run and urban planning in the long run. We describe a two-step framework that achieves these two goals in a robust, fast, online, and unsupervised manner. First, we adapt stable principal component pursuit to detect anomalies for each road segment. This allows us to pinpoint traffic anomalies early and precisely in space. Then we group the road-level anomalies across time and space into meaningful anomaly events using a simple graph expansion procedure. These events can be easily clustered, visualized, and analyzed by urban planners. We demonstrate the effectiveness of our system using 7 weeks of anonymized and aggregated cellular location data in Dallas-Fort Worth. We suggest potential opportunities for urban planners and policy makers to use our methodology to make informed changes. These applications include real-time re-routing of traffic in response to abnormally high traffic, or identifying candidates for high-impact infrastructure projects. version:1
arxiv-1609-09563 | Asynchronous Multi-Task Learning | http://arxiv.org/abs/1609.09563 | id:1609.09563 author:Inci M. Baytas, Ming Yan, Anil K. Jain, Jiayu Zhou category:cs.LG cs.DC  published:2016-09-30 summary:Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and real-world datasets demonstrate the efficiency and effectiveness of the proposed framework. version:1
arxiv-1609-09552 | Controlling Output Length in Neural Encoder-Decoders | http://arxiv.org/abs/1609.09552 | id:1609.09552 author:Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, Manabu Okumura category:cs.CL  published:2016-09-30 summary:Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task. version:1
arxiv-1609-09545 | Two-stage Convolutional Part Heatmap Regression for the 1st 3D Face Alignment in the Wild (3DFAW) Challenge | http://arxiv.org/abs/1609.09545 | id:1609.09545 author:Adrian Bulat, Georgios Tzimiropoulos category:cs.CV  published:2016-09-29 summary:This paper describes our submission to the 1st 3D Face Alignment in the Wild (3DFAW) Challenge. Our method builds upon the idea of convolutional part heatmap regression [1], extending it for 3D face alignment. Our method decomposes the problem into two parts: (a) X,Y (2D) estimation and (b) Z (depth) estimation. At the first stage, our method estimates the X,Y coordinates of the facial landmarks by producing a set of 2D heatmaps, one for each landmark, using convolutional part heatmap regression. Then, these heatmaps, alongside the input RGB image, are used as input to a very deep subnetwork trained via residual learning for regressing the Z coordinate. Our method ranked 1st in the 3DFAW Challenge, surpassing the second best result by more than 22%. version:1
arxiv-1609-09544 | Algorithms for item categorization based on ordinal ranking data | http://arxiv.org/abs/1609.09544 | id:1609.09544 author:Josh Girson, Shuchin Aeron category:cs.LG cs.SI  published:2016-09-29 summary:We present a new method for identifying the latent categorization of items based on their rankings. Complimenting a recent work that uses a Dirichlet prior on preference vectors and variational inference, we show that this problem can be effectively dealt with using existing community detection algorithms, with the communities corresponding to item categories. In particular we convert the bipartite ranking data to a unipartite graph of item affinities, and apply community detection algorithms. In this context we modify an existing algorithm - namely the label propagation algorithm to a variant that uses the distance between the nodes for weighting the label propagation - to identify the categories. We propose and analyze a synthetic ordinal ranking model and show its relation to the recently much studied stochastic block model. We test our algorithms on synthetic data and compare performance with several popular community detection algorithms. We also test the method on real data sets of movie categorization from the Movie Lens database. In all of the cases our algorithm is able to identify the categories for a suitable choice of tuning parameter. version:1
arxiv-1609-09525 | Multi-dimensional signal approximation with sparse structured priors using split Bregman iterations | http://arxiv.org/abs/1609.09525 | id:1609.09525 author:Yoann Isaac, Quentin BarthÃ©lemy, CÃ©dric Gouy-Pailler, MichÃ¨le Sebag, Jamal Atif category:cs.DS cs.CV cs.LG  published:2016-09-29 summary:This paper addresses the structurally-constrained sparse decomposition of multi-dimensional signals onto overcomplete families of vectors, called dictionaries. The contribution of the paper is threefold. Firstly, a generic spatio-temporal regularization term is designed and used together with the standard $\ell_1$ regularization term to enforce a sparse decomposition preserving the spatio-temporal structure of the signal. Secondly, an optimization algorithm based on the split Bregman approach is proposed to handle the associated optimization problem, and its convergence is analyzed. Our well-founded approach yields same accuracy as the other algorithms at the state-of-the-art, with significant gains in terms of convergence speed. Thirdly, the empirical validation of the approach on artificial and real-world problems demonstrates the generality and effectiveness of the method. On artificial problems, the proposed regularization subsumes the Total Variation minimization and recovers the expected decomposition. On the real-world problem of electro-encephalography brainwave decomposition, the approach outperforms similar approaches in terms of P300 evoked potentials detection, using structured spatial priors to guide the decomposition. version:1
arxiv-1609-09522 | Charged Point Normalization: An Efficient Solution to the Saddle Point Problem | http://arxiv.org/abs/1609.09522 | id:1609.09522 author:Armen Aghajanyan category:cs.LG  published:2016-09-29 summary:Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks. version:1
arxiv-1609-09519 | Max-plus statistical leverage scores | http://arxiv.org/abs/1609.09519 | id:1609.09519 author:James Hook category:stat.ML cs.LG  published:2016-09-29 summary:The statistical leverage scores of a complex matrix $A\in\mathbb{C}^{n\times d}$ record the degree of alignment between col$(A)$ and the coordinate axes in $\mathbb{C}^n$. These score are used in random sampling algorithms for solving certain numerical linear algebra problems. In this paper we present a max-plus algebraic analogue for statistical leverage scores. We show that max-plus statistical leverage scores can be used to calculate the exact asymptotic behavior of the conventional statistical leverage scores of a generic matrices of Puiseux series and also provide a novel way to approximate the conventional statistical leverage scores of a fixed or complex matrix. The advantage of approximating a complex matrices scores with max-plus scores is that the max-plus scores can be computed very quickly. This approximation is typically accurate to within an order or magnitude and should be useful in practical problems where the true scores are known to vary widely. version:1
arxiv-1609-09481 | Fast learning rates with heavy-tailed losses | http://arxiv.org/abs/1609.09481 | id:1609.09481 author:Vu Dinh, Lam Si Tung Ho, Duy Nguyen, Binh T. Nguyen category:stat.ML cs.LG  published:2016-09-29 summary:We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function $\sup_{f \in \mathcal{F}} \ell \circ f $, where $\ell$ is the loss function and $\mathcal{F}$ is the hypothesis class, exists and is $L^r$-integrable, and (ii) $\ell$ satisfies the multi-scale Bernstein's condition on $\mathcal{F}$. Under these assumptions, we prove that learning rate faster than $O(n^{-1/2})$ can be obtained and, depending on $r$ and the multi-scale Bernstein's powers, can be arbitrarily close to $O(n^{-1})$. We then verify these assumptions and derive fast learning rates for the problem of vector quantization by $k$-means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints. version:1
arxiv-1609-09471 | Classifier comparison using precision | http://arxiv.org/abs/1609.09471 | id:1609.09471 author:Lovedeep Gondara category:cs.LG stat.ML  published:2016-09-29 summary:New proposed models are often compared to state-of-the-art using statistical significance testing. Literature is scarce for classifier comparison using metrics other than accuracy. We present a survey of statistical methods that can be used for classifier comparison using precision, accounting for inter-precision correlation arising from use of same dataset. Comparisons are made using per-class precision and methods presented to test global null hypothesis of an overall model comparison. Comparisons are extended to multiple multi-class classifiers and to models using cross validation or its variants. Partial Bayesian update to precision is introduced when population prevalence of a class is known. Applications to compare deep architectures are studied. version:1
arxiv-1609-09468 | Reconstructing Vechicles from a Single Image: Shape Priors for Road Scene Understanding | http://arxiv.org/abs/1609.09468 | id:1609.09468 author:J. Krishna Murthy, G. V. Sai Krishna, Falak Chhaya, K. Madhava Krishna category:cs.CV cs.RO  published:2016-09-29 summary:We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in \emph{shape priors}, which are learnt over a small keypoint-annotated dataset. We then formulate a shape-aware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle \emph{keypoints} in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches. version:1
arxiv-1609-09451 | Redefining Binarization and the Visual Archetype | http://arxiv.org/abs/1609.09451 | id:1609.09451 author:Anguelos Nicolaou, Liwicki Marcus category:cs.CV  published:2016-09-29 summary:Although binarization is considered passe, it still remains a highly popular research topic. In this paper we propose a rethinking of what binarization is. We introduce the notion of the visual archetype as the ideal form of any one document. Binarization can be defined as the restoration of the visual archetype for a class of images. This definition broadens the scope of what binarization means but also suggests ground-truth should focus on the foreground. version:1
arxiv-1609-09444 | Contextual RNN-GANs for Abstract Reasoning Diagram Generation | http://arxiv.org/abs/1609.09444 | id:1609.09444 author:Arnab Ghosh, Viveka Kulharia, Amitabha Mukerjee, Vinay Namboodiri, Mohit Bansal category:cs.CV cs.AI cs.LG  published:2016-09-29 summary:Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art. version:1
arxiv-1609-09432 | A Searchlight Factor Model Approach for Locating Shared Information in Multi-Subject fMRI Analysis | http://arxiv.org/abs/1609.09432 | id:1609.09432 author:Hejia Zhang, Po-Hsuan Chen, Janice Chen, Xia Zhu, Javier S. Turek, Theodore L. Willke, Uri Hasson, Peter J. Ramadge category:stat.ML cs.CV q-bio.NC  published:2016-09-29 summary:There is a growing interest in joint multi-subject fMRI analysis. The challenge of such analysis comes from inherent anatomical and functional variability across subjects. One approach to resolving this is a shared response factor model. This assumes a shared and time synchronized stimulus across subjects. Such a model can often identify shared information, but it may not be able to pinpoint with high resolution the spatial location of this information. In this work, we examine a searchlight based shared response model to identify shared information in small contiguous regions (searchlights) across the whole brain. Validation using classification tasks demonstrates that we can pinpoint informative local regions. version:1
arxiv-1609-09430 | CNN Architectures for Large-Scale Audio Classification | http://arxiv.org/abs/1609.09430 | id:1609.09430 author:Shawn Hershey, Sourish Chaudhuri, Daniel P. W. Ellis, Jort F. Gemmeke, Aren Jansen, R. Channing Moore, Manoj Plakal, Devin Platt, Rif A. Saurous, Bryan Seybold, Malcolm Slaney, Ron J. Weiss, Kevin Wilson category:cs.SD cs.LG stat.ML  published:2016-09-29 summary:Convolutional Neural Networks (CNNs) have proven very effective in image classification and have shown promise for audio classification. We apply various CNN architectures to audio and investigate their ability to classify videos with a very large data set of 70M training videos (5.24 million hours) with 30,871 labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet, VGG, Inception, and ResNet. We explore the effects of training with different sized subsets of the training videos. Additionally we report the effect of training using different subsets of the labels. While our dataset contains video-level labels, we are also interested in Acoustic Event Detection (AED) and train a classifier on embeddings learned from the video-level task on Audio Set [5]. We find that derivatives of image classification networks do well on our audio classification task, that increasing the number of labels we train on provides some improved performance over subsets of labels, that performance of models improves as we increase training set size, and that a model using embeddings learned from the video-level task does much better than a baseline on the Audio Set classification task. version:1
arxiv-1609-09408 | Cooperative Training of Descriptor and Generator Networks | http://arxiv.org/abs/1609.09408 | id:1609.09408 author:Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu category:stat.ML cs.CV  published:2016-09-29 summary:This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. %In the training of the descriptor net, the Langevin sampling is used to sample synthesized examples from the model. In the training of the generator net, the Langevin sampling is used to sample the latent factors from the posterior distribution. The Langevin sampling in both algorithms can be time consuming. We observe that the two training algorithms can cooperate with each other by jumpstarting each other's Langevin sampling, and they can be naturally and seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously. version:1
arxiv-1609-09405 | Evaluating Induced CCG Parsers on Grounded Semantic Parsing | http://arxiv.org/abs/1609.09405 | id:1609.09405 author:Yonatan Bisk, Siva Reddy, John Blitzer, Julia Hockenmaier, Mark Steedman category:cs.CL cs.AI  published:2016-09-29 summary:We compare the effectiveness of four different syntactic CCG parsers for a semantic slot-filling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems. version:1
arxiv-1609-09382 | Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks | http://arxiv.org/abs/1609.09382 | id:1609.09382 author:Othman Zennaki, Nasredine Semmar, Laurent Besacier category:cs.CL  published:2016-09-29 summary:This work focuses on the rapid development of linguistic annotation tools for resource-poor languages. We experiment several cross-lingual annotation projection methods using Recurrent Neural Networks (RNN) models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between the source and target language. More precisely, our method has the following characteristics: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. We investigate both uni- and bi-directional RNN models and propose a method to include external information (for instance low level information from POS) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers. version:1
arxiv-1609-09365 | Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks | http://arxiv.org/abs/1609.09365 | id:1609.09365 author:Julie Dequaire, Dushyant Rao, Peter Ondruska, Dominic Wang, Ingmar Posner category:cs.CV cs.AI cs.LG cs.RO  published:2016-09-29 summary:This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work. version:1
arxiv-1609-09341 | Machine Learning Techniques for Stackelberg Security Games: a Survey | http://arxiv.org/abs/1609.09341 | id:1609.09341 author:Giuseppe De Nittis, Francesco TrovÃ² category:cs.GT cs.LG  published:2016-09-29 summary:The present survey aims at presenting the current machine learning techniques employed in security games domains. Specifically, we focused on papers and works developed by the Teamcore of University of Southern California, which deepened different directions in this field. After a brief introduction on Stackelberg Security Games (SSGs) and the poaching setting, the rest of the work presents how to model a boundedly rational attacker taking into account her human behavior, then describes how to face the problem of having attacker's payoffs not defined and how to estimate them and, finally, presents how online learning techniques have been exploited to learn a model of the attacker. version:1
arxiv-1609-09315 | Semantic Parsing with Semi-Supervised Sequential Autoencoders | http://arxiv.org/abs/1609.09315 | id:1609.09315 author:TomÃ¡Å¡ KoÄiskÃ½, GÃ¡bor Melis, Edward Grefenstette, Chris Dyer, Wang Ling, Phil Blunsom, Karl Moritz Hermann category:cs.CL cs.AI cs.NE  published:2016-09-29 summary:We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. version:1
arxiv-1609-07160 | Deep Learning in Multi-Layer Architectures of Dense Nuclei | http://arxiv.org/abs/1609.07160 | id:1609.07160 author:Yonghua Yin, Erol Gelenbe category:cs.NE cs.CV  published:2016-09-22 summary:We assume that, within the dense clusters of neurons that can be found in nuclei, cells may interconnect via soma-to-soma interactions, in addition to conventional synaptic connections. We illustrate this idea with a multi-layer architecture (MLA) composed of multiple clusters of recurrent sub-networks of spiking Random Neural Networks (RNN) with dense soma-to-soma interactions, and use this RNN-MLA architecture for deep learning. The inputs to the clusters are first normalised by adjusting the external arrival rates of spikes to each cluster. Then we apply this architecture to learning from multi-channel datasets. Numerical results based on both images and sensor based data, show the value of this novel architecture for deep learning. version:2
arxiv-1609-09296 | Comprehensive Evaluation of OpenCL-based Convolutional Neural Network Accelerators in Xilinx and Altera FPGAs | http://arxiv.org/abs/1609.09296 | id:1609.09296 author:R. Tapiador, A. Rios-Navarro, A. Linares-Barranco, Minkyu Kim, Deepak Kadetotad, Jae-sun Seo category:cs.CV cs.DC  published:2016-09-29 summary:Deep learning has significantly advanced the state of the art in artificial intelligence, gaining wide popularity from both industry and academia. Special interest is around Convolutional Neural Networks (CNN), which take inspiration from the hierarchical structure of the visual cortex, to form deep layers of convolutional operations, along with fully connected classifiers. Hardware implementations of these deep CNN architectures are challenged with memory bottlenecks that require many convolution and fully-connected layers demanding large amount of communication for parallel computation. Multi-core CPU based solutions have demonstrated their inadequacy for this problem due to the memory wall and low parallelism. Many-core GPU architectures show superior performance but they consume high power and also have memory constraints due to inconsistencies between cache and main memory. FPGA design solutions are also actively being explored, which allow implementing the memory hierarchy using embedded BlockRAM. This boosts the parallel use of shared memory elements between multiple processing units, avoiding data replicability and inconsistencies. This makes FPGAs potentially powerful solutions for real-time classification of CNNs. Both Altera and Xilinx have adopted OpenCL co-design framework from GPU for FPGA designs as a pseudo-automatic development solution. In this paper, a comprehensive evaluation and comparison of Altera and Xilinx OpenCL frameworks for a 5-layer deep CNN is presented. Hardware resources, temporal performance and the OpenCL architecture for CNNs are discussed. Xilinx demonstrates faster synthesis, better FPGA resource utilization and more compact boards. Altera provides multi-platforms tools, mature design community and better execution times. version:1
arxiv-1609-08151 | Nonnegative autoencoder with simplified random neural network | http://arxiv.org/abs/1609.08151 | id:1609.08151 author:Yonghua Yin, Erol Gelenbe category:cs.LG  published:2016-09-25 summary:This paper proposes new nonnegative (shallow and multi-layer) autoencoders by combining the spiking Random Neural Network (RNN) model, the network architecture typical used in deep-learning area and the training technique inspired from nonnegative matrix factorization (NMF). The shallow autoencoder is a simplified RNN model, which is then stacked into a multi-layer architecture. The learning algorithm is based on the weight update rules in NMF, subject to the nonnegative probability constraints of the RNN. The autoencoders equipped with this learning algorithm are tested on typical image datasets including the MNIST, Yale face and CIFAR-10 datasets, and also using 16 real-world datasets from different areas. The results obtained through these tests yield the desired high learning and recognition accuracy. Also, numerical simulations of the stochastic spiking behavior of this RNN auto encoder, show that it can be implemented in a highly-distributed manner. version:2
arxiv-1609-09267 | Robust Moving Objects Detection in Lidar Data Exploiting Visual Cues | http://arxiv.org/abs/1609.09267 | id:1609.09267 author:Gheorghii Postica, Andrea Romanoni, Matteo Matteucci category:cs.RO cs.CV  published:2016-09-29 summary:Detecting moving objects in dynamic scenes from sequences of lidar scans is an important task in object tracking, mapping, localization, and navigation. Many works focus on changes detection in previously observed scenes, while a very limited amount of literature addresses moving objects detection. The state-of-the-art method exploits Dempster-Shafer Theory to evaluate the occupancy of a lidar scan and to discriminate points belonging to the static scene from moving ones. In this paper we improve both speed and accuracy of this method by discretizing the occupancy representation, and by removing false positives through visual cues. Many false positives lying on the ground plane are also removed thanks to a novel ground plane removal algorithm. Efficiency is improved through an octree indexing strategy. Experimental evaluation against the KITTI public dataset shows the effectiveness of our approach, both qualitatively and quantitatively with respect to the state- of-the-art. version:1
arxiv-1609-08349 | Multi-label Methods for Prediction with Sequential Data | http://arxiv.org/abs/1609.08349 | id:1609.08349 author:Jesse Read, Luca Martino, Jaakko HollmÃ©n category:cs.LG cs.DS stat.ML  published:2016-09-27 summary:The number of methods available for classification of multi-label data has increased rapidly over recent years, yet relatively few links have been made with the related task of classification of sequential data. If labels indices are considered as time indices, the problems can often be seen as equivalent. In this paper we detect and elaborate on connections between multi-label methods and Markovian models, and study the suitability of multi-label methods for prediction in sequential data. From this study we draw upon the most suitable techniques from the area and develop two novel competitive approaches which can be applied to either kind of data. We carry out an empirical evaluation investigating performance on real-world sequential-prediction tasks: electricity demand, and route prediction. As well as showing that several popular multi-label algorithms are in fact easily applicable to sequencing tasks, our novel approaches, which benefit from a unified view of these areas, prove very competitive against established methods. version:2
arxiv-1609-09251 | Kernel Methods on Approximate Infinite-Dimensional Covariance Operators for Image Classification | http://arxiv.org/abs/1609.09251 | id:1609.09251 author:HÃ  Quang Minh, Marco San Biagio, Loris Bazzani, Vittorio Murino category:cs.CV  published:2016-09-29 summary:This paper presents a novel framework for visual object recognition using infinite-dimensional covariance operators of input features in the paradigm of kernel methods on infinite-dimensional Riemannian manifolds. Our formulation provides in particular a rich representation of image features by exploiting their non-linear correlations. Theoretically, we provide a finite-dimensional approximation of the Log-Hilbert-Schmidt (Log-HS) distance between covariance operators that is scalable to large datasets, while maintaining an effective discriminating capability. This allows us to efficiently approximate any continuous shift-invariant kernel defined using the Log-HS distance. At the same time, we prove that the Log-HS inner product between covariance operators is only approximable by its finite-dimensional counterpart in a very limited scenario. Consequently, kernels defined using the Log-HS inner product, such as polynomial kernels, are not scalable in the same way as shift-invariant kernels. Computationally, we apply the approximate Log-HS distance formulation to covariance operators of both handcrafted and convolutional features, exploiting both the expressiveness of these features and the power of the covariance representation. Empirically, we tested our framework on the task of image classification on twelve challenging datasets. In almost all cases, the results obtained outperform other state of the art methods, demonstrating the competitiveness and potential of our framework. version:1
arxiv-1609-09247 | Training Dependency Parsers with Partial Annotation | http://arxiv.org/abs/1609.09247 | id:1609.09247 author:Zhenghua Li, Yue Zhang, Jiayuan Chao, Min Zhang category:cs.CL cs.LG  published:2016-09-29 summary:Recently, these has been a surge on studying how to obtain partially annotated data for model supervision. However, there still lacks a systematic study on how to train statistical models with partial annotation (PA). Taking dependency parsing as our case study, this paper describes and compares two straightforward approaches for three mainstream dependency parsers. The first approach is previously proposed to directly train a log-linear graph-based parser (LLGPar) with PA based on a forest-based objective. This work for the first time proposes the second approach to directly training a linear graph-based parse (LGPar) and a linear transition-based parser (LTPar) with PA based on the idea of constrained decoding. We conduct extensive experiments on Penn Treebank under three different settings for simulating PA, i.e., random dependencies, most uncertain dependencies, and dependencies with divergent outputs from the three parsers. The results show that LLGPar is most effective in learning from PA and LTPar lags behind the graph-based counterparts by large margin. Moreover, LGPar and LTPar can achieve best performance by using LLGPar to complete PA into full annotation (FA). version:1
arxiv-1609-09240 | Modelling depth for nonparametric foreground segmentation using RGBD devices | http://arxiv.org/abs/1609.09240 | id:1609.09240 author:Gabriel MoyÃ -Alcover, Ahmed Elgammal, Antoni Jaume-i-CapÃ³, Javier Varona category:cs.CV  published:2016-09-29 summary:The problem of detecting changes in a scene and segmenting the foreground from background is still challenging, despite previous work. Moreover, new RGBD capturing devices include depth cues, which could be incorporated to improve foreground segmentation. In this work, we present a new nonparametric approach where a unified model mixes the device multiple information cues. In order to unify all the device channel cues, a new probabilistic depth data model is also proposed where we show how handle the inaccurate data to improve foreground segmentation. A new RGBD video dataset is presented in order to introduce a new standard for comparison purposes of this kind of algorithms. Results show that the proposed approach can handle several practical situations and obtain good results in all cases. version:1
arxiv-1609-09227 | A comparative study of complexity of handwritten Bharati characters with that of major Indian scripts | http://arxiv.org/abs/1609.09227 | id:1609.09227 author:Manali Naik, V. Srinivasa Chakravarthy category:cs.CV  published:2016-09-29 summary:We present Bharati, a simple, novel script that can represent the characters of a majority of contemporary Indian scripts. The shapes/motifs of Bharati characters are drawn from some of the simplest characters of existing Indian scripts. Bharati characters are designed such that they strictly reflect the underlying phonetic organization, thereby attributing to the script qualities of simplicity, familiarity, ease of acquisition and use. Thus, employing Bharati script as a common script for a majority of Indian languages can ameliorate several existing communication bottlenecks in India. We perform a complexity analysis of handwritten Bharati script and compare its complexity with that of 9 major Indian scripts. The measures of complexity are derived from a theory of handwritten characters based on Catastrophe theory. Bharati script is shown to be simpler than the 9 major Indian scripts in most measures of complexity. version:1
arxiv-1609-09220 | CNN-aware Binary Map for General Semantic Segmentation | http://arxiv.org/abs/1609.09220 | id:1609.09220 author:Mahdyar Ravanbakhsh, Hossein Mousavi, Moin Nabi, Mohammad Rastegari, Carlo Regazzoni category:cs.CV  published:2016-09-29 summary:In this paper we introduce a novel method for general semantic segmentation that can benefit from general semantics of Convolutional Neural Network (CNN). Our segmentation proposes visually and semantically coherent image segments. We use binary encoding of CNN features to overcome the difficulty of the clustering on the high-dimensional CNN feature space. These binary codes are very robust against noise and non-semantic changes in the image. These binary encoding can be embedded into the CNN as an extra layer at the end of the network. This results in real-time segmentation. To the best of our knowledge our method is the first attempt on general semantic image segmentation using CNN. All the previous papers were limited to few number of category of the images (e.g. PASCAL VOC). Experiments show that our segmentation algorithm outperform the state-of-the-art non-semantic segmentation methods by large margin. version:1
arxiv-1609-09199 | Structure-Aware Classification using Supervised Dictionary Learning | http://arxiv.org/abs/1609.09199 | id:1609.09199 author:Yael Yankelevsky, Michael Elad category:cs.LG cs.CV  published:2016-09-29 summary:In this paper, we propose a supervised dictionary learning algorithm that aims to preserve the local geometry in both dimensions of the data. A graph-based regularization explicitly takes into account the local manifold structure of the data points. A second graph regularization gives similar treatment to the feature domain and helps in learning a more robust dictionary. Both graphs can be constructed from the training data or learned and adapted along the dictionary learning process. The combination of these two terms promotes the discriminative power of the learned sparse representations and leads to improved classification accuracy. The proposed method was evaluated on several different datasets, representing both single-label and multi-label classification problems, and demonstrated better performance compared with other dictionary based approaches. version:1
arxiv-1609-09196 | EXTRACT: Strong Examples from Weakly-Labeled Sensor Data | http://arxiv.org/abs/1609.09196 | id:1609.09196 author:Davis W. Blalock, John V. Guttag category:stat.ML cs.DB cs.LG  published:2016-09-29 summary:Thanks to the rise of wearable and connected devices, sensor-generated time series comprise a large and growing fraction of the world's data. Unfortunately, extracting value from this data can be challenging, since sensors report low-level signals (e.g., acceleration), not the high-level events that are typically of interest (e.g., gestures). We introduce a technique to bridge this gap by automatically extracting examples of real-world events in low-level data, given only a rough estimate of when these events have taken place. By identifying sets of features that repeat in the same temporal arrangement, we isolate examples of such diverse events as human actions, power consumption patterns, and spoken words with up to 96% precision and recall. Our method is fast enough to run in real time and assumes only minimal knowledge of which variables are relevant or the lengths of events. Our evaluation uses numerous publicly available datasets and over 1 million samples of manually labeled sensor data. version:1
arxiv-1609-09194 | Multi Model Data mining approach for Heart failure prediction | http://arxiv.org/abs/1609.09194 | id:1609.09194 author:Priyanka H U, Vivek R category:cs.LG cs.CY  published:2016-09-29 summary:Developing predictive modelling solutions for risk estimation is extremely challenging in health-care informatics. Risk estimation involves integration of heterogeneous clinical sources having different representation from different health-care provider making the task increasingly complex. Such sources are typically voluminous, diverse, and significantly change over the time. Therefore, distributed and parallel computing tools collectively termed big data tools are in need which can synthesize and assist the physician to make right clinical decisions. In this work we propose multi-model predictive architecture, a novel approach for combining the predictive ability of multiple models for better prediction accuracy. We demonstrate the effectiveness and efficiency of the proposed work on data from Framingham Heart study. Results show that the proposed multi-model predictive architecture is able to provide better accuracy than best model approach. By modelling the error of predictive models we are able to choose sub set of models which yields accurate results. More information was modelled into system by multi-level mining which has resulted in enhanced predictive accuracy. version:1
arxiv-1609-09189 | Learning Sentence Representation with Guidance of Human Attention | http://arxiv.org/abs/1609.09189 | id:1609.09189 author:Shaonan Wang, Jiajun Zhang, Chengqing Zong category:cs.CL  published:2016-09-29 summary:The most existing sentence representation models typically treat each word in sentences equally. However, extensive studies have proven that human read sentences by making a sequence of fixation and saccades (Rayner 1998), which is extremely efficient. In this paper, we propose two novel approaches, using significant predictors of human reading time, e.g., surprisal and word classes, implemented as attention models to improve representation capability of sentence embeddings. One approach utilizes surprisal directly as the attention weight over baseline models. The other one builds attention model with the help of POS tag and CCG supertag vectors which are trained together with word embeddings in the process of sentence representation learning. In experiments, we have evaluated our models on 24 textual semantic similarity datasets and the results demonstrate that the proposed models significantly outperform the state-of-the-art sentence representation models. version:1
arxiv-1609-09188 | Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis | http://arxiv.org/abs/1609.09188 | id:1609.09188 author:Leonard K. M. Poon, Nevin L. Zhang category:cs.CL cs.IR cs.LG  published:2016-09-29 summary:Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. To address this problem, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet Allocation, we use a recently proposed method called hierarchical latent tree analysis for topic modeling. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently. version:1
arxiv-1609-09178 | OPML: A One-Pass Closed-Form Solution for Online Metric Learning | http://arxiv.org/abs/1609.09178 | id:1609.09178 author:Wenbin Li, Yang Gao, Lei Wang, Luping Zhou, Jing Huo, Yinghuan Shi category:cs.LG cs.CV  published:2016-09-29 summary:To achieve a low computational cost when performing online metric learning for large-scale data, we present a one-pass closed-form solution namely OPML in this paper. Typically, the proposed OPML first adopts a one-pass triplet construction strategy, which aims to use only a very small number of triplets to approximate the representation ability of whole original triplets obtained by batch-manner methods. Then, OPML employs a closed-form solution to update the metric for new coming samples, which leads to a low space (i.e., $O(d)$) and time (i.e., $O(d^2)$) complexity, where $d$ is the feature dimensionality. In addition, an extension of OPML (namely COPML) is further proposed to enhance the robustness when in real case the first several samples come from the same class (i.e., cold start problem). In the experiments, we have systematically evaluated our methods (OPML and COPML) on three typical tasks, including UCI data classification, face verification, and abnormal event detection in videos, which aims to fully evaluate the proposed methods on different sample number, different feature dimensionalities and different feature extraction ways (i.e., hand-crafted and deeply-learned). The results show that OPML and COPML can obtain the promising performance with a very low computational cost. Also, the effectiveness of COPML under the cold start setting is experimentally verified. version:1
arxiv-1609-06804 | Decoupled Asynchronous Proximal Stochastic Gradient Descent with Variance Reduction | http://arxiv.org/abs/1609.06804 | id:1609.06804 author:Zhouyuan Huo, Bin Gu, Heng Huang category:cs.LG math.OC  published:2016-09-22 summary:In the era of big data, optimizing large scale machine learning problems becomes a challenging task and draws significant attention. Asynchronous optimization algorithms come out as a promising solution. Recently, decoupled asynchronous proximal stochastic gradient descent (DAP-SGD) is proposed to minimize a composite function. It is claimed to be able to off-loads the computation bottleneck from server to workers by allowing workers to evaluate the proximal operators, therefore, server just need to do element-wise operations. However, it still suffers from slow convergence rate because of the variance of stochastic gradient is nonzero. In this paper, we propose a faster method, decoupled asynchronous proximal stochastic variance reduced gradient descent method (DAP-SVRG). We prove that our method has linear convergence for strongly convex problem. Large-scale experiments are also conducted in this paper, and results demonstrate our theoretical analysis. version:2
arxiv-1609-09171 | Empirical Evaluation of RNN Architectures on Sentence Classification Task | http://arxiv.org/abs/1609.09171 | id:1609.09171 author:Lei Shen, Junlin Zhang category:cs.CL  published:2016-09-29 summary:Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP and two most popular RNN architectures are Tail Model and Pooling Model. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare performance of the three RNN structures on sentence classification task. Experimental results show that the Tail Model and Hybrid Model consistently get a better performance over Pooling Model, and Hybrid Model is comparable with Tail Model. version:1
arxiv-1609-09162 | Universum Learning for Multiclass SVM | http://arxiv.org/abs/1609.09162 | id:1609.09162 author:Sauptik Dhar, Naveen Ramakrishnan, Vladimir Cherkassky, Mohak Shah category:cs.LG  published:2016-09-29 summary:We introduce Universum learning for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We also propose a span bound for MU-SVM that can be used for model selection thereby avoiding resampling. Empirical results demonstrate the effectiveness of MU-SVM and the proposed bound. version:1
arxiv-1609-09158 | Proposal for a Leaky-Integrate-Fire Spiking Neuron based on Magneto-Electric Switching of Ferro-magnets | http://arxiv.org/abs/1609.09158 | id:1609.09158 author:Akhilesh Jaiswal, Sourjya Roy, Gopalakrishnan Srinivasan, Kaushik Roy category:cs.NE  published:2016-09-29 summary:The efficiency of the human brain in performing classification tasks has attracted considerable research interest in brain-inspired neuromorphic computing. Hardware implementations of a neuromorphic system aims to mimic the computations in the brain through interconnection of neurons and synaptic weights. A leaky-integrate-fire (LIF) spiking model is widely used to emulate the dynamics of neuronal action potentials. In this work, we propose a spin based LIF spiking neuron using the magneto-electric (ME) switching of ferro-magnets. The voltage across the ME oxide exhibits a typical leaky-integrate behavior, which in turn switches an underlying ferro-magnet. Due to the effect of thermal noise, the ferro-magnet exhibits probabilistic switching dynamics, which is reminiscent of the stochasticity exhibited by biological neurons. The energy-efficiency of the ME switching mechanism coupled with the intrinsic non-volatility of ferro-magnets result in lower energy consumption, when compared to a CMOS LIF neuron. A device to system-level simulation framework has been developed to investigate the feasibility of the proposed LIF neuron for a hand-written digit recognition problem version:1
arxiv-1609-09156 | Similarity Mapping with Enhanced Siamese Network for Multi-Object Tracking | http://arxiv.org/abs/1609.09156 | id:1609.09156 author:Minyoung Kim, Stefano Alletto, Luca Rigazio category:cs.CV cs.LG  published:2016-09-28 summary:Multi-object tracking has recently become an important area of computer vision, especially for Advanced Driver Assistance Systems (ADAS). Despite growing attention, achieving high performance tracking is still challenging, with state-of-the- art systems resulting in high complexity with a large number of hyper parameters. In this paper, we focus on reducing overall system complexity and the number hyper parameters that need to be tuned to a specific environment. We introduce a novel tracking system based on similarity mapping by Enhanced Siamese Neural Network (ESNN), which accounts for both appearance and geometric information, and is trainable end-to-end. Our system achieves competitive performance in both speed and accuracy on MOT16 challenge, compared to known state-of-the-art methods. version:1
arxiv-1609-09154 | MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization | http://arxiv.org/abs/1609.09154 | id:1609.09154 author:Ramakrishnan Kannan, Grey Ballard, Haesun Park category:cs.DC cs.NA stat.ML  published:2016-09-28 summary:Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for $W$ and $H$. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans for few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online. version:1
arxiv-1609-09058 | A Simple, Fast and Highly-Accurate Algorithm to Recover 3D Shape from 2D Landmarks on a Single Image | http://arxiv.org/abs/1609.09058 | id:1609.09058 author:Ruiqi Zhao, Yan Wang, Aleix Martinez category:cs.CV  published:2016-09-28 summary:Three-dimensional shape reconstruction of 2D landmark points on a single image is a hallmark of human vision, but is a task that has been proven difficult for computer vision algorithms. We define a feed-forward deep neural network algorithm that can reconstruct 3D shapes from 2D landmark points almost perfectly (i.e., with extremely small reconstruction errors), even when these 2D landmarks are from a single image. Our experimental results show an improvement of up to two-fold over state-of-the-art computer vision algorithms; 3D shape reconstruction of human faces is given at a reconstruction error < .004, cars at .0022, human bodies at .022, and highly-deformable flags at an error of .0004. Our algorithm was also a top performer at the 2016 3D Face Alignment in the Wild Challenge competition (done in conjunction with the European Conference on Computer Vision, ECCV) that required the reconstruction of 3D face shape from a single image. The derived algorithm can be trained in a couple hours and testing runs at more than 1, 000 frames/s on an i7 desktop. We also present an innovative data augmentation approach that allows us to train the system efficiently with small number of samples. And the system is robust to noise (e.g., imprecise landmark points) and missing data (e.g., occluded or undetected landmark points). version:1
arxiv-1609-09049 | Deep Reinforcement Learning for Tensegrity Robot Locomotion | http://arxiv.org/abs/1609.09049 | id:1609.09049 author:Xinyang Geng, Marvin Zhang, Jonathan Bruce, Ken Caluwaerts, Massimo Vespignani, Vytas SunSpiral, Pieter Abbeel, Sergey Levine category:cs.RO cs.LG  published:2016-09-28 summary:Tensegrity robots, composed of rigid rods connected by elastic cables, have a number of unique properties that make them appealing for use as planetary exploration rovers. However, control of tensegrity robots remains a difficult problem due to their unusual structures and complex dynamics. In this work, we show how locomotion gaits can be learned automatically using a novel extension of mirror descent guided policy search (MDGPS) applied to periodic locomotion movements, and we demonstrate the effectiveness of our approach on tensegrity robot locomotion. We evaluate our method with real-world and simulated experiments on the SUPERball tensegrity robot, showing that the learned policies generalize to changes in system parameters, unreliable sensor measurements, and variation in environmental conditions, including varied terrains and a range of different gravities. Our experiments demonstrate that our method not only learns fast, power-efficient feedback policies for rolling gaits, but that these policies can succeed with only the limited onboard sensing provided by SUPERball's accelerometers. We compare the learned feedback policies to learned open-loop policies and hand-engineered controllers, and demonstrate that the learned policy enables the first continuous, reliable locomotion gait for the real SUPERball robot. version:1
arxiv-1609-09028 | Stance classification in Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations | http://arxiv.org/abs/1609.09028 | id:1609.09028 author:Arkaitz Zubiaga, Elena Kochkina, Maria Liakata, Rob Procter, Michal Lukasik category:cs.CL cs.SI  published:2016-09-28 summary:Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users' replies to one another, which results in a nested tree-like structure. Previous work addressing stance classification task in Twitter has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversation structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations. version:1
arxiv-1609-09025 | Learning to Push by Grasping: Using multiple tasks for effective learning | http://arxiv.org/abs/1609.09025 | id:1609.09025 author:Lerrel Pinto, Abhinav Gupta category:cs.RO cs.CV cs.LG  published:2016-09-28 summary:Recently, end-to-end learning frameworks are gaining prevalence in the field of robot control. These frameworks input states/images and directly predict the torques or the action parameters. However, these approaches are often critiqued due to their huge data requirements for learning a task. The argument of the difficulty in scalability to multiple tasks is well founded, since training these tasks often require hundreds or thousands of examples. But do end-to-end approaches need to learn a unique model for every task? Intuitively, it seems that sharing across tasks should help since all tasks require some common understanding of the environment. In this paper, we attempt to take the next step in data-driven end-to-end learning frameworks: move from the realm of task-specific models to joint learning of multiple robot tasks. In an astonishing result we show that models with multi-task learning tend to perform better than task-specific models trained with same amounts of data. For example, a deep-network learned with 2.5K grasp and 2.5K push examples performs better on grasping than a network trained on 5K grasp examples. version:1
arxiv-1609-09019 | Psychologically Motivated Text Mining | http://arxiv.org/abs/1609.09019 | id:1609.09019 author:Ekaterina Shutova, Patricia Lichtenstein category:cs.CL  published:2016-09-28 summary:Natural language processing techniques are increasingly applied to identify social trends and predict behavior based on large text collections. Existing methods typically rely on surface lexical and syntactic information. Yet, research in psychology shows that patterns of human conceptualisation, such as metaphorical framing, are reliable predictors of human expectations and decisions. In this paper, we present a method to learn patterns of metaphorical framing from large text collections, using statistical techniques. We apply the method to data in three different languages and evaluate the identified patterns, demonstrating their psychological validity. version:1
arxiv-1609-09018 | Deep Architectures for Face Attributes | http://arxiv.org/abs/1609.09018 | id:1609.09018 author:Tobi Baumgartner, Jack Culpepper category:cs.CV  published:2016-09-28 summary:We train a deep convolutional neural network to perform identity classification using a new dataset of public figures annotated with age, gender, ethnicity and emotion labels, and then fine-tune it for attribute classification. An optimal sharing pattern of computational resources within this network is determined by experiment, requiring only 1 G flops to produce all predictions. Rather than fine-tune by relearning weights in one additional layer after the penultimate layer of the identity network, we try several different depths for each attribute. We find that prediction of age and emotion is improved by fine-tuning from earlier layers onward, presumably because deeper layers are progressively invariant to non-identity related changes in the input. version:1
arxiv-1609-09007 | Unsupervised Neural Hidden Markov Models | http://arxiv.org/abs/1609.09007 | id:1609.09007 author:Ke Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, Kevin Knight category:cs.CL cs.LG  published:2016-09-28 summary:In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context. version:1
arxiv-1609-09004 | Byte-based Language Identification with Deep Convolutional Networks | http://arxiv.org/abs/1609.09004 | id:1609.09004 author:Johannes Bjerva category:cs.CL  published:2016-09-28 summary:We report on our system for the shared task on discriminating between similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network's architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies. version:1
arxiv-1609-09001 | Learning from the Hindsight Plan -- Episodic MPC Improvement | http://arxiv.org/abs/1609.09001 | id:1609.09001 author:Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, Pieter Abbeel category:cs.RO cs.AI cs.LG  published:2016-09-28 summary:Model predictive control (MPC) is a popular control method that has proved effective for robotics, among other fields. MPC performs re-planning at every time step. Re-planning is done with a limited horizon per computational and real-time constraints and often also for robustness to potential model errors. However, the limited horizon leads to suboptimal performance. In this work, we consider the iterative learning setting, where the same task can be repeated several times, and propose a policy improvement scheme for MPC. The main idea is that between executions we can, offline, run MPC with a longer horizon, resulting in a hindsight plan. To bring the next real-world execution closer to the hindsight plan, our approach learns to re-shape the original cost function with the goal of satisfying the following property: short horizon planning (as realistic during real executions) with respect to the shaped cost should result in mimicking the hindsight plan. This effectively consolidates long-term reasoning into the short-horizon planning. We empirically evaluate our approach in contact-rich manipulation tasks both in simulated and real environments, such as peg insertion by a real PR2 robot. version:1
arxiv-1609-09000 | StruClus: Structural Clustering of Large-Scale Graph Databases | http://arxiv.org/abs/1609.09000 | id:1609.09000 author:Till SchÃ¤fer, Petra Mutzel category:cs.DB cs.DS stat.ML 68W05 I.5.3; I.1.2; F.2.2  published:2016-09-28 summary:We present a structural clustering algorithm for large-scale datasets of small labeled graphs, utilizing a frequent subgraph sampling strategy. A set of representatives provides an intuitive description of each cluster, supports the clustering process, and helps to interpret the clustering results. The projection-based nature of the clustering approach allows us to bypass dimensionality and feature extraction problems that arise in the context of graph datasets reduced to pairwise distances or feature vectors. While achieving high quality and (human) interpretable clusterings, the runtime of the algorithm only grows linearly with the number of graphs. Furthermore, the approach is easy to parallelize and therefore suitable for very large datasets. Our extensive experimental evaluation on synthetic and real world datasets demonstrates the superiority of our approach over existing structural and subspace clustering algorithms, both, from a runtime and quality point of view. version:1
arxiv-1609-08976 | Variational Autoencoder for Deep Learning of Images, Labels and Captions | http://arxiv.org/abs/1609.08976 | id:1609.08976 author:Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, Lawrence Carin category:stat.ML cs.LG  published:2016-09-28 summary:A novel variational autoencoder is developed to model images, as well as associated labels or captions. The Deep Generative Deconvolutional Network (DGDN) is used as a decoder of the latent image features, and a deep Convolutional Neural Network (CNN) is used as an image encoder; the CNN is used to approximate a distribution for the latent DGDN features/code. The latent code is also linked to generative models for labels (Bayesian support vector machine) or captions (recurrent neural network). When predicting a label/caption for a new image at test, averaging is performed across the distribution of latent codes; this is computationally efficient as a consequence of the learned CNN-based encoder. Since the framework is capable of modeling the image in the presence/absence of associated labels/captions, a new semi-supervised setting is manifested for CNN learning with images; the framework even allows unsupervised CNN learning, based on images alone. version:1
arxiv-1609-08965 | Graph Based Convolutional Neural Network | http://arxiv.org/abs/1609.08965 | id:1609.08965 author:Michael Edwards, Xianghua Xie category:cs.CV  published:2016-09-28 summary:The benefit of localized features within the regular domain has given rise to the use of Convolutional Neural Networks (CNNs) in machine learning, with great proficiency in the image classification. The use of CNNs becomes problematic within the irregular spatial domain due to design and convolution of a kernel filter being non-trivial. One solution to this problem is to utilize graph signal processing techniques and the convolution theorem to perform convolutions on the graph of the irregular domain to obtain feature map responses to learnt filters. We propose graph convolution and pooling operators analogous to those in the regular domain. We also provide gradient calculations on the input data and spectral filters, which allow for the deep learning of an irregular spatial domain problem. Signal filters take the form of spectral multipliers, applying convolution in the graph spectral domain. Applying smooth multipliers results in localized convolutions in the spatial domain, with smoother multipliers providing sharper feature maps. Algebraic Multigrid is presented as a graph pooling method, reducing the resolution of the graph through agglomeration of nodes between layers of the network. Evaluation of performance on the MNIST digit classification problem in both the regular and irregular domain is presented, with comparison drawn to standard CNN. The proposed graph CNN provides a deep learning method for the irregular domains present in the machine learning community, obtaining 94.23% on the regular grid, and 94.96% on a spatially irregular subsampled MNIST. version:1
arxiv-1609-08938 | A Discriminative Framework for Anomaly Detection in Large Videos | http://arxiv.org/abs/1609.08938 | id:1609.08938 author:Allison Del Giorno, J. Andrew Bagnell, Martial Hebert category:cs.CV stat.ML  published:2016-09-28 summary:We address an anomaly detection setting in which training sequences are unavailable and anomalies are scored independently of temporal ordering. Current algorithms in anomaly detection are based on the classical density estimation approach of learning high-dimensional models and finding low-probability events. These algorithms are sensitive to the order in which anomalies appear and require either training data or early context assumptions that do not hold for longer, more complex videos. By defining anomalies as examples that can be distinguished from other examples in the same video, our definition inspires a shift in approaches from classical density estimation to simple discriminative learning. Our contributions include a novel framework for anomaly detection that is (1) independent of temporal ordering of anomalies, and (2) unsupervised, requiring no separate training sequences. We show that our algorithm can achieve state-of-the-art results even when we adjust the setting by removing training sequences from standard datasets. version:1
arxiv-1609-08934 | Multiplicative weights, equalizers, and P=PPAD | http://arxiv.org/abs/1609.08934 | id:1609.08934 author:Ioannis Avramopoulos category:cs.GT cs.CC cs.LG  published:2016-09-28 summary:We show that, by using multiplicative weights in a game-theoretic thought experiment (and an important convexity result on the composition of multiplicative weights with the relative entropy function), a symmetric bimatrix game (that is, a bimatrix matrix wherein the payoff matrix of each player is the transpose of the payoff matrix of the other) either has an interior symmetric equilibrium or there is a pure strategy that is weakly dominated by some mixed strategy. Weakly dominated pure strategies can be detected and eliminated in polynomial time by solving a linear program. Furthermore, interior symmetric equilibria are a special case of a more general notion, namely, that of an "equalizer," which can also be computed efficiently in polynomial time by solving a linear program. An elegant "symmetrization method" of bimatrix games [Jurg et al., 1992] and the well-known PPAD-completeness results on equilibrium computation in bimatrix games [Daskalakis et al., 2009, Chen et al., 2009] imply then the compelling P = PPAD. version:1
arxiv-1609-05130 | SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks | http://arxiv.org/abs/1609.05130 | id:1609.05130 author:John McCormac, Ankur Handa, Andrew Davison, Stefan Leutenegger category:cs.CV  published:2016-09-16 summary:Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz. version:2
arxiv-1609-08913 | The Famine of Forte: Few Search Problems Greatly Favor Your Algorithm | http://arxiv.org/abs/1609.08913 | id:1609.08913 author:George D. Montanez category:stat.ML cs.IT cs.LG math.IT  published:2016-09-28 summary:No Free Lunch theorems show that the average performance across any closed-under-permutation set of problems is fixed for all algorithms, under appropriate conditions. Extending these results, we demonstrate that the proportion of favorable problems is itself strictly bounded, such that no single algorithm can perform well over a large fraction of possible problems. Our results explain why we must either continue to develop new learning methods year after year or move towards highly parameterized models that are both flexible and sensitive to their hyperparameters. version:1
arxiv-1609-08886 | Sparse principal component regression for generalized linear models | http://arxiv.org/abs/1609.08886 | id:1609.08886 author:Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko Shiroishi category:stat.ML stat.ME  published:2016-09-28 summary:Principal component regression (PCR) is a widely-used two-stage procedure: we first perform principal component analysis (PCA) and next consider a regression model in which selected principal components are regarded as new explanatory variables. We should remark that PCA is based only on the explanatory variables, so the principal components are not selected using the information on the response variable. In this paper, we propose a one-stage procedure for PCR in the framework of generalized linear models. The basic loss function is based on a combination of the regression loss and PCA loss. The estimate of the regression parameter is obtained as the minimizer of the basic loss function with sparse penalty. The proposed method is called the sparse principal component regression for generalized linear models (SPCR-glm). SPCR-glm enables us to obtain sparse principal component loadings that are related to a response variable, because the two loss functions are simultaneously taken into consideration. A combination of loss functions may cause the identifiability problem on parameters, but it is overcome by virtue of sparse penalty. The sparse penalty plays two roles in this method. The parameter estimation procedure is proposed using various update algorithms with the coordinate descent algorithm. We apply SPCR-glm to two real datasets, Doctor visit data and mouse consomic strain data. SPCR-glm provides easier interpretable PC scores and clearer classification on PC plots than the usual PCA. version:1
arxiv-1609-08870 | Approachability of convex sets in generalized quitting games | http://arxiv.org/abs/1609.08870 | id:1609.08870 author:JÃ¡nos Flesch, Rida Laraki, Vianney Perchet category:cs.GT stat.ML  published:2016-09-28 summary:We consider Blackwell approachability, a very powerful and geometric tool in game theory, used for example to design strategies of the uninformed player in repeated games with incomplete information. We extend this theory to "generalized quitting games" , a class of repeated stochastic games in which each player may have quitting actions, such as the Big-Match. We provide three simple geometric and strongly related conditions for the weak approachability of a convex target set. The first is sufficient: it guarantees that, for any fixed horizon, a player has a strategy ensuring that the expected time-average payoff vector converges to the target set as horizon goes to infinity. The third is necessary: if it is not satisfied, the opponent can weakly exclude the target set. In the special case where only the approaching player can quit the game (Big-Match of type I), the three conditions are equivalent and coincide with Blackwell's condition. Consequently, we obtain a full characterization and prove that the game is weakly determined-every convex set is either weakly approachable or weakly excludable. In games where only the opponent can quit (Big-Match of type II), none of our conditions is both sufficient and necessary for weak approachability. We provide a continuous time sufficient condition using techniques coming from differential games, and show its usefulness in practice, in the spirit of Vieille's seminal work for weak approachability.Finally, we study uniform approachability where the strategy should not depend on the horizon and demonstrate that, in contrast with classical Blackwell approacha-bility for convex sets, weak approachability does not imply uniform approachability. version:1
arxiv-1609-08864 | Towards the effectiveness of Deep Convolutional Neural Network based Fast Random Forest Classifier | http://arxiv.org/abs/1609.08864 | id:1609.08864 author:Mrutyunjaya Panda, Vani Vihar category:cs.CV  published:2016-09-28 summary:Deep Learning is considered to be a quite young in the area of machine learning research, found its effectiveness in dealing complex yet high dimensional dataset that includes but limited to images, text and speech etc. with multiple levels of representation and abstraction. As there are a plethora of research on these datasets by various researchers , a win over them needs lots of attention. Careful setting of Deep learning parameters is of paramount importance in order to avoid the overfitting unlike conventional methods with limited parameter settings. Deep Convolutional neural network (DCNN) with multiple layers of compositions and appropriate settings might be is an efficient machine learning method that can outperform the conventional methods in a great way. However, due to its slow adoption in learning, there are also always a chance of overfitting during feature selection process, which can be addressed by employing a regularization method called dropout. Fast Random Forest (FRF) is a powerful ensemble classifier especially when the datasets are noisy and when the number of attributes is large in comparison to the number of instances, as is the case of Bioinformatics datasets. Several publicly available Bioinformatics dataset, Handwritten digits recognition and Image segmentation dataset are considered for evaluation of the proposed approach. The excellent performance obtained by the proposed DCNN based feature selection with FRF classifier on high dimensional datasets makes it a fast and accurate classifier in comparison the state-of-the-art. version:1
arxiv-1609-08843 | Hierarchical Memory Networks for Answer Selection on Unknown Words | http://arxiv.org/abs/1609.08843 | id:1609.08843 author:Jiaming Xu, Jing Shi, Yiqun Yao, Suncong Zheng, Bo XuBo Xu category:cs.IR cs.AI cs.CL  published:2016-09-28 summary:Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, (k)-max pooling is exploited following reasoning module on the sentence-level memory to sample the (k) most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks. version:1
arxiv-1609-08824 | Equation Parsing: Mapping Sentences to Grounded Equations | http://arxiv.org/abs/1609.08824 | id:1609.08824 author:Subhro Roy, Shyam Upadhyay, Dan Roth category:cs.CL  published:2016-09-28 summary:Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing -- given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in $70\%$ of the cases. In $60\%$ of the time, it also identifies the correct noun phrase $\rightarrow$ variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation. version:1
arxiv-1609-08789 | Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition | http://arxiv.org/abs/1609.08789 | id:1609.08789 author:Zhiyuan Tang, Ying Shi, Dong Wang, Yang Feng, Shiyue Zhang category:cs.LG cs.CL cs.NE  published:2016-09-28 summary:Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks. version:1
arxiv-1609-08779 | Using Natural Language Processing and Qualitative Analysis to Intervene in Gang Violence: A Collaboration Between Social Work Researchers and Data Scientists | http://arxiv.org/abs/1609.08779 | id:1609.08779 author:Desmond Upton Patton, Kathleen McKeown, Owen Rambow, Jamie Macbeth category:cs.CY cs.CL  published:2016-09-28 summary:The U.S. has the highest rate of firearm-related deaths when compared to other industrialized countries. Violence particularly affects low-income, urban neighborhoods in cities like Chicago, which saw a 40% increase in firearm violence from 2014 to 2015 to more than 3,000 shooting victims. While recent studies have found that urban, gang-involved individuals curate a unique and complex communication style within and between social media platforms, organizations focused on reducing gang violence are struggling to keep up with the growing complexity of social media platforms and the sheer volume of data they present. In this paper, describe the Digital Urban Violence Analysis Approach (DUVVA), a collaborative qualitative analysis method used in a collaboration between data scientists and social work researchers to develop a suite of systems for decoding the high- stress language of urban, gang-involved youth. Our approach leverages principles of grounded theory when analyzing approximately 800 tweets posted by Chicago gang members and participation of youth from Chicago neighborhoods to create a language resource for natural language processing (NLP) methods. In uncovering the unique language and communication style, we developed automated tools with the potential to detect aggressive language on social media and aid individuals and groups in performing violence prevention and interruption. version:1
arxiv-1609-08777 | Character Sequence Models for ColorfulWords | http://arxiv.org/abs/1609.08777 | id:1609.08777 author:Kazuya Kawakami, Chris Dyer, Bryan R. Routledge, Noah A. Smith category:cs.CL  published:2016-09-28 summary:We present a neural network architecture to predict a point in color space from the sequence of characters in the color's name. Using large scale color--name pairs obtained from an online color design forum, we evaluate our model on a "color Turing test" and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at http://colorlab.us. version:1
arxiv-1609-08764 | Understanding data augmentation for classification: when to warp? | http://arxiv.org/abs/1609.08764 | id:1609.08764 author:Sebastien C. Wong, Adam Gatt, Victor Stamatescu, Mark D. McDonnell category:cs.CV I.5.2; I.4.7  published:2016-09-28 summary:In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting. version:1
arxiv-1609-08758 | Video Summarization using Deep Semantic Features | http://arxiv.org/abs/1609.08758 | id:1609.08758 author:Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne HeikkilÃ¤, Naokazu Yokoya category:cs.CV  published:2016-09-28 summary:This paper presents a video summarization technique for an Internet video to provide a quick way to overview its content. This is a challenging problem because finding important or informative parts of the original video requires to understand its content. Furthermore the content of Internet videos is very diverse, ranging from home videos to documentaries, which makes video summarization much more tough as prior knowledge is almost not available. To tackle this problem, we propose to use deep video features that can encode various levels of content semantics, including objects, actions, and scenes, improving the efficiency of standard video summarization techniques. For this, we design a deep neural network that maps videos as well as descriptions to a common semantic space and jointly trained it with associated pairs of videos and descriptions. To generate a video summary, we extract the deep features from each segment of the original video and apply a clustering-based summarization technique to them. We evaluate our video summaries using the SumMe dataset as well as baseline approaches. The results demonstrated the advantages of incorporating our deep semantic features in a video summarization technique. version:1
arxiv-1609-08752 | Stabilizing Linear Prediction Models using Autoencoder | http://arxiv.org/abs/1609.08752 | id:1609.08752 author:Shivapratap Gopakumar, Truyen Tran, Dinh Phung, Svetha Venkatesh category:stat.ML  published:2016-09-28 summary:To date, the instability of prognostic predictors in a sparse high dimensional model, which hinders their clinical adoption, has received little attention. Stable prediction is often overlooked in favour of performance. Yet, stability prevails as key when adopting models in critical areas as healthcare. Our study proposes a stabilization scheme by detecting higher order feature correlations. Using a linear model as basis for prediction, we achieve feature stability by regularising latent correlation in features. Latent higher order correlation among features is modelled using an autoencoder network. Stability is enhanced by combining a recent technique that uses a feature graph, and augmenting external unlabelled data for training the autoencoder network. Our experiments are conducted on a heart failure cohort from an Australian hospital. Stability was measured using Consistency index for feature subsets and signal-to-noise ratio for model parameters. Our methods demonstrated significant improvement in feature stability and model estimation stability when compared to baselines. version:1
arxiv-1609-08740 | Scalable Discrete Supervised Hash Learning with Asymmetric Matrix Factorization | http://arxiv.org/abs/1609.08740 | id:1609.08740 author:Shifeng Zhang, Jianmin Li, Jinma Guo, Bo Zhang category:cs.CV  published:2016-09-28 summary:Hashing method maps similar data to binary hashcodes with smaller hamming distance, and it has received a broad attention due to its low storage cost and fast retrieval speed. However, the existing limitations make the present algorithms difficult to deal with large-scale datasets: (1) discrete constraints are involved in the learning of the hash function; (2) pairwise or triplet similarity is adopted to generate efficient hashcodes, resulting both time and space complexity are greater than O(n^2). To address these issues, we propose a novel discrete supervised hash learning framework which can be scalable to large-scale datasets. First, the discrete learning procedure is decomposed into a binary classifier learning scheme and binary codes learning scheme, which makes the learning procedure more efficient. Second, we adopt the Asymmetric Low-rank Matrix Factorization and propose the Fast Clustering-based Batch Coordinate Descent method, such that the time and space complexity is reduced to O(n). The proposed framework also provides a flexible paradigm to incorporate with arbitrary hash function, including deep neural networks and kernel methods. Experiments on large-scale datasets demonstrate that the proposed method is superior or comparable with state-of-the-art hashing algorithms. version:1
arxiv-1609-09353 | Deep Multi-Species Embedding | http://arxiv.org/abs/1609.09353 | id:1609.09353 author:Di Chen, Yexiang Xue, Shuo Chen, Daniel Fink, Carla Gomes category:cs.LG q-bio.PE stat.ML  published:2016-09-28 summary:Understanding how species are distributed across landscapes over time is a fundamental question in biodiversity research. Unfortunately, most species distribution models only target a single species at a time, despite the fact that there is strong evidence that species are not independently distributed. We propose Deep Multi-Species Embedding (DMSE), which jointly embed vectors corresponding to multiple species as well as vectors representing environmental covariates into a common high dimensional feature space via a deep neural network. Applied to \textit{eBird} bird watching data, our single-species DMSE model outperforms commonly used random forest models in terms of accuracy. Our multi-species DMSE model further improves the single species version. Through this model, we are able to confirm quantitatively many species-species interactions, which are only understood qualitatively among ecologists. As an additional contribution, we provide a graphical embedding of hundreds of bird species in the Northeast US. version:1
arxiv-1609-08703 | Optimizing Neural Network Hyperparameters with Gaussian Processes for Dialog Act Classification | http://arxiv.org/abs/1609.08703 | id:1609.08703 author:Franck Dernoncourt, Ji Young Lee category:cs.CL cs.NE stat.ML  published:2016-09-27 summary:Systems based on artificial neural networks (ANNs) have achieved state-of-the-art results in many natural language processing tasks. Although ANNs do not require manually engineered features, ANNs have many hyperparameters to be optimized. The choice of hyperparameters significantly impacts models' performances. However, the ANN hyperparameters are typically chosen by manual, grid, or random search, which either requires expert experiences or is computationally expensive. Recent approaches based on Bayesian optimization using Gaussian processes (GPs) is a more systematic way to automatically pinpoint optimal or near-optimal machine learning hyperparameters. Using a previously published ANN model yielding state-of-the-art results for dialog act classification, we demonstrate that optimizing hyperparameters using GP further improves the results, and reduces the computational time by a factor of 4 compared to a random search. Therefore it is a useful technique for tuning ANN models to yield the best performances for natural language processing tasks. version:1
arxiv-1609-08686 | Training a Probabilistic Graphical Model with Resistive Switching Electronic Synapses | http://arxiv.org/abs/1609.08686 | id:1609.08686 author:S. Burc Eryilmaz, Emre Neftci, Siddharth Joshi, SangBum Kim, Matthew BrightSky, Hsiang-Lan Lung, Chung Lam, Gert Cauwenberghs, H. -S. Philip Wong category:cs.NE cs.DC cs.ET  published:2016-09-27 summary:Current large scale implementations of deep learning and data mining require thousands of processors, massive amounts of off-chip memory, and consume gigajoules of energy. Emerging memory technologies such as nanoscale two-terminal resistive switching memory devices offer a compact, scalable and low power alternative that permits on-chip co-located processing and memory in fine-grain distributed parallel architecture. Here we report first use of resistive switching memory devices for implementing and training a Restricted Boltzmann Machine (RBM), a generative probabilistic graphical model as a key component for unsupervised learning in deep networks. We experimentally demonstrate a 45-synapse RBM realized with 90 resistive switching phase change memory (PCM) elements trained with a bio-inspired variant of the Contrastive Divergence (CD) algorithm, implementing Hebbian and anti-Hebbian weight updates. The resistive PCM devices show a two-fold to ten-fold reduction in error rate in a missing pixel pattern completion task trained over 30 epochs, compared to untrained case. Measured programming energy consumption is 6.1 nJ per epoch with the resistive switching PCM devices, a factor of ~150 times lower than conventional processor-memory systems. We analyze and discuss the dependence of learning performance on cycle-to-cycle variations as well as number of gradual levels in the PCM analog memory devices. version:1
arxiv-1609-08685 | Understanding and Exploiting Object Interaction Landscapes | http://arxiv.org/abs/1609.08685 | id:1609.08685 author:SÃ¶ren Pirk, Vojtech Krs, Kaimo Hu, Suren Deepak Rajasekaran, Hao Kang, Bedrich Benes, Yusuke Yoshiyasu, Leonidas J. Guibas category:cs.GR cs.CG cs.CV  published:2016-09-27 summary:Interactions play a key role in understanding objects and scenes, for both virtual and real world agents. We introduce a new general representation for proximal interactions among physical objects that is agnostic to the type of objects or interaction involved. The representation is based on tracking particles on one of the participating objects and then observing them with sensors appropriately placed in the interaction volume or on the interaction surfaces. We show how to factorize these interaction descriptors and project them into a particular participating object so as to obtain a new functional descriptor for that object, its interaction landscape, capturing its observed use in a spatio-temporal framework. Interaction landscapes are independent of the particular interaction and capture subtle dynamic effects in how objects move and behave when in functional use. Our method relates objects based on their function, establishes correspondences between shapes based on functional key points and regions, and retrieves peer and partner objects with respect to an interaction. version:1
arxiv-1609-08677 | A Fast Factorization-based Approach to Robust PCA | http://arxiv.org/abs/1609.08677 | id:1609.08677 author:Chong Peng, Zhao Kang, Qiang Chen category:cs.CV cs.AI stat.ML  published:2016-09-27 summary:Robust principal component analysis (RPCA) has been widely used for recovering low-rank matrices in many data mining and machine learning problems. It separates a data matrix into a low-rank part and a sparse part. The convex approach has been well studied in the literature. However, state-of-the-art algorithms for the convex approach usually have relatively high complexity due to the need of solving (partial) singular value decompositions of large matrices. A non-convex approach, AltProj, has also been proposed with lighter complexity and better scalability. Given the true rank $r$ of the underlying low rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\times n$ is the size of data matrix. In this paper, we propose a novel factorization-based model of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound of the true rank. Our method does not need the precise value of the true rank. From extensive experiments, we observe that AltProj can work only when $r$ is precisely known in advance; however, when the needed rank parameter $r$ is specified to a value different from the true rank, AltProj cannot fully separate the two parts while our method succeeds. Even when both work, our method is about 4 times faster than AltProj. Our method can be used as a light-weight, scalable tool for RPCA in the absence of the precise value of the true rank. version:1
arxiv-1609-08675 | YouTube-8M: A Large-Scale Video Classification Benchmark | http://arxiv.org/abs/1609.08675 | id:1609.08675 author:Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan category:cs.CV  published:2016-09-27 summary:Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics. version:1
arxiv-1609-08669 | A Transportation $L^p$ Distance for Signal Analysis | http://arxiv.org/abs/1609.08669 | id:1609.08669 author:Matthew Thorpe, Serim Park, Soheil Kolouri, Gustavo K. Rohde, Dejan SlepÄev category:cs.CV  published:2016-09-27 summary:Transport based distances, such as the Wasserstein distance and earth mover's distance, have been shown to be an effective tool in signal and image analysis. The success of transport based distances is in part due to their Lagrangian nature which allows it to capture the important variations in many signal classes. However these distances require the signal to be nonnegative and normalized. Furthermore, the signals are considered as measures and compared by redistributing (transporting) them, which does not directly take into account the signal intensity. Here we study a transport-based distance, called the $TL^p$ distance, that combines Lagrangian and intensity modelling and is directly applicable to general, non-positive and multi-channelled signals. The framework allows the application of existing numerical methods. We give an overview of the basic properties of this distance and applications to classification, with multi-channelled, non-positive one and two-dimensional signals, and color transfer. version:1
arxiv-1609-08667 | Deep Reinforcement Learning for Mention-Ranking Coreference Models | http://arxiv.org/abs/1609.08667 | id:1609.08667 author:Kevin Clark, Christopher D. Manning category:cs.CL  published:2016-09-27 summary:Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment with two approaches: the REINFORCE policy gradient algorithm and a reward-rescaled max-margin objective. We find the latter to be more effective, resulting in significant improvements over the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task. version:1
arxiv-1609-08663 | Learning Genomic Representations to Predict Clinical Outcomes in Cancer | http://arxiv.org/abs/1609.08663 | id:1609.08663 author:Safoora Yousefi, Congzheng Song, Nelson Nauata, Lee Cooper category:cs.NE cs.LG  published:2016-09-27 summary:Genomics are rapidly transforming medical practice and basic biomedical research, providing insights into disease mechanisms and improving therapeutic strategies, particularly in cancer. The ability to predict the future course of a patient's disease from high-dimensional genomic profiling will be essential in realizing the promise of genomic medicine, but presents significant challenges for state-of-the-art survival analysis methods. In this abstract we present an investigation in learning genomic representations with neural networks to predict patient survival in cancer. We demonstrate the advantages of this approach over existing survival analysis methods using brain tumor data. version:1
arxiv-1609-08661 | Task Specific Adversarial Cost Function | http://arxiv.org/abs/1609.08661 | id:1609.08661 author:Antonia Creswell, Anil A. Bharath category:cs.CV  published:2016-09-27 summary:The cost function used to train a generative model should fit the purpose of the model. If the model is intended for tasks such as generating perceptually correct samples, it is beneficial to maximise the likelihood of a sample drawn from the model, Q, coming from the same distribution as the training data, P. This is equivalent to minimising the Kullback-Leibler (KL) distance, KL[Q P]. However, if the model is intended for tasks such as retrieval or classification it is beneficial to maximise the likelihood that a sample drawn from the training data is captured by the model, equivalent to minimising KL[P Q]. The cost function used in adversarial training optimises the Jensen-Shannon entropy which can be seen as an even interpolation between KL[Q P] and KL[P Q]. Here, we propose an alternative adversarial cost function which allows easy tuning of the model for either task. Our task specific cost function is evaluated on a dataset of hand-written characters in the following tasks: Generation, retrieval and one-shot learning. version:1
arxiv-1609-08550 | Correct classification for big/smart/fast data machine learning | http://arxiv.org/abs/1609.08550 | id:1609.08550 author:Sander Stepanov category:cs.LG cs.IT math.IT  published:2016-09-27 summary:Table (database) / Relational database Classification for big/smart/fast data machine learning is one of the most important tasks of predictive analytics and extracting valuable information from data. It is core applied technique for what now understood under data science and/or artificial intelligence. Widely used Decision Tree (Random Forest) and rare used rule based PRISM , VFST, etc classifiers are empirical substitutions of theoretically correct to use Boolean functions minimization. Developing Minimization of Boolean functions algorithms is started long time ago by Edward Veitch's 1952. Since it, big efforts by wide scientific/industrial community was done to find feasible solution of Boolean functions minimization. In this paper we propose consider table data classification from mathematical point of view, as minimization of Boolean functions. It is shown that data representation may be transformed to Boolean functions form and how to use known algorithms. For simplicity, binary output function is used for development, what opens doors for multivalued outputs developments. version:1
arxiv-1609-06764 | Saturating Splines and Feature Selection | http://arxiv.org/abs/1609.06764 | id:1609.06764 author:Nicholas Boyd, Trevor Hastie, Stephen Boyd, Benjamin Recht, Michael Jordan category:stat.ML  published:2016-09-21 summary:We extend the adaptive regression spline model by incorporating saturation, the natural requirement that a function extend as a constant outside a certain range. We fit saturating splines to data using a convex optimization problem over a space of measures, which we solve using an efficient algorithm based on the conditional gradient method. Unlike many existing approaches, our algorithm solves the original infinite-dimensional (for splines of degree at least two) optimization problem without pre-specified knot locations. We then adapt our algorithm to fit generalized additive models with saturating splines as coordinate functions and show that the saturation requirement allows our model to simultaneously perform feature selection and nonlinear function fitting. Finally, we briefly sketch how the method can be extended to higher order splines and to different requirements on the extension outside the data range. version:2
arxiv-1609-08502 | Exact and Inexact Subsampled Newton Methods for Optimization | http://arxiv.org/abs/1609.08502 | id:1609.08502 author:Raghu Bollapragada, Richard Byrd, Jorge Nocedal category:math.OC stat.ML  published:2016-09-27 summary:The paper studies the solution of stochastic optimization problems in which approximations to the gradient and Hessian are obtained through subsampling. We first consider Newton-like methods that employ these approximations and discuss how to coordinate the accuracy in the gradient and Hessian to yield a superlinear rate of convergence in expectation. The second part of the paper analyzes an inexact Newton method that solves linear systems approximately using the conjugate gradient (CG) method, and that samples the Hessian and not the gradient (the gradient is assumed to be exact). We provide a complexity analysis for this method based on the properties of the CG iteration and the quality of the Hessian approximation, and compare it with a method that employs a stochastic gradient iteration instead of the CG method. We report preliminary numerical results that illustrate the performance of inexact subsampled Newton methods on machine learning applications based on logistic regression. version:1
arxiv-1609-08496 | Topic Modeling over Short Texts by Incorporating Word Embeddings | http://arxiv.org/abs/1609.08496 | id:1609.08496 author:Jipeng Qiang, Ping Chen, Tong Wang, Xindong Wu category:cs.CL cs.IR cs.LG  published:2016-09-27 summary:Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this prob- lem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn se- mantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo- texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models. version:1
arxiv-1609-08492 | WS4A: a Biomedical Question and Answering System based on public Web Services and Ontologies | http://arxiv.org/abs/1609.08492 | id:1609.08492 author:Miguel J. Rodrigues, Miguel FalÃ©, Andre Lamurias, Francisco M. Couto category:cs.CL cs.IR  published:2016-09-27 summary:This paper describes our system, dubbed WS4A (Web Services for All), that participated in the fourth edition of the BioASQ challenge (2016). We used WS4A to perform the Question and Answering (QA) task 4b, which consisted on the retrieval of relevant concepts, documents, snippets, RDF triples, exact answers and ideal answers for each given question. The novelty in our approach consists on the maximum exploitation of existing web services in each step of WS4A, such as the annotation of text, and the retrieval of metadata for each annotation. The information retrieved included concept identifiers, ontologies, ancestors, and most importantly, PubMed identifiers. The paper describes the WS4A pipeline and also presents the precision, recall and f-measure values obtained in task 4b. version:1
arxiv-1609-08475 | Blind Facial Image Quality Enhancement using Non-Rigid Semantic Patches | http://arxiv.org/abs/1609.08475 | id:1609.08475 author:Ester Hait, Guy Gilboa category:cs.CV  published:2016-09-27 summary:We propose to combine semantic data and registration algorithms to solve various image processing problems such as denoising, super-resolution and color-correction. It is shown how such new techniques can achieve significant quality enhancement, both visually and quantitatively, in the case of facial image enhancement. Our model assumes prior high quality data of the person to be processed, but no knowledge of the degradation model. We try to overcome the classical processing limits by using semantically-aware patches, with adaptive size and location regions of coherent structure and context, as building blocks. The method is demonstrated on the problem of cellular photography enhancement of dark facial images for different identities, expressions and poses. version:1
arxiv-1609-08445 | AP16-OL7: A Multilingual Database for Oriental Languages and A Language Recognition Baseline | http://arxiv.org/abs/1609.08445 | id:1609.08445 author:Dong Wang, Lantian Li, Difei Tang, Qing Chen category:cs.CL cs.AI  published:2016-09-27 summary:We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research. version:1
arxiv-1609-08442 | Collaborative Learning for Language and Speaker Recognition | http://arxiv.org/abs/1609.08442 | id:1609.08442 author:Lantian Li, Zhiyuan Tang, Dong Wang, Yang Feng, Shiyue Zhang category:cs.SD cs.CL  published:2016-09-27 summary:This paper presents a unified model to perform language and speaker recognition simultaneously and altogether. The model is based on a multi-task recurrent neural network where the output of one task is fed as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by borrowing information from each other. Our experiments demonstrated that the multi-task model outperforms the task-specific models on both tasks. version:1
arxiv-1609-08441 | Weakly Supervised PLDA Training | http://arxiv.org/abs/1609.08441 | id:1609.08441 author:Lantian Li, Yixiang Chen, Dong Wang, Chenghui Zhao category:cs.LG cs.AI cs.CL cs.SD  published:2016-09-27 summary:PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training. Our experimental results on real-life large-scale telephony customer service achieves demonstrated that the weak training can offer good performance when human-labelled data are limited. More interestingly, the weak training can be employed as a discriminative adaptation approach, which is more efficient than the prevailing unsupervised method when human-labelled data are insufficient. version:1
arxiv-1609-08438 | Flows Generating Nonlinear Eigenfunctions | http://arxiv.org/abs/1609.08438 | id:1609.08438 author:Raz Z. Nossek, Guy Gilboa category:cs.CV cs.NA  published:2016-09-27 summary:Nonlinear variational methods have become very powerful tools for many image processing tasks. Recently a new line of research has emerged, dealing with nonlinear eigenfunctions induced by convex functionals. This has provided new insights and better theoretical understanding of convex regularization and introduced new processing methods. However, the theory of nonlinear eigenvalue problems is still at its infancy. We present a new flow that can generate nonlinear eigenfunctions of the form $T(u)=\lambda u$, where $T(u)$ is a nonlinear operator and $\lambda \in \mathbb{R} $ is the eigenvalue. We develop the theory where $T(u)$ is a subgradient element of a regularizing one-homogeneous functional, such as total-variation (TV) or total-generalized-variation (TGV). We introduce two flows: a forward flow and an inverse flow; for which the steady state solution is a nonlinear eigenfunction. The forward flow monotonically smooths the solution (with respect to the regularizer) and simultaneously increases the $L^2$ norm. The inverse flow has the opposite characteristics. For both flows, the steady state depends on the initial condition, thus different initial conditions yield different eigenfunctions. This enables a deeper investigation into the space of nonlinear eigenfunctions, allowing to produce numerically diverse examples, which may be unknown yet. In addition we suggest an indicator to measure the affinity of a function to an eigenfunction and relate it to pseudo-eigenfunctions in the linear case. version:1
arxiv-1609-08435 | Asynchronous Stochastic Proximal Optimization Algorithms with Variance Reduction | http://arxiv.org/abs/1609.08435 | id:1609.08435 author:Qi Meng, Wei Chen, Jingcheng Yu, Taifeng Wang, Zhi-Ming Ma, Tie-Yan Liu category:cs.LG  published:2016-09-27 summary:Regularized empirical risk minimization (R-ERM) is an important branch of machine learning, since it constrains the capacity of the hypothesis space and guarantees the generalization ability of the learning algorithm. Two classic proximal optimization algorithms, i.e., proximal stochastic gradient descent (ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely used to solve the R-ERM problem. Recently, variance reduction technique was proposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and ProxSVRCD have better convergence rate. These proximal algorithms with variance reduction technique have also achieved great success in applications at small and moderate scales. However, in order to solve large-scale R-ERM problems and make more practical impacts, the parallel version of these algorithms are sorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG) and asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that Async-ProxSVRG can achieve near linear speedup when the training data is sparse, while Async-ProxSVRCD can achieve near linear speedup regardless of the sparse condition, as long as the number of block partitions are appropriately set. We have conducted experiments on a regularized logistic regression task. The results verified our theoretical findings and demonstrated the practical efficiency of the asynchronous stochastic proximal algorithms with variance reduction. version:1
arxiv-1609-08433 | Local Training for PLDA in Speaker Verification | http://arxiv.org/abs/1609.08433 | id:1609.08433 author:Chenghui Zhao, Lantian Li, Dong Wang, April Pu category:cs.SD cs.CL  published:2016-09-27 summary:PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labeled development data, which is highly expensive in most cases. A possible approach to mitigate the problem is various unsupervised adaptation methods, which use unlabeled data to adapt the PLDA scattering matrices to the target domain. In this paper, we present a new `local training' approach that utilizes inaccurate but much cheaper local labels to train the PLDA model. These local labels discriminate speakers within a single conversion only, and so are much easier to obtain compared to the normal `global labels'. Our experiments show that the proposed approach can deliver significant performance improvement, particularly with limited globally-labeled data. version:1
arxiv-1609-08414 | Reactive Collision Avoidance using Evolutionary Neural Networks | http://arxiv.org/abs/1609.08414 | id:1609.08414 author:Hesham Eraqi, Youssef EmadEldin, Mohamed Moustafa category:cs.NE  published:2016-09-27 summary:Collision avoidance systems can play a vital role in reducing the number of accidents and saving human lives. In this paper, we introduce and validate a novel method for vehicles reactive collision avoidance using evolutionary neural networks (ENN). A single front-facing rangefinder sensor is the only input required by our method. The training process and the proposed method analysis and validation are carried out using simulation. Extensive experiments are conducted to analyse the proposed method and evaluate its performance. Firstly, we experiment the ability to learn collision avoidance in a static free track. Secondly, we analyse the effect of the rangefinder sensor resolution on the learning process. Thirdly, we experiment the ability of a vehicle to individually and simultaneously learn collision avoidance. Finally, we test the generality of the proposed method. We used a more realistic and powerful simulation environment (CarMaker), a camera as an alternative input sensor, and lane keeping as an extra feature to learn. The results are encouraging; the proposed method successfully allows vehicles to learn collision avoidance in different scenarios that are unseen during training. It also generalizes well if any of the input sensor, the simulator, or the task to be learned is changed. version:1
arxiv-1609-08412 | OC16-CE80: A Chinese-English Mixlingual Database and A Speech Recognition Baseline | http://arxiv.org/abs/1609.08412 | id:1609.08412 author:Dong Wang, Zhiyuan Tang, Difei Tang, Qing Chen category:cs.CL  published:2016-09-27 summary:We present the OC16-CE80 Chinese-English mixlingual speech database which was released as a main resource for training, development and test for the Chinese-English mixlingual speech recognition (MixASR-CHEN) challenge on O-COCOSDA 2016. This database consists of 80 hours of speech signals recorded from more than 1,400 speakers, where the utterances are in Chinese but each involves one or several English words. Based on the database and another two free data resources (THCHS30 and the CMU dictionary), a speech recognition (ASR) baseline was constructed with the deep neural network-hidden Markov model (DNN-HMM) hybrid system. We then report the baseline results following the MixASR-CHEN evaluation rules and demonstrate that OC16-CE80 is a reasonable data resource for mixlingual research. version:1
arxiv-1609-08409 | Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks | http://arxiv.org/abs/1609.08409 | id:1609.08409 author:Savelie Cornegruta, Robert Bakewell, Samuel Withey, Giovanni Montana category:cs.CL stat.ML  published:2016-09-27 summary:Motivated by the need to automate medical information extraction from free-text radiological reports, we present a bi-directional long short-term memory (BiLSTM) neural network architecture for modelling radiological language. The model has been used to address two NLP tasks: medical named-entity recognition (NER) and negation detection. We investigate whether learning several types of word embeddings improves BiLSTM's performance on those tasks. Using a large dataset of chest x-ray reports, we compare the proposed model to a baseline dictionary-based NER system and a negation detection system that leverages the hand-crafted rules of the NegEx algorithm and the grammatical relations obtained from the Stanford Dependency Parser. Compared to these more traditional rule-based systems, we argue that BiLSTM offers a strong alternative for both our tasks. version:1
arxiv-1609-08399 | House price estimation from visual and textual features | http://arxiv.org/abs/1609.08399 | id:1609.08399 author:Eman Ahmed, Mohamed Moustafa category:cs.CV  published:2016-09-27 summary:Most existing automatic house price estimation systems rely only on some textual data like its neighborhood area and the number of rooms. The final price is estimated by a human agent who visits the house and assesses it visually. In this paper, we propose extracting visual features from house photographs and combining them with the house's textual information. The combined features are fed to a fully connected multilayer Neural Network (NN) that estimates the house price as its single output. To train and evaluate our network, we have collected the first houses dataset (to our knowledge) that combines both images and textual attributes. The dataset is composed of 535 sample houses from the state of California, USA. Our experiments showed that adding the visual features increased the R-value by a factor of 3 and decreased the Mean Square Error (MSE) by one order of magnitude compared with textual-only features. Additionally, when trained on the benchmark textual-only features housing dataset, our proposed NN still outperformed the existing model published results. version:1
arxiv-1609-08397 | Generalization Error Bounds for Optimization Algorithms via Stability | http://arxiv.org/abs/1609.08397 | id:1609.08397 author:Qi Meng, Yue Wang, Wei Chen, Taifeng Wang, Zhi-Ming Ma, Tie-Yan Liu category:stat.ML cs.LG  published:2016-09-27 summary:Many machine learning tasks can be formulated as Regularized Empirical Risk Minimization (R-ERM), and solved by optimization algorithms such as gradient descent (GD), stochastic gradient descent (SGD), and stochastic variance reduction (SVRG). Conventional analysis on these optimization algorithms focuses on their convergence rates during the training process, however, people in the machine learning community may care more about the generalization performance of the learned model on unseen test data. In this paper, we investigate on this issue, by using stability as a tool. In particular, we decompose the generalization error for R-ERM, and derive its upper bound for both convex and non-convex cases. In convex cases, we prove that the generalization error can be bounded by the convergence rate of the optimization algorithm and the stability of the R-ERM process, both in expectation (in the order of $\mathcal{O}((1/n)+\mathbb{E}\rho(T))$, where $\rho(T)$ is the convergence error and $T$ is the number of iterations) and in high probability (in the order of $\mathcal{O}\left(\frac{\log{1/\delta}}{\sqrt{n}}+\rho(T)\right)$ with probability $1-\delta$). For non-convex cases, we can also obtain a similar expected generalization error bound. Our theorems indicate that 1) along with the training process, the generalization error will decrease for all the optimization algorithms under our investigation; 2) Comparatively speaking, SVRG has better generalization ability than GD and SGD. We have conducted experiments on both convex and non-convex problems, and the experimental results verify our theoretical findings. version:1
arxiv-1609-08393 | Semi Automatic Color Segmentation of Document Pages | http://arxiv.org/abs/1609.08393 | id:1609.08393 author:StÃ©phane Bres, VÃ©ronique Eglin, Vincent Poulain category:cs.CV  published:2016-09-27 summary:-This paper presents a semi automatic method used to segment color documents into different uniform color plans. The practical application is dedicated to administrative documents segmentation. In these documents, like in many other cases, color has a semantic meaning: it is then possible to identify some specific regions like manual annotations, rubber stamps or colored highlighting. A first step of user-controlled learning of the desired color plans is made on few sample documents. An automatic process can then be performed on the much bigger set as a batch. Our experiments show very interesting results in with a very competitive processing time. version:1
arxiv-1609-08391 | Multiple protein feature prediction with statistical relational learning | http://arxiv.org/abs/1609.08391 | id:1609.08391 author:Luca Masera category:stat.ML q-bio.QM  published:2016-09-27 summary:High throughput sequencing techniques have highly impactedon modern biology, widening the gap between sequenced andannotated data. Automatic annotation tools are thereforeof the foremost importance to guide biologists' experiments. However, most of the state-of-the-art methods rely on annotation transfer, offering reliable predictions only in homology settings. In this work we present a novel appraoch to protein feature prediction, which exploits the Semanti Based Regularization to inject prior knowledge in the learning process. The experimental results conducted on the yeast genome show that the introduction of the constraints positively impacts on the overall prediction quality. version:1
arxiv-1610-01989 | Regularized Dynamic Boltzmann Machine with Delay Pruning for Unsupervised Learning of Temporal Sequences | http://arxiv.org/abs/1610.01989 | id:1610.01989 author:Sakyasingha Dasgupta, Takayuki Yoshizumi, Takayuki Osogami category:cs.LG cs.NE stat.ML  published:2016-09-22 summary:We introduce Delay Pruning, a simple yet powerful technique to regularize dynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a particularly structured Boltzmann machine, as a generative model of a multi-dimensional time-series. This Boltzmann machine can have infinitely many layers of units but allows exact inference and learning based on its biologically motivated structure. DyBM uses the idea of conduction delays in the form of fixed length first-in first-out (FIFO) queues, with a neuron connected to another via this FIFO queue, and spikes from a pre-synaptic neuron travel along the queue to the post-synaptic neuron with a constant period of delay. Here, we present Delay Pruning as a mechanism to prune the lengths of the FIFO queues (making them zero) by setting some delay lengths to one with a fixed probability, and finally selecting the best performing model with fixed delays. The uniqueness of structure and a non-sampling based learning rule in DyBM, make the application of previously proposed regularization techniques like Dropout or DropConnect difficult, leading to poor generalization. First, we evaluate the performance of Delay Pruning to let DyBM learn a multidimensional temporal sequence generated by a Markov chain. Finally, we show the effectiveness of delay pruning in learning high dimensional sequences using the moving MNIST dataset, and compare it with Dropout and DropConnect methods. version:1
arxiv-1610-01546 | Conversational Recommendation System with Unsupervised Learning | http://arxiv.org/abs/1610.01546 | id:1610.01546 author:Yueming Sun, Yi Zhang, Yunfei Chen, Roger Jin category:cs.CL cs.IR cs.LG  published:2016-09-22 summary:We will demonstrate a conversational products recommendation agent. This system shows how we combine research in personalized recommendation systems with research in dialogue systems to build a virtual sales agent. Based on new deep learning technologies we developed, the virtual agent is capable of learning how to interact with users, how to answer user questions, what is the next question to ask, and what to recommend when chatting with a human user. Normally a descent conversational agent for a particular domain requires tens of thousands of hand labeled conversational data or hand written rules. This is a major barrier when launching a conversation agent for a new domain. We will explore and demonstrate the effectiveness of the learning solution even when there is no hand written rules or hand labeled training data. version:1
